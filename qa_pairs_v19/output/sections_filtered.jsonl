{"corpusid_sectionid": "8729907-s1", "title": "The Language ENvironment Analysis (LENA) System: A Literature Review", "date": "2016-11-15", "section_title": "The LENA System", "section": "The LENA System's hardware includes a digital language processor (DLP) that can audio record for up to 16 hours. It measures 3-3/8\" x 2-3/16\" x 1/2\", weighs less than two ounces, and consists of a display screen, a USB port for uploading, and two buttons for powering and recording. The processor is held in a specially designed t-shirt or vest with a pocket on the front to secure the device. The audio quality is a 16-bit channel at a 16kHz sample rate (Ford, Baer, Xu, Yapanel, & Gray, 2008). Once the recording is complete it can be uploaded to the LENA software. Recordings are stored in the software by participant, allowing repeated recordings of one participant to be saved and compared over time. Once uploaded and recharged, the same participant or a new participant can use the DLP again without affecting the data stored in the software. The LENA System automatically segments the recordings into 12 categories including speakers, environmental sounds, and silence using Gaussian mixture models. A daylong audio file typically consists of 20,000 to 50,000 segments (VanDam et al., 2016). The software then estimates: adult word count (AWC), child vocalization count (CVC), and conversational turn count (CTC). The amount of background noise, electronic sounds, meaningful speech, and silence that were part of the child's listening environment are reported as percentages of the total sound present in the day and are displayed in user-friendly LENA generated graphs along with the AWC, CVC, and CTC. Additional details can be extracted using ADEX software provided by the LENA Foundation (Ford, et al., 2008;VanDam, Ambrose, & Moeller, 2012).\n\nIn addition to the raw data counts, Richards, Gilkerson, Paul, & Xu (2008) discuss the Automatic Vocalization Assessment (AVA) generated by the LENA System, which is correlated with traditional expressive language standard scores including those from the Preschool Language Scale -4 th Edition (PLS-4) (Zimmerman, Steiner, & Pond, 2002) and the Receptive-Expressive Emergent Language Test -3 rd Edition (REEL-3) (Bzoch, League, & Brown, 2003). To learn more about the LENA hardware and software, consult Ford et al. (2008) and .\n\nIn order to establish reliability, human transcribers coded 70 full day English recordings and their results were compared with those obtained by the automated software (Xu, Yapanel, Gray, & Baer, 2008). This data was collected as part of the Natural Language Study (NLS), the LENA Foundation's normative study . The LENA System correctly identified 82 and 76 percent of the segments humans coded as adult speech and child vocalizations respectively, indicating reasonable levels of agreement Warren et al., 2010;Xu et al., 2008;). Validity has also been shown in Spanish, French, Mandarin, Korean, and Vietnamese (Canault, Le Normand, Foudil, Loundon, & Thai- Van, 2015;Ganek & Eriks-Brophy, in revision;Gilkerson et al., 2015;Pae et al., 2016;Weisleder & Fernald, 2013). Although these studies show high fidelity, recording in a child's natural environment can produce a degraded auditory signal that may negatively impact validation. Possible causes of interference might include environmental factors such as background noise, overlapping speech, and reverberation, speaker variation like pitch or voice quality, and hardware variability. Although LENA clothing has been rigorously tested, fabric sound absorption rates may also impact accuracy ).", "filtered_refids": [[null, "b10", "b39", "b41"], ["b33", "b10", "b2", "b55"], ["b52", "b14", "b47", "b31", "b45", null, "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3438, "num_references": 15}
{"corpusid_sectionid": "8729907-s4", "title": "The Language ENvironment Analysis (LENA) System: A Literature Review", "date": "2016-11-15", "section_title": "Type of Study", "section": "Studies were divided into three types: comparative studies that examined LENA results between at least two cohorts, longitudinal studies that measured children's progress over time, and crosssectional studies that investigated children's ability at a specific point in time. Sixteen of the papers reviewed were comparative. They generally matched typically developing children to children with a communication disorder, though some compared language groups or treatment versus control groups. Eleven longitudinal studies evaluated child development over time. Both comparative and longitudinal studies measured the effects of treatment. Treatments including traditional speech therapy , formal established treatment programs such as Hanen's It Takes Two to Talk (Manolson, 1992;Weil & Middleton, 2011), and treatment associated specifically with provision of LENA feedback (Pae et al., 2016;Suskind et al., 2013). The remaining eleven cross-sectional studies often relied on a single day of recording.", "filtered_refids": [["b38", "b23", "b31", "b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1001, "num_references": 4}
{"corpusid_sectionid": "8729907-s15", "title": "The Language ENvironment Analysis (LENA) System: A Literature Review", "date": "2016-11-15", "section_title": "Socio-Economic Status (SES)", "section": "Socio-economic status (SES) is a measure of a person's social position based on income, education, and occupation. Hart and Risley (1995) famously reported a correlation between SES, language stimulation, and language abilities. Their study, and those like it, inspired the creation of the LENA System. Even though the impact of SES on language outcomes is widely known, few of the studies reported here were able to control for it. Ten studies failed to report SES and another six reported that comparative groups were matched either to each other or to census data. Six represented a range of maternal educational levels. Nine of the studies reported that their samples skewed towards high SES participants while five others reported collecting only low SES participants. Two studies also reported an SES mismatch between comparative groups (Jackson & Callender, 2014;Wood, et al., 2016).", "filtered_refids": [["b49", "b16", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 890, "num_references": 3}
{"corpusid_sectionid": "263153015-s2", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Background", "section": "In recent years, with the continuous expansion of computing power, large-scale language models have sprung up (Brown et al., 2020;OpenAI, 2023;Touvron et al., 2023a;Scao et al., 2022;Touvron et al., 2023b;Zhao et al., 2023b), and as the model size continues to grow, many new capabilities have emerged, such as in-context learning and chain-ofthought reasoning (Brown et al., 2020;Wei et al., 2022b,a;Schaeffer et al., 2023).Brown et al. (2020) finds that large-scale language models have excellent in-context learning (ICL) ability.ICL incorporates input-output demonstrations into the prompt text.With ICL, off-the-shelf LLMs can be employed without additional fine-tuning while achieving comparable performance.Nevertheless, this end-to-end approach tends to underperform when faced with complex reasoning tasks.Wei et al. (2022b) finds that the reasoning ability of LLMs can be improved by adding step-by-step reasoning processes to the demonstration, which is known as chain-of-thought prompting.CoT prompting enables the model to gain a more precise understanding of both the question's intricacies and the reasoning process.Furthermore, the model generates a sequence of reasoning steps, which grants us a transparent view of the model's cognitive process, further enhancing interpretability.", "filtered_refids": [["b158", null, "b8", "b124"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1299, "num_references": 4}
{"corpusid_sectionid": "263153015-s4", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Mathematical Reasoning", "section": "Mathematical reasoning is often used to measure the reasoning power of a model.Early benchmarks contain simple arithmetic operations (Hosseini et al., 2014;Koncel-Kedziorski et al., 2015;Roy and Roth, 2015;Koncel-Kedziorski et al., 2016).Ling et al. (2017) labels the reasoning process in natural language form, and Amini et al. (2019) builds on AQUA by labeling the reasoning process in program form.Later benchmarks (Miao et al., 2020;Patel et al., 2021;Cobbe et al., 2021;Gao et al., 2023) contain more complex and diverse questions.(Zhu et al., 2021;Chen et al., 2021Chen et al., , 2022b) ) require reasoning based on the table content.\n\nThere are also general benchmarks (Hendrycks et al., 2021;Mishra et al., 2022a,b) and reading comprehension form benchmarks (Dua et al., 2019;Chen et al., 2023).Recently, (Yu et al., 2021a) endowed pre-trained model with the ability of mathematical reasoning by using hierarchical reasoning and knowledge.", "filtered_refids": [["b43", "b14", "b203", "b13", "b31", "b112", "b120", null, "b99", "b16", "b83", "b2", "b65"], ["b26", "b44", "b179", "b41", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 947, "num_references": 18}
{"corpusid_sectionid": "263153015-s5", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Commonsense Reasoning", "section": "Commonsense reasoning is the process of making inferences, judgments, and understandings based on knowledge that is generally known and commonly perceived in the everyday world.How to acquire and understand commonsense knowledge  (Ling et al., 2017) 100,000 Question Option Natural Language Math reasoning with NL rationale ASDiv (Miao et al., 2020) 2305 Question Number Equation Multi-step math reasoning SVAMP (Patel et al., 2021) 1,000 Question Number Equation Multi-step math reasoning GSM8K (Cobbe et al., 2021) 8,792 Question Number Natural Language Multi-step math reasoning GSM-Hard (Gao et al., 2023) 936 Question Number Natural Language GSM8K with larger number MathQA (Amini et al., 2019) 37,297 Question Number Operation Annotated based on AQUA DROP (Dua et al., 2019) 96,567 Question+Passage Number+Span Equation Reading comprehension form TheoremQA (Chen et al., 2023) 800 Question+Theorem Number \u2717 Answer based on theorems TAT-QA (Zhu et al., 2021) 16,552 Question+Table+Text Number+Span Operation Answer based on tables FinQA (Chen et al., 2021) 8  (Park et al., 2020) 1,465,704 Image+Event Action+Intent \u2717 Visual commonsense reasoning PMR (Dong et al., 2022) 15,360 Image+Background Option \u2717 Premise-based multi-modal reasoning ScienceQA (Lu et al., 2022) 21,208 Q+Image+Context Option Natural Language Multi-modal reasoning with NL rationales VLEP (Lei et al., 2020) 28,726 Premise+Video Option \u2717 Video event prediction CLEVRER (Yi et al., 2020) 305,280 Question+Video Option/Free-form Program Video temporal and causal reasoning STAR (Wu et al., 2021) 600,000 Question+Video Option \u2717 Video situated reasoning NEXT-QA (Xiao et al., 2021) 47 is a major impediment to models facing commonsense reasoning.Many benchmarks and tasks are proposed focusing on commonsense understanding (Talmor et al., 2019(Talmor et al., , 2021;;Bhakthavatsalam et al., 2021;Mihaylov et al., 2018;Geva et al., 2021;Huang et al., 2019;Bisk et al., 2020), event temporal commonsense reasoning (Rashkin et al., 2018;Zhou et al., 2019) , and commonsense verification (Wang et al., 2019).", "filtered_refids": [["b90", "b13", "b160", "b83", "b174", "b203", "b49", "b112", "b7", "b16", "b2", "b200", "b163", "b99", "b119", "b24", "b145", "b138", "b5", "b111", "b100", "b26", "b44", "b31", "b137", "b72", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 2078, "num_references": 27}
{"corpusid_sectionid": "263153015-s12", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Manual XoT", "section": "While large language models perform few-shot incontext learning via prompting, they are still limited in reasoning tasks.In order to explore the potential reasoning ability of large language models, one standard approach is to provide different forms of thoughts in demonstrations.Wei et al. (2022b) first propose chain-of-thought prompting (Few-shot CoT) by manually providing natural language form rationales to the demonstrations.To further ensure certainty in the reasoning process and reduce inconsistencies between reasoning path and answers, PAL (Gao et al., 2023), PoT (Chen et al., 2022a) and NLEP (Zhang et al., 2023e) leverage programming language as annotated rationales, which transforms the problemsolving into an executable Python program.Meanwhile, to take both advantages of natural language and programming language and raise the confidence of reasoning output, MathPrompter (Imani et al., 2023) uses the zero-shot chain-of-thought prompting to generate multiple algebraic expressions or Python functions, which can verify each other and improve the reliability of results.Furthermore, since the reasoning complexity of samples in demonstrations, such as chains with more reasoning steps, results in performance improvement, Fu et al. (2023a) proposes complexity-based prompting, where voting among high-complexity rationales is performed to get the final answer.\n\nManually constructed X-of-thought methods expand on in-context learning by adding different types of step-by-step intermediate reasoning processes to demonstrations.They allow LLMs to mimic and generate reasoning paths.Although manual XoT methods provide greater interpretability as well as trustworthiness for human understanding and outperform on complex tasks, i.e., mathematical reasoning, commonsense reasoning, symbolic reasoning, etc., manual annotating of rationales entails significant costs and suffers from drawbacks such as difficulty in demonstration selection and task generalization.Specifically, different tasks require different ways of demonstrations.Therefore, other works attempt to construct the reasoning path automatically, as discussed in \u00a74.1.2.", "filtered_refids": [["b11", "b52", "b31", "b29", "b158", "b190"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2153, "num_references": 6}
{"corpusid_sectionid": "263153015-s13", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Automatic XoT", "section": "Chain-of-thought prompting (Wei et al., 2022b) elicits the complex reasoning ability of LLMs with task-specific exemplars in a few-shot setting, which limits the scalability and generalization.To reduce the cost of hand-crafted few-shot exemplars, Kojima et al. (2022) proposes zero-shot CoT by introducing a magic phrase Let's think step by step after question, which enables LLMs to generate reasoning chains in a zero-shot manner.However, zero-shot CoT suffers from poor-quality reasoning paths, coming with many mistakes.Since the diversity of demonstration plays a vital role in reasoning chains generation, Auto-CoT (Zhang et al., A survey of X-of-Thought Methods ( \u00a74) XoT Construction ( \u00a74.1)\n\nManual XoT E.g., Few-shot CoT (Wei et al., 2022b) , PoT (Chen et al., 2022a) Automatic XoT E.g., Zero-shot CoT (Kojima et al., 2022), Auto-CoT (Zhang et al., 2023f) Semi-automatic XoT E.g., AutoMate CoT (Shum et al., 2023), BoostedPrompt (Pitis et al., 2023) XoT Structural Variants ( \u00a74.2) E.g., AoT (Sel et al., 2023), ToT (Yao et al., 2023b), SoT (Ning et al., 2023), GoT (Lei et al., 2023a) XoT Enhancement Methods ( \u00a74.3)\n\nVerify and Refine E.g., VerifyCoT (Ling et al., 2023), Self-Refine (Madaan et al., 2023) Question Decompose E.g., Least-to-Most Prompting (Zhou et al., 2023b) External Knowledge E.g., CoK (Wang et al., 2023b), KD-CoT (Wang et al., 2023c) Vote and Rank E.g., Verifiers (Cobbe et al., 2021), Self-Consistency (Wang et al., 2023j) Efficiency E.g., SoT (Ning et al., 2023), ActivePrompting (Diao et al., 2023) Frontier Application ( \u00a75)\n\nTool Using ( \u00a75.1) E.g., MRKL (Karpas et al., 2022), TAML (Parisi et al., 2022a), Toolformer (Schick et al., 2023) Planning ( \u00a75.2) E.g., ToT (Long, 2023), ReAct (Yao et al., 2023c), Reflexion (Shinn et al., 2023) Distillation ( \u00a75.3) E.g., STaR (Zelikman et al., 2022), SCoTD (Li et al., 2023b), SCOTT (Wang et al., 2023h) Future Directions ( \u00a76)\n\nMulti-modal ( \u00a76.1) E.g., MMCoT (Zhang et al., 2023g), T-SciQ (Wang et al., 2023d), ToMT (Hu et al., 2023b) Faithfulness ( \u00a76.2) E.g., Rethinking and Retrievaling (He et al., 2023a), Measure Faithful (Lanham et al., 2023) CoT Theory ( \u00a76.3) E.g., Tang (Tang et al., 2023), Li (Li et al., 2023e), Feng (Feng et al., 2023), Wu (Wu et al., 2023) Benchmarks ( \u00a73) Mathematical Reasoning ( \u00a73.1) E.g., MultiArith (Roy and Roth, 2015), AQUA-RAT (Ling et al., 2017), SVAMP (Patel et al., 2021), GSM8K (Cobbe et al., 2021), DROP (Dua et al., 2019), LILA (Mishra et al., 2022a) Commonsense Reasoning ( \u00a73.2) E.g., CSQA (Talmor et al., 2019), ARC (Bhakthavatsalam et al., 2021), PIQA (Bisk et al., 2020), McTaco (Zhou et al., 2019), CosmosQA (Huang et al., 2019), StrategyQA (Geva et al., 2021) Symbolic Reasoning ( \u00a73.3) E.g.,Last Letter Concat.(Wei et al., 2022b), Coin Flip (Wei et al., 2022b), Reverse List (Wei et al., 2022b) BigBench (Srivastava et al., 2022), BigBench-Hard (Suzgun et al., 2023) Logical Reasoning ( \u00a73.4) E.g., ReClor (Yu et al., 2020), LogiQA (Liu et al., 2020), ProofWriter (Tafjord et al., 2021), FOLIO (Han et al., 2022), PrOntoQA (Saparov and He, 2023) Multi-modal Reasoning ( \u00a73.5) Image: ScienceQA (Lu et al., 2022), VCR (Zellers et al., 2019), VisualCOMET (Park et al., 2020) Video: CLEVRER (Yi et al., 2020), STAR (Wu et al., 2021), NExT-QA (Xiao et al., 2021) Figure 1: XoT Methods, Frontier Application, Future Direction, and Benchmarks.\n\n2023f) generates the demonstrations automatically via clustering and representative exemplars selection, which improves the diversity and consistently matches or exceeds the performance of Few-shot CoT.COSP (Wan et al., 2023) introduces the outcome entropy of the question to aid demonstration selection.Xu et al. (2023) proposes Reprompting to find the effective CoT prompt by employing Gibbs sampling iteratively.Meanwhile, some mistakes in reasoning chains come from missing-step errors, Wang et al. (2023f) extend the zero-shot CoT into Plan-and-Solve (PS) Prompting via devising a plan to divide the entire task into smaller sub-tasks and carrying out the sub-tasks according to the plan with more detailed instructions.Logi-CoT (Zhao et al., 2023c) uses symbolic logic to validate the zero-shot reasoning process, thus reducing errors in reasoning.Besides, PoT (Chen et al., 2022a) also explore language models, such as Codex, to generate an executable Python program to solve math problems in zero-shot setting via adding Let's write a Python program step by step..., which mitigates errors in intermediate rea-soning steps.Some work introduces agents to solve reasoning problems.For example, Agent Instruct (Crispino et al., 2023a) utilizes agents to generate task-related, informative instructions, which guides LLMs to perform zero-shot reasoning.Unlike manual XoT, automatic XoT, using zeroshot prompt engineering or sampling, is scalable and can be generalized between domains without human intervention.However, due to the lack of human alignment, automatically generated chain-ofthought encounters challenges such as poor quality, hallucinations, and factual inconsistencies.Therefore, constructing XoT in a semi-automatic way is necessary, which is introduced in \u00a74.1.3.", "filtered_refids": [["b158", null, "b62"], ["b11", "b98", "b192", "b158", "b30", "b114", "b62", "b126", "b70", "b132"], ["b155", "b98", "b146", "b147", null, "b22", "b84", "b16", "b201"], ["b182", "b58", "b109", "b130", "b152", "b88", "b76", "b125", "b169"], ["b135", "b90", "b136", "b87", "b158", "b160", "b33", "b83", "b174", "b178", "b46", "b49", "b112", "b7", "b16", "b183", "b161", "b28", "b200", "b163", "b193", "b68", "b5", "b111", "b39", "b26", "b82", "b101", "b137", "b122", "b120", null, "b139", "b148", "b37"], ["b11", "b197", "b150", null, "b142", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 5162, "num_references": 72}
{"corpusid_sectionid": "263153015-s14", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Semi-automatic XoT", "section": "Semi-automatic XoT methods integrate the advantages of both manual and automatic construction methods.Shao et al. (2023) proposes Synthetic Prompting, which leverages a few humanannotated examples to prompt models to generate more examples through an alternated forwardbackward process and selects effective demonstrations to elicit better reasoning, alleviating the lack of human alignment in AutoCoT.Although previous work solves the problem of manual annotating, demonstration selection can also significantly affect performance.Automate-CoT (Shum et al., 2023) employs reinforcement learning with a variance-reduced policy gradient strategy to estimate the significance of each example in a blackbox language model, eliciting better demonstration selection.Similarly, Lu et al. (2023b) proposes PromptPG, which utilizes policy gradient to learn to select demonstrations in tabular reasoning.Ye and Durrett (2023) initially uses two proxy metrics to evaluate each example and then searches over examples to find demonstrations that yield the best performance in a silver-labeled development set.Meanwhile, Pitis et al. (2023) proposes Boosted Prompting, a prompt ensembling way to improve the performance, which iteratively expands the examples when encountering the problem that the current demonstration is challenging to handle.Zou et al. (2023) introduce Meta-CoT, which automatically selects demonstrations based on the question category, eliminating the need for the task-specific prompt design.\n\nThe semi-automatic XoT methods reduce the workload of manual labeling while introducing human alignment signals and demonstration selection strategies to enhance the capability and stability of reasoning.Additionally, it enables cost-effective domain generalization.However, the demonstration selection problem has not been entirely resolved and requires more effort and research.", "filtered_refids": [["b204", "b91", "b127", "b114", "b172", "b132"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1886, "num_references": 6}
{"corpusid_sectionid": "263153015-s15", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "XoT Structural Variants", "section": "The most primitive chain-of-thought is a chain structure that describes intermediate reasoning steps in natural language.In this section, we introduce structural variants that modify the original chain structure, including chain structure variants, tree structure variants, and graph structure variants.\n\nChain Structure PAL (Gao et al., 2023) and PoT (Chen et al., 2022a) introduce programming languages to describe the reasoning process, thereby converting the reasoning problem into the implementation of an executable program to obtain the final answer.Since the program execution is deterministic and performs arithmetic computations accurately, this approach shows excellent performance in mathematical reasoning.Besides, symbol sequence is another type of thought representation.Chain-of-Symbol (Hu et al., 2023a) represents the complex environments with condensed symbolic chain representations during planning, which reduces the complexity of the simulation environment.Chain structure variants are shown in Figure 2(c,d) Algorithm of Thought (Sel et al., 2023) injects algorithmic capabilities into the model, making the model's reasoning more logical by adding examples based on algorithms.Its absence of the huge search space of tree search (Long, 2023;Yao et al., 2023b) saves computational resources and achieves excellent performance.\n\nTree Structure The original chain structure inherently limits the scope of exploration.Through the incorporation of tree structures and tree search algorithms, models gain the capability to efficiently explore and backtrack during the reasoning process (Long, 2023;Yao et al., 2023b), as shown in Figure 2(e).Combined with self-assessment of intermediate thoughts, models can achieve global optimum solutions.The reasoning process of ToT involves uncertainty, which can potentially lead to cascading errors.TouT (Mo and Xin, 2023) introduces Monte Carlo Dropout in reasoning, taking into account the uncertainty.Yu et al. (2023b) delves into analogous problems, harnessing their solutions to elevate the intricate reasoning abilities of LLMs.These analogous problems exhibit a tree-like structure, ultimately converging to solve the main problem.However, the current tree-ofthought has considerable limitations on task selection and requires specific prompt designing for each task, which hinders its widespread application.SoT (Ning et al., 2023) is another variant of the tree structure, which decomposes a problem into subproblems that can be processed in parallel and solved simultaneously to speed up reasoning.However, its utility is restricted to parallel decomposable problems and is not suited for complex reasoning tasks.\n\nGraph Structure Compared to trees, graphs introduce loops and rings, which bring more complex topological relationships and allow for modeling more complex reasoning, as shown in Figure 2(f).GoT (Besta et al., 2023;Lei et al., 2023a) regards intermediate thought as nodes within a graph, combining exploration and backtracking operations, and additionally introduces aggregation and refinement operations compared to treeof-thought.The additional operations, aggregation and refinement elicit better reasoning in complex tasks.Nevertheless, it faces the same dilemmas as the tree-of-thought, i.e., task limitations and poor generalizability.Besides, it has increased reasoning costs.Unlike GoT, which explicitly constructs a thought graph, ResPrompt (Jiang et al., 2023a) introduces residual connections between thoughts in the prompt text, allowing the reasoning of different steps to interact with each other.\n\nAs models transition from linear chains to hierarchical trees and intricate graphs, the interplay of thoughts becomes progressively more complex, thereby gradually enhancing the capacity to address intricate problems.However, as the complexity of the topology increases, associated methods impose more constraints on task selection, leading to a significant reduction in their generalizability and making their application difficult.Extending complex topology structure-based methods to general domains is a major challenge for future research.", "filtered_refids": [[], ["b11", "b31", "b45", "b88", "b30", "b126"], ["b98", "b103", "b177", "b30", "b88"], ["b70", null, "b4"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 4141, "num_references": 14}
{"corpusid_sectionid": "263153015-s17", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Verify and Refine", "section": "Chain-of-thought reasoning often tends to be hallucinatory, producing incorrect reasoning steps.Errors in intermediate reasoning steps can, in turn, trigger a cascade of errors.Incorporating verification to obtain feedback and subsequently refining the reasoning process based on this feedback can be a highly effective strategy for mitigating this phenomenon, which is similar to the process of human reflection.Figure 3 depicts the overview of verification and refinement.\n\nVerifyCoT (Ling et al., 2023) devises a Natural Program, a deductive reasoning form, which allows models to produce accurate reasoning steps, with each subsequent step strictly based on the previous steps.DIVERSE (Li et al., 2022c) utilizes a voting mechanism to eliminate incorrect answers, followed by a fine-grained verification of each reasoning step independently.SCREWS (Shridhar et al., 2023) thinks that the post-modification result may not necessarily be superior to the origin, so it introduces a selection module to select a better result between the origin and modification.To facilitate knowledge-intensive tasks, Verify-and-Edit (Zhao et al., 2023a) incorporates external knowledge to re-reason uncertain examples, reducing factual mistakes in reasoning.Some research efforts attempt to unearth the internal knowledge of models.Some research efforts attempt to unearth the internal knowledge of models.To address factual errors, some research attempts to unearth the intrinsic knowledge of LLMs.They acquire knowledge from the model before answering the questions (Dhuliawala et al., 2023;Zheng et al., 2023).Ji et al. (2023) 2023) trains a critic model to provide structured feedback on the reasoning process.Self-Refine (Madaan et al., 2023) performs iterative self-feedback and refinement to alleviate errors in reasoning.Compared with Self-Refine, Reflexion (Shinn et al., 2023) introduces reinforcement learning for reflection, which additionally brings decision-making capability.Meanwhile, some work introduces backward reasoning (Yu et al., 2023a) for verification.RCoT (Xue et al., 2023) reconstructs the question according to the reasoning chains, and its inconsistency with the original question exposes errors in the reasoning process.FOBAR (Jiang et al., 2023b) and Self Verification (Weng et al., 2022) perform verification by deducing the conditions in the question from the answer.FOBAR infers the variables in the question, and Self Verification infers the conditions in the question.However, Huang et al. (2023a) finds that LLMs struggle to self-correct without external feedback, and it could even lead to a performance decline.\n\nLLM reasoning is an unsupervised process in which feedback signals from intermediate reasoning steps play a crucial role in improving reasoning.", "filtered_refids": [[], ["b54", "b176", "b53", "b130", "b159", "b194", "b21", null, "b48", "b164", "b84", "b81", "b131"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2783, "num_references": 13}
{"corpusid_sectionid": "263153015-s20", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "External Knowledge", "section": "The parameterized knowledge in the model is fixed at the end of the pre-training, which leads to its shortcomings in terms of knowledge capacity and knowledge updating.While introducing external knowledge can alleviate this to some extent, it remains an imperfect solution.To fundamentally tackle this issue, continual learning (Lange et al., 2022;Wang et al., 2023g) stands as a promising avenue for future research endeavors.Owing to the inherent stochasticity in the generation process, LLM reasoning exhibits an element of randomness and uncertainty.This problem can be effectively alleviated through multiple sampling strategies, as shown in Figure 6.Some methods adopt ranking, such as (Cobbe et al., 2021), which trains a verifier to select high-confidence reasoning chains through ranking.Meanwhile, other methods select reasoning chains through a voting mechanism.Selfconsistency (Wang et al., 2023j) selects the most consistent answer by majority voting among sampled reasoning chains based on final answers.Furthermore, (Fu et al., 2023a) proposes Complex CoT, which utilizes a complexity-based voting strategy that leans towards selecting answers generated by more complex reasoning chains.However, answer-based voting mechanisms do not take into account the correctness of reasoning chains.Miao et al. (2023) takes the reasoning steps into account when voting, which can obtain both consistent answers and trustworthy reasoning processes simultaneously.Moreover, to consider the relations between intermediate steps across chains, Yoran et al. ( 2023) mixes information between reasoning chains and selects the most relevant facts to perform meta-reason over multiple reasoning chains.GRACE (Khalifa et al., 2023) trains a discriminator through contrastive learning and uses this discriminator to rank each intermediate reasoning step.Previous methods sample based on the probability distribution, while Diversity-of-Thought (Naik et al., 2023) obtains multiple reasoning paths by prompting with different instructions.", "filtered_refids": [["b155", "b67", "b98", "b151", "b60", "b29", "b104", "b16"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2032, "num_references": 8}
{"corpusid_sectionid": "263153015-s22", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Efficiency", "section": "LLM reasoning and manually annotated reasoning chains impose expensive overheads.Aggarwal et al. (2023) improves self-consistency by dynamically adjusting the number of samples, which can significantly reduce inference costs with marginal performance degradation.Ning et al. (2023) decomposed the questions in parallel and handled them simultaneously, reducing the reasoning time overhead.But it cannot handle complex questions.Zhang et al. (2023b) accelerates the reasoning by selectively skipping some intermediate layers and then verifies the draft in another forward pass.Diao et al. (2023) borrows ideas from active learning to annotate examples with high uncertainty, reducing the human annotating cost.\n\nLarge-scale language models have showcased immense capabilities, but they also come with substantial overhead.Balancing the trade-off between performance and overhead may require significant attention in future research endeavors.", "filtered_refids": [[null, "b22", "b98", "b0"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 941, "num_references": 4}
{"corpusid_sectionid": "263153015-s24", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Tool Use", "section": "Despite the extensive knowledge exhibited by LLMs, it is accompanied by several challenges.These encompass the incapacity to access upto-the-minute news, proclivity towards hallucinations when responding to queries involving out-ofdomain knowledge, and the absence of sophisticated reasoning capacities like mathematical calculations or symbolic reasoning.By granting LLMs the ability to employ external tools, it becomes possible to augment the model's reasoning capabilities and assimilate external knowledge, enabling it to engage in information retrieval and environmental interaction.\n\nMRKL (Karpas et al., 2022) introduces a novel framework comprising scalable modules (referred to as experts) and a router.These experts can take the form of neural networks or symbols.However, this study primarily focuses on conceptualization and training an LLM specifically for mathematical computation while not delving into implementing other module contents.TALM (Parisi et al., 2022a) and Toolformer (Schick et al., 2023) integrate a text-centric methodology with supplementary tools to enhance the capabilities of language models.They employ a self-supervise mechanism to initiate performance enhancements, commencing with a limited set of tooltips.In a similar vein, HuggingGPT (Shen et al., 2023) leverages visual and speech models to process information from diverse modalities, thereby endowing LLMs with the capacity for multi-modal understanding and generation.Another question is how to select the appropriate tool.LATM (Cai et al., 2023) enables the tool-making ability of LLMs to make generalized API across different tasks, and GEAR (Lu et al., 2023c) considers the efficiency of tool-using by using smaller models to delegate tool grounding and execution.\n\nHowever, converting a user request into API format is often not straightforward.The existing approaches mentioned above have limitations in facilitating multiple invocations of the tool and rectifying query errors.To tackle this problem, Re-Act (Yao et al., 2023c) integrates the strengths of reasoning and action to enhance and complement each other, augmenting problem-solving capability mutually.ART (Paranjape et al., 2023) uses a task library to select relevant tool usage and reasoning chains.MM-REACT (Yang et al., 2023) further utilizes vision experts to enable multi-modal reasoning and action.\n\nThe aforementioned research endeavors focus on designing tools (or APIs) to enhance the capabilities of LLMs in various domains.Combining XoT with tools effectively addresses the challenges faced by LLMs.X-of-thought reasoning enables models to effectively elicit, track, and update action plans while managing exceptions.Simultaneously, action operations facilitate the model's interaction with external sources, such as knowledge bases and environments, enabling it to gather additional information.To assess the proficiency of tools, API-Bank (Li et al., 2023c) and MetaTool (Huang et al., 2023c) introduce comprehensive benchmarks, providing a robust foundation to evaluate the performance and effectiveness of tool-augmented LLMs.", "filtered_refids": [[], ["b92", "b58", "b109", "b9", "b125", "b128"], ["b108", "b169", "b165"], ["b77", "b187"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3106, "num_references": 11}
{"corpusid_sectionid": "263153015-s25", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Planning", "section": "LLMs face challenges in providing accurate responses directly for intricate problems, necessitating the need to decompose them into sequential steps and sub-tasks.While CoT offers a straightfor-ward approach to planning, it falls short in addressing highly complex problems and lacks the ability to evaluate and rectify errors through backtracking.\n\nNumerous studies have extended the framework of chain-of-thought to various formats to enhance the capacity for planning further.Treeof-Thought (Yao et al., 2023b) enables LLMs to consider multiple reasoning paths in a tree and self-evaluate to determine the next course of action.In cases where global decisions are necessary, ToT allows forward or backward exploration through techniques like deep-first search or breadthfirst search.Reasoning via Planning (RAP) (Hao et al., 2023) also divides the problem into a tree and explores them by Monto Carlo tree search algorithm, using LLMs as both world-model and reasoning agent.Another method, Graph of Thought (GoT) (Yao et al., 2023d), employs graph nodes to represent individual thoughts and external Graph Neural Networks for organization.LLM+P (Liu et al., 2023a) and LLM+DP (Dagan et al., 2023) facilitate the generation of Planning Domain Definition Language (PDDL) (Gerevini, 2020) by LLMs.PDDL assists in decomposing complex problems and utilizing specialized models for planning before converting the results into natural language for LLM processing.However, it is essential to note that these methods use tree/graph/PDDL nodes to represent thoughts, which have limitations regarding their representation forms and can only handle specific planning problems.\n\nAnother technique is to improve the model's ability to correct errors and summarize historical experience.Self-Refine (Madaan et al., 2023) employs a unique approach where the output generated by the model is evaluated and provided with feedback using the same model.Reflexion (Shinn et al., 2023) enables the model to reflect on and rectify errors made in previous actions, resembles reinforcement learning in textual format, and involves dividing memory into long and short-term components.However, Reflexion cannot update the plan when an out-of-plan error occurs.AdaPlanner (Sun et al., 2023) introduces adaptive closed-loop plan refinement, which iterative refines the task plan based on the feedback of the environment.ISR-LLM (Zhou et al., 2023c) combines Self-Refine with PDDL to achieve a better success rate in longhorizon sequential tasks.Meanwhile, LATS (Zhou et al., 2023a) utilizes LM-based Monte Carlo Tree Search for a more flexible planning procedure.\n\nPlanning can be flexibly combined with tools (Ruan et al., 2023) or agents (Crispino et al., 2023b) to enrich reasoning ability.ToRA (Gou et al., 2023) designs mathematical specialized agents with external tools, and AutoUI (Zhang and Zhang, 2023) directly interacts with the multi-modal environment instead of converting visual inputs into text, which enhances the reasoning efficiency and reduces error propagation.\n\nPlanning augmented approaches have advanced conventional sequential planning by introducing search-based, graph-based, and definition languagebased methods.On the other hand, some methods incorporate action, planning, reflection, or tools, aiming to enhance LLMs' long-term planning and error resilience capabilities.", "filtered_refids": [[], ["b85", "b27", "b30", "b19", "b170"], ["b199", "b130", "b134", null, "b202"], ["b121", "b34", "b18", null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3376, "num_references": 14}
{"corpusid_sectionid": "263153015-s26", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "CoT Distillation", "section": "LLM can be self-improved by distilling reasoning steps to solve complex problems.Huang et al. (2022) employs an LLM with self-consistency to generate reasoning chains from unlabeled data.These chains are subsequently utilized to fine-tune the model, enhancing its generalized reasoning capabilities.Zelikman et al. (2022) proposes STaR, a few-shot learning approach to improve LM's reasoning capabilities using a self-loop bootstrap strategy.SECToR (Zhang and Parkes, 2023) uses chainof-thought to obtain arithmetic answers, then finetune the model to generate the answer without CoT directly.\n\nThought CoT is an emerging ability primarily observed in LLMs, with limited advancements in small models.However, enhancing small models' CoT ability is conceivable through techniques like distillation.Magister et al. (2023) demonstrates that fine-tuning T5 with reasoning chains generated by larger teacher models and utilizing an external calculator for answer resolution can substantially enhance task performance across diverse datasets.Ho et al. (2023) generates and filters multiple reasoning paths to enrich the diversity.\n\nNumerous endeavors can be undertaken to reduce human costs using unannotated (or very few annotated) data by utilizing the selfconsistency (Wang et al., 2023j).Hsieh et al. (2023) employs prompts to generate answers from much fewer labeled/unlabeled data, followed by the generation of rationales that prompt the language model to provide reasoning for the given answer.SCoTD (Li et al., 2023b) finds that sampling multi-ple reasoning chains per instance from teachers is paramount for improving the capability of students.SCOTT (Wang et al., 2023h) utilizes contrastive decoding (Li et al., 2022b;O'Brien and Lewis, 2023) during rationale generation for teacher models.Furthermore, to tackle the shortcut problem, it employs a counterfactual reasoning objective while training student models.DialCoT (Han et al., 2023) decomposes reasoning steps into a multi-round dialog and selects the correct path using the PPO algorithm.Jie et al. (2023); Wang et al. (2023i) add special tokens for mathematic problems.This high-level information improves the consistency of reasoning steps.\n\nThe studies above adopt a shared paradigm wherein reasoning chains are generated through LLMs possessing superior reasoning capabilities.These reasoning chains are then distilled into smaller models.The effectiveness of the distillation process is improved by augmenting the sampling strategy from the larger model, for example, through the utilization of multiple sampling paths, consistency, or contrastive decoding, which leads to improved diversity and accuracy in the generated reasoning chains, ultimately benefiting the distillation process to smaller models.It's notable that language models have intricate tradeoffs and complex balances associated with multidimensional capabilities.Fu et al. (2023b) emphasizes that increasing task-specific chain-of-thought capabilities through distillation may also adversely impact the models' performance in solving generalized problems.", "filtered_refids": [["b182", "b47", "b185"], ["b42", "b96"], ["b155", "b36", "b44", "b152", "b55", null, "b76", "b154", "b106"], ["b30"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3092, "num_references": 15}
{"corpusid_sectionid": "263153015-s28", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Multi-modal CoT", "section": "The shift from text unimodal to vision-text multimodal introduces richer information, meanwhile bringing more challenges.Some works have attempted to explore X-of-thought reasoning in multimodal scenarios by fine-tuning multi-modal models to generate a high-quality chain of thoughts.\n\nMultimodal-CoT (Zhang et al., 2023g) firstly fine-tunes multi-modal models to generate chainof-thoughts and then reasons over the rationales to obtain final answers.However, it suffers from the limitation of the linearity of the reasoning process and has difficulties in interacting between different modalities.To alleviate the challenges encountered by Multimodal-CoT, (Yao et al., 2023d) proposes Graph-of-Thought (GoT), which models the thought processes as a graph.It parses the reasoning chains into a thought graph, which enables a more realistic representation of thought processes by capturing non-sequential information interactions.This measure breaks the limitations of linear structure through graphical structures and further improves performance.Furthermore, Yao et al. (2023a) proposes Hypergraph-of-Thought (HoT), replacing thought graphs with hypergraphs, which enables models with better ability of highorder multi-hop reasoning and multi-modal comparative judgment.Meanwhile, some work takes an approach based on knowledge distillation.T-SciQ (Wang et al., 2023d) generates high-quality CoT rationales from LLMs as fine-tuning signals and introduces a novel data mixing strategy to produce effective samples for different questions.\n\nThe aforementioned studies explore multi-modal reasoning in small models and fine-tuning scenarios, which we regard as an initial endeavor in the realm of multi-modal chain-of-thought reasoning.We believe that video multi-modal reasoning combined with in-context learning should be the focus of future research.On the one hand, videos introduce additional temporal information with innate chaining relationships compared with images.Through chain-of-thought reasoning, the information in different frames can be naturally connected to explicitly model the temporal relationship, which is well-suited for video multi-modal reasoning.On the other hand, small models are capacity-limited and need fine-tuning to gain chainof-thought ability.Worse still, multi-modal reasoning chains are difficult to obtain, which further exacerbates the challenge.In comparison, contemporary vision-language foundation models (VLMs) (Alayrac et al., 2022;Li et al., 2023a;Wang et al., 2022b;Huang et al., 2023b;Peng et al., 2023;Yu et al., 2021b) have strong visionlanguage comprehension and are already capable of in-context learning with interleaved text and images.They provide a solid foundation for chain-ofthought reasoning with in-context learning.Utiliz-ing chain-of-thought for video reasoning remains an unexplored territory with only a few studies.CoMT (Hu et al., 2023b) combines fast-thinking and slow-thinking in video reasoning and introduces a tree search strategy for planning, which firstly applies CoT in video multi-modal reasoning.\n\nAlthough some works have started to utilize chain-of-thought reasoning and solve multi-modal reasoning tasks, previous works only focus on how to construct high-quality fine-tuned data, and there are still several challenges remaining:\n\n\u2022 How to unify visual and language features to elicit better multi-modal understanding.\n\n\u2022 How to use VLMs for chain-of-thought reasoning without fine-tuning.\n\n\u2022 How to adapt image multi-modal reasoning into video multi-modal reasoning.", "filtered_refids": [[], ["b148", "b170", "b29", "b193"], ["b46", "b50", "b153", null, "b75", "b1", "b180"], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3548, "num_references": 11}
{"corpusid_sectionid": "263153015-s29", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Faithfulness", "section": "Extensive research indicates that chain-of-thought reasoning can lead to hallucination phenomena, such as factual mistakes and contextual inconsistencies.Considering that language models fundamentally belong to statistical models, and due to factors such as data noise and knowledge forgetting, hallucination phenomena are unavoidable.Some works focus on mitigating factual mistakes.He et al. (2023a) introduces external knowledge to evaluate reasoning chains and votes to filter out chains that contain factual mistakes but without correcting them Wang et al. (2023b) adopts a similar way, with the difference that it additionally introduces a reflection mechanism to correct low-scoring reasoning.Zhao et al. (2023a) filters out low-confidence reasoning by consistency and guides models to re-reasoning based on relevant external knowledge.While the aforementioned methods work well on knowledge-intensive tasks, they fall short in addressing the challenge of contextual inconsistencies.Zhang et al. (2023d) explores the hallucination snowballing phenomena during the reasoning process.Others aim to address the inconsistency issues.Radhakrishnan et al. (2023) observes that models are more faithful when dealing with simple questions.Thus, it improves faithfulness through question decomposition.Faithful CoT (Lyu et al., 2023) initially generates symbolic reasoning chains and later deterministically executes symbolic functions, mitigating reasoning inconsistencies.Lanham et al. (2023) explores the factors that influence faithfulness, which provides an empirical perspective.It finds faithfulness varies on different tasks and decreases as the model size increases.CoNLI (Lei et al., 2023b) proposes a post-editing strategy to diminish the hallucinations.SynTra (Jones et al., 2023) performs prefix-tuning on a synthetic dataset designed to elicit hallucination easily, and then transfers this capability to real tasks.\n\nDespite numerous efforts aimed at addressing the hallucination issues in large language models, these works have only mitigated the problem to some extent.There is still a long way to fully enhance the faithfulness of large language models.We summarize the future directions as follows:\n\n\u2022 Improving the ability to recognize hallucination phenomena in the reasoning processes.\n\n\u2022 Improving the accuracy of external knowledge retrieval and utilization to reduce factual mistakes.\n\n\u2022 Improving the ability to recognize and correct contextual inconsistencies and logical mistakes, which is more challenging.\n\n\u2022 How to fundamentally eliminate hallucination phenomena from alternative approaches, e.g.specific pre-training.", "filtered_refids": [["b57", "b39", "b146", "b188", "b71", "b194", "b117", "b93", "b68"], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2646, "num_references": 9}
{"corpusid_sectionid": "263153015-s30", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "CoT Theory", "section": "Despite the impressive capability of chain-ofthought reasoning, the ability to generate chainof-thought following instructions still lacks a comprehensive explanation.Some work addresses from an empirical perspective and can serve as a practical guide.Madaan and Yazdanbakhsh (2022) decomposes prompts into three components: symbols, patterns, and text, exploring the impact of CoT through counterfactual prompting.Wang et al. (2023a) analyzes the impact of demonstration selection.They find that the correctness of reasoning chains has a negligible effect, while the relevance to the question and correct reasoning order matters.Tang et al. (2023) explores the role of semantics.They find that chain-of-thought reasoning relies heavily on semantic knowledge introduced during pre-training and performs poorly in symbolic reasoning.\n\nOthers work analyze theoretically, exploring the underlying principles and internal mechanisms.Li et al. (2023e) deconstructs chain-of-thought reasoning as a multi-step combinatorial function.They demonstrate that chain-of-thought reduces the complexity of in-context learning to tackle complex questions.Feng et al. (2023) theoretically proves that a fixed-size Transformer is sufficient for computational tasks and dynamic planning with chainof-thought.Merrill and Sabharwal (2023) observes that chain-of-thought can boost reasoning ability, with the extent of improvement increasing as the number of intermediate reasoning steps grows.Wu et al. (2023) leverages gradient-based feature attribution methods to explore the impact of chainof-thought on outputs.The results indicate that chain-of-thought exhibits robustness to perturbations and variations in the question.In addition, there are some claims suggesting that the chainof-thought ability stems from code data during the pre-training phase (Madaan et al., 2022;Zhang et al., 2023c), but there is currently no systematic work to substantiate this opinion.\n\nCurrent research on chain-of-thought theory is still in its preliminary exploration stage.We summarize future research directions as follows:\n\n\u2022 Explore the sources of chain-of-thought ability to achieve targeted improvements in CoT reasoning.\n\n\u2022 Theoretically analyzing the advantages of chain-of-thought over in-context learning and exploring the boundaries of its capabilities.", "filtered_refids": [["b139", "b94", "b144"], ["b97", "b161", "b28", "b82", "b187", null], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2331, "num_references": 9}
{"corpusid_sectionid": "263153015-s32", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Comparison of XoT Construction", "section": "There are three main ways of constructing an Xof-thought for existing methods: (1) Manual labeling reasoning chains.(2) Automatic generating reasoning chains by models.(3) Semi-automatic generation with automatic expansion on a small number of manually labeled reasoning chains.We observe that the manual construction methods (Wei et al., 2022b;Gao et al., 2023) face similar challenges to in-context learning, i.e., demonstration selection, instruction formatting, etc (Dong et al., 2023).This causes numerous difficulties in its application and hinders the transfer ability across different tasks.Automatic construction methods (Zhang et al., 2023f;Chen et al., 2022a;Xu et al., 2023) lack the guidance of high-quality annotations, resulting in performance deficiencies.Benefiting from the signals brought by manual annotations, semi-automatic methods (Shum et al., 2023;Shao et al., 2023) can generate high-quality reasoning chains through self-bootstrapping and similar techniques, effectively addressing the challenges faced by previous approaches.While achieving excellent performance, it allows for easy transfer across different tasks.", "filtered_refids": [["b11", "b192", "b31", "b158", null, "b127", "b23", "b132"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1143, "num_references": 8}
{"corpusid_sectionid": "263153015-s33", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Comparison between", "section": "Verification/Refinement and Planning\n\nNumerous parallels exist between planning methods and verification/refinement-based methods, as both rely on feedback from intermediate processes to adjust and refine behavior.The distinction lies in the fact that planning methods encompass decisionmaking, while verification/refinement-based methods solely address intermediate errors without delving into higher-level cognitive processes.LLM reasoning processes are often hallucinatory, causing factual and logical mistakes.Verify and edit based methods (Ling et al., 2023;Zhao et al., 2023a;Madaan et al., 2023;Shinn et al., 2023) verify the correctness of the reasoning process and refine reasoning step that may cause hallucinatory.Through verification and refinement, cascading errors and hallucinatory phenomena in the reasoning process are significantly reduced.\n\nThe planning methods (Long, 2023;Yao et al., 2023b,c;Liu et al., 2023a;Shinn et al., 2023) introduce a decision-making process in the reasoning.They evaluate the intermediate reasoning steps to get feedback, and based on the feedback, they engage in exploration and backtracking to achieve superior solutions at a global level.Their specialization lies in handling complex problems, enabling them to achieve remarkable performance, especially when confronted with intricate multi-hop reasoning and planning tasks.", "filtered_refids": [[], ["b194", null, "b130", "b84"], ["b130", "b88", null, "b85"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1373, "num_references": 8}
{"corpusid_sectionid": "263153015-s34", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Compensate for Innate Weaknesses", "section": "LLMs have many inherent limitations when it comes to reasoning, such as the inability to access external information, arithmetic errors, and inconsistent reasoning.These issues can be cleverly circumvented by entrusting specific responsibilities to dedicated modules or models.\n\nIn response to the models' limitation in accessing external information, (Li et al., 2023d;Wang et al., 2023b;Lu et al., 2023a;Schick et al., 2023;Karpas et al., 2022;Yoran et al., 2023) utilizes external knowledge resources like knowledge base, search engines, and open-domain questionanswering systems.Some work introduces a calculator to address arithmetic errors (Schick et al., 2023;Karpas et al., 2022;Parisi et al., 2022b).Code execution is deterministic, and certain work enhances the consistency of the reasoning process by introducing code executor (Gao et al., 2023;Chen et al., 2022a;Bi et al., 2023;Imani et al., 2023).We believe that employing LLMs as an agent for central planning and reasoning, delegating specific sub-tasks to dedicated sub-models, is a potential avenue for applying large models in complex scenarios in the future (Wang et al., 2023e;Xi et al., 2023).", "filtered_refids": [[], ["b11", "b52", "b6", "b175", "b58", "b146", "b89", "b31", "b80", "b149", "b172", "b110", "b125"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 1165, "num_references": 13}
{"corpusid_sectionid": "263153015-s35", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Other Work", "section": "In this chapter, we will list other works that represent early attempts at chain-of-thought reasoning or are designed for specific domains.Katz et al. (2022); Zhang et al. (2022) provide benchmarks and resources.Some work has empirically demonstrated the effectiveness of chainof-thought prompting (Lampinen et al., 2022;Ye and Durrett, 2022;Arora et al., 2023) and Shi et al. (2023) explores multi-lingual CoT reasoning.Other work focuses on specific domains, such as machine translation (He et al., 2023b), sentiment analysis (Fei et al., 2023), sentence embeddings (Zhang et al., 2023a), summarization (Wang et al., 2023k), arithmetic (Lee and Kim, 2023), and tabular reasoning (Chen, 2023;Jin and Lu, 2023), etc. Besides, some research utilizes specific pretraining to enhance certain capabilities, such as mathematical reasoning (Lewkowycz et al., 2022;Zhao et al., 2022).", "filtered_refids": [["b10", "b56", "b40", "b156", "b184", "b27", "b59", "b66", null, "b171", "b69", "b129", "b73", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 877, "num_references": 14}
{"corpusid_sectionid": "1040974-s15", "title": "A SURVEY OF MACHINE TRANSLATION: ITS HISTORY, CURRENT STATUS, AND FUTURE PROSPECTS", "date": 1985, "section_title": "Jonathan Sloeum", "section": "A Survey of Machine Translation SYSTRAN SYSTRAN was one of the first MT systems to be marketed; the first installation replaced the IBM Mark II Russian-English system at the USAF FTD in 1970, and is still operational. NASA selected SYSTRAN in 1974 to translate materials relating to the Apollo-Soyuz collaboration, and EURATOM replaced GAT with SYSTRAN in 1976. Also by 1976, FTD was augmenting SYSTRAN with word-processing equipment to increase productivity (e.g., to eliminate the use of punched cards). The system has continued to evolve, for example by the shift toward a more modular design and by the allowance of topical glossaries (essentially, dictionaries specific to the subject area of the text). The system has been argued to be ad hoc -particularly in the assignment of semantic features (Pigott 1979). The USAF FTD dictionaries number over a million entries; Bostad (1982) reports that dictionary updating must be severely constrained, lest a change to one entry disrupt the activities of many others. (A study by Wilks (1978) reported an improvement/degradation ratio [after dictionary updates] of 7:3, but Bostad implies a much more stable situation after the introduction of stringent quality-control measures.)\n\nIn 1976 the Commission of the European Communities purchased an English-French version of SYSTRAN for evaluation and potential use. Unlike the FTD, NASA, and EURATOM installations, where the goal was information acquisition, the intended use by CEC was for information disseminationmeaning that the output was to be carefully edited before human consumption. Van Slype (1982) reports that \"the English-French standard vocabulary delivered by Prof. Toma to the Commission was found to be almost entirely useless for the Commission environment.\" Early evaluations were negative (e.g., Van Slype 1979), but the existing and projected overload on CEC human translators was such that investigation continued in the hope that dictionary additions would improve the system to the point of usability. Additional versions of SYSTRAN were purchased (French-English in 1978, andEnglish-Italian in 1979). The dream of acceptable quality for post-editing purposes was eventually realized: Pigott (1982) reports that \" . . . the enthusiasm demonstrated by [a few translators] seems to mark something of a turning point in [machine translation].\" Currently, about 20 CEC translators in Luxembourg are using SYSTRAN on a Siemens 7740 computer for routine translation; one factor accounting for success is that the English and French dictionaries now consist of well over 100,000 entries in the very few technical areas for which SYSTRAN is being employed.\n\nAlso in 1976, General Motors of Canada acquired SYSTRAN for translation of various manuals (for vehicle service, diesel locomotives, and highway transit coaches) from English into French on an IBM mainframe. GM's English-French dictionary had been expanded to over 130,000 terms by 1981 (Sereda 1982). Subseque~ly~ GM purchased an English-Spanish version of SYSTRAN, and began to build the necessary [very large] dictionary. Sereda (1982) reports a speed-up of 3-4 times in the productivity of his human translators (from about 1000 words per day); he also reveals that developing SYSTRAN dictionary entries costs the company approximately $4 per term (word-or idiom-pair).\n\nWhile other SYSTRAN users have applied the system to unrestricted texts (in selected subject areas), Xerox has developed a restricted input language (Multinational Customized English) after consultation with LATSEC. That is, Xerox requires its English technical writers to adhere to a specialized vocabulary and a strict manual of style. SYSTRAN is then employed to translate the resulting documents into French, Italian, Spanish, German, and Portuguese. Ruffino (1982) reports \"a five-to-one gain in translation time for most texts\" with the range of gains being 2-10 times. This approach is not necessarily feasible for all organizations, but Xerox is willing to employ it and claims it also enhances source-text clarity.\n\nCurrently, SYSTRAN is being used in the CEC for the routine translation, followed by human post-editing, of around 1,000 pages of text per month in the couples English-French, French-English, and English-Italian (Wheeler 1983). Given this relative success in the CEC environment, the Commission has recently ordered an English-German version as well as a French-German version. Judging by past experience, it will be quite some time before these are ready for production use, but when ready they will probably save the CEC translation bureau valuable time, if not real money as well.", "filtered_refids": [["b52", "b34", "b4"], ["b48", "b35", null], ["b39"], ["b36"], ["b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 4655, "num_references": 9}
{"corpusid_sectionid": "1040974-s16", "title": "A SURVEY OF MACHINE TRANSLATION: ITS HISTORY, CURRENT STATUS, AND FUTURE PROSPECTS", "date": 1985, "section_title": "LOGOS", "section": "Development of the LOGOS system was begun in 1964. The first installation, in 1971, was used by the U.S. Air Force to translate English maintenance manuals for military equipment into Vietnamese. Due to the termination of U.S. involvement in that war, its use was ended after two years. (A report by Sinaiko and Klare (1973) disparaged LOGOS's cost-effectiveness, but this claim was argued to be seriously flawed and was formally protested (Scott, personal communication).) The linguistic foundations of LOGOS are not well advertised, presumably for reasons involving trade secrecy. The system developer states that \"our linguistic approach ... has evolved in ways analogous to case grammar/valency theory . . . mapping natural language into a semantosyntactic abstraction language organized as a tree\" (Scott, personal communication).\n\nLOGOS continued to attract customers. In 1978, Siemens AG began funding the development of a LOGOS German-English system for telecommunications manuals. After three years LOGOS delivered a \"production\" system, but it was not found suitable for use (due in part to poor quality of the translations, and in part to the economic situation within Siemens which had resulted in ff much-reduced demand for translation, hence no imme-diate need for an MT system). Eventually LOGOS forged an agreement with the Wang computer company that allowed the :implementation of the German-English system (formerly restricted to large IBM mainframes) on Wang office computers.\n\nThis system reached the commercial market, and has been purchased by several multi-national organizations (e.g., Nixdorf, Triumph-Adler, Hewlett-Packard); development of other language pairs (e.g., English-French, English-German) is underway (Scott, personal communication). METEO TAUM-METEO is the world's only example of a truly fully-automatic MT system. Developed as a spin-off of the TAUM technology, as discussed earlier, it was fully integrated into the Canadian Meteorological Center's (CMC's) nation-wide weather communications network by 1977. METEO scans the network traffic for English weather reports, translates them \"directly\" into French, and sends the translations back out over the communications network automatically. Rather than relying on post-editors to discover and correct errors, METEO detects its own errors and passes the offending input to human editors; output deemed \"correct\" by METEO is dispatched without human intervention, or even overview.\n\nTAUM-METEO was probably also the first MT system where translators were involved in all phases of the design/development/refinement; indeed, a CMC translator instigated the entire project. Since the restrictions on input to METEO were already in place before the project started (i.e., METEO imposed no new restrictions on weather forecasters), METEO cannot quite be classed with the Xerox SYSTRAN system, which relies on restrictions geared to the characteristics of SYSTRAN. But METEO is not extensible -though similar systems could be built for equally restricted textual domains, if they exist.\n\nOne of the more remarkable side effects of the METEO installation is that the translator turnover rate within the CMC went from 6 months, prior to METEO, to several years, once the CMC translators began to trust METEO's operational decisions and not review its output (Brian Harris, personal communication). METEO's input constitutes over 24,000 words per day, or 8.5 million words per year. Of this, it now correctly translates 90-95%, shuttling the other (\"more interesting\") 5-10% to the human CMC translators.\n\nAlmost all of these \"analysis failures\" are attributable to communications noise (the CMC network garbles some traffic), misspellings (METEO does not attempt corrections), or words missing from the dictionary, though some failures are due to the inability of the system to handle certain linguistic constructions. METEO's computational requirements total about 15 CPU minutes per day on a CDC 7600 (Thouin 1982). By 1981, it appeared that the builtin limitations of METEO's theoretical basis had been reached, and further improvement was not likely to be cost-effective.", "filtered_refids": [[null, "b41"], [], [null], [], [], ["b45"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 4160, "num_references": 4}
{"corpusid_sectionid": "1040974-s18", "title": "A SURVEY OF MACHINE TRANSLATION: ITS HISTORY, CURRENT STATUS, AND FUTURE PROSPECTS", "date": 1985, "section_title": "SPANAM", "section": "Following a promising feasibility study, the Pan American Health Organization in Washington, D.C. decided in 1975 to undertake work on a machine translation system, utilizing some of the same techniques developed for GAT. Consultants were hired from nearby Georgetown University, the home of GAT. The official PAHO languages are English, French, Portuguese, and Spanish; Spanish-English was chosen as the initial language pair, due to the belief that \"This combination requires fewer parsing strategies in order to produce manageable output [and other reasons relating to expending effort on software rather than linguistic rules]\" (Vasconcellos 1983). Actual work started in 1976, and the first prototype was running in 1979, using punched card input on an IBM mainframe. With the subsequent integration of a word-processing system, production use could be seriously considered.\n\nAfter further upgrading, an in-house translation service based on SPANAM was created in 1980. Later that year, in its first major test, SPANAM reduced manpower requirements for a test translation effort by 45%, resulting in a monetary savings of 61% (Vasconcellos 1983). (Because these SPANAM translation and on-line post-editing figures appear to be contrasted against the purely manual, hardcopy translation tradition at PAHO, the gains from using SPANAM per se may be hopelessly confounded with the gains of working on-line; thus, it is difficult or impossible to say how much increase in productivity is accounted for by SPANAM alone.) Since 1980, SPANAM has been used to translate well over a million words of text, averaging about 4,000 words per day per post-editor. The post-editors have amassed \"a bag of tricks\" for speeding the revision work, and special string functions have also been built into the word processor for handling SPANAM's English output.\n\nConcerning the early status of SPANAM, sketchy details implied that the linguistic technology underlying it was essentially that of GAT; the grammar rules seemed to be built into the programs, in the GAT tradition. The software technology was updated in that the programs are modular. The system is not sophisticated: it adopts the direct translation strategy, and settles for local analysis of phrases and some clauses via a sequence of primitive, independent processing stages (e.g., homograph resolution) -again, in the Georgetown tradition. SPANAM is currently used by three PAHO translators in their routine work.\n\nA follow-on project to develop ENGSPAN (for English-Spanish), underway since 1981, has also delivered a production system -this one characterized by a more advanced design (e.g., an ATN parser), some features of which may find their way into SPANAM. (SPANAM is currently \"undergoing a major overhaul\" (Vasconcellos, personal communication).)\n\nFour PAHO translators already employ ENGSPAN in their daily work. Based on the successes of these two systems, development of ENGPORT (with Portuguese as the Target Language) has begun. In the future, \"all translators [in the Language Services bureau of PAHO will be] expected to use MT at least part of the time, and the corresponding duties are included in the post descriptions\". (Vasconcellos, personal communication).", "filtered_refids": [[null], [], [], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3233, "num_references": 3}
{"corpusid_sectionid": "1040974-s19", "title": "A SURVEY OF MACHINE TRANSLATION: ITS HISTORY, CURRENT STATUS, AND FUTURE PROSPECTS", "date": 1985, "section_title": "CULT: CHINESE UNIVERSITY LANGUAGE TRANSLATOR", "section": "CULT is possibly the most successful of the Machine-aided Translation systems, Development began at the Chinese University of Hong Kong around 1968. CULT translates Chinese mathematics and physics journals (published in Beijing) into English through a highly-interactive process [or, at least, with a lot of human intervention]. The goal was to eliminate post-editing of the results by allowing a large amount of pre-editing of the input, and a certain [unknown] degree of human intervention during translation. Although published details (Loh 1976(Loh , 1978(Loh , 1979 are not unambiguous, it is clear that humans intervene by marking sentence and phrase boundaries in the input, and by indicating word senses where necessary, among other things. (What is not clear is whether this is strictly a pre-editing task, or an interactive task.) CULT runs on the ICL 1904A computer.\n\nBeginning in 1975, the CULT system was applied to the task of translating the Acta Mathematica Sinica into English; in 1976, this was joined by the Acta Physica Sini6a. Originally the Chinese character transcription problem was solved by use of the standard telegraph codes invented a century ago, and the input data was punched on cards. But in 1978 the system was updated by the addition of word-processing equipment for on-line data entry and pre-or post-editing.\n\nIt is not cleat' how general the techniques behind CULT are -whether, for example, it could be applied to the translation of other texts -nor how cost-effective it is in operation. Other factors may justify its continued use. It is also unclear whether R&D is continuing, or whether CULT, like METEO, is unsuited to design modification beyond a certain point already reached. In the absence of answers to these questions, and perhaps despite them, CULT does appear to be an MAT success story: the amount of post-editing said to be required is trivial -limited to the re-introduction of certain untranslatable formulas, figures, etc., into the translated output. At some point, other translator intervention is required, but it seems to be limited to the manual inflection of verbs and nouns for tense and number, and perhaps the introduction of a few function words such as determiners.\n\nALPS: AUTOMATED LANGUAGE PROCESSING SYSTEMS ALPS was incorporated by a group of five Brigham Young University ITS developers in 1980; this group seems to have been composed of linguists interested in producing machine aids for human translators (dictionary look-up and substitution, etc.) and later grew to include virtually all of the major figures from the ITS staff (Melby and Tenney, personal communication). Thus the new ALPS system is interactive in all respects, and does not seriously pretend to perform translation; rather, ALPS provides the translator with a set of software tools to automate many of the tasks encountered in everyday translation experience. ALPS adopted the language pairs that the BYU ITS system had supported: English into French, German, Portuguese, and Spanish. Since then, other languages (e.g., Arabic) have been announced, but their commercial status is unclear. In addition to selling MAT systems, ALPS now includes its own translation service bureau. The new ALPS system is intended to work on any of three \"levels\" -providing capabilities from multilingual word processing and dictionary lookup, through wordfor-word (actually, term-for-term) translation, to highlyautomated (though human-assisted) sentence-level translation; ~the latter mode of operation, judging by ALPS demonstrations and the reports of users, is seldom if ever employed. The central tool provided by ALPS is thus a menu-driven word-processing system coupled to the on-line dictionary. One of the first ALPS customers seems to have been Agnew TechTran -a commercial translation bureau which acquired the ALPS system for in-house use. Other customers include Xerox, Compu-terVision, Control Data (in France), IBM (in Italy) and Hewlett-Packard (in Mexico). Recently, another shakeup at Weidner Communication Corporation (the Provo R&D group was disbanded) has allowed ALPS to hire a large group of former Weidner workers: ALPS might itself be intending to enter the fully-automatic MT arena.", "filtered_refids": [[null, "b19", "b17"], [], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 4234, "num_references": 4}
{"corpusid_sectionid": "1040974-s21", "title": "A SURVEY OF MACHINE TRANSLATION: ITS HISTORY, CURRENT STATUS, AND FUTURE PROSPECTS", "date": 1985, "section_title": "MT R & D IN JAPAN", "section": "In 1982 Japan electrified the technological world by widely publicizing its new Fifth Generation project and establishing the Institute for New Generation Computer Technology (ICOT) as its base. Its goal is to leapfrog Western technology and place Japan at the forefront of the digital electronics world in the 1990's. MITI (Japan's Ministry of International Trade and Industry) is the motivating force behind this project, and intends that the goal be achieved through the development and application of highly innovative techniques in both computer architecture and Artificial Intelligence.\n\nOf the application areas considered as an applications candidate by the ICOT scientists and engineers, Machine Translation played a prominent role (Moto-oka 1982). Among the western Artificial Intelligentsia, the inclusion of MT seems out of place: AI researchers have been trying (successfully) to ignore all MT work in the two decades since the ALPAC debacle, and almost universally believe that success is impossible in the foreseeable future -in ignorance of the successful, cost-effective applications already in place. To the Japanese leadership, however, the inclusion of MT is no accident. Foreign language training aside, translation into Japanese is still one of the primary means by which Japanese researchers acquire information about what their Western competitors are doing, and how they are doing it. Translation out of Japanese is necessary before Japan can export products to its foreign markets, because the customers demand that the manuals and other documentation not be written only in Japanese, and in general translation is seen as a way to \"diffuse the Japanese scientific and technological information to outer world\" (Nagao, personal communication).\n\nThe Japanese correctly view translation as necessary to their technological survival, but have found it extremely difficult -and expensive -to accomplish by human means: the translation budgets of Japanese companies, when totalled, are estimated to exceed 1 trinion yen, and most of this involves the export trade (Philippi 1985). Accordingly, the Japanese government and industry have sponsored MT research for several decades. There has been no rift between AI and MT researchers in Japan, as there has been in the West -especially in the U.S. Nomura (1982) numbers the MT R&D groups in Japan at more than 18. (By contrast, there might be a dozen significant MT groups in all of the U.S. and Europe, including commercial vendors.) Several of the Japanese projects are quite large. (By contrast, only one MT project in the western world (EUROTRA) even appears as large, but most of the 80 individuals involved work on EUROTRA only a fraction of their time.) Most of the Japanese projects are engaged in research as much as development. (Most Western projects are engaged in pure development.) Japanese progress in MT has not come fast: until a few years ago, their hardware technology was inferior; so was their software competence, but this situation has been changing rapidly. Another obstacle has been the great differences between Japanese and Western languages -especially English, which is of greatest interest to them -and the relative paucity of knowledge about these differences. The Japanese are working to eliminate this ignorance: progress has been made, and production-quality systems already exist for some applications. None of the Japanese MT systems are direct, and all engage in global analysis; most are based on a transfer approach, but a few groups are pursuing the interlingua approach.\n\nMT research has been pursued at Kyoto University since 1964. There were once two MT projects at Kyoto (one for long-term research, one for near-term application). The former project, recently abandoned, was working on an English-Japanese translation system based on formal semantics (Cresswell's simplified version of Montague Grammar (Nishida et al. , 1983). The latter has developed a practical system for translating English titles of scientific and technical papers into Japanese (Nagao 1980, and is working on other applications of English-Japanese (Tsujii 1982) as well as Japanese-English (Nagao 1981). This effort, funded by the Agency of Science and Technology and headed by Prof. Nagao, \"consists of more than 20 people [at Kyoto], with three other organizations involved [comprising another 20 workers]\" (Nagao personal communication). The goal of this four-year, $2.7 million (U.S.) project is to create a practical system for translating technical and scientific documents from Japanese into English and vice versa (Philippi 1985). Kyushu University has been the home of MT research since 1955, with projects by Tamachi and Shudo (1974). The University of Osaka Prefecture and Fukuoka University also host MT projects.\n\nHowever, most Japanese MT research (like other research) is performed in the industrial laboratories. Fujitsu (Sawai et al. 1982), Hitachi, Toshiba (Amano 1982), and NEC (Muraki & Ichiyama 1982), among others, support large projects generally concentrating on the translation of computer manuals. Nippon Telegraph and Telephone is working on a system to translate scientific and technical articles from Japanese into English and vice versa (Nomura et al. 1982), and is looking into the future as far as simultaneous machine translation of telephone conversations (Nomura, personal communication). Recently a joint venture by Hitachi and Quick has resulted in a English-Japanese system which will be used to offer Japanese readers news from Europe and the U.S. on the economy, stock market, and commodities; eventually, this service will be offered via Quick's on-line market information service (AAT 1984). In addition, Fujitsu has announced its bi-directional Atlas Japanese-English system for translating technical texts; this system is now available for lease (AAT 1984). NEC and IBM Japan have also recently announced development of systems intended for near-term commercial introduction (Philippi 1985).\n\nJapanese industrialists are not confining their attention to work at home. Several AI groups in the U.S. (e.g., SRI International) have been approached by Japanese companies desiring to fund MT R&D projects, and the Linguistics Research Center of the University of Texas is currently engaged in MT-related research funded by Hitachi.\n\nMore than that, some U.S. MT vendors (SYSTRAN and Weidner, at least) have recently sold partial interests to Japanese investors, and delivered production MT systems. Various Japanese corporations (e.g., NTT and Hitachi) and trade groups (e.g., JEIDA (Japan Electronic Industry Development Association)) have sent teams to visit MT projects around the world and assess the state of the art. University researchers have been given sabbaticals to work at Western MT centers (Prof. Shudo at Texas, Prof. Tsujii at Grenoble). Other representatives have indicated Japan's desire to establish close working communications with the CEC's EUROTRA project (King and Nagao, personal communication). Japan evidences a long-term, growing commitment to acquire and develop MT technology. The Japanese leadership is convinced that success in MT is vital to their future. METAL One of the major MT R&D groups around the world, the METAL project at the Linguistics Research Center of the University of Texas, has recently delivered a commercial-grade system. The METAL German-English system passed tests in a production-style setting in late 1982, mid-1983, and twice in 1984, and the system was then installed at the sponsor's site in Germany for further testing and final development of a translator interface. Renamed LITRAS, it was introduced for sale at the Hanover Fair in Germany in April 1985. The METAL dictionaries are now being expanded for maximum possible coverage of selected technical areas, and work on other language pairs has begun in earnest.\n\nOne of the particular strengths of the METAL system is its accommodation of a variety of linguistic theories/strategies. The German analysis component is based on a context-free phrase-structure grammar, augmented by procedures with facilities for, among other things, arbitrary transformations. The English analysis component, on tile other hand, employs a modified GPSG approach and makes no use of transformations. Analysis is completely separated from transfer, and the system is multilingual in that a given constituent structure analysis can be used for transfer and synthesis into multiple target languages. (Translation from German into Chinese and Spanish, as well as from English into German, has transpired on an experimental basis.)\n\nThe transfer component of METAL includes two transformation packages, one used by transfer grammar rules and the other by transfer dictionary entries; these cooperate during transfer, which is effected during a top-down exploration of the (highest-scoring) tree produced in the analysis phase. The strategy for the top-down pass is controlled by the linguist who writes the transfer rules. These are most often paired 1-1 with the grammar rules used to perform the original analysis, so that there is no need to search through a general transfer grammar to find applicable rules (potentially allowing application of the wrong ones); however, the option of employing a more general transfer grammar is available, and is in fact used for the translation of clauses. As implied above, structural and lexical transfer are performed in the same pass, so that each may influence the operation of the other; in particular, transfer dictionary entries may specify the syntactic and/or semantic contexts in which they are valid. If no analysis is achieved for a given input, the longest phrases which together span that input are selected for independent transfer and synthesis, so that every input (a sentence, or perhaps a phrase) results in some translation.\n\nIn addition to producing a translation system per se, the Texas group has developed software packages for text processing (so as to format the output translations like the original input documents), data base management (of dictionary entries and grammar rules), rule validation (to eliminate most errors in dictionary entries and grammar rules), dictionary construction (to enhance human efficiency in coding lexical entries), etc. Aside from the word-processing front-end (developed by the project sponsor), the METAL group has developed a complete system, rather than a basic machine translation engine that leaves much drudgery for its human developers/users. Lehmann et al. (1981), Bennett (1982), and Slocum (1983Slocum ( , 1984Slocum ( , 1985 present more details about the METAL system.", "filtered_refids": [[], [null, "b23"], ["b33", "b31"], ["b46", "b40", "b25", "b30", null, "b24", "b33"], ["b31", null, "b24", "b1", "b33", "b37"], [], [null], [], [], ["b42", "b16", null, "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 57, "num_chars": 10700, "num_references": 22}
{"corpusid_sectionid": "1040974-s24", "title": "A SURVEY OF MACHINE TRANSLATION: ITS HISTORY, CURRENT STATUS, AND FUTURE PROSPECTS", "date": 1985, "section_title": "EUROTRA", "section": "EUROTRA is the largest MT project in the Western world. It is the first serious attempt to produce a true multilingual system, in this case intended for all seven European Economic Community languages. The justification for the project is simple, inescapable economics: over a third of the entire administrative budget of the EEC for 1982 was needed to pay the translation division (average individual cost: $43,000 per year), which still could not keep up with the demands placed on it; technical translation costs the EEC $0.20 per word for each of six translations (from the seventh original language), and doubles the cost of the technology documented; with the addition of Spain and Portugal, the translation staff would have to double for the current demand level (unless highly productive machine aids were already in place) (Perusse 1983).\n\nThe high cost of writing SYSTRAN dictionary entries is presently justifiable for reasons of speed in translation, but this situation is not viable in the long term. The EEC must have superior quality MT at lower cost for dictionary work. Human translation alone will never suffice.\n\nEUROTRA is a true multi-national development project. There is no central laboratory where the work will take place, but instead designated University representatives of each member country will produce the analysis and synthesis modules for their native language; only the transfer modules will be built by a \"central\" groupand the transfer modules are designed to be as small as possible, consisting of little more than lexical substitution (King 1982).\n\nSoftware development will be almost entirely separated from the linguistic rule development; indeed, the production software, though designed by the EUROTRA members, will be written by whichever commercial software house wins the contract in bidding competition. Several co-ordinating committees are working with the various language and emphasis groups to ensure co-operation.\n\nThe theoretical linguistic basis of EUROTRA is not novel. The basic structures for representing \"meaning\" are dependency trees, marked with feature-value pairs partly at the discretion of the language groups writing the grammars (anything a group wants, it can add), and partly controlled by mutual agreement among the language groups (a certain set of feature-value combinations has been agreed to constitute minimum information; all are constrained to produce this set when analyzing sentences in their language, and all may expect it to be present when synthesizing sentences in their language) (King 1981(King , 1982. This is not to say that no new linguistic knowledge is being gained for, aside from the test of theory that EUROTRA is about to perform, there is the very substantial matter of the background contrastive linguistic investigation that has been going on since about 1978.\n\nComputational Linguistics, Volume 11, Number 1, January- March 1985 In one sense, the software basis of EUROTRA will not be novel either. The basic rule interpreter will be \"a general re-write system with a control language over grammars/processes\" (King, personal communication). As with ARIANE-78, the linguistic rules can be bundled into packets of subgrammars, and the linguists will be provided with a means of controlling which packets of rules are applied, and when; the individual rules will be non-destructive re-write rules, so that the application of any given rule may create new structure, but will never erase any old information.\n\nIn another sense, however, the software basis of EUROTRA is quite remarkably different from other systems that have preceded it. The analysis, transfer, and synthesis strategies will not be incorporated into algorithms that the programmers implement; rather, they will be formulated by linguists and represented in a special control language (not the rule-writing language, which is algorithm-independent). This formulation of the dynamic control strategies will be compiled into a program that will then interpret the \"static\" rules describing the linguistic facts. This is a bold step. There are, of course, pitfalls to any such action. Aside from the usual risk of unforeseen problems, there are two rather obvious unresolved issues. First, it remains to be seen whether linguists, trained mostly in the static, \"descriptive\" framework of linguistics (modern or otherwise), can accommodate themselves to the expression of dynamic algorithms -a mode of thinking that programmers (including almost all computational linguists) are far more adept at. Second, it also remains to be seen whether the system can be designed sufficiently flexibly to adjust to the wide range of experimental strategies that is sure to come when the staff is given such a large degree of freedom (remembering that the software implementation is seen as an essentially one-shot process to be performed on'contract basis), while at the same time retaining sufficient speed to ensure that the computing requirements are affordable. Affordability is not merely an issue belonging to the eventual production system! On the contrary, it is critically important that a development group be able to conduct experiments that produce results in a reasonable amount of time. After too long a delay, the difference becomes one of category rather than degree, and progress is substantially -perhaps fatally -impeded.\n\nThe EUROTRA charter requires delivery of a small representative prototype system by late 1987, and a prototype covering one technical area by late 1988. The system must translate among the official languages of all member countries that sign a \"contract of association\"; thus, not all seven EEC languages will necessarily be represented, but by law at least four languages must be represented if the project is to continue. It appears that the requisite number of member states have committed to join. It will be interesting to see whether this, the most ambitious of all MT projects, succeeds; either way, the consequences promise to be noteworthy.", "filtered_refids": [[null], [], ["b14"], [], ["b14", "b13"], [null], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 31, "num_chars": 6039, "num_references": 5}
{"corpusid_sectionid": "245124108-s7", "title": "Surfer100: Generating Surveys From Web Resources, Wikipedia-style", "date": "2021-12-13", "section_title": "Content Selection", "section": "We first tested the quality of the content selection methods for generic retrieval of content relevant to a topic on our data. We choose the Semantic Search, WikiCite, and RoBERTa-Rank methods from Table 1 for analysis. For Semantic Search, we experiment with three types of sentence embeddings, the original sentence-transformer BERT embeddings (SS-BERT), embeddings fine-tuned with SciBERT (SS-SciBERT), and a version fine-tuned to differentiate whether two paragraphs belong to the same Wikipedia section (SS-Wiki). Surprisingly, we found such content was often returned during retrieval despite the poor grammaticality and relevance. We hypothesize that the tendency to return short sentences, often with odd punctuation may relate to the extension of these methods to paragraph levels while inherently being developed for sentencelevel tasks. We then remove sentences shorter than 6 tokenized words, as well as apply heuristics for removing sentences based on the number of parentheses, brackets, and other tokens such as equal signs. We required that each paragraph returned consist of at least two sentences and require that the topic word (or one word within the topic, for multi-word topics) appear in the paragraph. About 85 paragraphs per topic remain after this filtering. The comparison of results before and after preprocessing and filtering is found in Table 3. Notably, the WikiCite method performs much better than semantic search and close to RoBERTa. We believe this is because the method is trained for content selection based on a topic and not simply trained for returning content with high recall. A potential problem with current methods in this two-step approach is that content selection is trained and evaluated with recall in mind, to capture as large a range of the topic, which produces models without the precision necessary in a real-world application. This aligns with previous work in extractive summarization suggesting that optimizing for recall gives suboptimal results (Zopf et al., 2018). Section-Specific Content Selection: We investigated the ability of our content selection models to retrieve content specific for each chosen section, for example, querying \"History of BERT\"rather than \"BERT.\"We 2 https://github.com/IreneZihuiLi/Surfer100  observed large overlaps between the returned results, between 5 and 9 paragraph overlap between the top 10 results for each section. Among all methods, Wikicite has the least overlap. As an alternative method to select distinct content for each section, we investigate clustering methods, using out-of-the-box Agglomerative (M\u00fcllner, 2011) clustering provided by scikit-learn 3 . We cluster the embeddings obtained before the final output layer from the WikiCite and RoBERTa methods, and the Search-Wiki embeddings. We annotated the coherence of each cluster. Clusters obtained using embeddings from RoBERTa, Search-Wiki and Wi-kiCite had a corresponding average coherence of 3.07, 3.40, and 3.52 on a 1-5 scale, signaling slightly aboveaverage coherence for each clustering. Again, the poor performance of RoBERTa in clustering may be due to the more general topic training method. As suggested by Deutsch and Roth (2019), the WikiCite method may dilute topic information in the final layer despite topic attention in previous layers and thus benefit from using embeddings before the final layer as clustering.", "filtered_refids": [["b12", null, "b2", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3395, "num_references": 4}
{"corpusid_sectionid": "251307817-s5", "title": "Rhetorical Structure Approach for Online Deception Detection: A Survey", "date": 2022, "section_title": "Online Deception Detection", "section": "Over the past year, on the sharp growth of the web and social media, cyber-crimes such as identity blows, thief, fraud, and misinformation have become increasingly common. Theses deceptive activities often are characterized by the ease of deception and concealment of one's real identity (P\u00e9rez-Rosas et al., 2017). The research area responsible for investigating and providing methods to detect deceptive activities is known as deception detection. According to , automated deception detection, as a field within NLP and Information Science (IS), is responsible for the development of methods to distinguish truth from deception in textual data, identifying linguistic predictors of deception with text processing and machine learning techniques. Deception detection in textual information has became a relevant study area within NLP, mainly due to the sharing of fake news on the web and social media around the world. Online deceptive activities are addressed by literature on different tasks, which handle a wide range of aspects, such as credibility of users and sources, information veracity, information verification, and linguistic aspects of deceptive language (Atanasova et al., 2019). Unless otherwise stated, these tasks include the discovery of fake news (Lazer et al., 2018); rumor detection in social media (Vosoughi et al., 2018); information verification in question answering systems (Mihaylova et al., 2018); detection of information manipulation agents (Chen et al., 2013;Mubarak et al., 2020); assertive technologies for investigative journalism (Hassan et al., 2015); detection of fake reviews (Ott et al., 2011); detection of deceptive discussions (Larcker and Zakolyukina, 2012). A definition with relevance for the area rotates around the concept of \"deceptive language\". Deceptive language is defined by Communication, Linguistics and Psychology literature as a type of language deliberately used with aim of attempting to mislead others. For instance, falsehoods communicated by people who are mistaken or self-deceived are not lies, nevertheless, literal truths designed to mislead are lies as a deliberate attempt to mislead others. Besides that, most relevant literature on deception refers mainly to levels of deceit and typology of media (e.g., face-to-face, voice, text) (Zhou et al., 2003). DePaulo et al. (2003) claim that deceptive linguistic style may present weak employment of singular and third-person pronouns, negative polarity, and high employment of movement verbs. Nahari et al. (2019) suggest that a basic assumption related to deceptive language is that liars differ from truth-tellers in their verbal behavior, making it possible to classify them by inspecting their verbal accounts. Additionally, a set of linguistic behaviors may predict deception, as tones of words and kinds of preposition, conjunctions, and pronouns (Newman et al., 2003). Taking advantage of the discourse-level analysis, Galasi\u0144ki (2000) presents a pioneer study on fictional deceptive stories. According to the author, discourse analysis of deceptive texts deception is intrinsically tied with \"information manipulation\", which consists of presenting a reality that is misrepresented. The author argues that deception should be classified in three different levels: (i) falsification (i.e., attributing false statements to a debater), (ii) distortion (i.e., manipulating by understating or overstating what a debater states), and (iii) de-contextualization (i.e., taking the words a debater uses out of their context).", "filtered_refids": [["b12", "b44", "b4", "b26", "b18", "b47", "b25", "b32", "b27", "b29", "b30", "b19", "b5", "b8", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3541, "num_references": 15}
{"corpusid_sectionid": "251307817-s7", "title": "Rhetorical Structure Approach for Online Deception Detection: A Survey", "date": 2022, "section_title": "Corpora, Models and Methods", "section": "A very plausible assumption, when one opts for the discourse-aware approach applied to deception detection, would be that there are significant differences between structures of truthful and deceptive stories. Indeed, it has been proposed by various authors. While the research community currently lacks discourse annotated corpora for deception detection tasks, recent works have proposed discourse-tagged corpora for the English, Portuguese and Russian languages. Table 1 provides a summary of the discourse-tagged corpora proposed in literature. As it is shown in Table 1, the discourse-tagged corpora for the fake news and fake reviews detection tasks were proposed for the English, Russian, and Portuguese languages. As being particularly a human time-onerous task and a kind of challenging annotation process, the corpora present a small set of documents. Furthermore, both monolingual and multilingual corpora were proposed.\n\nMoving forward, as it is known from research proposals on fake news and fake reviews, a wide variety of models have been proposed to tackle online deception detection. Most of them rely on linguistic features such as n-grams, language complexity, part-of-speech tags, and syntactic and semantic features. On the other hand, discourse-level structure approach is usually framed as a supervised learning problem, which embodies in a model coherence relations followed by hierarchical nuclearity information to build automatic classifiers. In Table 2, we also summarize discourse-aware models and methods proposed in literature. Notice that models use bag-of-rst, dependency parsing, embeddings and BERT tokenizer as features, and both classical and neural machine learning have been applied. Finally, f1score performance is reported in column \"%\", except for Karimi and Tang (2019), whose authors reported values related to accuracy. \n\nBag-of-rst SVM English 63% Fake News 4.2. Fake News Detection Kuzmin et al. (2020) Fake news prediction is a global problem, and most of approaches have been developed for the English language (Kuzmin et al., 2020). Nevertheless, fake news is spread around the world, and it may be written originally in several languages. In this proposal, the authors trained and compared different models for fake news detection in Russian. They assess whether different language-based features including the vectorization of rhetorical structure obtained from both -a RST parsing and a rst manually annotated corpus -could be helpful for the fake news detection task. This proposal was implemented and evaluated using classical machine learning methods, as Support Vector Machine (SVM) and Logistic Regression (LR) over bag-of-ngrams and bag-of-rst representations. Besides that, sophisticated machine learning techniques, as BERT (Devlin et al., 2019) were also implemented. The authors used three different corpora of fake news in Russian. The first one was proposed by Pisarevskaya (2017) (see Table 1 -manually annotated). The second one was proposed by Zaynutdinova et al. (2019); it is composed of 1,366 fake news and 7,501 true news. Finally, Taiga Corpus 2 was also applied. Furthermore, three distinct representations were used (i) bag-of-ngrams with tfidf preprocessing, (ii) bag-of-rst, which consists of the vectorization of coherence relations and nuclearity, and (iii) pre-trained BERT-based model, more specifically, the RuBERT2 obtained using DeepPavlov 3 (Burtsev et al., 2018) with Transformers (Wolf et al., 2020). The authors reported that classical approaches using bagof-n-grams and bag-of-rst presented high results (90% of F1-score) overcoming the neural network approach, which uses the RuBERT ((88% of F1-score). Moreover, the authors suggest that satire is similar to fake news, and satire differs from real news. The authors also concluded that humans rarely perform better than chance at detecting deceptive activities. Therefore, humans performed worse than the best automated model.", "filtered_refids": [[], ["b15"], ["b6", "b46", "b45", "b2", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 31, "num_chars": 3967, "num_references": 6}
{"corpusid_sectionid": "251307817-s8", "title": "Rhetorical Structure Approach for Online Deception Detection: A Survey", "date": 2022, "section_title": "Karimi and Tang (2019)", "section": "Discourse-level structure analysis of deceptive and truthful news is a tremendous challenge, mainly due to existing methods for capturing discourse-level structure rely on annotated corpora, which are not available for fake news datasets (Karimi and Tang, 2019). In this proposal, the authors provide a new dependency parsing approach, titled \"Hierarchical Discourse-level Structure for Fake news detection\". The HDSF consists of an automated manner to learn a discourselevel structure for a given document through an approach based on the dependency parsing at the sentence level. It should be noted that in this approach, sentences are classified as elementary discourse units (EDU's). An example of discourse-level structure of a document (fake news) using the proposed dependency tree is shown in Figure 5. Note that a document is segmented into sentences (S1, S2, S3, S4 and S5), and hierarchically organized. Figure 5: Hierarchical discourse-level structure of a document using a dependency tree. This fake news was extracted from Politifact.\n\nThe HDSF framework build a hierarchical structure between sentences without relying on an annotated corpus, as may be seen in Figure 6. Note that the HDSF receives as input a corpus of fake/real news documents (i.e., D). A model M may automatically learn hierarchical and structurally rich representations for documents in D. Meanwhile, given binary labels Y, model M uses the hierarchical representations to automatically predict the labels of unseen news documents. In order to compare the HDSF approach with baseline and state-of-art models, the authors implemented seven different models including the proposed methods: Ngrams, LIWC (Pennebaker et al., 2015), Bag-of-rst (Rubin and Lukoianova, 2015), BiGRNN-CNN (Ren and Zhang, 2016), LSTM and LSTM[w+s] (Karimi and Tang, 2019). Based on the obtained results, the HDSF overcame the other implemented approaches (82.19% of Accuracy). They concluded that discourselevel structure analysis is effectively rich for fake news prediction. In addition, the structures of fake news documents at the discourse level are substantially different from those of true ones, and real news documents indicate more degree of textual coherence.", "filtered_refids": [["b15"], ["b15", "b34", "b31", "b35"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2230, "num_references": 5}
{"corpusid_sectionid": "251307817-s9", "title": "Rhetorical Structure Approach for Online Deception Detection: A Survey", "date": 2022, "section_title": "Atanasova et al. (2019)", "section": "In this proposal, the authors focus on contextual and discourse-level structure information, which, according to them, provide important information that is typically not found over usual feature sets. The authors model the problem of fake news detection into two main tasks: (i) claim classification, which consists of automated identification of claims in political debates that a journalist should fact-check, and (ii) answer fact-checking, which consists of automatic verification of political answers in community-driven Web forums. They implemented an extensive block of experiments for both tasks using both classical and neural machine learning methods. The datasets used were: CW-USPD-2016 (Gencheva et al., 2017), which is annotated at the sentence level as check-worthy or not, and the context of the full debate was kept. it provides a binary annotation: whether a sentence was annotated for factuality by a given fact-checking, and composed of 4,5355 positive documents, and 880 negative documents; and CQA-QL-FACT (Nakov et al., 2016), which consists of a dataset composed by (i) a good vs. a bad answers, and (ii) a factually true vs. a factually false one. CQA-QL-FACT dataset provides 373 answers classified as factual, 689 answers classified as opinion, and 295 answers classified as socializing.\n\nAs previously stated, the authors propose methods for two different tasks: claim identification and answer fact-checking. For claim identification, a robust neural model that embodies a set of rich contextual and discourse features was proposed. Figure 7 shows the proposed models. A RST-based discourse parser (Joty et al., 2015) was used to obtain rhetorical structure features. As displayed in Figure 7, each segment is defined as a \"maximal set of consecutive sentences by the same speaker, without intervention of another speaker or the moderator\". In addition, the authors use a feed-forward neural network (FNN) with two hidden layers (with 200 and 50 neurons, respectively) and a softmax output unit for the binary classification. ReLU (Glorot et al., 2011) was used as the activation function and training happened for 300 epochs with a batch size of 550. They set the L2 regularization to 0.0001 and kept a constant learning rate of 0.04. On the other hand, for the answer fact-checking task, the authors built an interesting and robust model. The model combines an LSTM-based neural network with support vector machines to classify three question categories (factual, opinion, and socializing), as shown in Figure 8. Notice that a layer of pre-trained embeddings, and the blind layers are used in order to supplement the other features proposed by the authors (e.g., web support, ql support, similarities, etc.). Finally, the best performance for both tasks was obtained with the model that embodies discourse and contextual features as a supplement to other features (69% of F1-Score). Therefore, the authors concluded that contextual and discourse information may improve the performance of fake news detection systems.   ", "filtered_refids": [["b9", "b28"], ["b14", "b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3051, "num_references": 4}
{"corpusid_sectionid": "251307817-s10", "title": "Rhetorical Structure Approach for Online Deception Detection: A Survey", "date": 2022, "section_title": "Pisarevskaya (2017)", "section": "In this research, the author reiterates the importance of understanding the difference between true and fake news evaluating the reliability of sources, mainly due to fact that large-scale data that have been shared daily on the web and social media. According to author, new methods for deception detection and information verification must be created for different languages. Accordingly, this proposal consists of investigating the suitability of RST-based coherence relation features in order to build a deception detection model for fake news detection in Russian. While this proposal is inspired by research of  for the English language, the author has also considered the linguistic distinctions between the English and Russian languages. At first, the authors proposed a new discourse-level manually annotated corpus using RST framework. For data collection, news stories were manually analyzed in retrospect, when the factuality was already known, and fake stories were classified with a negative class (0) and truthful stories were classified with a positive class (1). According to the author, towards class balancing, the texts were collected from different sources: wellknown news agencies' websites, local or topic-based news portals, online newspapers from different countries (Russia, Ukraine, Armenia, etc.). Blog texts, social media content, news reports based on opinions (not on facts) were excluded from the data. Over the annotation process, the author reports the average number of rhetorical relations for each document such as 17.43.\n\nIn the same settings, the reported total of rhetorical relations in the corpus is equal to 2.340. Clauses were taken as elementary discourse units (EDU's). In order to support the annotation process, an accurate RST guideline was proposed towards minimize the problem of subjectivity of annotators' interpretation. Moreover, an evaluation measure was applied, obtaining a human agreement of 75%. An overview of this annotated corpus is shown in Table 1. Moving forward, a subjective lexicon-based analysis was also performed. More specifically, this analysis consists of assessing behavior of positive and negative lemmatized words using a lexicon composed of 5,000 sentiment words from reviews devoted to various topics. Consideration the building of the model, RST coherence relations occurrences and their respective nuclearity were represented as features into a machine learning-based model. The authors titled this representation as \"bag-of-rstrelation-types\". Support Vector Machine (Scholkopf and Smola, 2001) and Random Forest (Breiman, 2001) were used as learning methods. The results reported by the author are quite incipient. The best obtained performance reached F1-score of 65%. The author also proposed the evaluation of human performance to classify deceptive and truthful stories in Russian, whose results evidenced that human performance is highly unsatisfactory (50% of F1-score) and worse than the best performance automated classification. , Rubin and Lukoianova (2015) This proposal is the first one that uses RST applied to deception detection. Therefore, it may be considered a baseline method. The authors examined the rhetorical structure, discourse constituent parts, and their coherence relations for deceptive (fabricated) and truthful (authentic) news to uncover systematic language differences and inform deception verification systems. The proposed approach for fake news detection using RST-annotated corpus was performed using a dataset of 132 documents, with an equal amount of deceptive and non-deceptive news. An overview of this dataset is exhibited in Table 1. The data was collected from the US National Public Radio (NPR) 4 , during the period of March 2010 to May 2014, and contains transcripts of the weekly radio show \"Wait, Wait, Don't Tell Me\" with its \"Bluff the Listener\". According to the authors, most news reports are typically humorous and a set of them are highly unlikely or unbelievable (e.g., a ship captain plotting his ship's course across land or a swim instructor not knowing how to swim). Moreover, the corpus was manually annotated by two analysts using RST. The authors report a human inter-annotator agreement of 69% using Cohen's kappa. In this proposal, an automated news verification method using RST and Vector Space Modeling (VSM) was proposed. The method titled \"RST-VSM approach\" applies the RST towards discourse analysis of true and fake news, and VSM in order to interpret the discourse features. The RST-VSM proposed approach was divided into three different experiments: (i) centering, (ii) clustering, and (iii) predictive model. The first experiment -centering -consists of the VSM representation used to assess each news report's position in a multi-dimensional RST space. Clustering of truthful and deceptive data points in this space was evaluated based on distances to hypothetical cluster centers. The authors obtained relevant differences between truthful and deceptive centers for each set of rhetorical relations. The second experiment -clustering -consists of a clusterization process of fake news and true news based on their similarity according to a chosen agglomerative clustering algorithm, with k-nearest neighbor clustering. They used the gCLUTO clustering package 5 . As a result of this experiment, four similarity clusters were formed. The clustering model was able to correctly assess 63% (20 out of 32 stories). The third experiment -predictive model -consists of a logistic regression model based on the training lumped dataset. Based on the performed experiments the authors report that (i) four logistic regression indicators were identified (from a set of 18) pointed to truth (DISJUNCTION, PURPOSE, RESTATEMENT, SOLUTIONHOOD), while another predictor (CONDI-TION) pointed to deception. Finally, the proposed approach RST-SVM obtained 63% of accuracy. Popoola (2017) In this proposal, the author analyzes RST coherence relations on a forensic collection of authentic and fake Amazon book reviews. The author concludes that paid review writers deploy deceptive pragmatics (i.e., a coherent set of linguistic strategies) to support the intent to deceive. At aiming to analyze deceptive and true intentions from reviews, the author annotated fifty reviews classified equally in fake and real reviews from the DeRev corpus (Fornaciari and Poesio, 2014). This corpus was collected of 6,819 Amazon book reviews of 68 books written by 4,811 different reviewers. A complete corpus overview is shown in Table 3. In this proposal, the author provides a robust corpus study. Based on obtained results, fake reviews present more ELABORATION, JOINT and BACKGROUND coherence relations, while the true reviews have more EVALUATION, CONTRAST, EXPLANATION relations. Moreover, the coherence relation of COMPAR-ISON was found only in the real reviews. In addition, the author qualitatively evaluated the content and location of most Nuclear Discourse Units (NDU) (Stede, 2008), which, in accordance with the author, were predictors of deception. Results are shown in Table 4. As shown in Table 4, the NDUs were mainly located in the opening sentence, mentioned the book title, and often provided the author name with a brief plot/content description. Therefore, the author concluded that RST analysis provides rich qualitative data for the generation of a set of regulatory heuristics that might include consumer warnings such as: (i) fake reviews are more likely to mention book titles, and authors, as well as give details of a book's content; (ii) fake 5-star reviews tend to be all positive, whereas genuine 5-star reviews usually contain caveats.", "filtered_refids": [[], ["b39", "b41", "b35", "b7", "b1", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 55, "num_chars": 7697, "num_references": 6}
{"corpusid_sectionid": "251307817-s12", "title": "Rhetorical Structure Approach for Online Deception Detection: A Survey", "date": 2022, "section_title": "Discourse-Aware Deception Detection: Main Challenges", "section": "Although the RST has been applied to a wide variety of successful applications, we should not simply see it without any criticism. For instance, there are several vague statements and definitions described in the RST original proposal. Indeed, it has been criticized by various authors mainly concerning the aspects related to the absence of a minimal text unit's granularity. Furthermore, rhetorical relations are also highly ambiguous. Since the author have suggested that a level of ambiguity is completely natural between the relations, there are not any instructions or enough scientific and methodology elements to address the relations ambiguity. According to Schauer and Hahn (2000), the number and nature of the rhetorical relations are faintly defined. For example, could any researcher propose and use a set of coherence relations that suits her purposes, and would be them really rhetorical relations? In spite of the disapproval from various authors, a couple of authors proposed to address these \"open questions\". For example, Maier and Hovy (1993) suggested a taxonomy of three different levels: ideational, interpersonal, and textual in order to group rhetorical relations. In the same setting, Stede et al. (2017), Carlson and Marcu (2001), Vargas et al. (2021) have proposed new rhetorical relations and updated the RST framework. Lastly, another relevant challenge consists of lack of RST-annotated corpus for the deception detection tasks, and low-performance of RST parsers.", "filtered_refids": [["b43", "b40", "b21", "b37", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1495, "num_references": 5}
{"corpusid_sectionid": "252461144-s4", "title": "A Survey of Machine Translation Tasks on Nigerian Languages", "date": 2022, "section_title": "Machine Translation Techniques", "section": "Machine translation has been extensively studied for decades (Bahdanau et al., 2016;Luong et al., 2015;Koehn et al., 2007b) with neural machine translation providing the most recent state-of-the art results. There are three types of machine translation techniques that have been explored in the literature -Rule-based machine translation (RBMT), Statistical machine translation (SMT), and Neural machine translation (NMT). A high level overview of these techniques are outlined as follows:\n\nRule-based Approach: This is one of the oldest form of machine translation technique used. This approach is based on understanding the linguistic properties of the source and target languages using dictionaries and expert knowledge to define grammar rules. This process involves morphology analysis, syntax, and lexical semantics. Linguistic analysis is performed on the source language to identify morphology, parts of speech, phrases, named entity, and word disambiguation. Each word is replaced in the target language using a dictionary which represents mappings between source and target words. In order to preserve sentence semantics across translated languages, most RBMT approach utilizes a combination of finite state machines to develop their knowledge graphs (Forcada et al., 2011;Scott and Barreiro, 2009). (Forcada et al., 2011) utilizes finite-state transducers for lexical processing, Hidden Markov models for part-ofspeech tagging, and multi-stage finite-state chunking for structural transfer. (Eisele et al., 2008) utilizes a modified phrase table with entries from translating various data with rule-based systems. One of the main advantage of this approach is that it does not require as much parallel sentence pairs as with most NMT approaches. Also, translation errors can be corrected by updating the dictionary. This allows for flexibility in updating language constructs. Consequently, one major drawback of this approach is that the translation quality is mostly defined by the strength of the dictionary which requires frequent updates from domain experts. RBMT also tends to produce translations that are more repetitive and less fluid which can be attributed to its mechanical approach of using rules for translation.\n\nStatistical-based Approach: This approach involves the use of statistical techniques such as probability distribution models to provide a means for machine translation between source and target languages. This is achieved by assigning a probability score to word or phrase contained in every target sentence where words or phrases with the highest probability contains the best translation for the target sentence (Koehn et al., 2007b;Brown et al., 1993). SMT can be applied at a word or phrase level and consists of a translation and language model. The translation model is defined as the probability that the source sentence is the translated version of the target word. The language model tries to describe how representative the target sentence is to the natural spoken language. It assigns probabilities to sentence similar to the sentence ordering. One approach utilized in developing the probability distributions is the use of Bayes theorem (Zens et al., 2002) and Hidden Markov Model (Deng and Byrne, 2008;Alkhouli et al., 2016). (Koehn et al., 2007a) developed Moses, an open-source machine translation toolkit which utilizes linguistic information that captures semantics in mapping text phrases and a confusion network decoding for translating ambiguous text inputs.\n\nOne advantage of SMT approach over RBMT is the improved translation quality. It allows for translation that captures not just linguistic morphology but the use of a probability distribution which improves with semantic quality.\n\nNeural-based Approach: This approach is referred to as the state of the art in machine translation as it is widely used and has shown to provide results with higher accuracy as compared to the other approaches (Bahdanau et al., 2016;Luong et al., 2015;Cho et al., 2014). Neural machine translation involves the use of deep learning techniques to provide a means of inferring high level semantics from language translations. A popular neural machine translation approach (Vaswani et al., 2017) utilize transformer based models with encoder-decoder architecture. These models consists of stacks of multiple hidden layers with multi-head attention mechanisms and have been shown (Vaswani et al., 2017) to outperform traditional neural architecture such as Recurrent Neural Networks for machine translation task.\n\nCurrent implementation for language models consists of multilingual language model embeddings (Pires et al., 2019;Lample and Conneau, 2019) where one language model is trained on multiple languages. This allows for zero-shot transfer learning where cross language representation is learned without the need for a parallel language corpus across all language pairs. This has been shown to produce better results than monolingual model training (Conneau et al., 2020) especially for low-resource languages. Supervised neural approach relies heavily on a large corpus of quality translated sentence pairs; as such this poses a limitation to the quality of language translation. There are some approaches that work well with limited datasets (Mikolov et al., 2013;Artetxe et al., 2018) and can provide a means of translating from one language to another based on translations derived from a similar language ( ", "filtered_refids": [["b10", "b34", "b32"], ["b24", "b48", "b20"], ["b6", "b58", "b14", "b31", "b32", "b19"], [], ["b53", "b10", "b34", "b17"], ["b18", "b47", "b33", "b8", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 33, "num_chars": 5464, "num_references": 21}
{"corpusid_sectionid": "252461144-s5", "title": "A Survey of Machine Translation Tasks on Nigerian Languages", "date": 2022, "section_title": "State of the Art", "section": "There has been a limited number of work centered on machine translation of Nigerian languages. Most of the cutting edge research on machine translation utilizes neural machine translation approaches (Stahlberg, 2020). However, most of the machine translation work on Nigerian languages focuses on rule-based approach using context free grammars while a few focuses on neural machine translation techniques such as transformer-based models. We outline the work that has been conducted over the years and categorize each work based on the different approach utilized.\n\n3.1. Rule-based Approach (Ayegba et al., 2014) utilizes a rule-based approach for machine translation of English to Igala language. This approach utilizes noun phrases from English language while performing a series of processes such as parts of speech tagging, morphological analysis which analyzes words based on its root or base form, and comparing noun phrases to components contained in a bilingual dictionary. Their approach was tested on 120 randomly selected English noun phrases and achieves a Bilingual Evaluation Understudy (BLEU) accuracy of 90.9%. (Akinwale et al., 2015) proposed a web-based English to Yoruba translation model utilizing a similar approach as (Ayegba et al., 2014). The translator component utilizes a set of twenty rules which were specified using context free grammar. This approach achieves an accuracy of 90.5%. (Eludiora and Ajibade, 2021) proposed a rule-based model for English to Yoruba translation of Yoruba verbs based on tone changing. It is their intuition that some Yoruba verbs change tone in the bilingual dictionary from low-tone to mid-tone which sometimes changes the meaning of the sentence. Their approach is implemented using 20 tone changing verbs. They evaluate the efficacy of their approach by performing language expert evaluation which entails comparing the output derived from their model with the output generated from Google translation. According to the authors, this approach is very time-consuming but very extensive. In addition, they evaluate their approach using human evaluators. In a total of 70 respondents, 69% of the respondents agree that their system correctly translates verb-phrases while 29% of the respondents agrees that Google translation works efficiently. (Ezeani et al., 2016) developed a model using the Igbo Bible corpus to detect and restore missing didactics in texts at word level toknization. Their approach on didatic replacement utilizes work conducted by (Simard, 1998) which consists of using Hidden Markov Model in which the input text is viewed as a stochastic process. (Onyenwe et al., 2019) develops a parts of speech (POS) tagger for Igbo language. Their approach utilizes a host of post tagging approach including Hidden Markov Model. They achieve an accuracy of of 93.17% to 98.11% on the overall words, and 7.13% to 83.95% on unknown words. (Orife, 2020) developed a neural machine translation model for translating Edoid languages to English. Edoid languages are primarily spoken by the southern Nigeria (Edo and Delta states) consisting of Edo, Esan, Urhobo, and Isoko languages. They utilize transformer models with encoder decoder and multi-head self attention. To evaluate the effectiveness of their approach, they trained their model using JW300 dataset (Agi\u0107 and Vuli\u0107, 2019) consisting of over 100 African languages The training was conducted using tokenization processeses such as Byte-pair encoding (BPE) and word-level tokenization . The results shows that Urhobo and Isoko consists of larger training dataset performed best with higher BLEU scores. BPE tokenization provided a 37% boost for the development and test dataset of Edo and Esan languages and a 32% boost for Urhobo language. However, BPE produced worse results when compared to word-level tokenization for Isoko languages. (Ahia and Ogueji, 2020) developed supervised and unsupervised neural machine translation models to serve as a baseline for future works to come in the translation of Nigerian pidgin. For their approach, they utilized a transformer architecture proposed by (Vaswani et al., 2017) while experimenting with word-level and Byte-Pair encoding subword tokenization. The supervised approach produced a BLEU score of 17.73 while the unsupervised model produced a BLEU score of 5.18 for English to Pidgin Translation. (Nguyen and Chiang, 2018) developed a model that improves the mistranslation of rare words. This approach is based on a modified version of attention based encoderdecoder models. Their approach hones on the premise that the output layer which consists of the inner product of the context vector and all possible word embeddings improperly rewards frequently occurring words. In their approach, instead of using the dot product, the norm vectors are set to a constant value. In addition, they include new terms which provides direct connection from source sentence and this makes the model properly memorize rare word translations. They evaluate their approach on 8 language pairs which includes Hausa to English language pair. (Hedderich et al., 2020) demonstrates that a transfer learning approach through multilingual transformer models (mBERT and XLM-RoBERTa) can be utilized for tasks such as name entity recognition and topic classification on low-resource languages. The approach involves fine-tuning the target language dataset on high-resource language models. Their approach is evaluated on three African languages Hausa, isiXhosa and Yoruba out of which two of the languages (Hausa, and Yoruba) are Nigerian languages. They produce results comparable to the state-of-the-art with as little as 10 or 100 labelled sentences. They achieve at least an improvement of 10 points in the F-1 score for a shared label of named entity recognition. Their result shows promise and is consistent with their hypothesis which also validates work shown in prior research. Their approach however does not produce good results for topic classification. This might be as a result of mismatch in the label set. (Ogueji et al., 2021) developed AfriBERTa, an approach which involves training multilingual models on low-resource language. According to the authors, it is a general assumption that low-resource multilingual language models benefit from being trained in combination with high-resource languages. low-resource multilingual models do not need to be trained in combination with highresource languages and does not require as much dataset used for training high-resource languages. The authors accomplish multilingual model training on low-resource languages with a dataset consisting of 11 African languages of which Igbo, Yoruba, Hausa, and Nigerian Pidgin are Nigerian languages. They also show that the state of the art accuracy can be achieved with training on less than 1GB of data. Furthermore, they apply their pre-trained transformer model on downstream tasks such as name entity recognition and text classification task. Their model outperforms the state of the art multilingual models such as mBERT and XLM-R.", "filtered_refids": [["b50"], ["b44", "b4", "b46", "b9", "b28", "b41", "b49", "b21", "b45", "b22", "b53", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 53, "num_chars": 7092, "num_references": 13}
{"corpusid_sectionid": "252461144-s8", "title": "A Survey of Machine Translation Tasks on Nigerian Languages", "date": 2022, "section_title": "Data Acquisition", "section": "One of the major impediments to corpus-based machine translation of low-resource languages is the quality and quantity of the dataset utilized for model training. However, there has been limited work in generating datasets for machine translation tasks of Nigerian languages. Some of the prominent dataset utilized for this task are outlined below.  developed an open-source dataset of Yoruba speech which consists of over four hours of recordings from 36 male and female volunteers with transcription and disfluency annotation. (Adelani et al., 2021) developed a publicly available parallel corpus known as MENYO-20K which consists of a parallel corpus of texts in English-Yoruba language with over 20,000 sentences obtained from news articles, TED talks, movie and radio transcripts, science and technology texts and short articles from the web which were annotated by professional translators with proficiency in Yoruba language. (Butryna et al., 2020) developed a crowd-sourced speech corpus for low-resource languages which consists of languages in South and Southeast Asia, Africa (South Africa, and Nigeria), Europe and South America. The only Nigerian language supported was Nigerian Pidgin. They achieve this task by partnering with local communities and universities in the region. (Agi\u0107 and Vuli\u0107, 2019) introduces JW300, a parallel corpus of over 300 languages containing around over 100,000 sentences per language pair. The corpus consists of a total of 1,335,376 articles with over 109 million sentences and 1.48 billion tokens. They achieve this by crawling publications from jw.org. OPUS (Tiedemann, 2012), is one of the largest open source parallel corpora repository of translated text. It consists of over 90 languages with a total of 3,800 language pairs comprising of over 40 billion tokens in 2.7 billion parallel units. (Goyal et al., 2021) introduces Flores-101, an open-source benchmark for evaluating lowresource multilingual machine translation task. This dataset consists of 3,000 sentences extracted from Wikipeadia. In addition, the sentences have been converted into 101 languages which includes three major languages in Nigeria (Igbo, Yoruba, and Hausa). (Ezeani et al., 2020) developed a publicly available standard evaluation benchmark dataset for Igbo to English machine translation. This includes over 10,000 high quality English to Igbo sentence pairs which were derived mostly from news (BBC Igbo 2 and PUNCH newspaper 3 ) domains.", "filtered_refids": [["b15", "b25", "b23", "b1", "b51", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2469, "num_references": 6}
{"corpusid_sectionid": "252461144-s10", "title": "A Survey of Machine Translation Tasks on Nigerian Languages", "date": 2022, "section_title": "Limited Open Source Datasets", "section": "There is a strong need to create more high-quality dataset that can be used for neural machine translation. Most of the parallel corpora available consists of less than 100,000 translated sentence pairs. One approach to generating highquality parallel corpora in addition to utilizing linguistic experts with domain knowledge is to leverage crowd-sourcing platforms like Amazon Mechanical Turk 4 to provide translation from native speakers. It has been shown in previous literature (Bloodgood and Callison-Burch, 2014) that crowd-sourcing platforms can provide translation at an expert level with a reduced cost. One drawback to the use of crowd-sourcing platforms is the difficulty in evaluating the competency of the reviewers (Allahbakhsh et al., 2013). There are metrics to circumvent this issue such as evaluating the reviewer translation (Nowak and R\u00fcger, 2010). Another drawback to using crowd-sourcing for machine translation is that it often does not establish real connections between the linguist and the language community which is an essential component for fostering an efficient translation ecosystem and also for understanding the needs of the community (Bird, 2020). For more information on all of the stakeholders contained in the language translation process, we refer the reader to view the work by (Nekoto et al., 2020a). In the absence of high quality large training datasets, one can employ the use of unsupervised learning approaches (Artetxe et al., 2018), zero-shot learning (Johnson et al., 2017) and various data augmentation and transfer-learning approaches (Zoph et al., 2016;Nguyen and Chiang, 2017) which requires minimal training datasets.", "filtered_refids": [["b43", "b11", "b40", "b13", "b29", "b59", "b7", "b38", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1672, "num_references": 9}
{"corpusid_sectionid": "252461144-s11", "title": "A Survey of Machine Translation Tasks on Nigerian Languages", "date": 2022, "section_title": "Fairness in Language Models", "section": "A number of language models are developed without considering the variety of the training dataset and as such might not effectively transfer to low-resource languages (Wu and Dredze, 2020). Ensuring that our language models are able to cater to a diverse set of machine translation tasks while producing appropriate results is as crucial as the machine translation task (Nekoto et al., 2020b). More emphasis needs to be placed on evaluating the fairness of machine learning (ML) and artificial intelligence (AI) algorithms with a focus on learning algorithms used to develop these machine translation models while taking into consideration the effects of the diversity of its training dataset. It is important to note that AI fairness has become a focal point and an active area of research amongst the machine learning community (Mehrabi et al., 2019). It is important that fairness in incorporated into the entire machine translation process as a lack of fairness can possibly lead to socio-economic inequalities and also language misrepresentation based on gender (Vanmassenhove et al., 2018) ethnic groups (Nekoto et al., 2020a).", "filtered_refids": [["b52", "b39", "b56", "b35", "b38"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1133, "num_references": 5}
{"corpusid_sectionid": "246017121-s16", "title": "Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset", "date": "2022-01-07", "section_title": "Implementation Details", "section": "Data pre-processing. We implement spectral augmentation (SpecAugment), a state-of-the-art audio data augmentation method, which is implemented by masking certain frequency and time values on the spectrogram (Park et al., 2019). We use SpecAugment for the Common Voice zh-HK baseline, where it shows an improvement in overall results. Furthermore, we apply cepstral mean and variance normalisation (CMVN) for all the utterances (Strand and Egeberg, 2004). In Fairseq S2T, pre-processed audio can be used directly or stored in the form of .npy files. The latter is the way in which we store features extracted from Cantonese datasets to achieve faster training. For tokenization of the transcribed data, we use the SentencePiece tokenizer (Kudo and Richardson, 2018) with unigram subword tokenization (Kudo, 2018) and an 8,000-word vocabulary. The vocabulary covers 99.95% of the characters in the MDCC (the default coverage for character-based languages).", "filtered_refids": [["b11", "b21", "b22", "b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 954, "num_references": 4}
{"corpusid_sectionid": "14642384-s3", "title": "DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL", "date": "2004-07-01", "section_title": "Descriptions serving as disguised PROPER", "section": "NAMES, such as The Federal Communications Commission or the Iran-Iraq war. The heuristics for recognizing these definite descriptions were primarily based on capitalization (of the head or the modifiers).\n\n3. PREDICATIVE descriptions, i.e., descriptions semantically functioning as predicates rather than as referring. These include descriptions occurring in appositive position (as in Glenn Cox, the president of Phillips Petroleum) and in certain copular constructions (as in the man most likely to gain custody of all this is a career politician named Dinkins). The heuristics used to recognize these cases examined the syntactic structure of the NP and the clause in which it appeared.\n\n4. Descriptions ESTABLISHED (i.e., turned into functions in context) by restrictive modification, particularly by establishing relative clauses (Loebner, 1987) and prepositional phrases, as in The hotel where we stayed last night was pretty good.\n\nThese heuristics, as well, examined the syntactic structure of the NP. 5. LARGER SITUATION definite descriptions (Hawkins, 1978), i.e., definite descriptions like the sun, the pope or the long distance market which denote uniquely on the grounds of shared knowledge about the situation (these are Loebner's 'situational functions'). Vieira and Poesio's system had a small list of such definites.\n\nThese heuristics were included as tests both of a decision tree concerned only with the task of DN detection, and of decision trees determining the classification of DDs as anaphoric, bridging or discourse new. In both cases, the DN detection tests were intertwined with attempts to identify an antecedent for such DDs. Both hand-coded decision trees and automatically acquired ones (trained using ID3, (Quin-lan, 1986)) were used for the task of two-way classification into discourse-new and anaphoric. Vieira and Poesio found only small differences in the order of tests in the two decision trees, and small differences in performance. The hand-coded decision tree executes in the following order:\n\n1. Try the DN heuristics with the highest accuracy (recognition of some types of semantically functional DDs using special predicates, and of potentially predicative DDs occurring in appositions);\n\n2. Otherwise, attempt to resolve the DD as direct anaphora;\n\n3. Otherwise, attempt the remaining DN heuristics in the order: proper names, descriptions established by relatives and PPs, proper name modification, predicative DDs occurring in copular constructions.\n\nIf none of these tests succeeds, the algorithm can either leave the DD unclassified, or classify it as DN. The automatically learned decision tree attempts direct anaphora resolution first. The overall results on the 195 DDs on which the automatically trained decision tree was tested are shown in  ", "filtered_refids": [[], [], ["b4"], ["b2"], [null], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2799, "num_references": 3}
{"corpusid_sectionid": "14642384-s5", "title": "DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL", "date": "2004-07-01", "section_title": "Ng and Cardie", "section": "The discourse-new detectors proposed by Ng and Cardie are statistical classifiers taking as input 37 features and trained using either C4.5 (Quinlan, 1993) or RIPPER (Cohen, 1995). The 37 features of a candidate anaphoric expression specify, in addition to much of the information proposed in previous work, a few new types of information about NPs.\n\n\u2022 The four boolean so-called LEXICAL features are actually string-level features: for example, str_match is Y if a preceding NP string-matches the anaphoric expression (except for the determiner), and head_match = Y if a preceding NP's head string-matches the anaphoric expression's. embedded=Y if the anaphoric expression is a prenominal modifier.\n\n\u2022 The second group of 11 (mostly boolean) features specifies the type of NP: e.g., pronoun is Y if the anaphoric expression is a pronoun, else N.\n\n\u2022 The third group of 7 features specifies syntactic properties of the anaphoric expression, including number, whether NP j is the first of two NPs in an appositive or predicative construction, whether NP j is pre-or post-modified, whether it contains a proper noun, and whether it is modified by a superlative.\n\n\u2022 The next group of 8 features are mostly novel, and capture information not used by previous DN detectors about the exact composition of definite descriptions: e.g., the_2n=Y if the anaphoric expression starts with determiner the followed by exactly two common nouns, the_num_n=Y if the anaphoric expression starts with determiner the followed by a cardinal and a common noun, and the_sing_n=Y if the anaphoric expression starts with determiner the followed by a singular NP not containing a proper noun.\n\n\u2022 The next group of features consists of 4 features capturing a variety of 'semantic' information, including whether a previous NP is an 'alias' of NP j , or whether NP j is the title of a person (the president).\n\n\u2022 Finally, the last three features capture information about the position in the text in which NP j occurs: the header, the first sentence, or the first paragraph.\n\nNg and Cardie's discourse-new predictor was trained and tested over the MUC-6 and MUC-7 coreference data sets, achieving accuracies of 86.1% and 84%, respectively, against a baseline of 63.8% and 73.2%, respectively. Inspection of the top parts of the decision tree produced with the MUC-6 suggests that head_match is the most important feature, followed by the features specifying NP type, the alias feature, and the features specifying the structure of definite descriptions.\n\nNg and Cardie discuss two architectures for the integration of a DN detector in a coreference system. In the first architecture, the DN detector is run first, and the coreference resolution algorithm is run only if the DN detector classifies that NP as anaphoric. In the second architecture, the system first computes str_match and alias, and runs the anaphoric resolver if any of them is Y; otherwise, it proceeds as in the first architecture. The results obtained on the MUC-6 data with the baseline anaphoric resolver, the anaphoric resolver augmented by a DN detector as in the first architecture, and as in the second architecture (using C4.5), are shown in Table 3. The results for all NPs, pronouns only, proper names only, and common nouns only are shown. 2 As indicated in the Table, running the DN detector first leads to worse results-this is because the detector misclassifies a number of anaphoric NPs as nonanaphoric. However, looking first for a same-head antecedent leads to a statistically significant improvement over the results of the baseline anaphoric resolver. This confirms the finding both of Vieira and Poesio and of Bean and Riloff that the direct anaphora should be called very early.\n\n2 It's not clear to us why the overall performance of the algorithm is much better than the performance on the three individual types of anaphoric expressions considered-i.e., which other anaphoric expressions are handled by the coreference resolver.  Table 3: Evaluation of the three anaphoric resolvers discussed by Ng and Cardie. Uryupina (2003) trained two separate classifiers (using RIPPER, (Cohen, 1995)): a DN detector and a UNIQUENESS DETECTOR, i.e., a classifier that determines whether an NP refers to a unique object. This is useful to identify proper names (like 1998, or the United States of America), semantic definites (like the chairman of Microsoft) and larger situation definite descriptions (like the pope). Both classifiers use the same set of 32 features. The features of an NP encode, first, of all, string-level information: e.g., whether the NP contains capitalized words, digits, or special symbols. A second group of features specifies syntactic information: whether the NP is postmodified, and whether it contains an apposition. Two types of appositions are distinguished, with and without commas. CONTEXT features specify the distance between the NP and the previous NP with the same head, if any. Finally, Uryupina's system computes four features specifying the NP's definite probability. Unlike the definite probability used by Bean and Riloff, these features are computed from the Web, using Altavista. From each NP, its head H and entire NP without determiner Y are determined, and four ratios are then computed:", "filtered_refids": [["b15", "b1"], [], [], [], [], [], [], [], [null], ["b1", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 5284, "num_references": 5}
{"corpusid_sectionid": "14642384-s9", "title": "DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL", "date": "2004-07-01", "section_title": "How much does DN-detection help the Vieira / Poesio algorithm?", "section": "GUITAR (Poesio and Alexandrov-Kabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov's algorithm for pronoun resolution (Mitkov, 1998). It is implemented in Java, takes its input in XML format and returns as output its input augmented with the anaphoric relations it has discovered. GUITAR has been implemented in such a way as to be fully modular, making it possible, for example, to replace the DD resolution method with alternative implementations. It includes a pre-processor incorporating a chunker so that it can run over both hand-parsed and raw text. A version of GUITAR without the DN detection aspects of the Vieira / Poesio algorithm was evaluated on the GNOME corpus (Poesio, 2000;, which contains 554 definite descriptions, of which 180 anaphoric, and 305 third-person pronouns, of which 217 anaphoric. The results for definite descriptions over hand-parsed text are shown in Table 6   Notice that although these results are not particularly good, they are still better than the results reported by Ng and Cardie for pronouns and definite NPs.", "filtered_refids": [["b6", "b10", "b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1162, "num_references": 3}
{"corpusid_sectionid": "237353268-s3", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Visualization", "section": "A simple way to discover the role of a neuron is by visualizing its activations and manually identifying the underlying concept over a set of sentences (Karpathy et al., 2015;Fyshe et al., 2015;Li et al., 2016a). Given that deep NLP models are \u2020 Table 3 in Appendix gives a more comprehensive list. trained using billions of neurons, it is impossible to visualize all the neurons. A number of clues have been used to shortlist the neurons for visualization, for example, selecting saturated neurons, high/low variance neurons, or ignoring dead neurons (Karpathy et al., 2015) when using ReLU activation function. \u2021 Limitation While visualization is a simple approach to find an explanation for a neuron, it has some major limitations: i) it is qualitative and subjective, ii) it cannot be scaled to the entire network due to an extensive human-in-the-loop effort, iii) it is difficult to interpret polysemous neurons that acquire multiple roles in different contexts, iv) it is ineffective in identifying group neurons, and lastly and v) not all neurons are visually interpretable. Visualization nevertheless remains a useful tool when applied in combination to other interpretation methods that are discussed below.", "filtered_refids": [["b11", "b22", "b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1216, "num_references": 3}
{"corpusid_sectionid": "237353268-s7", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Linear Classifiers", "section": "The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.\n\nLimitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.\n\nGaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.\n\nLimitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.", "filtered_refids": [["b21", "b38"], ["b15", "b55"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2535, "num_references": 4}
{"corpusid_sectionid": "237353268-s8", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Causation-based methods", "section": "The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.\n\nAblation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.\n\nLimitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.\n\nKnowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.\n\nLimitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.", "filtered_refids": [[], ["b21", "b22"], [null, "b9", "b56"], ["b26", "b46", "b49", null, "b1"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2745, "num_references": 11}
{"corpusid_sectionid": "237353268-s9", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Miscellaneous Methods", "section": "In this section, we cover a diverse set of methods that do not fit in the above defined categories.\n\nCorpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (K\u00e1d\u00e1r et al., 2017) in interpreting neurons.\n\nLimitation Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation. A thorough evaluation is necessary to know its true potential and efficacy in NLP.\n\nMatrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.\n\nLimitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into. Moreover, the scope of the method is limited to local interpretation.\n\nClustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.\n\nLimitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.\n\nMulti-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.\n\nLimitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons. They can nevertheless be useful in tandem with the other interpretation methods. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.", "filtered_refids": [[], ["b19", "b6", "b37"], [], ["b35"], [], ["b28", "b29"], [], ["b39"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 37, "num_chars": 4902, "num_references": 8}
{"corpusid_sectionid": "237353268-s18", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Lexical Concepts", "section": "Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.\n\nVisualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example \"I like this movie a lot\" or \"the movie is incredibly good\". Similarly they discovered neurons that captured \"negation\". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. K\u00e1d\u00e1r et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items \"camera, laptop, cables\" and salad items \"broccoli, noodles, carrots etc\". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. \"law, legal\" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a \"horse racing\" neuron identified via concept search method was in fact a general \"racing\" neuron by generating novel contexts against this neuron.", "filtered_refids": [[], ["b34", "b20", "b19", "b22", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1540, "num_references": 5}
{"corpusid_sectionid": "237353268-s20", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Linguistic Concepts", "section": "A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasovi\u0107, 2018). \u00a7 For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012). Below we discuss major findings along this line of work:\n\nNeurons specialize in core linguistic concepts Dalvi et al. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept. For example: closed class \u00b6 concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.\n\nNeurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several \u00a7 but is not the only reason to carry such an analysis. \u00b6 closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example \"chillax\" a verb formed blending \"chill\" and \"relax\". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.\n\nNeurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.", "filtered_refids": [["b51", "b18", "b27"], [null], ["b54", null], ["b21", "b42", "b34", "b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2411, "num_references": 10}
{"corpusid_sectionid": "237353268-s23", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Information Distribution", "section": "Human languages are hierarchical in structure where morphology and phonology sit at the bottom followed by lexemes, followed by syntactic structures. Concepts such as semantics and pragmatics are placed on the top of the hierarchy.  analyzed linguistic hierarchy by studying the spread of neurons across layers in various pre-trained language models. They extracted salient neurons with respect to different linguistic concepts (e.g. morphology and syntax) and found that neurons that capture word morphology were predominantly found in the lower and middle layers and those learning about syntax were found at the higher layers. The observation was found to be true in both LSTMand the transformer-based architectures, and are inline with the findings of representation analysis (Liu et al., 2019;Tenney et al., 2019;Belinkov et al., 2020b). Similarly Suau et al. (2020) analyzed sub-modules within GPT and RoBERTa transformer blocks and showed that lower layers within a transformer block accumulate more salient neurons than higher layers on the tasks of word sense disambiguation or homograph detection. They also found that the neurons that learn homographs are distributed across the network as opposed to sense neurons that were more predominantly found at the lower layers.", "filtered_refids": [["b48", "b25", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1281, "num_references": 3}
{"corpusid_sectionid": "263835243-s3", "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances", "date": "2023-10-11", "section_title": "Knowledge Editing", "section": "Since tuning LLMs to learn new knowledge can be prohibitively expensive (Patterson et al., 2021), researchers seek efficient methods to directly update more specific, localized, or fine-grained knowledge that is preserved in LLMs (Mitchell et al., 2022a). Knowledge editing (KE) is an arising and promising research area that aims to alter the parameters of some specific knowledge stored in pre-trained models so that the model can make new predictions on those revised instances while keeping other irrelevant knowledge unchanged (Sinitsin et al., 2020;De Cao et al., 2021;Mitchell et al., 2022a;Meng et al., 2022a;Hase et al., 2023b;.\n\nIn this section, we categorize existing methods into meta-learning, hypernetwork, and locate-and-edit -based methods.\n\nMeta-learning. This line of work generally focuses on the intrinsic editability of the model itself, aiming to modify the model parameters so that they can be easily updated during inference (De Cao et al., 2021;Mitchell et al., 2022a  concept and propose a gradient-based knowledge attribution method to identify these knowledge neurons in FFNs. Further, without fine-tuning, they directly modify the corresponding value slots (e.g., embeddings) in the located knowledge neurons and successfully update or delete knowledge, demonstrating a preliminary potential to edit knowledge in LMs. Other.  propose an evaluation framework and dataset for measuring the effectiveness of knowledge editing of LLMs, as well as the ability to reason with the altered knowledge and cross-lingual knowledge transfer. Similarly, Cohen et al. (2023) evaluate the implications of an edit on related facts and show that existing methods fail to introduce consistent changes in the model's knowledge. Ju and Zhang (2023) propose an evaluation benchmark for locate-and-edit-based methods, aiming to reassess the validity of the locality hypothesis of factual knowledge.  and  take multilingual into account and extend existing knowledge editing methods into cross-lingual scenarios.", "filtered_refids": [[null, "b31"], [], [null, "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 2018, "num_references": 4}
{"corpusid_sectionid": "263835243-s4", "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances", "date": "2023-10-11", "section_title": "Continual Learning", "section": "Continual learning (CL) aims to enable a model to learn from a continuous data stream across time while reducing catastrophic forgetting of previously acquired knowledge (Biesialska et al., 2020). With CL, a deployed LLM has the potential to adapt to the changing world without costly re-training from scratch (Bubeck et al., 2023). In this section, we introduce approaches that employ CL for aligning LLMs with the current world knowledge, including continual pre-training and continual knowledge editing.\n\nContinual Pre-training. Unlike traditional continual learning, which sequentially fine-tunes a pre-trained LM on some specific downstream tasks (e.g., QA, text classification), continual pretraining is used to further pre-train an LM to acquire new knowledge, where the data corpus is usually unsupervised (Gururangan et al., 2020;Ke and Liu, 2023). Since our target is the versatile foundation LLMs (e.g., GPT-4) that can be applied to many different use cases rather than a fine-tuned model designed for a specific task, we focus on the literature on continual pre-training.\n\nEarly works (Gururangan et al., 2020;R\u00f6ttger and Pierrehumbert, 2021;Lazaridou et al., 2021;Dhingra et al., 2022)  1 Regularization. To mitigate forgetting, regularization-based methods apply regulations to penalize the changes of the critical parameters learned from previous data. Chen et al. (2020) improve the traditional EWC (Kirkpatrick et al., 2017) by recalling previously learned knowledge through the pre-trained parameters, and the method continually learns new information using a multitask learning objective.  compute the importance of each unit (i.e., attention head and neuron) to the general knowledge in the LM using a proxy based on model robustness to preserve learned knowledge. When continually learning new domains, the approach prevents catastrophic forgetting of the general and domain knowledge and encourages knowledge transfer via soft-masking and contrastive loss.\n\n2 Replay. These methods generally reduce forgetting by replaying previous training data when learning new data. Assuming that the initial pretraining corpus is available, He et al. (2021b) use a gradual decay mix-ratio to adjust the quantity of the pre-training corpus mixed in the new data when learning sequentially. ELLE  and CT0 (Scialom et al., 2022) also mix the old data while learning new data. However, ELLE starts the pre-training from a newly initialized and relatively small BERT ( ", "filtered_refids": [[null], [null], [null, "b34", "b27"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2475, "num_references": 6}
{"corpusid_sectionid": "264490542-s3", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Paper Databases", "section": "We source papers from eminent databases in the fields of NLP, the rest of CS, and medicine, as these are integral knowledge areas in the study of mental health CA.These databases include the ACL Anthology (referred to as ACL throughout this paper)2 , AAAI3 , IEEE4 , ACM5 , and PubMed6 .ACL is recognized as a leading repository that highlights pioneering research in NLP.AAAI features cuttingedge studies in AI.IEEE, a leading community, embodies the forefront of engineering and technology research.ACM represents the latest trends in Human Computer Interaction (HCI) along with several other domains of CS.PubMed, the largest search engine for science and biomedical topics including psychology, psychiatry, and informatics among others provides extensive coverage of the medical spectrum.\n\nDrawing on insights from prior literature reviews (Valizadeh and Parde, 2022;Montenegro et al., 2019;Laranjo et al., 2018) and discussion with experts from both the CS and medical domains, we opt for a combination of specific keywords.These search terms represent both our areas of focus: conversational agents (\"conversational agent\", \"chatbot\") and mental health (\"mental health\", \"depression\").Furthermore, we limit our search criteria to the paper between 2017 to 2022 to cover the most recent articles.We also apply the \"research article\" filter on ACM search, and \"Free Full Text or Full Text\" for PubMed search.Moreover, we manually add 3 papers recommended by the domain experts (Fitzpatrick et al., 2017;Laranjo et al., 2018;Montenegro et al., 2019).This results in 534 papers.", "filtered_refids": [[], ["b69", "b132", "b44", "b88"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1580, "num_references": 4}
{"corpusid_sectionid": "264490542-s8", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Target Demographic", "section": "Most of the papers (>65%) do not specify the target demographic of users for their CAs.The target demographic distribution is shown in Table 4.An advantage of the models proposed in these papers is that they could potentially offer support to a broad group of users irrespective of the underlying mental health condition.Papers without a target demographic and a target mental health category focus on proposing methods such as using generative language models for psychotherapy (Das et al., 2022a), or to address specific modules of the CAs such as leveraging reinforcement learning for response generation (Saha et al., 2022b).\n\nOn the other hand, 31% papers focus on one specific user group such as young individuals, students, women, older adults, etc, to give advanced assistance.Young individuals, including adolescents and teenagers, received the maximum attention (Rahman et al., 2021).Several papers also focus on the mental health care of women, for instance in prenatal and postpartum women (Green et al., 2019;Chung et al., 2021) and sexual abuse survivors (Maeng and Lee, 2022;Park and Lee, 2021).Papers targeting older adults are mainly designed for companionship and supporting isolated elders (Sidner et al., 2018;Razavi et al., 2022).", "filtered_refids": [["b114", "b29"], ["b79", "b21", null, "b123", "b110", "b106"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1251, "num_references": 8}
{"corpusid_sectionid": "264490542-s9", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Model Technique", "section": "Development of Large Language Models such as GPT-series (Radford et al., 2019;Brown et al., 2020) greatly enhanced the performance of generative models, which in turn made a significant impact on the development of CAs (Das et al., 2022b;Nie et al., 2022).However, as shown in Table 5, LLMs are yet to be utilized in the development of mental health CAs (as of the papers reviewed in this study), especially in medicine.No paper from PubMed in our final list dealt with generative models, with the primary focus being rule-based and retrieval-based CAs.\n\nRule-based models operate on predefined rules and patterns such as if-then statements or decision trees to match user inputs with predefined responses.The execution of Rule-based CAs can be straightforward and inexpensive, but developing and maintaining a comprehensive set of rules can be challenging.Retrieval-based models rely on a predefined database of responses to generate replies.They use techniques like keyword matching (Daley et al., 2020), similarity measures (Collins et al., 2022), or information retrieval (Morris et al., 2018)  and Eliza (Weizenbaum, 1966).\n\ntures.While they can often generate more diverse and contextually relevant responses compared to rule-based or retrieval-based models, they could suffer from hallucination and inaccuracies (Azaria and Mitchell, 2023).", "filtered_refids": [["b105", "b30", "b16", "b93"], ["b22", "b139", "b26", "b89"], ["b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1347, "num_references": 9}
{"corpusid_sectionid": "264490542-s10", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Outsourced Models", "section": "Building a CA model from scratch could be challenging for several reasons such as a lack of sufficient data, compute resources, or generalizability.Publicly available models and architectures have made building CAs accessible.Google Dialogflow (Google, 2021) and Rasa (Bocklisch et al., 2017) are the two most used outsourced platforms and frameworks.Alexa, DialoGPT (Zhang et al., 2019), GPT (2 and 3) (Radford et al., 2019;Brown et al., 2020) and X2AI (now called Cass) (Cass, 2023) are also frequently used for building CA models.A summary can be found in Table 6.Google Dialogflow is a conversational AI platform developed by Google that enables developers to build and deploy chatbots and virtual assistants across various platforms.Rasa is an opensource conversational AI framework that empowers developers to create and deploy contextual chatbots and virtual assistants with advanced natural language understanding capabilities.Alexa is a voice-controlled virtual assistant developed by Amazon.It enables users to interact with a wide range of devices and services using voice commands, offering capabilities such as playing music, answering questions, and providing personalized recommendations.DialoGPT is a large, pre-trained neural conversational response generation model that is trained on the GPT2 model with 147M conversation-like exchanges from Reddit.X2AI is 9 https://manychat.com the leading mental health AI assistant that supports over 30M individuals with easy access.", "filtered_refids": [["b18", "b105", "b13", "b142", "b16"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1490, "num_references": 5}
{"corpusid_sectionid": "264490542-s11", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Evaluation", "section": "Automatic: Mental health CAs are evaluated with various methods and metrics.Multiple factors, including user activity (total sessions, total time, days used, total word count), user utterance (sentiment analysis, LIWC (Pennebaker et al., 2015)), CA response quality (BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), lexical diversity, perplexity), and performance of CA's sub-modules (classification f1 score, negative log-likelihood) are measured and tested.We find that papers published in the CS domain focus more on technical evaluation, while the papers published in medicine are more interested in user data.\n\nHuman outcomes: Human evaluation using survey assessment is the most prevalent method to gauge mental health CAs' performance.Some survey instruments measure the pre-and post-study status of participants and evaluate the impact of the CA by comparing mental health (e.g.PHQ-9 (Kroenke et al., 2001), GAD-7 (Spitzer et al., 2006), BFI-10 (Rammstedt et al., 2013)) and mood scores (e.g.WHO-5 (Topp et al., 2015)), or collecting user feedback on CA models (usability, difficulty, appropriateness), or asking a group of individuals to annotate user logs or utterances to collect passive feedbacks (self-disclosure level, competence, motivational).", "filtered_refids": [[null, "b97", "b73"], ["b130", null, "b68"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1261, "num_references": 6}
{"corpusid_sectionid": "264490542-s14", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Technology Gap", "section": "CS and medical domains are also different in the technical aspects of the CA model.In the CS domain (ACL, AAAI, IEEE, ACM), 41 (of 73 papers) developed CA models, while 14 (out of 63) from the medical domain (PubMed) developed models.Among these papers, 8 from the CS domain are based on generative methods, but no paper in PubMed uses this technology.The NLP community is actively exploring the role of generative LLMs (e.g.GPT-4) in designing CAs including mental healthcare-related CAs (Das et al., 2022a;Saha et al., 2022b;Yan and Nakashole, 2021).With the advent of more sophisticated LLMs, fluency, repetitions and, ungrammatical formations are no longer concerns for dialogue generation.However, stochastic text generation coupled with black box architecture prevents wider adoption of these models in the health sector (Vaidyam et al., 2019).Unlike task-oriented dialogues, mental health domain CAs predominantly involve unconstrained conversation style for talk-therapy that can benefit from the advancements in LLMs (Abd-Alrazaq et al., 2021).\n\nPubMed papers rather focus on retrieval-based and rule-based methods, which are, arguably, previous-generation CA models as far as the technical complexity is concerned.This could be due to a variety of factors such as explainability, accuracy, and reliability which are crucial when dealing with patients.", "filtered_refids": [["b141", "b29", "b114", "b131", "b2"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1361, "num_references": 5}
{"corpusid_sectionid": "264490542-s15", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Response Quality vs Health Outcome", "section": "The difference in evaluation also reveals the varying focus across CS and medicine domains.From the CS domains, 30 (of 59 papers) applied automatic evaluation, which checks both model's performance (e.g.BLEU, ROUGE-L, perplexity) and participant's CA usage (total sessions, word count, interaction time).In contrast, only 13 out of 43 papers from PubMed used automatic evaluation, and none of them investigated the models' performance.\n\nThe difference is also spotted in human evaluation.40 (of 43 papers) from PubMed consist of human outcome evaluation, and they cover a wide range of questionnaires to determine participants' status (e.g.PHQ-9, GAD-7, WHO-5).The focus is on users' psychological well-being and evaluating the chatbot's suitability in the clinical setup (Martinengo et al., 2022).Although these papers do not test the CA model's performance through automatic evaluation, they asked for participants' ratings to oversee their model's quality (e.g.helpfulness, System Usability Scale (Brooke et al., 1996), WAI-SR (Munder et al., 2010)).\n\nAll 6 ACL papers that satisfied our search criteria, solely focus on dialogue quality (e.g.fluency, friendliness etc.) with no discussion on CA's effect on users' well-being through clinical measures such as PHQ-9.CAs that aim to be the first point of contact for users seeking mental health support, should have clinically validated mechanisms to monitor the well-being of their users (Pacheco-Lorenzo et al., 2021;Wilson and Marasoiu, 2022).Moreover, the mental health CAs we review are designed without any underlying theory for psychotherapy or behavior change that puts the utility of CAs providing emotional support to those suffering from mental health challenges in doubt.", "filtered_refids": [[], ["b15", "b90", "b84"], ["b140", "b96"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1735, "num_references": 5}
{"corpusid_sectionid": "264305746-s5", "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis", "date": "2023-10-18", "section_title": "Affect or Feeling", "section": "Author's disposition towards a specific theme (Birjali et al., 2021) 3-D polarity Framework with 3 dimensions of polarities: Subjective\\Objective, Positive\\Negative, Strength (Sebastiani and Esuli, 2006) Emoticons Emoticons as sentiment indicators (Lou et al., 2020) Object's orientation Measure of the attitude towards individual aspects of an entity (Mowlaei et al., 2020) Implicit Emotional tendencies implied by commonsense knowledge of the effect of concepts or events (Zhang and Liu, 2011) Human Annotation Sentiment ratings collected from experts or crowd-sourced data collection (Kenyon-Dean et al., 2018) Table 2: Frameworks of Sentiment and corresponding definitions in Sentiment Analysis (2002) experimented with using the semantic orientation of words to find whether product reviews are positive or negative.Readily available data in the form of product reviews on e-commerce websites influenced early SA works and firmly established it to almost exclusively mean opinion mining, with sentiment defined as: 'overall opinion towards the subject matter' (Pang et al., 2002).Following this, Read (2005) proposed the use of emoticons as a proxy for ground truth data to measure sentiment in text.They defined SA as the method to 'identify a piece of text according to its author's general feeling toward their subject, be it positive or negative.'This marked a stark deviation of SA from 'opinion mining.'This expansion of the meaning of sentiment can also be seen in the work of Wilson et al. (2005b) where they defined SA as 'the task of identifying positive and negative opinions, emotions, and evaluations '. Subsequently, Sebastiani and Esuli (2006) proposed that SA consists of three dimensions: subjective-objective polarity, positive-negative polarity, and strength of polarity.\n\nThe first use of SA as a sociotechnical system is marked by Go et al. (2009)'s approach to train a SA model using data from a social media platform, namely Twitter.While most prior work still treated SA as a method to extract an author's subjective or objective opinion regarding an entity or an object, Go et al. (2009) defined sentiment from the perspective of a general feeling or emotion in text.Their definition of sentiment as 'a personal positive or negative feeling or opinion', is a marked deviation that influenced much of the literature in SA.Maas et al. (2011)'s work recognized sentiment as a 'complex, multi-dimensional concept' and attempted to operationalize it through a vector representation.Similarly, Zhang and Liu (2011) defined sentiment as an 'emotional tendency im-plied by commonsense knowledge of the effect of concepts or events' to define an implicit form of sentiment.To quantify sentiment from a 'human perspective', Kenyon-Dean et al. (2018) used human annotation, as a methodology to define and measure sentiment, using crowd-sourced data.\n\nTable 2 tabulates the multifarious frameworks encountered in SA.Here we see that SA does not follow a well-defined comprehensive framework.With the evolution of the field, different researchers adapted SA in dissimilar ways while not making a clear distinction between concepts such as emotions, opinions, and attitudes.We posit that there is a need for a nuanced, socially informed, and theoretically motivated framework for sentiment in SA.\n\nTo understand sentiment from an interdisciplinary perspective and draw out an interdisciplinary framework, we examine its meaning from a sociological perspective.", "filtered_refids": [["b115", "b140", "b92", "b107", "b179", "b258", "b80", null, "b125", "b169"], ["b80", "b54", "b179", "b234"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3476, "num_references": 15}
{"corpusid_sectionid": "264305746-s6", "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis", "date": "2023-10-18", "section_title": "The Social Perception of Sentiment", "section": "A notable distinction exists between computational and psycho-linguistic perspectives on sentiment.In psychology, sentiment is often defined as \"socially constructed patterns of sensations, expressive gestures, and cultural meanings organized around a relationship to a social object, usually another person or group such as a family.\"(Gordon, 1981).While sentiment is most commonly categorized as positive, negative, or neutral in computational literature, it encompasses a broader spectrum, ranging from mild to intense (Taboada, 2016;Jo et al., 2017).Furthermore, sentiment (in psychology) is captured through physiological indicators, like facial expressions and heart rate variability (Wiebe et al., 2005;Plutchik, 2001).\n\nPsychological research widely recognizes that a simplistic positive-negative dichotomy is inade-quate for capturing the intricate range of human emotions (Hoffmann, 2018).This is evident in the distinction between seemingly negative emotions such as sadness and fear, which exhibit significant differences in their physiological and psychological effects (Plutchik, 2001).\n\nWe have seen that three primary and interrelated themes are commonly linked to sentiment: opinions, emotions/feelings, and subjectivity.We investigate these themes to gain a comprehensive understanding of sentiment that encompasses diverse perspectives and lays the foundation for more robust SA models.\n\nOpinions: From a psychological perspective, opinion is an individual's stance regarding an object or issue, formed after an evaluation through their own lens or perspective (Vaidis and Bran, 2019).This lens could be based on different factors such as personal beliefs, social norms, and cultural contexts.Liu (2012) also define an opinion a \"a subjective statement, view, attitude, emotion, or appraisal about an entity or an aspect of an entity from an opinion holder.\"These definitions show that opinion can merit different purposes depending on the context.\n\nFeelings/Emotions: Izard (2010) posit that the word emotion has both a descriptive definition i.e. based on its use in everyday life and a prescriptive definition i.e. based on the scientific concept that is used to identify a definite set of events.Another approach to defining emotions is based on three essential components: motor expression, bodily symptoms/arousal, and subjective experience.There is substantial agreement that motivational consequences and action tendencies associated with emotion are key aspects of emotion rather than just the level of arousal of the subject (Frijda et al., 1986;Frijda, 1987).\n\nSubjectivity: Banfield (2014) referred to sentences that take a character's psychological point of view as subjective, contrasted against sentences that narrate an event in a definite but yielding manner.Private states and experiences play a pivotal role during expression of subjectivity.Here private states could refer to intellectual factors, such as believing, wondering, knowing; or emotive factors, such as hating, being afraid; and perceptual ones, such as seeing or hearing something (Wiebe, 1994).Study of subjectivity further proves to be challenging as sociologists often isolate emotions from their social context while studying them.\n\nTerms like opinion, emotion, and subjectivity hold distinct meanings and are studied separately.Therefore, they are not synonymous with sentiment.Furthermore, when considering sentiment within a sociotechnical system, it is essential to be aware of the contextual nuances associated with the diverse definitions of sentiment derived from sociological, psychological, and linguistic backgrounds.Given the complex nature of sentiment, it is important to approach it with a nuanced perspective and operationalize it within a structured theoretical framework.Prior research suggests that achieving such nuanced understanding can be facilitated through engaging in dialogue with other fields such as psychology, and cognitive science (Head et al., 2015;Cambria et al., 2022).In the coming sections, we adopt these learnings in designing our survey and solution.", "filtered_refids": [["b57", "b78", "b149", "b235", "b117"], ["b65", "b117"], [], ["b155", null], ["b48", "b47"], ["b167"], ["b64", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 4095, "num_references": 14}
{"corpusid_sectionid": "264305746-s14", "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis", "date": "2023-10-18", "section_title": "Finance", "section": "Applications developed to comprehend the patterns and dynamics of financial management, creation, and investment analysis.\n\nTable 3: List of applications, defined through thematic analysis, their corresponding definitions, and frequency of papers categorized to the groups.\n\nanalysis (Vaismoradi et al., 2013) to uncover the various applications of SA.Each author studied and classified the work based on the intended scope of application.To ensure accuracy and prevent misclassification, this recursive process was employed.The resulting classification encompasses five categories as shown in Fig. 2 and Table 3 1 .Notably, the Health and Medicine domain emerged as the most prominent application area for SA where studies leverage SA to understand individual reactions in diverse medical scenarios (Rodrigues et al., 2016).Following closely, Government and Policy Making emerged as the second most prevalent category, where sentiment analysis plays a pivotal role in comprehending human behavior in governance solutions (Joyce and Deng, 2017).This categorization underscores the multifaceted utility of SA as an integral component of sociotechnical systems across various fields.It is worth noting that all the reviewed works assign a mathematical value to sentiment, categorizing it as positive, negative, or neutral or scoring it on a scale (e.g., -1 to +1).Most of the reviewed works lack clear definitions of sentiment or SA.Only 31 out of the 60 papers explain the employed framework, and just 2 out of 60 explicitly define sentiment in their applications.Only one takes an interdisciplinary perspective, defining sentiment in the context of finance for understanding market behavior (Kraaijeveld and De Smedt, 2020).Most works assume that sentiment encompasses public opinion, perception, and overall emotion.Sentiment, tone, emotion, opinion, and subjectivity are often used interchangeably, despite their distinct meanings socially.\n\nThe lack of precise sentiment definitions can result in misrepresented measurements.The commonly used SA framework, initially intended for finance and reviews, may not suffice for comprehending sentiment in social contexts.Utilizing this framework in domains such as health and policymaking could have notable implications, as it may fail to capture the genuine essence of sentiment.", "filtered_refids": [[], [], ["b79", "b128", "b197", "b156"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2327, "num_references": 4}
{"corpusid_sectionid": "264305746-s15", "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis", "date": "2023-10-18", "section_title": "Study 2: Sentiment Analysis as a Service", "section": "In this study, we will explore various published models and datasets of SA available for public consumption, examining their characteristics and limitations, and emphasizing the need for an interdisciplinary approach to their development.\n\nThe market has witnessed a rapid proliferation of AI as a Service (AIaaS) models that offer convenient \"plug-and-play\" AI services and tools (Lewicki et al., 2023) for public consumption across diverse interdisciplinary fields (S\u00e1nchez-Rada and Iglesias, 2019).We gathered SA datasets and popularly used models, that are publicly accessible for use as AIaaS, by leveraging existing repositories such as Sentibench (Ribeiro et al., 2016).We also conducted targeted searches using key-words such as 'sentiment analysis' and 'model' across peer-reviewed platforms such as the ACL Anthology, NeurIPS proceedings, AAAI, and ACM anthology.Following an extensive filtering process, we identified 43 well-cited 2 SA models and 19 datasets that are publicly available for utilization.\n\nWe now look at these models and datasets, using a critical lens as our intention is to examine them on interdisciplinary and sociotechnical awareness.We, therefore, examine them by formulating the following key questions:\n\n\u2022 Do these works mention the framework or definition of sentiment analysis and sentiment?\n\n\u2022 How do these works measure sentiment?\n\n\u2022 How accessible are these models for its use as an AIaaS solution?\n\nQ1-Analysis of Frameworks: Among the 62 collected models and datasets, we observed that merely 18 papers presented a definition of the SA framework employed, while just 2 works attempted to provide a definition for sentiment.Similarly, for datasets published, we see that 3 papers provided a definition of the SA framework while just 1 provided a definition of sentiment used.The most common framework used is of opinions.The deficiency in coherent structuring of sentiment and sentiment analysis definitions shows an absence of uniformity in terminology across the domain, as illustrated by the following examples:\n\n\"Sentiment analysis refers to the general method to extract subjectivity and polarity from the text.\"- (Taboada et al., 2011) \"Sentiment analysis or opinion mining analyzes people's opinions, sentiments, evaluations, attitudes, and emotions via the computational treatment of subjectivity in text.\" - (Hutto and Gilbert, 2014) \"Sentiment analysis is a branch of affective computing research that aims to classify text into either positive or negative, but sometimes also neutral.\" - (Ma et al., 2018) These quotes demonstrate the varied use of SA in each study, highlighting its focus on quantifying latent constructs such as 'emotion,' 'subjectivity,' and 'attitude,' which are not fully explained.The following two quotes demonstrate the framework used to define sentiment: 2 average citation count of 1130 \"the hedonic feelings of pleasantness; referred to in the psychological literature as \"affect\"\" - (Hannak et al., 2012) \"sentiment helps convey meaning and react to sentiments expressed towards them or others.\"- (Ma et al., 2018) These two examples serve to demonstrate the inadequacy of the information provided regarding the definition of sentiment.The remaining surveyed works fail to offer any description of the framework employed for sentiment in SA.\n\nQ2: Analysis of Metrics Our analysis of the 43 models and 19 datasets reveals the utilization of 11 distinct metrics to gauge the sentiment expressed in statements 3 .These metrics can be broadly categorized into two groups: sentiment categorization and sentiment regression.\n\nThe first group, sentiment categorization, focuses on classifying text into categories associated with positive or negative sentiment, or subjective and objective tone.However, these categories are not well-defined, as certain models further categorize sentiment based on emotions such as Joy, Sadness, Anger, Fear, Disgust, Surprise, (Mohammad, 2012) or Self-assurance, Attentiveness, Fatigue, Guilt, Fear, Sadness, Hostility, Joviality, Serenity, Surprise, and Shyness (Gon\u00e7alves et al., 2013) or between emotion categories of Valence, Arousal, and Dominance (Warriner et al., 2013).We see no synchronization in the categories used.\n\nIn contrast, the second group, sentiment regression, focuses on evaluating a numerical value for a sentence, which is subsequently categorized as positive, neutral, or negative.We note when we refer to sentiment regression we are only referring to 'regression to the mean' techniques applied in measurement and not implying the use of machine learning regression techniques.Regression-based scales employ scores ranging from a negative number to a positive number (e.g., -1 to +1) to quantify the intensity and sentiment of the sentence.\n\nWithout standardized measures, it becomes challenging to compare results, establish a common understanding of sentiment, and benchmark performance.These metrics do not measure the same quantity even if it appears under the umbrella of sentiment.Standardizing sentiment measures would address these issues by promoting consistency, enhancing interpretation, and improving in-tegration with social applications.", "filtered_refids": [[], ["b251", "b86"], [], [], [], [], [], ["b202", "b61", "b213", "b219"], [], ["b207", "b210", null], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 5186, "num_references": 9}
{"corpusid_sectionid": "264305746-s17", "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis", "date": "2023-10-18", "section_title": "Study 3: The Bias and Harm of Sentiment Analysis Applications", "section": "In the prior sections, we showed that not only is there a general lack of effort in defining sentiment in SA models, but SA contains multiple frameworks that can hinder collaboration within the field.\n\nAdditionally, such work tend to not disclose details on how they are developed.Next, we explore the issues that can arise due to the lack of explanation in creating solutions using an interdisciplinary lens.\n\nDue to limited and restricted data and the subjective nature of sentiment, the training data used to train SA models are not representative of all perspectives (Kiritchenko and Mohammad, 2018;Gupta et al., 2023) and thus result in biases that can be harmful to real-world applications.We demonstrate this with an experiment on Textblob, a SA model.Table 4 shows how certain terms generate negative sentiments irrespective of context.However, it is difficult to comprehend what the negative scores mean in a social setting where they can be interpreted as toxic or hateful (Venkit et al., 2023;Kiritchenko and Mohammad, 2018).Thus, the use of sentiment analysis models can lead to discrimination against certain groups (Huang et al., 2020;Shen et al., 2018).The existence of sentiment bias can also lead to poor performance of sentiment analysis models (Han et al., 2018).\n\nSA models are shown to perform differently for different age groups (D\u00edaz et al., 2018).They show that SA models are more likely to be positively biased towards 'young' adjectives than 'old' adjectives.Hutchinson et al. (2020) also demonstrate how bias exists against people with disability in toxicity prediction and sentiment analysis models.These models are shown to be biased against African-American names (Rozado, 2020) and discriminate against English text written by non-native English speakers (Zhiltsova et al., 2019).Hube et al. (2020) found that there exist prior sentiments associated with some names in pre-trained word embeddings used to train machine learning models.Such biased machine learning models can have harmful implications when used in real-world settings (Rudin, 2019;Bender et al., 2021;Schwartz et al., 2021).\n\nThe works by Stark and Hoey (2021) & Mohammad (2022) argue that the complexity of human emotion and the limits of technical computation raise serious social, political, and ethical considerations that merit further discussion in AI ethics.The field of AI has not caught up well with the complexities of human behavior.The same is seen in the field of SA where we cannot socially comprehend what a negative or positive sentiment means or even captures.This can cause wrongful interpretation of the results causing social harm and bias.Dev et al. (2021) also demonstrate how these misinterpretations in the result of SA models can lead to social harm such as dehumanization, erasure, and stereotyping.Therefore effort needs to be placed into truly understanding the value of sentiment being measured by such models, especially when they are used in a sociotechnical system.Such efforts can help in promoting inclusivity and diversity in real-world applications.", "filtered_refids": [[], [], ["b281", "b67", "b275", "b59", "b271", "b268"], ["b15", "b265", "b269", "b270", null, "b139", "b69", "b132"], ["b145", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3083, "num_references": 16}
{"corpusid_sectionid": "264305746-s23", "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis", "date": "2023-10-18", "section_title": "Term Definition Framework", "section": "References sentiment affective state or feeling associated with a particular object or event (Hoffmann, 2018) opinion subjective statement, view, attitude, emotion, or appraisal about an entity or an aspect of an entity from an opinion holder (Liu, 2012) emotion/feelings By \"descriptive definition,\" we mean a definition of the word emotion as it is used in everyday life.By \"prescriptive definition,\" we mean a definition of the scientific concept that is used to pick out the set of events that a scientific theory of emotion purports to explain.\n\n(Izard, 2010) subjectivity subjectivity analysis deals with the detection of \"private states\" -a term that encloses sentiment, opinions, emotions, evaluations, beliefs and speculations.(Wiebe, 1994) Table 5: Examples of a few definitions of different themes concerning sentiment from different fields to demonstrate the difference in framework between these terms that are synonymously used in the field of SA in NLP.", "filtered_refids": [[null, "b65"], ["b167"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 968, "num_references": 3}
{"corpusid_sectionid": "258832362-s3", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Word Representations", "section": "A well-known challenge in NLP is creating continuous dense vector representations of words in high-dimensional spaces to capture their semantic and syntactic meaning. The most widely used algorithm for creating word embeddings is Word2Vec (Mikolov et al., 2013). Traditional approaches to representing words before Word2Vec, like one-hot encoding or bag-ofwords, have a number of drawbacks: They require a lot of memory to hold sparse vectors and fail to capture the links between words or their meaning. By using a neural network to learn word embeddings, Word2Vec solved these issues. The model trains neural networks using a large corpus of text as input to predict the likelihood of a word given its context or vice versa. The weights of the network are changed during training to reduce the discrepancy between the expected and actual probabilities. The network weights are employed as the word embeddings after training is finished. It has been widely used and inspired other models such as GloVe (Pennington et al., 2014) and fastText (Joulin et al., 2016).", "filtered_refids": [["b15", "b8", "b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1064, "num_references": 3}
{"corpusid_sectionid": "258832362-s7", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Natural Language Inference", "section": "Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence). The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013). NLI serves as a proxy for evaluating natural language understanding. According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.\n\nIn \u00a7 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019). There are two noteworthy components to this model. First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model. The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019). This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings. In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks. Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.\n\nMeanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks. Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings. They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder. This is one of the first studies that shows the utility of generative models for obtaining sentence representations.", "filtered_refids": [[null], ["b22", "b4"], ["b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2248, "num_references": 4}
{"corpusid_sectionid": "258832362-s11", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Surface Level", "section": "To create a sentence that carries the same meaning as another, one can modify the words or characters in a way that retains the sentence's semantic value. Recent research Wu et al., 2022d) suggests certain transformations that preserve the semantic meaning.  propose randomly flipping the case of some tokens, while  mask spans of tokens to get positive instances and Wu et al. (2022d) suggest to repeat certain words or subwords. Representations generated by transformer networks are biased towards the frequency of tokens, the case of words and subwords, and the length of the sentence (Wu et al., 2022d). For example, researchers found that avoiding to use high-frequency tokens can result in better sentence representations . These transformation help in overcoming such biases.\n\nHowever altering the surface characteristics of sentences can lead to models relying on shortcuts rather than learning the semantics of the sentences (Du et al., 2021). To address this issue, Wu et al. (2022a) propose the use of multiple augmentation strategies rather than a single transformation. They use shuffling, repeating, and dropping words as transformation strategies to improve model robustness. Additionally, they implement mechanisms to enhance learning from multiple positive examples.", "filtered_refids": [["b37"], [null, "b34"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1283, "num_references": 3}
{"corpusid_sectionid": "258832362-s12", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Model Level", "section": "Another approach to generating positive examples is by leveraging the distinctive characteristics of the backbone model utilized in contrastive learning. These characteristics might be the architectural choices or using representation from certain components of the model.\n\nDropout is a regularization technique used in deep learning to prevent overfitting of a model. During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times. These different representations can be used as positive examples for sentence representations. Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy. Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).\n\nSpecific components of language models can be trained to generate semantically similar representations. One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model. In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations. a", "filtered_refids": [[], ["b11", null], ["b30", "b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1420, "num_references": 4}
{"corpusid_sectionid": "258832362-s14", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Alternative Methods", "section": "Researchers have explored various other methods for obtaining positive samples for unsupervised sentence representations. One approach involves using other spans of text that either overlap with the given span or come from the same document (Giorgi et al., 2021). Another strategy is to use related entities as positives instead of text spans (Nishikawa et al., 2022). This approach is particularly useful for cross-lingual sentence representation learning, as information about the same entity is often available in different languages.\n\nOther approaches use large language models that are capable of performing various tasks from observing few examples, even for tasks that they have never encountered before. Generally, larger models with more parameters tend to perform better (Brown et al., 2020). Moreover, these models can be directed to perform tasks by providing them with specific instructions-a technique known as \"prompting\". This prompting paradigm has become a popular approach in natural language processing. Researchers have used prompts to obtain better sentence representations, as demonstrated in studies such as , which employs the \"[X] means [MASK]\" prompt to extract sentence representations from the representation of the \"[MASK]\" token in a sentence. Another study by (Zeng et al., 2022) combines prompt-derived sentence representations with contrastive learning to improve the quality of the representations.", "filtered_refids": [[null, "b19"], [null, "b38"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1433, "num_references": 4}
{"corpusid_sectionid": "258832362-s15", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Alternative Loss and Objectives", "section": "In \u00a7 2, we discuss Contrastive loss, which is widely used in machine learning. However, this loss suffers from several limitations: for instance it only considers binary relationships between instances, lacks a mechanism to incorporate \"hard negatives\". To overcome these drawbacks, researchers have explored supplementary losses that can be used in conjunction with the Contrastive loss. Moreover, they have proposed alterations to the loss function that make it more effective for learning, as well as alternative loss functions for contrastive learning of sentence representations. This section provides an overview of these approaches.\n\nTo improve the learning of sentence representations using the Contrastive loss, researchers have introduced several supplementary losses. These include: (1) the hinge loss proposed by Jiang et al. (2022b), which enhances discrimination between positive and negative pairs; (2) losses suggested by Wu et al. (2022b) for reconstructing the original sentence from its representation to better capture sentence semantics; (3) a loss developed by Chuang et al. (2022) to identify masked words and improve sensitivity to meaningless semantic transformations; and (4) a loss proposed by Chen et al. (2022a) to minimize redundant information from transformations by minimizing information entropy.\n\nBesides such supplementary losses, some studies have suggested modifications to the original contrastive loss for better effectiveness. For instance, Wu et al. (2022c) have proposed an additional term that incorporates random noise from a Gaussian distribution as negative instances. In the same vein, Zhang et al. (2022d) have introduced two lossesthe angular loss and margin-based triplet loss-to better address the intricacies of similarity between pairs of examples.\n\nHowever, recent research has moved away from using the contrastive loss and has employed different loss functions to learn sentence representations. For instance, Zhang et al. (2020) maximize the mutual information between a local and a global representation of a sentence using .  identify an alternative sub-manifold within the sentence representation space that considers the geometric structure of sentences to compute semantic similarity.", "filtered_refids": [[], [null, "b7", "b35"], ["b44", "b36"], ["b42"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2247, "num_references": 6}
{"corpusid_sectionid": "258832362-s16", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Better Negative Sampling", "section": "The efficacy of contrastive learning hinges on the quality of negative samples used during training. While most methods prioritize selecting positive samples that bear similarity to the query text, it's equally crucial to include hard negatives that are dissimilar to the query text and pose a challenge for the model to classify. Failure to do so leads to a gradual diminution of the loss gradients, impeding the learning of useful representations (Zhang et al., 2022c). Additionally, using an adequate number of negative samples is also imperative for effective learning (Cao et al., 2022).\n\nGiven the importance of incorporating hard negatives, several innovative strategies have emerged. For instance, researchers have found that mixednegatives-a combination of representations of a positive and a randomly chosen negative-serve as an excellent hard negative representation (Zhang et al., 2022c). Similarly, Zhou et al. (2022) have leveraged noise from a uniform Gaussian distribution to foster uniformity in the learned representation space-a metric to assess learned sentence representation. To further refine their approach, they also implemented techniques to identify and penalize false negative instances, where similarity scores exceed a predetermined threshold with the positive instance.", "filtered_refids": [["b43", "b38"], ["b43", "b45"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1300, "num_references": 4}
{"corpusid_sectionid": "258832362-s19", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Computer Vision Inspired", "section": "The effectiveness of computer vision techniques in NLP is often debated. Some researchers are interested in exploring their transferability to NLP. For example, momentum encoder , a queue-based approach, has been proposed to improve the stability of training during contrastive learning. This approach involves using a queue of representations from previous batches as negatives for the current batch, effectively decoupling the batch size from the learning process. Several researchers have incorporated the momentum encoder technique for learning sentence representations, reporting improved performance (Cao et al., 2022;Wu et al., 2022a,d;Tan et al., 2022).\n\nBootstrap Your Own Latent (BYOL) (Grill et al., 2020 Figure 2) and (e) AVERAGE shows the average STS score.\n\nnetwork to predict a set of \"target\" representations of an input data point, given another \"online\" representation of the same data point. The network is optimized using a contrastive loss function that incentivizes the online representation to be similar to the target representation. One advantage of BYOL is that BYOL completely removes the requirement of negative samples and instead uses the augmented versions of the same data point as positive samples. This approach has been successfully applied to natural language processing by Zhang et al. (2021) for learning sentence representations.", "filtered_refids": [[null, "b38", "b25"], ["b2"], ["b41"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1368, "num_references": 5}
{"corpusid_sectionid": "258832362-s20", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Challenges for Sentence Representations", "section": "Practical Applications and the rise of Tools: Sentence representations are commonly employed for sentence retrieval in practical applications, as evidenced by the increasing number of benchmarks (Thakur et al., 2021b) However, their utility extends beyond retrieval, as demonstrated by recent work (Schuster et al., 2022) that leverages sentence representations to make practical enhancements for processing long documents. Specifically, the authors propose novel methods for identifying documents that share a similar stance on a topic, as well as for isolating documents that diverge from the consensus.\n\nWith the increasing use of sentence representations in practical applications such as retrieval, there is a growing need for efficient storage and indexing solutions that enable fast retrieval. These solutions are commonly referred to as vector databases and include popular options such as Pinecone, Milvus, and Faiss, among others 3 . Furthermore, these vector databases can be integrated with other frameworks that facilitate the development of applications using large language models. Langchain and ChatGPT Retrieval Plugin are examples of such framewor Adapting to different domains: Research has shown that sentence representations learned in one domain may not accurately capture the semantic meaning of sentences in another domain (Jiang et al., 2022b;Thakur et al., 2021a). For instance, using sentence representations from one domain for text retrieval in another domain often results in poor performance. Although some solutions have been proposed in the literature, such as generating queries using a pretrained T5 model on a paragraph from the target domain or using a pretrained crossencoder to label the query and paragraph, or using a denoising objective (Wang et al., 2021). Training models that work well across domains remains a challenging task Cross-lingual sentence representations : Creating sentence representations that can be used across languages, especially those with limited annotated data, poses a significant challenge. New solutions for cross-lingual retrieval are being developed and deployed for real-world use cases. 4 Many scholarly works (Nishikawa et al., 2022) have addressed cross-lingual sentence representation learning in recent times. Given that there are thousands of languages spoken worldwide, developing models that can operate across languages is increasingly important.", "filtered_refids": [[null], ["b26", "b19", "b7", "b29"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2428, "num_references": 5}
{"corpusid_sectionid": "3254638-s1", "title": "A Critical Survey of the Methodology for IE Evaluation", "date": 2004, "section_title": "IE Evaluation Methodology", "section": "The MUC conferences can be considered the starting point of the IE evaluation methodology as currently defined. The MUC participants borrowed the Information Retrieval concepts of precision and recall for scoring filled templates. Given a system response and an answer key prepared by a human, the system's precision was defined as the number of slots it filled correctly, divided by the number of fills it attempted. Recall was defined as the number of slots it filled correctly, divided by the number of possible correct fills, taken from the human-prepared key. All slots were given the same weight. F-measure, a weighted combination of precision and recall, was also introduced to provide a single figure to compare different systems' performances.\n\nApart from the definition of precise evaluation measures, the MUC conferences made other important contributions to the IE field: the availability of large amount of annotated data (which have made possible the development of Machine Learning based approaches), along with the evaluation software (i.e., the MUC scorer (Douthat, 1998)), the emphasis on domain-independence and portability, and the identification of a number of different tasks which can be evaluated separately (Hirschman, 1998).\n\nIt should be noticed that MUC evaluation concentrated mainly on IE from relatively unrestricted text, i.e. newswire articles. In independent efforts, other researchers developed and made available annotated corpora developed from somewhat more constrained texts. Califf compiled and annotated a set of 300 job postings from the Internet (Califf, 1998), and Freitag compiled corpora of seminar announcements and university web pages, as well as a corporate acquisitions corpus from newswire texts (Freitag, 1998). Several of these corpora are available from the RISE repository (RISE, 1998) where a number of tagged corpora have been made available by researchers in Machine Learning for IE. Freitag (1998) uses the term Information Extraction in a more restricted sense than MUC. In the Seminar Announcement collection, the templates are simple and include slots for the seminar speaker, location, start time, and end time. This is in strong contrast with what happened in MUC where templates might be nested (i.e., the slot of a template may take another template as its value), or there might be several templates from which to choose, depending on the type of document encountered. In addition, MUC domains include irrelevant documents which a correctly behaving extraction system must discard. A template slot may be filled with a lower-level template, a set of strings from the text, a single string, or an arbitrary categorical value that depends on the text in some way (a so-called \"set fill\"). Califf (1988) takes an approach that is somewhat inbetween Freitag's approach and more complex MUC extraction tasks. All of the documents are relevant to the task, and the assumption is that there is precisely one template per document, but that many of the slots in the template can have multiple fillers.\n\nAlthough the tasks to be accomplished are different, the methodology adopted by (Freitag, 1998) and (Califf, 1998) is similar to the one used in the MUC competition: precision, recall, and F-measure are employed as measures of the performances of the systems.", "filtered_refids": [[], ["b12", "b8"], [null, "b9", "b3"], ["b9", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3322, "num_references": 7}
{"corpusid_sectionid": "3254638-s2", "title": "A Critical Survey of the Methodology for IE Evaluation", "date": 2004, "section_title": "Problematic Issues in IE Evaluation", "section": "In Section 2. we have summarized the current status of the methodology adopted in IE. However, the definition of an evaluation methodology and the availability of standard annotated corpora do not guarantee that the experiments performed with different approaches and algorithms proposed in the literature can be reliably compared. Some of the problems are common to other NLP tasks (e.g., see (Daelemans and Hoste, 2002)): the difficulty of exactly identifying the effects on performances of the data used (the sample selection and the sample size), of the information sources used (the features selected), and of the algorithm parameter settings.\n\nOne of the most relevant issues is that of the exact split between training set and test set, considering both the numerical proportions between the two sets (e.g., a 50/50 vs. a 80/20 split) and the procedure adopted to partition the documents (e.g., n repeated random splits vs. n-fold crossvalidation).\n\nFurthermore, the question of how to formalize the learning-curve sampling method and its associated costbenefit trade-off may cloud comparison further. For example, the following two approaches have been used: (1) For each point on the learning curve, train on some fraction of the available data and test on the remaining fraction; or (2) Hold out some fixed test set to be used for all points on the learning curve. The second approach is generally preferable: with the first procedure, points on the \"high\" end of the learning curve will have a larger variance than points on the \"low\" end.\n\nAnother important issue concerns the features used by the algorithm and their contribution to the performances of the algorithm. In IE, for instance, it would be relevant to extensively investigate the effectiveness of the use of simple orthographic features with respect to the use of more complex linguistic features such as PoS tags or semantic labels extracted from gazetteers (Ciravegna, 2001b).\n\nApart from those problematic issues mentioned above, there are some others that are specific to IE evaluation. A first issue concerns how to deal with issues related to tokenization, which is often considered something obvious and non problematic but it is not so and can affect the performance of the IE algorithms.\n\nA second issue is related to how to evaluate an extracted fragment -e.g., if an extra comma is extracted should it count as correct, partial or wrong? This issue is related to the question of how relevant is the exact identification of the boundaries of the extracted items. (Freitag, 1998) proposes three different criteria for matching reference instances and extracted instances:\n\nExact The predicted instance matches exactly an actual instance.", "filtered_refids": [["b6"], [], [], ["b5"], [], ["b9"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2720, "num_references": 3}
{"corpusid_sectionid": "3254638-s3", "title": "A Critical Survey of the Methodology for IE Evaluation", "date": 2004, "section_title": "Contains", "section": "The predicted instance strictly contains an actual instance, and at most k neighboring tokens.\n\nOverlap The predicted instance overlaps an actual instance.\n\nEach of these criteria can be useful, depending on the situation, and it can be interesting to observe how performance varies with changing criteria. (De Sitter and Daelemans, 2003) mention such criteria and present the results of their algorithm for all of them.\n\nA third issue concerns which software has been used for the evaluation. The only publicly available tool for such aim is the MUC scorer. Usually IE researchers have implemented their own scorer, relying on a number of implicit assumptions that have a strong influence on performance's evaluation.\n\nWhen multiple fillers are possible for a single slot, there is an additional ambiguity -usually glossed over in papers -that can influence performance. For example, (Califf and Mooney, 2003) remark that there are differences in counting between RAPIER (Califf, 1998), SRV (Freitag, 1998), and WHISK (Soderland, 1999). In his test on Job Postings (Soderland, 1999) does not eliminate duplicate values. When applied to Seminar Announcements SRV and RAPIER behave differently: SRV assumes only one possible answer per slot, while RAPIER makes no such assumption since it allows for the possibility of needing to extract multiple independent strings.\n\nDe Sitter and Daelemans (2003) also discuss this question and claim that in such cases there are two different ways of evaluating performance in extracting slot fillers: to find all occurrences (AO) of an entity (e.g. every mention of the job title in the posting) or only one occurrence for each template slot (one best per document, OBD). The choice of one alternative over the other may have an impact on the performance of the algorithm. (De Sitter and Daelemans, 2003) provide results for the two alternative ways of evaluating performances. This issue is often left underspecified in papers and, given the lack of a common software for evaluation, this further amplifies the uncertainty about the reported results.\n\nNote that there are actually three ways to count:\n\none answer per slot (where \"2pm\" and \"2:00\" are considered one correct answer)\n\none answer per occurrence in the document (each individual appearance of a string to be extracted in the document where two separate occurrences of \"2pm\" would be counted separately)\n\none answer per different string (where two separate occurrences of \"2pm\" are considered one answer, but \"2:00\" is yet another answer)\n\nFreitag takes the first approach, Soderland takes the second, and Califf takes the third.\n\nTo summarize, an information extraction task should specify all of the following: 1. A set of fields to extract.\n\n2. The legal numbers of fillers for each field, such as \"exactly one value\", \"zero or one values\", \"zero or more values\", or \"one or more values\". For example, in Seminar Announcements, the fields stime, etime and location are \"0-1\", speaker is \"1+\"; for Job Postings, title is \"0-1 or 0+\", required programming languages is \"0+\", etc. Thus, in the following seminar announcement:\n\nSpeakers will be Joel S. Birnbaum and Mary E.S. Loomis.\n\nif the task specifies that there should be one or more speaker, then to be 100% correct the algorithm must extract both names, while if the task specifies that zero or more speakers are allowed, then extracting either name would result in 100% correct performance.\n\n3. The possibility of multiple varying occurrences of any particular filler. For example, a seminar announcement with 2 speakers might refer to them each twice, but slightly differently: ", "filtered_refids": [[], [], ["b7"], [], ["b14", "b9", "b2", "b3"], ["b7"], [], [], [], [], [], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3637, "num_references": 6}
{"corpusid_sectionid": "260063224-s7", "title": "How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques", "date": 2023, "section_title": "Complete Training", "section": "Betti et al. (2020) propose a text GAN composed of one generator and two discriminators. The generator is a Relational Memory with self-attention (Santoro et al., 2018) with the objective to generate text consistent with the specified control attribute. The syntax discriminator distinguishes between real and generated sentences, while the semantic discriminator assesses whether the generated sentence expresses the control attribute, e.g. positive sentiment. To solve the well-known problem of differentiation in GANs applied to text, the Gumbel-softmax trick (Jang et al., 2016) is applied. This approach enables control only for one attribute at a time and it has been evaluated on sentiment and topic control.\n\nIn order to enable multi-attribute control, Qiao et al. (2020) propose a Sentiment-Controllable topic-to-essay generator that deploys a Conditional Variational Auto-Encoder in adversarial training. The model simultaneously controls the topics of the essay and the sentiment of each sentence composing the essay. The topic control is achieved using a Topic Graph Attention, which includes a topic knowledge graph in the generation process. Sentiment control is achieved by injecting the sentiment representation both in the encoder and the decoder.\n\nIn a different direction, Xie et al. (2022) propose a psychology-guided story generation method that controls storytelling as the protagonist's psychological state changes. This technique enables multi-attribute control considering the protagonist of the story (Character), their chain of emotions (Emotion), and chain of needs (Need) representing the evolution of the psychological state of the protagonist. The model is an encoder-decoder architecture with the addition of psychology controllers designed to integrate the local and global psychological state into the story context representation.", "filtered_refids": [["b13"], ["b8"], ["b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1865, "num_references": 3}
{"corpusid_sectionid": "260063224-s10", "title": "How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques", "date": 2023, "section_title": "Modification of Token Distribution", "section": "Dathathri et al. (2019) propose a Plug and Play Language Model (PPLM) which uses external attribute classifiers to guide text generation without requiring any training of the PLM. The PLM is used to obtain the next token distribution, which is fed to external classifiers, called Attribute Models, to assess whether the token correctly expresses the desired attributes. The internal latent representations of the LM are updated with a backward pass using the gradients of the attribute models to increase the likelihood of the desired attributes. Finally, the next token distribution is recomputed taking into account the updated latent representations. This model allows control of multiple attributes at a time, such as sentiment and topic.\n\nInspired by this work, Madotto et al. (2020) propose a variation of PPLMs in which the backward pass is executed n times depending on the desired intensity of the control attribute. Furthermore, they add Residual Adapters (Houlsby et al., 2019) on top of each transformer layer to steer the PLM output distribution without changing its parameters.\n\nGoswamy et al. (2020) propose a different variation of PPLMs based on GPT-2, in which a modified loss is considered to take into account the intensity of the controlled sentiment. Furthermore, instead of considering only positive/negative sentiment, control over 8 emotion categories is enabled.\n\nStarting from PPLMs, Gu et al. (2022a) observe that using a controller alone leads to the trade-off problem, i.e. the controller used to modify the token distribution only focuses on how to make the prefix related to the desired attribute without taking into account the original distribution of the LM. In this way, the controller takes over the LM's control for the next token distribution. In order to alleviate  this problem, they propose a weighted decoding method that adds a regulator module that permits fine-grained adjustment of a bias signal from the controller. At every step, the regulator detects differences between the PLM distribution and the target attribute and it determines whether to suppress or amplify the bias signal. This method is model agnostic and has been evaluated with sentiment, topic, and toxicity attributes. The last two methods propose sampling procedures that can be applied to any LM. Landsman et al. (2022) propose to modify beam search by reweighing the token candidate likelihoods to control different attributes. Diverse beam search (Vijayakumar et al., 2016) is used to decode k candidates, which are then scored using an attribute model. The obtained scores are used to reweigh the original likelihoods to produce a reweighed candidate distribution that considers both fluency and attribute characteristics. The resulting distribution is used to sample the next token.\n\nLastly, Kumar et al. (2022) propose a sampling method combining LM log-likelihoods with arbitrary constraints in a single energy function generating samples in a non-autoregressive manner. The idea is to use a PLM without changing its distribution but sampling from it considering different constraints, i.e. control attributes. The constraints are discriminative classifiers trained from scratch or fine-tuned. This method allows multi-attribute control (sentiment and toxicity).", "filtered_refids": [[], [], [], [null, "b18"], ["b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3285, "num_references": 3}
{"corpusid_sectionid": "260063224-s11", "title": "How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques", "date": 2023, "section_title": "Hybrid", "section": "Hybrid techniques combine two or more Control Implementation techniques. One possibility is to combine Complete Training and Fine-Tuning, for example, designing a model composed of different modules in which some modules are trained from scratch and some are fine-tuned models. In this context, Tian et al. (2022) propose a conversa-tion model that generates empathetic responses and guides the mood of the conversation in a positive direction while acknowledging the user's emotion. The idea is to extract the sentiment from the conversation context using a fine-tuned sentiment evaluator and use both the context and the extracted sentiment to steer the generation of the next response by generating a responding strategy that will be used by the Conditional Conversation model to generate the final response. The proposed method enables only single-attribute control (of sentiment).\n\nAnother way to enable controllability using a hybrid technique is to combine Fine-Tuning and Modification of Token Distribution.  propose a technique to control Story Generation by fine-tuning an encoder that learns the representation of new special tokens identifying the control attributes, thus allowing the model to properly include this information in the generation process. The next token distribution is obtained by combining the decoder distribution and the attention distribution, which allows the model to copy important information from the specified control attributes. The model allows fine-grained control taking into account the characters of the story with their actions and emotions.\n\nIn contrast to  who learn the representation of special tokens during fine-tuning, Liu et al. (2021) propose to modify an LM's token distribution including two fine-tuned versions of the PLM: an expert, focused on the desired attribute, and an anti-expert, focused on the opposite of the desired attribute. The next token distribution is obtained by subtracting the anti-expert distribution from the expert one and combining the result with the distribution of the frozen PLM to maintain fluency. This method enables the control only of one control attribute at a time and it has been tested on sentiment and toxicity attributes.\n\nSimilarly, Krause et al. (2021) propose to con-  trast the desired control attribute and its opposite. Instead of fine-tuning specialised LMs for each attribute, GPT-2 is fine-tuned with control codes to obtain a Class-Conditional LM (CCLM). At each time step, the generation is guided by computing classification probabilities for all possible next tokens via the Bayes rule by normalizing two classconditional distributions: conditioned on the desired attribute and conditioned on the undesired attribute. Like the previous method, it allows the control of one attribute at a time and has been evaluated using sentiment, topic, and toxicity attributes. Liu et al. (2022) also use a CCLM which is finetuned using an external discriminator to generate texts with the desired attributes, supporting multiattribute control. The token distribution is modified based on a contrastive generator that learns effective representations by bringing together positive samples, i.e. samples with desired attributes, and separating negative samples, i.e. samples without desired attributes. The obtained distribution is combined with the distribution of a PLM to maintain the fluency of the generated text. The generated text is fed to the external discriminator to assess whether it contains the desired attributes or not. The model has been tested on the joint control of sentiment and topic.  explore the contrast between desired and undesired attributes proposing a fine-tuned LM incorporating the attribute knowledge of a discriminator, similarly to Liu et al. (2022), to optimize continuous virtual tokens called control-prompts. The learned control-prompts are used as prefixes to steer a fixed conditional LM to generate attribute-specific texts. The LM is finetuned using (i) likelihood training, encouraging the LM to generate tokens with higher probability as scored by the discriminator assessing the desired attribute, and (ii) unlikelihood training, keeping the generated tokens away from lower-probability candidates.", "filtered_refids": [["b16"], [], ["b21"], [null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4241, "num_references": 4}
{"corpusid_sectionid": "260063224-s14", "title": "How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques", "date": 2023, "section_title": "Generality and parameter efficiency", "section": "In terms of generality, some of the techniques we have discussed are highly specialised and require many modifications to adapt them to include more or new control attributes. For example, the technique proposed by Xie et al. (2022) is specifically designed to control emotions and needs representing the psychological state of the story's protagonist. Other techniques require the training or finetuning of specific models for each control attribute (Liu et al., 2021).\n\nIn terms of efficiency, we see some techniques that require the storage and usage of multiple LMs (Liu et al., 2021). On the other hand, many techniques are model agnostic, so they can be applied to any PLM allowing reuse of existing models (Landsman et al., 2022 and. In Table 4, we compare the studied techniques in terms of the number of trainable parameters. In Model Agnostic techniques, we consider the number of parameters considering the models used in the reference paper. Unfortunately, it is not possible for all techniques to correctly identify the number of parameters. In general, the modification of token  ", "filtered_refids": [["b21", "b20"], ["b21", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1094, "num_references": 4}
{"corpusid_sectionid": "264832783-s1", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Background and Notation", "section": "We refine the definition in Wei et al. (2022b), and specify the emergent abilities as the abilities that LLMs can leverage to achieve satisfactory results2 across diverse tasks, with only a few examples or chain-of-thought demonstrations, and without the need for re-training.\n\nFormally, let's define some key variables.D \u2208 T train represents a subset of demonstrations selected from the training set.Q \u2208 T test is the query taken from the test set, and Y stands for the label associated with each query.M represents the LLM with its parameters frozen as \u0398, and F denotes the evaluation metric function.For example, F is typically used to measure accuracy or F1 score in classification tasks, such as sentiment classification, and is often used to represent metrics like ROUGE (Lin, 2004) or BLEU (Papineni et al., 2002) in text generation tasks, such as summarization and machine translation.The concept of emergent ability can be formally expressed using the equation:\n\nwhere M is usually considered to exhibit emergent abilities if the computed value using F exceeds a pre-defined threshold.Under this definition, we can group similar concepts within the few-shot prompting paradigm.CoT can be viewed as a variant of ICL, with the primary distinction being the format of the demonstration.Specifically, ICL demonstrations typically rely on a standard prompt with optional demonstration examples, whereas CoT prompting incorporates an additional textual reasoning process.\n\nAccording to our definition, we organize existing literature (summarized in Table 1) on interpreting emergent capabilities into macro and micro perspectives.Researchers in the macro category focus on factors such as overall loss or the model architecture.Their goal is to establish a connection between the outcome of F and the behavior of M. Conversely, those in the micro category primarily centre their attention on the relationship between the outcome of F and the characteristics of the demonstration set D.", "filtered_refids": [["b62"], ["b43", "b31"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1988, "num_references": 3}
{"corpusid_sectionid": "264832783-s3", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Mechanistic Interpretability", "section": "With the goal of reverse-engineering components of frontier models into more understandable algorithms, Elhage et al. (2021) developed a mathematical framework for decomposing operations within transformers (Vaswani et al., 2017).They initially introduced the concept of \"induction heads\" in a two-layer attention-only model to explain the functioning of ICL within transformers with Circuits (Cammarata et al., 2020).They found that one-layer attention-only models perform relatively basic ICL in a crude manner, whereas two-layer models perform very general ICL using very different algorithms.Specifically, they discovered that one-layer models essentially function as an ensemble of bigram and \"skip-trigram\" models that can be accessed directly from the model weights without running the entire model.Most attention heads in these models allocate significant capacity to copying mechanisms, resulting in very simple ICL.In contrast, the two-layer models manifest a significantly powerful mechanism that employs more advanced, qualitative algorithms at inference time, referred to as \"induction heads\".This allows them to perform ICL in a manner that resembles a computer program executing an algorithm, rather than merely referencing skip-trigrams.Building on this foundation, Olsson et al. (2022) later investigated the internal structures responsible for ICL by extending the concept of \"induction head\" (Elhage et al., 2021).They implemented circuits consist of two attention heads: the \"previous token head\", which copies information from one token to its successor, and the actual \"induction head\", which uses this information to target tokens that precede the current one.Their study revealed a phase change occurring early in the training of LLMs of various sizes.This phase change involves circuits that perform \"fuzzy\" or \"nearest neighbor\" pattern completion in a mechanism similar to the two-layer induction heads.These circuits play a crucial role in implementating most ICL in large models.One pivotal insight from (Olsson et al., 2022) presented six arguments supporting their hypothesis that induction heads may serve as the primary mechanistic source of ICL in a significant portion of LLMs, particularly those based on transformer architectures.\n\nWhile Elhage et al. (2021) and Olsson et al. (2022) contribute to our understanding of ICL by probing the internal architecture of LLMs, it is important to note that their findings represent initial steps towards the comprehensive reverseengineering of LLMs.It becomes particularly intricate when dealing with LLMs characterized by complex structures comprising hundreds of layers and spanning billions to trillions of parameters.This complexity introduces significant challenges.Moreover, a substantial portion of their conclusions relies primarily on empirical correlations, which might be susceptible to confounding from various factors, thereby introducing potential vulnerabilities into their findings.", "filtered_refids": [[null, "b57", "b9", "b40"], [null, "b40"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2976, "num_references": 6}
{"corpusid_sectionid": "264832783-s4", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Regression Function Learning", "section": "Several research studies posited that the emergence of LLMs' competence in ICL can be attributed to their intrinsic capability to approximate regression functions for a novel query Q based on the demonstrations D. Garg et al. (2022) first formally de-fined ICL as a problem of learning functions and explored whether LLMs can be trained from scratch to learn simple and well-defined function classes, such as linear regression functions.To achieve this, they generated examples D using these functions, and trained models to predict the function value for the corresponding query Q.Their empirical findings revealed that trained Transformers exhibited ICL abilities, as they manifested to \"learn\" previously unseen linear functions from examples, achieving an average error comparable to that of the optimal least squares estimator.Furthermore, Garg et al. (2022) demonstrated that ICL can be applied to more complex function classes, including sparse linear functions, decision trees, and two-layer neural networks, and posited that the capability to learn a function class through ICL is an inherent property of the model M \u0398 , irrespective of its training methodology.\n\nLater, Li et al. (2023b) extended Garg et al. (2022) to interpret ICL from a statistical perspective.They derived generalization bounds for ICL, considering two types of input examples: sequences that are independently and identically distributed (i.i.d.) and trajectories originating from a dynamical system.They established a multitask generalization rate of 1/", "filtered_refids": [["b18"], [null, "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1536, "num_references": 3}
{"corpusid_sectionid": "264832783-s5", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "\u221a", "section": "nT for both types of examples, addressing temporal dependencies by associating generalization to algorithmic stability, abstracting ICL as an algorithm learning problem.They found that transformers can indeed implement near-optimal algorithms on classical regression problems with both types of input example by ICL.Furthermore, they provided theoretical proof highlighting that self-attention possesses favourable stability properties, established through a rigorous analysis quantifying the influence of one token over another.\n\nAt the same time, Li et al. (2023a) took a further step from the work of (Garg et al., 2022) to gain a deeper understanding of the role of the softmax unit within the attention mechanism of LLMs.They sought to mathematically interpret ICL based on the softmax regression formulation represented as min x || \u27e8exp(Ax), 1 n \u27e9 \u22121 exp(Ax) \u2212 b|| 2 .Their analysis revealed that the upper bounds of data transformations, induced either by a singular selfattention layer or by the application of gradient descent on an L 2 regression loss, align with the softmax regression formulation.This suggests a noteworthy similarity between models learned through gradient descent and those learned by Transformers, especially when trained solely on fundamental regression tasks using self-attention.\n\nConversely, Aky\u00fcrek et al. (2022) took a different approach by delving into the process through which ICL learns linear functions, rather than analysing the types of functions that ICL can learn.Through an examination of the inductive biases and algorithmic attributes inherent in transformer-based ICL, they discerned that ICL can be understood in algorithmic terms, and linear learners within the model may essentially rediscover standard estimation algorithms.More specifically, Aky\u00fcrek et al. (2022) provided a theoretical proof to support the claim that transformers can implement learning algorithms for linear models using gradient descent and closed-form ridge regression.They also empirically demonstrated that trained ICLs closely align with the predictors derived from gradient descent, ridge regression, and precise least-squares regression.They also introduced preliminary findings suggesting that ICL exhibits algorithmic characteristics, with both predictors of learners' late layers encoding weight vectors and moment matrices in a non-linear manner.\n\nAlthough these studies have either provided theoretical proofs or showcased empirical evidence interpreting the ICL ability of LLMs as a problem of learning regression functions, their conclusions are limited to simplified model architectures and controlled synthetic experimental settings.These findings may not necessarily apply directly to realworld scenarios.", "filtered_refids": [[], ["b28", "b18"], ["b1"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2747, "num_references": 3}
{"corpusid_sectionid": "264832783-s6", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Gradient Descent & Meta-Optimization", "section": "In the realm of gradient descent, Dai et al. (2023) adopted a perspective of viewing LLMs as metaoptimizers and interpreting ICL as a form of implicit fine-tuning.They first conducted a qualitative analysis of Transformer attention, representing it in a relaxed linear attention form, and identified a dual relationship between it and gradient descent.Through a comparative analysis between ICL and explicit fine-tuning, Dai et al. (2023) interpreted ICL as a meta-optimization process.They further provided evidence that the transformer attention head possesses a dual nature similar to gradient descent (Irie et al., 2022), where the optimizer produces meta-gradients based on the provided examples for ICL through forward computation.Concurrently, von Oswald et al. (2022) also proposed a connection between the training of Transformers on auto-regressive objectives and gradient-based meta-learning formulations.They specifically examined how Transformers define a loss function based on the given examples and, subsequently, the mechanisms by which Transformers assimilate knowledge using the gradients of this loss function.Their findings suggest that ICL may manifest as an emergent property, approximating gradient-based few-shot learning within the forward pass of the model.\n\nHowever, it is worth noting that both of these investigations only focused on ICL within Transformer architectures, without considering other architectural variations or emergent capabilities, such as CoT and instruction following.In addition, their analyses predominantly rely on a simplifed form of linear attention for qualitative assessment.This poses a challenge since the operation of standard Transformer attention, without any approximation, may be intricate.Therefore, there is a need for more nuanced explorations into this mechanism in future studies.", "filtered_refids": [["b14", null, "b21"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1848, "num_references": 3}
{"corpusid_sectionid": "264832783-s7", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Bayesian Inference", "section": "In their work, Xie et al. ( 2022) first provided an interpretation of ICL through the lens of Bayesian inference, proposing that LLMs have the capability to perform implicit Bayesian inference via ICL.Specifically, they synthesized a small-scale dataset to examine how ICL emerges in LSTM and Transformer models during pretraining on text with extended coherence.Their findings revealed that both models are capable of inferring latent concepts to generate coherent subsequent tokens during pretraining.Additionally, these models were shown to perform ICL by identifying a shared latent concept among examples during the inference process.Their theoretical analysis confirms that this phenomenon persists even when there is a distribution mismatch between the examples and the data used for pretraining, particularly in settings where the pretraining distribution is derived from a mixture of Hidden Markov Models (HMMs) (Baum and Petrie, 1966).Furthermore, Xie et al. ( 2022) observed that the ICL error decreases as the length of each example increases, emphasizing the significance of the inherent information within inputs.This goes beyond mere input-label correlations and highlights the roles of intrinsic input characteristics in facilitating ICL.\n\nFollowing on, Wang et al. (2023b) expanded the investigation of ICL by relaxing the assumptions made by Xie et al. (2022) and posited that ICL in LLMs essentially operates as a form of topic modeling that implicitly extracts task-relevant information from examples to aid in inference.Wang et al. (2023b) grounded their theoretical analysis in a setting with a finite number of demonstrations, and under a more general language generation process.Specifically, they characterized the data generation process using a causal graph with three variables and imposed no constraints on the distribution or quantity of samples.Their empirical and theoretical investigations revealed that ICL can approximate the Bayes optimal predictor when a finite number of samples are chosen based on the latent concept variable.Moreover, Wang et al. (2023b) devised an effective practical algorithm for demonstration selection tailored to real-world LLMs.\n\nAt the same time, Jiang (2023) also introduced a novel latent space theory extending the idea of Xie et al. ( 2022) to explain emergent abilities in LLMs.Instead of focusing on specific data distributions generated by HMMs, they delved into general sparse data distributions and employed LLMs as a universal density approximator for the marginal distribution, allowing them to probe these sparse structures more broadly.Jiang (2023) demonstrated that ICL, CoT, and instruction-following abilities in LLMs can be ascribed to Bayesian inference operating on the broader sparse joint distribution of languages.To shed light on the significance of the attention mechanism for ICL from a Bayesian view, Zhang et al. (2023) defined ICL as the task of predicting a response that aligns with a given covariate based on examples derived from a latent variable model.They established that ICL implicitly implements the Bayesian Model Averaging (BMA) algorithm, which is approximated by the attention mechanism.Furthermore, they demonstrated that certain attention mechanisms converge towards the conventional softmax attention as the number of examples goes to infinity.These attentions, due to their encoding of BMA within their structure, empower the Transformer model to perform ICL.\n\nAlthough their conclusions are insightful, there is a room for improvement.Their findings might be influenced by various factors, such as the formats of the examples, the nature of tasks, and the choice of evaluation metrics.Additionally, many of these studies are based on analyses conducted using small synthetic datasets, potentially restricting their relevance and applicability to real-world scenarios.", "filtered_refids": [["b4"], [null, "b60"], ["b70", "b22"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3879, "num_references": 5}
{"corpusid_sectionid": "264832783-s9", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Pre-training Data", "section": "Some studies have suggested that factors related to pre-traning data such as data domain, data term frequency, and data distribution (Chan et al., 2022;Razeghi et al., 2022), are crucial elements influencing the development of emergent abilities.\n\nData Domain Shin et al. (2022) conducted a study to explore the variations of ICL performance concerning the domain source and the size of the pre-training corpus, focusing primarily on the Korean lexicon.They utilized seven subcorpora from the HyperCLOVA corpus (Kim et al., 2021) to pretrain various language models and evaluated these models on Korean downstream tasks.Interestingly, Shin et al. (2022) found that the size of the pretraining corpus does not always determine the emergence of ICL.Instead, the domain source of the corpus significantly influences ICL performance.For example, language models trained with subcorpora constructed from blog posts exhibited the best ICL capability.This phenomenon may be attributed to the greater token diversity presented in the blog posts corpus compared with other sources like news.Moreover, their experiments highlighted that combining multiple corpora can lead to the emergence of ICL, even if individual corpora did not produce such learning on their own.Surprisingly, Shin et al. (2022) also found that a language model pre-trained with a corpus related to a downstream task did not always guarantee competitive ICL performance.For instance, a model trained on a news-related dataset (Park et al., 2021) showed superior performance in zero-shot news topic classification, but its few-shot performance was not superior.In a similar vein, The authors focused particularly on a crucial type of reasoning in LLMs -numerical reasoning in fewshot settings; and examined the extend to which the frequency of terms from the pre-training data correlates with model performance in these situations.Their analysis focused on the prevalence of numerical reasoning tasks within the training instances and established a connection between frequencies and reasoning performance.This connection is quantified by introducing the \"performance gap\", which is defined as the accuracy of terms appearing more than 90% of the time minus the accuracy of terms appearing less than 10% of the time.They conducted their experiments using GPTbased language models trained on the Pile dataset (Gao et al., 2021), ranging in size from 1.3B to 6B parameters.Evaluation was carried out on 11 datasets spanning three types of mathematical reasoning tasks: Arithmetic, Operation Inference and Time-Unit Conversion.The findings consistently show that models perform better in instances where terms from the pre-training data are more prevalent (Razeghi et al., 2022).In some scenarios, there is a substantial performance gap of over 70% between the most frequently occurring terms and the least frequently occurring terms.The significant performance difference raises questions about the actual generalization capabilities of these models beyond their pre-training data.Razeghi et al. (2022)'s observations suggest that the more prevalent content included in the pre-training data may exert an influ-ence on the emergent abilities, and it is possible that these language models are not actually reasoning to solve arithmetic tasks.In line with this research, Kandpal et al. (2023) (2023) theoretically demonstrated that unseen tasks can be efficiently learned via ICL when the pretraining data distribution comprises a mixture of latent tasks.", "filtered_refids": [["b11", "b48"], ["b44", "b25", "b49", "b48", null, "b24", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3513, "num_references": 9}
{"corpusid_sectionid": "264832783-s10", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Pre-training Model", "section": "Wei et al. (2022b) embarked on an investigation into the emergent abilities of LLMs.Their ap-proach to interpreting this phenomenon involved conducting a comprehensive survey of existing literature and analyzing the unpredictable nature of how certain abilities manifest as these models scale (Brown et al., 2020).Wei et al. (2022b) emphasized that while model scale has been correlated with LLM performance, it is not the sole determinant.Task-specific abilities can also be examined by considering a language model's performance (perplexity) on general text corpora, such as WikiText103.Their experiments showed that, despite having fewer parameters, the PaLM 62B model outperformed LaMDA 137B and GPT-3 175B in certain tasks.This suggests that other factors, like high-quality data and architectural differences, also play a role.Moreover, continued pre-training on different objectives, like the mixture-of-denoisers objective, has shown potential in enabling emergent abilities (Tay et al., 2022).\n\nResearch is also advancing to make these discovered abilities accessible for smaller-scale models.\n\nFor instance, instruction-based fine-tuning showed potential in smaller models with different architectures.Additionally, the emergence of syntactic rulelearning can be triggered by threshold frequencies in training data, similar to \"aha\" moments in human learning (Abend et al., 2017;Zhang et al., 2021).While the majority of research agrees that model scale is a key factor for emergent abilities.Kirsch et al. (2022) presented an interesting perspective.They found that among the factors determining the inductive bias of the model, the state-size (such as the hidden state size in a recurrent network) is a more crucial parameter than the overall model size for the emergence of ICL ability.", "filtered_refids": [["b62", null, "b7", "b54"], [], ["b69", "b26", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1799, "num_references": 7}
{"corpusid_sectionid": "264832783-s12", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Demonstration Order", "section": "The order of the demonstrations has a significant impact on downstream task performance.Lu et al. (2022) showed that it be the deciding factor between achieving near stateof-the-art and random guessing.They designed demonstrations containing four samples with a balanced label distribution and conducted experiments involving all 24 possible permutations of sample orders.The experimental results showed that the performance variations among different permutations exist across various model sizes, especially for smaller models.Besides, is was observed that effective prompts are not transferrable across models, indicating that the optimal order is model-dependent, and what works well for one model does not guarantee good results for another model.Zhao et al. (2021) identified a phenomenon that LLMs tend to repeat answers found at the end of demonstrations, which they termed \"recency bias\".Similarly, in multi-document question answering and key-value retrieval tasks, Liu et al. ( 2023) made analogous observations.These tasks involve identifying relevant information within lengthy input contexts.The results showed that LLMs performed best when the relevant information is located at the beginning or end of their input contexts.However, their performance degraded when they are forced to use information from the middle of their input.In addition, they noted that model performance declines as the input context length increases, suggesting that current models struggle to effectively reason over their entire context window.Although these studies offer insights into how demonstration order influences emergent abilities, they do not delve into the underlying reasons of these obesrvations.In an effort to investigate the impact of semantic similarity between ICL examples and test examples on downstream task, Liu et al. (2022) proposed retrieving examples semantically similar to a test example for creating its demonstration.They utilized the CLS embeddings from a pre-trained RoBERTa-large (Liu et al., 2019) model to represent sentences and assessed the semantic similarity between two sentences by computing the cosine similarity of their respective representations.For each test example, they identified the nearest K neighbors from the training set and concatenated them in descending order of semantic similarity to create the demonstration.Their experiments on Web Questions (Berant et al., 2013) and Trivia Question Answering (Joshi et al., 2017) benchmarks showed that the default order performed slightly better than the reverse order.However, the reverse order performed better on the Natural Questions (Kwiatkowski et al., 2019) benchmark.Consequently, the choice of order appears to be dependent on the specific dataset in use.\n\nInput-Label Mapping Some studies have been conducted to investigate how input-label mappings influence the performance of ICL.Min et al. (2022) revealed that substituting the correct labels of incontext examples in demonstrations with random labels only leads to a marginal decrease in performance across a variety of classification and multichoice tasks.They also conducted ablation experiments to investigate the impact of the number of correct labels on performance.Surprisingly, the results showed that the performance was not sensitive to the number of correct labels in demonstrations.This led to the counter-intuitive conclusion that LLMs do not heavily rely on input-label mappings to perform tasks.\n\nHowever, Yoo et al. ( 2022), Wei et al. (2023), and Kossen et al. (2023) disagreed with the claim put forth by Min et al. (2022).Yoo et al. (2022) pointed out that the claim exhibited overgeneralization in two aspects: (1) Aggregating the mean performance across various datasets was found to be inadequate in capturing the insensitivity behavior observed within individual datasets.\n\n(2) The experimental setting lacked generalizability, and the results were sensitive to minor adjustments to the experimental setup.To delve deeper into the topic of input-label mapping, Yoo et al. (2022) introduced two novel metrics.The first metric, Label-Correctness Sensitivity, quantifies the impact on downstream classification performance when a fixed amount of label corruption is introduced into the demonstration.The second metric, Ground-Truth Label Effect Ratio, assesses how much the presence of ground-truth labels improves the performance compared to a baseline with random labels.Their experimental results showed that the sensitivity exhibited significant variation across 17 datasets, with the aggregate sensitivity considerably high.This indicated that label correctness does indeed affect downstream task performance.Furthermore, Yoo et al. (2022) suggested a strong correlation between sensitivity and task difficulty, revealing that LLMs displayed low sensitivity on challenging tasks.Wei et al. (2023) further explored how semantic priors and input-label mappings affect ICL.They suggested that LLMs possess the ability to override semantic priors from pre-training in favour of inputlabel mappings from demonstrations.This explains why the performance of LLMs drops below random guessing when all the labels in the demonstrations are flipped.They also found that smaller models experienced a less severe decline in performance because they lack the capacity to override semantic priors to the same extent.More specifically, Wei et al. (2023) conducted experiments where they replaced the labels with semantically unrelated labels.The results showed that the performance drop was more significant for small models compared to LLMs.This led them to suggest that small models rely heavily on the semantic meanings of labels rather than learning the input-label mappings provided in the demonstrations.Kossen et al. (2023) also found that larger models are more sensitive to randomized labels, and they highlighted that LLMs can learn new input-label mappings from demonstrations.\n\nHowever, the ability to learn new input-label mappings can, at times, have adverse effect on performance.Tang et al. (2023) revealed that LLMs sometimes tend to exploit shortcuts within demonstrations for downstream tasks.These shortcuts represent spurious correlations between in-context examples and their corresponding labels.Tang et al. (2023) designed several types of shortcuts, and their experimental results showed that LLMs are \"lazy reasoners\".They relied heavily on the shortcuts within demonstrations to deduce the final answers.Furthermore, Si et al. (2023) discovered that when presented with a set of non-specific demonstrations (For example, the labels are semantically unrelated), LLMs exhibited feature bias.This indicated that LLMs tend to favour one feature over another, even when both features are equally capable of predicting the label, as mentioned in the prompt.For example, in a sentiment analysis setting, LLMs showed a significant bias towards predicting labels based on sentiment rather than shallow lexical features.Nevertheless, feature bias has the potential to detrimentally affect performance when the model's feature bias does not align with the intended task.Si et al. (2023) suggested that certain interventions could help mitigate feature bias, such as employing natural-language instructions and incorporating label words that have semantic relevance to the intended feature.\n\nTo further investigate the underlying mechanism of how LLMs learn from input-label mappings, Wang et al. (2023a) conducted an extensive study into the workings of ICL from the perspective of information flow.They computed saliency scores for each element within the attention matrix to unveil the significant token interactions.The experimental results demonstrated that label words within demonstrations play a crucial role in this process.\n\nSpecifically: (1) During the processing of shallow computation layers, semantic information becomes concentrated within the representations of label words.\n\n(2) The aggregated information contained within label words serves as a reference for the final predictions made by LLMs.Their findings confirmed that label words can indeed have a substantial impact on the performance of the final task.\n\nChain-of-Thought Prompting Some studies have focused on exploring the impact of COT prompting on LLM performance.Wang et al. (2022) found that the validity of the reasoning process in demonstrations has only a minimal impact on performance.To assess this, they constructed invalid reasoning processes manually for all incontext examples.Surprisingly, the experimental results showed that LLMs can retain 80-90% of their performance even when presented with invalid reasoning steps in demonstrations.They also found that the coherence of the reasoning process and its relevance to the query are significantly more crucial factors for the effectiveness of CoT.\n\nRegarding the explanations generated by LLMs, Turpin et al. (2023) found that CoT explanations produced by LLMs can occasionally misrepresent the true underlying rationales behind their predictions.They introduced two types of bias in the prompt design to investigate this phenomenon.The first bias involves consistently reordering the multiple-choice options of in-context examples to make the answer 'A'.The second bias entails including the suggested answers directly in the prompt.The experimental results indicated that, in both bias scenarios, LLMs tend to provide answers aligned with stereotypes and generate explanations that do not faithfully support the answer.Furthermore, there was a large drop in performance when comparing biased demonstrations to unbiased demonstrations.", "filtered_refids": [["b35", null, "b23", "b72", "b37", "b33", "b5"], ["b38"], ["b64", "b38", "b68", "b27"], ["b64", "b68", "b27"], ["b53", "b50"], ["b59"], [], [], ["b58"], ["b56"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 63, "num_chars": 9656, "num_references": 20}
{"corpusid_sectionid": "264832783-s14", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Unified Framework", "section": "There is currently no standardized framework available for understanding or interpreting emergent abilities.While researchers often investigate factors contributing to emergent abilities based on empirical insights, the resulting conclusion may not always be robust or broadly applicable to realworld applications.The challenge lie in the multitude of factors that influence emergent abilities, many of which may not be directly modifiable with respect to the abilities themselves, as noted by Wei et al. (2022b).For instance, apart from the attention mechanism, Li et al. (2023a) found that softmax unit plays a pivotal role in understanding ICL through function regression problems (Garg et al., 2022;Aky\u00fcrek et al., 2023;von Oswald et al., 2022).From the micro-perspective, when examining how the extent of pre-training impacts emergent abilities, data quality serves a crucial role alongside factors like data scale and training time.", "filtered_refids": [["b18", "b28", "b41", "b62", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 3, "num_chars": 938, "num_references": 5}
{"corpusid_sectionid": "264832783-s15", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Evaluation Metrics", "section": "Current research efforts typically measure emergent abilities by assessing task performance or optimizing criteria such as gradient (von Oswald et al., 2022) and token loss (Olsson et al., 2022) during the pre-training stage.Another line of research (Shin et al., 2022;Razeghi et al., 2022) has discovered that the relationship between the evaluation measures of language models during training does not strongly correlate with the conventional evaluation metrics, such as F1-score, that have been used to measure performance of emergent abilities under most experimental setups.However, a dedicated criterion explicitly designed for the assessment of emergent abilities is currently lacking.In addition, assessing emergent abilities often becomes complicated due to the interwined emergence of other competencies (Lu et al., 2023).In this work, we postulate that the assessment of emergent ability can be based on its capability to produce satisfactory results in comparison to a finetuned model.This approach provides a preliminary framework for devising evaluation criteria.However, it is important to note that this methodology is preliminary and not yet comprehensive or definitive.Further refinement and development of formal criteria are necessary to establish a robust and universally applicable evaluation metric for emergent ability itself.", "filtered_refids": [["b40", "b41", "b49", "b48", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1350, "num_references": 5}
{"corpusid_sectionid": "264832783-s16", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Cost and Computational Resources", "section": "LLMs faced constraints related to their token capacity, which can lead to deficiency in coherence when dealing with longer demonstration examples or text generation.This limitation can result in challenges for emergent abilities, making it difficult to maintain a consistent and extended logical flow.What's more, there are some experimental limitations that have hindered the exploration of this type of research.The pre-training stage of these models demands a huge amount of computational resources, which could become a barrier for researchers who lack the necessary resources (Shin et al., 2022;Brown et al., 2020;Wei et al., 2022b;Berglund et al., 2023).This limitation has restricted investigations into the sources of emergent abilities in commonly used LLMs.Furthermore, the limited knowledge of the detailed lexical resources utilized during the pre-training stage adds complexity to the examination of their abilities (Berglund et al., 2023).", "filtered_refids": [["b49", "b62", "b6", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 953, "num_references": 4}
{"corpusid_sectionid": "264832783-s18", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Causality rather than Correlation", "section": "As demonstrated by Razeghi et al. (2022) and Power et al. (2022), intervening on the pre-training dataset, particularly with an emphasis on the emergence of reasoning abilities, offers a promising path for gaining deeper into the question of whether LLMs indeed possess reasoning abilities.Moreover, as highlighted by Chan et al. (2022), delving into the intricacies of in-context and in-weights learning deserves further investigation, especially concerning how prior knowledge is signaled.It is crucial to make comparison between transformers and recurrent architectures, particularly in understanding their in-context learning capacities.What's more, there is a need for a more comprehensive interpretation of the impact of the \"Reversal Curse\" in extensive pre-training datasets for LLMs, considering the varying frequencies of reversed information.", "filtered_refids": [["b11", "b45", "b48"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 853, "num_references": 3}
{"corpusid_sectionid": "222124957-s2", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Transfer: The Pretrain-Finetune Framework", "section": "While text representations can be learned in any manner, ultimately, they are evaluated using specific target tasks. Historically, the learned representations (e.g. word vectors) were used as initialization for task-specific models. Dai and Le (2015) are credited with using pretrained language model outputs as initialization, McCann et al. (2017) use pretrained outputs from translation as frozen word embeddings, and Howard and Ruder (2018) and Radford et al. (2018) demonstrate the effectiveness of finetuning to different target tasks by updating the full (pretrained) model for each task. We refer to the embeddings produced by the pretrained models (or encoders) as contextualized text representations. As our goal is to discuss the encoders and their representations, we do not cover the innovations in finetuning (Liu et al., 2015;Ruder et al., 2019;Phang et al., 2018;Liu et al., 2019c;Zhu et al., 2020, inter alia).\n\nEvaluation Widely adopted evaluations of text representations relate them to downstream natural language understanding (NLU) benchmarks. This full-stack process necessarily conflates representation power with finetuning strategies. Common language understanding benchmarks include (1) a diverse suite of sentence-level tasks covering paraphrasing, natural language inference, sentiment, and linguistic acceptability (GLUE) and its more challenging counterpart with additional commonsense and linguistic reasoning tasks (Super-GLUE) (Wang et al., 2019c,b;Clark et al., 2019a;De Marneffe et al., 2019;Roemmele et al., 2011;Khashabi et al., 2018;Zhang et al., 2018;Dagan et al., 2006;Bar Haim et al., 2006;Giampiccolo et al., 2007;Bentivogli et al., 2009;Pilehvar and Camacho-Collados, 2019;Poliak et al., 2018;Levesque et al., 2011); (2) crowdsourced questions derived from Wikipedia articles (Rajpurkar et al., 2016, 2018; and (3) multiple-choice reading comprehension (Lai et al., 2017, RACE).", "filtered_refids": [["b63", "b6", "b57", "b40", "b41", "b45", null, "b72"], ["b58", "b60", "b28", null, "b19", "b123", "b70", "b33", "b68"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1921, "num_references": 17}
{"corpusid_sectionid": "222124957-s4", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Token Prediction", "section": "Predicting (or generating) the next word has historically been equivalent to the task of language modeling. Large language models perform impressively on a variety of language understanding tasks while maintaining their generative capabilities (Radford et al., 2018(Radford et al., , 2019Keskar et al., 2019;Brown et al., 2020), often outperforming contemporaneous models that use additional training objectives.\n\nELMo (Peters et al., 2018) is a BiLSTM model with a language modeling objective for the next (or previous) token given the forward (or backward) history. This idea of looking at the full context was further refined as a cloze 3 task (Baevski et al., 2019), or as a denoising Masked Language Modeling (MLM) objective (Devlin et al., 2019, BERT). MLM replaces some tokens with a [mask] symbol and provides both right and left contexts (bidirectional context) for predicting the masked tokens. The bidirectionality is key to outperforming a unidirectional language model on a large suite of natural language understanding benchmarks (Devlin et al., 2019;Raffel et al., 2019).\n\nThe MLM objective is far from perfect, as the use of [mask] introduces a pretrain/finetune vo-cabulary discrepancy. Devlin et al. (2019) look to mitigate this issue by occasionally replacing [mask] with the original token or sampling from the vocabulary. Yang et al. (2019) convert the discriminative objective into an autoregressive one, which allows the [mask] token to be discarded entirely. Naively, this would result in unidirectional context. By sampling permutations of the factorization order of the joint probability of the sequence, they preserve bidirectional context. Similar ideas for permutation language modeling (PLM) have also been studied for sequence generation (Stern et al., 2019;Chan et al., 2019;Gu et al., 2019). The MLM and PLM objectives have since been unified architecturally Bao et al., 2020) and mathematically (Kong et al., 2020).\n\nELECTRA (Clark et al., 2020) replaces [mask] through the use of a small generator (trained with MLM) to sample a real token from the vocabulary. The main encoder, a discriminator, then determines whether each token was replaced.\n\nA natural extension would mask units that are more linguistically meaningful, such as rarer words, 4 whole words, or named entities (Devlin et al., 2019;Sun et al., 2019b). This idea can be simplified to random spans of texts (Yang et al., 2019;Song et al., 2019). Specifically, Joshi et al. (2020) add a reconstruction objective which predicts the masked tokens using only the span boundaries. They find that masking random spans is more effective than masking linguistic units.\n\nAn alternative architecture uses an encoderdecoder framework (or denoising autoencoder) where the input is a corrupted (masked) sequence the output is the full original sequence (Wang et al., 2019d;Lewis et al., 2020;Raffel et al., 2019).", "filtered_refids": [["b15", "b64", "b63", "b17"], ["b65", null, "b56"], [null, "b23", "b119", "b84"], [null], ["b90", "b13", "b82", null, "b119"], ["b29", "b104", "b65"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2900, "num_references": 20}
{"corpusid_sectionid": "222124957-s5", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Nontoken Prediction", "section": "Bender and Koller (2020) argue that for the goal of natural language understanding, we cannot rely purely on a language modeling objective; there must be some grounding or external information that relates the text to each other or to the world. One solution is to introduce a secondary objective to directly learn these biases.\n\nSelf-supervised discourse structure objectives, such as text order, has garnered significant attention. To capture relationships between two sentences, 5 Devlin et al. (2019) introduce the next 4 Clark et al. (2020) report negative results for rarer words. 5 Sentence unfortunately refers to a text segment containing sentence prediction (NSP) objective. In this task, either sentence B follows sentence A or B is a random negative sample. Subsequent works showed that this was not effective, suggesting the model simply learned topic (Yang et al., 2019;. Jernite et al. (2017) propose a sentence order task of predicting whether A is before, after, or unrelated to B, and Wang et al. (2020b) and Lan et al. (2020) use it for pretraining encoders. They report that (1) understanding text order does contribute to improved language understanding; and (2) harder-to-learn pretraining objectives are more powerful, as both modified tasks have lower intrinsic performance than NSP. It is still unclear, however, if this is the best way to incorporate discourse structure, especially since these works do not use real sentences.\n\nAdditional work has focused on effectively incorporating multiple pretraining objectives. Sun et al. (2020a) use multi-task learning with continual pretraining (Hashimoto et al., 2017), which incrementally introduces newer tasks into the set of pretraining tasks from word to sentence to document level tasks. Encoders using visual features (and evaluated only on visual tasks) jointly optimize multiple different masking objectives over both token sequences and regions of interests in the image (Tan and Bansal, 2019). 6 Prior to token prediction, discourse information has been used in training sentence representations. Conneau et al. (2017Conneau et al. ( , 2018a use natural language inference sentence pairs, Jernite et al. (2017) use discourse-based objectives of sentence order, conjunction classifier, and next sentence selection, and  use discourse markers. While there is weak evidence suggesting that these types of objectives are less effective than language modeling (Wang et al., 2019a), we lack fair studies comparing the relative influence between the two categories of objectives.", "filtered_refids": [[], ["b11", "b26", "b119", "b106"], ["b93", "b91", null, "b11"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2554, "num_references": 8}
{"corpusid_sectionid": "222124957-s8", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Training", "section": "One area of research decreases wall-clock training time through more compute and larger batches. You et al. (2020) reduce the time of training BERT by introducing the LAMB optimizer, a large batch stochastic optimization method adjusted for attention models. Rajbhandari et al. (2020) analyze memory usage in the optimizer to enable parallelization of models resulting in higher throughput in training. By reducing the training time, models can be practically trained for longer, which has also been shown to lead to benefits in task performance Lan et al., 2020, inter alia).\n\nAnother line of research reduces the compute through attention sparsification (discussed in \u00a74.2) or increasing the convergence rate (Clark et al., 2020). These works report hardware and estimate the reduction in floating point operations (FPOs). 8 These kinds of speedup are orthogonal to hardware parallelization and are most encouraging as they pave the path for future work in efficient training.\n\nNote that these approaches do not necessarily affect the latency to process a single example nor the compute required during inference, which is a function of the size of the computation graph.", "filtered_refids": [["b66", "b121", null], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1173, "num_references": 4}
{"corpusid_sectionid": "222124957-s9", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Inference", "section": "Reducing model size without impacting performance is motivated by lower inference latency, hardware memory constraints, and the promise that naively scaling up dimensions of the model will improve performance. Size reduction techniques produce smaller and faster models, while occasionally improving performance. Rogers et al. (2020) survey BERT-like models and present in Table 1 the differences in sizes and performance across several models focused on inference efficiency.\n\nArchitectural changes have been explored as one avenue for reducing either the model size or inference time. In Transformers, the self-attention pattern scales quadratically in sequence length. To reduce the asymptotic complexity, the self-attention can be sparsified: each token only attending to a small \"local\" set (Vaswani et al., 2017;Child et al., 2019;Sukhbaatar et al., 2019). This has further been applied to pretraining on longer sequences, resulting in sparse contextualized encoders Ye et al., 2019;Kitaev et al., 2020;Beltagy et al., 2020, inter alia). Efficient Transformers is an emerging subfield with applications beyond NLP; Tay et al. (2020) survey 17 Transformers that have implications on efficiency.\n\nAnother class of approaches carefully selects weights to reduce model size. Lan et al. (2020) use low-rank factorization to reduce the size of the embedding matrices, while Wang et al. (2019f) factorize other weight matrices. Additionally, parameters can be shared between layers (Dehghani et al., 2019;Lan et al., 2020) or between an encoder and decoder (Raffel et al., 2019). However, models that employ these methods do not always have smaller computation graphs. This greatly reduces the usefulness of parameter sharing compared to other methods that additionally offer greater speedups relative to the reduction in model size.\n\nClosely related, model pruning (Denil et al., 2013;Han et al., 2015;Frankle and Carbin, 2018) during training or inference has exploited the overparameterization of neural networks by removing up to 90%-95% parameters. This approach has been successful in not only reducing the number of parameters, but also improving performance on downstream tasks. Related to efforts for pruning deep networks in computer vision (Huang et al., 2016), layer selection and dropout during both training and inference have been studied in both LSTM (Liu et al., 2018a) and Transformer (Fan et al., 2020) based encoders. These also have a regularization effect resulting in more stable training and improved performance. There are additional novel pruning methods that can be performed during training (Guo et al., 2019;. These successful results are corroborated by other efforts (Gordon et al., 2020) showing that low levels of pruning do not substantially affect pretrained representations. Additional successful efforts in model pruning directly target a downstream task (Sun et al., 2019a;Michel et al., 2019;McCarley, 2019;Cao et al., 2020a). Note that pruning does not always lead to speedups in practice as sparse operations may be hard to parallelize.\n\nKnowledge distillation (KD) uses an overparameterized teacher model to rapidly train a smaller student model with minimal loss in performance (Hinton et al., 2015) and has been used for translation (Kim and Rush, 2016), computer vision (Howard et al., 2017), andadversarial examples (Carlini andWagner, 2016). This has been applied to ELMo  and BERT Sun et al., 2020b, inter alia). KD can also be combined with adaptive inference, which dynamically adjusts model size (Liu et al., 2020b), or performed on submodules which are later substituted back into the full model .\n\nQuantization with custom low-precision hardware is also a promising method for both reducing the size of models and compute time, albeit it does not reduce the number of parameters or FPOs (Shen et al., 2020;Zafrir et al., 2019). This line of work is mostly orthogonal to other efforts specific to NLP.", "filtered_refids": [["b71"], ["b98", "b87", "b21", "b120", null, "b95", "b64"], ["b65", "b108", "b26", null], ["b34", "b46", "b89", "b120", null, "b48", "b8"], ["b92", "b4", "b20", null, "b5"], ["b80", "b122"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 3952, "num_references": 26}
{"corpusid_sectionid": "222124957-s12", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Data Quantity", "section": "There has not yet been observed a ceiling to the amount of data that can still be effectively used in training (Baevski et al., 2019;Yang et al., 2019;Brown et al., 2020). Raffel et al. (2019) curate a 745GB subset of Common Crawl (CC), 10 which starkly contrasts with the 13GB used in BERT. For multilingual text encoding, Wenzek et al. (2020) curate 2.5TB of language-tagged CC. As CC continues to grow, there will be even larger datasets (Brown et al., 2020). Sun et al. (2017) explore a similar question for computer vision, as years of progress iterated over 1M labeled images. By using 300M images, they improved performance on several tasks with a basic model. We echo their remarks that we should be cognizant of data sizes when drawing conclusions.\n\nIs there a floor to the amount of data needed to achieve current levels of success on language understanding benchmarks? As we decrease the data size, LSTM-based models start to dominate in perplexity (Yang et al., 2019;Melis et al., 2020), suggesting there are challenges with either scaling up LSTMs or scaling down Transformers. While probing contextualized models and representations is an important area of study (see \u00a76), prior work focuses on pretrained models or models further pretrained on domain-specific data (Gururangan et al., 2020). We are not aware of any work which probes identical models trained with decreasingly less data. How much (and which) data is necessary for high performance on probing tasks? 11", "filtered_refids": [["b15", "b112", null, "b88", "b119", "b65"], [null, "b119"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1483, "num_references": 8}
{"corpusid_sectionid": "222124957-s13", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Data Quality", "section": "While text encoders should be trained on language, large-scale datasets may contain web-scraped and uncurated content (like code). Raffel et al. (2019) ablate different types of data for text representations and find that naively increasing dataset size does not always improve performance, partially due to data quality. This realization is not new. Parallel data and alignment in machine translation (Moore and Lewis, 2010;Duh et al., 2013;Xu and Koehn, 2017;Koehn et al., 2018, inter alia) and speech (Peddinti et al., 2016) often use language models to filter out misaligned or poor data. Sun et al. (2017) use automatic data filtering in vision. These successes on other tasks suggest that improved automated methods of data cleaning would let future models consume more high-quality data.\n\nIn addition to high quality, data uniqueness appears to be advantageous. Raffel et al. (2019) show that increasing the repetitions (number of epochs) of the pretraining corpus hurts performance. This is corroborated by , who find that random, unique masks for MLM improve over repeated masks across epochs. These findings together suggest a preference to seeing more new text. We suspect that representations of text spans appearing multiple times across the corpus are better shaped by observing them in unique contexts. Raffel et al. (2019) find that differences in domain mismatch in pretraining data (web crawled vs. news or encyclopedic) result in strikingly different performance on certain challenge sets, and Gururangan et al. (2020) find that continuing pretraining on both domain and task specific data lead to gains in performance.", "filtered_refids": [["b118", "b55", "b49", null, "b22", "b88", "b65"], [null, "b65"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1638, "num_references": 9}
{"corpusid_sectionid": "222124957-s14", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Datasets and Evaluations", "section": "With these larger and cleaner datasets, future research can better explore tradeoffs between size and quality, as well as strategies for scheduling data during training.\n\nAs we continue to scrape data off the web and publish challenge sets relying on other web data, we need to cautiously construct our training and evaluation sets. For example, the domains of many benchmarks (Wang et al. (2019c, GLUE), Rajpurkar et al. (2016Rajpurkar et al. ( , 2018 2020) highlight the prevalance of toxic language in the common pretraining corpora and stress the important of pretraining data selection, especially for deployed models. We are not aware of a comprehensive study that explores the effect of leaving out targeted subsets of the pretraining data. We hope future models note the domains of pretraining and evaluation benchmarks, and for future language understanding benchmarks to focus on more diverse genres in addition to diverse tasks.\n\nAs we improve models by training on increasing sizes of crawled data, these models are also being picked up by NLP practitioners who deploy them in real-world software. These models learn biases found in their pretraining data (Gonen and Goldberg, 2019; May et al., 2019, inter alia). It is critical to clearly state the source 12 of the pretraining data and clarify appropriate uses of the released models. For example, crawled data can contain incorrect facts about living people; while webpages can be edited or retracted, publicly released \"language\" model are frozen, which can raise privacy concerns (Feyisetan et al., 2020). raise the second. Inspired by prior work (Lipton, 2018;Belinkov and Glass, 2019;Alishahi et al., 2019), we organize here the major probing methods that are applicable to all encoders in hopes that future work will use comparable techniques.", "filtered_refids": [[], [null, "b67", "b68"], [null, "b33", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1813, "num_references": 6}
{"corpusid_sectionid": "222124957-s15", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Probing with Tasks", "section": "One technique uses the learned model as initialization for a model trained on a probing task consisting of a set of targeted natural language examples. The probing task's format is flexible as additional, (simple) diagnostic classifiers are trained on top of a typically frozen model (Ettinger et al., 2016;Hupkes et al., 2018;Poliak et al., 2018;Tenney et al., 2019b). Task probing can also be applied to the embeddings at various layers to explore the knowledge captured at each layer (Tenney et al., 2019a;. Hewitt and Liang (2019) warn that expressive (nonlinear) diagnostic classifiers can learn more arbitrary information than constrained (linear) ones. This revelation, combined with the differences in probing task format and the need to train, leads us to be cautious in drawing conclusions from these methods.", "filtered_refids": [["b97", "b60", "b9", null, "b96", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 819, "num_references": 6}
{"corpusid_sectionid": "222124957-s16", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Model Inspection", "section": "Model inspection directly opens the metaphorical black box and studies the model weights without additional training. For examples, the embeddings themselves can be analyzed as points in a vector space (Ethayarajh, 2019). Through visualization, attention heads have been matched to linguistic functions (Vig, 2019;Clark et al., 2019b). These works suggest inspection is a viable path to debugging specific examples. In the future, methods for analyzing and manipulating attention in machine translation (Lee et al., 2017;Liu et al., 2018b;Bau et al., 2019;Voita et al., 2019) can also be applied to text encoders.\n\nRecently, interpreting attention as explanation has been questioned (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019;Clark et al., 2019b). The ongoing discussion suggests that this method may still be insufficient for uncovering the rationale for predictions, which is critical for real-world applications.", "filtered_refids": [["b100", "b27", null, "b99", "b38"], [null, "b113"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 949, "num_references": 7}
{"corpusid_sectionid": "222124957-s17", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Input Manipulation 13", "section": "Input manipulation draws conclusions by recasting the probing task format into the form of the pre-training task and observing the model's predictions. As discussed in \u00a73, word prediction (cloze task) is a popular objective. This method has been used to investigate syntactic and semantic knowledge (Goldberg, 2019; Ettinger, 2020; Kassner and Sch\u00fctze, 2019). For a specific probing task, Warstadt et al. (2019) show that cloze and diagnostic classifiers draw similar conclusions. As input manipulation is not affected by variables introduced by probing tasks and is as interpretable than inspection, we suggest more focus on this method: either by creating new datasets (Warstadt et al., 2020) or recasting existing ones (Brown et al., 2020) into this format. A disadvantage of this method (especially for smaller models) is the dependence on both the pattern used to elicit an answer from the model and, in the few-shot case where a couple examples are provided first, highly dependent on the examples (Schick and Sch\u00fctze, 2020).", "filtered_refids": [["b111", "b16", "b110"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1031, "num_references": 3}
{"corpusid_sectionid": "222124957-s19", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Area V: Multilinguality", "section": "The majority of research on text encoders has been in English. 15 Cross-lingual shared representations have been proposed as an efficient way to target multiple languages by using multilingual text for pretraining (Mulcaire et al., 2019;Devlin et al., 2019;Lample and Conneau, 2019;Liu et al., 2020c, inter alia). For evaluation, researchers have devised multilingual benchmarks mirroring those for NLU in English (Conneau et al., 2018b;Liang et al., 2020;Hu et al., 2020). Surprisingly, without any explicit cross-lingual signal, these models achieve strong zero-shot cross-lingual performance, outperforming prior cross-lingual word embedding-based methods (Wu and Dredze, 2019;Pires et al., 2019).\n\nA natural follow-up question to ask is why these models learn cross-lingual representations. Some answers include the shared subword vocabulary (Pires et al., 2019;Wu and Dredze, 2019), shared Transformer layers (Conneau et al., 2020b;Artetxe et al., 2020) across languages, and depth of the network (K et al., 2020). Studies have also found the geometry of representations of different languages in the multilingual encoders can be aligned with linear transformations (Schuster et al., 2019;Wang et al., 2019e, 2020cLiu et al., 2019b), which has also been observed in independent monolingual encoders (Conneau et al., 2020b). These alignments can be further improved (Cao et al., 2020b).", "filtered_refids": [["b115", "b25", "b50", "b31", "b59", null, "b7"], ["b115", "b59", "b77", null, "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1390, "num_references": 12}
{"corpusid_sectionid": "222124957-s20", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Evaluating Multilinguality", "section": "All of the areas discussed in this paper are applicable to multilingual encoders. However, progress in training, architecture, datasets, and evaluations are occurring concurrently, making it difficult to draw conclusions. We need more comparisons between competitive multilingual and monolingual systems or datasets. To this end, Wu and Dredze (2020) find that monolingual BERTs in low-resource languages are outperformed by multilingual BERT. Additionally, as zero-shot (or few-shot) cross-lingual transfer has inherently high variance (Keung et al., 2020), the variance of models should also be reported.\n\nWe anticipate cross-lingual performance being a new dimension to consider when evaluating text representations. For example, it will be exciting to discover how a small, highly-performant mono- 15 Of the monolingual encoders in other languages, core research in modeling has only been performed so far for a few non-English languages (Sun et al., 2019b(Sun et al., , 2020a lingual encoder contrasts against a multilingual variant; e.g., what is the minimum number of parameters needed to support a new language? Or, how does model size relate to the phylogenetic diversity of languages supported?", "filtered_refids": [["b116", "b18"], ["b91", null, "b90"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1204, "num_references": 5}
{"corpusid_sectionid": "52011136-s1", "title": "A review of Spanish corpora annotated with negation", "date": "2018-08-01", "section_title": "Negation in Spanish", "section": "Processing negation is not as easy as using a list of negation markers and applying look-up methods. They can be used to find out potential negation cues but they are not adequate because the presence of a cue does not imply that it acts as a negation. In the sentence \"You bought the car to use it, didn't you?\" the cue \"not\" is not used as a negation but it is used to reinforce the first part of the sentence. Moreover, it is also necessary to identify the scope or part of the sentence affected by the negation and its focus, the part more prominently negated. If we want to advance in the study of this phenomenon, as for most of NLP tasks, the availability of annotated corpora is essential to train algorithms. According to existing resources for English, annotating negation involves the annotation of the following aspects:\n\n\u2022 Negation cue: lexical item(s) that modify the truth value of the propositions that are within its scope.\n\nThere are different types of negation according to the type of the negation cue used: . It is also known as affixal negation.\n\n\u2022 Scope: the part of the sentence affected by the negation cue . The scope can be continuous or discontinuous.\n\n\u2022 Focus: the part of the scope that is most prominently or explicitly negated (Blanco and Moldovan, 2011).\n\n\u2022 Negated event: the event that is directly negated by the negation cue, usually a verb, a noun or an adjective (Kim et al., 2008). This is just a list of the main aspects that have been annotated for negation. However, each language has specific linguistic resources to express negation and specific negation structures, which should also be reflected in the information annotated in corpora. As we will show in Section 4, most existing annotation schemes for Spanish do not account for the complexity of the linguistic structures used to express negation that are present in texts. This happens mainly because of two reasons: first, annotation of negation started with the annotation of clinical reports in English (Chapman et al., 2001;Goldin and Chapman, 2003;Mutalik et al., 2001a), where there is not too much variation of negation structures. Second, corpora have been created for specific purposes, such as extracting negated clinical events, and not with the intention of accounting for all the linguistic complexity of the negation phenomenon.\n\nAn exception to this is the SFU Review SP -NEG corpus (Jim\u00e9nez-Zafra et al., 2018;. The guidelines specify a great variety of negation patterns at the syntactic level that we summarize below. Additionally, the guidelines also specify expressions that involve a negation cue but do not express negation.\n\nOn the one hand, patterns that express negation can be divided into three categories:\n\n1. Simple negation markers, if they are composed of only one single negation marker (i.e. no ['no/not'], nunca ['never']).\n\n3. Negation markers in contrastive constructions, if negation markers are used to counterpose different ideas, to correct something, to introduce new information or to express obligation, rather than to express negation (i.e. No hay m\u00e1s soluci\u00f3n que comprar una lavadora ['There is no other solution than to buy a washing machine']). 4. Negation markers in comparative constructions, if negation markers are used to compare some property with something, that is, negation is used to place an entity below or above another entity on a scale (i.e. No es tan grande como me lo imaginaba ['It is not as big as I imagined']).", "filtered_refids": [[], [], [], [], ["b1"], ["b15", "b9", "b5", "b3"], ["b8"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3479, "num_references": 6}
{"corpusid_sectionid": "52011136-s3", "title": "A review of Spanish corpora annotated with negation", "date": "2018-08-01", "section_title": "UAM Spanish Treebank", "section": "The first Spanish corpus annotated with negation that we are aware of is the UAM Spanish Treebank (Moreno et al., 2003), which was enriched with the annotation of negation cues and their scopes (Sandoval and Salazar, 2013). The initial UAM Spanish Treebank consisted of 1,500 sentences extracted from newspaper articles (El Pa\u00eds Digital and Compra Maestra) that were annotated syntactically. Trees were encoded in a nested structure, including syntactic category, syntactic and semantic features, and constituent nodes, following the Penn Treebank model. Later, this version of the corpus was extended with the annotation of negation and 10.67% of the sentences were found to contain negations (160 sentences).\n\nIn this corpus, syntactic negation was annotated but not lexical nor morphological negation. It was annotated by two experts in corpus linguistics who followed similar guidelines to those of Bioscope corpus Vincze, 2010). They included negation cues within the scope as in Bioscope and NegDDI-DrugBank (Bokharaeian et al., 2014). All the arguments of the negated events were also included in the scope of negation, including the subject, which was excluded from the scope in active sentences in Bioscope. There is no information about inter-annotator agreement.\n\nThe UAM Spanish Treebank corpus is freely available at http://www.lllf.uam.es/ESP/ Treebank.html. It is in XML format, negation cues are tagged with the label Type=\"NEG\" and the scope of negation is tagged with the label Neg=\"YES\" in the syntactic constituent on which negation acts.", "filtered_refids": [["b14"], ["b23", "b2"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1558, "num_references": 3}
{"corpusid_sectionid": "52011136-s5", "title": "A review of Spanish corpora annotated with negation", "date": "2018-08-01", "section_title": "SFU Review SP -NEG", "section": "The SFU Review SP -NEG 9 (Jim\u00e9nez-Zafra et al., 2018) is the first Spanish corpus that includes the event in the annotation of negation and that takes into account discontinuous negation markers. Moreover, it is the first corpus where the effect of the negation on the words that are within its scope is annotated, that is, whether there is a change in the polarity or an increment or reduction of its value. It is an extension of the Spanish part of the SFU Review corpus (Taboada et al., 2006) and it could be considered as the counterpart of the SFU Review Corpus with negation and speculation annotations 10 (Konstantinova et al., 2012).\n\nThe Spanish SFU Review corpus consists of 400 reviews extracted from the website Ciao.es that belong to 8 different domains: cars, hotels, washing machines, books, cell phones, music, computers, and movies. For each domain there are 50 positive and 50 negative reviews, defined as positive or negative based on the number of stars given by the reviewer (1-2=negative; 4-5=positive; 3-star review were not included). Later, it was extended to the SFU Review SP -NEG corpus in which each review was automatically annotated at the token level with POS-tags and lemmas, and manually annotated at the sentence level with negation cues and their corresponding scopes and events. It is composed of 9,455 sentences, out of which 3,022 sentences (31.97%) contain at least one negation marker.\n\nIn this corpus, syntactic negation was annotated but not lexical nor morphological negation, as in the UAM Spanish Treebank corpus. Unlike this one, annotations on the event and on how negation affects the polarity of the words within its scope were included. The annotations were performed by two senior researchers with in-depth experience in corpus annotation who supervised the whole process and two trained annotators who carried out the annotation task. The Kappa coefficient for inter-annotator agreement was of 0.97 for negation cues, 0.95 for negated events and 0.94 for scopes. 11 A detailed discussion of the main sources of disagreements can be found in (Jim\u00e9nez-Zafra et al., 2016).\n\nThe guidelines of the Bioscope corpus were taken into account but after a thorough analysis of negation in Spanish, a typology of Spanish negation patterns was defined . As in Bioscope, NegDDI-DrugBank and UAM Spanish Treebank, negation markers were included within the scope. Moreover, the subject was also included within the scope when the word directly affected by negation is the verb of the sentence, as in ConanDoyle-neg corpus (Morante and Daelemans, 2012). The event was also included in the scope of negation as in ConanDoyle-neg corpus.\n\nThe SFU Review SP -NEG is publicly available and can be downloaded at http://sinai.ujaen. es/sfu-review-sp-neg-2/.", "filtered_refids": [["b21", "b10"], [], ["b7"], ["b13"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2788, "num_references": 4}
{"corpusid_sectionid": "52011136-s7", "title": "A review of Spanish corpora annotated with negation", "date": "2018-08-01", "section_title": "IULA Spanish Clinical Record", "section": "The IULA Spanish Clinical Record corpus (Marimon et al., 2017) contains 300 anonymized clinical records from several services of one of the main hospitals in Barcelona (Spain) that was annotated with negation markers and their scopes. It contains 3,194 sentences, out of which 1,093 (34.22%) were annotated with negation cues.\n\nIn this corpus, syntactic negation and lexical negation were annotated but not morphological negation. It was annotated with negation cues and their scopes by three computational linguists annotators advised by a clinician. The inter-annotator agreement Kappa rates were 0.85 between annotators 1 and 2, and annotators 1 and 3; and 0.88 between annotators 2 and 3. The authors defined their own annotation guidelines taking into account the currently existing guidelines for corpora in English (Mutalik et al., 2001b;Morante and Daelemans, 2012). Differently from previous work, they did not include the negation cue nor the subject in the scope (except when the subject is located after the verb).\n\nThe corpus is publicly available with a CC-BY-SA 3.0 license and it can be downloaded at http: //eines.iula.upf.edu/brat//#/NegationOnCR_IULA/.", "filtered_refids": [["b11"], ["b16", "b13"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1171, "num_references": 3}
{"corpusid_sectionid": "47019063-s1", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "The concept of semantic shifts", "section": "Human languages change over time, due to a variety of linguistic and non-linguistic factors and at all levels of linguistic analysis. In the field of theoretical (diachronic) linguistics, much attention has been devoted to expressing regularities of linguistic change. For instance, laws of phonological change have been formulated (e.g., Grimm's law or the great vowel shift) to account for changes in the linguistic sound system. When it comes to lexical semantics, linguists have studied the evolution of word meaning over time, describing so-called lexical semantic shifts or semantic change, which Bloomfield (1933) defines as \"innovations which change the lexical meaning rather than the grammatical function of a form.\" Historically, much of the theoretical work on semantic shifts has been devoted to documenting and categorizing various types of semantic shifts (Br\u00e9al, 1899;Stern, 1931;Bloomfield, 1933). The categorization found in Bloomfield (1933) is arguably the most used and has inspired a number of more recent studies (Blank and Koch, 1999;Geeraerts, 1997;Traugott and Dasher, 2001). Bloomfield (1933) originally proposed nine classes of semantic shifts, six of which are complimentary pairs along a dimension. For instance, the pair 'narrowing' -'broadening' describes the observation that word meaning often changes to become either more specific or more general, e.g. Old English mete 'food' becomes English meat 'edible flesh,' or that the more general English word dog is derived from Middle English dogge which described a dog of a particular breed. Bloomfield (1933) also describes change along the spectrum from positive to negative, describing the speaker's attitude as one of either degeneration or elevation, e.g. from Old English cniht 'boy, servant' to the more elevated knight.\n\nThe driving forces of semantic shifts are varied, but include linguistic, psychological, sociocultural or cultural/encyclopedic causes (Blank and Koch, 1999;Grzega and Schoener, 2007). Linguistic processes that cause semantic shifts generally involve the interaction between words of the vocabulary and their meanings. This may be illustrated by the process of ellipsis, whereby the meaning of one word is transferred to a word with which it frequently co-occurs, or by the need for discrimination of synonyms caused by lexical borrowings from other languages. Semantic shifts may be also be caused by changes in the attitudes of speakers or in the general environment of the speakers. Thus, semantic shifts are naturally separated into two important classes: linguistic drifts (slow and regular changes in core meaning of words) and cultural shifts (culturally determined changes in associations of a given word). Researchers studying semantic shifts from a computational point of view have shown the existence of this division empirically (Hamilton et al., 2016c). In the traditional classification of Stern (1931), the semantic shift category of substitution describes a change that has a non-linguistic cause, namely that of technologi-cal progress. This may be exemplified by the word car which shifted its meaning from non-motorized vehicles after the introduction of the automobile.\n\nThe availability of large corpora have enabled the development of new methodologies for the study of lexical semantic shifts within general linguistics (Traugott, 2017). A key assumption in much of this work is that changes in a word's collocational patterns reflect changes in word meaning (Hilpert, 2008), thus providing a usage-based account of semantics (Gries, 1999). For instance, Kerremans et al. (2010) study the very recent neologism detweet, showing the development of two separate usages/meanings for this word ('to delete from twitter,' vs 'to avoid tweeting') based on large amounts of web-crawled data. The usage-based view of lexical semantics aligns well with the assumptions underlying the distributional semantic approach (Firth, 1957) often employed in NLP . Here, the time spans studied are often considerably shorter (decades, rather than centuries) and we find that these distributional methods seem well suited for monitoring the gradual process of meaning change. Gulordava and Baroni (2011), for instance, showed that distributional models capture cultural shifts, like the word sleep acquiring more negative connotations related to sleep disorders, when comparing its 1960s contexts to its 1990s contexts.\n\nTo sum up, semantic shifts are often reflected in large corpora through change in the context of the word which is undergoing a shift, as measured by co-occurring words. It is thus natural to try to detect semantic shifts automatically, in a 'data-driven' way. This vein of research is what we cover in the present survey. In the following sections, we overview the methods currently used for the automatic detection of semantic shifts and the recent academic achievements related to this problem.", "filtered_refids": [["b59", null, "b7", "b62", "b5", "b17"], ["b59", "b19", "b23", "b5"], ["b15", "b63", "b18", "b28", "b20", "b33"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 4932, "num_references": 16}
{"corpusid_sectionid": "47019063-s4", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "Training data", "section": "The time unit (the granularity of the temporal dimension) can be chosen before slicing the text collection into subcorpora. Earlier works dealt mainly with long-term semantic shifts (spanning decades or even centuries), as they are easier to trace. One of the early examples is Sagi et al. (2011) who studied differences between Early Middle, Late Middle and Early Modern English, using the Helsinki Corpus (Rissanen and others, 1993).\n\nThe release of the Google Books Ngrams corpus 1 played an important role in the development of the field and spurred work on the new discipline of 'culturomics,' studying human culture through digital media (Michel et al., 2011). Mihalcea and Nastase (2012) used this dataset to detect differences in word usage and meaning across 50-years time spans, while Gulordava and Baroni (2011) compared word meanings in the 1960s and in the 1990s, achieving good correlation with human judgments. Unfortunately, Google Ngrams is inherently limited in that it does not contain full texts. However, for many cases, this corpus was enough, and its usage as the source of diachronic data continued in Mitra et al. (2014) (employing syntactic ngrams), who detected word sense changes over several different time periods spanning from 3 to 200 years.\n\nIn more recent work, time spans tend to decrease in size and become more granular. In general, corpora with smaller time spans are useful for analyzing socio-cultural semantic shifts, while corpora with longer spans are necessary for the study of linguistically motivated semantic shifts. As researchers are attempting to trace increasingly subtle cultural semantic shifts (more relevant for practical tasks), the granularity of time spans is decreasing: for example, Kim et al. (2014) and Liao and Cheng (2016) analyzed the yearly changes of words. Note that, instead of using granular 'bins', time can also be represented as a continuous differentiable value (Rosenfeld and Erk, 2018).\n\nIn addition to the Google Ngrams dataset (with granularity of 5 years), Kulkarni et al. (2015) used Amazon Movie Reviews (with granularity of 1 year) and Twitter data (with granularity of 1 month). Their results indicated that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collection, the New-York Times Annotated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA 2 ), with time slices equal to one decade. Hamilton et al. (2016a) continued the usage of COHA (along with the Google Ngrams corpus). Kutuzov et al. (2017b) started to employ the yearly slices of the English Gigaword corpus (Parker et al., 2011) in the analysis of cultural semantic drift related to armed conflicts.", "filtered_refids": [[null, "b57"], ["b45", "b20", "b44", "b48"], ["b34", "b41", "b55"], ["b39", "b58", "b14", "b50", "b35", "b21", "b69", "b68"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3171, "num_references": 17}
{"corpusid_sectionid": "47019063-s5", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "Test sets", "section": "Diachronic corpora are needed not only as a source of training data for developing semantic shift detection systems, but also as a source of test sets to evaluate such systems. In this case, however, the situation is more complicated. Ideally, diachronic approaches should be evaluated on human-annotated lists of semantically shifted words (ranked by the degree of the shift). However, such gold standard data is difficult to obtain, even for English, let alone for other languages. General linguistics research on language change like that of Traugott and Dasher (2001) and others usually contain only a small number of hand-picked examples, which is not sufficient to properly evaluate an automatic unsupervised system.\n\nVarious ways of overcoming this problem have been proposed. For example, Mihalcea and Nastase (2012) evaluated the ability of a system to detect the time span that specific contexts of a word undergoing a shift belong to (word epoch disambiguation). A similar problem was offered as SemEval-2015 Task 7: 'Diachronic Text Evaluation' (Popescu and Strapparava, 2015). Another possible evaluation method is so-called cross-time alignment, where a system has to find equivalents for certain words in different time periods (for example, 'Obama' in 2015 corresponds to 'Trump' in 2017). There exist several datasets containing such temporal equivalents for English (Yao et al., 2018). Yet another evaluation strategy is to use the detected diachronic semantic shifts to trace or predict real-world events like armed conflicts (Kutuzov et al., 2017b). Unfortunately, all these evaluation methods still require the existence of large manually annotated semantic shift datasets. The work to properly create and curate such datasets is in its infancy.\n\nOne reported approach to avoid this requirement is borrowed from research on word sense disambiguation and consists of making a synthetic task by merging two real words together and then modifying the training and test data according to a predefined sense-shifting function. Rosenfeld and Erk (2018) successfully employed this approach to evaluate their system; however, it still operates on synthetic words, limiting the ability of this evaluation scheme to measure the models' performance with regards to real semantic shift data. Thus, the problem of evaluating semantic shift detection approaches is far from being solved, and practitioners often rely on self-created test sets, or even simply manually inspecting the results.", "filtered_refids": [["b62"], ["b53", "b39", "b68"], ["b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2498, "num_references": 5}
{"corpusid_sectionid": "47019063-s6", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "Methodology of extracting semantic shifts from data", "section": "After settling on a diachronic data set to be used in the system, one has to choose the methods to analyze it. Before the broad adoption of word embedding models, it was quite common to use change in raw word frequencies in order to trace semantic shifts or other kinds of linguistic change; see, among others, Juola (2003), Hilpert and Gries (2009), Michel et al. (2011), Lijffijt et al. (2012, or Choi and Varian (2012) for frequency analysis of words in web search queries. Researchers also studied the increase or decrease in the frequency of a word A collocating with another word B over time, and based on this inferred changes in the meaning of A (Heyer et al., 2009).\n\nHowever, it is clear that semantic shifts are not always accompanied with changes in word frequency (or this connection may be very subtle and non-direct). Thus, if one were able to more directly model word meaning, such an approach should be superior to frequency-proxied methods. A number of recent publications have showed that distributional word representations (Turney et al., 2010;Baroni et al., 2014) provide an efficient way to solve these tasks. They represent meaning with sparse or dense (embedding) vectors, produced from word co-occurrence counts. Although conceptually the source of the data for these models is still word frequencies, they 'compress' this information into continuous lexical representations which are both efficient and convenient to work with. Indeed, Kulkarni et al. (2015) explicitly demonstrated that distributional models outperform the frequency-based methods in detecting semantic shifts. They managed to trace semantic shifts more precisely and with greater explanatory power. One of the examples from their work is the semantic evolution of the word gay: through time, its nearest semantic neighbors changed, manifesting the gradual move away from the sense of 'cheerful' to the sense of 'homosexual.'\n\nIn fact, distributional models were being used in diachronic research long before the paper of Kulkarni et al. (2015), although there was no rigorous comparison to the frequentist methods. Already in 2009, it was proposed that one can use distributional methods to detect semantic shifts in a quantitative way. The pioneering work by Jurgens and Stevens (2009) described an insightful conceptualization of a sequence of distributional model updates through time: it is effectively a Word:Semantic Vector:Time tensor, in the sense that each word in a distributional model possesses a set of semantic vectors for each time span we are interested in. It paved the way for quantitatively comparing not only words with regard to their meaning, but also different stages in the development of word meaning over time. Jurgens and Stevens (2009) employed the Random Indexing (RI) algorithm (Kanerva et al., 2000) to create word vectors. Two years later, Gulordava and Baroni (2011) used explicit count-based models, consisting of sparse co-occurrence matrices weighted by Local Mutual Information, while Sagi et al. (2011) turned to Latent Semantic Analysis (Deerwester et al., 1990). In Basile et al. (2014), an extension to RI dubbed Temporal Random Indexing (TRI) was proposed. However, no quantitative evaluation of this approach was offered (only a few hand-picked examples based on the Italian texts from the Gutenberg Project), and thus it is unclear whether TRI is any better than other distributional models for the task of semantic shift detection.\n\nFurther on, the diversity of the employed methods started to increase. For example, Mitra et al. (2014) analyzed clusters of the word similarity graph in the subcorpora corresponding to different time periods. Their distributional model consisted of lexical nodes in the graphs connected with weighted edges. The weights corresponded to the number of shared most salient syntactic dependency contexts, where saliency was determined by co-occurrence counts scaled by Mutual Information (MI). Importantly, they were able to detect not only the mere fact of a semantic shift, but also its type: the birth of a new sense, splitting of an old sense into several new ones, or merging of several senses into one. Thus, this work goes into a much less represented class of 'fine-grained' approaches to semantic shift detection. It is also important that Mitra et al. (2014) handle natively the issue of polysemous words, putting the much-neglected problem of word senses in the spotlight.\n\nThe work of Kim et al. (2014) was seminal in the sense that it is arguably the first one employing prediction-based word embedding models to trace diachronic semantic shifts. Particularly, they used incremental updates (see below) and Continuous Skipgram with negative sampling (SGNS) (Mikolov et al., 2013a). 3 Hamilton et al. (2016a) showed the superiority of SGNS over explicit PPMI-based distributional models in semantic shifts analysis, although they noted that low-rank SVD approximations (Bullinaria and Levy, 2007) can perform on par with SGNS, especially on smaller datasets. Since then, the majority of publications in the field started using dense word representations: either in the form of SVD-factorized PPMI matrices, or in the form of prediction-based shallow neural models like SGNS. 4 There are some works employing other distributional approaches to semantic shifts detection. For instance, there is a strong vein of research based on dynamic topic modeling (Blei and Lafferty, 2006;Wang and McCallum, 2006), which learns the evolution of topics over time. In Wijaya and Yeniterzi (2011), it helped solve a typical digital humanities task of finding traces of real-world events in the texts. Heyer et al. (2016) employed topic analysis to trace the so-called 'context volatility' of words. In the political science, topic models are also sometimes used as proxies to social trends developing over time: for example, Mueller and Rauh (2017) employed LDA to predict timing of civil wars and armed conflicts. Frermann and Lapata (2016) drew on these ideas to trace diachronic word senses development. But most scholars nowadays seem to prefer parametric distributional models, particularly predictionbased embedding algorithms like SGNS, CBOW or GloVe (Pennington et al., 2014). Following their widespread adoption in NLP in general, they have become the dominant representations for the analysis of diachronic semantic shifts as well.", "filtered_refids": [["b44", "b9", "b29", "b27", "b24", "b42"], ["b64", "b2", "b35"], ["b57", "b10", "b4", "b20", "b35", "b32", "b30"], ["b48"], ["b52", "b6", "b34", "b46", "b25", "b49", "b21", null, "b8", "b65"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 37, "num_chars": 6408, "num_references": 27}
{"corpusid_sectionid": "47019063-s7", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "Comparing vectors across time", "section": "It is rather straightforward to train separate word embedding models using time-specific corpora containing texts from several different time periods. As a consequence, these models are also time-specific. However, it is not that straightforward to compare word vectors across different models.\n\nIt usually does not make sense to, for example, directly calculate cosine similarities between embeddings of one and the same word in two different models. The reason is that most modern word embedding algorithms are inherently stochastic and the resulting embedding sets are invariant under rotation. Thus, even when trained on the same data, separate learning runs will produce entirely different numerical vectors (though with roughly the same pairwise similarities between vectors for particular words). This is expressed even stronger for models trained on different corpora. It means that even if word meaning is completely stable, the direct cosine similarity between its vectors from different time periods can still be quite low, simply because the random initializations of the two models were different. To alleviate this, Kulkarni et al. (2015) suggested that before calculating similarities, one should first align the models to fit them in one vector space, using linear transformations preserving general vector space structure. After that, cosine similarities across models become meaningful and can be used as indicators of semantic shifts. They also proposed constructing the time series of a word embedding over time, which allows for the detection of 'bursts' in its meaning with the Mean Shift model (Taylor, 2000). Notably, almost simultaneously the idea of aligning diachronic word embedding models using a distance-preserving projection technique was proposed by Zhang et al. (2015). Later, Zhang et al. (2016) expanded on this by adding the so called 'local anchors': that is, they used both linear projections for the whole models and small sets of nearest neighbors for mapping the query words to their correct temporal counterparts.\n\nInstead of aligning their diachronic models using linear transformations, Eger and Mehler (2016) compared word meaning using so-called 'second-order embeddings,' that is, the vectors of words' similarities to all other words in the shared vocabulary of all models. This approach does not require any transformations: basically, one simply analyzes the word's position compared to other words. At the same time, Hamilton et al. (2016a) and Hamilton et al. (2016c) showed that these two approaches can be used simultaneously: they employed both 'second order embeddings' and orthogonal Procrustes transformations to align diachronic models.\n\nRecently, it was shown in Bamler and Mandt (2017) ('dynamic skip-gram' model) and Yao et al. (2018) ('dynamic Word2Vec' model) that it is possible to learn the word embeddings across several time periods jointly, enforcing alignment across all of them simultaneously, and positioning all the models in the same vector space in one step. This develops the idea of model alignment even further and eliminates the need to first learn separate embeddings for each time period, and then align subsequent model pairs. Bamler and Mandt (2017) additionally describe two variations of their approach: a) for the cases when data slices arrive sequentially, as in streaming applications, where one can not use future observations, and b) for the cases when data slices are available all at once, allowing for training on the whole sequence from the very beginning. A similar approach is taken by Rosenfeld and Erk (2018) who train a deep neural network on word and time representations. Word vectors in this setup turn into linear transformations applied to a continuous time variable, and thus producing an embedding of word w at time t.\n\nYet another way to make the models comparable is made possible by the fact that prediction-based word embedding approaches (as well as RI) allow for incremental updates of the models with new data without any modifications. This is not the case for the traditional explicit count-based algorithms, which usually require a computationally expensive dimensionality reduction step. Kim et al. (2014) proposed the idea of incrementally updated diachronic embedding models: that is, they train a model on the year y i , and then the model for the year y i+1 is initialized with the word vectors from y i . This can be considered as an alternative to model alignment: instead of aligning models trained from scratch on different time periods, one starts with training a model on the diachronically first period, and then updates this same model with the data from the successive time periods, saving its state each time. Thus, all the models are inherently related to each other, which, again, makes it possible to directly calculate cosine similarities between the same word in different time period models, or at least makes the models more comparable.\n\nSeveral works have appeared recently which aim to address the technical issues accompanying this approach of incremental updating. Among others, Peng et al. (2017) described a novel method of incrementally learning the hierarchical softmax function for the CBOW and Continuous Skipgram algorithms. In this way, one can update word embedding models with new data and new vocabulary much more efficiently, achieving faster training than when doing it from scratch, while at the same time preserving comparable performance. Continuing this line of research, Kaji and Kobayashi (2017) proposed a conceptually similar incremental extension for negative sampling, which is a method of training examples selection, widely used with prediction-based models as a faster replacement for hierarchical softmax.\n\nEven after the models for different time periods are made comparable in this or that way, one still has to choose the exact method of comparing word vectors across these models. Hamilton et al. (2016a) and Hamilton et al. (2016c) made an important observation that the distinction between linguistic and cultural semantic shifts is correlated with the distinction between global and local embedding comparison methods. The former take into account the whole model (for example, 'second-order embeddings,' when we compare the word's similarities to all other words in the lexicon), while the latter focus on the word's immediate neighborhood (for example, when comparing the lists of k nearest neighbors). They concluded that global measures are sensitive to regular processes of linguistic shifts, while local measures are better suited to detect slight cultural shifts in word meaning. Thus, the choice of particular embedding comparison approach should depend on what type of semantic shifts one seeks to detect.", "filtered_refids": [[], ["b69", "b70", "b61", "b35"], ["b21", "b23"], ["b1", "b68", "b55"], ["b34"], ["b51", "b31"], ["b21", "b23"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 35, "num_chars": 6791, "num_references": 14}
{"corpusid_sectionid": "47019063-s8", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "Laws of semantic change", "section": "The use of diachronic word embeddings for studying the dynamics of word meaning has resulted in several hypothesized 'laws' of semantic change. We review some of these law-like generalizations below, before finally describing a study that questions their validity. Dubossarsky et al. (2015) experimented with K-means clustering applied to SGNS embeddings trained for evenly sized yearly samples for the period 1850-2009. They found that the degree of semantic change for a given word -quantified as the change in self-similarity over time -negatively correlates with its distance to the centroid of its cluster. They proposed that the likelihood for semantic shift correlates with the degree of prototypicality (the 'law of prototypicality' in Dubossarsky et al. (2017)).\n\nAnother relevant study is reported by Eger and Mehler (2016), based on two different graph models; one being a time-series model relating embeddings across time periods to model semantic shifts and the other modeling the self-similarity of words across time. Experiments were performed with time-indexed historical corpora of English, German and Latin, using time-periods corresponding to decades, years and centuries, respectively. To enable comparison of embeddings across time, second-order embeddings encoding similarities to other words were used, as described in 3.3, limited to the 'core vocabulary' (words occurring at least 100 times in all time periods). Based on linear relationships observed in the graphs, Eger and Mehler (2016) postulate two 'laws' of semantic change:\n\n1. word vectors can be expressed as linear combinations of their neighbors in previous time periods;\n\n2. the meaning of words tend to decay linearly in time, in terms of the similarity of a word to itself; this is in line with the 'law of differentiation' proposed by Xu and Kemp (2015).\n\nIn another study, Hamilton et al. (2016a) considered historical corpora for English, German, French and Chinese, spanning 200 years and using time spans of decades. The goal was to investigate the role of frequency and polysemy with respect to semantic shifts. As in Eger and Mehler (2016), the rate of semantic change was quantified by self-similarity across time-points (with words represented by Procrustes-aligned SVD embeddings). Through a regression analysis, Hamilton et al. (2016a) investigated how the change rates correlate with frequency and polysemy, and proposed another two 'laws':\n\n1. frequent words change more slowly ('the law of conformity'); 2. polysemous words (controlled for frequency) change more quickly ('the law of innovation'). Azarbonyad et al. (2017) showed that these laws (at least the law of conformity) hold not only for diachronic corpora, but also for other 'viewpoints': for example, semantic shifts across models trained on texts produced by different political actors or written in different genres (Kutuzov et al., 2016). However, the temporal dimension allows for a view of the corpora under analysis as a sequence, making the notion of 'semantic shift' more meaningful.\n\nLater, Dubossarsky et al. (2017) questioned the validity of some of these proposed 'laws' of semantic change. In a series of replication and control experiments, they demonstrated that some of the regularities observed in previous studies are largely artifacts of the models used and frequency effects. In particular, they considered 10-year bins comprising equally sized yearly samples from Google Books 5-grams of English fiction for the period 1990-1999. For control experiments, they constructed two additional data sets; one with chronologically shuffled data where each bin contains data from all decades evenly distributed, and one synchronous variant containing repeated random samples from the year 1999 alone. Any measured semantic shifts within these two alternative data sets would have to be due to random sampling noise. Dubossarsky et al. (2017) performed experiments using raw co-occurrence counts, PPMI weighted counts, and SVD transformations (Procrustes aligned), and conclude that the 'laws' proposed in previous studies -that semantic change is correlated with frequency, polysemy (Hamilton et al., 2016a) and prototypicality (Dubossarsky et al., 2015) -are not valid as they are also observed in the control conditions. Dubossarsky et al. (2017) suggested that these spurious effects are instead due to the type of word representation used -count vectors -and that semantic shifts must be explained by a more diverse set of factors than distributional ones alone. Thus, the discussion on the existence of the 'laws of semantic change' manifested by distributional trends is still open.", "filtered_refids": [["b11", "b13"], ["b14"], [], ["b67"], ["b14", "b21"], ["b37", "b0"], ["b11", "b21", "b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4665, "num_references": 11}
{"corpusid_sectionid": "47019063-s9", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "Diachronic semantic relations", "section": "Word embedding models are known to successfully capture complex relationships between concepts, as manifested in the well-known word analogies task (Mikolov et al., 2013a), where a model must 'solve' equations of the form 'A is to B is as C is to what?' A famous example is the distributional model capturing the fact that the relation between 'man' and 'woman' is the same as between 'king' and 'queen' (by adding and subtracting the corresponding word vectors). Thus, it is a natural development to investigate whether changes in semantic relationships across time can also be traced by looking at the diachronic development of distributional models. Zhang et al. (2015) considered the temporal correspondences problem, wherein the objective is to identify the word in a target time period which corresponds to a query term in the source time period (for example, given the query term iPod in the 2000s, the counterpart term in the 1980s time period is Walkman). This is proposed as a means to improve the results of information retrieval from document collections with significant time spans. Szymanski (2017) frames this as the temporal word analogy problem, extending the word analogies concept into the temporal dimension. This work shows that diachronic word embeddings can successfully model relations like 'word w 1 at time period t \u03b1 is like word w 2 at time period t \u03b2 '. To this end, embedding models trained on different time periods are aligned using linear transformations. Then, the temporal analogies are solved by simply finding out which word vector in the time period t \u03b2 is the closest to the vector of w 1 in the time period t \u03b1 . A variation of this task was studied in Rosin et al. (2017), where the authors learn the relatedness of words over time, answering queries like 'in which time period were the words Obama and president maximally related'. This technique can be used for a more efficient user query expansion in generalpurpose search engines. Kutuzov et al. (2017a) modeled a different semantic relation: 'words w 1 and w 2 at time period t \u03b1 are in the same semantic relation as words w 3 and w 4 at time period t \u03b2 '. To trace the temporal dynamics of these relations, they re-applied linear projections learned on sets of w 1 and w 2 pairs from the model for the period t n to the model trained on the subsequent time period t n+1 . This was used to solve the task of detecting lasting or emerging armed conflicts and the violent groups involved in these conflicts.", "filtered_refids": [["b56", "b60", "b46", "b69", "b38"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2502, "num_references": 5}
{"corpusid_sectionid": "47019063-s10", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "Applications", "section": "Applications of diachronic word embeddings approaches can generally be grouped into two broad categories: linguistic studies which investigate the how and why of semantic shifts, and event detection approaches which mine text data for actionable purposes.\n\nThe first category generally involves corpora with longer time spans, since linguistic changes happen at a relatively slow pace. Some examples falling into this category include tracking semantic drift of particular words (Kulkarni et al., 2015) or of word sentiment (Hamilton et al., 2016b), identifying the breakpoints between epochs (Sagi et al., 2011;Mihalcea and Nastase, 2012), studying the laws of semantic change at scale (Hamilton et al., 2016c) and finding different words with similar meanings at different points in time (Szymanski, 2017). This has been held up as a good use case of deep learning for research in computational linguistics (Manning, 2015), and there are opportunities for future work applying diachronic word embeddings not only in the field of historical linguistics, but also in related areas like sociolinguistics and digital humanities.\n\nThe second category involves mining texts for cultural semantic shifts (usually on shorter time spans) indicating real-world events. Examples of this category are temporal information retrieval (Rosin et al., 2017), predicting civil turmoils (Kutuzov et al., 2017b;Mueller and Rauh, 2017), or tracing the popularity of entities using norms of word vectors (Yao et al., 2018). They can potentially be employed to improve user experience in production systems or for policy-making in governmental structures.\n\nWe believe that the near future will see a more diverse landscape of applications for diachronic word embeddings, especially related to the real-time analysis of large-scale news streams. 'Between the lines,' these data sources contain a tremendous amount of information about processes in our world, manifested in semantic shifts of various sorts. The task of researchers is to reveal this information and make it reliable and practically useful.", "filtered_refids": [[], ["b43", "b57", "b60", "b35", "b45", "b22", "b23"], ["b49", "b68", "b39", "b56"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2083, "num_references": 11}
{"corpusid_sectionid": "47019063-s11", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "Open challenges", "section": "The study of temporal aspects of semantic shifts using distributional models (including word embeddings) is far from being a solved problem. The field still has a considerable number of open challenges. Below we briefly describe the most demanding ones.\n\n\u2022 The existing methods should be expanded to a wider scope of languages. Hamilton et al. (2016a), Kutuzov and Kuzmenko (2018) and others have started to analyze other languages, but the overwhelming majority of publications still apply only to English corpora. It might be the case that the best methodologies are the same for different languages, but this should be shown empirically.\n\n\u2022 There is a clear need to devise algorithms that work on small datasets, as they are very common in historical linguistics, digital humanities, and similar disciplines.\n\n\u2022 Carefully designed and robust gold standard test sets of semantic shifts (of different kinds) should be created. This is a difficult task in itself, but the experience from synchronic word embeddings evaluation (Hill et al., 2015) and other NLP areas proves that it is possible.\n\n\u2022 There is a need for rigorous formal mathematical models of diachronic embeddings. Arguably, this will follow the vein of research in joint learning across several time spans, started by Bamler and Mandt (2017) and Yao et al. (2018), but other directions are also open.\n\n\u2022 Most current studies stop after stating the simple fact that a semantic shift has occurred. However, more detailed analysis of the nature of the shift is needed. This includes:\n\n1. Sub-classification of types of semantic shifts (broadening, narrowing, etc). This problem was to some degree addressed by Mitra et al. (2014), but much more work is certainly required to empirically test classification schemes proposed in much of the theoretical work described in Section 2. 2. Identifying the source of a shift (for example, linguistic or extra-linguistic causes). This causation detection is closely linked to the division between linguistic drifts and cultural shifts, as proposed in Hamilton et al. (2016c). 3. Quantifying the weight of senses acquired over time. Many words are polysemous, and the relative importance of senses is flexible (Frermann and Lapata, 2016). The issue of handling senses is central for detecting semantic shifts, but most of the algorithms described in this survey are not sense-aware. To address this, methods from sense embeddings research (Bartunov et al., 2016) might be employed. 4. Identifying groups of words that shift together in correlated ways. Some work in this direction was started in Dubossarsky et al. (2016), who showed that verbs change more than nouns, and nouns change more than adjectives. This is also naturally related to proving the (non-)existence of the 'laws of semantic change' (see Section 4).\n\n\u2022 Last but not least, we believe that the community around diachronic word embeddings research severely lacks relevant forums, like topical workshops or shared tasks. Diachronic text evaluation tasks like the one at SemEval-2015 (Popescu and Strapparava, 2015) are important but not enough, since they focus on identifying the time period when a text was authored, not the process of shifting meanings of a word. Organizing such events can promote the field and help address many of the challenges described above.", "filtered_refids": [[], ["b21", "b36"], [], ["b26"], ["b1", "b68"], [], ["b12", "b48", "b23", "b16", "b3"], ["b53"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 3337, "num_references": 11}
{"corpusid_sectionid": "49587276-s3", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models", "date": "2018-08-01", "section_title": "NER datasets", "section": "Since the first shared task on NER (Grishman and Sundheim, 1996) 1 , many shared tasks and datasets for NER have been created. CoNLL 2002 (Tjong Kim Sang, 2002) 2 and CoNLL 2003(Tjong Kim Sang and De Meulder, 2003 3 were created from newswire articles in four different languages (Spanish, Dutch, English, and German) and focused on 4 entities -PER (person), LOC (location), ORG (organization) and MISC (miscellaneous including all other types of entities).\n\nNER shared tasks have also been organized for a variety of other languages, including Indian languages (Rajeev Sangal and Singh, 2008), Arabic (Shaalan, 2014), German (Benikova et al., 2014), and slavic languages (Piskorski et al., 2017). The named entity types vary widely by source of dataset and language. For example, Rajeev Sangal and Singh (2008)'s southeast Asian language data has named entity types person, designation, temporal expressions, abbreviations, object number, brand, etc. Benikova et al. (2014)'s data, which is based on German wikipedia and online news, has named entity types similar to that of CoNLL 2002 and 2003: PERson, ORGanization, LOCation and OTHer. The shared task 4 or-ganized by Piskorski et al. (2017) covering 7 slavic languages (Croatian, Czech, Polish, Russian, Slovak, Slovene, Ukrainian) also has person, location, organization and miscellaneous as named entity types.\n\nIn the biomedical domain, Kim et al. (2004) organized a BioNER task on MedLine abstracts, focusing on protien, DNA, RNA and cell attribute entity types. Uzuner et al. (2007) presented a clinical note de-identification task that required NER to locate personal patient data phrases to be anonymized. The 2010 I2B2 NER task 5 (Uzuner et al., 2011), which considered clinical data, focused on clinical problem, test and treatment entity types. Segura Bedmar et al. (2013) organized a Drug NER shared task 6 as part of SemEval 2013 Task 9, which focused on drug, brand, group and drug n (unapproved or new drugs) entity types. (Krallinger et al., 2015) introduced the similar CHEMDNER task 7 focusing more on chemical and drug entities like trivial, systematic, abbreviation, formula, family, identifier, etc. Biology and microbiology NER datasets 8 (Hirschman et al., 2005;Bossy et al., 2013;Del\u0117ger et al., 2016) have been collected from PubMed and biology websites, and focus mostly on bacteria, habitat and geolocation entities. In biomedical NER systems, segmentation of clinical and drug entities is considered to be a difficult task because of complex orthographic structures of named entities (Liu et al., 2015).\n\nNER tasks have also been organized on social media data, e.g., Twitter, where the performance of classic NER systems degrades due to issues like variability in orthography and presence of grammatically incomplete sentences (Baldwin et al., 2015). Entity types on Twitter are also more variable (person, company, facility, band, sportsteam, movie, TV show, etc.) as they are based on user behavior on Twitter.\n\nThough most named entity annotations are flat, some datasets include more complex structures. Ohta et al. (2002) constructed a dataset of nested named entities, where one named entity can contain another. Strassel et al. (2003) highlighted both entity and entity head phrases. And discontinuous entities are common in chemical and clinical NER datasets (Krallinger et al., 2015). Eltyeb and Salim (2014) presented an survey of various NER systems developed for such NER datasets with a focus on chemical NER. Grishman and Sundheim (1996) scored NER performance based on type, whether the predicted label was correct regardless of entity boundaries, and text, whether the predicted entity boundaries were correct regardless of the label. For each score category, precision was defined as the number of entities a system predicted correctly divided by the number that the system predicted, recall was defined as the number of entities a system predicted correctly divided by the number that were identified by the human annotators, and (micro) F-score was defined as the harmonic mean of precision and recall from both type and text.", "filtered_refids": [["b72", null, "b25", "b71"], [null, "b67", "b56", "b5"], ["b28", "b66", "b30", "b74", "b42", "b33", "b8", "b73", "b17"], ["b4"], ["b52", "b25", "b20", "b69", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 4128, "num_references": 23}
{"corpusid_sectionid": "49587276-s4", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models", "date": "2018-08-01", "section_title": "NER evaluation metrics", "section": "The exact match metrics introduced by CoNLL (Tjong Kim Sang and De Meulder, 2003;Tjong Kim Sang, 2002) considers a prediction to be correct only when the predicted label for the complete entity is matched to exactly the same words as the gold label of that entity. CoNLL also used (micro) F-score, taking the harmonic mean of the exact match precision and recall.\n\nThe relaxed F1 and strict F1 metrics have been used in many NER shared tasks (Segura Bedmar et al., 2013;Krallinger et al., 2015;Bossy et al., 2013;Del\u0117ger et al., 2016). Relaxed F1 considers a prediction to be correct as long as part of the named entity is identified correctly. Strict F1 requires the character offsets of a prediction and the human annotation to match exactly. In these data, unlike CoNLL, word offsets are not given, so relaxed F1 is intended to allow comparison despite different systems having different word boundaries due to different segmentation techniques (Liu et al., 2015).", "filtered_refids": [["b72", "b71"], ["b66", "b42", "b33", "b8", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 967, "num_references": 7}
{"corpusid_sectionid": "49587276-s6", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models", "date": "2018-08-01", "section_title": "Unsupervised and bootstrapped systems", "section": "Some of the earliest systems required very minimal training data. Collins and Singer (1999) used only labeled seeds, and 7 features including orthography (e.g., capitalization), context of the entity, words contained within named entities, etc. for classifying and extracting named entities. Etzioni et al. (2005) proposed an unsupervised system to improve the recall of NER systems applying 8 generic pattern extractors to open web text, e.g., NP is a <class1>, NP1 such as NPList2. Nadeau et al. (2006) presented an unsupervised system for gazetteer building and named entity ambiguity resolution based on Etzioni et al. (2005) and Collins and Singer (1999) that combined an extracted gazetteer with commonly available gazetteers to achieve F-scores of 88%, 61%, and 59% on MUC-7 (Chinchor and Robinson, 1997) location, person, and organization entities, respectively.\n\nZhang and Elhadad (2013) used shallow syntactic knowledge and inverse document frequency (IDF) for an unsupervised NER system on biology (Kim et al., 2004) and medical (Uzuner et al., 2011) data, achieving 53.8% and 69.5% accuracy, respectively. Their model uses seeds to discover text having potential named entities, detects noun phrases and filters any with low IDF values, and feeds the filtered list to a classifier (Alfonseca and Manandhar, 2002) to predict named entity tags.", "filtered_refids": [["b11", "b22", "b13", "b50"], ["b30", "b1", "b74"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1354, "num_references": 7}
{"corpusid_sectionid": "49587276-s7", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models", "date": "2018-08-01", "section_title": "Feature-engineered supervised systems", "section": "Supervised machine learning models learn to make predictions by training on example inputs and their expected outputs, and can be used to replace human curated rules. Hidden Markov Models (HMM), Support Vector Machines (SVM), Conditional Random Fields (CRF), and decision trees were common machine learning systems for NER. Zhou and Su (2002) used HMM (Rabiner and Juang, 1986;Bikel et al., 1997) an NER system on MUC-6 and MUC-7 data, achieving 96.6% and 94.1% F score, respectively. They included 11 orthographic features (1 numeral, 2 numeral, 4 numeral, all caps, numerals and alphabets, contains underscore or not, etc.) a list of trigger words for the named entities (e.g., 36 trigger words and affixes, like river, for the location entity class), and a list of words (10000 for the person entity class) from various gazetteers. Malouf (2002) compared the HMM with Maximum Entropy (ME) by adding multiple features. Their best model included capitalization, whether a word was the first in a sentence, whether a word had appeared before with a known last name, and 13281 first names collected from various dictionaries. The model achieved 73.66%, 68.08% Fscore on Spanish and Dutch CoNLL 2002 dataset respectively.\n\nThe winner of CoNLL 2002 (Carreras et al., 2002) used binary AdaBoost classifiers, a boosting algorithm that combines small fixed-depth decision trees (Schapire, 2013). They used features like capitalization, trigger words, previous tag prediction, bag of words, gazetteers, etc. to represent simple binary relations and these relations were used in conjunction with previously predicted labels. They achieved 81.39% and 77.05% F scores on the Spanish and Dutch CoNLL 2002 datasets, respectively. Li et al. (2005) implemented a SVM model on the CoNLL 2003 dataset and CMU seminar documents. They experimented with multiple window sizes, features (orthographic, prefixes suffixes, labels, etc.) from neighboring words, weighting neighboring word features according to their position, and class weights to balance positive and negative class. They used two SVM classifiers, one for detecting named entity starts and one for detecting ends. They achieved 88.3% F score on the English CoNLL 2003 data.\n\nOn the MUC6 data, Takeuchi and Collier (2002) used part-of-speech (POS) tags, orthographic features, a window of 3 words to the left and to the right of the central word, and tags of the last 3 words as features to the SVM. The final tag was decided by the voting of multiple one-vs-one SVM outputs.\n\nAndo and Zhang (2005a) implemented structural learning (Ando and Zhang, 2005b) to divide the main task into many auxiliary tasks, for example, predicting labels by looking just at the context and masking the current word. The best classifier for each auxiliary task was selected based on its confidence. This model had achieved 89.31% and 75.27% F score on English and German, respectively.\n\nAgerri and Rigau (2016) developed a semi-supervised system 9 by presenting NER classifiers with features including orthography, character n-grams, lexicons, prefixes, suffixes, bigrams, trigrams, and unsupervised cluster features from the Brown corpus, Clark corpus and k-means clustering of open text using word embeddings (Mikolov et al., 2013). They achieved near state of the art performance on CoNLL datasets: 84.16%, 85.04%, 91.36%, 76.42% on Spanish, Dutch, English, and German, respectively.\n\nIn 6.4 Feature-inferring neural network systems Collobert and Weston (2008) proposed one of the first neural network architectures for NER, with feature vectors constructed from orthographic features (e.g., capitalization of the first character), dictionaries and lexicons. Later work replaced these manually constructed feature vectors with word embeddings (Collobert et al., 2011), which are representations of words in n-dimensional space, typically learned over large collections of unlabeled data through an unsupervised process such as the skip-gram model (Mikolov et al., 2013). Studies have shown the importance of such pre-trained word embeddings for neural network based NER systems (Habibi et al., 2017), and similarly for pre-trained character embeddings in character-based languages like Chinese (Li et al., 2015;Yin et al., 2016).\n\nModern neural architectures for NER can be broadly classified into categories depending upon their representation of the words in a sentence. For example, representations may be based on words, characters, other sub-word units or any combination of these.", "filtered_refids": [["b59", "b45", "b7", "b82"], ["b9", "b38"], ["b70"], ["b2", "b3"], ["b47"], ["b15", "b39", "b26", "b14", "b47", "b80"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 4515, "num_references": 16}
{"corpusid_sectionid": "49587276-s9", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models", "date": "2018-08-01", "section_title": "Character level architectures", "section": "In this model, a sentence is taken to be a sequence of characters. This sequence is passed through an RNN, predicting labels for each character (Figure 2). Character labels transformed into word labels via post processing. The potential of character NER neural models was first highlighted by Kim et al. (2016) using highway networks over convolution neural networks (CNN) on character sequences of words and then using another layer of LSTM + softmax for the final predictions.\n\nThis model was implemented by Pham and Le-Hong (2017) for Vietnamese NER and achieved 80.23% F-score on Nguyen et al. (2016)'s Vietnamese test data. Character models were also used in various other languages like Chinese (Dong et al., 2016) where it has achieved near state of the art performance. Kuru et al. (2016) proposed CharNER 11 which implemented the character RNN model for NER on 7 different languages. In this character model, tag prediction over characters were converted to word tags using Viterbi decoder (Forney, 1973) Ling et al. (2015) proposed word representation using RNN (Bi-LSTM) over characters of the word and achieved state of the art results on POS task using this representation in multiple languages including 97.78% accuracy on English PTB (Marcus et al., 1993). Gillick et al. (2015) implemented sequence to sequence model (Byte to Span-BTS) using encoder decoder architecture over sequence of characters of words in a window of 60 characters. Each character was encoded in bytes and BTS achieved high performance on CoNLL 2002 and 2003 dataset without any feature engineering. BTS achieved 82.95%, 82.84%,86.50%,76.22% Fscore on Spanish, Dutch, English and German CoNLL datasets respectively.", "filtered_refids": [["b31"], ["b34", "b46", "b41", "b55", null, "b19", "b23"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1703, "num_references": 8}
{"corpusid_sectionid": "49587276-s10", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models", "date": "2018-08-01", "section_title": "Character+Word level architectures", "section": "Systems combining word context and the characters of a word have proved to be strong NER systems that need little domain specific knowledge or resources. There are two base models in this category. The first type of model represents words as a combination of a word embedding and a convolution over the characters of the word, follows this with a Bi-LSTM layer over the word representations of a sentence, and finally uses a softmax or CRF layer over the Bi-LSTM to generate labels. The architecture diagram for this model is same as Figure 3 but with the character Bi-LSTM replaced with a CNN 12 . Ma and Hovy (2016) implemented this model to achieve 91.21% F1 score on the CoNLL 2003 English dataset and 97.55% POS-tagging accuracy on the WSJ portion of PTB (Marcus et al., 1993). They also showed lower performance by this model for out of vocabulary words.\n\nChiu and Nichols (2015)  This model has also been utilized for NER in languages like Japanese where Misawa et al. (2017) showed that this architecture outperformed other neural architectures on the organization entity class. Limsopatham and Collier (2016) implemented a character+word level NER model for Twitter NER (Baldwin et al., 2015) by concatenating a CNN over characters, a CNN over orthographic features of characters, a word embedding, and a word orthographic feature embedding. This concatenated representation is passed through another Bi-LSTM layer and the output is given to CRF for predicting. This model achieved 65.89% F score on segmentation alone and 52.41% F score on segmentation and categorization. Santos and Guimaraes (2015) implemented a model with a CNN over the characters of word, concatenated with word embeddings of the central word and its neighbors, fed to a feed forward network, and followed by the Viterbi algorithm to predict labels for each word. The model achieved 82.21% F score on Spanish CoNLL 2002 data and 71.23% F score on Portuguese NER data (Santos and Cardoso, 2007).\n\nThe second type of model concatenates word embeddings with LSTMs (sometimes bi-directional) over the characters of a word, passing this representation through another sentence-level Bi-LSTM, and predicting the final tags using a final softmax or CRF layer (Figure 3). Lample et al. (2016) 13 introduced this architecture and achieved 85.75%, 81.74%, 90.94%, 78.76% Fscores on Spanish, Dutch, English and German NER dataset respectively from CoNLL 2002 and 2003. Dernoncourt et al. (2017) implemented this model in the NeuroNER toolkit 14 with the main goal of providing easy usability and allowing easy plotting of real time performance and learning statistics of the model. The BRAT annotation tool 15 is also integrated with NeuroNER to ease the development of NN NER models in new domains. NeuroNER achieved 90.50% F score on the English CoNLL 2003 data. Habibi et al. (2017) implemented the model for various biomedical NER tasks and achieved higher performance than the majority of other participants. For example, they achieved 83.71 F-score on the CHEMDNER data (Krallinger et al., 2015). Bharadwaj et al. (2016) 16 utilized phonemes (from Epitran) for NER in addition to characters and words. They also utilize attention knowledge over sequence of characters in word which is concatenated with the word embedding and character representation of word. This model achieved state of the art performance (85.81% F score) on Spanish CoNLL 2002 dataset. A slightly improved system focusing on multi-task and multi-lingual joint learning was proposed by Yang et al. (2016) where word representation given by GRU (Gated Recurrent Unit) cell over characters plus word embedding was passed through another RNN layer and the output was given to CRF models trained for different tasks like POS, chunking and NER. Yang et al. (2017) Kim Sang, 2002;Cucerzan and Yarowsky, 2002(Tjong Kim Sang and De Meulder, 2003 and for biomedical NER (Saha et al., 2009), but had not been used in neural NER systems. They extended the Lample et al. (2016) character+word model to learn affix embeddings 17 alongside the word embeddings and character RNNs (Figure 4). They considered all n-gram prefixes and suffixes of words in the training corpus, and selected only those whose frequency was above a threshold, T . Their word+character+affix model achieved 87.26%, 87.54%, 90.86%, 79.01% on Spanish, Dutch, English and German CoNLL datasets respectively. Yadav et al. (2018) also showed that affix embeddings capture complementary information to that captured by RNNs over the characters of a word, that selecting only high frequency (realistic) affixes was important, and that embedding affixes was better than simply expanding the other embeddings to reach a similar number of hyper-parameters. Table 1 shows the results of all the different categories of systems discussed in section 6 on the CoNLL 2002 and 2003 datasets. The table also indicates, for each model, whether it makes use of external knowledge like a dictionary or gazetteer. Table 2 presents a similar analysis on the DrugNER dataset from SemEval 2013 task 9 (Segura Bedmar et al., 2013). Our first finding from the survey is that feature-inferring NN systems outperform feature-engineered systems, despite the latter's access to domain specific rules, knowledge, features, and lexicons. For example, the best feature-engineered system for Spanish, Agerri and Rigau (2016), is 1.59% below the best feature-inferring neural network system, (Lample et al., 2016), and 1.65% below the best neural network system that incorporates lexical resources (Bharadwaj et al., 2016). Similarly, the best featureengineered system for German, Agerri and Rigau (2016), is 2.34% below the best feature-inferring neural network system, Lample et al. (2016). The differences are smaller for Dutch and English, but in neither case is the best feature-engineered model better than the best neural network model. In DrugNER, the word+character NN model outperforms the feature engineered system by 8.90% on MedLine test data and 3.50% on the overall dataset.", "filtered_refids": [["b44", "b46"], ["b63", "b4", "b40", "b48", "b64"], ["b6", "b26", "b18", "b78", "b35", "b79", "b71", null, "b72", "b16", "b33", "b76", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 37, "num_chars": 6062, "num_references": 20}
{"corpusid_sectionid": "49215220-s2", "title": "A Survey on Open Information Extraction", "date": "2018-06-14", "section_title": "Learning-based Systems", "section": "The line of work on Open IE begins with TEXTRUNNER (Banko et al., 2007), a self-supervised learning approach consisting of three modules. First, given a small sample of sentences from the Penn Treebank, the learner applies a dependency parser to heuristically identify and label a set of extractions as positive and negative training examples. This data is then used as input to a Naive Bayes classifier which learns a model of trustworthy relations using unlexicalized POS and noun phrase (NP) chunk features. The selfsupervised nature mitigates the need for hand-labeled training data, and unlexicalized features help scale to the multitudes of relations found on the Web. The second component, the extractor, then generates candidate tuples by first identifying pairs of NP arguments and then heuristically designating each word in between as part of a relation phrase or not. Next, each candidate extraction is presented to the classifier, whereupon only those labeled as trustworthy are kept. Restricting to the use of shallow features in this step makes TEXTRUNNER highly efficient. Finally, a redundancy-based assessor assigns a probability to each retained tuple based on the number of sentences from which each extraction was found, thus exploiting the redundancy of information in Web text and assigning higher confidence to extractions that occur multiple times. Figure 2: OLLIE's system architecture (Mausam et al., 2012). OLLIE begins with seed tuples from REVERB, uses them to build a bootstrap learning set, and learns open pattern templates. These are applied to individual sentences at extraction time.\n\nWOE (Wu and Weld, 2010) also learns an open information extractor without direct supervision. It makes use of Wikipedia as a source of training data by bootstrapping from entries in Wikipedia infoboxes, i.e. by heuristically matching infobox attribute-value pairs with corresponding sentences in the article. This data is then used to learn extraction patterns on both POS tags (WOE pos ) and dependency parses (WOE parse ). Former extractor utilizes a linear-chain Conditional Random Field (CRF) to train a model of relations on shallow features which outputs certain text between two NPs when it denotes a relation. Latter approach, in contrast, makes use of dependency trees to build a classifier that decides whether the shortest dependency path between two NPs indicates a semantic relation. By operating over dependency parses, even long-range dependencies can be captured. Accordingly, when comparing their two approaches, Wu and Weld (2010) show that the use of dependency features results in an increase in precision and recall over shallow linguistic features, though, at the cost of extraction speed, hence negatively affecting the scalability of the system. OLLIE (Mausam et al., 2012) follows the idea of bootstrap learning of patterns based on dependency parse paths. However, while WOE relies on Wikipedia-based bootstrapping, OLLIE applies a set of high precision seed tuples from its predecessor system REVERB (see section 2.2) to bootstrap a large training set over which it learns a set of extraction pattern templates using dependency parses (see Figure 2). In contrast to previously presented systems that fully ignore the context of a tuple and thus extract propositions that are not asserted as factual, but are only hypothetical or conditionally true, OLLIE includes a context-analysis step in which contextual information from the input sentence around an extraction is analyzed to expand the output representation by adding attribution and clausal modifiers, if necessary, and thus increasing the precision of the system (see extractions (1) and (2) in Figure 1; for details, see section 2.4). Moreover, OLLIE is the first Open IE approach to identify not only verb-based relations, but also relationships mediated by nouns and adjectives (see extractions (3) and (4) in Figure 1). In that way, it expands the syntactic scope of relational phrases to cover a wider range of relation expressions, resulting in a much higher yield (at comparable precision) as compared to previous systems.\n\nMore recently, Yahya et al. (2014) proposed ReNoun, an Open IE system that entirely focuses on the extraction of noun-mediated relations. Starting with a small set of high-precision seed facts relying on manually specified lexical patterns that are specifically tailored for NPs, a set of dependency parse patterns for the extraction of noun-based relations is learned with the help of distant supervision (Mintz et al., 2009). These patterns are then applied to generate a set of candidate extractions which are assigned a confidence score based on the frequency and coherence of the patterns producing them.", "filtered_refids": [["b20", "b3"], ["b20", "b40"], ["b24", "b42"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 4745, "num_references": 6}
{"corpusid_sectionid": "49215220-s3", "title": "A Survey on Open Information Extraction", "date": "2018-06-14", "section_title": "Rule-based Systems", "section": "The second category of Open IE systems make use of hand-crafted extraction rules. This includes RE-VERB (Fader et al., 2011), a shallow extractor that addresses three common errors of hitherto existing Open IE systems: the output of such systems frequently contains a great many of uninformative extractions (i.e. extractions that omit critical information), incoherent extractions (i.e. extractions where the relational phrase has no meaningful interpretation) and overly-specific relations that convey too much information to be useful in further downstream semantic tasks. REVERB improves over those approaches by introducing a syntactic constraint that is expressed in terms of a simple POS-based regular expres-sion (see Figure 3), covering about 85% of verb-based relational phrases in English text, as a linguistic analysis has revealed. In that way, the amount of incoherent and uninformative extractions is reduced. Moreover, in order to avoid overspecified relational phrases, a lexical constraint is presented which is based on the idea that a valid relational phrase should take many distinct arguments in a large corpus. Besides, while formerly proposed approaches start with the identification of candidate argument pairs, REVERB follows a relation-centric approach by first determining relational phrases that satisfy abovementioned constraints, and then finding a pair of NP arguments for each such phrase. An example output produced by ReVerb can be seen in Figure 1 (6-7). Whereas previously mentioned Open IE systems focus on the extraction of binary relations, commonly leading to extraction errors such as incomplete, uninformative or erroneous propositions, KRAKEN (Akbik and L\u00f6ser, 2012) is the first approach to be specifically built for capturing complete facts from sentences by gathering the full set of arguments for each relational phrase within a sentence, thus producing tuples of arbitrary arity. The identification of relational phrases and their corresponding arguments is based on hand-written extraction rules over typed dependency parses.\n\nEXEMPLAR (Mesquita et al., 2013) applies a similar approach for extracting n-ary relations, using hand-crafted patterns based on dependency parse trees to detect a relation trigger and the arguments connected to it. Based on the task of Semantic Role Labeling (SRL), whose key idea is to classify semantic constituents into different semantic roles (Christensen et al., 2010), it assigns each argument its corresponding role (such as subject, direct object or prepositional object).\n\nA more abstract approach, PROPS, was suggested by , who argue that it is hard to read out from a dependency parse the complete structure of a sentence's propositions, since, amongst others, different predications are represented in a non-uniform manner and proposition boundaries are not easy to detect. Therefore, they introduce a more semantically-oriented sentence representation that is generated by transforming a dependency parse tree into a directed graph which is tailored to directly represent the proposition structure of an input sentence. Consequently, extracting propositions from this novel output format is straightforward. The conversion of the dependency tree into the proposition structure is carried out by a rule-based converter.\n\nPredPatt (White et al., 2016) follows a similar approach. It employs a set of non-lexicalized rules defined over Universal Dependency (UD) parses (Marneffe et al., 2014) to extract predicate-argument structures. In doing so, PredPatt constructs a directed graph, where a special dependency ARG is built between the head token of a predicate and the head tokens of its arguments, while the original UD relations are preserved within predicate and argument phrases. As PredPatt uses language-agnostic patterns on UD structures, it is one of the few Open IE systems that work across different languages.", "filtered_refids": [["b11", "b1"], ["b8", "b23"], [], ["b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3912, "num_references": 5}
{"corpusid_sectionid": "49215220-s5", "title": "A Survey on Open Information Extraction", "date": "2018-06-14", "section_title": "Systems Capturing Inter-Proposition Relationships", "section": "Aforementioned Open IE systems lack the expressiveness needed for a proper interpretation of complex assertions, since they ignore the context under which a proposition is complete and correct. Thus, they do not distinguish between information asserted in a sentence and information that is only hypothetical or conditionally true. For example, extracting the relational tuple the earth; is the center of ; the universe from the sentence \"Early scientists believed that the earth is the center of the universe.\" would be inappropriate, since the input is not asserting it, but only noting that is was believed by early scientists (Mausam, 2016). To properly handle such cases, OLLIE attempts a first solution by additionally extracting an attribution context, denoting a proposition that is reported or claimed by some entity:\n\n( the earth; be the center of ; the university ; AttributedTo believe; Early astronomers)\n\nIn that way, it extends the default Open IE representation of arg 1 , rel, arg 2 with an extra field. Besides, OLLIE pays attention to clausal modifiers, such as: ( Romney; will be elected; President ; ClausalModifier if ; he wins five key states)\n\nBoth types of modifiers are identified by matching patterns with the dependency parse of the sentence. Clausal modifiers are determined by an adverbial-clause edge and filtered lexically (the first word of the clause must match a list of cue terms, e.g. if, when, or although), while attribution modifiers are identified by a clausal-complement edge whose context verb must match one of the terms given in VerbNet's list of common verbs of communication and cognition (Mausam et al., 2012). A similar output is produced by OLLIES's successor OPENIE4 (Mausam, 2016), which combines SRLIE (Christensen et al., 2010) and RELNOUN (Pal and Mausam, 2016). Former is a system that converts the output of a SRL system into an Open IE extraction by treating the verb as the relational phrase, while taking its role-labeled arguments as the Open IE argument phrases related to the relation. Latter, in contrast, represents a rule-based Open IE system that extracts noun-mediated relations, thereby paying special attention to demonyms and compound relational nouns. In addition, OPENIE4 marks temporal and spatial arguments by assigning them a T or S label, respectively. Lately, its successor OPENIE 5.0 was released 1 . It integrates BONIE (Saha et al., 2017) and OpenIEListExtractor 2 . While the former focuses on extracting tuples where one of the arguments is a number or a quantity-unit phrase, the latter targets the extraction of propositions from conjunctive sentences.\n\nSimilar to OLLIE, Bast and Haussmann (2013), who explore the use of contextual sentence decomposition (CSD) for Open IE, advocate to further specify propositions with information on which they depend. Their system CSD-IE is based on the idea of paraphrasing-based approaches described in section 2.3. Using a set of hand-crafted rules over the output of a constituent parser, a sentence is first split into sub-sequences that semantically belong together, forming so-called \"contexts\". Each such context now contains a separate fact, yet it is often dependent on surrounding contexts. In order to preserve such inter-proposition relationships, tuples may contain references to other propositions. However, as opposed to OLLIE, where additional contextual modifiers are directly assigned to the corresponding relational tuples, Bast and Haussmann (2013) represent contextual information in the form of separate, linked propositions. To do so, each extraction is given a unique identifier that can be used in the argument position of an extraction for a later substitution with the corresponding fact by a downstream application. An example for an attribution is shown below (Bast and Haussmann, 2013): #1: The Embassy; said; that #2 #2: 6,700 Americans; were; in Pakistan.\n\nAnother current approach that captures inter-proposition relationships is proposed by Bhutani et al. (2016), who present a nested representation for Open IE that is able to capture high-level dependencies, allowing for a more accurate representation of the meaning of an input sentence. Their system NESTIE uses bootstrapping over a dataset for textual entailment to learn both binary and nested triple representations for n-ary relations over dependency parse trees. These patterns can take on the form of binary triples arg 1 ; rel; arg 2 or nested triples such as arg 1 ; rel; arg 2 ; rel 2 ; arg 3 for n-ary relations. Using a set of manually defined rules, contextual links between extracted propositions are inferred from the dependency parse in order to generate a nested representation of assertions that are complete and closer in meaning to the original statement. Similar to OLLIE, contextual links are identified as clausal complements, conditionals and relation modifiers. Linked propositions are represented by arguments that refer to the corresponding propositions using identifiers, e.g. (Bhutani et al., 2016) (Gashteovski et al., 2017), another recent Open IE system, is built on top of ClausIE, a system that was found to often produce overly specific extractions. Such overly specific constituents that combine multiple, potentially semantically unrelated propositions in a single relational or argument phrase generally hurt the performance of downstream semantic applications, such as question answering or textual entailment. In fact, those approaches benefit from extractions that are as compact as possible. Therefore, MinIE aims to minimize both relational and argument phrases by identifying and removing parts that are considered overly specific. For this purpose, MinIE provides four different minimization modes which differ in their aggressiveness, thus allowing to control the trade-off between precision and recall. Moreover, it semantically annotates extractions with information about polarity, modality, attribution and quantities instead of directly representing it in the actual extractions, as the following example shows (Gashteovski et al., 2017):\n\n\"Pinocchio believes that the hero Superman was not actually born on beautiful Krypton.\" with + and -signifying positive and negative polarity, respectively.\n\nIn that way, the output generated by MinIE is further reduced to its core constituents, producing maximally shortened, semantically enriched extractions.\n\nTo further enhance the expressiveness of extracted propositions and sustain their interpretability in downstream artificial intelligence tasks, Cetto et al. (2018) propose Graphene, an Open IE framework that uses a set of hand-crafted simplification rules to transform complex natural language sentences into clean, compact structures by removing clauses and phrases that present no central information from the input and converting them into stand-alone sentences. In that way, a source sentence is transformed into a hierarchical representation in the form of core facts and accompanying contexts (Niklaus et al., 2016). In addition, inspired by the work on Rhetorical Structure Theory (Mann and Thompson, 1988), a set of syntactic and lexical patterns is used to identify the rhetorical relations by which core sentences and their associated contexts are connected in order to preserve their semantic relationships and return a set of semantically typed and interconnected relational tuples (see extractions (15-17) in Figure 1). Graphene's extraction workflow is illustrated in Figure 5.", "filtered_refids": [["b21"], [], [], ["b20", "b27", "b21", "b30", "b8"], ["b4"], ["b14", "b5"], [], [], ["b26", "b7", "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 7506, "num_references": 12}
{"corpusid_sectionid": "49215220-s6", "title": "A Survey on Open Information Extraction", "date": "2018-06-14", "section_title": "Evaluation", "section": "Though a multitude of systems for Open IE have been developed over the last decade, a clear formal specification of what constitutes a valid relational tuple is still missing. This lack of a well-defined, generally accepted task definition prevented the creation of an established, large-scale annotated corpus serving as a gold standard dataset for an objective and reproducible cross-system comparison. As a consequence, to date, Open IE systems were predominantly evaluated by hand on small-scale corpora that consist of only a few hundred sentences, thereby ignoring one of the fundamental goals of Open IE: scalability to large amounts of text. Moreover, none of the datasets that were used for assessing the  Figure 5: Graphene's extraction workflow for an example sentence (Cetto et al., 2018).\n\nperformance of different systems is widely agreed upon. As can be seen in Table 2, the corpora compiled by Del Corro and Gemulla (2013), Xu et al. (2013), Fader et al. (2011 and Banko et al. (2007) are occasionally re-used. However, new datasets are still collected, hindering a fair comparison of the proposed approaches. Besides, although Open IE methods are targeted at being domain independent and able to cope with heterogeneous datasets, the corpora used in the evaluation process are restricted to the news, Wikipedia and Web domains for the most part. Accordingly, no clear statement about the portability of the approaches to various genres of text is possible. In addition, most evaluation procedures described in the literature focus on precision-oriented metrics, while either completely ignoring recall or using some kind of proxy, such as yield, i.e. the total number of extractions labeled as correct, or coverage, i.e. the percentage of text from the input that is contained in at least one of the extractions. Hence, the absence of a standard evaluation procedure makes it hard to replicate and compare the performance of different Open IE systems. Table 2 provides a detailed overview of both the datasets and measures used for intrinsically evaluating the various approaches described above, while Table 3 shows the tasks that were used for an extrinsic evaluation of a small set of Open IE systems. In order to address aforementioned difficulties,  recently made a first attempt in standardizing the Open IE evaluation by providing a large gold benchmark corpus. It is based on a set of consensual guiding principles that underly most Open IE approaches proposed so far, as they have identified. Those principles cover the core aspects of the task of Open IE, allowing for a clearer formulation of the problem to be solved. The three key features to consider are the following:\n\nAssertedness. The assertedness principle states that extracted propositions should be asserted by the original sentence. Usually, instead of inferring propositions out of implied statements, e.g. the tuple Sam; convinced; John out of Sam; succeeded in convincing; John , Open IE systems tend to extract the full relational phrase ( Sam; succeeded in convincing; John ), incorporating matrix verbs (\"succeeded\") and other elements, such as negotiations or modals (e.g. John; could not join; the band ).\n\nMinimal Propositions. In order to serve for semantic tasks, it is beneficial for Open IE systems to extract compact, self-contained propositions that do not combine several unrelated facts. Therefore, systems should aim to generate valid propositions with minimal spans for both relation and argument slots, while preserving the meaning of the input. As an example, the coordination in the sentence \"Bell distributes electronic and building products\" should ideally yield the two propositions: Bell; distributes; electronic products and Bell; distributes; building products .\n\nCompleteness and Open Lexicon. The completeness and open lexicon principle aims to extract all relations that are asserted in the input text. This principle was one of the fundamental ideas that have been introduced in the work of Banko et al. (2007) together with the Open IE terminology. In their work, the Open IE task was defined as a domain-independent task which extracts all possible   relations from heterogeneous corpora, instead of only extracting a set of pre-specified classes of relations. The majority of current Open IE systems realize this challenge by considering all possible verbs as potential relations. Accordingly, their scope is often limited to the extraction of verbal predicates, while ignoring relations mediated by more complex syntactic constructs, such as nouns or adjectives.\n\nRealizing that above-mentioned requirements are subsumed by the task of Question Answering (QA) driven Semantic Role Labeling (SRL) (He et al., 2015),  converted the annotations of a QA-SRL dataset to an Open IE corpus, resulting in more than 10,000 extractions over 3,200 sentences from Wikipedia and the Wall Street Journal.\n\nIn addition, Schneider et al. (2017) presented RelVis, another benchmark framework for Open IE that allows for a large-scale comparative analysis of Open IE approaches. Besides Stanovsky and Dagan (2016)'s benchmark suite, it comprises the n-ary news dataset proposed in Mesquita et al. (2013), Banko et al. (2007)'s Web corpus and the Penn sentences from Xu et al. (2013). Similar to the toolkit proposed in , RelVis supports a quantitative evaluation of the performance of Open IE systems in terms of precision, recall and F 2 -score. In addition, it facilitates a manual qualitative error analysis. For this purpose, six common error classes are distinguished to which inaccurate extractions can be assigned: (1) wrong boundaries, where the relational or argument phrase is either too long or too small; (2) redundant extraction, where the proposition asserted in an extraction is already expressed in another extraction; (3) uninformative extraction, where critical information is omitted; (4) missing extraction, i.e. a false negative, where either a relation is not detected by the system or the argumentfinding heuristics choose the wrong arguments or none argument at all; (5) wrong extraction, where no meaningful interpretation of the proposition is possible; and (6) out of scope extraction, where a system yields a correct extraction that was not recognized by the authors of the gold dataset.", "filtered_refids": [["b7"], ["b11", "b41", "b3"], [], [], ["b3"], ["b15"], ["b23", "b34", "b41", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 6323, "num_references": 10}
{"corpusid_sectionid": "237099284-s3", "title": "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey", "date": 2021, "section_title": "Dialog State Tracking Challenge (DSTC)", "section": "The dialog state tracking challenge (DSTC) is a series of dialogue related challenges that serves as a common test and evaluation suite for dialogue state tracking (Williams and Young, 2007;Williams et al., 2013Williams et al., , 2016b. The challenge was later renamed as dialog system technology challenge to accommodate various other dialogue related tasks. The most widely used datasets in the context of the DST challenge are DSTC2 and DSTC3.\n\nDSTC2 and DSTC3. The dialog state tracking challenges 2 (DSTC2 - (Henderson et al., 2014a)) and 3 (DSTC3 - (Henderson et al., 2014b)) are human-machine conversation dialogue datasets collected using Amazon Mechanical Turk, respectively for the restaurant and the tourist domain.\n\nDSTC2 is a spoken dialogue dataset consisting of automatic speech recognition (ASR) hypotheses and turn-level semantic labels along with the transcriptions. The dataset consists of 1,612 dialogues for training, 506 dialogues for development, and 1,117 dialogues for testing. DSTC3 aims to evaluate DST models on their ability to track unseen slot values and on their adaptability to a new domain. For this purpose, the dataset does not contain training dialogues and consists of 2,265 dialogues for testing. Typically, the models trained on the DSTC2 dataset were evaluated with the DSTC3 dataset to estimate their performance.", "filtered_refids": [["b35", "b32", "b36"], ["b6", "b7"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1355, "num_references": 5}
{"corpusid_sectionid": "237099284-s10", "title": "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey", "date": 2021, "section_title": "Static Ontology DST Models", "section": "The main distinguishing characteristic of DST models, in our opinion, is their capacity to predict dialogue states either from a fixed set of slot-values (i.e., from a static ontology) or from a possible open set of slot-values (i.e., from a dynamic ontology).\n\nStatic ontology models rely on a fixed ontology to predict the dialogue state. This means that the set of slot-values is predefined, and that a model can only predict for those predefined values. These models typically consist of an input layer that transforms each input token into an embedding, of an encoder layer that encodes the input to a hidden state h t , and of an output layer that predicts the slot value based on h t . Considering that the set of possible slot-values is predefined, there are two approaches used for the output layer: i) a feed-forward layer, which receives the input representation and produces scores equal to the # of slot-values; ii) an output layer that receives both the input and the slot-value representations and compares them with each of the slot-value representations providing a score for each slot-value. The obtained score can then be normalized using a non-linear activation function, either softmax, to get a probability distribution over all the slot-value pairs, or sigmoid, to get the individual probability for each slot-value pair. Figure 2 shows the standard architecture of the two approaches.\n\nWe now review few challenges that have been addressed in static ontology models, including delexicalization, data-driven DST, parameter sharing, latency in prediction, and the use of pre-trained language models. Performances of the systems are all reported in Table 2.\n\nDelexicalization. Delexicalization is an effective approach adopted to counter imbalanced training data for slot-values. In this regard, the slot values in the input are replaced with labels corresponding to slot names. For instance, I want Chinese food is delexicalised as I want F.VALUE F.SLOT. It has to be noted that replacing slot-values needs a semantic dictionary listing the possible values for each slot. (Henderson et al., 2014c; has proposed a word-based DST with recurrent neural networks that uses delexicalization on top of an input representation based on Automatic Speech Recognition. This allows to improve the system robustness with respect to the user expressions mentioning slot values.\n\nData-driven DST. Although delexicalization showed to be effective, it requires additional manual feature engineering. An alternative, data-driven methodology, was proposed by the neural belief tracker (NBT) (Mrk\u0161i\u0107 et al., 2017a). Instead of delexicalizing the input, a separate module was learned to represent the slot-value pairs. Then, the slot-value representation and the input representation are passed through a binary decision maker before applying softmax activation. Similarly, a fully statistical NBT was proposed by (Mrk\u0161i\u0107 and Vuli\u0107, 2018), where a statistical update function replaces the rule-based update mechanism in NBT. The experimental results showed the statistical update function to outperform the rule-based update.\n\nParameter sharing. While the previous models consist of a separate encoder for each slot whose values have to be predicted, the DST efficiency crucially depends on the number of model parameters. In this direction, (Ren et al., 2018) proposed   StateNet, a DST sharing the parameters for all slots, thus reducing the number of model parameters.\n\nStateNet combines a n-gram input feature representation with a slot representation, and uses long short term memory (LSTM) to encode them into a single vector. The value representation is then compared with the encoded vector to obtain the score for each slot-value. A semantically specialised Paragram-SL999 (Wieting et al., 2015) was used to encode the tokens. Compared with fully statistical NBT, StateNet achieves high performance even with a rule-based update function.", "filtered_refids": [[], [], [], ["b8"], ["b19", "b18"], ["b26"], ["b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3949, "num_references": 5}
{"corpusid_sectionid": "237099284-s11", "title": "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey", "date": 2021, "section_title": "RNN and latency in DST.", "section": "A relevant issue for DST models is prediction time, due to the number of dialogue states they have to consider at each dialogue turn. (Zhong et al., 2018) combined both a shared representation and a slot-specific representation in the Global-Locally Self Attentive Dialogue State Tracker (GLAD). The GLAD model consists of an RNN-based global module, to learn global features, and a local module that learns slot-specific features. The representations of slot-values and user input are then scored using a scoring module that predicts their probability. However, GLAD needs an RNN for each slot-value representation, this way increasing the latency of the model. Further improvements on latency were proposed in GCE, Globally-Conditioned Encoder (Nouri and Hosseini-Asl, 2018), which uses only the global encoder, and in (Balaraman and Magnini, 2019), proposing a Global encoder and Slot-Attentive decoders (G-SAT). The G-SAT model uses an RNN to encode the user input and slot-specific feedforward networks to represent the slot-values.\n\nEncoders based on pre-trained LM. The use of pre-trained language models, such as BERT (Bidirectional Encoder Representation from Transformers) (Devlin et al., 2019), is meant to increase the DST capacity to capture the semantics of slot and values names. (Lee et al., 2019) proposed a slot-utterance matching belief tracker (SUMBT) using BERT to encode slots, user input, and slotvalues. The representations of the slots and of the user input are combined using multi-head attention (Vaswani et al., 2017) to obtain the input representation of the model, and then compared with the slot-value representation to obtain the probability.", "filtered_refids": [["b21", "b42"], ["b11", "b28"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1674, "num_references": 4}
{"corpusid_sectionid": "237099284-s12", "title": "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey", "date": 2021, "section_title": "Dynamic Ontology DST Models", "section": "The models discussed in Section 4 rely on a fixed slot-value set, which is assumed to be available before making the prediction. This is a severe limitation to domains where compiling the slot-value set . is costly, or the set of possible slot-values is open (e.g., DEPARTURE TIME, RESTAURANT NAME, etc.). For this reason, various studies have focused on developing models that can track slot values even if they are not defined in the ontology. Two major approaches for dynamic ontology models are: i) copy the slot value from the user input to the output; and ii) generate the slot value as the output. Figure 3 presents the schema of a model using the combination of both approaches. One significant difference between static ontology and dynamic ontology models is that while the output vocabulary in the static ontology is limited (i.e., equal to # of slot-values), in a dynamic ontology setting the output vocabulary is much larger.\n\nCopy and pointer networks. Copy mechanism (Gu et al., 2016) and pointer networks (Vinyals et al., 2015) are the main approaches in neural networks to make predictions on the input tokens. They both rely on the attention mechanism (Bahdanau et al., 2015) to obtain scores over the input tokens. (Xu and Hu, 2018) proposed an end-to-end DST architecture based on pointer networks, showing efficient tracking of unseen slot values in a datadriven approach on the DSTC2 dataset. However, since pointer networks can only make predictions on the input tokens, they cannot be directly applied for all slots and require postprocessing of predicted values.  proposed a Transferable Multi-Domain State Generator TRADE, the first generation-based DST that incorporates the copy mechanism with a slot-gate. Figure 3 shows the architecture of the TRADE model. TRADE is based on an encoder-decoder architecture consisting of a three-way classifier that predicts over probabilities ptr, none, and dontcare. If the value is not expressed, it is predicted as none, if no constraint then dontcare and, if the value is expressed in the input, then ptr is predicted by the slot-gate. On ptr prediction, the corresponding value needs to be decoded by the decoder layer (referred as state generator). The state generator layer is initialized with both the domain and the slot representation, and generates the dialogue state using a recurrent architecture. As all the parameters are shared for all slots and domains, TRADE enables the transfer of knowledge from one domain to another, which has opened research directions in zero-shot approaches for DST with promising results.", "filtered_refids": [[], [null, "b38", "b29", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2595, "num_references": 4}
{"corpusid_sectionid": "237099284-s13", "title": "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey", "date": 2021, "section_title": "Categorical and non-categorical slot-values.", "section": "DST models based on dynamic ontology are supposed to address predictions particularly for noncategorical slots, which admit an open set of values.\n\nIn this direction (Zhang et al., 2019) proposed a dual-strategy approach that can predict both over a predefined set of slot-values and can generate values based on the input dialogue. If a given slot is labeled as categorical (i.e., possible values for the slot are predefined), the output layer predicts a score over the possible slot-values, while, if the slot is labeled as non-categorical, the span (i.e., start and end positions) of the value is decoded from the input tokens. (Heck et al., 2020) proposed a triple copy strategy (TripPy) for DST. The slot-values are predicted based on one of the following three scenarios: i) explicitly expressed by the user; ii) expressed by the system and referred to by the user; and iii) expressed in an earlier dialogue turn for another domain-slot. TripPy uses a slot gate to predict the slot status and then uses a copy mechanism to predict the slot-value.\n\nFunction-based update. The approaches reported so far for dynamic ontology either use a rule-based update mechanism or they predict the complete dialogue state at each turn from scratch. A function-based update mechanism is proposed in SOM-DST, Selectively Overwriting Memory model (Kim et al., 2020), that tracks the dialogue state in memory and predicts only the dialogue state update. First, one of the four slot operations (i.e., {CARRYOVER, DELETE, DONTCARE, UP-DATE}) is initially predicted to decide the decoding strategy for the slot. CARRYOVER denotes that the slot-value from the previous dialogue state is carried over, DELETE denotes that the user retracts the slot-value and UPDATE denotes that a new slot-value needs to be predicted and updated to the dialogue state. Then, based on the state update prediction, a dialogue state is decoded.\n\nSchema-guided models. So far, all of DST approaches focus on modeling a given ontology, without considering the portability and flexibility of the model to accommodate other datasets or domains. Though some models, such as TRADE, SOM-DST, DS-Picklist and TripPy Kim et al., 2020;Zhang et al., 2019;Heck et al., 2020) can make predictions for a new domain, they are typically modeled only for the domains in a specific dataset, and the flexibility of the model to incorporate new domains is not an inherent feature. This is basically due to the different ontology schema used in each dataset, which make them incompatible. In this context, the schema-guided dataset (SGD) (see Section 3.4), puts forth a standard schema to be adopted for all domains. In SGD, a standard schema structure is adopted, slots are classified as either categorical or non-categorical, and each slot includes a brief natural language description. Then, a new dataset needs to follow this schema, which would enable the model to predict dialogue states without any change in the architecture. Several works exploit the potential of the SGD dataset. (Balaraman and Magnini, 2021) proposed a Domain Aware DST DA-DST based on  to effectively predict slot-values specific to each domain. DA-DST uses multiple multi-head attention to extract both a domain-and a slot-specific representation from the input, and then combines them to predict the dialogue state. (Chen et al., 2020) use a graph attention network exploiting the slot relations to learn the representation of the ontology schema and the input simultaneously. (Gao et al., 2019) propose a neural reading comprehension approach to DST. Here, for each slot i a question (q i : what is the value for slot i?) is formulated and treat the dialogue D t as a passage. Finally, (Le et al., 2020) propose the first non-auto-regressive DST approach (NADST) to learn the inter-dependencies across slots. This approach allows for a parallel decoding strategy to considerably reduce the latency of the models incomparison with recurrent architectures.", "filtered_refids": [[], ["b41", "b4"], ["b9"], ["b10", "b4", "b9", "b41", "b1"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 3979, "num_references": 8}
{"corpusid_sectionid": "237099284-s17", "title": "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey", "date": 2021, "section_title": "Few-shot and Zero-shot Models", "section": "Initial DST datasets were domain specific and models actually focused on effectively tracking dialogue states defined for those domains (see section 4 and 5). However, the recently published multidomain datasets and the progress in the field of NLP, have driven the DST community to propose more advanced models that can track multiple domains and even are flexible to be adapted to new domains that are not predefined in the dataset (Mrk\u0161i\u0107 et al., 2015;Ramadan et al., 2018;Rastogi et al., 2017;Zhong et al., 2018;Nouri and Hosseini-Asl, 2018). TRADE (see section 5) was the first model investigating zero-shot and few-shot learning approaches on the MultiWoZ dataset, showing promising results on multiple domains. TRADE relied on the parameter sharing across all domains and slots to improve performance for low resource domains.\n\nTo effectively represent new domains and low resource domains, pre-trained language models were used to encode the user input representation and domain/slot representations (Lee et al., 2019;Kim et al., 2020;Heck et al., 2020;Balaraman and Magnini, 2021). In addition, the schema guided dataset enabled models to be able to predict dialogue states for any domains that adopt the proposed schema, paving the way for further progress in zero-shot learning approaches for DST Balaraman and Magnini, 2021;Gao et al., 2019).\n\nFinally,  used the pre-trained T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) language model, and proposed a minimalist transfer learning approach called MinTL. Unlike other models that predict the dialogue state, MinTL generates the change in the dialogue state as a Levenshtein belief state. This unique approach showed more robust results in low resource domains.", "filtered_refids": [["b21", "b23", "b24", "b42", "b17"], ["b11", "b4", "b9", null, "b1"], ["b12", "b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1730, "num_references": 12}
{"corpusid_sectionid": "237099284-s23", "title": "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey", "date": 2021, "section_title": "Model Values Slots Schema Update", "section": "Word-based DST (Henderson et al., 2014c) Closed Closed Fixed Function Multi-domain DST (Mrk\u0161i\u0107 et al., 2015) Closed Closed Fixed Function FS-NBT (Mrk\u0161i\u0107 and Vuli\u0107, 2018) Closed Closed Fixed Function Scalable Multi-domain DST (Rastogi et al., 2017) Closed Closed Fixed Rules CNN-Delex (Wen et al., 2017) Closed Closed Fixed Rules NBT (Mrk\u0161i\u0107 et al., 2017a) Closed Closed Fixed Rules StateNet (Ren et al., 2018) Closed Open* Fixed Rules Pointer (Xu and Hu, 2018) Open Closed Fixed Rules GLAD (Zhong et al., 2018) Closed Closed Fixed Rules GCE (Nouri and Hosseini-Asl, 2018) Closed Open Fixed Rules GSAT (Balaraman and Magnini, 2019)\n\nClosed Closed Fixed Rules BERT-DST (Chao and Lane, 2019)\n\nOpen Closed Fixed Rules TRADE  Open Open* Dynamic None DS-Picklist (Zhang et al., 2019) Closed Open Fixed None SUMBT (Lee et al., 2019) Closed Open Fixed Function SST  Closed Open* Fixed Function SGD-Baseline  Open Open Dynamic Rules DA-DST (Balaraman and Magnini, 2021) Open Open Dynamic Rules SOM-DST (Kim et al., 2020) Open Open Dynamic Function TripPy (Heck et al., 2020) Open Open Dynamic Function MinTL  Open Open Dynamic Function Nerual Reading (Gao et al., 2019) Open Open Dynamic Function NARDST (Le et al., 2020) Open Open Dynamic None Table 3: Tracking approach of implemented by various DST models. * denotes the requirement of a pretrained embedding", "filtered_refids": [["b26", "b18", "b21", null, "b19", "b24", "b42", "b38", "b8", "b17"], [], ["b11", "b10", "b4", "b9", "b41", "b1"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1352, "num_references": 16}
{"corpusid_sectionid": "9089503-s2", "title": "A Review of Corpus-based Statistical Models of Language Variation", "date": "2015-10-01", "section_title": "Modeling phonetic variation", "section": "This vein of corpus-based language variation research first started with studies on phonetic variationprobably because phonetic features are readily quantifiable. Some of the pioneering works on English pronunciation variation were completed around the turn of the century (Bell et al. 2009;Fosler-Lussier and Morgan 1999;Gregory, et al. 1999;Jurafsky et al. 1998Jurafsky et al. , 2001a, among others), with phonetic data from the Switchboard corpus of telephone conversations (Godfrey et al. 1992), which contains 240 hours of speech (of which 4 hours are phonetically transcribed and used in the statistical models).\n\nThe studies above mostly examined word duration and vowel pronunciation (full vs. reduced) as parameters of pronunciation variation. In addition to describing the general picture of variation, these studies were also deeply interested in the effects of probabilistic factors (e.g. word frequency, contextual probability, etc) on pronunciation variation. The results presented in these studies are cited as empirical support for the general claim that probabilistic relations have profound influence on the representation and production of words in speech (Jurafsky et al., 2001b) Later on, with the completion of the Buckeye corpus (Pitt et al., 2007), which contains 40 hours of phonetically transcribed conversational speech, another batch of corpus-based phonetic variation studies appeared (Johnson, 2004;Gahl et al., 2012;Yao, 2009Yao, , 2011. Since the Buckeye corpus is recorded in a studio, the recording quality is high enough to warrant automatic measurement of VOT (Yao, 2009) and vowel formants . This allows for modeling of gradient vowel dispersion, measured by the distance between a specific vowel token from the center of the vowel space on a F1-F2 plane (Bradlow et al., 1996). Furthermore, some of the variation studies based on the Buckeye corpus (Gahl et al., 2012;Yao, 2011) focused on the effects of a particular lexical measure called phonological neighborhood density. Phonological neighborhood density refers to the number of similar-sounding words given a specific target word. Thus, the models built in these studies had one critical predictor (i.e. phonological neighborhood density), and all the other non-neighborhood predictors were included as control variables. Results from these studies revealed the effects of phonological neighborhood structure in word production when all other factors that could also influence word production were statistically controlled.\n\nIn addition to English, corpus-based pronunciation variation research has also been conducted in other languages (Dutch: Pluymaekers et al., 2005, among others;French: Meunier and Espesser, 2011;Yao and Meunier, 2014;Taiwan Southern Min: Myers and Li, 2009).", "filtered_refids": [["b12", "b10", "b9", "b13", "b7", "b1"], ["b11", "b14", "b22", "b23", "b8", "b2", "b17"], [null, "b26"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2777, "num_references": 15}
{"corpusid_sectionid": "9089503-s3", "title": "A Review of Corpus-based Statistical Models of Language Variation", "date": "2015-10-01", "section_title": "Modeling syntactic variation", "section": "The work on modeling syntactic variation started later than the work on modeling phonetic variation. Most of the pioneering works were done by Bresnan and her colleagues at Stanford (Bresnan, 2007;Bresnan et al., 2007;Bresnan and Ford, 2010;Tily et al., 2009;Wolk et al. 2011, etc) on dative variation (e.g. I gave John a book vs. I gave a book to John) and genitive variation (e.g. John's book vs. the book of John) in English. For the American English data, Bresnan and colleagues also used the Switchboard corpus. Since syntactic variation has a discrete set of variants (i.e. different sentence forms), the phenomenon is modelled by generalized regression models. Bresnan and colleagues' work showed that the choice of the surface form under investigation was predictable from a set of factors relating to different components in the local sentence (e.g. semantic type of the verb, NP accessibility, pronominality, definiteness, syntactic complexity, etc) and the context (e.g. presence of parallel structures). When taking all the factors into consideration, Bresnan et al.'s models can correctly predict the surface dative/gentive form in more than 90% of the cases (compare with a baseline accuracy around 79%). Variation patterns revealed in Bresnan et al.'s works were later confirmed in behavioral experiments (e.g. Bresnan and Ford, 2010).\n\nInspired by Bresnan and colleagues' work on English syntactic variation, there have also been a few studies that apply a similar modeling approach to the study of syntactic variation in Chinese languages (Cantonese: Starr, 2015;Mandarin: Yao, 2014;Yao and Liu, 2010).\n\nIn particular, Yao and colleagues (Yao, 2014;Yao and Liu, 2010) investigated both dative variation and BA-form variation in written Mandarin using data from the Academia Sinica corpus (Chen et al., 1996). Sentence patterns involved in Mandarin dative-variation (e.g. \u6211\u9001\u5c0f \u5f20\u4e00\u672c\u4e66 'I gave Xiaozhang a book' vs. \u6211\u9001\u4e00\u672c \u4e66\u7ed9\u5c0f\u5f20 'I gave a book to Xiaozhang' vs. \u6211\u628a\u4e00 \u672c\u4e66\u9001\u7ed9\u5c0f\u5f20 'I (BA) a book gave to Xiaozhang') are more complicated than those in English. In addition to the two dative constructions similar to those in English, Mandarin Chinese also allows the direct object to be preposed before the verb. Yao and Liu' work showed that the three-way dative variation in Mandarin Chinese can be modeled by a hierarchy of two models: one on the upper level for the pre-verbal vs. post-verbal distinction and the other on the lower level for the dative vs. double object distinction. Yao and Liu' models raise the prediction accuracy by 27% (upper level) and 7% (lower level) compared to the baseline accuracy levels.\n\nFurthermore, to understand the general properties of the pre-verbal vs. post-verbal word order variation, Yao also built general models on syntactic variation between BA and non-BA sentences. The results from this study showed that the surface word order in Mandarin Chinese is most significantly influenced by the prominence (accessibility, definiteness, etc) and length of the NP, as well as the presence of a similar word order in the nearby context (i.e. parallel structure).", "filtered_refids": [["b4", "b20", null, "b5", "b3"], [null, "b25", "b18"], ["b24", "b6", "b25"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 3101, "num_references": 11}
{"corpusid_sectionid": "262460726-s2", "title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning", "date": "2023-09-22", "section_title": "Prompts for Encoder-only Transformer Models (BERT)", "section": "Before the advent of in-context learning, some research efforts have been devoted to studying how to design effective prompts to enhance the performance of BERT models.As depicted in Figure 2, prompts in BERT are usually combined with input to form a cloze-style structure, while for transformer decoder-based models, prompts are more flexible.\n\nNumerous studies have investigated prompt design in BERT.In the work by (Jiang et al., 2020), the authors proposed heuristic-based approaches for designing discrete prompts.Dependency parsing is employed to identify useful prompts from Wikipedia.In (Gao et al., 2021), the authors utilized T5 as a prompt generator with a beam search to create a set of diversified prompts.They then used D dev to select a single prompt with the best performance.In (Shin et al., 2020), a gradient-based prompt search approach was proposed, wherein each prompt token is learned by directly optimizing LMs on the downstream task.\n\nIn addition to prompt designing strategies, other research work focuses on enriching the prompt can-didates and ensembling the output from multiple prompts for the same input.To enrich prompts, (Jiang et al., 2020) employed back-translation to paraphrase prompts.Building on this work, (Haviv et al., 2021) trained a separate BERT model to rewrite prompts using the nearest BERT vector embedding.\n\nThe concept of in-context learning originates from the work by (Brown et al., 2020).However, BERT models can also perform similar tasks by using a single token as output.For example, France's capital is [MASK].\n\nOnly the output for the [MASK] position is used for inference.This characteristic enables the ensemble of answers from different prompts, although it is not apparent for similar practices in GPT-style models.In (Jiang et al., 2020), the authors proposed rank-based ensemble and optimized ensemble methods to aggregate answers generated from different prompts.\n\nAmong the studies designing prompts for BERT models, the majority focus on discrete prompts (i.e., hard prompts).To the best of our knowledge, we did not find any work attempting to generate continuous prompts.In general, optimizing prompts in BERT brings only marginal improvements to the original model.Given the size and structure of BERT, it is more favorable to fine-tune on downstream tasks.", "filtered_refids": [[], ["b24", "b8", "b4"], ["b6", "b8"], [null, "b0"], ["b8"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2327, "num_references": 8}
{"corpusid_sectionid": "262460726-s4", "title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning", "date": "2023-09-22", "section_title": "Continuous Prompt", "section": "Another line of research has focused on optimizing soft prompts, which eliminate the constraint that prompts have to be natural language.Soft prompts can be learned and optimized directly within the same language model.The key difference between soft prompt tuning and fine-tuning is that prompt tuning typically fixes the weights of the language model and only performs gradient updates on the network that generates the prompt.Prefix-Tuning (Li and Liang, 2021) is one of the early works that tunes prompts on GPT-2 with a small amount of data per task, achieving comparable performance to the full data fine-tuning setting.Prefix-Tuning does not use a separate network; instead, it utilizes the same transformer network but only optimizes the input embedding of the prompt.In P-Tuning V1 (Liu et al., 2021b) and V2 (Liu et al., 2022), the authors employ a separate LSTM network to generate the input prompt for the language model.While using soft prompts provides more flexibility in prompt design, it requires access to either the weights of language models or the ability to input vectors into language models.As recent language models are hosted as cloud services and large language models are difficult to access via vector inputs, this practice becomes less feasible when using GPT-3 or PaLM (Chowdhery et al., 2022).", "filtered_refids": [["b16", "b12", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1325, "num_references": 3}
{"corpusid_sectionid": "262460726-s5", "title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning", "date": "2023-09-22", "section_title": "Few-Shot Learning", "section": "In the GPT paper (Brown et al., 2020), fewshot learning demonstrates strong NLP capabilities across various benchmarks.As the title suggests, Language Models are Few-Shot Learners.In the few-shot setting, a task description along with a few examples are presented to the model, which is then asked to complete the task for an unseen example.Numerous studies have been conducted to optimize few-shot examples and prompts to enhance performance.In (Liu et al., 2021a), the authors discovered that GPT-3 generally performs better when in-context examples are similar to the test examples.As a result, they proposed an incontext example algorithm based on example similarities.Similarity is measured using RoBERTa embedding distance in Euclidean space or cosine distance.Other works, such as (Rubin et al., 2021) and (Gutierrez et al., 2022), have adopted similar example selection logic and demonstrated better performance over randomly selected examples.In addition to example selection methods, research efforts like (Wu et al., 2022) and (Kumar and Talukdar, 2021) have been made to optimize the rank and order of retrieved examples.While few-shot learning exhibits remarkable performance, according to the no free lunch(NFL) theorem (Wolpert andMacready, 1995, 1997), providing examples inevitably introduces bias to the prediction algorithm.In cases where out-ofdistribution samples occur, applying few-shot learning can hinder the inference process.", "filtered_refids": [["b15", "b10", "b28", null, "b22", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1452, "num_references": 6}
{"corpusid_sectionid": "262460726-s7", "title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning", "date": "2023-09-22", "section_title": "Manual Design", "section": "In their work (Reynolds and McDonell, 2021), the authors argue that GPT (or other LLMs) resemble a superposition of human authors.Therefore, it can be helpful to ask GPT to pretend to be a character in the prompt or use the prompt to signify a dialogue between people (i.e., task specification by memetic proxy).The authors also discuss the idea of MetaPrompts, which encapsulate a general intention that will develop towards specific meanings when additional information, such as a task question, is provided.The example prompts they provide, such as \"Let's solve this problem by splitting it into steps,\" have been proven to be significantly helpful by subsequent works.\n\nIn the work (Mishra et al., 2021), the authors propose five principles for designing prompts for GPT-3 based on their observations of GPT-3's failures.These principles include: (1) using simple patterns to specify expected output, (2) using bulleted lists and assertions, (3) breaking down complex tasks into multiple simpler ones, (4) adding explicit textual statements of output constraints, and (5) customizing the instructions so that the model can directly output the results.These principles can be a good starting point for manual design.\n\nAnother line of work focuses on improving the reasoning capabilities of large language models via prompt design.The work Chain-of-Thought (CoT) (Wei et al., 2022) was initially proposed in few-shot learning, where the reasoning steps were presented as part of the solution for several few-shot examples.The zero-shot version of CoT was later proposed in (Kojima et al., 2022), which demonstrates that inserting the single prompt \"let's think step by step\" into the task instruction significantly improves performance on mathematical reasoning.The authors also experimented with different templates for prompts and found that instructive prompts help improve the model's performance in mathematical reasoning, while misleading or irrelevant prompts do not contribute to performance enhancement.", "filtered_refids": [["b21"], ["b18"], ["b9", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2014, "num_references": 4}
{"corpusid_sectionid": "262460726-s8", "title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning", "date": "2023-09-22", "section_title": "Prompt Optimization", "section": "Finding the optimal prompt can also be treated as an optimization process, where the goal is to optimize the performance of the target task.Similar to finding the best soft prompt or finding the optimal examples for few-shot learning, algorithms can be implemented to find the best zero-shot prompt.However, such work typically requires a small set of evaluation data to assess the prompt performance.In the work by (Zhou et al., 2022), the authors proposed Automatic Prompt Engineer (APE) for zero-shot prompt design.A LLM is used to generate a group of prompts given the task example or human description, and an iterative Monte Carlo search method is used to search for the optimal prompt given the objective function.In addition to using Monte Carlo search for prompt optimization, a gradient-free, edit-based search approach called Gradientfree Instructional Prompt Search (GRIPS) is introduced in (Prasad et al., 2022).GRIPS starts from a manually designed instruction and iteratively searches among generated prompts from four operations (delete, add, swap, paraphrase) to find the optimal prompt for a target task.\n\nAnother line of research uses gradient-based methods but to generate discrete zero-shot prompts.The work FluentPrompt (Shi et al., 2022) follows the idea from AutoPrompt (Shin et al., 2020), using a gradient-based method to generate discrete prompts.They also use a fluency constraint to encourage human-readable prompt outcomes, which helps improve performance.Another gradientbased prompt generation method RLPROMPT is introduced in (Deng et al., 2022).This work uses a reinforcement learning structure to generate prompts that optimize the task-based reward function.The prompts generated from this framework are often incoherent gibberish but are claimed to achieve significant performance improvement.", "filtered_refids": [["b19", "b29"], ["b24", "b23", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1830, "num_references": 5}
{"corpusid_sectionid": "262460726-s9", "title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning", "date": "2023-09-22", "section_title": "Evaluation", "section": "Evaluating prompt design is very challenging.As there is no ground truth dataset for prompt generation, there is no \"best\" prompt but only better prompts.Therefore, the evaluation of the prompt performance for in-context learning usually falls into the following categories.\n\nConditional Probability (Likelihood): To evaluate the performance of a text generation model, we can measure the probability of the generated text.In our case, we can calculate the conditional probability of ground truth(y) given prompt (p), input(x) or calculate the joint probability of x, y, p averaging over the training data, as shown in (2) P rob (y|x, p) x,y\u2208X,Y\n\n(2) This is a simple strategy because the models for in-context learning are generative language models which will generate the joint probability (likelihood) automatically.However, this metric sometimes fails to represent the actual performance of the downstream task.\n\nExecution Accuracy: A more direct method to measure the performance of a prompt is to use metrics from the target task (Zhou et al., 2022), as ultimately the performance on the task is what we care about.In addition to measuring the execution accuracy directly on the entire training set, there are ways to efficiently estimate the performance on a subset of training data to save computational cost (Zhou et al., 2022), (Li et al., 2022).\n\nPrompt Transferability is another evaluation metric reported in (Zhou et al., 2022), (Deng et al., 2022) which is used to prove the quality of the prompt generation methods.However, this metric is more useful in selecting the prompt designing method than evaluating the performance of a single prompt.\n\nGeneral Metrics for Language Models should be used when using large language models via zeroshot in-context learning.It is also important to measure the performance from additional aspects.For example, if we are to build a Question-Answering system, we need to measure the risk of hallucination (Ji et al., 2022).If we are to build an email generation system, we may need to measure the toxicity and prevent generating any aggressive content.The work of Holistic Evaluation of Language Models (HELM) (Liang et al., 2022) provides a great example in evaluating the performance for language models via in-context learning.Although various metrics have been reported in HELM for existing models, it is worth noting that the design of our prompt will directly impact the models' performance.", "filtered_refids": [[], [null], [], ["b13", "b29"], ["b2", "b29"], ["b14", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2449, "num_references": 7}
{"corpusid_sectionid": "261822277-s9", "title": "Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text", "date": "2023-09-14", "section_title": "Methods", "section": "In this section, we report on the various methods proposed for detecting ChatGPT-generated text.The scope of this review does not include the evaluation or comparison of the results obtained from these methods.This limitation primarily arises from the absence of a common experimental setup and the utilization of different datasets and metrics.Table 2 provides an overview of these recent approaches.Some previous works have utilized transformerbased models to classify text generated by Chat-GPT and human-written text, as demonstrated by Mitrovi\u0107 et al. (2023).Their approach consists of two components: a detection model and a framework to explain the decisions made by this model.They first fine-tune an uncased version of Distil-BERT (Sanh et al., 2019) and then employ SHAP to provide local explanations in the form of feature importance scores to gain insights into the significance of different input features of the model's results.As a baseline comparison, they implement a perplexity-based classifier that categorizes text based on its perplexity score, where GPT-2 is used for calculating perplexity scores.Their results show that the DistilBERT-based detector outperforms the perplexity-based classifier.However, its performance decreases when considering the rephrased dataset by ChatGPT.\n\nIn Liao et al. (2023), different models are proposed to detect medical text generated by Chat-GPT: a fine-tuned BERT model (Devlin et al., 2019), a model based on Classification and Regression Trees (CART), an XGBoost model (Chen and Guestrin, 2016) and a perplexity classifier that utilizes BioGPT (Luo et al., 2022) for calculating text perplexity.Predictions by the BERT model are explained by visualizing the local features of the samples, where it can be seen that using conjuncts is an essential feature for the model classifying a medical text as machine-generated.Liu et al. (2023) fine-tune RoBERTa to detect argumentative essays generated by different GPT models, including ChatGPT, and evaluate its performance on document, paragraph, and sentencelevel classification.The essays are broken down into paragraphs and sentences for paragraph and sentence-level classification.They train and compare the performance of SVM models using different linguistic features.These models serve as a baseline to compare with the RoBERTa model and to understand which linguistic features differentiate between human and ChatGPT-generated text.Guo et al. (2023) implement a machine learning and deep learning-based detector.They utilize a logistic regression model trained on the GLTR Test-2 dataset (Gehrmann et al., 2019) and two deep classifiers based on fine-tuning the pre-trained transformer model RoBERTa.One deep classifier is designed explicitly for single-text detection, while the other is intended for QA detection.The authors construct various training and testing datasets versions to assess the models' robustness.They create full-text, sentence-level, and mixed subsets of the collected corpus.Each subset has both a raw version and a filtered version where prominent indicating words referring to humans (such as \"Nope\" and \"Hmm\") or ChatGPT words (such as \"AI assistant\") are removed.The evaluation of the models reveals that the RoBERTa-based models outper- form GLTR in terms of performance and exhibit more robustness against interference.Moreover, the RoBERTa-based models are not influenced by indicating words.\n\nBuilding upon the work of Guo et al. (2023), Antoun et al. (2023a) propose an approach for developing robust detectors able to detect ChatGPT-generated text in different languages, with a focus on French.Their approach consists of fine-tuning pre-trained transformer-based models on English, French, and multilingual datasets.They train RoBERTa and ELECTRA (Clark et al., 2020) models on the English dataset, CamemBERT (Martin et al., 2020) and CamemBERTa (Antoun et al., 2023b) on the French datasets and XLM-R (Conneau et al., 2020) on the combined English and French dataset.They evaluate the robustness of these models against adversarial attacks, such as replacing characters with homoglyphs and adding misspelled words.Considering in-domain text, their results show that French models perform well in detecting machine-generated text.Still, they were outperformed by the English models, while XLM-R provides the best and most resilient performance against adversarial attacks for both English and French.However, this performance decreases when evaluated on out-of-domain text.\n\nAnother method proposed for detecting ChatGPT-generated text is a metric-based approach proposed by Vasilatos et al. (2023) to detect machine-generated student assignments by calculating perplexity scores using GPT-2.They show that having category-wise thresholds (derived from dataset metadata) results in better detection performance than only having one threshold value.", "filtered_refids": [["b43", "b37"], ["b34", "b14", "b20", "b32", null, "b19", "b7"], ["b10", "b9", "b20", "b35", "b1", "b2"], ["b48"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 4894, "num_references": 16}
{"corpusid_sectionid": "261822277-s10", "title": "Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text", "date": "2023-09-14", "section_title": "Analysis of Human and", "section": "ChatGPT-Generated Text\n\nThe textual characteristics of ChatGPT-generated text as well as its syntactic and linguistic features, are of significant focus in the works we reviewed.These linguistic and stylistic features are compared to the human-written texts in the datasets.In this section, we summarize and provide an overview of the findings of such analyses for the different domains and datasets we reviewed.\n\n\u2022 Medical domain: Medical texts generated by ChatGPT have lower text perplexity and are more fluent, neutral, positive, and logical but more general in content and language style, while medical texts written by humans are more diverse and specific (Liao et al., 2023).\n\n\u2022 English argumentative essays: ChatGPT produces syntactically more complex sentences than English language learners, but ChatGPT-authored essays tend to have lower lexical diversity (Liu et al., 2023).\n\n\u2022 Multi-domain question answering: Chat-GPT writes in an organized and neutral way, offers less bias and harmful information, and refuses to answer questions where it believes it does not know.ChatGPT answers are formal, less emotional, and more objective than human answers (Guo et al., 2023).\n\n\u2022 Scientific abstracts: ChatGPT has a better choice of vocabulary, can generate more unique words, uses more connecting words, and has fewer grammatical errors (Yu et al., 2023).\n\n\u2022 Language-agnostic characteristics: The linguistic and syntactic characteristics of ChatGPT-generated text tend to be languageagnostic.Text generated in different languages, such as English, French, and Chinese, shows similar characteristics where ChatGPT tends to produce didactic and impersonal text without errors.Such errors can indicate human text, like grammatical, spelling or punctuation mistakes (Antoun et al., 2023a;Guo et al., 2023).", "filtered_refids": [[], [], [null], ["b32"], ["b20"], ["b52"], ["b1", "b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1810, "num_references": 6}
{"corpusid_sectionid": "252683270-s5", "title": "A Decade of Knowledge Graphs in Natural Language Processing: A Survey", "date": "2022-09-30", "section_title": "Classification Scheme and Data Extraction", "section": "According to our RQs, the included papers had to be categorized with respect to three facets: task, research type, and contribution. Established classification schemes from Wieringa et al. (2006) and Shaw (2003) were adapted for the research and contribution type as presented in Appendix A. For classifying tasks, we constructed a task taxonomy, following the iterative procedure suggested by Petersen et al. (2008), in which an initial classification scheme derived from keywords continuously evolves through adding, merging, or splitting categories during the classification process. Our task taxonomy is based on existing schemes from Paulheim (2017) Once the initial schemes were set up, all papers were sorted into the classes as part of the data extraction process. The 507 included studies were divided between two of the authors. In regular sessions, they discussed changes to the classification schemes or clarified uncertain labels. While each paper got assigned one label for the research type assigned, multiple labels were possible with regard to tasks and contributions. To assess the reliability of the inter-annotator agreement, the two authors independently classified a random sample of 50 papers. We calculated Cohen's Kappa coefficient of these annotations for each facet (Cohen, 1960). ", "filtered_refids": [["b9", "b61", "b50", "b40"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1308, "num_references": 4}
{"corpusid_sectionid": "252683270-s8", "title": "A Decade of Knowledge Graphs in Natural Language Processing: A Survey", "date": "2022-09-30", "section_title": "<HDU", "section": "Another finding of the data extraction process concerns the diverse application areas of KGs in NLP. We observed that the number of domains explored in the research literature grew rapidly in parallel with the annual count of papers. To reveal the great variety of areas, we list all 20 discovered domains and their subdomains in Table 6 in the Appendix. In Figure 3, the ten most frequent domains are displayed. It is striking that health is by far the most prominent domain. The latter appears more than twice as often as the scholarly domain, which ranks second. Other popular areas are engineering, business, social media, or law. In view of the domain diversity, it becomes evident that KGs are naturally applicable to many different contexts, as has been stated in prior work (Abu-Salih, 2021;Ji et al., 2021;Zou, 2020).  omy shown in Figure 1. The two top-level categories consist of knowledge acquisition and knowledge application. Knowledge acquisition contains NLP tasks to construct KGs from unstructured text (knowledge graph construction) or to conduct reasoning over already constructed KGs (knowledge graph reasoning). KG construction tasks are further split into two subcategories: knowledge extraction, which is used to populate KGs with entities, relations, or attributes, and knowledge integration, which is used to update KGs. Knowledge application, being the second top-level concept, encompasses common NLP tasks, which are enhanced through structured knowledge from KGs. As might be expected, the frequency of occurrence in the literature for the tasks from our taxonomy varies greatly. While Table 2 gives an overview of the most popular tasks, Figure 5 compares their popularity over time. Figure 4 displays the number of detected domains for the most prominent tasks. It shows that certain tasks are adopted to more domain-specific contexts than others. ", "filtered_refids": [["b70", null, "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 1880, "num_references": 3}
{"corpusid_sectionid": "252683270-s9", "title": "A Decade of Knowledge Graphs in Natural Language Processing: A Survey", "date": "2022-09-30", "section_title": "Knowledge Graph Construction", "section": "The task of entity extraction is a starting point in constructing KGs and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020). Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a). A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018). Entity linking is a task of linking entities recognized in some text to already existing entities in KGs (Moon et al., 2018;. Since synonymous or similar entities often exist in different KGs or in different languages, entity alignment can be performed to reduce redundancy and repetition in future tasks (Gangemi et al., 2016;Chen et al., 2018). Coming up with the rules and schemes of KGs, i.e., their structure and format of knowledge presented in it, is done with the task of ontology construction (Haussmann et al., 2019).", "filtered_refids": [["b6", "b46", "b29", "b32", "b19", "b1", "b65", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1025, "num_references": 8}
{"corpusid_sectionid": "252683270-s11", "title": "A Decade of Knowledge Graphs in Natural Language Processing: A Survey", "date": "2022-09-30", "section_title": "Knowledge Application", "section": "Existing KGs can be used in a multitude of popular NLP tasks. Here we outline the most popular ones.\n\nQuestion answering (QA) was found to be the most common NLP task using KGs. This task is typically divided into textual QA and question answering over knowledge bases (KBQA). Textual QA derives answers from unstructured documents while KBQA does so from predefined knowledge bases (Fu et al., 2020). KBQA is naturally tied to KGs while textual QA can also be approached by using KGs as a source of common-sense knowledge when answering questions. As Zhu et al. (2021) conclude, this approach is desired not only because it is helpful for generating answers, but also because it makes answers more interpretable.\n\nSemantic search refers to \"search with meaning\", where the goal is not just to search for literal matches, but to understand the search intent and query context as well (Bast et al., 2016). This label denoted studies that use KGs for search, recommendations, and analytics. Examples are a big semantic network of everyday concepts called ConceptNet (Speer et al., 2017) and a KG of scholarly communications and the relationships, among them the Microsoft Academic Graph .\n\nConversational interfaces constitute another NLP field that can benefit from world knowledge contained in KGs. Zhou et al. (2018)  Natural language generation (NLG) is a subfield of NLP and computational linguistics that is concerned with models which generate natural language output from scratch. KGs are used in this subfield for producing natural language text from KGs (Koncel-Kedziorski et al., 2019), generating question-answer pairs (Reddy et al., 2017), the multi-modal task of image captioning (Lu et al., 2018), or data augmentation in low-resource settings (Sharifirad et al., 2018).\n\nText analysis combines various analytical NLP techniques and methods that are applied to process and understand textual data. Exemplary tasks are sentiment detection (Kumar et al., 2018), topic modeling , or word sense disambiguation .\n\nAugmented language models are a combination of large pretrained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) with knowledge contained in KGs. Since PLMs derive their knowledge from huge amounts of unstructured training data, a rising research trend is in combining them with structured knowledge. Knowledge from KGs can be infused into language models in their input, architecture, output, or some combination thereof (Colon-Hernandez et al., 2021). Some notable examples we outline are ERNIE (Zhang et al., 2019b), COMET (Bosselut et al., 2019), K-BERT (Liu et al., 2020b), and KEPLER (Wang et al., 2021b). Table 3 shows the distribution of papers according to the different research and contribution types as defined in Table 4 and 5 in the Appendix. It shows that most papers conduct validation research, investigating new techniques or methods that have not yet been implemented in practice. A considerable number of papers, although significantly less, focus on solution proposals of approaches by demonstrating their advantages and applicability by a small example or argumentation. However, these papers usually lack a profound empirical evaluation. Secondary research accounts for only a small number of papers and is severely underrepresented in the research field of KGs in NLP. As already mentioned in Section 1 and Section 2, there is a notable lack of studies that summarize, compile, or synthesize existing research regarding KGs in NLP. Moreover, evaluation research papers that implement and evaluate approaches in an industry context are equally scarce. Opinion papers are almost non-existent.", "filtered_refids": [[], [null], ["b53", "b4"], ["b49", "b44", "b28", "b68"], ["b21"], ["b43", "b12", "b11", "b67", "b59", null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3676, "num_references": 15}
{"corpusid_sectionid": "252762171-s6", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Triples: Crowd-Sourcing of Facts", "section": "Popular large-scale KGs, like Wikidata (Vrandecic and Kr\u00f6tzsch, 2014) and DBpedia (Auer et al., 2007) are the products of continuous crowdsourcing efforts. Both of these examples are closely related to Wikipedia, where the top five languages (English, Cebuano, German, Swedish, and French) constitute 35% of all articles on this platform. 3 It can be said that Wikipedia is Euro-centric in tendency. Moreover, the majority of authors are white males. 4 As a result, the data transport a particular homogeneous set of interests and knowledge (Beyt\u00eda et al., 2022;Wagner et al., 2015). This sampling bias affects the geospatial coverage of information (Janowicz et al., 2018) and leads to higher barriers for female personalities to receive a biographic entry (Beyt\u00eda et al., 2022). In an experiment, Demartini (2019) asked crowd contributors to provide a factual answer to the (politically charged) question of whether or not Catalonia is a part of Spain. The diverging responses indicated that participants' beliefs of what counts as true differed largely. This is an example of bias that is beyond a subliminal psychological level. In this case, structural aspects like consumed media and social discourse play an important role. To counter this problem, Demartini (2019) suggests actively asking contributors for evidence supporting their statements, as well as keeping track of their demographic backgrounds. This makes underlying motivations and possible sources for bias traceable.", "filtered_refids": [["b42", "b6", "b85", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1486, "num_references": 4}
{"corpusid_sectionid": "252762171-s7", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Ontologies: Manual Creation of Rules", "section": "Ontologies determine rules regarding allowed types of entities and relations or their usage. They are often hand-made and a source of bias (Janowicz et al., 2018) due to the influence of opinions, motivations, and personal choices (Keet, 2021): Factors like scientific opinions (e.g., historical ideas about race), socio-culture (e.g., how many people a person can be married to), or political and religious views (e.g., classifying a person of type X as a terrorist or a protestor) can proximately lead to an encoding of social bias. Also structural constraints like the ontologies' granularity levels can induce bias (Keet, 2021). Furthermore, issues can arise from the types of information used to characterize a person entity. Whether one attributes the person with their skin color or not could theoretically determine the emergence of racist bias in a downstream application (Paparidis and Kotis, 2021). Geller and Kollapally (2021) give a practical example for detection and alleviation of ontology bias in a real-world scenario. The authors discovered that ontological gaps in the medical context lead to an under-reporting of racespecific incidents. They were able to suggest countermeasures based on a structured analysis of real incidents and external terminological resources.", "filtered_refids": [["b45", "b42", "b30", "b64"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1288, "num_references": 4}
{"corpusid_sectionid": "252762171-s8", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Extraction: Automated Extraction of Information", "section": "Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively). Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias. They used a number of template sentences, like \"<Name> is going to school\" or \"<Name> is a person\" using male and female names 5 from 139 years of census data. The model returned more erroneous tags for female names. Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders. A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF) (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models). Across models, nonwhite names yielded on average lower performance scores than white names. Generally, ELMo exhibited the least bias. Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values. Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019). For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia). All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms). The most notable bias found was the spouse relation. It was more reliably predicted for male than female entities. This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias. The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016). Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.\n\nNowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE. Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)). Thus, it is likely that these biases also affect the downstream tasks discussed here.  used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks. For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE). The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information. This hints at what the authors call semantic bias.\n\nA Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence. Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., \"a banana is yellow\" is too trivial to be reported). This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.", "filtered_refids": [["b67", "b60", "b56", "b40", "b78", "b28", "b9", "b66", null, "b73", "b37"], ["b90", "b9", "b47", "b50", "b59", null, "b19", "b69", "b0"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 3933, "num_references": 20}
{"corpusid_sectionid": "252762171-s11", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Semantic Polarity", "section": "Mehrabi et al. (2021b) focused on biases in common sense KGs like ConceptNet (Speer et al., 2017) and GenericsKB (Bhakthavatsalam et al., 2020) (contains sentences) which are at risk of causing representational harms (see Section 2.2). They utilized regard (Sheng et al., 2019) and sentiment as intermediate bias proxies. Both concepts express the polarity of statements and can be measured via classifiers that predict a neutral, negative, or positive label (Sheng et al., 2019;Dhamala et al., 2021). Groups that are referred to in a mostly positive way are interpreted as favored and vice versa. Mehrabi et al. (2021b) applied this principle to natural language statements generated from Con-ceptNet triples. They found that subject and object entities relating to the professions CEO, nurse, and physician were more often favored while performing artist, politician, and prisoner were more often disfavored. Similarly, several Islam-related entities were on the negative end while Christian and Hindu were more ambiguously valuated. As for gender, no significant difference was found.", "filtered_refids": [["b78", "b20", "b77", null, "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1087, "num_references": 5}
{"corpusid_sectionid": "252762171-s13", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Stereotypical Analogies", "section": "The idea behind analogy tests is to see whether demographics are associated with attributes in stereotypical ways (e.g., \"Man is to computer programmer as woman is to homemaker\" (Bolukbasi et al., 2016)). In their in-depth analysis of a TransEembedded Wikidata KG, Bourli and Pitoura (2020) investigated occupational analogies for binary gender seeds. TransE (Bordes et al., 2013) represents (h, r, t) (with head h, relation r, tail t) in a single space such that h+r \u2248 t. The authors identified the model's most likely instance of the claim \"a is to x as b is to y\" (with (a,b) being a set of demographics seeds and (x,y) a set of attributes) via a cosine score:\n\nwhere r is the relation has_occupation. In their study, the highest scoring analogy was \"woman is to fashion model as man is to businessperson\". This example appears rather stereotypical, but other highly ranked analogies less so, like \"Japanese entertainer\" versus \"businessperson\" (Bourli and Pitoura, 2020). A systematic evaluation of how stereotypical the results are is missing here. In comparison, the work that originally introduced analogy testing for word2vec (Bolukbasi et al., 2016) employed human annotators to rate stereotypical and gender-appropriate analogies (e.g., \"sister\" versus \"brother\").", "filtered_refids": [["b11", "b12", "b9"], ["b12", "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1274, "num_references": 5}
{"corpusid_sectionid": "252762171-s15", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Update-Based Measurement", "section": "The translational likelihood (TL) metric was tailored for translation-based modeling approaches (Fisher et al., 2020b). To compute this metric, the embedding of a person entity is updated for one step towards one pole of a seed dimension. This update is done in the same way as the model was originally fit in. For example, if head entity person x is updated in the direction of male gender, the TL value is given by the difference between the likelihood of person x being a doctor after versus before the update. If the absolute value averaged across all human entities is high, this indicates a bias regarding the examined seed-attribute pair. Fisher et al. (2020b) argue that this measurement technique avoids model-specificity as it generalizes to any scoring function. However, Keidar et al. (2021) found that the TL metric does not compare well between different types of embeddings (details in Section 6). It should, thus, only be used for the comparison of biases within one kind of representation. Du et al. (2022) propose an approach comparable to Fisher et al. (2020b) to measure individual-level bias. Instead of updating towards a gender dimension, the authors suggest flipping the entity's gender and fully re-training the model afterward. The difference between pre-and postupdate link prediction errors gives the bias metric. A validation of the approach was done on TransE for a Freebase subset (FB5M (Bordes et al., 2015)) (Du et al., 2022). The summed per-gender averages (group-level metric) were found to correlate with U.S. census gender distributions of occupations.", "filtered_refids": [[null, "b22", "b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1589, "num_references": 3}
{"corpusid_sectionid": "252762171-s16", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Downstream Task Bias: Link Prediction", "section": "Link prediction is a standard downstream task that targets the prediction of relations between entities in a given KG. Systematic deviations in the relations suggested for entities with different demographics indicate reproduced social bias. For the measurement of fairness or bias in link prediction, Keidar et al. (2021) distinguish between demographic parity versus predictive parity. The assumption underlying demographic parity is that the equality between predictions for demographic counterfactuals (opposite demographics, for example, female versus male in binary understanding) is the ideal state (Dwork et al., 2012). That is, the probability of predicting a label should be the same for both groups. Predictive parity is given, on the other hand, if the probability of true positive predictions (positive predictive value or precision) is equal between groups (Chouldechova, 2017). Hence, this measure factors in the label  Bourli and Pitoura (2020) distribution by demographic. With these metrics, Keidar et al. (2021) analyzed different embedding types, namely TransE, ComplEx, RotatE, and Dist-Mult, each fit on the benchmark datasets FB15k-237 (Toutanova and Chen, 2015) and Wikidata5m (Wang et al., 2021). They averaged the scores across a large set of human-associated relations to detect automatically which relations are most biased. The results showed that position played on a sports team was most consistently genderbiased across embeddings. Arduini et al. (2020) analyzed link prediction parity regarding the relations gender and occupation to estimate debiasing effects on TransH (Wang et al., 2014) and TransD (Ji et al., 2015). The comparability between different forms of vector representations is a strength of downstream metrics. In contrast, measures like the analogy test or projection score (Bourli and Pitoura, 2020) are based on specific distance metrics and TL (Fisher et al., 2020b) was shown to lack transferability across representations (Keidar et al., 2021) (Section 5.3). Du et al. (2022) interpret the correlation between gender and link prediction errors as an indicator of group bias. With this, they found, for example, that engineer and nurse are stereotypically biased in FB5M. However, the ground truth gender ratio was found not predictive of the bias metric (e.g., despite its higher male ratio, animator produced a stronger female bias value). For validation, it was shown that the predicted bias values correlate to the gender distributions of occupations according to U.S. census (again, on TransE). Furthermore, the authors investigated how much single triples contribute to group bias via an influence function. They found that gender bias is mostly driven by triples containing gendered entities and triples of low degree.", "filtered_refids": [["b15", "b86", "b43", "b12", "b87", "b82", null, "b22", "b23", "b1"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2778, "num_references": 10}
{"corpusid_sectionid": "252762171-s18", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Data Balancing", "section": "Radstok et al. (2021) explored the effects of training an embedding model on a gender-balanced subset of Wikidata triples. First, the authors worked with the originally gender-imbalanced Wikidata12k (Leblay and Chekol, 2018;Dasgupta et al., 2018) and DBpedia15k (Sun et al., 2017) on which they fit a TransE and a DistMult model (Yang et al., 2015). They then added more female triples from the Wikidata/DBpedia graph to even out the binary gender distribution among the top-5 most common occupations. Through link prediction, they compared the number of male and female predictions with the ground truth frequencies. More female entities were predicted after the data balancing intervention. However, the absolute difference between the female ratios in the data and the predictions increased, causing the model to be less accurate and fair. Moreover, the authors note that this process is not scalable since for some domains there are no or only a limited amount of female entities (e.g., female U.S. presidents do not exist in Wikidata). Du et al. (2022) experimented with adding and removing triples to gender-balance a Freebase subset (Bordes et al., 2015). For the first approach, the authors added synthetic triples (as opposed to real entities from another source as was done by Radstok et al. (2021)) for occupations with a higher male ratio. The resulting bias change was inconsis-tent across occupations. This appears in line with the authors' finding that ground truth gender ratios are not perfectly predictive of downstream task bias (Section 6). For the second strategy, the triples that most strongly influenced an existing bias were determined and removed. This outperformed random triple removal.", "filtered_refids": [["b10", "b88", "b22", "b81", "b70", "b16", "b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1714, "num_references": 7}
{"corpusid_sectionid": "252762171-s20", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Hard Debiasing", "section": "Bourli and Pitoura (2020) propose applying the projection-based approach explained in Section 5.2 for the debiasing of TransE occupation embeddings. To achieve this, its linear projection onto the previously computed gender direction is subtracted from the occupation embedding. A variant of this technique (\"soft\" debiasing) aims to preserve some degree of gender information by applying a weight 0 < \u03bb < 1 to the projection value before subtraction. In the authors' experiments, the correlation between gender and occupation was effectively removed -as indicated by the projection measure (Bourli and Pitoura, 2020). However, the debiasing degree determined by \u03bb was found to be in trade-off with model accuracy. This technique was closely adapted from Bolukbasi et al. (2016), regarding which Gonen and Goldberg (2019) criticize that gender bias is only reduced according to their specific measure and not the \"complete manifestation of this bias\".", "filtered_refids": [["b12", "b9", "b35"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 951, "num_references": 3}
{"corpusid_sectionid": "252762171-s22", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Lack of Validation", "section": "Most of the KGE bias metrics presented here are interpreted as valid if they detect unfairly discriminating association patterns that intuitively align with existing stereotypes. Besides that, several works investigate the comparability between different metrics. Although both of these practices deliver valuable information on validity, they largely ignore the societal context. Only Du et al. (2022) compared embedding-level bias metrics with census-aligned data to assess compatibility with real-world inequalities. We suggest that future work consider a more comprehensive study of construct validity (Does the measurement instrument measure the construct in a meaningful and useful capacity?) (Jacobs and Wallach, 2021). One requirement is that the obtained measurements capture all relevant aspects of the construct the instrument claims to measure. That is, a gender bias measure must measure all relevant aspects of gender bias (Stanczak and Augenstein, 2021) (including, e.g., nonbinary gender and a distinction between benevolent and hostile forms of sexist stereotyping (Glick and Fiske, 1997)). Unless proven otherwise, we must be skeptical that this is achieved by existing approaches (Gonen and Goldberg, 2019). As a result of minimal validation, detailed interpretation guidelines are generally not provided. Therefore, the distinctions between strong and weak bias or weak bias and random variation are mostly vague.", "filtered_refids": [["b41", "b79", "b35", "b22", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1433, "num_references": 5}
{"corpusid_sectionid": "252762171-s25", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Recommendations", "section": "To avoid harms caused by biases in KGs and their embeddings, we identify and recommend several actions for practitioners and researchers.\n\nTransparency and Accountability KGs should by default be published with bias-sensitive documentation to facilitate transparency and accountability regarding potential risks. Data Statements (Bender and Friedman, 2018) report curation criteria, language variety, demographics of the data authors and annotators, relevant indicators of context, quality, and provenance. Datasheets for Datasets (Gebru et al., 2021) additionally state motivation, composition, preparation, distribution, and maintenance. The associated questionnaire can accompany the dataset creation process to avoid risks early on. Especially in the case of ongoing crowdsourcing efforts for encyclopedic KGs the demographic background of contributors should be reported (Demartini, 2019). Researchers using subsets of these KGs, should investigate respective data dumps for potential biases and report limitations transparently. Similarly, KG embedding models should be published with Model Cards (Mitchell et al., 2019) documenting intended use, underlying data, ethical considerations, and limitations. Stating the contact details for reporting problems and concerns establishes accountability (Mitchell et al., 2019;Gebru et al., 2021).\n\nImproving Representativeness To tackle selection bias, data collection should aim to employ authors and annotators from diverse social groups and with varied cultural imprints. Annotations should be determined via aggregation (see Hovy and Prabhumoye, 2021). For open editable KGs, interventions like edit-a-thons are helpful to introduce more authors from underrepresented groups (Vetter et al., 2022) (e.g., the Art+Feminism campaign aims to fill the gender gap in Wikimedia knowledge bases 8 ). In order for such interventions to take effect, research must update data bases and benchmarks frequently (see Koch et al., 2021). In addition, the timeliness of encyclopedic data is necessary to avoid perpetuating historic biases.\n\nTackling Algorithmic Bias Evaluation and prevention of harmful biases must become part of the development pipeline (Stanczak and Augenstein, 2021). Algorithmic biases are best evaluated with a combination of multiple quantitative (Section 5) and qualitative measures (Kraft et al., 2022;Dev et al., 2021), considering multiple demographic dimensions (beyond gender and occupation). Evaluating the content of attributions in light of social discourse and the intended use of a technology facilitates an assessment of potential harms (Selbst et al., 2019). Downstream task bias may exist independently from a measured embedding bias (Goldfarb-Tarrant et al., 2021), therefore a taskand context-oriented evaluation is preferred (Section 6). We have presented several bias-mitigating strategies for different KGEs, which might alleviate the issue in some cases (Section 7). However, more research is needed to establish more effective and robust mitigation methods, as well as metrics used to evaluate their impact (Gonen and Goldberg, 2019;Blodgett et al., 2020).", "filtered_refids": [[], ["b61", "b29", "b5", "b17"], ["b83", "b39", "b48"], ["b34", "b18", "b79", "b35", "b49", "b74", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3138, "num_references": 14}
{"corpusid_sectionid": "252992688-s3", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Output Uncertainty", "section": "Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schr\u00f6der et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).\n\nAnother way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).", "filtered_refids": [["b11", "b12", "b10", "b14", "b91", "b25", "b49", null, "b19"], ["b72", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1258, "num_references": 11}
{"corpusid_sectionid": "252992688-s6", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Performance Prediction", "section": "Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.\n\nRecently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.\n\nA similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).", "filtered_refids": [[null], ["b53", null, "b98"], [null, "b38", "b62", "b106"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2155, "num_references": 8}
{"corpusid_sectionid": "252992688-s9", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Discriminative 3", "section": "Another direction is to select instances that are different from already labeled instances. Again, for NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005;Bloodgood and Callison-Burch, 2010;Erdmann et al., 2019). Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006;). Another interesting idea is to train a model to discriminate the labeled and unlabeled sets. Gissin and Shalev-Shwartz (2019) directly train a classifier for this purpose, while naturally adversarial training can be also adopted (Sinha et al., 2019;. In domain adaptation scenarios, the same motivation leads to the usage of a domain separator to filter instances (Rai et al., 2010).", "filtered_refids": [["b91", "b34", "b2", "b106"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 847, "num_references": 4}
{"corpusid_sectionid": "252992688-s10", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Batch Diversity", "section": "Ideally, only one most useful instance would be selected in each iteration. However, it is more efficient and practical to adopt batch-mode AL (Settles, 2009), where each time a batch of instances is selected. In this case, we need to consider the dissimilarities not only between selected instances and labeled ones but also within the selected batch.\n\nTo select a batch of diverse instances, there are two common approaches. 1) Iterative selection collects the batch in an iterative greedy way (Brinker, 2003;. In each iteration, an instance is selected by comparing it with previously chosen instances to avoid redundancy. Some more advanced diversity-based criteria, like coreset (Geifman and El-Yaniv, 2017;Sener and Savarese, 2018) and determinantal point processes , can also be approximated in a similar way. 2) Clustering-based methods partition the unlabeled data into clusters and select instances among them Xu et al., 2003;Nguyen and Smeulders, 2004;Zhdanov, 2019;Yu et al., 2022). Since the chosen instances come from different clusters, diversity can be achieved to some extent.\n\nFor the calculation of similarity, in addition to comparing the input features or intermediate neural representations, other methods are also investigated, such as utilizing model-based similarity ), gradients (Ash et al., 2020Kim, 2020), and masked LM surprisal embeddings .", "filtered_refids": [[], ["b63", "b60", "b78", null, "b17"], ["b91", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1370, "num_references": 7}
{"corpusid_sectionid": "252992688-s11", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Hybrid", "section": "There is no surprise that informativeness and representativeness can be combined for instance querying, leading to hybrid strategies. A simple combination can be used to merge multiple criteria into one. This can be achieved by a weighted sum (Kim et al., 2006;Chen et al., 2011) or multiplication .\n\nThere are several strategies to naturally integrate multiple criteria. Examples include (uncertainty) weighted clustering (Zhdanov, 2019), diverse gradient selection (Ash et al., 2020;Kim, 2020) where the gradients themselves contain uncertainty information ( \u00a72.1.3) and determinantal point processes (DPP) with quality-diversity decomposition .\n\nMoreover, multi-step querying, which applies multiple criteria in series, is another natural hybrid method. For example, one can consider first filtering certain highly uncertain instances and then performing clustering to select a diverse batch from them (Xu et al., 2003;Mirroshandel et al., 2011). An alternative strategy of selecting the most uncertain instances per cluster has also been utilized .\n\nInstead of statically merging into one query strategy, dynamic combination may better fit the AL learning process, since different strategies may excel at different AL phases. For example, at the start of AL, uncertainty sampling may be unreliable due to little labeled data, and representativenessbased methods could be preferable, whereas in later stages where we have enough data and target finergrained decision boundaries, uncertainty may be a suitable strategy. DUAL (Donmez et al., 2007) is such a dynamic strategy that can switch from a density-based selector to an uncertainty-based one. Ambati et al. (2011b) further propose GraDUAL, which gradually switches strategies within a switching range. Wu et al. (2017) adopt a similar idea with a pre-defined monotonic function to control the combination weights.", "filtered_refids": [["b91"], ["b91", "b78", null], ["b102", "b60"], [null, "b57"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1871, "num_references": 8}
{"corpusid_sectionid": "252992688-s14", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Full-structure AL", "section": "First, if we regard the full output structure of an instance as a whole and perform query and annotation at the full-instance level, then AL for structured prediction tasks is not very different than for simpler classification tasks. Nevertheless, considering that the output space is usually exponentially large and infeasible to explicitly enumerate, querying may require further inspection.\n\nSome uncertainty sampling strategies, such as entropy, need to consider the full output space. Instead of the infeasible explicit enumeration, dynamic-programming algorithms that are similar to the ones in decoding and inference processes can be utilized, such as algorithms for tree-entropy (Hwa, 2000(Hwa, , 2004 and sequence-entropy (Mann and McCallum, 2007;. Instead of considering the full output space, topk approximation is a simpler alternative that takes k-best predicted structures as a proxy. This is also a frequently utilized method Kim et al., 2006;Rocha and Sanchez, 2013).\n\nFor disagreement-based strategies, the measurement of partial disagreement may be required, since full-match can be too strict for structured objects. Fine-grained evaluation scores can be reasonable choices for this purpose, such as F1 score for sequence labeling (Ngai and Yarowsky, 2000).\n\nSince longer instances usually have larger uncertainties and might be preferred, length normalization is a commonly-used heuristic to avoid this bias Hwa, 2000Hwa, , 2004). Yet,  argue that longer sequences should not be discouraged and may contain more information.\n\nInstead of directly specifying the full utility of an instance, aggregation is also often utilized by gathering utilities of its sub-structures, usually along the factorization of the structured modeling. For example, the sequence uncertainty can be obtained by summing or averaging the uncertainties of all the tokens . Other aggregation methods are also applicable, such as weighted sum by word frequency  or using only the most uncertain (least probable) one (Myers and Palmer, 2021;Liu et al., 2022).", "filtered_refids": [[], ["b91", null, "b104"], [null], [null], [null, "b99"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2050, "num_references": 7}
{"corpusid_sectionid": "252992688-s15", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Partial-structure AL", "section": "A structured object can be decomposed into smaller sub-structures with different training utilities. For example, in a dependency tree, functional relations are usually easier to judge while prepositional attachment links may be more informative for the learning purpose. This naturally leads to AL with partial structures, where querying and annotating can be performed at the sub-structure level.\n\nFactorizing full structures into the finestgrained sub-structures and regarding them as the annotation units could be a natural choice. Typical examples include individual tokens for sequence labeling , word boundaries for segmentation (Neubig et al., 2011;Li et al., 2012b), syntactic-unit pairs for dependency parsing (Sassano and Kurohashi, 2010) and mention pairs for coreference (Gasperin, 2009;Miller et al., 2012;Sachan et al., 2015). The querying strategy for the sub-structures can be similar to the classification cases, though inferences are usually needed to calculate marginal probabilities. Moreover, if full structures are desired as annotation outputs, semi-supervised techniques such as self-training ( \u00a74.2) could be utilized to assign pseudo labels to the unannotated parts Majidi and Crane, 2013).\n\nAt many times, choosing larger sub-structures is preferable, since partial annotation still needs the understanding of larger contexts and frequently jumping among different contexts may require more reading time ( \u00a73.2.1). Moreover, increasing the sampling granularity may mitigate the missed class effect, where certain classes may be overlooked . Typical examples of larger sub-structures include sub-sequences for sequence labeling Chaudhary et al., 2019;, word-wise head edges for dependency parsing (Flannery and Mori, 2015;Li et al., 2016), neighborhood pools (Laws et al., 2012) or mention-wise anaphoric links Espeland et al., 2020) for coreference, and phrases for MT (Bloodgood and Callison-Burch, 2010;Miura et al., 2016;Hu and Neubig, 2021). In addition to increasing granularity, grouping queries can also help to make annotation easier, such as adopting a two-stage selection of choosing uncertain tokens from uncertain sentences (Mirroshandel and Nasr, 2011;Flannery and Mori, 2015) and selecting nearby instances in a row (Miller et al., 2012).\n\nFor AL with partial structures, output modeling is of particular interest since the model needs to learn from partial annotations. If directly using local discriminative models where each substructure is decided independently, learning with partial annotations is straightforward since the annotations are already complete to the models (Neubig et al., 2011;Flannery and Mori, 2015). For more complex models that consider interactions among output sub-structures, such as global models, special algorithms are required to learn from incomplete annotations (Scheffer et al., 2001;Wanvarie et al., 2011;Li et al., 2016). One advantage of these more complex models is the interaction of the partial labels and the remaining parts. For example, considering the output constraints for structured prediction tasks, combining the annotated parts and the constraints may reduce the output space of other parts and thus lower their uncertainties, leading to better queries (Roth and Small, 2006;Sassano and Kurohashi, 2010;Mirroshandel and Nasr, 2011). More generally, the annotation of one label can intermediately influence others with cheap re-inference, which can help batch-mode selection (Marcheggiani and Arti\u00e8res, 2014) and interactive correction (Culotta and McCallum, 2005).\n\nIn addition to classical structured-prediction tasks, classification tasks can also be cast as structured predictions with partial labeling. Partial feedback is an example that is adopted to make the annotating of classification tasks simpler, especially when there are a large number of target labels. For example, annotators may find it much easier to answer yes/no questions (Hu et al., 2019) or rule out negative classes (Lippincott and Van Durme, 2021) than to identify the correct one.", "filtered_refids": [[], ["b101", "b91", "b100", null], ["b100", "b91", "b101", null, "b95", "b109", "b106"], ["b10", "b56", "b91", null, "b95"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 4050, "num_references": 16}
{"corpusid_sectionid": "252992688-s17", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Cost Measurement", "section": "Most AL works adopt simple measurements of unit cost, that is, assuming that annotating each instance requires the same cost. Nevertheless, the annotation efforts for different instances may vary . For example, longer sentences may cost more to annotate than shorter ones. Because of this, many works assume unit costs to tokens instead of sequences, which may still be inaccurate. Especially, AL tends to select difficult and ambiguous instances, which may require more annotation efforts (Hachey et al., 2005;Lynn et al., 2012). It is important to properly measure annotation cost since the measurement directly affects the evaluation of AL algorithms. The comparisons of query strategies may vary if adopting different cost measurement (Haertel et al., 2008a;Bloodgood and Callison-Burch, 2010;Chen et al., 2015).\n\nProbably the best cost measurement is the actual annotation time (Baldridge and Palmer, 2009). Especially, when the cost comparisons are not that straightforward, such as comparing annotating data against writing rules (Ngai and Yarowsky, 2000) or partial against full annotations ( \u00a73.1; Flannery and Mori, 2015;Li et al., 2016, time-based evaluation is an ideal choice. This requires actual annotating exercises rather than simulations.\n\nSince cost measurement can also be used for querying ( \u00a73.2.2), it would be helpful to be able to predict the real cost before annotating. This can be cast as a regression problem, for which several works learn a linear cost model based on input features Ringger et al., 2008;Haertel et al., 2008a;Arora et al., 2009).", "filtered_refids": [["b91", null, "b89", "b106"], [null, "b95"], [null, "b89", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1576, "num_references": 9}
{"corpusid_sectionid": "252992688-s18", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Cost-sensitive Querying", "section": "Given the goal of reducing actual cost, the querying strategies should also take it into consideration. That is, we want to select not only highutility instances but also low-cost ones. A natural cost-sensitive querying strategy is return-oninvestment (ROI; Haertel et al., 2008b;Donmez and Carbonell, 2008). In this strategy, instances with higher net benefit per unit cost are preferred, which is equivalent to dividing the original querying utility by cost measure.\n\nTomanek and Hahn (2010) evaluate the effectiveness of ROI together with two other strategies, including constraining maximal cost budget per instance and weighted rank combination. Haertel et al. (2015) provide further analytic and empirical evaluation, showing that ROI can reduce total cost.\n\nIn real AL scenarios, things can be much more complex. For example, there can be multiple annotators with different expertise (Baldridge and Palmer, 2009;Huang et al., 2017;Cai et al., 2020), and the annotators may refuse to answer or make mistakes (Donmez and Carbonell, 2008). Being aware of these scenarios, Donmez and Carbonell (2008) propose proactive learning to jointly select the optimal oracle and instance.  further extend proactive learning to NER tasks.", "filtered_refids": [[null], [], [null, "b57"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1230, "num_references": 3}
{"corpusid_sectionid": "252992688-s19", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Directly Reducing Cost", "section": "In addition to better query strategies, there are other ways of directly reducing annotation cost, such as computer-assisted annotation. In AL, models and annotators usually interact in an indirect way where models only query the instances to present to the annotators, while there could be closer interactions.\n\nPre-annotation is such an idea, where not only the raw data instances but also the model's best or top-k predictions are sent to the annotators to help them make decisions. If the model's predictions are reasonable, the annotators can simply select or make a few corrections to obtain the gold annotations rather than creating from scratch. This method has been shown effective when combined with AL (Baldridge and Osborne, 2004;Vlachos, 2006;Ringger et al., 2008;Skeppstedt, 2013;Ca\u00f1izares-D\u00edaz et al., 2021). Post-editing for MT is also a typical example (Dara et al., 2014).\n\nMoreover, the models could provide help at real annotating time. For example, Culotta and Mc-Callum (2005) present an interactive AL system where the user's corrections can propagate to the model, which generates new predictions for the user to further refine. Interactive machine translation (IMT) adopts a similar idea, where the annotator corrects the first erroneous character, based on which the model reproduces the prediction. AL has also been combined with IMT to further reduce manual efforts (Gonz\u00e1lez-Rubio et al., 2012;Peris and Casacuberta, 2018;.", "filtered_refids": [[], ["b94", "b35", "b102", null, "b51", "b8"], ["b107", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1452, "num_references": 8}
{"corpusid_sectionid": "252992688-s20", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Wait Time", "section": "In AL iterations, the annotators may need to wait for the training and querying steps (Line 3 and 4 in Algorithm 1). This wait time may bring some hidden costs, thus more efficient querying and training would be preferable for faster turnarounds.\n\nTo speed up querying, sub-sampling is a simple method to deal with large unlabeled pools (Roy and McCallum, 2001;Ertekin et al., 2007;Tsvigun et al., 2022). For some querying strategies, precalculating and caching unchanging information can also help to speed up (Ashrafi Asli et al., 2020;Citovsky et al., 2021). In addition, approximation with k-nearest neighbours can also be utilized to calculate density (Zhu et al., 2009) or search for instances after adversarial attacks (Ru et al., 2020).\n\nTo reduce training time, a seemingly reasonable strategy is to apply incremental training across AL iterations, that is, continuing training previous models on the new instances. However, Ash and Adams (2020) show that this type of warm-start may lead to sub-optimal performance for neural models and many recent AL works usually train models from scratch (Hu et al., 2019;Ein-Dor et al., 2020). Another method is to use an efficient model for querying and a more powerful model for final training. However, this might lead to sub-optimal results, which will be discussed in \u00a74.1.\n\nAnother idea to reduce wait time is to simply allow querying with stale information. Actually, batch-mode AL ( \u00a72.2.3) is such an example where instances in the same batch are queried with the same model. Haertel et al. (2010) propose parallel AL, which maintains separate loops of annotating, training, and scoring, and allows dynamic and parameterless instance selection at any time.", "filtered_refids": [[], [null, "b50", "b84"], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1713, "num_references": 5}
{"corpusid_sectionid": "252992688-s22", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Model Mismatch", "section": "While it is natural to adopt the same bestperforming model throughout the AL process, there are cases where the query and final (successor) models can mismatch (Lewis and Catlett, 1994). Firstly, more efficient models are preferable for querying to reduce wait time ( \u00a73.2.4). Moreover, since data usually outlive models, re-using ALbase data to train another model would be desired (Baldridge and Osborne, 2004;Tomanek et al., 2007). Several works show that model mismatch may make the gains from AL be negligible or even negative (Baldridge and Osborne, 2004;Lowell et al., 2019;, which raises concerns about the utilization of AL in practice.\n\nFor efficiency purposes, distillation can be utilized to improve querying efficiency while keeping reasonable AL performance.  show that using a smaller distilled version of a pre-trained model for querying does not lead to too much performance drop. Tsvigun et al. (2022) combine this idea with pseudo-labeling and sub-sampling to further reduce computational cost. Similarly, Nguyen et al. (2022) keep a smaller proxy model for query and synchronize the proxy with the main model by distillation.", "filtered_refids": [[null, "b94", "b48"], ["b50"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1145, "num_references": 4}
{"corpusid_sectionid": "252992688-s23", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Learning", "section": "AL can be combined with other advanced learning techniques to further reduce required annotations.\n\nSemi-supervised learning. Since AL usually assumes an unlabeled pool, semi-supervised learning can be a natural fit. Combining these two is not a new idea: (McCallum and Nigam, 1998) adopt the EM algorithm to estimate the outputs of unlabeled data and utilize them for learning. This type of self-training or pseudo-labeling technique is often utilized in AL Majidi and Crane, 2013;Yu et al., 2022). With a similar motivation, (Dasgupta and Ng, 2009) use an unsupervised algorithm to identify the unambiguous instances to train an active learner. For the task of word alignment, which can be learned in an unsupervised manner, incorporating supervision with AL can bring further improvements in a data-efficient way (Ambati et al., 2010b,c).\n\nTransfer learning. AL can be easily combined with transfer learning, another technique to reduce required annotations. Utilizing pre-trained models is already a good example (Ein-Dor et al., 2020;Tamkin et al., 2022) and continual training (Gururangan et al., 2020) can also be applied (Hua and Wang, 2022;Margatina et al., 2022). Moreover, transductive learning is commonly combined with AL by transferring learning signals from different domains (Chan and Ng, 2007;Shi et al., 2008;Rai et al., 2010;Saha et al., 2011;Wu et al., 2017;Kasai et al., 2019; or languages (Qian et al., 2014;Fang and Cohn, 2017;Fang et al., 2017;Chaudhary et al., 2019Moniz et al., 2022). In addition to the task model, the model-based query policy ( \u00a72.1.4) is also often obtained with transfer learning.\n\nWeak supervision. AL can also be combined with weakly supervised learning. Examples include learning from inputs and execution results for semantic parsing (Ni et al., 2020), labeling based on identical structure vectors for entity representations (Qian et al., 2020), learning from gazetteers and dictionaries for sequence labeling  and interactively discovering labeling rules .\n\nData augmentation. Augmentation is also applicable in AL and has been explored with iterative back-translation , mixup for sequence labeling  and phraseto-sentence augmentation for MT (Hu and Neubig, 2021). As discussed in \u00a72.1.1, augmentation can also be helpful for instance querying (Jiang et al., 2020;Zhang et al., 2022b). Another interesting scenario involving augmentation and AL is query synthesis, which directly generates instances to be annotated instead of selecting existing unlabeled ones. Though synthesizing texts is still a hard problem generally, there have been successful applications for simple classification tasks (Schumann and Rehbein, 2019;Quteineh et al., 2020).", "filtered_refids": [[], [null, "b63"], ["b67", "b39", "b57", "b91", "b32", null, "b2"], ["b112"], ["b15", "b72", null, "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2699, "num_references": 14}
{"corpusid_sectionid": "252992688-s25", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Starting AL", "section": "While there are cases where there are already enough labeled data to train a reasonable model and AL is utilized to provide further improvements (Bloodgood and Callison-Burch, 2010;Geifman and El-Yaniv, 2017), at many times we are facing the cold-start problem, where instances need to be selected without a reasonable model. Especially, how to select the seed data to start the AL process is an interesting question, which may greatly influence the performance in initial AL stages Horbach and Palmer, 2016).\n\nRandom sampling is probably the most commonly utilized strategy, which is reasonable since it preserves the original data distribution. Some representativeness-based querying strategies ( \u00a72.2) can also be utilized, for example, selecting points near the clustering centroids is a way to obtain representative and diverse seeds (Kang et al., 2004;Hu et al., 2010). Moreover, some advanced learning techniques ( \u00a74.2) can also be helpful here, such as transfer learning (Wu et al., 2017) and unsupervised methods (Vlachos, 2006;Dasgupta and Ng, 2009). In addition, language model can be a useful tool, with which Dligach and Palmer (2011) select low-probability words in the context of word sense disambiguation and  choose cluster centers with surprisal embeddings by pre-trained contextualized LMs.", "filtered_refids": [[null, "b106"], ["b51", null, "b57"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1310, "num_references": 5}
{"corpusid_sectionid": "252992688-s26", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Stopping AL", "section": "When adopting AL in practice, it would be desirable to know the time to stop AL when the model performance is already near the upper limits, before running out of all the budgets. For this purpose, a stopping criterion is needed, which checks certain metrics satisfying certain conditions. There can be simple heuristics. For example, AL can be stopped when all unlabeled instances are no closer than any of the support vectors with an SVM (Schohn and Cohn, 2000;Ertekin et al., 2007) or no new n-grams remain in the unlabeled set for MT (Bloodgood and Callison-Burch, 2010). Nevertheless, these are specific to the underlying models or target tasks. For the design of a general stopping criterion, there are three main aspects to consider: metric, dataset and condition.\n\nFor the metric, measuring performance on a development set seems a natural option. However, the results would be unstable if this set is too small and it would be impractical to assume a large development set. Cross-validation on the training set also has problems since the labeled data by AL is usually biased. In this case, metrics from the query strategies can be utilized. Examples include uncertainty or confidence (Zhu and Hovy, 2007;Vlachos, 2008), disagreement (Tomanek et al., 2007;Olsson and Tomanek, 2009), estimated performance (Laws and Sch\u00fctze, 2008), expected error (Zhu et al., 2008a), confidence variation (Ghayoomi, 2010), as well as actual performance on the selected instances (Zhu and Hovy, 2007). Moreover, comparing the predictions between consecutive AL iterations is another reasonable option (Zhu et al., 2008b;Bloodgood and Vijay-Shanker, 2009a).\n\nThe dataset to calculate the stopping metric requires careful choosing. The results could be unstable if not adopting a proper set . Many works suggest that a separate unlabeled dataset should be utilized Vlachos, 2008;Bloodgood and Vijay-Shanker, 2009a;Beatty et al., 2019;Kurlandski and Bloodgood, 2022). Since the stopping metrics usually do not rely on gold labels, this dataset could potentially be very large to provide more stable results, though wait time would be another factor to consider in this case ( \u00a73.2.4).\n\nThe condition to stop AL is usually comparing the metrics to a pre-defined threshold. Earlier works only look at the metric at the current iteration, for example, stopping if the uncertainty or the error is less than the threshold (Zhu and Hovy, 2007). In this case, the threshold is hard to specify since it relies on the model and the task. (Zhu et al., 2008b) cascade multiple stopping criteria to mitigate this reliance. A more stable option is to track the change of the metrics over several AL iterations, such as stopping when the confidence consistently drops (Vlachos, 2008), the changing rate flattens (Laws and Sch\u00fctze, 2008) or the predictions stabilize across iterations (Bloodgood and Vijay-Shanker, 2009a;Bloodgood and Grothendieck, 2013). Pullar-Strecker et al. (2021) provide an empirical comparison over common stopping criteria and would be a nice reference. Moreover, stopping AL can be closely related to performance prediction and early stopping. Especially, the latter can be of particular interest to AL since learning in early AL stages need to face the low-resource problem and how to perform early stopping may also require careful considerations.\n\n6 Related Topics and Future Directions", "filtered_refids": [["b12", null, "b106"], ["b52", "b47", "b82", "b83", "b48", null, "b81"], ["b52", null], ["b52", "b81", null, "b83"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 3388, "num_references": 16}
{"corpusid_sectionid": "256461177-s1", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "What is Frame or Framing?", "section": "This section provides a conceptual understanding of framing. A classic example of framing concerns a debate over whether to permit Ku Klux Klan to hold a public rally. One news story with the headline \"Ku Klux Klan Tests OSU's Commitment to Free Speech\" reported the rally as a free speech issue, while another one with the headline \"Possible Ku Klux Klan Rally Raises Safety Concerns\" reported it as a disruption of public order. As reflected in the headlines, the two stories used different frames. People who read the free speech news story expressed higher tolerance toward KKK's rally compared to those who read the public order news story (Nelson et al., 1997, p. 581). Scholars are not agreed upon any unified framing definition (Hertog and McLeod, 2001;Van Dijk, 2016). However, a prominent definition, widely used in both traditional and computational framing studies, was provided by Entman (1993). He says:\n\nTo frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described. (p. 52) As per this definition, a frame is largely determined by its outcome effects, such as four functions: a) defining problems, b) diagnosing causes, c) making judgments, and d) suggesting remedies. The functions depend on how some selected aspects of \"perceived\" reality are made salient. In 2003, he defined it a bit differently, \"Framing entails selecting and highlighting some facets of events or issues, and making connections among them so as to promote a particular interpretation, evaluation, and/or solution\" (Entman, 2003, p. 417). This definition seems to have made a few shifts, such as from \"causal interpretation\" to \"interpretation,\" from \"moral evaluation\" to \"evaluation,\" and from \"treatment recommendation\" to \"solution.\" The salient aspects are also interconnected.\n\nWhile approaching frames as cultural phenomena, Hertog and McLeod (2001) identified a frame as a cultural \"[structure] of meaning that includes a set of core concepts and ideas,\" including \"conflicts, metaphors, myths, and narratives\" (p. 160). A frame has also been explained as \"a central organizing idea. . . for making sense of relevant events, suggesting what is at issue\" (Gamson and Modigliani, 1989, p. 3). Reese et al. (2001) defined a frame from the sociological perspective and focused on six aspects (italicize): \"Frames are organizing principles that are socially shared and persistent over time, that work symbolically to meaningfully structure the social world\" (p. 11). In a recent definition, D'angelo (2018) defined news framing as \"how journalists, their sources, and audiences work within conditions that shape the messages they construct as well as the ways they understand and interpret these messages\" (p. xxiv).\n\nTo describe a frame's aspect highlighting some selected facets of an issue or event, Fairhurst (2005) utilized an analogy that \"choosing language to frame people's actions and events is like moving a telescope into position\" (p. 125). The selected aspects are then coherently organized in a way to make an argument, which finally promotes a particular interpretation, evaluation, and solution. This organization of selected aspects could even be subtle, as framing also \"refers to subtle alterations in the statement or presentation of judgment and choice problems\" (Iyengar, 1994, p. 11). Another crucial aspect of framing is \"to choose one particular meaning (or set of meanings) over another\" (Fairhurst and Sarr, 1996, p. 3) that is also supported by Entman (1993), who says a frame \"operates by selecting and highlighting some features of reality while omitting others\" (p. 53).", "filtered_refids": [["b58", null, "b26"], [null], [null, "b26", "b46"], [null, "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3798, "num_references": 9}
{"corpusid_sectionid": "256461177-s2", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "Contexts in Framing.", "section": "A frame is considered context-sensitive. It is shaped in four locations: i) communicator, ii) texts, iii) receiver, and iv) culture (Entman, 1993). The culture is the stock of commonly invoked frames and explained as (a part of) contexts. A news report's content is fully comprehensible when its contextual information is at the disposal of readers. They interpret a frame and its meaning following contextual information (Baden and D'Angelo, 2018;Tewksbury and Riles, 2018).\n\nFraming Devices. Framing devices can be defined as tools that are used to make a piece of information more salient, which is, in other words, \"making a piece of information more noticeable, meaningful, or memorable to audiences\" (Entman, 1993, p. 53). While conceptualizing a frame, we accumulated framing devices (see Table 1). To make the list concise and convenient, we combined similar devices and put them into four groups: a) content, b) action, c) context, and d) communicator. The devices or tools can be used to provide either higher or lower salience to selected aspects of reality. In some cases, multiple devices can be applied together as a new device. For example, jargon, metaphors, and contrast can together be used to develop a \"story\" (Fairhurst and Sarr, 1996). ", "filtered_refids": [["b14", "b2", "b55"], [null, "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1258, "num_references": 5}
{"corpusid_sectionid": "256461177-s7", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "Policy Frames Codebook", "section": "Boydstun et al. (2013) and Boydstun et al. (2014) proposed a codebook named \"policy frames codebook\" (PFC). The PFC consists of 14 categories of \"frame dimensions\" and an \"other\" category. The dimensions include \"economic frames,\" \"capacity and resources frames,\" \"morality frames,\" etc. For example, a news report is labeled as an economic frame if it focuses on \"the costs, benefits, or monetary/financial implications of the issue (to an individual, family, community, or to the economy as a whole)\" (Boydstun et al., 2014, p. 6).\n\nThey developed the codebook through brainstorming and iteration of applying it to random texts. With the codebook, they deployed 3,033 coders to manually code three sets of articles on immigration, tobacco, and same-sex marriage. Using the labeled documents, they finally developed a logistic regression binary text classifiers (i.e., present or absent) (Boydstun et al., 2013(Boydstun et al., , 2014.", "filtered_refids": [[null, "b7"], ["b8", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 936, "num_references": 4}
{"corpusid_sectionid": "256461177-s8", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "Media Frames Corpus", "section": "Using PFC, Card et al. (2015) offered a manuallyannotated corpus of news reports named \"media frames corpus\" (MFC). The news reports were collected from three domains: immigration, smoking, and same-sex marriage. The MFC was applied in other studies (e.g., . Card et al. (2015) annotated the three datasets based on PFC's 15 framing dimensions (Boydstun et al., 2013). The authors, however, did not apply the annotations to any new datasets. In 2016, they added four more categories-pro, neutral, anti, and irrelevant.\n\nConceptualization in PFC & MFC. Boydstun et al. (2013Boydstun et al. ( , 2014 conceptualized framing by resorting to the widely used framing definition of Entman (1993). Overall, they put \"language\" at the center of identifying and analyzing frames. PFC's development is motivated by three framing concepts: a) frame selection varies based on various situations, b) frames evolve over time, and c) frames spread across issues, geographic locations, and institutions or organizations. Card et al. (2015) also used Entman (1993)'s definition in conceptualizing frames. They focused on some framing elements that work coherently as a framing package.\n\nReview. The authors conceptualized frames with existing framing definitions. However, framing aspects they mentioned (e.g., Entman, 1993) were not utilized in developing the 15 \"framing dimensions.\" Considering the development process and broader definitions of each frame, the 15 dimensions seem to be more fit with \"topics,\" not frames. As per the framing theory, the categorization of these dimensions looks arbitrary and too broad to understand a frame's nuances. For example, a text is identified as an \"economic frame\" if it focuses on anything of the whole economy. Let's consider the Ku Klux Klan's example mentioned above. As per MFC's 15 dimensions, both KKK news reports could probably be identified as a \"law and order, crime and justice frame\" under the PFC. Here, it does not answer the \"how\" question at all. The dimensions, however, can be considered as topics. The MFC corpus inherited the same limitations as it was developed using the PFC codebook.", "filtered_refids": [["b11", "b8"], ["b11", "b8", "b7"], ["b14"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 2136, "num_references": 6}
{"corpusid_sectionid": "256461177-s11", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "Topic Modeling", "section": "Various prior studies utilized topic modeling (TM) to explore frames (e.g. DiMaggio et al., 2013).\n\nMethod. The TM algorithm discovers latent themes in a large collection of documents (Blei, 2012). A topic is a probability distribution over a fixed vocabulary (p. 78). The algorithm produces a number (k) of lists of words based on the words' higher probability of being in a list. Each list of words is considered to be a topic, and each topic has a different probability distribution. The latent Dirichlet allocation (LDA) topic model provides an assignment of each document to the topic(s). As a mixed-membership model, each of its documents may be assigned to multiple topics, considering that a document could have elements of multiple topics. DiMaggio et al. (2013) used the LDA topic modeling to explore frames. They view each topic as a frame, saying that a topic \"includes terms that call attention to particular ways\" (p. 593).\n\nConceptualization. In the study of DiMaggio et al. (2013), they conceptualized a frame as \"a set of discursive cues (e.g., words, images, and narrative) that suggests a particular interpretation of a person, event, organization, practice, condition, or situation\" (p. 593). They cited Gamson et al. (1992)'s definition that a frame is \"a central organizing principle that holds together and gives coherence and meaning to a diverse array of symbols.\" They considered each topic as a frame.\n\nReview. Here, the conceptualization of a frame looks consistent with the overall framing idea. However, the topic model's output (i.e., lists of words) and their interpretation seem not aligned with framing aspects. A list of words in the topic model comes without any connection among them due to its features (e.g., bag-of-words). The interpretation of each word list in DiMaggio et al. (2013) also indicates it as a theme or issue, not a frame. For example, they reported the results by utilizing words like \"highlight,\" \"emphasize,\" and \"concerned with\" (e.g., this topic highlights legislative actions). Framing nuances like a problem and causal interpretation could not be extracted here.", "filtered_refids": [["b12"], ["b12", "b5"], ["b20"], ["b12"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2124, "num_references": 5}
{"corpusid_sectionid": "256461177-s12", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "Structural Topic Modeling (STM)", "section": "Method. The STM model was also used to explore frames (e.g., Roberts et al., 2014). Compared to LDA topic modeling (Blei, 2012), STM allows including metadata or covariates in the model. With metadata (e.g., political ideology and time) added to the dataset and model, the STM allows researchers to interpret how the topics are associated with those metadata. For example, in terms of political ideology, such as conservatives and lib-erals, researchers might identify a topic as more aligned with conservatives and another topic with liberals. Metadata can also be used in predicting the topics' prevalence by metadata (Gilardi et al., 2021;Nicholls and Culpepper, 2021).\n\nIn their study exploring topics in a corpus of newspaper texts, Gilardi et al. (2021)  Review. Like the topic modeling approach (Gilardi et al., 2021), the STM algorithm is also constrained by considering a topic as a frame. So, the STM contains similar limitations in terms of framing analysis. Compared to topic modeling, the STM offers additional insights into the topics or themes through the analysis of covariates. Both methods are based on the bag-of-words idea, indicating the lack of semantic contextualization needed for exploring frames.", "filtered_refids": [["b47", "b23", "b44"], ["b23"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1222, "num_references": 4}
{"corpusid_sectionid": "256461177-s13", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "Hierarchical Topic Modeling", "section": "Method. Studies also used hierarchical topic modeling (HTM) to explore frames. Nguyen (2015) and  introduced an HTM model named \"Supervised Hierarchical Latent Dirichlet Allocation (SHLDA)\" that aims to analyze frames in a large dataset. As the SHLDA works, each document in the corpus is associated with a continuous level of scores (e.g., conservative vs. liberal ideology). It produces a hierarchy of topics, where the first-level nodes are considered agendas and the second-level nodes as frames. Documents' scores help explain how the topics are framed concerning respective people's positions. Its document generative process combines the hierarchical LDA and hierarchical Dirichlet process (HDP). The authors applied it to three datasets and conducted qualitative and quantitative analyses to validate the models' agenda and frames.\n\nConceptualization. Nguyen (2015) also used the framing definition of Entman (1993) in conceptualizing a frame. However, unlike Gilardi et al. (2021), Nguyen (2015) considered a topic as an agenda (e.g., what topics are talked about) and a sub-topic as a second-level agenda or a frame (e.g., how these topics are talked about).\n\nReview. As elaborated above, the SHLDA is one step ahead of topic modeling. However, a crucial incongruity remains in how they conceptualized a frame (e.g., sub-topics) and interpreted the results. Though there is a lack of unified framing definition, the idea of considering a sub-topic as a frame does not align with traditional framing conceptualization (Entman, 1993;McCombs et al., 1997;Ghanem, 1997). Like many prior framing studies, the SHLDA output might also be considered as simply topics and their relevant attributes, not frames. Moreover, Nguyen (2015)'s qualitative analysis to validate the output as frames is not systematically executed, and the presentation of its results does not illustrate any framing aspects (Entman, 1993)", "filtered_refids": [["b42"], ["b42", "b23"], ["b14", "b38", "b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1914, "num_references": 6}
{"corpusid_sectionid": "256461177-s14", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "Cluster Analysis", "section": "Method. The k-means clustering algorithm is another unsupervised approach used to explore frames. Burscher et al. (2016) conducted two k-means clustering in a dataset. One includes all words, and another includes selected words (i.e., nouns, adjectives, and adverbs). After creating document vectors with TFIDF in both groups, they conducted k-means clustering to find clusters. As a centroid-based clustering approach works, a certain number of clusters (k) is specified in advance, and each cluster is represented by its center. They select the number of clusters (k) using the \"elbow method.\" Each document is assigned to a cluster based on its relatively closer distance to that cluster center (Burscher et al., 2016). Unlike topic modeling, k-means clustering is a single-membership approach where each document generally belongs to one cluster.\n\nConceptualization. Burscher et al. (2016) conceptualized a frame in terms of \"word frequencies\" and mentioned words as highly reliable and less biased in producing frames. They \"used word frequencies as features [of a frame] in [their] cluster analyses\" (p. 533). They utilized traditional framing definition partially (e.g., presence or absence of certain keywords, stock phrases) (Entman, 1993).\n\nReview. As Burscher et al. (2016) conceptualized and interpreted frames in terms of word frequencies and co-occurrences, the framing devices listed in Table 1 suggest that word(s) are simply one of the many devices to construct a frame. They utilized such conceptualization that does not help explore frames despite their acknowledgment that \"based on plain word features, a cluster analysis cannot reveal complex semantic and logical relationships like causality\" (Burscher et al., 2016, p. 541). As a single-membership approach, this method is also against one of the core framing ideas that a framing device may belong to multiple frames. The results were presented with words, including \"refer to.\" For example, \"cluster B5 refers to nuclear power . . . in Iran\" (p. 439). The results indicate these as a topic or issue. It does not indicate \"how\" the \"nuclear\" issue was discussed and evaluated as a problem. Both conceptualization and output seem to illustrate certain topics, not frames.", "filtered_refids": [["b9"], [], [null, "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2245, "num_references": 3}
{"corpusid_sectionid": "256461177-s15", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "Neural Network Model", "section": "Method. Some studies utilized the neural network approach to build frame-identifying classifiers and analyzed frames in various text documents (e.g., news reports and tweets). Mainly, two annotated datasets namely, MFC and GVFC, were used in building these models.\n\nMFC was utilized in a number of such studies, including probabilistic soft logic (PSL) (Johnson et al., 2017), LSTM neural network (Naderi and Hirst, 2017), recursive neural network (Ji and Smith, 2017), and transformer-based language models such as BERT and RoBERTa (Khanehzar et al., 2019;Cabot et al., 2020;Mendelsohn et al., 2021). Some studies used MFC's annotated news reports partially and some used the full corpus.\n\nManually annotating the GVFC dataset,  used it to build a classifier using BERT. It was later applied in other studies (e.g., Aky\u00fcrek et al., 2020;Tourni et al., 2021;Bhatia et al., 2021).\n\nConceptualization. As mentioned above, Liu et al. (2019) used traditional framing definitions (e.g., Entman, 1993) while conceptualizing a frame. The studies applying MFC in building a neural network-based classifier also conceptualize it by drawing works from prior studies in both social and computational science.\n\nReview. In terms of the approach, both groups of studies seem to have applied the state-of-theart pre-trained models based on transfer learning that looks promising for advancing computational framing analysis. However, the quality of the annotated training dataset appears not up to the mark, which is reflected in the lack of results interpreta-tion in those studies. As reviewed above, the MFC dataset seems more about categorizing a text into broad topics (e.g., \"economic frames\"), not frames. The subsequent studies applying MFC dataset also did not adequately justify MFC's 15 dimensions as frames. Their results mainly focused on the accuracy of the model built on MFC training dataset, but not whether the results provide framing nuances.\n\nCompared to MFC, GVFC's annotations look more coherent but still lack in capturing framing nuances, as mentioned above in sub-section 4.1.3. For example, based on GVFC's \"politics\" code,  interpreted its result saying, \"it appears that news media of all types have largely politicized the gun violence issue right after each major mass shooting\" (p. 511). Here, the politicization result and its interpretation do not align with how the code is defined. The results might indicate the texts \"discussed\" \"a politician\" or politics, which is a simple topic or an issue, not any major framing element like problem definition and its coherent argument.", "filtered_refids": [[], ["b72", "b40", "b28", "b32", "b70"], ["b1", "b56", "b4"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2596, "num_references": 8}
{"corpusid_sectionid": "256461177-s16", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "Parsing Semantic Relations", "section": "Another line of computational framing analysis relates to the exploration of semantic relations, going beyond the bag-of-word model.\n\nMethod. Sturdza et al. (2018) operationalized Entman (1993)'s four framing elements as their semantic relations in texts. This approach proposed the utilization of a rule-based system that uses existing computational software, such as TurboParser, and implicature rules. Using the parser, the author proposed identifying syntactic structures in texts and then using a set of rules to transform the syntactic structure into semantic networks. The networks determine the semantic roles of each word (e.g., actors, events) through a set of sentiment analysis implicature rules using a sentiment lexicon.\n\nOn the other hand, Ziems and Yang (2021) computationally parsed various attributes (e.g., race) of police shooting victims in news reports and explored how differently they are portrayed in news media. They called it \"entity-centric framing.\" A recent study by  looked at iterative adverbs (e.g., again) in the political discourse considering the adverbs evoke different attitudinal subtexts. After extracting sentences with relevant adverbs, the author grouped the sentences through k-means clustering and identified the most representative keywords in each cluster by a keyword mining tool.\n\nConceptualization. In conceptualizing a frame, Sturdza et al. (2018) relied on four framing elements of Entman (1993, p. 52). However, two other studies lack adequate conceptualization of framing. For instance, Ziems and Yang (2021) mainly explored \"entity-centric\" frames but did not elaborate on it from existing literature.\n\nReview. Compared to the topic modeling method, this approach looks innovative in terms of understanding semantic relations between words and phrases. However, the idea seems not adequately exploited in understanding the nuances of frames. For example, Sturdza et al. (2018) did not apply the operationalization in a practical dataset. Results of Ziems and Yang (2021) reported frequency and correlations while Yu (2022)'s results ended up with clustering and keywords, instead of exploring the coherent argument and relations among various framing devices. However, by its design, the semantic relations approach holds the potential for being used in advancing the computational methods of framing analysis.", "filtered_refids": [[], [], [], ["b64", null, "b51"], ["b64", "b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2365, "num_references": 5}
{"corpusid_sectionid": "256461177-s19", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "Analysis of Topic Model Networks", "section": "Walter and Ophir (2019) proposed this mixed method approach, \"Analysis of Topic Model Networks\" (ANTMN), that combines topic modeling and semantic network analysis. It was applied in other studies (e.g., .\n\nMethod. ANTMN includes three steps. First, the authors apply LDA topic modeling (Blei, 2012) to the dataset. They label each topic by qualitatively examining three types of information: a) words with the highest loading over each topic, b) prevalent and exclusive words in each topic, and c) full documents that are the most representative of each topic. Second, ANTMN creates a semantic network, where the topics serve as nodes, and topics' similarity relationships serve as edges. The relationship is calculated based on the topics' cooccurrence in the documents. The output provides a fully connected, undirected, and weighted network. Finally, a community detection algorithm was used to cluster the topics into various communities in the network based on the topics' prevalence in similar documents (Walter and Ophir, 2019).\n\nConceptualization. As the authors noted, ANTMN can analyze emphasis frames (e.g., highlighting one side), not equivalency frames (e.g., gain vs. loss issue). They conceptualize a frame as \"a communit[y] in a network of topics\" (p. 248), based on linguistic patterns. Borrowing van Atteveldt and Peng (2018)'s idea of arranging various framing devices around an overarching idea (e.g., a cluster of relevant framing devices), they con-sider each topic in topic modeling as a framing device. The cluster of topics was named as a frame in ANTMN. They embraced the patterns of a frame that \"repeatedly invokes the same objects and traits, using identical or synonymous words and symbols in a series of similar communications that are concentrated in time\" (Entman et al., 2009, p. 177).\n\nReview. A few things seem to have restricted ANTMN as a framing analysis model. As per the framing conceptualization, the topics (aka framing devices) under each network community need to be coherently connected with each other to render a coherent framing argument. The authors did not explain how the devices are coherently interconnected. This lack is reflected in the interpretation of the results. For instance, they reported a framing result, saying that \"the largest community on the right consisted of topics about the cultural and economic consequences. . . . Articles dominated by these topics portrayed the impact of diseases on the economy at large. . . . (Walter and Ophir, 2019, p. 259). Here, the authors mentioned topics' names and what these topics portray with words like \"consists of\" and \"portrayed.\" The results did not provide a coherent argument of the problem or how one aspect is interconnected with another. Though the output demonstrated some topics, the authors' claim of the communities as frames is not supported with adequate evidence.\n\nDespite the authors' claim of this method as unsupervised, manual human labor is still needed in at least two places: a) an examination of words and documents to label topics and b) an interpretation of findings. However, no systematic method was provided for executing the manual analysis.", "filtered_refids": [[], ["b59", "b5"], [null], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 3180, "num_references": 4}
{"corpusid_sectionid": "1509090-s3", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Use", "section": "Cases. The article focuses on two use cases of MWE processing at the heart of language technology for which the correct handling of MWEs is equally crucial: parsing and MT. These uses were chosen because they are representative of past and current efforts to develop MWE-aware applications. There have been some attempts to integrate MWE processing into other applications such as information retrieval and sentiment analysis, but we do not cover them in this survey.\n\nParsing is generally concerned with the definition of algorithms that map strings to grammatical structures. Although MWE-aware parsers represent only a small portion of the total parsing literature, we argue that proper MWE identification can improve parser performance (Cafferkey, Hogan, and van Genabith 2007). In particular, complex function words that have a key role in syntax may be ambiguous (e.g., by the way). Failing to identify MWEs will lead to parsing errors. Clearly, a key characteristic of all MWE-aware parsing algorithms is that they must in some way have access to preexisting MWE resources. There are many ways to represent such resources and to incorporate them into the parsing process and this gives rise to the observed variation in the design of such algorithms (Section 4).\n\nMT is more complex than parsing insofar as it involves not only the identification of source MWEs but also their translation into the target language. Although phrase-based approaches aimed at capturing the translation of multiword units and may in principle handle contiguous MWE categories such as compounds, these approaches will certainly not be able to handle discontiguous MWEs, and neither will they cater for variants of MWEs, unseen in the training data. Attempts at MWE-aware MT have shown variable results, according to the category of MWE under consideration and the given language pair, but have proved beneficial in a number of cases (Pal, Naskar, and Bandyopadhyay 2013;Cap et al. 2015). As with parsing, pre-existing resources are necessary and there are several ways to integrate such resources in the translation process (Section 5).\n\nBecause the properties of MWEs represent challenges to one process, but opportunities for another (Section 1.2), they induce a complex pattern of bidirectional interactions. Figure 1 gives an overview of the main support relations between the two processes involved in MWE processing and our two selected use cases.\n\nThe single arrows in Figure 1 indicate a support relationship. So the arrow from discovery to identification means that discovery supports identification in virtue of the lexical resources that discovery yields. Similarly, the arrows from MT and parsing to discovery indicate that the outputs of both parsing and MT have been shown to support discovery. Syntactic analysis can help deal with discontiguity, as exemplified above, and non-literal translations can serve as a cue for ranking non-compositional MWEs for discovery. 9 The bidirectional arrows indicate two-way support. Parsing can support identification, for example, when a grammatical relationship must hold between MWE components. Translation can also support identification on the target side given a pair of parallel texts. The converse relations also hold. Identification can support parsing in that the identified MWE legitimates special treatment by the parser. It can also support the correct translation of an MWE identified on the source side.\n\nNote that this picture shows the main support relations found in previous work only. Additional arrows are possible and in Section 6 we argue for a large-scale evaluation over a systematic set of experiments that cover the less populated areas of the interaction landscape as well.\n\n1.3.3 Orchestration. This complex set of interactions, and in particular the directions in which they operate, give rise to a variety of architectures that differ in how MWE processing is scheduled with respect to the use case. More precisely, they define whether MWE processing is done before (as preprocessing), after (as postprocessing), or during (jointly with) the given use case. Although joint systems perform both tasks simultaneously, preprocessing and postprocessing can be seen as pipelines, in which the output of one process constitutes the input of the next one.\n\nThe following sections on MWE discovery, MWE identification, parsing, and MT further explain how the core tasks of MWE processing are incorporated into the use cases and vice versa. In particular, they develop the notion of orchestration, the effort of trying to find the best entry-point for one process to help the other-for example, the optimum moment to introduce MWE identification into the parsing pipeline.\n\nThe parsing literature reveals that authors have chosen different entry-points for MWE identification in the process. The choice of MWE identification before parsing, where methods are used to partly annotate words and sequences in advance, can reduce the search space of the parsing algorithm. Otherwise one can opt to do MWE identification after parsing, allowing it to benefit from the available syntactic analysis. MWE identification during parsing has the benefit that several alternatives can be maintained and resolved with joint learning models.\n\nWe see alternative approaches to orchestration in the literature on MT as well. On the one hand, we find MWE identification before translation methods (the so-called static approaches) that concatenate MWEs as a preprocessing step or conversely split compositional closed compounds in Germanic languages to distinguish them from noncompositional compounds. On the other hand, we find MWE identification during the translation process itself (so-called dynamic approaches).", "filtered_refids": [[], ["b37"], [null, "b41"], [], [null], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 39, "num_chars": 5761, "num_references": 4}
{"corpusid_sectionid": "1509090-s4", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Resources.", "section": "Much of the glue that holds together the network of interactions shown in Figure 1 is composed of resources, which in the case of MWEs fall into three basic categories: lexical resources, (parallel) corpora, and treebanks. Lexical resources are essentially databases, and include MWE lexicons and general-purpose dictionaries containing MWE material. Both are useful for handling specific categories of MWE, such as multiword named entities (Steinberger et al. 2013), multiword terms, or idioms. Lexical resources are particularly effective for the identification of highly fixed MWEs. Otherwise, they may be combined with rules describing possible syntactic constraints within MWEs.\n\nCorpora consist of natural text, and may be annotated in different ways. Minimally, tags are simply used to delimit MWEs. Further information, concerning MWE categories, for example, can be added as tag features. Progressively more refined information can approach the level of expressiveness found in treebanks. Examples of annotated corpora with MWE tags include Wiki50 (Vincze, Nagy, and Berend 2011), STREUSLE (Schneider et al. 2014b), and the PARSEME shared task corpora (Savary et al. 2017). Two or more corpora can also be set in correspondence. For example, parallel corpora in different languages include sentence-level alignment and are used to detect manyto-many, one-to-many, or many-to-one translations. An example of MWE-annotated parallel corpus is the English-Hungarian SzegedParallelFX corpus (Vincze 2012).\n\nFinally, treebanks are special corpora that include syntactic relations between nodes over text segments and are arguably the most valuable resources for data-driven parsing systems and syntax-aware MT systems. In the literature, there exist different opinions on whether syntactically regular but semantically idiomatic MWEs should be identified in syntactic treebanks. Although the Penn Treebank designers prefer not to annotate verbal MWEs (Marcus, Marcinkiewicz, and Santorini 1993), these are annotated in the Prague Treebank (Bej\u010dek et al. 2012). For example, whereas the syntactic structure within light-verb constructions such as to make a decision is annotated with special MWE relations in the Prague Treebank, they are annotated as regular verb-object pairs in the Penn Treebank. Although, at the time of writing this survey, there is still not a universal standard for MWE annotation, one of the main goals of the PARSEME network is to develop annotation guidelines for MWE representation in both constituency and dependency treebanks. For an up-to-date status of the current annotations for different languages, see Ros\u00e9n et al. (2015Ros\u00e9n et al. ( , 2016. Appendix A provides a complementary list of resources and tools for MWE processing.\n\nIdentification relies on lexical resources that can be either the fruit of discovery or hand-built. Both parsing and MT rely on lexical resources as well, either through a separate identification step or by using them internally. For example, MWE lexicons are important for MT within preprocessing, postprocessing, and translation phases of different paradigms: They are mainly used to delimit MWEs, replacing them by either a single token, a sense identifier, or by a translation equivalent before alignment takes place. In addition to lexicons, statistical parsing depends on treebanks annotated with MWEs, and MT relies on parallel corpora (or treebanks for syntax-aware MT) to retrieve translation equivalents for the MWEs it has identified.", "filtered_refids": [["b190"], ["b177", "b203", "b204", "b172"], ["b22", "b113", "b164", "b165"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 3511, "num_references": 9}
{"corpusid_sectionid": "1509090-s7", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "MWE Discovery", "section": "Our survey focuses on interactions of MWE processing with parsing and MT. However, we cannot discuss these interactions without providing an overview of approaches in discovery and identification. Other surveys on these tasks have been previously published (Baldwin and Kim 2010;Seretan 2011;. Our main contributions are to cover the latest advances and group references across languages and MWE categories according to each method's characteristics. Hence, the goal of this section is to define MWE discovery and provide a concise overview of the current state of affairs.\n\nThis section describes existing approaches for MWE discovery. As defined in Section 1.3, discovery is a process that takes as input a text and generates a list of MWE candidates, which can be further filtered by human experts before their integration into lexical resources. This process is depicted in Figure 2.\n\nAutomatic MWE discovery (hereafter simply referred to as discovery) has been an active research topic since the end of the 1980s when a number of seminal papers were published (Choueka 1988;Church and Hanks 1990). The famous \"pain-in-the-neck\" paper  and the related MWE workshops (Bond et al. 2003) have put discovery in focus as one of the main bottlenecks of NLP technology. Since then, considerable progress has been made, notably in the context of national and international research projects like PARSEME (Savary et al. 2015).\n\nWhereas discovery methods generate lists of MWE types out of context, MWE identification marks MWE tokens in running text. However, several terms have been used to designate what we have defined as discovery in our conceptual framework (Section 1.3), such as identification, extraction, acquisition, dictionary induction, and learning. Because one of the aims of this article is to clearly delineate the tasks of, on the one hand, discovering MWE types, and on the other, identifying MWE tokens in running text (Section 3), discovery seemed the most suitable term at the right level of specificity. Our survey focuses on empirical strategies for MWE discovery as opposed to expert lexicon construction by human language experts. Empirical methods try to automatically learn lexical information from textual data. In practice, empirical and expert methods", "filtered_refids": [["b184", "b13"], [], ["b48", null, "b165"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2277, "num_references": 5}
{"corpusid_sectionid": "1509090-s10", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Main MWE Discovery Methods", "section": "Some methods are designed to deal with specific categories of MWEs, for instance, focusing on noun compounds (Girju et al. 2005;, lightverb constructions (Stevenson, Fazly, and North 2004), or verb-particle constructions (McCarthy, Keller, and Carroll 2003;Ramisch et al. 2008b). Others are generic and deal uniformly with many MWE categories (da Silva et al. 1999;Seretan 2011). In any case, discovery methods can be differentiated according to the linguistic properties of MWEs that they leverage, some of which have already been discussed (Section 1.2).\n\nr Collocation. Words that are part of an MWE tend to co-occur more often than expected by chance. This property is generally modeled by methods based on association measures such as prominent co-occurrence (Section 2.2.1).\n\nr Non-substitutability. It is not possible to replace part of an expression by a synonym or similar word. This property is generally modeled by variability or fixedness measures (Section 2.2.2).\n\nr Non-compositionality. The meaning of the whole expression cannot be inferred from the meanings of its parts. This property is generally leveraged by models based on vector-space semantic similarity (Section 2.2.3).\n\nr Non-literal translatability. Word-for-word translation tends to generate unnatural, ungrammatical and sometimes nonsensical results. Monolingual and multilingual discovery methods based on translation asymmetries use techniques inspired from MT, and are thus presented later (Section 5.2.1).\n\nThese properties are not orthogonal. For example, non-literal translation and non-substitutability are side-effects of non-compositionality, and because noncompositionality is hard to model, such derived properties are additionally used in discovery.\n\nThe use of morphosyntactic and syntactic patterns is quite common to generate a first list of MWE candidates 11 of a specific category. For instance, a list of candidate nominal compounds in English can be obtained by looking for nouns preceded by other nouns or adjectives. Justeson and Katz (1995) suggest a limited set of seven bigram and trigram POS patterns, combining nouns and adjectives, in order to discover nominal compound candidates in English. Baldwin (2005) investigates the use of increasingly sophisticated morphosyntactic and syntactic patterns to discover new verb-particle constructions in English.\n\nBecause such patterns are used as a preprocessing technique by many discovery strategies presented in the remainder of this section, we do not discuss them in detail. The benefits of using POS-taggers and parsers to help discovery is represented by the unidirectional arrow from parsing to discovery in Figure 1 and will be discussed further in Section 4.2.1.", "filtered_refids": [["b185", "b78", "b184", "b116", "b191", "b154"], [], [], [], [], [], ["b11", "b91"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2721, "num_references": 8}
{"corpusid_sectionid": "1509090-s11", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Association", "section": "Measures. Prominent co-occurrence, that is, collocation, is the simplest MWE property to model in computational systems. It can be accurately captured by statistical association measures, which estimate the association strength between words in a corpus based on their co-occurrence count and on their individual word counts. Most measures take into account the observed co-occurrence count of a group of n words w 1 , w 2 . . . w n compared with its expected count. The expected co-occurrence count is based on the assumption that words are independent, that is, it equals the product of their individual word probabilities. 12 A popular association measure in MWE discovery is pointwise mutual information. It was first proposed in terminology discovery by Church and Hanks (1990), and can be expressed as the log-ratio between observed and expected counts. Values close to zero indicate independence and the candidate words are discarded, whereas large values indicate probable MWEs. Other measures are based on hypothesis testing. If we assume as null hypothesis that words are independent, their observed and expected counts should be identical. Using a test statistic like Student's t, large values are strong evidence to reject the independence null hypothesis, that is, the candidate words are not independent and probably form an MWE.\n\nMore sophisticated test statistics for two-word MWE candidates take into account their contingency table. Examples of such measures are \u03c7 2 and the more robust likelihood ratio (Dunning 1993). Pedersen (1996) suggests using Fisher's exact test in automatic MWE discovery, and this measure is implemented among others in the Text:NSP package. 13 Another measure for MWE discovery is the average and standard deviation of the distance between words, implemented in Xtract (Smadja 1993). Because these measures are based on frequency counts, there have been some studies to use Web hits as an alternative to corpus counts, in order to avoid low-frequency estimates (Keller and Lapata 2003;Ramisch et al. 2008a).\n\nAlthough association measures work quite well for two-word expressions, they are hard to generalize to arbitrary n-word MWE candidates. One simple approach is to merge two-word MWEs as single tokens and then apply the measure recursively. For instance, in French, the MWE faire un faux pas (lit. to make a false step, 'to make a blunder') can be modeled as the verb faire (to make) combined with the compound faux_pas (blunder), which had been merged due to high association in a previous pass (Seretan 2011). The LocalMaxs algorithm finds optimal MWE boundaries by recursively including left and right context words, stopping when the association decreases (da Silva et al. 1999). 14 A similar approach, using a lexical tightness measure, was proposed to segment Chinese MWEs (Ren et al. 2009).\n\nAssociation measures can be adapted according to the morphosyntactic nature of lexical elements. Hoang, Kim, and Kan (2009) propose new measures where very frequent words such as prepositions are weighted differently from regular tokens. Comparisons between different association measures have been published, but to date no single best measure has been identified (Pearce 2002;Evert 2005;Pecina 2008;Ramisch, De Araujo, and Villavicencio 2012).", "filtered_refids": [[null], ["b152", "b187", "b59", null, "b95", "b145"], ["b184", "b185", "b158"], ["b86", "b151", "b143", "b142", "b64"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3297, "num_references": 15}
{"corpusid_sectionid": "1509090-s13", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "850", "section": "Pearce's (2001) early synonym substitution method replaces parts of the MWE by synonyms obtained from WordNet, and then obtains frequencies for the artificially generated MWE variants from external sources. Instead of using variant frequencies directly, it is possible to estimate an MWE candidate's frequency using a weighted sum of variant corpus frequencies (Lapata and Lascarides 2003) or Web-based frequencies (Keller and Lapata 2003). A similar approach is used by Villavicencio et al. (2007) and Ramisch et al. (2008a), but instead of synonym variations, the authors generate syntactic permutations by reordering words inside the MWE, combining frequencies using an entropy measure. Artificially generated variants can be transformed into features for supervised discovery methods, as we will see in Section 2.2.4 (Lapata and Lascarides 2003;Ramisch et al. 2008a).\n\nMethods based on variant generation and/or lookup were used to discover several MWE categories, such as English verb-particle constructions (McCarthy, Keller, and Carroll 2003;Ramisch et al. 2008b), English verb-noun idioms (Fazly and Stevenson 2006;Cook, Fazly, and Stevenson 2007), English noun compounds (Farahmand and Henderson 2016), and German noun-verb and noun-PP idioms (Weller and Heid 2010).\n\nSuch methods often require external lexicons or grammars describing possible variants, like synonym lists or local reorderings (e.g., Noun 1 Noun 2 \u2192 Noun 2 of Noun 1 ). Synonyms or related words in substitution methods can come from thesauri like a WordNet and VerbNet (Pearce 2001;Ramisch et al. 2008b). Related words can be found in automatically compiled thesauri built using distributional vectors (Riedl and Biemann 2015;Farahmand and Henderson 2016). When compared with association measures, most of these methods are hard to generalize, as they model specific limitations that depend on the language and MWE category.", "filtered_refids": [["b201", "b152", "b95", "b106"], ["b52", "b66", "b116", "b154", "b214", "b68"], ["b66", null, "b159", "b154"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1902, "num_references": 14}
{"corpusid_sectionid": "1509090-s14", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Semantic Similarity.", "section": "Models based on semantics account for the fact that many MWE categories are partly or fully non-compositional. Because the meaning of the parts does not add up to the meaning of the whole, there should be little similarity between the computational-semantic representation of MWEs and of words that constitute them. For instance, let us consider the items cat, dog, hot dog, and sandwich. We would expect that dog is similar to cat, dog is not similar to hot dog, and hot dog is similar to sandwich.\n\nSemantic similarity methods differ mainly in how they represent word and MWE senses, how they combine senses, and how they measure similarity. Word and MWE senses can be modeled using entries of semantic lexicons like WordNet synsets (McCarthy, Venkatapathy, and Joshi 2007). However, most discovery methods use distributional models (or word embeddings) instead, where senses are represented as vectors of co-occurring context words (Baldwin et al. 2003;Korkontzelos 2011). The creation of such vectors in distributional models has several parameters that affect the performance of MWE discovery, such as the number of vector dimensions and the type of context window (Cordeiro et al. 2016). The evaluation of discovery methods based on distributional similarity can use dedicated test sets (Reddy, McCarthy, and Manandhar 2011;Farahmand and Henderson 2016) or use handbuilt resources such as WordNet (Baldwin et al. 2003).\n\nBecause methods based on distributional semantics use contextual information to represent meaning, they are closely related to substitution methods described in Section 2.2.2. For instance, Riedl and Biemann (2015) design a measure that takes into account the similarity of an MWE with single words that appear in similar contexts. They assume that MWEs tend to represent more succinct concepts, thus their closest distributional neighbors tend to be single words. Therefore, their method can be classified both as a substitution-based method (replace an MWE by a single word in context) and as a semantic similarity method.\n\nThere are several ways to combine and compare distributional vectors for MWE discovery. McCarthy, Keller, and Carroll (2003) consider the overlap between the set of distributional neighbors of a verb-particle construction and its single-verb counterpart. For instance, if to break up and to break share many neighbors, then to break up must be more compositional than to give up, which has no shared neighbor with to give (Baldwin et al. 2003). A popular measure to discover idiomatic MWEs is the cosine similarity between the MWE vector and the member word vectors (Baldwin et al. 2003;Reddy, McCarthy, and Manandhar 2011).  compare two similarity measures: (a) the weighted average similarity of the MWE vector with its member word vectors, and (b) the similarity between the MWE vector and the average vector of the component words.\n\nSemantic similarity methods have been successfully evaluated on small samples of verb-particle constructions (Baldwin et al. 2003;Bannard 2005), verb-noun idioms (McCarthy, Venkatapathy, and Joshi 2007), and noun compounds (Reddy, McCarthy, and Manandhar 2011;Yazdani, Farahmand, and Henderson 2015;Cordeiro et al. 2016). Their adaptation to large-scale discovery remains to be demonstrated.", "filtered_refids": [[], ["b12", "b54", "b157", "b66", null, "b117"], ["b159"], ["b12", "b116", "b157"], ["b12", "b54", "b157", "b117", "b16", "b218"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3281, "num_references": 16}
{"corpusid_sectionid": "1509090-s15", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Supervised Learning.", "section": "Supervised learning approaches for discovery use annotated data sets as training material to learn how to distinguish regular word combinations from MWEs. Often, the features used in supervised methods include scores derived from unsupervised methods discussed above, such as association measures and semantic similarity.\n\nThe use of these as features has proven to be an effective way to combine scores, giving more weight to more discriminating features and reducing the weight of redundant ones (Ramisch et al. 2008a). It also provides a workaround for the problem of choosing a scoring method for a given data set among dozens of methods proposed in the literature. Furthermore, the learned models can provide insight into features' informativeness (Ramisch et al. 2008b).\n\nOne of the first experiments using a supervised approach was proposed by Lapata and Lascarides (2003). The authors use a C4.5 decision tree to classify noun-noun compounds into true MWEs and random co-occurrence. Logistic regression, linear discriminant analysis, support vector machines, and neural networks have been used as classifiers for collocation discovery in Czech and German (Pecina 2008). Rondon, Caseli, and Ramisch (2015) propose an iterative method for the perpetual discovery of novel MWEs. The system requires some initial supervision to build a seed MWE lexicon and classifier, and incrementally enriches it by mining texts in the web and bootstrapping from its results. Yazdani, Farahmand, and Henderson (2015) use light supervision in the form of a list of noun compounds automatically extracted from Wikipedia. They are used as training material to tune their composition function parameters. A similar approach was also used by Farahmand and Henderson (2016) to model MWE substitutability. Supervised methods are generally very precise but cannot be systematically preferred, as they require annotated data sets. Unfortunately, such data sets are usually (1) not readily available, (2) quite small and specific, and (3) not applicable when the target MWEs are highly ambiguous.", "filtered_refids": [[], ["b154", "b152"], ["b143", "b66", "b163", "b218", "b106"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2076, "num_references": 7}
{"corpusid_sectionid": "1509090-s16", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Evaluation of MWE Discovery", "section": "Discovery is a process whose goal is to find new lexical entries. Its evaluation is tricky because the utility and relevance of these entries is hard to assess. Most discovery methods output ranked MWE lists, which can be evaluated as follows:\n\nr Post hoc human judgments: Given the top n MWEs retrieved by a method, experts select the relevant ones. The proportion of positive entries found is used to evaluate the method (da Silva et al. 1999;Seretan 2011). r Dictionaries: The returned list can be automatically evaluated by checking the entries already present in a gold standard dictionary. This assumes that entries absent from the dictionary are wrong (Ramisch, De Araujo, and Villavicencio 2012;Riedl and Biemann 2015).\n\nr Dedicated data sets: Given a list of known positive and negative MWE examples, true MWEs should be ranked first. This can be seen as an information retrieval problem, and measures like average precision, precision at k, recall, and F1 can be used to evaluate the discovery method (Evert 2009;Pecina 2008;Yazdani, Farahmand, and Henderson 2015).\n\nr Extrinsic evaluation: Because evaluating discovered lexical entries is tricky, we can evaluate the performance of downstream tasks that use them, such as identification (Riedl and Biemann 2016), parsing (Villavicencio et al. 2007), and MT (Costa-Juss\u00e0, Daudaravicius, and Banchs 2010). In this case, not only discovery but also integration into the target application is evaluated. Noise in discovery's output often prevents integrated systems from showing significant performance gains. Extrinsic evaluation is further discussed in the following sections.", "filtered_refids": [[], ["b184", "b185", "b159", "b151"], ["b218", "b65", "b143"], ["b160", "b201"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1635, "num_references": 9}
{"corpusid_sectionid": "1509090-s17", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Open Issues in MWE Discovery", "section": "Association measures work well with two-word MWEs, but not straightforwardly for longer chunks, especially for measures relying on contingency tables (Banerjee and Pedersen 2003). Nesting and variability also pose problems for their use. Moreover, results are not systematically corroborated and the success of an association measure seems to be heavily dependent on corpus size, nature, and MWE category (Dunning 1993;Pearce 2002;Evert 2005;Pecina 2010).\n\nSubstitution and semantic similarity methods require large corpora with many occurrences of the target MWEs. In this kind of approach, discovery is often modeled as a task that consists of finding non-compositional combinations among a list of candidate MWEs. Therefore, their evaluation often requires data sets annotated with semantic compositionality, which are not easy to build. Even though such resources do exist, they are rare and often quite small, mainly available for English, with a few exceptions including nominal compounds in German (Schulte im Walde, M\u00fcller, and Roller 2013) and French (Cordeiro et al. 2016).\n\nThe main open issue concerns the trade-off between evaluation and usefulness. On the one hand, evaluation sets are not always available and, when they are, results may be hard to generalize because of their small size. On the other hand, large-scale discovery, although potentially useful, is hard to evaluate. As a result, methods tend to be specialized and will not work so well when ported to other MWE categories and 853 Computational Linguistics Volume 43, Number 4 languages. Finally, there has been little work on placing discovery methods back into the bigger picture-in other words, comparing discovery with manual lexicon construction. Therefore, it remains unclear whether discovery is required or even useful to support and/or replace lexicographic work in production scenarios.\n\nOne promising alternative is the extrinsic evaluation of discovery to help downstream tasks. Although some authors show that discovery can help MT quality (Costa-Juss\u00e0, Daudaravicius, and Banchs 2010), research on identification and MWE-aware parsing strongly relies on handcrafted lexicons (Schneider et al. 2014a;Constant and Nivre 2016) and rarely uses discovery results (Riedl and Biemann 2016). This is often due to a mismatch between discovered MWEs and test sets, and to the difficulty in integrating such noisy lexicons into applications. As a consequence, it is rare to observe convincing performance gains in downstream tasks. We believe that future research should focus on developing extrinsic evaluation measures, test sets, and guidelines for MWE discovery.", "filtered_refids": [["b14", "b144", "b59", "b142", "b64"], ["b54"], [], ["b160", "b51", "b175"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2648, "num_references": 9}
{"corpusid_sectionid": "1509090-s23", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Rule-Based Methods.", "section": "Rules can be as simple as direct matching, but can also use more sophisticated, context-sensitive constraints encoded as finite-state transducers for instance. Historically, rules based on finite-state transducers offered a simple generic framework to deal with variability, discontiguity, and ambiguity (Gross 1989;Breidt, Segond, and Valetto 1996). Identification methods for contiguous MWEs such as open compounds are generally based on dictionaries compiled into finite-state transducers. Standard pattern matching algorithms for finite-state transducers are then applied to the text to identify MWEs. These approaches have two advantages: Dictionaries are compressed using minimization algorithms and matching is extremely efficient in terms of time complexity.\n\nNonetheless, all inflected forms of MWEs are listed and usually dictionaries are automatically generated from lists of MWE canonical forms together with inflection rules. An example of this approach is integrated in the MT system Apertium (Forcada et al. 2011). The variable part of MWE lexical entries, as the verb echar in the Spanish expression echar de menos (lit. throw from less, 'to miss'), is inflected according to a regular verbal inflection paradigm. The inflection process may be based on finite-state transducers as in Silberztein (1997), possibly augmented with a unification mechanism for handling agreement between the MWE components (Savary 2009). These approaches are extremely precise, but costly. The manual assignment of inflection rules may be eased by tools like Leximir for predicting inflection classes (Krstev et al. 2013).\n\nAnother approach comprises two processing stages: morphological analysis of simple words followed by a composition of regular rules to identify MWEs, as in Oflazer, \u00c7etinoglu, and Say (2004) for Turkish. Breidt, Segond, and Valetto (1996) design regular rules that handle morphological variations and restrictions like the French idiom perdre ADV* :la :t\u00eate (lit. lose ADV* :the :head, 'to lose one's mind'), 16 lexical and structural variations (birth date = date of birth). Copestake et al. (2002) design an MWE lexicon for English based on typed feature structures that may rely on analysis of internal words of MWE. Silberztein (1997) also proposes the use of local grammars in the form of equivalence graphs. These approaches are very efficient in dealing with variability and short-distance discontiguity.\n\nConstraints encoded in the lexicon, such as obligatory or forbidden transformations, can be projected on text to disambiguate idiomatic constructions. Hashimoto, Sato, and Utsuro (2006) encode in a lexicon detailed properties of 100 Japanese verb-noun ambiguous idioms such as voice, adnominal modifications, modality, and selectional restrictions. Then, they only classify as idioms those occurrences that match the constraints in a dependency-parsed test set.\n\nMore recent approaches to rule-based identification use dictionaries containing canonical MWE forms with no additional constraints. They consist of two stages: (1) POS tagging and lemmatizing the text and (2) performing dictionary lookup (Carpuat and Diab 2010;Ghoneim and Diab 2013). The lookup relies on a maximum forward matching algorithm that locates the longest matching MWE. This simple method handles morphological variants of the MWEs, but tends to overgenerate them. This overgeneration is due to strong morphological constraints on some elements or agreement. For instance, the French idiom prendre la porte (lit. take the door) meaning get sacked has a strong morphological constraint: the noun porte (door) must be in the singular; if it is in the plural, the sequence has its literal meaning. Therefore, using lemmas to identify variants is a potential source of mistakes.\n\nTo handle discontiguity, it is possible to apply patterns on such preprocessed texts, including wildcards. For instance, Ramisch, Besacier, and Kobzar (2013) identify discontiguous verb-particle constructions in English made of a verb + at most five words + a particle, adapting the discovery method proposed by Baldwin (2005). General tools for deterministic MWE annotation like the mwetoolkit  17 allow fine tuning of matching heuristics, minimum and maximum gap size, surrounding POS patterns, and local variants. For example, it is possible to identify verb-particle constructions in which the particles up or down appear not further than five words after a content verb, constraining intervening words not to be verbs and the next word not to be the word there (to avoid regular verbs followed by up/down there). Some rule-based identification approaches output ambiguous MWE segmentation, postponing disambiguation until more linguistic context is available (Chanod and Tapanainen 1996). The MWE identification process often generates acyclic finite-state automata representing all possible segmentations for a given input sentence. Some finite state preprocessing tools allow ambiguous lexical analyses, like Intex, Macaon,Nooj,SxPipe,and Unitex. 19 This approach can be used as a preprocessing stage of MWEaware parsing (Section 4) and as a source of features for sequence-tagging identification (Section 3.2.3).", "filtered_refids": [["b33", "b82"], [null, "b104", "b73"], ["b53", "b33", "b138"], ["b84"], ["b77", "b42"], ["b11", "b150", null, "b44"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 35, "num_chars": 5203, "num_references": 15}
{"corpusid_sectionid": "1509090-s24", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Sense Disambiguation Methods.", "section": "Methods inspired by word sense disambiguation treat MWE identification as a specialized in-context classification task. Given a candidate combination in context, the classifier must decide whether it is a true MWE or just a regular co-occurrence. Common features for this task include surrounding words, their POS, lemmas, syntactic characteristics, and distributional information. Such methods often do not cover the identification of candidates. They assume that another process pre-identifies potentially idiosyncratic combinations, and instead focus on detecting which of these are true MWEs. We discuss both supervised and unsupervised classifiers. Uchiyama, Baldwin, and Ishizaki (2005) tackle the problem of identifying Japanese verb compounds. Sense labels correspond to the meaning added by the second verb (aspectual, spatial, adverbial) with respect to the first verb. Their support vector machine guesses the possible semantic classes of a given verb combination, using the semantic classes of other co-occurring verbs as features. Then, in a second step, identification proper is done simply by taking the most frequent sense. Hashimoto and Kawahara (2008) propose a supervised disambiguation system able to distinguish literal from idiomatic uses of Japanese idioms. Fothergill and Baldwin (2012) perform an extended evaluation using the same data set and methodology, including new features, a feature ablation study, and cross-idiom tests. Similar approaches based on support vector machines and surface-level features have also been proposed for English light-verb constructions and verb-particle constructions (Tu 2012). Birke and Sarkar (2006) present a nearly unsupervised system capable of distinguishing literal from non-literal verb uses. It uses a clustering strategy that tries to maximize transitive similarity with the seed set of literal or non-literal sentences using standard features. Sporleder and Li (2009) propose a completely unsupervised method based on lexical chains and text cohesion graphs. Their classifier considers an expression as literal if its presence in the sentence does not have a negative impact on cohesion, defined as the similarity between co-occurring words. For instance, play with fire reinforces cohesion in a sentence containing grilling, coals, and cooking but it would reduce lexical cohesion in a chain containing diplomacy, minister, and accuse. Katz and Giesbrecht (2006), detecting idiomatic verb-noun expressions in German, assume that the context of an idiomatic MWE differs from the contexts of its literal uses. Given two distributional vectors representing literal and idiomatic instances, a test instance is classified according to its similarity to the respective vectors. Cook, Fazly, and Stevenson (2007) propose a similar method based on canonical forms learned automatically from large corpora. Once a canonical form is recognized, distributional vectors for canonical and non-canonical forms are learned and then an instance is classified as idiomatic if it is closer to the canonical form vectors. Boukobza and Rappoport (2009) mix the supervised and unsupervised approach of Fazly, Cook, and Stevenson (2009) into a single supervised method to identify English verbal MWEs. In addition to literal and idiomatic uses, they also discuss and model accidental co-occurrence in which the member words of an MWE appear together only by chance, proposing the use of specialized multi-way support vector machines for each target candidate. Besides surface contextual features, they exploit the use of automatically obtained syntactic dependencies as features, which sometimes improves precision.", "filtered_refids": [["b52", "b94", "b67", "b197", "b31", "b195", "b189", "b23", "b84", "b74"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 3666, "num_references": 10}
{"corpusid_sectionid": "1509090-s25", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Sequence Tagging.", "section": "It is possible to build MWE taggers from annotated corpora using supervised structured learning. MWE taggers that model identification as a tagging problem use stochastic models such as conditional random fields, structured perceptron, or structured support vector machines combined with an IOB labeling scheme (see Figure 4) to predict MWE labels, given the local context, token-level features, and sometimes external resources. Blunsom and Baldwin (2006), whose work's main purpose is to acquire new tagged lexical items for two head-driven phrase structure grammars (ERG for English and JACY for Japanese), propose a supertagging approach based on conditional random fields to assign lexical types to the tokens of an input sequence using a pseudolikelihood method that accommodates large tag sets. The proposed approach only enables the identification of contiguous MWEs.\n\nThis work is very close to methods for joint (contiguous) MWE identification and POS tagging based on linear conditional random fields (Constant and Sigogne 2011;Shigeto et al. 2013). Their tagging scheme concatenates lexical segmentation information (B and I tags for the IOB tag set) with the POS tag of the lexical unit to which the current token belongs. Constant and Sigogne (2011)  Some MWE taggers concentrate on the identification task only, using an IOB-like annotation scheme (Vincze, Nagy, and Berend 2011;Constant, Sigogne, and Watrin 2012;Schneider et al. 2014a). Berend (2011) andConstant, Sigogne, andWatrin (2012) handle contiguous MWEs using conditional random fields. Diab and Bhutada (2009) use sequential taggers based on support vector machines to identify idiomatic verb-noun constructions. In addition to traditional features like left and right words, lemmas, and character n-grams, they also use multiword named entity placeholders. Schneider et al. (2014a) introduce a slightly more complex tagging scheme, allowing gaps and one-level nesting as shown in Figure 4. They use a linear model trained with the perceptron algorithm.\n\nExternal MWE resources can have a great impact and can be used in two different ways: (a) they are projected on a corpus in order to build an annotated data set for training (Vincze, Nagy, and Berend 2011); or (b) they are used as a source of features of the statistical model (Constant, Sigogne, and Watrin 2012;Schneider et al. 2014a). For instance, in order to tackle the lexical sparsity of MWEs, some studies showed interest in integrating lexicon-based features in sequence tagging models. Constant, Sigogne, and Watrin (2012) developed a generic approach to compute features from contiguous MWE lexicons. They use this approach to identify French compounds using conditional random fields, showing significant gains compared with settings without lexicon-based features. This method has also been successfully applied and updated for comprehensive MWE identification in English by Schneider et al. (2014a), who performed finegrained feature-engineering, designing specific features for different MWE lexicons.", "filtered_refids": [["b25"], ["b57", "b175", "b184", "b203", null, "b51"], [null, "b203", "b175"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3048, "num_references": 10}
{"corpusid_sectionid": "1509090-s26", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Evaluation of MWE Identification", "section": "The evaluation of MWE identification must take all the challenges mentioned in Section 3.1 into account. Automatic evaluation is possible if we compare the output of the MWE tagger with manually annotated sentences on a given test set using traditional measures such as precision, recall, and F-measure. However, these measures are based on full-MWE comparison and ignore partial matches (Constant, Sigogne, and Watrin 2012). Link-based precision and recall, inspired by the Message Understanding Conference criterion for coreference resolution, can be used to distinguish taggers that can identify only part of an MWE from systems that do not recognize it at all (Schneider et al. 2014a). It is also possible to consider partial matches on MWEs by taking the maximum precision and recall among all possible token alignments between the prediction and the gold standard (Savary et al. 2017). 20 It is worth noting that current partial matching evaluation metrics are not completely satisfactory and should be further investigated. Indeed, these metrics do not take into account the \"importance\" of tokens within the whole expression. For instance, if the tagger identifies into account instead of the whole expression take into account, which is partially correct, it misses the syntactic head of the expression and this will cause a downstream semantic parser to fail. We could therefore assume that partial matching evaluation metrics should strongly penalize the system in that case. In the other extreme case, sometimes missing some tokens or identifying additional tokens might not harm semantic analysis. For instance, in the sentence John had a bath, the inclusion of the determiner a as a lexicalized element of the light-verb construction have_bath is questionable and a tagging error on this element should not be strongly penalized. The other way around, missing had and bath, which are obligatory elements, should be strongly penalized. Evaluation metrics using weighting schemes that assess partial matches by the \"importance\" of the tokens should be developed in the future.\n\nMany advances in the development of better evaluation metrics are a by-product of shared tasks on MWE identification. The DiMSUM shared task for joint identification and supersense tagging of MWEs in English has put identification in focus ). Among the proposed systems, sequence taggers and the use of clustering methods for generalization seem to bring good results. The results of the PARSEME shared task on verbal MWE identification (Savary et al. 2017) confirm that sequence taggers can perform as well as parsing-based approaches, depending on the language and on the proportion of discontiguous MWEs.\n\nExtrinsic evaluation of MWE identification has also been performed. Identifying (discontiguous) MWEs influences the performance of information retrieval (Doucet and Ahonen-Myka 2004) and word sense disambiguation (Finlayson and Kulkarni 2011). Evaluation of identification when performed before parsing (Section 4.2) or before MT (Section 5.2) is detailed in the corresponding sections.", "filtered_refids": [[null, "b175"], ["b172"], ["b58", "b71"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3085, "num_references": 5}
{"corpusid_sectionid": "1509090-s27", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Open Issues in MWE Identification", "section": "In spite of receiving less attention than MWE discovery, MWE identification has made significant progress. On the one hand, experiments have shown that it is possible to identify specific MWE categories, especially ambiguous ones such as verbal idioms, using mostly unsupervised models (Katz and Giesbrecht 2006;Boukobza and Rappoport 2009;Fazly, Cook, and Stevenson 2009). On the other hand, supervised taggers have been successfully used to learn more general MWE identification models capable of handling several MWE categories simultaneously (Vincze, Nagy, and Berend 2011;Constant, Sigogne, and Watrin 2012;Schneider et al. 2014a).\n\nAchieving broad-coverage MWE identification is still an open issue for both unsupervised and supervised methods. Unsupervised methods are usually evaluated on small data sets and it is unclear to what extent the proposed models are generalizable. Supervised methods require sufficient training data and do not perform well on rare MWEs, which have not been seen often enough in the training data. Integrating the approaches presented in this section, for example, using unsupervised features in supervised taggers, could be a promising research direction to address this issue.\n\nMoreover, current identification models cannot always properly model and recognize discontiguous and overlapping expressions. As for discontiguous MWEs, the use of parsers can help (Section 4). As for overlap, some approaches can deal with nesting (Schneider et al. 2014a) but other types of overlap are considered sufficiently rare to be safely ignored. For example, partial overlapping like in pay 1 close 2 attention 1,2 containing the expressions pay attention and close attention is usually ignored. Although it is not straightforward to model overlapping MWEs within taggers and parsers, it would be interesting to develop new identification models that can elegantly handle overlap.\n\nThe success of MWE identification for languages like English and French has relied heavily on high-quality lexicons and annotated corpora, which are rare resources. Broad-coverage hand-crafted MWE lexicons take years to build, and the use of faster, automatic discovery methods directly for identification, bypassing lexicographers, has not been sufficiently studied. Furthermore, annotated corpora containing MWEs are often constructed for other purposes (e.g., treebanks), and MWE annotation is not always consistent (Green et al. 2011). Even when the annotation was performed explicitly for identification purposes, consistency problems always occur because of the complex nature of MWEs (Hollenstein, Schneider, and Webber 2016). Hence, the development of robust annotation guidelines and coherently annotated corpora is a bottleneck that requires attention in the near future.\n\nThe use of end-to-end sequence taggers based on recurrent and/or deep neural networks looks promising (Legrand and Collobert 2016) and remains to be explored. One of the potential advantages of these methods is that they can deal with word vectors (embeddings) that are of a semantic nature. Because MWEs are closely related to semantic compositionality, such models could learn how to tag MWEs when the compositionality of word vectors is breached.", "filtered_refids": [["b94", "b67", "b175", "b203", "b31", null], [], ["b175"], ["b87", "b79"], ["b110"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3240, "num_references": 10}
{"corpusid_sectionid": "1509090-s28", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "MWE Processing and Parsing", "section": "Parsing is a historical field of NLP and continues to receive much attention even after decades of research and development. A parser takes as input a sequence of tokens and generally computes one or more grammatical representations. Generally, these take the form of a tree whose structure depends on the grammatical framework (e.g., constituency or dependency trees). Parsers fall into two main classes: grammarbased parsers and grammarless parsers.\n\nGrammar-based parsers rely on computational grammars, that is, collections of rules that describe the language, expressed in a given grammatical formalism like tree adjoining grammar (TAG) (Joshi, Levy, and Takahashi 1975), combinatory categorial grammar (Steedman 1987), lexical functional grammar (LFG) (Kaplan 1989), and headdriven phrase-structure grammar (HPSG) (Pollard and Sag 1994). Grammars may also be composed of sets of finite-state rules that are incrementally applied (Joshi and Hopeli 1996;Ait-Mokhtar, Chanod, and Roux 2002). Two different strategies are generally used to handle ambiguity. In the first strategy, the process comprises two phases: an analysis phase, generating a set of possible syntactic trees, followed by a disambiguation phase based on heuristics Wehrli 2014) or statistical models (Riezler et al. 2002;Villemonte De La Clergerie 2013). In the second (mainstream) strategy, the grammar is accompanied by a statistical model. For instance, parsers based on generative-models assign probabilities to rules of an underlying grammatical formalism, as in probabilistic context-free grammars (PCFGs) (Charniak and Johnson 2005), tree-substitution grammars (Green et al. 2011), TAG (Resnik 1992), and LFG (Cahill 2004). The parsing algorithms generally rely on dynamic programming. They usually include one pass, but two-pass processes also exist. For instance, Charniak and Johnson (2005) successfully propose applying a discriminative reranker to the n-best parses produced by a generative PCFG-based parser.\n\nGrammarless parsing is performed without any underlying grammar and is based on discriminative approaches. It uses machine learning techniques only, mainly (not exclusively) in the dependency framework. The different parsing algorithms vary from local search approaches, such as transition-based systems (Nivre, Hall, and Nilsson 2004), to global ones, such as graph-based systems (McDonald et al. 2005).\n\nFor both main classes of parsers, significant progress has been made using deep learning techniques (Chen and Manning 2014;Durrett and Klein 2015;Dyer et al. 2015;Pei, Ge, and Chang 2015).\n\nMWE-aware parsing comprises a very tiny portion of this abundant literature. Most MWE-aware parsing strategies are adaptations of standard parsers that experiment with various orchestration schemes for identification. Depending on the scheme, adaptations include modifications to training data, grammatical formalisms, statistical models, and parsing algorithms, as well as specialized modes of interaction with lexicons.", "filtered_refids": [[], ["b90", "b147", "b89", "b209", "b79", null, "b202", "b93", "b38", "b5"], ["b118", "b134"], ["b45", "b61", "b60", "b146"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3012, "num_references": 16}
{"corpusid_sectionid": "1509090-s29", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Motivations and Challenges for MWE-Aware Parsing", "section": "The motivation for MWE-aware parsing is 3-fold: (1) to improve the syntactic parsing performances on sentences containing MWEs (both on internal MWE structure and on the surrounding sentence structure), (2) to improve MWE identification performance, and (3) to improve MWE discovery performance. The latter two items rely on the fact that processing of some MWEs hinges on their syntactic analysis. Parsing faces different challenges with respect to identification because of non-compositionality and ambiguity (Section 1.2). Paradoxically, both challenges may be tackled using parsing. Besides these, MWE-aware parsing addresses the other MWE-related challenges discussed in Section 1.2.\n\nAmbiguity. An MWE might be ambiguous between accidental co-occurrence and literal and idiomatic uses. An incorrect identification can mislead the parser. In particular, complex function words that have a key role in syntax may be ambiguous (e.g., up to). For instance, in John looked up to the sky, the sequence up to should not be identified as a multiword preposition. If so, it would prevent the right analysis: (John) ((looked) (up to the sky)) instead of (John) ((looked) (up) (to the sky)).\n\nConversely, combining MWE identification and parsing can help resolve such ambiguities, yielding both better identification and parsing models. Multiword function words such as complex prepositions, conjunctions, and adverbials (up to, now that, by the way) can be disambiguated by their syntactic context (Nasr et al. 2015). For example, the sequence de la in French can be either a compositional sequence (preposition de + determiner la), or a complex partitive determiner, as shown in the following examples and their corresponding syntactic analyses in Figure 5:\n\n(1) Je parle de la voiture I talk about the car (2) Je mange de la soupe I eat some soup MWE-aware parsing is a natural way to solve this ambiguity. The intransitive verb parle (talk) selects the preposition de (about), whereas mange (eat) requires a noun phrase as its object. Furthermore, one of the main challenges of parsing in general is attachment ambiguity. As MWEs tend to form full syntactic constituents, their identification can guide attachment decisions (Wehrli, Seretan, and Nerima 2010).\n\nNon-compositionality. MWEs can be defined as exceptions to regular composition rules. This non-compositionality can take place at different levels: morphological, distributional, syntactic, semantic, and pragmatic. In particular, some expressions display syntactic irregularity in their internal structure-that is, they are irregular with respect to a grammar. Therefore, if the parser is not aware of the existence of such cases, the analysis will fail. For instance, the adverbial by and large is the coordination of a  Syntactic analysis of the discontiguous verbal MWE take into account using the universal dependencies annotation scheme.\n\npreposition and an adjective that is an irregular pattern for a syntactic constituent in English.\n\nConversely, depending on the type of parser used, some types of syntactic noncompositionality can be captured by a parser as, like by and large, they violate standard grammatical rules. For example, informing a discriminative parser with POS n-gram features may help capture such non-compositionality. Here too, higher levels of noncompositionality are not directly relevant for parsing because they do not interfere with the resulting parse tree, even though information about non-compositional MWEs can guide the parser.\n\nDiscontiguity. Some MWEs can appear in discontiguous configurations, like give up in John gave it up, and take into account in Mary took your argument into account. The identification of such discontiguous MWEs can hardly be handled by purely sequential approaches (Section 3.2.3), except maybe for special cases when gaps are short (Schneider et al. 2014a). Because syntactic links can relate non-adjacent words, parsing may help the identification of discontiguous MWEs (Seretan 2011). In Figure 6, representing the syntactic analysis of a discontiguous instance of the expression take into account, the verb take is a syntactic neighbor of the noun account, which should facilitate the identification of the whole expression.\n\nVariability. Flexible MWEs may undergo syntactic variations. For instance, lightverb constructions accept verbal inflection (make/made/making a decision), passivization (John made a decision \u2192 a decision was made by John), and insertion of free modifiers (John made an important decision). As parsers provide syntactic structure, they can be useful to capture and aggregate MWE variants for discovery (Seretan 2011). In MWE identification, though, parsers need to take variability into account when matching MWE dictionary entries with their instances in text.\n\nThe main challenge for MWE-aware parsing is the orchestration of MWE identification and parsing, that is, the question of when one task should be performed with respect to the other (Section 4.2).", "filtered_refids": [[], [], ["b130"], ["b210"], [], [], [], ["b184", "b175"], ["b184"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 5015, "num_references": 5}
{"corpusid_sectionid": "1509090-s31", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Discovery after", "section": "Parsing. Discovery can be fruitfully informed by syntactic structure and can help tackle challenges like discontiguity and variability (Section 2.1), as MWE components usually belong to the same syntactic constituents. Therefore, having the syntactic structure may help capture MWEs with non-adjacent elements. For instance, Seretan (2011) empirically shows that such information enables the extraction of more relevant MWE candidates as compared to standard extraction methods based on fixedsize windows. Moreover, discovery of flexible constructions usually requires linguistic patterns based on parsing: For instance, verbal expressions combining a head verb and a noun or a prepositional phrase (Fazly and Stevenson 2006;Cook, Fazly, and Stevenson 2007;McCarthy, Venkatapathy, and Joshi 2007;Krenn 2008;Seretan 2011), verb-particle constructions (Bannard 2002;McCarthy, Keller, and Carroll 2003), or constructions formed from a head noun and a prepositional phrase (Weller and Heid 2010). For an interesting illustration, Baldwin (2005) proposes different morphosyntactic and syntactic patterns to extract English verb-particle constructions with valence information from raw text. In particular, he shows the effect of using the outputs of a POS tagger, a chunker, a chunk grammar, or a parser, either individually or combined via a classifier. It experimentally appears that the ensemble method significantly outperforms the individual performances. As for individual scores, the use of shallow syntactic information like chunks tends to be prevalent.\n\nIt is also possible to use pattern-free approaches like Martens and Vandeghinste (2010) and Sangati and van Cranenburgh (2015), who propose discovery methods not dedicated to a specific MWE category but based on recurring tree fragments and association measures.", "filtered_refids": [["b15", "b52", "b11", "b103", "b184", "b116", "b117", "b214", "b68"], ["b170", "b114"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1821, "num_references": 11}
{"corpusid_sectionid": "1509090-s32", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Identification Before", "section": "Parsing. When MWE identification is performed before parsing, the search space of the parsing algorithm is reduced. Hence, the main advantage of this type of orchestration is that the parsing process becomes less complex. The parser takes as input a sequence of partially analyzed linguistic units. This can be seen as a retokenization process, where the pre-identified MWE is merged into a single token (e.g., by the way \u2192 by_the_way). MWE identification prior to parsing has been implemented both in statistical (Cafferkey, Hogan, and van Genabith 2007;Korkontzelos and Manandhar 2010;Constant, Sigogne, and Watrin 2012;de Lhoneux 2015) and rule-based parsers (Brun 1998;Mamede et al. 2012).\n\nThis orchestration type has the advantage of simplicity and empirical efficiency. For instance, Cafferkey, Hogan, and van Genabith (2007) show that pre-identifying multiword named entities and prepropositional MWEs improves parsing accuracy in the constituent framework. The best system of the track on MWE-aware dependency parsing in the SPMRL 2013 shared task (Seddah et al. 2013) was the only one that included deterministic pre-identification (Constant, Candito, and Seddah 2013).\n\nLimitations. The pre-identification approach suffers from limitations. First, in this scenario, most of the proposed methods are limited to contiguous MWEs. Handling discontiguous ones may involve word reordering: John gave it up \u2192 John gave_up it. In addition, when MWE components are concatenated into a single token, their internal syntactic structure is lost, whereas it may be required for the semantic processing of semi-compositional MWEs. However, this can be performed a posteriori, for instance, by applying simple rules based on POS patterns (Candito and Constant 2014).\n\nThen, the retokenization increases data sparsity that negatively affects parsing performance, because the vocabulary size increases whereas the total amount of training data is the same. Eryigit,\u0130lbay, and Can (2011) showed that the concatenation operation of different MWE categories has different impacts on parsing performance for Turkish. Whereas retokenization of multiword named entities and numerical expressions improved dependency parsing performance, retokenization of light-verb constructions harmed it.\n\nAnother disadvantage is that pre-identification is deterministic, so the syntactic parser cannot recover from MWE identification errors. A sentence like He walked by and large tractors passed him cannot be analyzed correctly if by and large is pre-analyzed as a multiword adverb (by_and_large). Errors arise mainly due to challenging aspects like ambiguity (accidental co-occurrence identified as an MWE) and variability (an MWE missed because it is different from the canonical form in the lexicon, see Section 3.1).\n\nNon-Deterministic Approaches. To fully profit from using MWE identification as a preprocessing step, parsing has to be carried out non-deterministically, since several alternatives should be maintained and eventually resolved using a disambiguation model of some kind. Rule-based parsers deal with this problem to some extent by taking as input a lattice of possible POS sequences and MWE segmentations constructed from a lexicon-based preprocessing phase (Villemonte De La Clergerie 2013; Sagot and Boullier 2005). In the statistical paradigm, Constant, Le Roux, and Sigogne (2013) successfully used an MWE tagger based on conditional random fields that generates the most probable outputs in the form of a lattice of lexical units. The lattice is then fed into the parser, which is in charge of selecting the best lexical segmentation, as well as the best syntactic tree. With this rationale, the MWE identification module of Urieli (2013) successfully feeds the initial beam of a transition-based parser with its n-best segmentations, each associated with a score. These proposals suggest that a crucial aspect of MWE identification in MWE-aware parsing systems is whether ambiguous analyses can be handled.", "filtered_refids": [["b112", null, "b34", "b37"], ["b49", "b37"], ["b40"], [null], [], [null, "b199", "b167"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 4009, "num_references": 11}
{"corpusid_sectionid": "1509090-s33", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Identification During Parsing.", "section": "As mentioned earlier (Sections 3.1 and 4.1), MWEs may be discontiguous and their identification may require access to the syntactic environment available during parsing. Some studies have shown the great interest of performing MWE identification during syntactic parsing. We henceforth also use the term \"joint\" to refer to such approaches. Joint Grammar-Based Approaches. In a grammar-based parser, MWE identification is often integrated in the grammar. MWEs are generally found in a lexical resource, and parsers embody mechanisms to link MWE entries to grammar rules, as in Abeill\u00e9 (1995) for TAG, Attia (2006) for LFG, and Copestake et al. (2002) and Villavicencio et al. (2007) for HPSG. For instance, in the TAG paradigm, Abeill\u00e9 and Schabes (1989) link MWE lexical entries to tree rules that are anchored by multiple components present in the lexical entry. This mechanism has been integrated in the XTAG project that aims to construct a lexicalized TAG for English (XTAG 2001). In practice, rule-based parsers can also use MWE identification as a cue to locally select the best syntactic analysis: for instance, Wehrli (2014) applies heuristics favoring MWE analyses.\n\nWhere statistical grammar-based parsers are trained from a reference treebank, MWEs must be annotated within the treebank. Typically, each MWE is annotated with a specific subtree having a flat structure (Arun and Keller 2005;Green et al. 2011;Seddah et al. 2013). In particular, Green et al. (2011) andManning (2013) learned PCFG and probabilistic tree-substitution grammars from such treebanks. Sangati and van Cranenburgh (2015) successfully learned double data-oriented parsing models for which extracted subtrees only count if they happen to form a largest shared fragment from another pair of trees in the treebank. Joint Grammarless Approaches. In the case of grammarless parsers, seminal studies have confirmed the promise of joint statistical models for handling the parsing and identification tasks at the same time Eryigit,\u0130lbay, and Can 2011). In the dependency parsing framework, the model is trained on a treebank where MWEs are annotated in the form of a subtree and specific arcs denote MWE components with either a shallow structure Eryigit,\u0130lbay, and Can 2011;Seddah et al. 2013;Kong et al. 2014;Nasr et al. 2015), or a deeper one (Vincze, Zsibrita, and Nagy 2013;Candito and Constant 2014). For instance, Vincze, Zsibrita, and Nagy (2013) incorporate syntactic functions in the light-verb construction labels in order to retain the internal structure of the expression.\n\nSuch approaches generally use off-the-shelf parsers that perform well for MWEaware parsing as they are informed by many lexicalized features. 21 Such workaround approaches operating on very simple representations of MWEs have some limitations. Syntactic analysis and MWE identification are handled using the same types of mechanisms, whereas their underlying linguistic models are quite different, though interleaved. Some proposals try to handle this problem. For instance, in the dependency framework, Nivre (2014) mentioned the possible integration of a new action, namely, a transition dedicated to MWE identification, within a transition-based parsing system. In this line of research, Constant and Nivre (2016) proposed a transition-based system for joint lexical and syntactic analysis including specific transitions for MWE identification using a dual representation of MWEs (syntax-irregular vs. syntax-regular MWEs).\n\nOn the Use of Lexicons. The MWE identification section (Section 3) has shown that the use of lexicons increases MWE identification performances. Joint rule-based grammar-based MWE-aware parsing generally embodies mechanisms to link the grammar to a lexicon, as illustrated above for the TAG formalism. The use of lexicons is more complicated for grammar-based parsers based on generative statistical models. That is why their integration within non-deterministic processing chains is achieved by including support from MWE taggers that use lexicons (Sections 4.2.2 and 4.2.4).\n\nOne great advantage of joint grammarless MWE-aware parsers based on discriminative models is that they can be informed by external lexicons using additional features. For instance, Candito and Constant (2014) showed that incorporating MWE features based on MWE lexicons greatly improves the accuracy. Nasr et al. (2015) also showed that incorporating specific syntactic subcategorization lexicons helped the disambiguation of ambiguous complex function words. For instance, in French, the sequence bien que is either a multiword conjunction (although) or the literal sequence composed of an adverb (well) followed by a relative conjunction (that). This ambiguity may be resolved using the verb in the syntactic neighborhood. The authors included specific features indicating whether a given verb accepts a given grammatical element: manger (to eat) -QUE -DE, penser (to think) +QUE -DE, boire (to drink) -QUE -DE, parler (to speak) -QUE +DE. The QUE feature indicates whether the verb accepts a clausal complement introduced by que (that), and the DE feature indicates whether the verb accepts a nominal complement introduced by preposition de (of, from).\n\nDiscussion. Joint approaches are of great interest for MWEs having syntactic variability. In particular, Eryigit,\u0130lbay, and Can (2011) and Vincze, Zsibrita, and Nagy (2013) state that a joint approach using a dependency parser is very successful for the identification of light-verb constructions in Turkish and in Hungarian, respectively. Nonetheless, such approaches have the inconvenience of complicating the parsing stage through an increase in the size of the label sets. For instance, the literature shows mixed results for non-compositional open compounds for which a pre-identification approach is sometimes more accurate than a joint one (Constant and Nivre 2016). The right balance therefore has to be found.\n\nAn interesting way to deal with this issue is to combine the before and during strategies using dual decomposition, as shown to be very effective in parsing tasks (Martins et al. 2010;Le Roux, Rozenknop, and Foster 2013). 22 In the case of MWE-aware parsing, results can be improved using dual decomposition combining MWE pre-identification with a joint parser. For instance, Le Roux, Rozenknop, and Constant (2014) combine several sequential MWE taggers based on conditional random fields with several constituent MWE-aware joint parsers. All taggers and parsers are trained independently, and the system iteratively penalizes each parser and tagger until agreement on MWE segmentation is reached. Such an approach reaches state-of-the-art results for French MWE identification and parsing. Nonetheless, one drawback of their approach is that it only handles contiguous MWEs. It is interesting to note that the dual decomposition approach makes it possible to use several shallow MWE annotation schemes. For instance, taggers use IOB annotations, and parsers use constituent subtree annotations. From both schemes, one can compute the predicted MWE spans used for computing the agreement between MWE-aware systems. In light of inspiring results regarding shallow MWE annotations, we conclude that the dual decomposition approach may also benefit from other shallow MWE annotations. This hypothesis deserves future investigation.", "filtered_refids": [["b217", "b9", "b53", "b201", "b2", "b0"], ["b100", "b40", "b130", "b79", "b49", "b206", "b80", null, "b8"], ["b51"], [], ["b130", "b40"], ["b206", null, "b51"], ["b115", null, "b108", "b109"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 49, "num_chars": 7379, "num_references": 25}
{"corpusid_sectionid": "1509090-s34", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Identification after Parsing.", "section": "Although it is well known that predicting syntactic structure might help tackle challenges like discontiguity and variability, especially for verbal expressions, very few studies have experimented with identification after parsing. Fazly, Cook, and Stevenson (2009) used a parsed text in order to identify verb-noun idiomatic combinations. Nagy and Vincze (2014) also successfully use such an approach to identify verb-particle constructions in English. They positioned a classifier on top of a standard parser in order to select verb-particle constructions from a set of candidates extracted from the parsed text. Baptista et al. (2015) identify verbal idioms in Portuguese using the incremental finite-state parser XIP (Ait-Mokhtar, Chanod, and Roux 2002) as part of the STRING NLP pipeline (Mamede et al. 2012). The finite state parser first recognizes chunks then identifies syntactic relations between them by incrementally applying hand-crafted rules. Then, new rules are added in order to capture verbal idioms based on already predicted lexical and syntactic information. Also, most of the systems proposed in the PARSEME Shared Task (Savary et al. 2017) used tagging supervised models relying on syntactic features in order to identify verbal MWEs.\n\nInstead, most identification methods are based on less complex preprocessing stages (tokenization, POS tagging, lemmatization, etc.), as shown in Section 3. One reason could be that discontiguous MWEs tend to have small gaps, as shown in English by Schneider et al. (2014a). Another reason could be that parsers are error-prone and error-propagation might harm MWE identification (unlike the case for discovery where errors are compensated by large quantities of data). To tackle the problem of errors, an interesting approach is to make use of reranking (Charniak and Johnson 2005). For instance, Constant, Sigogne, and Watrin (2012) used an MWE-dedicated reranker on top of a parser generating the n-best parses (including MWE identification) and showed significant improvement in MWE identification accuracy.", "filtered_refids": [["b67", "b18", "b112", "b172", "b129", "b5"], [null, "b175"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2070, "num_references": 8}
{"corpusid_sectionid": "1509090-s35", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Evaluation of MWE-Aware Parsing", "section": "Evaluating a syntactic parser generally consists of comparing the output to reference (gold standard) parses from a manually labeled treebank. In the case of constituency parsing, a constituent is treated as correct if there exists a constituent in the gold standard parse with the same labels and starting and ending points. These parsers are traditionally evaluated through precision, recall, and F-measure metrics (Black et al. 1991;Sekine and Collins 1997).\n\nIn standard dependency parsing with the single-head constraint, 23 the number of dependencies produced by the parser should be equal to the number of total dependencies in the gold-standard parse tree. Common metrics to evaluate these parsers include the percentage of tokens with correct head, called unlabeled attachment score (UAS), and the percentage of tokens with correct head and dependency label, called labeled attachment score (LAS) (Buchholz and Marsi 2006;Nilsson, Riedel, and Yuret 2007).\n\nThe evaluation of identification and discovery has been discussed in previous sections. However, evaluation of MWE-aware parsers and of whether or not MWE identification helps to improve the parsing quality requires some additional care. In most work where MWE identification is realized before parsing, the MWEs are merged into single tokens (Section 4.2.2). As a result, the common metrics for parsing evaluation given above become problematic for measuring the impact of MWE identification on parsing performance (Eryigit,\u0130lbay, and Can 2011). For example, in dependency parsing, the concatenation of MWEs into single units decrements the total number of evaluated dependencies. It is thus possible to obtain different scores without actually changing the quality of the parser, but simply the representation of the results. Instead of UAS and LAS metrics, the attachment scores on the surrounding structures, namely, UAS surr and LAS surr (i.e., the accuracies on the dependency relations excluding the ones between MWE elements) are more appropriate for the extrinsic evaluation of the impact of MWE identification on parsing. Similar considerations apply to constituency parsing.\n\nAlthough UAS surr and LAS surr are valuable metrics for measuring the impact of different MWE categories on parsing, they are troublesome with automatic MWE identification when gold-standard MWE segmentation is not available, because erroneously identified MWEs would degrade parsing scores on the surrounding dependencies.\n\nAn alternative solution is to detach the concatenated MWE components (if any) into a dependency or constituency subtree (Candito and Constant 2014;Eryigit,\u0130lbay, and Can 2011). In this way, the standard evaluation metrics are still applicable in all different orchestration scenarios and work on both contiguous and non-contiguous cases, thus providing a means to assess the performance of joint syntactic parsing and MWE identification as a whole.", "filtered_refids": [["b24", "b182"], ["b132", "b35"], [null], [], [null, "b40"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2926, "num_references": 7}
{"corpusid_sectionid": "1509090-s36", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Open Issues in MWE-Aware Parsing", "section": "A few studies have established that optimal parsing of different MWE categories requires different treatment and different orchestration scenarios (Section 4.2). In order to design a method for finding the optimal orchestration, a more systematic investigation needs to be carried out involving the three MWE identification positions for every MWE category in every language.\n\nTo avoid the data sparsity problem caused by the concatenation strategy used in the MWE pre-identification scenario, another strategy that deserves investigation is constrained parsing (Nivre, Goldberg, and McDonald 2014). In these systems, the constraints are incorporated into the parsing system as a set of preconditions that force the parser to keep the given dependencies/constituents in the output parse.\n\nStudies on statistical MWE-aware parsing tend to work on very simple representations of MWEs. The benefit of adopting more complex, deeper representations capable of representing, for example, embedded MWEs (Finkel and Manning 2009;Constant, Le Roux, and Tomeh 2016;Constant and Nivre 2016), is as yet unclear. There is a case to be made for such approaches to be investigated more deeply on data sets with comprehensive MWE annotations in many different languages.\n\nPrevious sections have shown the different ways identification and parsing can interact. In particular, they show the great interest of using manually validated MWE lexicons. We could also imagine how MWE lexicons extracted from discovery might be directly integrated in the MWE-aware parser. Such methods are very close to those that integrate lexical affinities, acquired from large quantities of external raw data, into a standard statistical parser (Volk 2001;Bansal and Klein 2011;Mirroshandel, Nasr, and Le Roux 2012;Schneider 2012).", "filtered_refids": [[], ["b133"], ["b70", "b51", "b50"], ["b207", "b174", "b122", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1795, "num_references": 8}
{"corpusid_sectionid": "1509090-s37", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "MWE Processing and Machine Translation", "section": "MT systems aim to automatically translate a source text into a target text that retains the meaning and fluency of the source. They must take into account the lexical, morphosyntactic, syntactic, and semantic constraints in source and target languages. The main MT paradigms are summarized here.\n\nStatistical machine translation (SMT) acquires the ability to translate from parallel data using machine learning techniques. Like all such systems, it includes a training phase, which uses the data to build probabilistic models, and a decoding phase, where these models are deployed to actually carry out translation of an unseen source language sentence.\n\nDuring training, two kinds of probabilistic model are built: a translation model, derived from bilingual corpora, and a language model, from monolingual corpora. Both models assign probabilities: the translation model to source/target language fragments, and the language model to target language word sequences (Koehn 2010).\n\nDuring decoding, the system generates many hypothetical translations for each source sentence and chooses the most probable hypothesis. This is calculated for each by combining probabilities assigned by the acquired translation and target language models. The effective performance of this calculation requires considerable ingenuity, given the exponential number of possible translations and orderings of translated sentence fragments, the non-trivial computations involved, and the real time and memory constraints.\n\nVariants of these steps take into account contiguous sequences of words in so-called phrase-based SMT (Koehn, Och, and Marcu 2003), syntactic structures in syntax-based SMT (Chiang 2007;Hoang and Koehn 2010), or linguistic annotation layers in factorbased SMT (Koehn and Hoang 2007)).\n\nPhrase-based SMT and its variants build phrase tables-that is, a list of source fragments (words, phrases, subtrees), their translations, and their translation probabilities that take into account word sequences, not only simple words. In principle, therefore, such systems can naturally handle contiguous MWEs. Whether they can handle them correctly in all cases is, of course, a separate question.\n\nMore recently, neural machine translation (Kalchbrenner and Blunsom 2013;Cho et al. 2014) proposes alternative methods to compute translation probabilities, by using recurrent neural networks to model the translation task. Most neural translation systems use an encoder-decoder architecture. The input sentence is encoded into a fixedlength or variable-length vector and then one or more decoders use this representation to obtain the target sentence. The probability of the translation of one word is computed on the basis of the translation probabilities of previous words. An attention model is frequently used to represent larger contexts for the translated words and sentences. Indeed, attention models represent source word and larger-context words (using a dot product of vectors or multilayer perceptrons) to generate a target word. Few neural machine translation systems take into account fine linguistic descriptions (Sennrich and Haddow 2016). Neural machine translation obtains impressive improvements of the evaluation scores such as BLEU (Wu et al. 2016).\n\nRule-based machine translation (RBMT) uses large lexicons and explicit rules describing the syntactic and semantic constraints on both the source and the target language. Transfer rules are used to map source language structures to target language ones and to identify the right translation. These rules are based on formal grammars or intermediate language-independent structures (such as minimal recursion semantics [Oepen et al. 2004]) capable of generating correct translation equivalents.\n\nFinally, example-based machine translation (EBMT) is based mainly on examples in the form of large translation memories (large collections of source/target sentence pairs) but also uses rules to acquire new linguistic knowledge dynamically. EBMT is based on a translation by analogy approach, where at run time translations are obtained by looking up and using examples stored in translation memories. The translation process is organized in three stages: (i) matching of input sentences with translations previously stored, (ii) retrieval of these translations, and finally (iii) adaptation or recombination of the target sentences. An early review of EBMT appears in Somers (1999).\n\nIn this introductory section we defined the various approaches used in machine translation and their acronyms. In the following sections we will use the acronyms SMT, RBMT, and EBMT instead of the complete terms to improve text readability.", "filtered_refids": [[], [], ["b97"], [], ["b98", "b99", "b46", "b85"], [], ["b216", "b183", "b47", "b92"], ["b137"], ["b188"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 4679, "num_references": 11}
{"corpusid_sectionid": "1509090-s38", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Motivations and Challenges for MWE-Aware Machine Translation", "section": "The main motivation for MWE-aware MT is that none of these paradigms can consistently address the basic features of MWEs, some of which were already mentioned in Section 1.2: ambiguity, discontiguity, non-compositionality, and variability. We briefly review these challenges here.\n\nAmbiguity. Here the components of an MWE can be interpreted and translated literally or idiomatically according to the context, and the two readings are easily confused. For example, the French MWE jeter l'\u00e9ponge (lit. to throw the sponge) means to resign. However, the expression might be used literally (the person is washing the dishes and literally throws the sponge). The challenge concerns the nature of the information required to make the right choice, and how to represent it. In parsing we should note that this is likely to include more contextual, extra-linguistic, or multilingual information than is available in most MT systems.\n\nDiscontiguity. Translation is hampered by alien elements occurring between the components of an MWE. Google Translate renders John picked up the book as John ramassa le livre in French, which is correct, if a little literary. But John picked the book up is translated as John prit le livre jusqu'\u00e0, which is ungrammatical. The challenge, again, concerns the information required for such discontiguous MWEs to be recognized, and how that information is brought to bear. A pertinent issue is whether there are special cases to be exploited, as is the case for phrases having a fixed or semi-fixed frame with slots for one or more fillers, such as give [. . . ] a [. . . ] break or on the one hand, [. . . ] on the other hand.\n\nNon-compositionality. This implies that a compositional translation strategy will generally fail. Besides resolving the ambiguity of whether a given case is compositional there is another challenge that arises because compositionality is not necessarily all or nothing: It has degrees. At one extreme, we have non-compositional expressions like red tape, meaning excessive bureaucracy. At the other we have compound nouns like honeymoon for which a compositional translation may be correct (e.g., luna di miele in Italian). In between the two, we have semi-compositional expressions such as to take a decision. The challenge is whether the degree can be predicted and exploited, because if an MWE is mostly compositional, then a mostly compositional strategy stands a chance of producing an acceptable translation. There are special cases where semicompositional translation strategies work. So ask a question should be translated by a pune o \u00eentrebare (lit. put a question) in Romanian and not by *a cere o \u00eentrebare. The verb to ask could not be translated in Romanian as a cere/to demand but as a pune/to put due to the specific lexical constraints: a cere/to demand could not select the noun \u00eentrebare/question, so the other synonym is selected. The challenge is to select the right translation by taking into account all the constraints.\n\nVariability. MT systems must identify and translate all variants of an MWE. Some variants can be challenging to recognize, particularly when they involve syntactic/semantic constraints. For example, the expression he has cooked his goose means that he has fallen into a trap of his own making. But for this reading, he and his must corefer. If they do not, the reading is compositional. Not only is this condition tricky to verify, but there are variants-for example, involving number and gender: she has cooked her goose-whose identification might require hand-made or automatically learned rules. Once these are in place, variants of MWEs can be identified using methods presented in Section 3.2.\n\nTranslation asymmetries. These occur when an MWE in the source language is not necessarily translated by an MWE in the target language and vice versa. They represent an additional challenge. In general, we can have different correspondences, exemplified by the following English-Italian examples: many-to-many (to kick the bucket \u2192 tirare le cuoia), many-to-one (to kick the bucket \u2192 morire), and one-to-many (svegliare \u2192 to wake up). There are several challenges. One is to decide whether to choose an MWE target when a one-word alternative exists. Another is to determine what syntactic adjustments must be made to the target to retain fluency with asymmetric output. Another challenge is how best to exploit asymmetry for the purpose of discovery, as discussed further in Section 5.2.1.\n\nSome of these challenges might be indirectly handled by specific types of MT systems with various orchestration strategies. Idiomatic expressions or contiguous MWEs might be correctly translated in phrase-based SMT (Cap et al. 2015) or by neural MT (Wu et al. 2016), which take into account larger contexts. However, ambiguous MWEs are often translated in SMT with their literal meaning because of a larger translation probability. Syntax-based SMT might capture frequent terms or light-verb constructions without distinguishing MWEs from simple noun-noun or verb-noun combinations. Neural MT systems handle some specific compounds by specific segmentation strategies, but no discontiguous MWEs.\n\nEBMT systems handle contiguous MWEs through their specific translation strategies. RBMT designs specific hand-made rules to translate MWEs, but ambiguity is still a problem.\n\nMT systems with various degrees of MWE-awareness have striven to address these challenges to improve the quality of translation (Ren et al. 2009;Kordoni and Simova 2012;Ramisch, Besacier, and Kobzar 2013;Barreiro et al. 2014). The results of these strategies, presented in the next section, vary across language pairs or MWE categories. For example, Ren et al. (2009) report small improvements of the BLEU score for domainspecific terminology, Cap et al. (2015) report significant improvements of the BLEU score for German light-verb constructions, and Pal, Naskar, and Bandyopadhyay (2013) found significant BLEU improvements for English and Bengali multiword named entities. This variation can be explained by the inadequacy of measures used to evaluate MT (e.g., BLEU) for checking the quality of MWE translation. Section 5.2 presents these attempts in terms of some specific orchestration strategies linking MWE processing with MT.", "filtered_refids": [[], [], [null], [], [], [], ["b216", "b41"], [], ["b41", "b20", "b101", "b158", "b150"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 47, "num_chars": 6295, "num_references": 8}
{"corpusid_sectionid": "1509090-s40", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Discovery after MT.", "section": "MWE discovery methods based on parallel corpora use alignment to verify whether a source MWE translates word-for-word or rather as a unit on the target side. Alignment is used for both multilingual discovery, that is, finding new translation correspondences between MWEs in two languages, and monolingual discovery, that is, finding new MWEs in one language based on translation asymmetries. In fact, multilingual data could help monolingual discovery for less-resourced languages. Multilingual discovery. Word-level alignment (Och and Ney 2000) indicating that groups of two or more source words are frequently aligned with a single target word can indicate potential MWE candidates. For instance, the French compound appareil photo will be aligned with the English word camera. MWE candidates can therefore be extracted by using many-to-one alignments as in Caseli et al. (2010). However, because automatic word alignments tend to be very noisy, several techniques have been proposed to filter them. Caseli et al. (2010) used predefined POS patterns and frequency thresholds to obtain bilingual MWE lists, as presented in Section 2. Bouamor, Semmar, and Zweigenbaum (2012b) use an association measure to find the translations of each MWE in the target language counterpart without exploiting the alignment.\n\nParsed bilingual data has also been used to filter word-aligned MWE candidates. Thus, Zarrie\u00df and Kuhn (2009) propose a method for detecting verb-object MWEs in both source and target languages that are dependency-parsed, only retaining MWEs whose words are bilingually aligned and monolingually linked by syntactic dependencies. Tsvetkov and Wintner (2014) proposed supervised classifiers to distinguish MWEs from non-MWEs, using linguistically motivated features such as literal translatability derived from simple word alignments in parallel corpora. Here, a check is carried out to assess whether the MWE candidate could be literally translated from a bilingual dictionary. Similarly, Rondon, Caseli, and Ramisch (2015) model translatability as the probability of translating the words of the MWE from Portuguese into English and then back to the same Portuguese word.\n\nMonolingual discovery. For monolingual discovery we consider the possibility of using translation asymmetries identified in parallel corpora to compile lists of potential MWE candidates in one specific language without using precomputed word alignments. For example, Sinha (2009) discover Hindi by compiling a list of Hindi light verbs and then looking at mismatches (indicating the use of a verb in a more idiomatic sense) in meaning in the corresponding English counterpart, given a list of literal translations of Hindi light verbs into English.\n\nThis approach has also been extended to comparable corpora (texts from the same domain, genre, or type that are not in a translation relation). Morin and Daille (2010) collect bilingual terminology from comparable corpora with the help of a bilingual dictionary. This method applies a compositional word-for-word translation for an MWE candidate and searches the most probable translation in the comparable corpus, identified by a term extraction method. If the compositional translation strategy fails, derivation methods are used to find the nearest word in the dictionary and to find the potential translation.\n\nOne advantage of all these methods is that they use relatively well-behaved technologies that exploit translation asymmetries together with non-literal translatability to propose multilingual pairs of MWE candidates as well as monolingual MWEs. Unfortunately, they tend to require large parallel or comparable corpora to find appropriate candidates, and these resources are generally lacking. 5.2.2 Identification before MT. The training process in a standard phrase-based SMT 24 system consists of two steps: word alignment and phrase-table construction. Here, identification takes the form of a preprocessing step (before the training process) that transforms MWEs into an intermediate representation: single units, where component words are concatenated by an underscore character (Carpuat and Diab 2010), 25 a definition from the dictionary (Salton, Ross, and Kelleher 2014), 26 or a paraphrase (Ullman and Nivre 2014). 27 Such preprocessing methods identify MWEs in the monolingual parts of corpora by using external resources (bilingual dictionaries, term databases, or parallel corpora), by applying MWE identification tools (Section 3), or a combination of both (Ghoneim and Diab 2013).\n\nSMT systems. In SMT, MWE replacement takes place before the word alignment step (the \"static\" approach according to Carpuat and Diab [2010]), using rule-based MWE identification and lexicon look-up with monolingual general dictionaries, bilingual dictionaries, and specific bilingual lists containing multiword named entities or terms and their translations.\n\nWhen such resources are not available or incomplete, lists of MWE candidates obtained by MWE discovery methods can be applied directly to transform MWEs into single tokens. This strategy handles not only contiguous MWEs, but also their inflected variants. Some specific cases of discontiguous MWEs such as light-verb constructions might be handled by a specific annotation of the light verb (Cap et al. 2015).\n\nEBMT and RBMT systems. For both EBMT and RBMT, lexical resources are essential for handling contiguous MWEs (Barreiro 2008). EBMT uses translation memories to properly identify these, for which word sequences, sometimes based on parsed data (Kim, Brown, and Carbonell 2010), are listed as possible translations. RBMT also uses compositional rules that are combined with transfer rules to handle syntactic variants and discontiguity for some MWEs (Forcada et al. 2011). In the compositional approach, MWE handling is obtained by means of tagging and syntactic analysis of the different components of an MWE.\n\nSpecific MWE categories. Specific MWE categories such as multiword named entities or multiword terms pose particular challenges to MT systems, because they may require specific translations not directly deducible from the translations of their components. These categories are very productive, that is, new multiword named entities and terms are constantly being created, so it is difficult to have complete and updated lexical resources for use during the translation process. For term identification, bilingual or multilingual term glossaries might be applied to transform terms into an intermediate representation (words concatenated by underscore). But when these resources are missing, for new domains or for under-resourced languages, multiword named entities and multiword terms can be annotated as a single token with the help of specific techniques for named entity recognition (Tan and Pal 2014), or term extraction (Bouamor, Semmar, and Zweigenbaum 2012b) designed for monolingual, parallel, or comparable data (Morin and Daille 2010).\n\nClosed compounds, obtained by concatenating several lexemes with any parts of speech, are typical of Germanic languages and represent another difficult task for MT. This category of expressions can be lexicalized, that is, they belong to the lexicon of a language as a single meaning unit, such as the German word Schwiegereltern (parentsin-law) or non-lexicalized, that is, the individual words keep their meanings when combined, for instance, the German neologism Helikoptereltern (helicopter parents). They are usually translated into several target language words. Their meaning might be more or less compositional. MT systems fail to correctly translate these compounds because of their low frequencies and their variability. Moreover, non-compositional compounds have unpredictable meaning.\n\nSplitting strategies can be applied to cut the compounds into subsequent words to improve translation quality (Fritzinger and Fraser 2010;Stymne, Cancedda, and Ahrenberg 2013). Splitting is done by identifying component words in the corpus or by prefix and suffix identification together with distributional semantics  or by using a morphosyntactic tagger and parser . Oversplitting can also be a problem: Splitting non-compositional compounds may generate erroneous translations. Some methods aim to distinguish between compositional and non-compositional compounds and split only the compositional ones . A postprocessing step is required to merge components back into compounds once a translation is generated using a system trained on split compounds. Some methods replace the compounds by paraphrases (Ullman and Nivre 2014) before translating them.\n\nPreprocessing methods (concatenation or decomposition) tag MWEs in the input data: This strategy is effective for SMT or for RBMT and avoids data sparsity. As a drawback, such methods can only handle contiguous MWEs. Also, without filtering, MWE identification methods can add noise into the MT system by annotating candidates which are not really MWEs. This results in MT performance loss. 5.2.3 Identification During MT. MWE-aware strategies can be best differentiated according to the MT paradigm under discussion.\n\nMWE-aware strategies in SMT. Phrase-based SMT systems build probabilistic models during the training phase, based on simple word alignment and on a phrase table (a list of pairs of n-grams, their n-gram translation, and their translation scores). Then, the core translation process is ultimately determined by the contents of the phrase table-thus, one way to regard MWE identification during MT is in terms of changing the contents of the phrase table adaptively, during the training phrase (Carpuat and Diab 2010). Several observed approaches are: (1) changing the training data dynamically (word alignment or the parallel corpus) to take into account MWEs and then retraining the system; (2) modifying the phrase table directly by including information about MWEs and their translations. In both strategies, the use of MWE identification and discovery tools is essential to improve the quality of the translation.\n\nModifying training data. A frequent strategy completes simple word alignment with many-to-many, many-to-one, or one-to-many alignments to solve translation asymmetries (Melamed 1997;Carpuat and Diab 2010;Okita 2012). Word alignment completion is based on simple word alignment and on MWE identification tools, designed for specific MWE categories (Tan and Pal [2014] for multiword named entities; Bouamor, Semmar, and Zweigenbaum [2012b] and Okita and Way [2011] for terms; Ramisch, Villavicencio, and Boitet [2010] for general MWEs). Alternatively, MWE identification and alignment is performed using bilingual lexical resources, with translation alongside an n-gram language model to help with disambiguation (Bungum et al. 2013). The resulting many-to-many word alignment is used to retrain the system in order to build a new phrase table. As a consequence, the phrase table takes into account MWEs and their translations.\n\nAlternatively, bilingual dictionaries of MWEs are added as additional training data to the parallel corpus (Babych and Hartley 2010;Tan and Pal 2014).\n\nModifying the phrase table. Usually, a bilingual list of MWEs and their equivalents is dynamically extracted from the simple word alignment using specific MWE discovery tools (Bouamor, Semmar, and Zweigenbaum 2012b;Kordoni and Simova 2012;Pal, Naskar, and Bandyopadhyay 2013). Then, the phrase table is completed with the bilingual lists of MWEs and the probabilities are modified accordingly (Lambert and Banchs 2005) or added into a new phrase table with the probability set to 1 (Ren et al. 2009).\n\nAn alternate strategy consists of adding new features in the phrase table, such as the number of MWEs present in the bilingual aligned phrases (Carpuat and Diab 2010) or the property that the parallel phrase contains a bilingual MWE (Ren et al. 2009). In this way, the translation quality is improved for certain specific MWE categories or languages (Costa-Juss\u00e0, Daudaravicius, and Banchs 2010). The modified phrase table contains, indeed, the correct translations of MWEs, thus avoiding an incorrect wordfor-word translation during the decoding phase and helping disambiguation.\n\nMore complex models are proposed in syntax-based SMT (Na et al. 2010) or in hierarchical SMT (Chiang 2007). These approaches use grammars to handle discontiguous components and find their translation directly: parsing improves the translation process (according to BLEU and METEOR scores) by providing trees and transfer rules based on parsed data (Wei and Xu 2011).\n\nMWE-aware strategies in EBMT and RBMT. EBMT (Gangadharaiah, Brown, and Carbonell 2006) or RBMT strategies (Anastasiou 2008;Forcada et al. 2011;Monti et al. 2011) dynamically apply rules to handle MWE translations. Some rules are identified from the syntactic tree alignments (Segura and Prince 2011) and integrated into an EBMT system to handle discontiguous MWEs.\n\nRBMT systems use large lexicons to handle contiguous MWEs and apply the correct translation strategy: a simple word-for-word translation strategy or a compositional rule (Wehrli et al. 2009). Discontiguous MWEs are identified using parsing output or some linguistic patterns. Several RBMT systems identify MWEs and generate translations on the basis of formal representations of natural language texts such as parse trees (Wehrli et al. 2009) or intermediate representation languages like minimal recursion semantics (Oepen et al. 2004), a semantico-syntactic abstraction language (Monti et al. 2011;Barreiro et al. 2013). Transfer rules handle MWE variability and discontiguity (Forcada et al. 2011) and are manually defined or automatically learned from parallel corpora (Haugereid and Bond 2011).\n\nDiscontiguous or variable MWEs represent an important source of translation errors. These methods have the advantage of handling discontiguous or variable MWEs with the help of rules for RBMT or by completing word alignments dynamically in SMT.", "filtered_refids": [["b43", "b30", "b136"], ["b194", "b163"], ["b186"], ["b127"], ["b198", "b42", "b170", "b77"], ["b42"], ["b41"], ["b19", "b96", "b73"], ["b30", "b127", "b193"], [], ["b75", "b192"], [], ["b42"], ["b140", "b153", "b30", "b120", "b139", "b193", "b42", "b36"], ["b10", "b193"], ["b105", "b101", "b30", null, "b158"], ["b158", "b42"], ["b46", "b212", "b128"], ["b6", "b76", "b73", "b124"], ["b211", "b124", "b20", "b137", "b73"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 82, "num_chars": 14005, "num_references": 51}
{"corpusid_sectionid": "1509090-s41", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Evaluation of MWE-Aware MT", "section": "The evaluation of MWEs translation quality remains an open challenge, whatever MT paradigm is adopted (Monti et al. 2012;Ramisch, Besacier, and Kobzar 2013;Barreiro et al. 2014), because of a lack of shared assessment methodologies, benchmarking resources, and annotation guidelines.\n\nWith reference to the assessment methodologies, automatic evaluation metrics such as BLEU (Papineni et al. 2002) do not specifically take MWE translation quality into account. For instance, BLEU is based on shared words between the candidate and the reference translation, and gives only a very general indication about quality. Thus, it cannot be considered as a suitable metric for the kind of more differentiated analysis required to identify specific gaps in the coverage of the system, as is needed for MWEs. There have been a few attempts to adapt automatic evaluation metrics towards a more fine-grained MT error analysis (Babych and Hartley 2010;Stymne, Cancedda, and Ahrenberg 2013;. Extrinsic evaluations in MT have also been performed, mainly for SMT. For instance, Carpuat and Diab (2010) conducted a pilot study for a task-oriented evaluation of MWE translation in SMT, whereas Bouamor, Semmar, and Zweigenbaum (2012a) consider SMT as an extrinsic evaluation of the usefulness of automatically discovered MWEs and explore strategies for integrating them in a SMT system, aiming at a more thorough error analysis of MWE translation.\n\nAnother important drawback in this field is represented by the fact that parallel corpora annotated with MWEs, which are important and necessary gold standard resources for the evaluation of MT translation quality, are very scarce. MWE annotation is indeed a complex and time-consuming task. Annotated resources are usually produced manually and require a large number of experts. In addition, annotating MWEs in parallel corpora requires the correct delimitation of MWEs (contiguous vs. discontiguous expressions) to classify and to disambiguate them and to handle not only the multilingual dimension, but also translation asymmetries between languages (Section 5.1). Moreover, each category of MWEs has its own set of properties.\n\nMWE-annotated benchmarking resources useful for translation quality evaluation are usually available for (1) specific MWE categories, (2) specific language pairs, (3) a specific MWE alignment tool or integration strategy in MT systems, or (4) specific approaches to handling MWEs in MT. Evaluation data consist mainly of small parallel corpora, manually built by carefully selecting sentences containing specific categories of MWE to avoid data sparseness, aligned either with human translations collected from the Web or generated by commercial MT systems (Google Translate, Bing, OpenLogos). Previous work that makes use of such resources includes Ramisch, Besacier, and Kobzar (2013) for verb-particle constructions in English and French; Barreiro et al. (2013) for different categories of MWE in language pairs involving English to French, Italian, and Portuguese; Laporte (2014) for French-Romanian verb-noun idioms and collocations; Weller et al. (2014) for compositional noun compounds, compositional verb compounds, and a set of non-compositional compounds in German-English; Barreiro et al. (2014) for light-verb constructions in English to Italian, French, Portuguese, German, and Spanish; and Schottm\u00fcller and Nivre (2014) for verb-particle constructions in the German-English language pair. In addition, these linguistic resources are annotated only with a limited set of MWE categories such as, for instance, light-verb constructions (Vincze 2012;R\u00e1cz, Nagy, and Vincze 2014). They are of variable granularity, so some annotation schemes consider only MWE categories, whereas others include additional information such as POS and degree of fixedness.\n\nThere are only very few instances of parallel corpora annotated with several categories of MWEs and with different types of correspondences (many-to-one, one-tomany, and many-to-many translations), such as those created by Monti, Sangati, and Arcan (2015), Tutin et al. (2015), and Flickinger et al. (2012). Moreover, the lack of homogeneity represents a real obstacle to the effective reuse of existing annotated data.\n\nConcerning MWE annotation guidelines, only very few papers describe the procedures adopted during resource development. Comprehensive approaches to MWE annotation in parallel corpora, that is, which take into account a large inventory of MWE categories, include Monti, Sangati, and Arcan (2015), who developed a parallel English-Italian corpus, and Tutin et al. (2015), who worked on the French part of a parallel French-English corpus.\n\nIn conclusion, the evaluation of MWE processing in MT is still an open issue, as we will discuss in the next section.", "filtered_refids": [["b150", "b20", "b125"], ["b10", "b192", "b29", null, "b42"], [], ["b204", "b107", "b20", "b150", "b149", "b213"], ["b72", "b196", "b126"], ["b196", "b126"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 4805, "num_references": 19}
{"corpusid_sectionid": "226283737-s1", "title": "An Empirical Survey of Unsupervised Text Representation Methods on Twitter Data", "date": "2020-11-01", "section_title": "Text Representation Methods", "section": "In this section, we briefly introduce the methods used in our survey, sorted from oldest to newest. For word embedding methods like word2vec, GloVe, and fastText, which dot not explicitly support sentence embeddings, we average the word embeddings to get sentence embeddings. For deep models like ELMo, BERT, ALBERT, and XLNet, we take the average of the hidden state of the last layer on the input sequence axis. Note that some other works use the hidden state of the first token ([CLS]), but in our experiments, we use the pretrained model without fine-tuning, in this case, the hidden state of [CLS] is not a good sentence representation. Note that we use all these deep neural models without fine-tuning. This is because finetuning is usually based on specific downstream tasks which bias the information in the hidden states, weakening the general representation. Note that when we refer to n-gram models we mean models that capture all grams up to and including the n-gram (e.g., bigram models will include bigrams and unigrams).\n\n1. bag-of-words (BoW). This is a representation of text that describes the occurrence of words within a document. In our experiments, we use a random sample of 5 million tweets collected from the Internet Archive Twitter dataset 1 (IAT) to create a vocabulary. We also remove stop words from the tweets. We try unigram, bigram, and trigram models. 2. TF-IDF.. Term frequency-inverse document frequency (TF-IDF) reflects how important a word is with respect to documents in a collection or corpus. We use a similar experimental setup as BoW. 3. LDA (Hoffman et al., 2010). Latent Dirichlet allocation (LDA) is a generative statistical model for capturing the topic distribution of documents in a corpus. We train this model on the IAT dataset. We also remove stop-words and train models with 5, 10, 20, and 100 topics. 4. word2vec (Mikolov et al., 2013). word2vec is a distributed representation of words based on a model trained on predicting the current word from surrounding context words (CBOW). We train unigram, bigram, and trigram word2vec models using the IAT dataset. 5. doc2vec (Le and Mikolov, 2014). This model extends word2vec by adding another document vector based on ID. Our model is trained on the IAT dataset. 6. GloVe (Pennington et al., 2014). This model combines global matrix factorization and local context window methods for training distributed representations. We use the 200-dimensional version that was pre-trained on 2 billion tweets. 7. fastText (Joulin et al., 2016). fastText is another word embedding method that extends word2vec by representing each word as an n-gram of characters. We use the 300-dimensional off-the-shelf version which was pre-trained on Wikipedia. 8. Tweet2vec (Dhingra et al., 2016). This model finds vector-space representations of whole tweets by learning complex, non-local dependencies in character sequences. In our experiments, we use the pre-trained best model provided by the authors. 2 1 https://archive.org/search.php? query=collection%3Atwitterstream&sort= -publicdate 2 https://github.com/bdhingra/ tweet2vec/tree/master/tweet2vec/best_ model There is another tweet2vec model that uses a character-level cnn-lstm encoder-decoder (Vosoughi et al., 2016), but for the sake of brevity we only show the results for one of the tweet2vec models. 9. Universal Sentence Encoder (USE) (Cer et al., 2018). USE encodes sentences into high dimensional vectors. The pre-trained encoder comes in two versions, one trained with deep averaging network (DAN) (Iyyer et al., 2015) and one with Transformer. We use the DAN version of USE. 10. ELMo (Peters et al., 2018). This method provides context-dependent word representations based on bidirectional language models. We use the version pre-trained on the One Billion Word Benchmark. 11. BERT (Devlin et al., 2018). BERT is a largescale Transformer-based language representation model (Vaswani et al., 2017). We use two off-theshelf pre-trained versions BERT-base and BERTlarge, which are pre-trained on the BooksCorpus and English Wikipedia respectively. 12. ALBERT (Lan et al., 2019). This is a lite version of BERT, with far fewer parameters. We use two off-the-shelf versions, ALBERT-base and ALBERT-large, which are pre-trained on the BooksCorpus and English Wikipedia respectively. 13. XLNet (Yang et al., 2019). This is an autoregressive Transformer-based language model. Like BERT, XLNet is a large-scale language model with millions of parameters. We use the off-the-shelf versions pre-trained on the BooksCorpus and English Wikipedia. 14. Sentence-BERT (Reimers and Gurevych, 2019). Sentence-BERT modifies BERT by using siamese and triplet network structures to derive semantically meaningful sentence embeddings. We use five off-the-shelf versions provided by the authors, Sentence-BERT-base, Sentence-BERT-large, Sentence-Distilbert, Sentence-RoBERTa-base, and Sentence-RoBERTa-large, all pre-trained on NLI data.", "filtered_refids": [[null], ["b11", "b12", "b17", "b6", "b9", "b13", "b21", "b19", "b7", "b5", "b1", "b8", "b2", "b0", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 48, "num_chars": 4962, "num_references": 16}
{"corpusid_sectionid": "226283737-s3", "title": "An Empirical Survey of Unsupervised Text Representation Methods on Twitter Data", "date": "2020-11-01", "section_title": "Evaluation Metrics", "section": "We use a total of six metrics for evaluating the \"goodness\" of our clusters, described below. Except for the Silhouette score, all other metrics rely on ground-truth labels.\n\nSilhouette score (Rousseeuw, 1987): A good clustering will produce clusters where the elements inside the same cluster are close to each other and the elements in different clusters are far from each other. The Silhouette score takes both these factors into account. The score goes from -1.0 to 1.0, where higher values mean better clustering. Homogeneity, Completeness, and V-measure, (Rosenberg and Hirschberg, 2007): If clusters contain only data points that are members of a single class, in other words, high homogeneity, this usually indicates good clustering. Similarly, if all members of a given class are assigned to the same cluster, in other words, high completeness, this usually indicates good clustering. The Homogeneity and Completeness scores are between 0.0 and 1.0, where higher values correspond to better clustering. The V-measure score is the harmonic mean of Homogeneity and Completeness. Adjusted Rand Index (ARI) (Hubert and Arabie, 1985): The Rand Index can be used to compute the similarity between generated clusters and groundtruth labels. This is done by considering all pairs of samples and seeing whether their label agreement (i.e., belonging to the same ground-truth cluster or not) matches the generated cluster agreement (i.e., belonging to the same generated cluster or not). The raw RI score is then \"adjusted for chance\" into the ARI. score using the following formula: The ARI score can be between -1.0 and 1.0, where random clusterings have an ARI close to 0.0 and 1.0 stands for perfect clustering. Adjusted Mutual Information (AMI) (Vinh et al., 2010): The Mutual Information (MI) score is an information-theoretic metric that measures the amount of \"shared information\" between two clusterings. The Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information (MI) score to account for chance. It accounts for the fact that the MI is generally higher for two cluster- ings with a larger number of clusters, regardless of whether there is actually more information shared. The AMI score can be between 0.0 and 1.0, where random clusterings have an AMI close to 0.0 and 1.0 stands for perfect clustering.", "filtered_refids": [[], ["b18", "b14", "b4"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2336, "num_references": 3}
{"corpusid_sectionid": "251196750-s3", "title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking", "date": "2022-07-29", "section_title": "Datasets", "section": "Many public datasets have been published to advance machine learning approaches for DST. The evolution of these datasets is marked by increasing dialogue complexity, especially with the advent of multidomain corpora. We distinguish two main approaches for collecting dialogue datasets: (i) Wizard-of-Oz (WOZ or H2H for human-to-human) approaches where two humans (asynchronously) play the roles of user and agent according to a task description. This approach allows for natural and varied dialogues, but the subsequent annotation can be a source of errors. (ii) Simulation-based approaches (M2M for machine-to-machine) where two systems play the roles of user and agent and interact with each other to generate conversation templates that are then paraphrased by humans. The advantage of this method is that the annotations are obtained automatically. However, the complexity of the task and linguistic diversity are often limited because the dialogue is simulated. Table 1 lists the main datasets as well as recent datasets relevant to the problems discussed in Sec-  between paid participants and various telephone dialogue systems (H2M collection). The user needs to find a restaurant by specifying constraints such as the type of cuisine and can request specific information such as the phone number.\n\nMultiWOZ 9 (Budzianowski et al., 2018) The first large-scale multidomain corpus and currently the main benchmark for DST. It contains dialogues between a tourist and a travel clerk that can span several domains. A major problem related to the way the data was collected is the inconsistency and errors of annotation, which was crowdsourced. Four more versions were later released to try and fix these errors. (Eric et al., 2019;Han et al., 2021;Ye et al., 2021a). Multilingual versions 10 were obtained by a process of machine translation followed by manual correction (Gunasekara et al., 2020;Zuo et al., 2021).\n\nSGD 11 (Rastogi et al., 2020b) The Schema-Guided Dataset was created to elicit research on domain independence through the use of schemas. Schemas describe domains, slots, and intents in natural language and can be used to handle unseen domains. The test set includes unseen schemas to encourage model generalization. SGD-X is an extension designed to study model robustness to different schema wordings (Lee et al., 2021b).", "filtered_refids": [[], [null, "b58", "b46"], ["b30", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2345, "num_references": 5}
{"corpusid_sectionid": "251196750-s5", "title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking", "date": "2022-07-29", "section_title": "Modern Approaches", "section": "One feature that categorizes DST models is the way they predict slot values. The prediction can be made either from a predefined set of values (fixed ontology) or from an open set of values (open vocabulary). What follows is a description of these approaches. A selection of recent models is presented in Table 2 based on this taxonomy.\n\nFixed ontology Following the discriminative approaches that preceded them, traditional neural approaches are based on a fixed ontology and treat DST as a multiclass classification problem (Henderson et al., 2014b;Mrk\u0161i\u0107 et al., 2017). Predictions for a given slot are estimated by a probability distribution over a predefined set of values, restricting the prediction field to a closed vocabulary and thus simplifying the task considerably. The performance of this approach is therefore relatively high (Chen et al., 2020), however, its cost is proportional to the size of the vocabulary as all potential values have to be evaluated. In practice, the number of values can be large and a predefined ontology is rarely available.\n\nOpen vocabulary To overcome these limitations, approaches to predict on an open set of values have been proposed. The first method consists in extracting values directly from the dialogue history, e.g. by formulating DST as a reading comprehension task (Gao et al., 2019). This method depends solely on the dialogue context to extract value spans, however, slot values can be implicit or have different wordings (e.g. the value \"expensive\" may be expressed as \"high-end\"). An alternative is to generate slot values using an encoder-decoder architecture. For instance, TRADE uses a copy mechanism to generate a value for each slot based on a representation of the dialogue history (Wu et al., 2019). A common current approach is to decode the dialogue state using a pretrained autoregressive language model (Hosseini-Asl et al., 2020).\n\nHybrid methods A trade-off seems to exist between the level of value independence in a model and DST performance. Some works have sought to combine fixed ontology approaches with open vocabulary prediction to benefit from the advantages of both methods. This approach is based on the distinction between categorical slots for which a set of values is predefined, and non-categorical slots with an open set of values (Goel et al., 2019;Heck et al., 2020).  ", "filtered_refids": [[], [null, "b23"], [null, "b44"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2359, "num_references": 5}
{"corpusid_sectionid": "251196750-s8", "title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking", "date": "2022-07-29", "section_title": "Adapting PLMs to Dialogues", "section": "Though now commonly used for DST, existing PLMs are pretrained on free-form text using language modeling objectives. Their ability to model dialogue context and multi-turn dynamics is therefore limited. It has been shown that adapting a PLM to the target domain or task by continuing self-supervised learning can lead to performance gains (Gururangan et al., 2020). This method has been applied to TOD systems and DST.\n\nThere are two underlying questions with this approach: the selection of adaptation data and the formulation of self-supervised training objectives to learn better dialogue representations for the downstream task.  gather nine TOD corpora and continue BERT's pretraining with masked language modeling and next response selection. The obtained model TOD-BERT provides an improvement over a standard BERT model on several TOD tasks including DST. With a similar setup, Zhu et al. (2021) contrast these results and find that such adaptation is most beneficial when little annotated data is available. Based on TOD-BERT, Hung et al. (2022) show that it is advantageous not only to adapt a PLM to dialogues but also to the target domain. To do so, they use conversational data from Reddit filtered to contain terms specific to the target domain. Finally,  introduce two objective functions designed to inject inductive biases into a PLM in order to jointly represent dynamic dialogue utterances and ontology structure. They evaluate their method on conversational semantic parsing tasks including DST.", "filtered_refids": [[null], ["b4", "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1515, "num_references": 3}
{"corpusid_sectionid": "251196750-s9", "title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking", "date": "2022-07-29", "section_title": "Mitigating Annotated Data Scarcity", "section": "The lack of annotated data hinders the development of efficient and robust DST models. However, the data collection process is costly and timeconsuming. One approach to address this problem is to train a model on resource-rich domains and apply it to an unseen domain with little or no annotated data (cross-domain transfer; Wu et al., 2019). Dingliwal et al. (2021) adopt meta-learning and use the source domains to meta-learn the model's parameters and initialize fine-tuning for the target domain. Works around schema-based datasets (Rastogi et al., 2020a) use slot descriptions to handle unseen domains and slots (Lin et al., 2021c;Zhao et al., 2022). A drawback of these approaches is that they rely on the similarity between the unseen domain and the initial fine-tuning domains.\n\nAnother set of approaches tries to exploit external knowledge from other tasks with more abundant resources. Hude\u010dek et al. (2021) Lin et al. (2021b) propose different methods to pretrain a model on reading comprehension data before applying it to DST. Similarly, (Shin et al., 2022) reformulate DST as a dialogue summarization task based on templates and leverage external annotated data.\n\nNote that the PLM adaptation approaches seen above allow for more efficient learning when little data is available and are also a potential solution to the data scarcity problem. Along this line, Mi et al. (2021) present a self-learning method complementary to TOD-BERT for few-shot DST.", "filtered_refids": [["b14", null, "b44"], ["b13", "b31"], ["b21"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1465, "num_references": 6}
{"corpusid_sectionid": "251196750-s10", "title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking", "date": "2022-07-29", "section_title": "Prompting Generative Models to Address Unseen Domains", "section": "A recent paradigm tackles all text-based language tasks with a single model by converting them into a text-to-text format (Raffel et al., 2020). This approach relies on textual instructions called prompts that are prepended to the input to condition the model to perform a task. It has been successfully applied to DST, not only closing the performance gap between classification and generation methods but also opening opportunities to address unseen domains using schemas (cfr. Section 2). In addition to slot and domain names, Lee et al. (2021a) in-clude slot descriptions in a prompt to independently generate values for each slot. They also add possible values for categorical slots. Zhao et al. ( , 2022 expand on this approach by generating the entire dialogue state sequentially, as illustrated in Figure 2. In an analysis of prompt formulation, Cao and Zhang (2021) find that a question format yields better results for prompt-based DST.\n\nHand-crafting textual prompts may lead to suboptimal conditioning of a PLM. Instead of using discrete tokens, we can optimize these instructions as continuous embeddings, a method called prompt tuning (Lester et al., 2021), which has been applied to DST for continual learning, adding new domains through time (Zhu et al., 2022). Alternatively, Yang et al. (2022) reverse description-based prompts and formulate a prompt based on values extracted from the utterance to generate their respective slot. They argue that slots that appear in a small corpus do not represent all potential requirements whereas values are often explicitly mentioned. Rather than using descriptions that indirectly convey the semantics of a schema, others have sought to prompt a model with instructions, i.e. in-context learning, in which the prompt consists of a few example input-output pairs (Hu et al., 2022;Gupta et al., 2022). ", "filtered_refids": [["b6", null, "b28"], ["b57", "b32", "b45", null, "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1858, "num_references": 8}
{"corpusid_sectionid": "251196750-s11", "title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking", "date": "2022-07-29", "section_title": "Going Beyond English", "section": "Until recently, most works around DST were confined to English due to the lack of data in other languages, preventing the creation of truly multilingual models. In recent years, several works have addressed this issue. A DSTC9 track studied crosslingual DST. For this purpose, a Chinese version of MultiWOZ and an English version of CrossWOZ were obtained by a process of machine translation followed by manual correction (Gunasekara et al., 2020). Zuo et al. (2021) used the same method to translate MultiWOZ in seven languages.\n\nA problem with machine translations is that they lack naturalness and are not localized. Two Chinese datasets were obtained by H2H collection: Cross-WOZ and RiSAWOZ Quan et al., 2020), however, this type of collection is expensive. The M2M approach makes it possible to obtain adequate multilingual corpora by adapting conversation templates according to the target language. Lin et al. (2021d) ", "filtered_refids": [["b58"], ["b16", "b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 926, "num_references": 3}
{"corpusid_sectionid": "251196750-s13", "title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking", "date": "2022-07-29", "section_title": "Generalizability", "section": "TOD systems are intended to be deployed in dynamic environments that may involve different settings. In practice, the application domains of these systems are numerous and varied (e.g. customer service in telecommunications, banking, technical support, etc.), which makes manual annotation of corpora for each domain difficult or impossible. This reduces the effectiveness of traditional supervised learning and is one of the reasons why most systems in production are rule-based.\n\nLearning with little or no new annotated data offers an alternative to take advantage of the capabilities of neural networks to guarantee the flexibility of the systems. The importance of this aspect is reflected by the numerous recent works that address this problem, as presented in Section 3. Although significant progress has been made, these approaches remain limited. Models that rely on existing DST resources are unable to handle domains whose distribution deviates from that of the train-ing data (Dingliwal et al., 2021). Models that use external knowledge offer a more generic approach but achieve relatively poor performance (Lin et al., 2021b). Learning generalizable models thus remains an open problem. An interesting avenue is continual learning, which allows new skills to be added to a system over time after deployment. Without retraining with all the data, the model must be able to accumulate knowledge (Madotto et al., 2021;Zhu et al., 2022).\n\nIn a real scenario, entities that are not observed during training are bound to appear. A DST model must be able to extrapolate from similar entities that have been seen. However, MultiWOZ has been shown to exhibit an entity bias, i.e. the slot values distribution is unbalanced. When a generative DST model was evaluated on a new test set with unseen entities, its performance dropped sharply, hinting at severe memorization . Similarly, by its constrained nature, a TOD system may be perturbed by out-of-domain utterances. It is desirable to be able to recognize such utterances in order to provide an appropriate response. The ability of a model to generalize to new scenarios is therefore related to its robustness.", "filtered_refids": [[], [null, "b57", "b13", "b18"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2167, "num_references": 4}
{"corpusid_sectionid": "251196750-s14", "title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking", "date": "2022-07-29", "section_title": "Robustness", "section": "Since users may express similar requirements in different ways, a DST model must be able to interpret different formulations of a request in a consistent manner. In other words, it must be robust to variations in the input. Analytical work has shown that models' performance drops when they are confronted with realistic examples that deviate from the test set distribution Li et al., 2021a). Recent work has tried to address this issue through regularization techniques (Heck et al., 2022). Related to this, an understudied aspect is dealing with utterances that deviate from the norm in the case of a written dialogue system. A DST model must be able to take into account all the history and adjust its predictions of the dialogue state using all available information. Many works have found that performance degrades rapidly as the dialogue length increases. Another critical aspect is therefore efficiently processing long dialogues . The dialogue state condenses important information, but correcting an error made in an earlier turn may be difficult. To overcome this error propagation issue, Tian et al. (2021a) use a two-pass dialogue state generation to correct potential errors, while Manotumruksa et al. (2021) propose a turn-based objective function to penalize the model for incorrect prediction in early turns. Despite this need, Jakobovits et al. (2022) have shown that popular DST datasets are not conversational: most utterances can be parsed in isolation. Some works simulate longer and more realistic dialogues by inserting chit-chat into TODs (Kottur et al., 2021b;Sun et al., 2021).\n\nAnother point that has not been studied much in recent approaches is robustness to speech inputs (Faruqui and Hakkani-T\u00fcr, 2021). For spoken dialogue systems, new challenges arise such as ASR errors or verbal disfluencies. Early editions of DSTC provided spoken corpora that included a transcript and ASR hypotheses. Since then, DST datasets have been primarily text-based. A DSTC10 track considered this aspect again and proposed a DST task with validation and test sets containing ASR hypotheses (Kim et al., 2021).\n\nLearning robust models requires diverse datasets that represent real-world challenges. In this sense, several evaluation benchmarks have been published to study TOD systems' robustness (Lee et al., 2021b;Peng et al., 2021b;. Data augmentation is a potential solution to the lack of variety in datasets (Campagna et al., 2020;Li et al., 2021a;Aksu et al., 2022), especially for simulating ASR errors (Wang et al., 2020;Tian et al., 2021b).", "filtered_refids": [["b34", "b20", null, "b33", "b36"], [null], ["b25", null, "b7", "b51", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2562, "num_references": 11}
{"corpusid_sectionid": "251196750-s15", "title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking", "date": "2022-07-29", "section_title": "Relevance", "section": "As we have seen, existing datasets do not really reflect real-world conditions resulting in a rather artificial task. It is important to keep a holistic view of the development of DST models to ensure the relevance of their application in a dialogue system.\n\nSince the first TOD systems, the dialogue state has been considered as a form to be filled in as slotvalue pairs. This fixed representation is suitable for simple tasks like flight booking but is limited in domains with rich relational structures and a variable number of entities. Indeed, composition is not possible (e.g. \"itinerary for my next meeting\") and knowledge is not directly shared between slots. Section 3 presented approaches that attempt to address the latter point by using graphs for dialogue state representation. To promote work on more realistic scenarios, some have proposed richer representations with an associated corpus. Andreas et al. (2020) encode the dialogue state as a dataflow graph and introduce the SMCallFlow corpus. Cheng et al. (2020) propose a tree structure along with the TreeDST corpus. ThingTalk is another alternative representation of the dialogue state which was successfully applied to MultiWOZ (Lam et al., 2022;Campagna et al., 2022). In the ABCD corpus, Chen et al. (2021) adopt a representation of the procedures that a customer service employee must follow in accordance with company policies.\n\nThese approaches still rely on specific database schemas and are limited to one modality. For more capable virtual agents, we can extend the scope of dialogues to a multimodal world. Emerging efforts on multimodal DST seek to track the information of visual objects based on a multimodal context. With SIMMC 2.0, Kottur et al. (2021a) introduced a multimodal TOD dataset based on virtual reality rendered shopping scenes. Similarly, Le et al. (2022) proposed a synthetic dataset of dialogues consisting of multiple question-answer pairs about a grounding video input. The questions are based on 3D objects. In both these datasets, the system has to track visual objects in the dialogue state in the form of slot-value pairs. For a broader background on multimodal conversational AI, we refer the reader to Sundar and Heck (2022)'s survey.\n\nDialogue is dynamic: in a real scenario, an erroneous prediction of the dialogue state would have deviated the course of the conversation from the reference dialogue. However, most studies evaluate models in isolation, assuming that it is always possible to assemble a set of well-performing components to build a good TOD system. The overall performance of a system is rarely taken into account as evaluating the system as a whole is complicated and requires human evaluation. Moreover, it can be difficult to identify which component of the system is problematic and needs to be improved. Despite these hurdles, it is important to consider the impact that DST can have on the dialogue system as a whole. Takanobu et al. (2020) conducted automatic and human evaluations of dialogue systems with a wide variety of configurations and settings on MultiWOZ. They found a drop in task success rate using DST rather than NLU followed by a rule-based dialogue state update. They explain this result by the fact that NLU extracts the user's intentions in addition to the slot-value pairs. Another work showed how uncertainty estimates in belief tracking can lead to a more robust downstream policy (van Niekerk et al., 2021). These studies are rare in their kind and call for more similar work.", "filtered_refids": [[], ["b1", null, "b3"], ["b4"], [null, "b35"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 29, "num_chars": 3531, "num_references": 6}
{"corpusid_sectionid": "53593090-s3", "title": "A Review of Standard Text Classification Practices for Multi-label Toxicity Identification of Online Content", "date": 2018, "section_title": "Neural Network Classification Models", "section": "While word embeddings are a semantic representation of words, bidirectional neural networks are the technology known for generating a semantic representation for a given sequence of words. Bidirectional recurrent neural networks learn the meaning of a sentence not only from the individual words but by processing the dependencies of the surrounding words through forward and backward connections. Both bi-LSTM (Chen et al., 2016) and bi-GRU (Chung et al., 2015) architectures are shown to perform well in sentence representation. LSTM and GRU layers have a proficient learning ability for long text, because they can control how much information should be received in the current step, how much should be forgotten, and how much information should be passed back. Attention layers (Parikh et al., 2016;Felbo et al., 2017) are mechanisms suitable for converting sequence representations, which are usually in the form of matrices, to a vector representation that is tailored for the desired classification tasks. We investigated the impact of leveraging these technologies by training and testing of two neural network structures shown in Figures 2a and b. Pre-trained fasttext embeddings are used and stop words are not removed, since we want the LSTM and attention layer learn the complete sequences. The neural network shown in Figure 2a which contains two layers of biLSTM to encode the information of sequences achieves 0.9842 and the one shown in Figure 2b which uses attention mechanism to combine the context information from embedding layer and the sequence information from each biL-STM layer to get a summary vector of the sentence, reaches 0.9844 in AUC.", "filtered_refids": [[null, "b7", "b5", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1666, "num_references": 4}
{"corpusid_sectionid": "53593090-s4", "title": "A Review of Standard Text Classification Practices for Multi-label Toxicity Identification of Online Content", "date": 2018, "section_title": "Stacking of Classifiers", "section": "Stacking of classifiers is a standard way of increasing the accuracy of a classification task by combining the predictions of multiple classifiers to- gether (Merz, 1999). In this method, a supervisor model is trained and learns how to combine the predictions of different types of models that differ in their variance, bias and capability of dealing with noise (Sluban and Lavra\u010d, 2015). Figure 3 describes the stacking method applied in this work. We used a Light Gradient Boosting Machine (LGBM) stacking model which is a gradient boosting library implemented by Microsoft (Ke et al., 2017).\n\nLGBM is an implementation of fast gradient boosting on decision trees. Given a set of features, this classifier learns a linear combination of the predictions of preliminary classifiers to predict the label. The output of softmax layer from both classifiers (probabilities predicted for 6 classes) is fed to the LGBM. Also, the length of the text, frequency of exclamation marks and frequency of capital letters are considered as LGBM features. The LGBM classifier reached a 0.9847 score. In this section, we investigate the impact of pseudo-labeling as a semi-supervised training method (Lee, 2013). Simply put, we split the test dataset into 10 folds. We then trained the two classifiers described in Section 4, in a supervised fashion, with both training set and 9 folds of test set. For test set, pseudo-labels are used which are the predictions calculated by the best classifier (the LGBM model) as if they were true labels. The trained classifier is tested on the 10th fold and the experiment is repeated for all 10 folds. This method has shown to be equivalent to entropy regularization (Grandvalet and Bengio, 2005) and makes up for dissimilarities of distributions between test and train dataset. Semi-supervised training of classifier-1 and classifier-2 improves the AUC score to 0.9860 and 0.9862 respectively.", "filtered_refids": [["b12", "b10", "b17"], ["b11", "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 1917, "num_references": 5}
{"corpusid_sectionid": "265068198-s3", "title": "ChatGPT for translators: a survey", "date": 2023, "section_title": "As a translation engine", "section": "Even though ChatGPT was not trained explicitly to translate texts, it proved capable of translating between languages.Initial experiments used simple prompts like Translate the following sentences to [TARGET LANGUAGE] and showed that commercial translation engines like Google Translate and DeepL perform significantly better than ChatGPT (Jiao et al., 2023).More recent work, focused on prompt engineering to improve the quality of the translation (Gao et al., 2023).ChatGPT was also used as a translation tool that can help avoid gender bias, with better results than Neural Machine Translation (Castilho et al., 2023).A general observation is that the quality of translation is very different from one language pair to another.For example, Gao et al. (2023) report better results when the target language is English, whilst Castilho et al. (2023) observe poor results when Irish, a low resourced language, is involved.Small scale experiments conducted by the author of this paper seem to suggest that ChatGPT can translate noisy social media texts better than existing translation engines.", "filtered_refids": [["b1", "b0", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 1092, "num_references": 3}
{"corpusid_sectionid": "252624550-s1", "title": "IMTVault: Extracting and Enriching Low-resource Language Interlinear Glossed Text from Grammatical Descriptions and Typological Survey Articles", "date": 2022, "section_title": "Low Resource Languages and Diversity Linguistics", "section": "While the NLP community has not produced structured datasets for these low/no resource languages, structured data does indeed exist within the field of Diversity Linguistics. Diversity Linguistics is the field which concerns itself with the variety of languages spoken in the world. This concerns in-depth treatment of a particular language (grammatical description) as well as large-scale comparison of a given phenomenon (e. g. position of the verb before or after the object) in hundreds or thousands of languages. This comparative work can be found in articles in journals or edited volumes, in monographs, or also in databases.\n\n1 https://glottolog.org/glottolog/ glottologinformation 2 https://en.unesco.org/idil2022-2032\n\nWe can name AUTOYP 3 or the CLLD datasets (WALS,4 APiCS 5 ), of which there are 19 as of 2022. The academic inquiry is complemented by language archives where audiovisual data are stored, some of them transcribed, translated and glossed, in varying percentages. We can name ELAR, 6 AILLA, 7 TLA, 8 Paradisec. 9 See (Nordhoff, 2020a) for a breakdown of their accessible holdings. These different data sources have been tapped into over time: academic books and articles ( (Lewis and Xia, 2010;Xia et al., 2014)), typological databases ( (Chiarcos and Ionov, 2019;Ionov, 2021)), and language archives ( (Nordhoff, 2020a;Nordhoff, 2020b;von Prince and Nordhoff, 2020)), producing structured data which allows for programmatic and quantitative approaches.", "filtered_refids": [[], [], ["b11", "b21", null, "b19", "b16", "b8", "b0", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1480, "num_references": 8}
{"corpusid_sectionid": "252624550-s3", "title": "IMTVault: Extracting and Enriching Low-resource Language Interlinear Glossed Text from Grammatical Descriptions and Typological Survey Articles", "date": 2022, "section_title": "Data Modelling", "section": "The interlinear sentence has received quite some theoretical treatment. The first technical approach was the implementation in the program Shoebox, which would later become Toolbox. 10 The representation used therein was actually never intended to be used in a productive environment, but turned out to become the mainstay for language documenters for more than two decades. Shoebox/Toolbox was developed by SIL, who discontinued development in favour of FLEx, an XML based tool. 11 In parallel, ELAN 12 ( (Wittenburg et al., 2006)) is another XML-based tool for the representation of correpondences and part-whole relations 10 https://software.sil.org/toolbox 11 https://software.sil.org/fieldworks 12 https://archive.mpi.nl/tla/elan in glossed texts ( (Nordhoff, 2020a)). While XML suggest a good perspectives for programmatic extraction of data, (Nordhoff, 2020a) reports that while syntactically valid XML, the ELAN files retrieved from language archives are semantically wildly heterogeneous, making a principled approach very difficult (also compare (Cimiano et al., 2020, 4)). On a more theoretical level, (Drude, 2002) proposed a very elaborate model with a multiplicity of tiers. The XML Interlinear Glossed Text (XIGT, (Goodman et al., 2015)) format has a recursive structure instead, allowing for an arbitrary number of tiers ( (Xia et al., 2014)). (Chiarcos and Ionov, 2019) and (Ionov, 2021) developed a Linked Data version of XIGT, called LIGT, also used in (Nordhoff, 2020a;Nordhoff, 2020b). For the purposes of this paper, a very simple data model distinguishing the tiers of \"utterance\" and \"word\", with respective translations, is sufficient; the level of \"morpheme\" is disregarded. Basic storage is done in JSON, while transformations into JSON-LD, RDF, and CLDF are also made available. An additional morpheme tier could also have been made available, but it was determined that data consumers could easily create such more granular structures easily themselves should the need arise and that it was not necessary to provide an artificially inflated dataset.", "filtered_refids": [["b20", "b21", null, "b0", "b16", "b8", "b2", "b5", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2078, "num_references": 9}
{"corpusid_sectionid": "252624550-s4", "title": "IMTVault: Extracting and Enriching Low-resource Language Interlinear Glossed Text from Grammatical Descriptions and Typological Survey Articles", "date": 2022, "section_title": "Data Sources", "section": "Extraction of interlinear examples from documents has a comparatively long history. The ODIN project ( (Lewis and Xia, 2010;Xia et al., 2014)) 13 crawled the web for pdfs and tried to extract the examples. Copyright problems and the generally poor extraction facilities, however, posed great challenges for this endeavour. While ODIN is still up and running, it uses meanwhile outdated technology (eg HTML framesets), has encoding issues and does not provide dereferenceable URIs for the examples (Figure 2). Another source for interlinearized texts are crosslinguistic databases. The Atlas of Pidgin and Creole Figure 2: A screenshot of the ODIN website, showing an example of the Aari language. Note the URL, which does not give the ID, and the encoding problems. The example given has the \"Verified\" rating \"highest\". There is also \"high\", \"auto\" and \"low\", with presumably worse quality.\n\nLanguage Structures (APiCS 14 , (Michaelis et al., 2013)) offers its example sentences for download in the CLDF format ( (Forkel et al., 2018)). These examples were parsed by (Chiarcos and Ionov, 2019), who used them to develop the LIGT format. The APiCS data have the advantage of being available under a free license. (von Prince and Nordhoff, 2020) and (Nordhoff, 2020a;Nordhoff, 2020b) downloaded data from a variety of language archives, which store ELAN files. ELAN is an XML-format with explicit correspondences between morphemes, words, and sentences. These ELAN files were than converted to the RDF LIGT format, drawing on previous work by (Nordhoff et al., 2016). Published books, most databases and most of the language archives share the problem of unclear copyright status, which hinders dissemination and reuse. Enter Language Science Press.", "filtered_refids": [["b11", "b21"], ["b3", "b12", "b14", "b16", "b0", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 1748, "num_references": 8}
{"corpusid_sectionid": "252200083-s1", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Theoretical Research in Irony 2.1 Irony Theories", "section": "Various definitions have been given to irony. Early studies suggested that irony is the expression whose real meaning is contradictory to its literal meaning (Grice, 1975). The Merriam-Webster Dictionary, The Oxford English Dictionary, and The Collins English Dictionary all adopted this definition and used the words \"opposite\" or \"contrary\" to explain the relationship between the literal and contextual meanings of irony.\n\nHowever, more research into various types of ironic examples revealed that the contextual meaning of irony does not have to be \"opposite\" or \"contrary\" to the literal one. According to Sperber and Wilson (1986); Wilson and Sperber (2012), some expressions have no \"literal meaning\" to be challenged because no \"literal meaning\" is mentioned in the context, based on which they raised relevance theory and the \"echoic\" concept. They considered irony as \"an echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought\" (Wilson, 2006). That is, if the \"echoic use\" is incongruous in some ways, the expression can be ironic. Based on this theory, Seto (1998) put forward that there are some \"echo-markers\" like definitely, really, and indeed.\n\nLi and Huang (2020) provided instances to show that \"incongruity\" does not have to be between the literal and contextual meanings of irony in certain circumstances. They believed that irony's true nature is a psychological activity as much as a verbal representation. The speaker or listeners must finish the \"reversal\" process on a psychological level for it to be completed. When compared to the concepts of \"echoic\" and \"incongruity,\" \"reversal\" is concerned not only with the results but also with the psychological processes that the speakers/listeners go through.", "filtered_refids": [["b13"], ["b69", "b54", "b51", "b68"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1787, "num_references": 5}
{"corpusid_sectionid": "252200083-s4", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Irony Markers and Constructions", "section": "Although most of the studies saw irony as a pragmatics phenomenon, people also considered that it can be reflected on the verbal, grammatical, or semantic level. For example, on the verbal level, people often use words like thank, congratulate, welcome, happy, and interesting to express ironic meanings. Laszlo (2017) found 15 core evaluative words which often show in ironic expressions. She generated patterns from these core evaluative words to extract ironic sentences from the corpus. For example, when the word love is in the pattern \"NP + would/ 'd/ wouldn't + love\", it is highly possible to be an ironic expression. On a grammatical level, people often use the subjunctive when they intend to be ironic. Besides that, semantic conflict is the most direct way to express ironic meaning. The incompatibility between the main words of the proposition leads to the ridiculousness of the proposition (e.g. It's very considerate of you to make such a loud noise while I was asleep). Besides, (Ghosh and Muresan, 2018) also categorized irony markers according to trope, morphosyntactic, and typographic types. Li (2021) considered that ironies are often expressed by specific \"constructions\", especially in short discourses. Larger than \"core evaluative words\" in Laszlo (2017), the \"constructions\" mentioned in Li (2021) are mostly in the form of idioms or phrases. The crucial feature of them is the lack of predictability. Most of them do not have to rely on too much contextual information, they themselves can provoke the process of reversal for readers or listeners (e.g. \u8d35\u4eba\u591a\u5fd8\u4e8b (honorable people frequently forget things)).", "filtered_refids": [["b24", "b29", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1632, "num_references": 3}
{"corpusid_sectionid": "252200083-s5", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Irony in Communication", "section": "Researchers claim that by using ironies, people have several kinds of intentions.\n\nBe polite: According to Brown et al. (1987), when unfavorable attitudes such as resistance, criticism, and complaints are stated with irony, the threat to the listener's reputation is reduced. The irony, as stated in Giora (1995), is an indirect negation. People prefer to utilize indirect negation to be polite to their listeners because direct negation can generate great unhappiness;\n\nEase criticisms: As reported by Dews and Winner (1995), irony helps to ease the expression's evaluative function. They believe that the incompatibility between literal meaning and contextual meaning can make it difficult to articulate negative feelings. However, Toplak and Katz (2000) argued that, while irony literally avoids conflict, it is more aggressive from the perspective of the speaker's goal;\n\nSelf-protection: Sperber and Wilson (1986) proposed the \"echoic\" idea, which stated that irony is a detached utterance that is simply an echo of another people's thought. It's a self-protection tactic, especially when the speakers are members of marginalized groups. According to , the irony is an \"off-record\" statement that allows speakers to deny their true intentions and avoid being challenged;\n\nBe amusing:  reported that when young people intend to be humorous, 50% of their communication is ironic. It can assist people in creating a dialogue platform on which speakers and listeners can agree and communicate more easily.", "filtered_refids": [[], [null, "b10"], ["b60", "b2"], ["b54"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1506, "num_references": 5}
{"corpusid_sectionid": "252200083-s6", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Irony and Sarcasm", "section": "Most of the studies saw sarcasm as a subset of irony (Tannen et al., 2005;Barbe, 1995;Kumon-Nakamura et al., 1995;Leggitt and Gibbs, 2000;Bowes and Katz, 2011). Sarcasm is often recognized as \"a nasty, mean-spirited, or just relatively negative form of verbal irony, used on occasion to enhance the negativity expressed relative to direct, non-figurative criticism\" (Colston, 2017).\n\nOne of the peculiarities of sarcasm is whether or not the speakers intend to offend the listeners. Kumon-Nakamura et al. (1995), for example, believe that sarcastic irony always conveys a negative attitude and is intended to harm the object being discussed. The non-sarcastic irony, on the other hand, can communicate either a good or negative attitude, and it is rarely meant to be hurtful. Barbe (1995) concurred that the core difference was \"hurtful\". She claimed that irony is a face-saving strategy while sarcasm is a face-threatening action. Ridicule is another feature of sarcasm. According to Lee and Katz (1998), sarcasm is closer to ridicule than irony. Their experiment revealed that sarcasm is directed at a single person, but irony is directed toward a large group of people. Haiman et al. (1998) claimed that one of the most distinguishing characteristics of sarcasm is that the literal meaning of its words is always positive. However, he did not convey his thoughts on irony. Whereas Littman and Mey (1991) viewed this topic from another angle. While there are many various forms of ironies, they believe that there is only one type of sarcasm because \"sarcasm cannot exist independently of the communication setting.\"\n\nCognitive scientists approached the difference in experimental studies. Previous research in child language acquisition (Glenwright and Pexman, 2010) reported that children understood sarcastic criticism later than they could understand the nonliteral meanings of irony and sarcasm, implying different pragmatic purposes of irony and sarcasm. Filik et al. (2019) utilized fMRI and found out sarcasm is associated with wider activation of semantic network in human brains compared to irony.\n\nHowever, most computational linguistics researchers used irony and sarcasm interchangeably, since the boundary between these two concepts is too vague for even human beings, let alone for machines. Joshi et al. (2016) and Sulis et al. (2016) verified this claim from both human annotators and computational perspectives. Although in this paper we will mainly focus on the literal irony processing and discuss the inspirations from recent research output in sarcasm processing, we aim to encourage unify sarcasm under the framework of irony via fine-grained annotation schemes.", "filtered_refids": [["b59", null, "b23", "b26"], ["b14", null, "b23", "b25"], ["b11", "b4"], [null, "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2687, "num_references": 12}
{"corpusid_sectionid": "252200083-s8", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Irony Textual Datasets", "section": "Some main target databases for irony processing research include social media platforms like Twitter and online shopping websites like Amazon. For example, Reyes and Rosso (2012) collected a 11,861document irony dataset based on customer reviews from several websites. Some other preliminary attempts to build irony benchmark datasets is  and Reyes et al. (2013), in which they used self-generated hashtag #irony as the gold standard and constructed 40,000-tweet and 50000tweet datasets from Twitter respectively, each including 10,000 ironic tweets and remaining nonironic ones. The irony benchmark dataset that is now widely used is from Van Hee et al. (2018a), consisting of 4,792 tweets and half of them were ironic. This dataset was also constructed via searching hashtags including #irony, #sarcasm, and #not.\n\nThere were also tremendous attempts to construct benchmark datasets in other languages. For example, Tang and Chen (2014) firstly built the NTU Irony Corpus including 1,005 ironic messages from Plurk, a Twitter-like social media platform, by mining specific ironic patterns and manually checking extracted messages. A recent Chinese benchmark dataset for irony detection was constructed by Xiang et al. (2020), which includes 8,766 Weibo posts, labelled from not ironic to strongly ironic in a five-scale system. Besides, irony datasets in Spanish, Greek, and Italian are also widely available. A comprehensive overview of the irony datasets is listed in Table 1.", "filtered_refids": [["b46"], ["b58", "b71"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1480, "num_references": 3}
{"corpusid_sectionid": "252200083-s15", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Supervised non-neural network era", "section": "Most research took irony detection as a simple classification problem. Before the popularity of deep learning, feature engineering is crucial for accurate irony detection. Generally, features could be divided into several levels.\n\nLexical features Lexical features are at the foundational level of NLP features, basically divided into bags of words (BOW) sets, word form sets, and conditional n-gram probabilities (Van Hee et al., 2018b). Representative BOW sets mainly include n-grams and character n-grams. Word form sets fo-cus on number and frequency, such as punctuation numbers, emoticon frequencies, character repetitions, etc. Despite their easiness to get, lexical features were proved effective in much research.\n\nSyntactic features Syntax is mainly quantified via parts-of-speech and named entities. After tagging, the number and frequency of both characteristics could act as features in classification models. Besides, hand-crafted syntactic features also included clash before verb tenses (Reyes et al., 2013) and dependency parsing (Cignarella et al., 2020a). Linguistic-motivated features Irony processing is deeply associated with sentiments and emotions. Therefore, researchers have offered many characteristics to capture irony patterns. For example, Reyes et al. (2013) proposed the feature of contextual imbalance, which was quantified via measuring the semantic similarity pairwise. Generally, most features could be categorized into ambiguity  and incongruity (Joshi et al., 2015). Take incongruity as an example, implicit incongruity was defined as a boolean feature checking containing implicit sentiment phrases or not; explicit incongruity was defined as number of times a polarity contrast appears. Theoretical research (Ghosh et al., 2020) is encouraging more semantic and pragmatic features to better capture ironies.\n\nFeatures at various levels were concatenated with classifiers, including naive bayes, decision tress, support vector machines (SVMs), etc. (Van Hee et al., 2018b) to get final classification results.", "filtered_refids": [[], ["b63"], ["b18", null, "b8", "b46"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2048, "num_references": 5}
{"corpusid_sectionid": "252200083-s18", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Irony Generation", "section": "Irony generation is mostly an underexplored research field besides Zhu et al. (2019), in which they defined irony generation as a style transfer problem, and utilized a Seq2Seq framework (Sutskever et al., 2014) with reinforcement learning to generate ironical counterparts from a non-ironic sentence. Concretely, they designed the overall reward as a harmonic mean of irony reward and sentiment reward, which was trying to capture the sentiment incongruity. In terms of the evaluation, besides traditional natural language generation metrics like BLEU, they also designed task-specific evaluation metrics, which shoule be further enhanced in irony and even figurative language research. Future work in irony generation could be advanced in new PLMs and theoretical accounts. For example, no attempts were made to generate ironical expressions after generative PLMs like BART (Lewis et al., 2020). Controllable irony generation and its interaction with agents are interesting topics remaining for future exploration. Besides, irony theories could be further utilized. In recent research on unsupervised sarcasm generation (Mishra et al., 2019;Chakrabarty et al., 2020), context incongruity, valence reversal, and semantic incongruity were merged to enhance the generation.", "filtered_refids": [["b56", "b35", "b27", null, "b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1272, "num_references": 5}
{"corpusid_sectionid": "252200083-s20", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Irony for Downstream NLP Tasks", "section": "Irony is directly associated with downstream NLU tasks like sentiment analysis and opinion mining. For example, the sentence retrieved from Filatova (2012) I would recommend this book to friends who have insomnia or those who I absolutely despise. is classified as positive by fine-tuned sentiment analysis RoBERTa model (Heitmann et al., 2020), which is apparently opposite to human evaluation. Wrong sentiment judgments will potentially lead to contrary opinion mining. We suggest that irony could be further captured through introducing incongruity embedding or specific pattern matching. Joshi et al. (2015) designed linguistic-motivated features implicit and explicit incongruity, which are inspiring for enhancing irony understanding. Consider another example task, machine translation, in which wrong translation will potentially lead to totally opposite meanings. We encourage to model discourse features (Voigt and Jurafsky, 2012), such as ironic patterns and punctuation as embeddings for robust irony translation.\n\nIn addition, we are looking forward to the research of irony and sarcasm processing in NLP for social good (NLP4SG), especially considering the strong sentiments hidden in ironies. A recent work (Chia et al., 2021) explored cyberbullying detection and this was a starting point to handle online harmful ironical contents.", "filtered_refids": [["b15", "b65", "b18"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1347, "num_references": 3}
{"corpusid_sectionid": "252200083-s22", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Multimodal Irony Processing", "section": "Linguistic interactions are not solely consisted of texts. Besides, facial expressions and speech communications are crucial to convey emotions and feelings. For example, Skrelin et al. (2020) reported people could classify ironies based on phonetic characteristics only. Consequently, it is conceivable that multimodal methods could help with irony detection. Schifanella et al. (2016) made the first attempt in multimodal sarcasm detection, in which they extracted posts from three multimodal social media platforms based on hashtags. Then they used SVMs and neural networks to prove the validity of visual information in enhancing sarcasm detection.\n\nCastro et al. (2019) made a great improvement in multimodal sarcasm detection by introducing audio features into the dataset. The experiments also verified the importance of more modalities in sarcasm processing.\n\nFuture work in multimodal irony processing should include a comprehensive multimodal irony dataset based on MUStARD dataset (Castro et al., 2019) with more fine-grained annotation schemes. Additionally, most methods (Pan et al., 2020;Liu et al., 2021) explored sarcasm by introducing intermodality and intra-modality attention in singlestream setting. How double-stream multimodal pretrained models (MPMs) will encode and interact in complex discourse settings remains an interesting problem to solve.", "filtered_refids": [["b53", "b50"], [], ["b40", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1369, "num_references": 4}
{"corpusid_sectionid": "252200083-s23", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Multilingual Irony Processing", "section": "To understand irony in a multilingual context is even harder due to cultural gaps. Previously listed dataset includes a Hindi-English code-mixed irony dataset (Vijay et al., 2018), in which they offered an example:\n\n\u2022 Text: The kahawat 'old is gold' purani hogaee.\n\nAaj kal ki nasal kehti hai 'gold is old', but the old kahawat only makes sense. #MindF #Irony.\n\n\u2022 Translation: The saying 'old is gold' is old. Today's generation thinks 'gold is old' but only the old one makes sense. #MindF #Irony.\n\nCignarella et al. (2020a) explored how mBERT performed in multiple languages' irony detection tasks separately. Given it has been proved codeswitching patterns are beneficial for NLP tasks like humor, sarcasm, and hate speech detection in RNNs settings (Bansal et al., 2020), A future direction is to merge the irony detection datasets from multiple languages (consider Karoui et al. (2017)) or even code-mixed texts, and explore how multilingual datasets could enhance irony understanding in mBERT.", "filtered_refids": [["b64"], [], [], [], ["b21", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 999, "num_references": 3}
{"corpusid_sectionid": "252200083-s26", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "New Tasks: Inspiration from Sarcasm", "section": "Compared to sarcasm, irony is rarely seen as a term in NLP conferences. Recently we have witnessed great improvements in sarcasm processing and in this part we will discuss how new tasks in sarcasm could motivate irony research.\n\nData Collection As discussed, datasets are highly dependent on hashtags as a signal to extract ironical expressions. Shmueli et al. (2020) proposed an algorithm to detect sarcastic tweets from a thread based on exterior cue tweets. A distant supervision based method for extracting ironies from platforms is crucial, given ironies in conversational contexts are central topic in the future.\n\nIntended and Perceived Irony Oprea and Magdy (2019) explored how author profiling affected the perceived sarcasm (manual labelling) versus the intended sarcasm (hashtags), and verified the difference between both. Further, Oprea and Magdy (2020) introduced iSarcasm dataset which divided intended sarcasms and perceived sarcasms. The state-of-the-art sarcasm detection models performed obviously worse than human evaluation on this dataset. Future work could focus on multimodal perceived and intended irony, especially across various cultures.\n\nTarget Identification Sarcasm target identification was firstly proposed in Joshi et al. (2018), in which sarcasm targets were classified as one target, several targets and outside. Patro et al. (2019) introduced sociolinguistic features and a deep learning framework, and improved target identification by a lot. For irony processing, most ironical expressions do not equip a specific target in itself as previously discussed. However, its ironical effects are likely in dialogue or visually grounded environment, which encourages us to enhance irony datasets in aforementioned ways.\n\nIrony Explanation Irony, according to the definition, have opposite real meanings to literal meanings. However, this does not mean adding a single negation could interpret ironies well. Kumar et al. (2022) proposed a new task, sarcasm explanation in dialogue. Irony explanation might encounter more complex problems due to relatively low proportion of targets. Still, we should include irony explanation as a branch of multimodal irony processing like Desai et al. (2021).", "filtered_refids": [[], ["b52"], [null], ["b41", "b17"], ["b22", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2226, "num_references": 6}
{"corpusid_sectionid": "251719280-s3", "title": "Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect", "date": "2022-08-22", "section_title": "Single-Domain Datasets", "section": "Single-domain text-to-SQL datasets typically collect question-SQL pairs for a single database in some real-world tasks, including early ones such as Academic (Li and Jagadish, 2014), Advising (Finegan-Dollak et al., 2018), ATIS (Price, 1990;Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), Yelp and IMDB (Yaghmazadeh et al., 2017), Scholar (Iyer et al., 2017) and Restaurants (Tang and Mooney, 2000;Popescu et al., 2003), as well as recent ones such as SEDE (Hazoom et al., 2021), ESQL (Chen et al., 2021a) and MIMICSQL (Wang et al., 2020d). These single-domain datasets, particularly the early ones, are usually limited in size, containing only a few hundred to a few thousand examples. Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models that are trained on these single-domain datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other unless otherwise specified.\n\ndomains (Finegan-Dollak et al., 2018;Yu et al., 2018c). However, since these datasets are adapted from real-life applications, most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020). Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.\n\nAppendix B gives a detailed discussion on domain knowledge and dataset convention, and concrete text-to-SQL examples.\n\nLarge Scale Cross-domain Datasets Large cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) are proposed to better evaluate deep neural models. Wik-iSQL uses tables extracted from Wikipedia and lets annotators paraphrase questions generated for the tables. Compared to other datasets, WikiSQL is an order of magnitude larger, containing 80,654 natural utterances in total (Zhong et al., 2017). However, WikiSQL contains only simple SQL queries, and only a single table is queried within each SQL query (Yu et al., 2018c). Yu et al. (2018c) propose Spider, which contains 200 databases with an average of 5 tables for each database, to test models' performance on complicated unseen SQL queries and their ability to generalize to new domains. Furthermore, researchers expand Spider to study various issues of their inter-est (Lei et al., 2020;Zeng et al., 2020;Gan et al., 2021b;Taniguchi et al., 2021;Gan et al., 2021a).\n\nBesides, researchers build several large-scale text-to-SQL datasets in different languages such as CSpider (Min et al., 2019a), TableQA (Sun et al., 2020), DuSQL (Wang et al., 2020c) in Chinese, ViText2SQL (Tuan Nguyen et al., 2020) in Vietnamese, and PortugueseSpider (Jos\u00e9 and Cozman, 2021) in Portuguese. Given that human translation has shown to be more accurate than machine translation (Min et al., 2019a), these datasets are annotated mainly by human experts based on the English Spider dataset. These Spider-based datasets can serve as potential resources for multi-lingual text-to-SQL research.\n\nOther Datasets Several context-dependent textto-SQL datasets have been proposed, which involve user interactions with the text-to-SQL system in English (Price, 1990;Dahl et al., 1994;Yu et al., 2019a,b) and Chinese (Guo et al., 2021). In addition, researchers collect datasets to study questions in text-to-SQL being answerable or not (Zhang et al., 2020), lexicon-level mapping (Shi et al., 2020b) and cross-domain evaluation for real Web databases (Lee et al., 2021).\n\nAppendix C.1 discusses more details about datasets mentioned in \u00a7 2.", "filtered_refids": [["b30", null], [null, "b28", "b9"], [], [null, "b28", "b13", "b32"], ["b11", null], [null, "b6", "b34"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3649, "num_references": 14}
{"corpusid_sectionid": "251719280-s7", "title": "Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect", "date": "2022-08-22", "section_title": "Graph-based Methods", "section": "Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures. As summarized in \u00a7 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing. As a result, modeling DB schema receives little attention. Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas. Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure. In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL  encodes more relationships for DB schemas such as \"both columns are from the same table\" in their graph.\n\nGraphs have also been used to encode questions together with DB schema. Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema. Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.\n\nFinally, graph-based techniques are also exploited in context-dependent text-to-SQL. For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.\n\nSelf-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.\n\nRAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns. DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.\n\nAdapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task. PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas. The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021). Other methods adjust the embeddings by PLMs. On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings. Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.\n\nHydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on. After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction. Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question. They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question. Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns. By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.\n\nPre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task. For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT. Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered. GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.", "filtered_refids": [[null, "b36"], [null], [], [null, "b40"], ["b15", "b38"], [null, "b31", "b40"], ["b42"], ["b43"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 34, "num_chars": 5296, "num_references": 12}
{"corpusid_sectionid": "251719280-s8", "title": "Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect", "date": "2022-08-22", "section_title": "Decoding", "section": "Various methods have been proposed for decoding to achieve a fine-grained and easier process for SQL generation and bridge the gap between natural language and SQL queries. As shown in Table 3 Table 9 in Appendix D. IR: Intermediate Representation. and other technologies.\n\nTree-based Seq2Tree (Dong and Lapata, 2016) employs a decoder that generates logical forms in a top-down manner. The components in the sub-tree are generated conditioned on their parents apart from the input question. Note that the syntax of the logical forms is implicitly learned from data for Seq2Tree. Similarly, Seq2AST (Yin and Neubig, 2017) uses an abstract syntax tree (AST) for decoding the target programming language, where the syntax is explicitly integrated with AST. Although both Seq2Tree (Dong and Lapata, 2016) and Seq2AST (Yin and Neubig, 2017) do not study text-to-SQL datasets, their uses of trees inspire tree-based decoding in text-to-SQL. SyntaxSQL-Net (Yu et al., 2018b) employs a tree-based decoding method specific to SQL syntax and recursively calls modules to predict different SQL components.\n\nSketch-based SQLNet (Xu et al., 2017) designs a sketch aligned with the SQL grammar, and SQL-Net only needs to fill in the slots in the sketch rather than predict both the output grammar and the content. Besides, the sketch captures the dependency of the predictions. Thus, the prediction of one slot is only conditioned on the slots it depends on, which avoids issues of the same SQL query with varied equivalent serializations. Dong and Lapata (2018) decompose the decoding into two stages, where the first decoder predicts a rough sketch, and the second decoder fills in the lowlevel details conditioned on the question and the sketch. Such coarse-to-fine decoding has also been adopted in other works such as IRNet (Guo et al., 2019). To address the complex SQL queries with nested structures, RYANSQL (Choi et al., 2021) recursively yields SELECT statements and uses a sketch-based slot filling for each of the SELECT statements.\n\nBottom-up Both the tree-based and the sketchbased decoding mechanisms can be viewed as top-down decoding mechanisms. Rubin and Berant (2021) use a bottom-up decoding mechanism. Given K trees of height t, the decoder scores trees with height t + 1 constructed by SQL grammar from the current beam, and K trees with the highest scores are kept. Then, a representation of the new K trees is generated and placed in the new beam.\n\nAttention Mechanism To integrate the encoderside information at decoding, an attention score is computed and multiplied with hidden vectors from the encoder to get the context vector, which is then used to generate an output token (Dong and Lapata, 2016;Zhong et al., 2017). Variants of the attention mechanism have been used to better propagate the information encoded from questions and DB schemas to the decoder. SQLNet (Xu et al., 2017) designs column attention, where it uses hidden states from columns multiplied by embeddings for the question to calculate attention scores for a column given the question. Guo and Gao (2018)  Copy Mechanism Seq2AST (Yin and Neubig, 2017) and Seq2SQL (Zhong et al., 2017) employ the pointer network (Vinyals et al., 2015) to compute the probability of copying words from the input. Wang et al. (2018a) use types (e.g., columns, SQL operators, constant from questions) to explicitly restrict locations in the query to copy from and develop a new training objective to only copy from the first occurrence in the input. In addition, the copy mechanism is also adopted in context-dependent text-to-SQL task (Wang et al., 2020b).\n\nIntermediate Representations Researchers use intermediate representations to bridge the gap between natural language and SQL queries. Inc-SQL (Shi et al., 2018)  However, the intermediate representations are usually designed for a specific dataset and cannot be easily adapted to others (Suhr et al., 2020). To construct a more generalized intermediate representation, Herzig et al. (2021) propose to omit tokens in the SQL query that do not align to any phrase in the utterance.\n\nInspired by the success of text-to-SQL task, intermediate representations are also studied for SPARQL, another executable language for database systems (Saparina and Osokin, 2021;Herzig et al., 2021).\n\nOthers PICARD (Scholak et al., 2021b) and UniSAr (Dou et al., 2022) set constraints to the decoder to prevent generating invalid tokens. Several methods adopt an execution-guided decoding mechanism to exclude non-executable partial SQL queries from the output candidates (Wang et al., 2018b;. Global-GNN (Bogin et al., 2019b) employs a separately trained discriminative model to rerank the top-K SQL queries in the decoder's output beam, which is to reason about the complete SQL queries instead of considering each word and DB schemas in isolation.  (2018); Lee (2019) use separate submodules to predict different SQL components, easing the difficulty of generating a complete SQL query. Chen et al. (2020b) employ a gate to select between the output sequence encoded for the question and the output sequence from the previous decoding steps at each step for SQL generation. Inspired by machine translation, M\u00fcller and Vlachos (2019) apply byte-pair encoding (BPE) (Sennrich et al., 2016) to compress SQL queries to shorter sequences guided by AST, reducing the difficulties in SQL generation.", "filtered_refids": [[], ["b1", "b22", "b26"], [null], [], ["b16", null, "b22"], [null, "b9", "b5"], [null], [null, "b26"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 33, "num_chars": 5403, "num_references": 13}
{"corpusid_sectionid": "251719280-s9", "title": "Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect", "date": "2022-08-22", "section_title": "Learning Techniques", "section": "Apart from end-to-end supervised learning, different learning techniques have been proposed to help text-to-SQL research. Here we summarize these learning techniques, each addressing a specific issue for the task. In SQL generation, IncSQL (Shi et al., 2018) allows parsers to explore alternative correct action sequences to generate different SQL queries. Brunner and Stockinger (2021) search values in DB to insert values into SQL query.\n\nFor context-dependent text-to-SQL, researchers adopt techniques such as turn-level encoder and copy mechanism (Suhr et al., 2018;Zhang et al., 2019;Wang et al., 2020b), constrained decoding (Wang et al., 2020b), dynamic memory decay mechanism (Hui et al., 2021a), treating questions and SQL queries as two modalities, and using bimodal pre-trained models (Zheng et al., 2022).", "filtered_refids": [["b5"], [null, "b10", "b33", "b35"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 817, "num_references": 5}
{"corpusid_sectionid": "251719280-s16", "title": "Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect", "date": "2022-08-22", "section_title": "B.5 Complexity of Natural Language and SQL Query Pairs", "section": "In terms of the complexity for SQL queries, Finegan-Dollak et al. (2018) find that models perform better on shorter SQL queries than longer SQL queries, which indicates that shorter SQL queries are easier in general. Yu et al. (2018c) define the SQL hardness as the number of SQL components. The SQL query is harder when it contains more SQL keywords such as GROUP BY and nested subqueries. Yu et al. (2018c)  In terms of the complexity of natural utterance, there is no qualitative measure of how hard the utterance is. Intuitively, models' performance can decrease when faced with longer questions from users. However, the information conveyed in longer sentences can be more complete, while there can be ambiguity in shorter sentences. Besides, there can be domain-specific phrases that confuse the model in both short and long utterances (Suhr et al., 2020). Thus, researchers need to consider various perspectives to determine the complexity of natural utterance.  Nguyen et al., 2020) and Jos\u00e9 and Cozman (2021) translate all the English questions in Spider into Chinese, Vietnamese and Portuguese, respectively. TableQA (Sun et al., 2020) follows the data collection method from WikiSQL, while DuSQL (Wang et al., 2020c) follows Spider. Both TableQA and DuSQL collect Chinese utterance and SQL query pairs across different domains. Chen et al. (2021a) propose a Chinese domain-specific dataset, ESQL.", "filtered_refids": [["b11", "b14", "b28", "b9", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1407, "num_references": 5}
{"corpusid_sectionid": "252571112-s3", "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability", "date": 2022, "section_title": "Shapley Values Approximation and SHAP", "section": "The idea of utilizing Shapley values to compute feature attribution scores precedes the SHAP framework (Lipovetsky and Conklin, 2001;Song et al., 2016). In this case, the outcome val of the game is the prediction of a machine learning model f and Shapley values \u03d5 f (i) measure the influence that each feature i has based on its current value. The early literature also worked on approximation strategies, as the exponential number of coalitions renders the exact estimation of Shapley values unfeasible (\u0160trumbelj and Kononenko, 2014;Datta et al., 2016). The main idea from these works is to compute \u03d5 f (i) only for a smaller selection of subsets S \u2286 F and to estimate the effect of removing a feature by integrating over training samples. This eliminates the need to retrain the model for each choice of S.\n\nThe work from Lundberg and Lee (2017) introduces a new perspective that unifies Shapley value estimation with popular explainability methods such as LIME (Ribeiro et al., 2016), LRP (Binder et al., 2016), and DeepLIFT (Shrikumar et al., 2017). Furthermore, they propose SHAP values as a unified measure of feature importance and prove them to be the unique solution respecting the criteria of local accuracy, missingness, and consistency. The authors contribute a library of methods to efficiently approximate SHAP values in a variety of settings:\n\nKernelSHAP: Adaptation of LIME-hence model-agnostic-to approximate SHAP values. As it works for any model f , it cannot make any assumption on its structure and is thus the slowest within the framework.\n\nLinearSHAP: Specific to linear models, uses the model's weight coefficients and optionally accounts for inter-feature correlations.\n\nDeepSHAP: Adaptation of DeepLIFT-hence specific to neural networks-to approximate SHAP values. Considerably faster than its model-agnostic counterpart as it makes assumptions about the model's compositional nature.\n\nWhile not initially presented in Lundberg and Lee (2017), the following algorithms were later  GradientSHAP: An extension of the Integrated Gradients (IG) method (Sundararajan et al., 2017)again specific to neural networks-that aggregates gradients over the difference between the expected model output and the current output.\n\nTreeSHAP: A fast method for computing exact SHAP values for both trees and ensembles (Lundberg et al., 2020a). In comparison to KernelSHAP, it also accounts for interactions among features.\n\nOther minor approaches-PermutationSHAP, SamplingSHAP, ExactSHAP, and MimicSHAPare also available in the official library 1 . To avoid confusion, we point out that the implementations have slightly different names: they use \"Explainer\" instead of \"SHAP\". For instance, KernelSHAP and DeepSHAP are implemented with the names of KernelExplainer and DeepExplainer respectively. Figure 2 sketches an explanation generated with SHAP.", "filtered_refids": [["b45", "b44", "b25", "b12"], ["b42", "b39", "b5"], [], [], [], ["b47"], ["b26"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2859, "num_references": 9}
{"corpusid_sectionid": "252571112-s6", "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability", "date": 2022, "section_title": "Review: SHAP-Based Approaches", "section": "Several works proposed methods based on SHAP, or more generally on Shapley values, following the contribution from Lundberg and Lee (2017). While the changes and variations introduced have been at times criticized for not being as rigorous as SHAP in following its core assumptions (Sundararajan and Najmi, 2020), SHAP-based methods continue to increase in both quantity and popularity.\n\nOur review categorizes SHAP-based approaches available to date based on how they differ from and how they improve on the original SHAP framework. We identify five broad categories in the existing literature, each one of them describing a different research direction pursued by its members:\n\n(C1) Tailored to Different Input Data: This category contains approaches specialized on specific input data structures such as graphs , structured text (Chen et al., 2020), and images (Teneggi et al., 2021). In some cases, approaches are used complementary for applications dealing with multimodal inputs Mosca et al., 2022b).\n\n(C2) Explaining Different Models: Methods in this class are specifically designed to explain predictions from particular types of machine learning models such as random forests (Lundberg et al., 2018;Labreuche and Fossier, 2018) and neural networks (Ghorbani and Zou, 2021). Hence, these are model-specific.\n\n(C3) Modifying Core Assumptions: SHAP treats features as independent. Newer methods offer the possibility to account for dependencies between features (Frye et al., 2019) and for causal structures behind their interactions (Heskes et al., 2020).\n\n(C4) Producing Different Explanations Types: SHAP is a framework for local featureattribution explanations, i.e. it attributes scores to input components based on their instance-level contributions. Methods in this category have a different scope and generate explanations that convey a different type of information. This can vary from global explanations (Covert et al., 2020) to counterfactual explanations (Singal et al., 2019) and concept explanations (Yeh et al., 2020). Clearly, these categories are not designed to be exclusive. Therefore, an approach can fall in more than one if it differs from SHAP in multiple aspects. Table 1 provides an overview of all approaches with their main characteristics. As one can observe, the majority of approaches are identified as part of more categories, i.e. research directions.", "filtered_refids": [["b46"], [], ["b49", "b34", "b7"], ["b28", "b23", "b17"], ["b15", "b18"], ["b11", "b43", "b56"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2390, "num_references": 12}
{"corpusid_sectionid": "252571112-s7", "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability", "date": 2022, "section_title": "Approaches Tailored to Different Inputs", "section": "SHAP does not make strong assumptions on the target model's input. While this suggests that it is suitable for all input types, its lack of specificity results in limitations when applied directly to different inputs than tabular data.  For text data, only measuring each individual feature's effect is an oversimplification, as words present strong interactions and their meaning and contribution heavily rely on the context. Thus, when it comes to text data, only considering single words as features is quite restrictive and relevance scores should be applied to multi-level tokens or even to entire sentences. Hierarchical Explanation via Divisive GEneration (HEDGE) (Chen et al., 2020) is an example of a SHAP-based method addressing this issue for (long) texts. Based on the weakest token interactions, it iteratively divides the text into shorter phrases and words in a topdown fashion. At each level, a relevance score is attributed to each token, resulting in a hierarchical explanation (Chen et al., 2020). PartitionSHAP, recently added to the official SHAP repository 3 , follows a similar strategy by creating hierarchical features coalitions and measuring their interactions.  (Bhatt et al., 2020) neighbors to explain a given instance n.a. ASV (C1) (C3) Relaxes the symmetry axiom of Shapley values Potentially Applicable (Frye et al., 2019) to incorporate causal structure into explanations R BShap (C4) (C5) Baseline approach to facilitate comparison Adaptable (Sundararajan and Najmi, 2020) between different Shapley value based methods n.a. C-and L-Shapley (C3) (C5) Efficient feature attribution method that models data Ready Off-the-Shelf (Chen et al., 2018) as a graph by considering only neighboring features TensorFlow CASV   Figure 3 sketches an example of a hierarchical explanation for text data. For models trained on graph data, especially graph DNNs, Yuan et al. (2021) proposed to explain predictions by using Shapley values as a measure of subgraph importance. The resulting method-named SubgraphX-also captures the interactions between different subgraphs.\n\nOn images, SHAP can face computational limitations as the number of features, i.e. pixels, can become extremely large. h-SHAP (Teneggi et al., 2021) efficiently retrieves exact Shapley values by hierarchically excluding irrelevant image areas from the computation. This is done following the observation that, if a certain area in the image is uninformative, so are its constituent sub-areas, which are therefore not worth exploring.", "filtered_refids": [["b15", "b57", "b46", "b4", "b9", "b7"], ["b49"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2523, "num_references": 7}
{"corpusid_sectionid": "252571112-s8", "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability", "date": 2022, "section_title": "Approaches Explaining Different Models", "section": "Explanation methods making fewer assumptions on the target classifier benefit from better applicability as they can explain a wider range of models. However, this can hinder explanations in terms of accuracy, information granularity, and computational efficiency. As we have already seen in 2.2: KernelSHAP has the key advantage of being modelagnostic, but it is drastically more inefficient than its DNN-specific counterpart DeepSHAP (Lundberg and Lee, 2017).\n\nAn example of a highly-specialized explainability method is TreeSHAP, presented by Lundberg et al. (2018) as an extension of the SHAP framework. This approach, only applicable to decision trees or ensembles thereof, is a highly efficient algorithm for exact SHAP values retrieval. Not only the approach needs considerably less computational effort than the more general variants such as KernelSHAP, but it leverages the decision tree structure to compute SHAP interaction values and thus captures pairwise interactions between features. Ghorbani and Zou (2021) proposes Neuron Shapley, a framework targeting DNN models which is able to quantify each individual neuron's contribution to single predictions and overall model performance. An example of the kind of explanation enabled by Neuron Shapley is visualized in figure 4. By analyzing interactions between neurons and picking those which exhibit the largest Shapley value, this method is particularly suitable for identifying neurons responsible for biases and  (Devlin et al., 2019). A Shapley value is assigned to each neuron depending depending on how they contribute towards the prediction (green) or against it (red).\n\nvulnerabilities (Ghorbani and Zou, 2021).", "filtered_refids": [[], ["b28", "b13", "b17"], ["b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1682, "num_references": 4}
{"corpusid_sectionid": "252571112-s9", "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability", "date": 2022, "section_title": "Approaches Modifying Core Assumptions", "section": "Assumptions made by SHAP can be at times too restrictive or simplistic, which can prevent explanations from accessing and leveraging crucial information such as dependency relationships between input features. For instance, already the symmetry property of Shapley values treats features as independent. While this can be true in some cases, for instance when dealing with tabular data with uncorrelated variables, it is an oversimplification when it comes to texts, images, and more structured data. Frye et al. (2019) introduces Asymmetric Shapley Values (ASV), which drops the symmetry assumption and enables the generation of model-agnostic explanations incorporating any causal dependency known to be present in the data. Similar approaches are:\n\n\u2022 Causal Shapley (Heskes et al., 2020), additionally requiring a partial causal ordering of the features as input.\n\n\u2022 Shapley Flow , which leverages a causal graph, encoding relationships among input features.\n\n\u2022 Shapr (Aas et al., 2021), an extension of Ker-nelSHAP relaxing the feature independence assumption.\n\n\"terrible\" \"disaster\" \"catastrophy\", ... \"the\", \"above\", \"up\", ...\n\n\"not\", \"even\" ... ...\n\n\"funny\", \"interesting\", ... \"cinema\", \"theater\", ... \"great\", \"success\", \"good!\", ...", "filtered_refids": [["b15"], ["b18"], [], ["b0"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1242, "num_references": 3}
{"corpusid_sectionid": "252571112-s11", "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability", "date": 2022, "section_title": "Approaches Producing Different Explanation Types", "section": "The SHAP framework and many of its derivatives mainly focus on generating local explanations based on feature importance. However, the general applicability of Shapley values combined with its strong foundations also offers potential for different explainability settings. More recent works have explored the usage of Shapley values to build other types of explanations conveying different kinds of information about the model and the available data. For instance, Data Shapley (Ghorbani and Zou, 2019) estimates the importance of each training sample for a given machine learning model. Similarly, SealSHAP (Parvez and Chang, 2021) attributes usefulness scores to data sources for transfer learning. Covert et al. (2020) introduces Shapley Additive Global importancE (SAGE), an explainability method analogous to SHAP but with a core focus on global explainability. More in detail, SAGE is a model-agnostic method that quantifies the predictive power of each input feature for a given model while also accounting for their interactions. An instructive example for NLP is shown in figure 5.\n\nAlongside local and global explainability, works like Yeh et al. (2020) adapt the notion of Shapley values for concept analysis (Sajjad et al., 2021). Given a set of concepts extracted from a model, the authors define the notion of completeness as a measure to indicate how sufficient such concepts are in explaining the model's predictive behavior. Furthermore, they propose ConceptSHAP, an unsupervised approach able to automatically retrieve a set of interpretable concepts without needing to know them in advance.", "filtered_refids": [["b11"], [null, "b56"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1609, "num_references": 3}
{"corpusid_sectionid": "252571112-s15", "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability", "date": 2022, "section_title": "Recommendations for NLP Use Cases", "section": "To build feature attribution explanations, HEDGE (Chen et al., 2020) is arguably the most suitable choice, as hierarchical explanations can contain more information than their non-hierarchical counterpart, e.g. generated with SHAP. The strength of HEDGE becomes even more apparent when dealing with long texts, where sentence structure is of major relevance for the model to be explained. L-Shapley, C-Shapley (Chen et al., 2018) and Parti-tionSHAP can also be considered where hierarchical explanations are not necessary and very computationally efficient methods are required instead. For model debugging, Neuron Shapley is suitable to identify neurons that are responsible for unintended biases or that are particularly vulnerable to adversarial attacks (Ghorbani and Zou, 2021). Pruning these neurons can be an effective method of alleviating such model defects (Ghorbani and Zou, 2021). To gain a global understanding of what the model has learned in practice, SAGE (Covert et al., 2020) combined with word grouping provides a summary of the features-e.g. words-that are most relevant for the model's performance. In this case, pruning irrelevant features can be also tested to improve model accuracy. A similar summary can be provided by ConceptSHAP (Yeh et al., 2020), which can compile a comprehensive list of the concepts identified by the model in an unsupervised fashion. Furthermore, ConceptSHAP can be used to determine the amount of model variance covered by the whole set of identified concepts (Yeh et al., 2020).\n\nIf causal structures or dependencies present in the text are known and can be explicitly modeled, then methods such as ASV (Frye et al., 2019), Shapley Flow , and Causal Shapley (Heskes et al., 2020) can leverage such information. For use cases involving graphs as part of multi-modal inputs-e.g. modeling a social network )-any of the previous methods can be combined with SubGraphX (Yuan et al., 2021) to also produce explanations for the graph component of the input.\n\nWhen it comes to sequence-to-sequence tasks such as question answering and machine translation, the usage of SHAP-based methods has not been explored in depth. With a few exceptions 4 , available approaches seem particularly tailored only to classification settings. We believe this is a strong limitation and we encourage the reader to look for alternatives.", "filtered_refids": [["b11", "b56", "b9", "b7", "b17"], ["b15", null, "b18"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2362, "num_references": 8}
{"corpusid_sectionid": "252571112-s16", "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability", "date": 2022, "section_title": "Criticisms", "section": "The usage of Shapley values for generating model explanations has also been criticized. For instance, Kumar et al. (2020) shows that using Shapley values for feature importance leads to mathematical inconsistencies which can only be mitigated by introducing further complexity like causality assumptions. Moreover, the authors argue that Shapley values do not represent an intuitive solution to the human-centric goals of model explanations and thus are only suitable in a limited range of settings. Sundararajan and Najmi (2020), on the other hand, criticize some Shapley-value-based methods. In fact, while a strong case for utilizing Shapley values can be made thanks to their uniqueness result in satisfying certain properties (see 2.1), often methods employing them operate under different assumptions and hence the uniqueness results loses validity in their context. Merrick and Taly (2020) argues that existing SHAP-based literature focuses on the axiomatic foundation of Shapley values and their efficient estimation but neglects the uncertainty of the explanations produced. The authors illustrate how small differences in the underlying game formulation can lead to sudden leaps in Shapley values and can attribute a positive contribution to features that do not play any role in the machine learning model.", "filtered_refids": [["b21", "b31", "b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1317, "num_references": 3}
{"corpusid_sectionid": "252819333-s1", "title": "A Survey of Automatic Text Summarization using Graph Neural Networks", "date": 2022, "section_title": "Why graph neural networks ?", "section": "Contemporary solutions to the task of ATS suffer from a number of issues, chiefly an inconsistent evaluation protocol and, somewhat, a lack of progress, as noted by Kry\u015bci\u0144ski et al. (2019). In recent years GNNs have been successfully applied to a number of downstream NLP tasks such as classification    and translation (Xu et al., 2021) (Yin et al., 2020). Although GNNs may not be able to solve all problems related to the task of ATS, we believe that they can at least give a new perspective to this task. Generally GNNs bring a number of advantages to ATS which we believe to be significant enough to warrant further research, and this survey. In particular we want to highlight the following aspects of GNNs:\n\n\u2022 Scalability and Flexibility. A vast number of ATS models are based on BERT (Devlin et al., 2019). However, the computational complexity of BERT-based ATS models grows quadratic with the input length; due to the selfattention operation. This fact renders them impractical for long, or even medium sized text documents. Recently some work has been done in order to circumvent this limiting factor (Ding et al., 2020) (Zhang et al., 2021). In contrast, GNNs can scale by their nature to graphs of thousands of nodes and more. This is in part due to the linear scaling of the memory cost with regards to the input size. The total memory cost of a GNN model depends on the size of the graph, the number of layers and the feature vector size of the nodes present. Formally, for L layers and an input of N nodes with each node's feature vector being of size H the memory complexity is O(LN H). But even for very large graphs on the scale of millions of nodes one can utilize GNNs. This can be achieved using methods such as neighbour sampling or distributing the graph over multiple GPUs, as done for example by Jia et al. (2020b). We recommend the paper by  for insights as to how one can train large and very deep GNNs. As the input of a GNN is a graph, the input can vary in size, therefore GNNs are also able to cope with changing text sizes and structures. Both of these aspects combined allow GNNs to produce summaries which are not restricted by hard-coded limits related to input or output size.\n\n\u2022 Understanding and Explainability. It is often difficult to understand why a model arrived at a certain conclusion. Additionally it is often difficult to see how the model aggregates information. This is not the case with GNNs, as with the help of methods such as GNN Explainer (Ying et al., 2019) one can understand which nodes were used by the model to reach its output. This removes a layer of the blackbox magic present in many current non-GNN models. We recommend the survey by Yuan et al. (2020) for an overview of methods for generating explanations for GNNs.", "filtered_refids": [["b45", "b42", "b16"], ["b14", "b48", "b6", "b7"], ["b47", "b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 2783, "num_references": 9}
{"corpusid_sectionid": "252819333-s6", "title": "A Survey of Automatic Text Summarization using Graph Neural Networks", "date": 2022, "section_title": "Spatial Convolution and Message Passing", "section": "One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.\n\nDirectly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:\n\nThis first equation describes how messages are generated. A differentiable function \u03d5 generates messages m for each edge which connects nodes using the node features and edge feature present.\n\nThe above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function \u03c1. This function aggregates all incoming messages to a node. Then another differen-tiable function \u03c8 combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.\n\nThe convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.\n\nwhere M (i) represents the set of messages received by node i, \u03c3 is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.\n\nThe above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veli\u010dkovi\u0107 et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1\n\nwhere \u03b1 i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with\n\n. This score is then normalized to obtain the attention score per edge \u03b1 i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows\n\n). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.\n\nThere are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.\n\nIn ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.\n\nConvolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.\n\nWe want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.", "filtered_refids": [[], ["b11"], [], [], [], [], ["b31", "b32"], [], ["b3"], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 52, "num_chars": 6773, "num_references": 4}
{"corpusid_sectionid": "252819333-s8", "title": "A Survey of Automatic Text Summarization using Graph Neural Networks", "date": 2022, "section_title": "Standalone GNNs", "section": "We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.\n\nThe HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.\n\nThe feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.\n\nThe classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.\n\nThe results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.\n\nAn older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.\n\nA model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.\n\nThis idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.\n\nTaking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.\n\nHAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.\n\nThe results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.", "filtered_refids": [[], [], [], ["b30"], [], ["b43", "b29"], ["b43", "b22"], ["b1"], [null], [], ["b52"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 51, "num_chars": 5942, "num_references": 8}
{"corpusid_sectionid": "252819333-s9", "title": "A Survey of Automatic Text Summarization using Graph Neural Networks", "date": 2022, "section_title": "Embedded GNNs", "section": "The first embedded GNN model we will present is the GRU-GCN model by Yasunaga et al. (2017). Despite being an older model it provides an illustrative introduction as to how one can effectively incorporate GNNs into established deep learning methods. As this model utilizes GNNs within a sequence-sequence architecture it falls into the encoder-decoder category of embedded GNNs. The model works exclusively with multi-document summarization. We want to note that due to the age of this model the GNN uses spectral-based convolution instead of spatial-based convolution.\n\nAs the model is an encoding-decoding embedded GNN it features three parts: the encoding part, the GNN and a decoding part. The encoding and decod-GNN CNN/DailyMail Performance Overview Model Name Type Description R-1 R-2 R-L BERTSUMEXT (Liu and Lapata, 2019) Extractive BERT baseline  Topic-Graphsum  Extractive (   Extractive (Standalone) Sentence information encoding 43.16 20.14 39.49 HSG  Extractive ( ing are done by gated recurrent networks (GRUs). More specifically, sentence encodings are produced by the encoding GRU network. These sentence encodings are then used by the GNN whose input graph consists solely of sentence nodes. The edges are determined by semantic relatedness of the sentences. The resulting sentence node feature vectors produced by the GNN are passed to the decoding GRU which computes salience scores for each sentence. The results for this model have been surpassed by other models. Similar to the GRU-GCN model, the Topic-GraphSum model by  combines an established deep neural network method with a GNN. In it a variational autoencoder is utilized for modeling topics within a given text, that is, it learns latent topics via encoding-decoding. The GNN is fed a graph consisting of topic nodes and sentence nodes. The topic node embeddings are produced by the autoencoder and the sentence node embeddings are produced by BERT. The GNN utilizes a GAT for the prediction of sentences to be used for the summary. Note that the autoencoder and the GNN are trained jointly, which is why this model is classified as an embedded model. However, as it does not utilize the encoder-decoder architecture it falls into the category of other embedded GNNs.\n\nAnother embedded GNN which does not follow the encoder-decoder schema is the model by Xu et al. (2020b). Their DISCOBERT model incorporates a GNN into a BERTSUM (Liu and Lapata, 2019) like architecture.\n\nThe DSGSum model developed by Bi et al. (2021) utilizes a GNN to enhance the semantic information provided to the model. DSGSum uses the GNN mainly for encoding entity information. The input to the GNN consists of an entity graph which has been enriched with a knowledge graph (KG); specifically, the entities and their relations as defined in the KG are encoded into the graph. The GNN then utilizes GAT to produce entity embeddings which are directly used by the decoder part of the architecture. Hence DSGSum is an embedded encoder-decoder model. All of the following embedded GNN ATS models follow the ATS GNN encoder-decoder pattern. The general principle of these models is illustrated by Figure 4. This principle being the usage of the GNN to supplement the information provided to the decoder, while also utilizing an encoder for generating the input to the GNN.\n\nThe authors of (Wu et al., 2021b) produce a graph based on the semantics of each sentence. The encoding produced by the GNN and a textual encoding of each sentence are then passed to a decoder. Similarly, the model by  uses a GNN as an encoding component. Their model utilizes the dependency tree of the input as a graph input to the GNN. It also uses a modified attention mechanism which is used to decode the attention of the GAT directly into the decoder part of the architecture. Another model in this category is the model by Liang et al. (2021). Following this idea even further is the model by Li et al. (2020). However, in contrast to DSGSum or others, their model uses a GNN directly within the transformer based encoder and decoder blocks, and not as an outside component providing additional encodings.\n\nAnother area where GNNs have found some usage is in the abstractive summarization of di-alogues, which although a niche area, is still part of the ATS task. The model by  uses a GNN to encode the structure of the conversation into their sequence-to-sequence architecture. This is also done by Feng et al. (2020). Also for dialogue summarization Feng et al. (2021) introduce a special type of graph encoding to a sequenceto-sequence architecture. They utilize a GNN as an encoder and their input graph links information about the speaker, the spoken sentences and other information together.\n\nAt this point we also want to shortly mention a few models which do not perform classical ATS but do perform a specialized form of summarization with the help of a GNN. The models by  and LeClair et al. (2020) both perform code summarization, that is they generate a natural language description/summarization of code written in a programming language. They both utilize a GNN, and also both leverage the abstract syntax tree of the program given. Another interesting model is the model by Wu et al. (2019). Their model performs multi-video summarization with the help of a GNN.", "filtered_refids": [["b44"], ["b25"], ["b40"], ["b2"], ["b21", "b53", "b37"], ["b10", "b9"], ["b34", "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 47, "num_chars": 5310, "num_references": 11}
{"corpusid_sectionid": "232320384-s2", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Text", "section": "Due to the availability of large amounts of textual content, research on the text modality is comparatively richer than for other modalities. Notable work in this direction covers fake news spread on social media (Vosoughi et al., 2018b), fake news and fact-checking on news media (Rashkin et al., 2017), fact-checking such as fact-checked URL recommendation model (Vo and Lee, 2018) (Sathe et al., 2020). There have also been recent efforts for factchecking from political debates (Shaar et al., 2020(Shaar et al., , 2022(Shaar et al., , 2021Nakov et al., 2022b,a), fact-checking with evidence reasoning (Si et al., 2021;Jiang et al., 2021;Wan et al., 2021) and fact-checking by claim matching (Kazemi et al., 2021). Given that there have been surveys on the text modality for fake news/disinformation detection and fact-checking, here we will not go into more detail about the individual studies.", "filtered_refids": [["b52", "b58", "b50", "b45", null, "b69", "b74", "b42", "b51", "b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 898, "num_references": 10}
{"corpusid_sectionid": "232320384-s3", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Image", "section": "Text with visual content (e.g., images) in social media is more prominent as it is more intuitive; thus, it is easier to consume, it spreads faster, it gets 18% more clicks, 89% more likes, and 150% more retweets . Due to the growing number of claims disseminated with images, in the current literature, there have been various studies that address the visual content with text for predicting misleading information (Volkova et al., 2019), fake images (Gupta et al., 2013), images shared with misinformation in political groups (Garimella and Eckles, 2020), and fauxtography Wang et al., 2021). Some of these studies attempt to understand how two different modalities are used. Their analyses show that the extension of text with images increases the effectiveness of misleading content. Gupta et al. (2013) highlighted the role of Twitter to spread fake images. This study reports that 86% tweets spreading fake images are retweets. Garimella and Eckles (2020) manually annotated a sample of 2,500 images collected from public WhatsApp groups, and labeled them as misinformation, not misinformation, misinformation already fact-checked, and unclear; however, experiments were conducted with binary labels: misinformation vs. not-misinformation. The authors found that violent and graphic images spread faster. Nakamura et al. (2020) developed a multimodal dataset containing 1M posts including text, images, metadata, and comments collected from Reddit. The dataset was labeled with 2, 3, and 6-ways labels. Volkova et al. (2019) proposed models for detecting misleading information using images and text.\n\nFauxtography is defined as \"visual images, especially news photographs, which convey a questionable (or outright false) sense of the events they seem to depict\" (Cooper, 2007). It is also commonly used in social media in different forms such as a fake image with false claims, a true image with false claims.  defined that \"a post is a fauxtography if the image of the post (i) directly supports a false claim, or (ii) conveys misinformation of a true claim.\" An example is shown in Figure 2 (in Appendix A).  developed FauxBuster to detect fauxtographic social media content, which uses social media comments in addition to the content in the images and the texts. Zlatkova et al. (2019) investigated the factuality of claims with respect to images and compared the performance of different feature groups between text and images. Wang et al. (2021) analyzed fauxtography images in social media posts and found that posts with doctored images increase user engagement in the form of re-shares, likes, and comments, specifically in Twitter and Reddit. They pointed out that doctored images are often used as memes to mislead or as a means of satire, and that they have a 'clickbait' power to drive engagement.", "filtered_refids": [["b70", null, "b25"], ["b93", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2817, "num_references": 5}
{"corpusid_sectionid": "232320384-s4", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Speech/Audio", "section": "There have been attempts to use acoustic signals to predict the factuality of claims in political de-bates (Kopev et al., 2019;Shaar et al., 2020), leftcenter-right bias in YouTube channels (Dinkov et al., 2019), and deception in speech (Hirschberg et al., 2005). Kopev et al. (2019) found that the acoustic signal helps in improving the performance compared to using only textual and metadata features. Similarly, Dinkov et al. (2019) reported that the use of speech signal improves the performance of the system for detecting the political bias (i.e., left, center, right) of Youtube channels. Moreover, a large body of work was done on deception detection using the acoustic signal. Hirschberg et al. (2005) created the Columbia-SRI-Colorado (CSC) corpus by eliciting within-speaker deceptive and non-deceptive speech. Their experiments consist of the use of acoustic, prosodic, and a variety of lexical features including 68 LIWC categories, filled pauses, and paralinguistic information (e.g., speaker information, gender, field-pause). Using the same corpus, an evaluation campaign was organized, where different multimodal approaches were proposed, such as fusion of different acoustic, prosodic, lexical, and phonotactics representations (Levitan et al., 2016;Kaya and Karpov, 2016).", "filtered_refids": [["b52", null, "b9", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1291, "num_references": 4}
{"corpusid_sectionid": "232320384-s6", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Network and Temporal Information", "section": "The rationale for leveraging network information stems from early work (Shao et al., 2018;Vosoughi et al., 2018a) that showed that propagation and interaction networks of fake news are deeper and wider than those of real news. Vosoughi et al. (2018a) further found that fake information spreads faster than factual one, thus advocating for the use of temporal information.\n\nPropagation networks can be homogeneous or heterogeneous (e.g., encompassing news articles, publishers, users, and posts) and they can be analyzed at different scales (e.g., node-level, ego-level, triad-level, community-level and the overall network, as shown in Figure 3, in Appendix) (Zhou and Zafarani, 2019). Shu et al. (2020) tackled the fake news classification task by proposing an approach based on hierarchical propagation networks. At both micro-and macro-scale, they extracted and jointly considered network features, temporal features, and linguistic features. Experiments on PolitiFact and GossipCop datasets revealed that temporal features have maximum contribution, followed by network and linguistic features. Shu et al. (2019) provided one of the most thorough multimodal frameworks for fake news classification. Their experimental results suggest that social context (i.e., network-derived) features are more informative than news content ones. Vosoughi et al. (2017) proposed Rumor Gauge, a system that jointly exploits temporal and propagation features, in conjunction with linguistic and user credibility features, for checking the veracity of rumors. In particular, Rumor Gauge leverages text, and network propagation. The temporal modality does not directly provide features, but is instead considered by recomputing all other features at regular time steps, thus yielding multiple time series. Results by Vosoughi et al. (2017) and Kwon et al. (2017) also demonstrated that the contribution of the different data modalities change over time.\n\nTo mitigate the \"cold start\" problem of propagation-based early detection of fake news, Liu and Wu (2018) proposed an approach that is primarily based on user and temporal information. First, they built a propagation path of each news as a time series of user representations. The time series for a given news only contains the ordered representations of those users that shared such news. Then, they learned two vector representations of each propagation path via GRUs and CNNs, respectively. Zannettou et al. (2018) analyzed different aspects of memes, such as how they evolve and propagate in different mainstream and fringe web communities, and variants of memes that propa-gate. Finally, Nguyen et al. (2020) proposed Factual News Graph (FANG) to exploit the social structure and the engagement patterns of users for fake news detection.", "filtered_refids": [["b72", "b53"], ["b90", "b57", "b55", "b71", "b8"], ["b16", "b29", "b84"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2783, "num_references": 10}
{"corpusid_sectionid": "232320384-s7", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Multimodal Harmful Content Detection", "section": "In this section, we focus on the second aspect of disinformation: harmfulness. It is essential to filter or to flag online harmful content. The harmful content includes child abuse material, violent and extreme content, hate speech, graphic content, sexual content, and spam content (Banko et al., 2020). 3 In recent years, the ability to recognize harmful content within online communities has received a lot of attention by researchers (Pramanick et al., 2021a,b) and policymakers that aim to keep users safe in the digital world. Studies in this direction include detecting harmful contents in network science (Ribeiro et al., 2018), natural language processing (Waseem et al., 2017;Schmidt and Wiegand, 2017b;Fortuna and Nunes, 2018) and computer vision (Yang et al., 2019a;Vijayaraghavan et al., 2021;Gomez et al., 2020;Dimitrov et al., 2021b). In Table 2, we provide a list of relevant work addressing different types of harmful content, modalities, source of data, annotation approach, language of the content and the methods.", "filtered_refids": [["b44", "b78", "b47", null, "b81", "b38", "b68"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1033, "num_references": 7}
{"corpusid_sectionid": "232320384-s8", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Text", "section": "In the past few years there has been significant research effort on detecting harmful content (e.g., hate speech) from social media posts (Van Hee et al., 2015;Waseem and Hovy, 2016;Waseem et al., 2017;Schmidt and Wiegand, 2017b). Waseem and Hovy (2016) developed a dataset of hate speech consisting of 16K tweets, and reported a baseline results using char-and word-ngrams and a logistic regression classifier. (Davidson et al., 2017) distinguished between hate speech, and offensive language. They developed a dataset of \u223c24K labeled tweets with categories such as hate speech, offensive language and neither. Qian et al. (2018) took a different approach to classic hate speech classification. Instead of binary classes, they proposed 13 fine-grained hate categories such as nationalist, anti-immigrant, racist skinhead, among others, providing a dataset of tweets collected from 40 hate groups. Ribeiro et al. (2018) proposed an approach to find hateful users on Twitter. Mathew   users and 21M posts collected from Gab to understand the diffusion dynamics of hateful content. Their findings suggest that the posts from hateful user diffuse faster, wider, and have a greater outreach compared to the posts from non-hateful ones.", "filtered_refids": [["b67", "b44", "b40", "b78", "b47", "b79", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1231, "num_references": 7}
{"corpusid_sectionid": "232320384-s9", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Image", "section": "Among different types of harmful content, cyberbullying is one of the major growing problems, significantly affecting teens. Hosseinmardi et al. (2015) investigated Instagram images and their associated comments for detecting cyberbullying and online harassment. They developed a manually labeled dataset using CrowdFlower (which is now Appen), where they followed standard procedures for the annotation: using annotation guidelines, qualification tests, gold standard evaluation and quality control criteria such as minimum annotation time. The annotated dataset consists of 998 media sessions (images and their associated comments). A key finding of this study is that a large fraction of the annotated posts (48%) with a high percentage of negative words have not been labeled as cyberbullying. To train and to evaluate the model, the authors used n-grams from text, meta-data (e.g., the number of followers, followees, likes, and shared media), and image categories as features and experimented with Na\u00efve Bayes and SVM classifiers. Their study suggests that combining multiple modalities helps to improve the performance of the SVM classifier.\n\nHate speech is another important problem that spreads over social media. The \"Hateful Memes Challenge\" is an important milestone to advance the research on this topic and the tasks is to detect hateful memes (Kiela et al., 2020). Das et al. (2020) proposed different approaches for hatefulness detection in memes such as (i) extract the caption and include this information with the multimodal model, (ii) use sentiment as an additional feature with multimodal representations. For hate speech detection, Yang et al. (2019a) explored different fusion techniques such as concatenation, bilinear, gated summation, and attention, and reported that combining the text with image embedding boosted the performance in all cases. Vijayaraghavan et al. (2021) proposed methods for interpreting multimodal hate speech detection models, where the modalities consist of text and socio-cultural information rather than images. Concurrently, Gomez et al. (2020) introduced a larger dataset of 150K tweets for multimodal hate speech detection, consisting of six labels.\n\nPropaganda is another topic that has been explored in multimodal settings. Seo (2014) showed how Twitter was used as a propaganda tool during the 2012 Gaza conflict to build international support for each side of the conflict. Dimitrov et al. (2021b) addressed the detection of persuasion techniques in memes. Their analysis of the dataset showed that while propaganda is not always factually false or harmful, most memes are used to damage the reputation of a person or a group of people. Dimitrov et al. (2021a) highlighted the importance of both modalities for detecting finegrained propaganda techniques, with VisualBERT yielding 19% improvement compared to using the image modality only (with ResNet-152), and 11% improvement compared to using the text modality only (with BERT). Similar observations were made by (Kiela et al., 2020) for hateful meme detection. Glenski et al. (2019) explored multilingual multimodal content and categorizes disinformation, propaganda, conspiracy, hoax, and clickbait.", "filtered_refids": [["b41"], ["b68", null, "b2", "b81"], ["b49", "b38", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3214, "num_references": 8}
{"corpusid_sectionid": "232320384-s10", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Speech/Audio", "section": "Cues in spoken content can represent harmful behaviors and those cues can be used to automatically detect such content. Due to the lack of data, studies using the speech-only modality are comparatively lower than other modalities even though it plays a major role in many contexts. For example, for detecting violent content such as screaming and gunshots, the speech modality can play an important role, which other modalities might not be able to offer. This is important as most often user-generated contents are posted on newspapers or their social media accounts without verifying the content of the post, which can have serious consequences (Harkin et al., 2012;Rauchfleisch et al., 2017).\n\nGiannakopoulos (2009) studied the audio segmentation approaches for segmenting violent (e.g., gunshots, screams) and non-violent (e.g., music, speech) content in movies. The studies related to violent content detection using acoustic features also include (Acar et al., 2013), where the focus was on finding violent content in movies. Liang et al. (2017) proposed Localized Self-Paced Reranking (LSPaR) for detecting gunshots and explosion in videos using acoustic features. Soni and Singh (2018) investigated audio, visual and textual features for cyberbullying detection.\n\nTheir findings suggest that audio and visual features are associated with the occurrence of cyberbullying, and both these features complement textual features.", "filtered_refids": [["b43", null], [null, "b63", "b13"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1431, "num_references": 5}
{"corpusid_sectionid": "232320384-s11", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Video", "section": "There are multiple studies on detecting cyberbullying in video-based social networks such as Vine (Rafiq et al., 2015) and YouTube (Dadvar and Eckert, 2018). These studies show that although the percentage of cyberbullying in video sessions is quite low, automatic detection of these types of content is very challenging.  used textual, visual, and other meta-information to detect social media posts with bullying topics. Their proposed method was evaluated on publicly available multimodal cyberbullying datasets. Abd Kadir et al. (2016) investigated the relationship between emotion and propaganda techniques in Youtube videos. Their findings suggest that propaganda techniques in Youtube videos affect emotional responses. Content (e.g., Youtube videos) can also be attacked by hateful users via posting hateful comments through a coordinated effort. Mariconti et al. (2019) investigated whether a video is likely to be attacked using different modalities such as metadata, audio transcripts, and thumbnails.\n\nThere has been a recent interest from different government agencies to stop the spread of violent content. Constantin et al. (2020) developed a multimodal dataset, which consists of more than 96 hours of Hollywood and YouTube videos and high variability of content. Their study suggests that multimodal approaches with audio and images perform better.", "filtered_refids": [["b21", null, "b41"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1365, "num_references": 3}
{"corpusid_sectionid": "232320384-s12", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Network and Temporal Information", "section": "The use of network data for predicting factuality was motivated by results showing different propagation patterns for fake vs. real content. Such results are lacking for harmful content. However, the intention to harm in social media is often pursued via coordinated actions, for instance, by groups of users (e.g., social bots and trolls (Cresci, 2020)) that target certain people or minorities. These collaborative harmful actions, perpetrated to increase the efficacy of the harm, are best addressed using network analysis to detect likely coordinated harmful campaigns. Chatzakou et al. (2019) focused on detecting cyberbullying and cyberaggression by training machine learning models for detecting: (i) bullies, (ii) aggressors, (iii) spammers, and (iv) normal users on Twitter. To solve these tasks, they leveraged a combination of 38 features extracted from user profiles, the textual content of their posts, and network information (e.g., user degree and centrality measures in the social graph). Orthogonal and in synergy with respect to the detection of disinformation, scholars have recently focused on the novel task of detecting Coordinated Inauthentic Behavior (CIB) (Nizzoli et al., 2021). CIB is defined as coordinated activities that aim to mislead and manipulate others. 4 Detecting CIB typically involves analyzing both interaction networks to detect suspicious coordination, as well as the coordinated users and the content they shared to detect inauthentic users and harmful content (Nizzoli et al., 2021(Nizzoli et al., , 2020Pacheco et al., 2021). Given the importance of coordination in CIB, the analysis typically starts from the available network data by applying community detection algorithms, and subsequently moving to the analysis of textual data.", "filtered_refids": [["b30", "b33", "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1778, "num_references": 3}
{"corpusid_sectionid": "232320384-s13", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Modeling Techniques", "section": "In this section, we discuss modeling techniques for both factuality and harmfulness. To combine multiple modalities, there have been several approaches: (i) early-fusion, where low-level features from different modalities are learned, fused, and fed into a single prediction model ( Their findings suggest that self-supervised joint learning models, such as MMBT, ViLBERT, and Visual-BERT perform better in increasing order, respectively, compared to the other fusion methods. As 4 https://medium.com/1st-draft/how-to-improve-ouranalysis-of-coordinated-inauthentic-behavior-a4ec62ce9bff a part of \"Hateful Memes Challenge\" to classify hateful vs. non-hateful memes, several such models have been investigated by Kiela et al. (2020).\n\nAttempts to design unsupervised models are limited. M\u00fcller-Budack et al. (2020) introduced Crossmodal Consistency Verification Tool (CCVT) to check the coherence between images and associated texts. Yang et al. (2019b) defined trust of news and credibility of users who spread the news and used Bayesian learning to iteratively update these quantities. News with low trustworthiness is returned as fake news. Gangireddy et al. (2020) proposed GTUT, a graph-based approach that exploits the underlying bipartite network of users and news articles to detect the dense communities of fake news and fraud users.\n\nDue to the scarcity of labeled data, a few studies attempted to design semi-supervised methods by leveraging an ample amount of unlabelled data. Helmstetter and Paulheim (2018) Within a supervised learning setup, two other types of learning method have also been explored for disinformation detection such as adversarial learning and autoencoder based. Adversarial learning models for fake news detection include EANN , an event adversarial neural network to detect emerging and timecritical fake news, and SAME (Cui et al., 2019), a sentiment-aware multimodal embedding method which leverages multiple modalities with the sentiment expressed by readers in their comments.", "filtered_refids": [["b2"], [null, "b82"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2015, "num_references": 3}
{"corpusid_sectionid": "232320384-s14", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Major Challenges", "section": "Recently, several initiatives were undertaken by major companies and government entities to combat disinformation in social media (DIGI, 2021). 5 However, automatic detection of misleading and harmful content poses a number of challenges as discussed below and in Appendix (Section D).\n\nModels Combining Multiple Modalities. The major challenge is to devise a mechanism to combine multiple modalities in a systematic way so that one modality complements the others. Current stateof-the-art primarily adopts early and late fusion, which are limited and do not always yield strong results (Dimitrov et al., 2021a). Very recently, jointly trained multimodal transformer-based models (e.g., ViLBERT (Lu et al., 2019), Visual BERT (Lin et al., 2014) and Multimodal Bitransformers (MMBT) (Kiela et al., 2019)) have shown strong potential (Dimitrov et al., 2021b,a;Kiela et al., 2020). However, such models are trained considering only two modalities (textual and visual), while fact-checking or disinformation-related content consists of more than two modalities e.g., text, speech, video, network, etc. (Baly et al., 2020). Hence, there is a room for improvement in developing multimodal models that involve additional, and potentially more than two modalities. Another important problem is cross-modal inconsistency in social media content, as shown in Figure 2(c), which poses a challenge in a multimodal setting (Tan et al., 2020). Datasets. One of the major challenges when working with such diverse modalities, i.e., text, image, speech, video, and network, is to get access to an appropriate dataset, and moreover to one that considers both factuality and harmfulness. Furthermore, there is a need to integrate data from multiple platforms (e.g., news, posts from Twitter, Reddit and Instagram) as different data sources present different styles and focus on different topics.", "filtered_refids": [[null], ["b65", "b14", null, "b1", "b2", "b37", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1877, "num_references": 8}
{"corpusid_sectionid": "232320384-s18", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "A Examples of Factuality and Harmful Content", "section": "In Figure 2, we provide examples textual and visual content that are harmful and false, true image with false claim, and harmful meme.\n\nB Modeling Techniques Figure 4 shows various multimodal approaches that have been proposed in the literature.\n\nC Lessons Learned 1. A lot of progress has been made on the problem, but the two components in the definition of disinformation (falseness and harmfulness) have been considered mostly in isolation. We argue that there is a need for tight integration of the factuality and the intentional harmfulness into the same detection model. These two aspects have been addressed together in (Alam et al., 2021), which shows that 56% of Arabic false content is also harmful. From Tables 1 and 2, we observe that most multimodal datasets cover just 2-3 modalities, which combine some approaches depicted in Figure 4. Moreover, no multimodal dataset looks at both aspects of disinformation: factuality and harmfulness. While Alam et al.\n\n(2021) did address both aspects, they only covered the text modality.\n\n2. In the early phase of (dis)information spreading, user and content features are those that provide the highest contribution for detecting factuality. Indeed, at that time, a few interactions with content are available and the propagation network is small and sparse. As information spreads, the contribution of content-derived features remains constant, while propagation-derived features become richer and more informative. In summary, early prediction of factuality and veracity must necessarily rely heavily on users and content -be it text, image, audio or video. Instead, analyses carried out at later times benefit more from network and temporal data. In the past decade, research on multimodality has shown its potential in several fields, which include audio-visual fusion (Mroueh et al., Figure 3: Example of social network with users. Node: A node can be a users or a spreader. Ego: \"Ego\" is an individual \"focal\" node (central user) and the nodes that are directly connected to it are called \"alters/spreaders.\" Triad: It (a set of three connected users) is the most basic subgraph of the network. Community: A community structure refers to the occurrence of groups of nodes in a network that are more densely connected internally than with the rest of the network.  (Liu et al., 2021), multimedia retrieval and visual question answering (Summaira et al., 2021). For factuality, Baly et al. (2020) showed that combining different modalities such as text, speech, and metadata yields improved performance compared to using individual modalities. Similar phenomena have been observed for other tasks such as hateful memes (Kiela et al., 2020), and propaganda detection (Dimitrov et al., 2021b).\n\nD More Challenges 1. Contextualization. Existing methods of disinformation detection are mostly noncontextualized, i.e., the broader context of a news article in terms of the responses of the readers and how the users perceive them are not captured. We argue that the response", "filtered_refids": [[], [], ["b51"], [], ["b64", null, "b38", "b2"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3027, "num_references": 5}
{"corpusid_sectionid": "9840922-s2", "title": "Word Meaning Inducing via Character Ontology: A Survey on the Semantic Prediction of Chinese Two-Character Words", "date": 2005, "section_title": "Morpho-Semantic Description", "section": "As known, \"bound roots\" are the largest classes of morpheme types in Chinese morphology, and they are very productive and represent lexical rather than grammatical information (Packard 2000). This morphological phenomena leads many Chinese linguists to view the word components (i.e., characters) as building blocks in the semantic composition process of dis-or multisyllabic words. In many empirical studies (Tseng and Chen (2002); Tseng (2003); Lua (1993); Chen (2004)), this view has been confirmed repeatedly.\n\nIn the semantic studies of Chinese word formation, many descriptive and cognitive semantic approaches have been proposed, such as argument structure analysis (Chang 1998) and the frame-based semantic analysis (Chu 2004). However, among these qualitative explanation theoretical models, problems often appear in the lack of predictability on the one end of spectrum, or overgeneration on the other. 1 Empirical data have also shown that in many cases, -e.g., the abundance of phrasal lexical units in any natural language, -the principle of compositionality in a strict sense, that is, \"the meaning of a complex expression can be fully derivable from the meanings of its component parts, and from the schemas which sanction their combination\"(Taylor 2002), which is taken to be a fundamental proposition in some of morpho-semantically motivated analysis, is highly questionable.\n\nThis has given to the consideration of the embeddedness of linguistic meanings within broader conceptual structures. In what follows, we will argue that an ontology-based approach would provide an interesting and efficient prospective toward the character-triggered morpho-semantic analysis of Chinese words.", "filtered_refids": [["b12", null, "b8", "b2"], [null, "b3"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1702, "num_references": 6}
{"corpusid_sectionid": "9840922-s4", "title": "Word Meaning Inducing via Character Ontology: A Survey on the Semantic Prediction of Chinese Two-Character Words", "date": 2005, "section_title": "A Shift Toward Character Ontology", "section": "In prior studies, it is widely presumed that the category (be it syntactical or semantic) of a word, is somehow strongly associated with that of its composing characters. The semantic compositionality underlying two-character words appears in different terms in the literature. 2 Word semantic similarity calculation techniques have been commonly used to retrieve the similar compositional patterns based on semantic taxonomic thesaurus. However, one weak point in these studies is that they are unable to separate conceptual and semantic levels. Problem raises when words in question are conceptually correlated are not necessarily semantically correlated, viz, they might or might not be physically close in the CILIN thesaurus (Mei et al 1998). On closer observations, we found that most synonymic words (i.e., with the same CILIN semantic class) have characters which carry similar conceptual information. This could be best illustrated by examples. Table 1 shows the conceptual distribution of the modifiers of an example of VV compound by presuming the second character \u00a6 as a ceptable to native speakers. 2 Using statistical techniques, Lua (1993) found out that each Chinese two-character word is a result of 16 types of semantic transformation patterns, which are extracted from the meanings of its constituent characters. In Chen (2004), the combination pattern is referred to as compounding semantic template.\n\nhead. The first column is the semantic class of CILIN (middle level), the second column lists the instances with lower level classification number, and the third column lists their conceptual types adopted from a character ontology we will discuss later. As we can see, though there are 12 resulting semantic classes for the * \u00a6 compounds, the modifier components of these compounds involve only 4 concept types as follows:\n\nWe defined these patterns as conceptual aggregate pattern in compounding. Unlike statistical measure of the co-occurrence restrictions or association strength, a concept aggregate pattern provides a more knowledge-rich scenario to represent a specific manner in which concepts are aggregated in the ontological background, and how they affect the compounding words. We will propose that the semantic class prediction of Chinese two-character words could be improved by making use of their conceptual aggregate pattern of head/modifier component.", "filtered_refids": [["b8", "b13", "b2"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2392, "num_references": 3}
{"corpusid_sectionid": "9840922-s5", "title": "Word Meaning Inducing via Character Ontology: A Survey on the Semantic Prediction of Chinese Two-Character Words", "date": 2005, "section_title": "Semantic Prediction of Unknown Two-Character Words", "section": "The practical task intended to be experimented here involves the automatic classification of Chinese two-character words into a predetermined number of semantic classes. Difficulties encountered in previous researches could be summarized as follows: First, many models (Chen and Chen 1998;2000) cannot deal with the issue of \"incompleteness\" of characters in the lexicon, for these models depend heavily on CILIN, a Chinese Thesaurus containing only about 4,133 monosyllabic morphemic components (characters). As a result, if unknown words contain characters that are not listed in CILIN, then the prediction task cannot be performed automatically. Second, the ambiguity of characters is often shunned by  manual pre-selection of character meaning in the training step, which causes great difficulty for an automatic work. Third, it has long been assumed (Lua 1997;Chen and Chen 2000) that the overwhelming majority of Chinese compounds are more or less endocentric, where the compounds denote a hyponym of the head component in the compound. E.g, \u00das (\"electric-mail\"; e-mail) is a kind of mail. So the process of identifying semantic class of a compound boils down to find and to determine the semantic class of its head morpheme. However, there is also an amount of exocentric and appositional compounds 3 where no straightforward criteria can be made to determine the head component. For example, in a case of VV compound o\u00bd (\"denounce-scold\", drop-on), it is difficult (and subjective) to say which character is the head that can assign a semantic class to the compound. To solve above-mentioned problems, Chen (2004) proposed a non head-oriented charactersense association model to retrieve the latent senses of characters and the latent synonymous compounds among characters by measuring similarity of semantic template in compounding by using a MRD. However, as the author remarked in the final discussion of classification errors, the performance of this model relies much on the productivity of compounding semantic templates of the target compounds. To correctly predict the semantic category of a compound with an unproductive semantic template is no doubt very difficult due to a sparse existence of the template- 3 Lua reports a result of 14.14% (Z3 type). similar compounds. In addition, the statistical measure of sense association does not tell us any more about the constraints and knowledge of conceptual combination.\n\nIn the following, we will propose that a knowledge resource at the morpheme (character) level could be a straightforward remedy to these problems. By treating characters as instances of conceptual primitives, a character ontology thereof might provide an interpretation of conceptual grounding of word senses. At a coarse grain, the character ontological model does have advantages in efficiently defining the conceptual space within which character-grounded concept primitives and their relations, are implicitly located.", "filtered_refids": [["b12", null, "b9", "b1"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2959, "num_references": 4}
{"corpusid_sectionid": "9840922-s10", "title": "Word Meaning Inducing via Character Ontology: A Survey on the Semantic Prediction of Chinese Two-Character Words", "date": 2005, "section_title": "Circumventing \"Head-oriented\" Presupposition", "section": "As remarked in Chen (2004), the previous research concerning the automatic semantic classification of Chinese compounds (Lua 1997;Chen and Chen 2000) presupposes the endocentric feature of compounds. That is, by supposing that compounds are composed of a head and a modifier, determining the semantic category of the target therefore boils down to determine the semantic category of the head compound. In order to circumventing the strict \"headdetermination\" presumption, which might suffer problems in some borderline cases of V-V compounds, the weight value (\u03b2 and 1 \u2212 \u03b2) is proposed. The idea of weighting comes from the discussion of morphological productivity in Baayen (2001). We presume that, within a given two-character words, the more productive, that is, the more numbers of characters a character can combine with, the more possible it is a head, and the more weight should be given to it. The weight is defined as \u03b2 = C(n,1) N , viz, the number of candidate morphemic components divided by the total number of N. For instance, in the above-mentioned example, N S 1 should gain more weights than N S 2 , for\u02c6can combine with more characters (5 near-synonyms candidates) in\n\nN S 1 than \\ does in N S 2 (3 near-synonyms candidates). In this case, \u03b2 = 5 8 = 0.625. It is noted that the weight assignment should be character and position independent.", "filtered_refids": [["b1", "b9", "b2", "b0"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1358, "num_references": 4}
{"corpusid_sectionid": "229376920-s6", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "How to Define Style?", "section": "Linguistic Definition of Style. An intuitive notion of style refers to the manner in which the semantics is expressed (McDonald and Pustejovsky 1985). Just as everyone has their own signatures, style originates as the characteristics inherent to every person's utterance, which can be expressed through the use of certain stylistic devices such as metaphors, as well as choice of words, syntactic structures, and so on. Style can also go beyond the sentence level to the discourse level, such as the stylistic structure of the entire piece of the work, for example, stream of consciousness, or flashbacks. Beyond the intrinsic personal styles, for pragmatic uses, style further becomes a protocol to regularize the manner of communication. For example, for academic writing, the protocol requires formality and professionalism. Hovy (1987) defines style by its pragmatic aspects, including both personal (e.g., personality, gender) and interpersonal (e.g., humor, romance) aspects. Most existing literature also takes these well-defined categories of styles.\n\nData-Driven Definition of Style as the Scope of this Survey. This survey aims to provide an overview of existing neural TST approaches. To be concise, we will limit the scope to the most common settings of existing literature. Specifically, most deep learning work on TST adopts a data-driven definition of style, and the scope of this survey covers the styles in currently available TST datasets. The data-driven definition of style is different from the linguistic or rule-based definition of style, which theoretically constrains what constitutes a style and what not, such as a style guide (e.g., American Psychological Association 2020) that requires that formal text not include any contraction, e.g., \"isn't.\" The distinction of the two defintions of style is shown in Figure 1.\n\nWith the rise of deep learning methods of TST, the data-driven definition of style extends the linguistic style to a broader concept-the general attributes in text. It regards \"style\" as the attributes that vary across datasets, as opposed to the characteristics that stay invariant (Mou and Vechtomova 2020). The reason is that deep learning models (which are the focus of this survey) need large corpora to learn the style from, but not all styles have well-matched large corpora. Therefore, apart from the very few manually annotated datasets with linguistic style definitions, such as formality (Rao and Tetreault 2018) and humor & romance (Gan et al. 2017), many recent dataset collection works automatically look for meta-information to link a corpus to a certain attribute. A typical example is the widely used Yelp review dataset (Shen et al. 2017), where reviews with low ratings are put into the negative corpus, and reviews with high ratings are put into the positive corpus, although the negative vs. positive opinion is not a style that belongs to the linguistic definition, but more of a content-related attribute.\n\nMost methods mentioned in this survey can be applied to scenarios that follow this data-driven definition of style. As a double-edged sword, the prerequisite for most methods is that there exist style-specific corpora for each style of interest, either parallel or non-parallel. Note that there can be future works that do not take such an assumption, which will be discussed in Section 6.3.\n\nComparison of the Two Definitions. There are two phenomena rising from the data-driven definition of style as opposed to the linguistic style. One is that the data-driven definition of style can include a broader range of attributes including content and topic preferences of the text. The other is that data-driven styles, if collected through automatic classification by meta-information such as ratings, user information, and source of text, can be more ambiguous than the linguistically defined styles. As shown in Jin et al. (2019, Section 4.1.1), some automatically collected datasets have a concerningly high undecideable rate and inter-annotator disagreement rate when the annotators are asked to associate the dataset with human-defined styles such as political slant and gender-specific tones.\n\nThe advantage of the data-driven style is that it can marry well with deep learning methods because most neural models learn the concept of style by learning to distinguish the multiple style corpora. For the (non-data-driven) linguistic style, although it is under-explored in the existing deep learning works of TST, we provide in Section 6.3 a discussion of how potential future works can learn TST of linguistics styles with no matched data.", "filtered_refids": [["b59", "b119"], [], ["b174", "b123", "b148", "b32"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 29, "num_chars": 4620, "num_references": 6}
{"corpusid_sectionid": "229376920-s8", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Existing Subtasks with Datasets", "section": "We list the common subtasks and corresponding datasets for neural TST in Table 3. The attributes of interest vary from style features (e.g., formality and politeness) to content preferences (e.g., sentiment and topics). Each task of which will be elaborated below.\n\nFormality. Adjusting the extent of formality in text was first proposed by Hovy (1987). It is one of the most distinctive stylistic aspects that can be observed through many linguistic phenomena, such as more full names (e.g., \"television\") instead of abbreviations (e.g., \"TV\"), and more nouns (e.g., \"solicitation\") instead of verbs (e.g., \"request\"). The formality dataset, Grammarly's Yahoo Answers Formality Corpus (GYAFC) (Rao and Tetreault 2018), contains 50K formal-informal pairs retrieved by first getting 50K informal sentences from the Yahoo Answers corpus, and then recruiting crowdsource workers to rewrite them in a formal way. Briakou et al. (2021b) extend the formality dataset to a multilingual version with three more languages, Brazilian Portuguese, French, and Italian.\n\nPoliteness. Politeness transfer (Madaan et al. 2020) aims to control the politeness in text. For example, \"Could you please send me the data?\" is a more polite expression than \"send me the data!\". Madaan et al. (2020) compiled a dataset of 1.39 million automatically labeled instances from the raw Enron corpus (Shetty and Adibi 2004). As politeness is culture-dependent, this dataset mainly focuses on politeness in North American English. Table 3 List of common subtasks of TST and their corresponding attribute values and datasets. For datasets with multiple attribute-specific corpora, we report their sizes by the number of sentences of the smallest of all corpora. We also report whether the dataset is parallel (Pa?). ", "filtered_refids": [[], ["b59", "b148", "b6"], ["b112", "b175"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1783, "num_references": 5}
{"corpusid_sectionid": "229376920-s9", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "162", "section": "Gender. Linguistic phenomena related to gender is a heated research area (Trudgill 1972;Lakoff 1973;Tannen 1990;Argamon et al. 2003;Boulis and Ostendorf 2005). The gender-related TST dataset is proposed by Prabhumoye et al. (2018), who compiled 2.5M reviews from Yelp Dataset Challenge that are labeled with the gender of the user.\n\nHumor & Romance. Humor and romance are some artistic attributes that can provide readers with joy.  first propose to borrow the FlickrStyle stylized caption dataset (Gan et al. 2017) from the computer vision domain. In the FlickrStyle image caption dataset, each image has three captions, with a factual, a humorous, and a romantic style, respectively. By keeping only the captions of the three styles,  created a subset of the FlickrStyle dataset of 5K parallel (factual, humorous, romantic) triplets.\n\nBiasedness. Wiki Neutrality Corpus (Pryzant et al. 2020) is the first corpus of biased and neutralized sentence pairs. It is collected from Wikipedia revisions that adjusted the tone of existing sentences to a more neutral voice. The types of bias in the biased corpus include framing bias, epistemological bias, and demographic bias.\n\nToxicity. Another important use of TST is to fight against offensive language. Tran, Zhang, and Soleymani (2020) collect 350K offensive sentences and 7M non-offensive sentences by crawling sentences from Reddit using a list of restricted words.\n\nAuthorship. Changing the tone of the author is an artistic use of TST. Xu et al. (2012) created an aligned corpus of 18K pairs of Shakespearean English and their modern English translation. Carlson, Riddell, and Rockmore (2018) collected 28M parallel data from English versions of the Bible by different translators.\n\nSimplicity. Another important use of TST is to lower the language barrier for readers, such as translating legalese, medical jargon, or other professional text into simple English, to avoid discrepancies between expert wordings and lay understanding (Tan and Goonawardene 2017). Common tasks include converting standard English Wikipedia into Simple Wikipedia, whose dataset contains 108K samples (Zhu, Bernhard, and Gurevych 2010). Another task is to simplify medical descriptions to patient-friendly text, including a dataset with 2.2K samples (den Bercken, Sips, and Lofi 2019), another non-parallel dataset with 59K free-text discharge summaries compiled from MIMIC-III (Weng, Chung, and Szolovits 2019), and a more recent parallel dataset with 114K samples compiled from the health reference Merck Manuals (MSD), where discussions on each medical topic has one version for professionals, and the other for consumers .\n\nSentiment. Sentiment modification is the most popular task in previous work on TST. It aims to change the sentiment polarity in reviews, for example, from a negative review to a positive review, or vice versa. There is also work on transferring sentiments on finegrained review ratings (e.g., 1-5 scores). Commonly used datasets include Yelp reviews (Shen et al. 2017) and Amazon product reviews (He and McAuley 2016).\n\nTopic. There are a few works that cover topic transfer. For example, Huang et al. (2020) form a two-topic corpus by compiling Yahoo! Answers under two topics, entertainment and politics, respectively. There is also a recent dataset with 21 text styles such as Sciences, Sport, Politics, and others (Zeng, Shoeybi, and Liu 2020 Combined Attributes. Lample et al. (2019) propose a more challenging setting of text attribute transfer: multi-attribute transfer. For example, the source sentence can be a positive review on an Asian restaurant written by a male reviewer, and the target sentence is a negative review on an American restaurant written by a female. Each of their datasets has 1-3 independent categories of attributes. Their first dataset is FYelp, which is compiled from the Yelp Dataset Challenge, labeled with sentiment (positive or negative), gender (male or female), and eatery category (American, Asian, Mexican, bar, or dessert). Their second dataset, Amazon, which is based on the Amazon product review dataset , contains the following attributes: sentiment (positive or negative), and product category (book, clothing, electronics, movies, or music). Their third dataset, Social Media Content dataset, collected from internal Facebook data that is private, contains gender (male or female), age group (18-24 or 65+), and writerannotated feeling (relaxed or annoyed).", "filtered_refids": [["b141", "b91", null, "b191", "b1", "b186"], ["b32"], ["b142"], [], ["b13", "b213"], ["b185", "b201", "b230"], ["b174", "b52"], ["b64", "b94", "b219"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 4465, "num_references": 18}
{"corpusid_sectionid": "229376920-s14", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Automatic Evaluation", "section": "Automatic evaluation provides an economic, reproducible, and scalable way to assess the quality of generation results. However, due to the complexities of natural language, each metric introduced below can address certain aspects, but also has intrinsic blind spots.\n\nBLEU with Gold References. Similar to many text generation tasks, TST also has humanwritten references on several datasets (Yelp, Captions, etc.), so it is common to use the BLEU score (Papineni et al. 2002) between the gold references and model outputs. Using BLEU to evaluate TST models has been seen across pre-deep learning works (Xu et al. 2012;Jhamtani et al. 2017) and deep learning approaches (Rao and Tetreault 2018;Jin et al. 2019).\n\nThere are three problems with using BLEU between the gold references and model outputs:\n\nProblem 1. It mainly evaluates content and simply copying the input can result in high BLEU scores. Problem 2. BLEU is shown to have low correlation with human evaluation. Problem 3. Some datasets do not have human-written references.\n\nProblem 1: Different from machine translation, where using BLEU only is sufficient, TST has to consider the caveat that simply copying the input sentence can achieve high BLEU scores with the gold references on many datasets (e.g., \u223c40 on Yelp, \u223c20 on Humor & Romance, \u223c50 for informal-to-formal style transfer, and \u223c30 for formal-toinformal style transfer). This is because most text rewrites have a large extent of n-gram overlap with the source sentence. In contrast, machine translation does not have this concern, because the vocabulary of its input and output are different, and copying the input sequence does not give high BLEU scores. A possible fix to consider is to combine BLEU with PINC (Chen and Dolan 2011) as in paraphrasing (Xu et al. 2012;Jhamtani et al. 2017). By using PINC and BLEU as a 2-dimensional metric, we can minimize the n-gram overlap with the source sentence but maximize the n-gram overlap with the reference sentences.\n\nProblems 2 & 3: Other problems include insufficient correlation of BLEU with human evaluations (e.g., \u22640.30 with respect to human-rated grammaticality shown in  and \u22640.45 with respect to human evaluations shown in Mir et al. [2019]), and the unavailability of human-written references for some datasets (e.g., gender and political datasets [Prabhumoye et al. 2018], and the politeness dataset [Madaan et al. 2020]). A commonly used fix is to make the evaluation more fine-grained using three different independent aspects, namely, transferred style strength, semantic preservation, and fluency, which will be detailed below.\n\nTransferred Style Strength. To automatically evaluate the transferred style strength, most works separately train a style classifier to distinguish the attributes (Hu et al. 2017;Shen et al. 2017;Fu et al. 2018;Prabhumoye et al. 2018 .  shows that the attribute classifier correlates well with human evaluation on some datasets (e.g., Yelp and Captions), but has almost no correlation with others (e.g., Amazon). The reason is that some product genres has a dominant number of positive or negative reviews.\n\nSemantic Preservation. Many metrics can be applied to measure the similarity between the input and output sentence pairs, including BLEU (Papineni et al. 2002), ROUGE (Lin and Och 2004), METEOR (Banerjee and Lavie 2005), chrF (Popovi\u0107 2015), and Word Mover Distance (WMD) (Kusner et al. 2015). Recently, some additional deep-learningbased metrics have been proposed, such as cosine similarity based on sentence embeddings (Fu et al. 2018), and BERTScore ). There are also evaluation metrics that are specific for TST such as the Part-of-Speech distance (Tian, Hu, and Yu 2018). Another newly proposed metric is to first delete all attribute-related expressions in the text, and then apply the above similarity evaluations (Mir et al. 2019). Among all the metrics, Mir et al. (2019) and Yamshchikov et al. (2021) showed that METEOR and WMD have better correlation with human evaluation than BLEU, although, in practice, BLEU is the most widely used metric to evaluate the semantic similarity between the source sentence and style-transferred output Madaan et al. 2020).\n\nFluency. Fluency is a basic requirement for natural language outputs. To automate this evaluation, perplexity is calculated via a language model (LM) pretrained on the training data of all attributes ). However, the effectiveness of perplexity remains debatable, as Pang and Gimpel (2019) showed its high correlation with human ratings of fluency, whereas Mir et al. (2019) suggested no significant correlation between perplexity and human scores. We note that perplexity by LM can suffer from the following undesired properties:", "filtered_refids": [[], ["b134", "b213", "b69", "b148", "b73"], [], [], ["b69", "b213"], ["b112", "b122", "b141"], ["b141", "b174", "b62", "b31"], ["b89", "b104", "b134", "b31", "b187", "b122", "b112", "b214", "b138"], ["b133", "b122"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4723, "num_references": 25}
{"corpusid_sectionid": "229376920-s19", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "5.", "section": "LMs do not necessarily handle well the domain shift between their training corpus and the style-transferred text.\n\n6. Perplexity scores produced by LMs are sensitive to the training corpora, LM architecture and configuration, as well as optimization configuration. Therefore, different models' outputs must be evaluated by exactly the same LM for fair comparison, which adds more difficulty to benchmarking.\n\nSuch properties will bias against certain models, which is not desired for an evaluation metric. As a potential remedy, future researchers can try grammaticality checker to score the generated text.\n\nTask-Specific Criteria. As TST can serve as a component for other downstream applications, some task-specific criteria are also proposed to evaluate the quality of generated text. For example, Reiter, Robertson, and Osman (2003) evaluated the effect of their tailored text on reducing smokers' intent to smoke through clinical trials. Jin et al. (2020a) applied TST to generate eye-catchy headlines so they have an attractive score, and future works in this direction can also test the click-through rates. Hu et al. (2017) evaluated how the generated text as augmented data can improve the downstream attribute classification accuracy.\n\nTips for Automatic Metrics. For the evaluation metrics that rely on the pretrained models, namely, the style classifier and LM, we need to beware of the following:\n\n1. The pretrained models for automatic evaluation should be separate from the proposed TST model.", "filtered_refids": [[], [], [], ["b62", "b151", "b71"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1509, "num_references": 3}
{"corpusid_sectionid": "229376920-s21", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "3.", "section": "The pretrained models are imperfect in the sense that they will favor toward a certain type of methods.\n\nFor the first point, it is important to not use the same style classifier or LM in the proposed TST approach, otherwise it can overfit or hack the metrics. For the second point, we need to understand what the false positives and false negatives of the generated outputs can be. An illustrative example is that if the style classifier only reports 80+% performance (e.g., on the gender dataset [Prabhumoye et al. 2018] and Amazon dataset ), even perfect style rewrites can only score 80+%, but maybe an imperfect model can score 90% because it can resemble the imperfect style classification model more and takes advantage of the false positives. Other reasons for false positives can be adversarial attacks. Jin et al. (2020b) showed that merely paraphrasing using synonyms can drop the performance of high-accuracy classification models from TextCNN (Kim 2014) to BERT (Devlin et al. 2019) by 90+%. Therefore, higher scores by the style classifier do not necessarily indicate more successful transfer. Moreover, the style classifier can produce false negatives if there is a distribution shift between the training data and style-transferred outputs. For example, in the training corpus, a product may appear often with the positive attribute, and in the style-transferred outputs, this product co-occurs with the opposite, negative attribute. Such false negatives are observed on the Amazon product review dataset ). On the other hand, the biases of the LM correlate with sentence length, synonym replacement, and prior context.\n\nThe third point is a direct result implied by the second point, so in practice, we need to keep in mind and check whether the proposed model takes advantage of the evaluation metrics or makes improvements that are generalizable.", "filtered_refids": [[], ["b72", "b25", "b82", "b141"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1865, "num_references": 4}
{"corpusid_sectionid": "229376920-s22", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Human Evaluation", "section": "Compared with the pros and cons of the automatic evaluation metrics mentioned above, human evaluation stands out for its flexibility and comprehensiveness. For example, when asking humans to evaluate the fluency, we do not need to worry for the bias toward shorter sentences as in the LM. We can also design criteria that are not computationally easy such as comparing and ranking the outputs of multiple models. There are several ways to conduct human evaluation. In terms of evaluation types, there are pointwise scoring, namely, asking humans to provide absolute scores of the model outputs, and pairwise comparison, namely asking humans to judge which of the two outputs is better, or providing a ranking for multiple outputs. In terms of the criteria, humans can provide overall evaluation, or separate scores for transferred style strength, semantic preservation, and fluency.\n\nHowever, the well-known limitations of human evaluation are cost and irreproducibility. Performing human evaluations can be time consuming, which may result in significant time and financial costs. Moreover, the human evaluation results in two studies are often not directly comparable, because human evaluation results tend to be subjective and not easily irreproducible (Belz et al. 2020). Moreover, some styles are very difficult to evaluate without expertise and extensive reading experience.\n\nAs a remedy, we encourage future researchers to report inter-rater agreement scores such as the Cohen's kappa (Cohen 1960) and Krippendorff's alpha (Krippendorff 2018). Briakou et al. (2021a) also recommends standardizing and describing evaluation protocols (e.g., linguistic background of the annotators, compensation, detailed annotation instructions for each evaluation aspect), and releasing annotations.\n\nTips for Human Evaluation. As common practice, most works use 100 outputs for each style transfer direction (e.g., 100 outputs for formal\u2192informal, and 100 outputs for informal\u2192formal), and two human annotators for each task (Shen et al. 2017;Fu et al. 2018;).", "filtered_refids": [[], [null], ["b86", null, "b22"], ["b174", "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2052, "num_references": 6}
{"corpusid_sectionid": "229376920-s24", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Methods on Parallel Data", "section": "Over the last several years, various methods have been proposed for TST. In general, they can be categorized based on whether the dataset has parallel text with different styles or several non-parallel mono-style corpora. The rightmost column \"Pa?\" in Table 3 shows whether there exist parallel data for each TST subtask. In this section, we will cover TST methods on parallel datasets, and in Section 5 we will detail the approaches on non-parallel datasets. To ease the understanding for the reader, we will in most cases explain TST on one attribute between two values, such as transferring the formality between informal and formal tones, which can potentially be extended to multiple attributes.\n\nMost methods adopt the standard neural sequence-to-sequence (seq2seq) model with the encoder-decoder architecture, which was initially developed for neural machine translation (NMT) (Sutskever, Vinyals, and Le 2014;Bahdanau, Cho, and Bengio 2015;Cho et al. 2014) and extensively seen on text generation tasks such as summarization (Rush, Chopra, and Weston 2015) and many others (Song et al. 2019). The encoderdecoder seq2seq model can be implemented by either LSTM as in Rao and Tetreault (2018); and Shang et al. (2019) or Transformer (Vaswani et al. 2017) as in Xu, Ge, and Wei (2019). Copy mechanism (G\u00fcl\u00e7ehre et al. 2016;See, Liu, and Manning 2017) is also added to better handle stretches of text that should not be changed (e.g., some proper nouns and rare words) (Gu et al. 2016;Merity et al. 2017). Based on this architecture, recent work has developed multiple directions of improvement: multi-tasking, inference techniques, and data augmentation, which will be introduced below.\n\nMulti-tasking. In addition to the seq2seq learning on paired attributed-text, Xu, Ge, and Wei (2019) propose adding three other loss functions: (1) classifier-guided loss, which is calculated using a well-trained attribute classifier and encourages the model to generate sentences conforming to the target attribute, (2) self-reconstruction loss, which encourages the seq2seq model to reconstruct the input itself by specifying the desired style the same as the input style, and (3) cycle loss, which first transfers the input sentence to the target attribute and then transfers the output back to its original attribute. Each of the three losses can gain performance improvement of 1-5 BLEU points with the human references (Xu, Ge, and Wei 2019). Another type of multi-tasking is to jointly learn TST and machine translation from French to English, which improves the performance by 1 BLEU score with human-written references (Niu, Rao, and Carpuat 2018). Specifically for formality transfer, Zhang, Ge, and Sun (2020) multi-task TST and grammar error correction (GEC) so that knowledge from GEC data can be transferred to the informal-to-formal style transfer task.\n\nApart from the additional loss designs, using the pretrained language model GPT-2 (Radford et al. 2019) can lead to improvement by at least 7 BLEU points with human references .\n\nInference Techniques. To avoid the model copying too many parts of the input sentence and not performing sufficient edits to flip the attribute, Kajiwara (2019) first identifies words in the source sentence requiring replacement, and then changes the words by negative lexically constrained decoding (Post and Vilar 2018) that avoids naive copying. Because this method only changes the beam search process for model inference, it can be applied to any TST model without model re-training.\n\nData Augmentation. Because style transfer data is expensive to annotate, there are not as many parallel datasets as in machine translation. Hence, various methods have been proposed for data augmentation to enrich the data. For example, Rao and Tetreault (2018) first train a phrase-based machine translation (PBMT) model on a given parallel dataset and then use back-translation (Sennrich, Haddow, and Birch 2016b) to construct a pseudo-parallel dataset as additional training data, which leads to an improvement of around 9.7 BLEU points with respect to human written references.\n\nMost recently, Zhang, Ge, and Sun (2020) use a data augmentation technique by making use of largely available online text. They scrape informal text from online forums and generate back-translations, that is, informal English \u2192 a pivot language such as French \u2192 formal English, where the formality of the back-translated English text is ensured with a formality classifier that is used to only keep text that is classified as formal text.", "filtered_refids": [[], ["b183", "b178", "b4", "b212", "b161", "b46", "b44", "b121", "b21", null, "b194", "b148", "b166"], ["b129", "b212"], ["b146"], ["b139"], ["b148", "b168"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 4553, "num_references": 19}
{"corpusid_sectionid": "229376920-s26", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Disentanglement", "section": "Disentanglement-based models usually perform the following three actions:\n\n\u2022\n\nEncode the text x with attribute a into a latent representation z (i.e., x \u2192 z)\n\n\u2022 Manipulate the latent representation z to remove the source attribute (i.e., z \u2192 z )\n\n\u2022 Decode into text x with the target attribute a (i.e., z \u2192 x )\n\nTo build such models, the common workflow in disentanglement papers consists of the following three steps:\n\nStep 1. Select a model as the backbone for the encoder-decoder learning (Section 5.1.1).\n\nStep 2. Select a manipulation method of the latent representation (Section 5.1.2).\n\nStep 3. For the manipulation method chosen above, select (multiple) appropriate loss functions (Section 5.1.3).\n\nThe organization of this section starts with Section 5.1.1, which introduces the encoder-decoder training objectives that is used for Step 1. Next, Section 5.1.2 overviews three main approaches to manipulate the latent representation for Step 2, and Section 5.1.3 goes through a plethora of training objectives for Step 3. Table 5 provides an overview of existing models and their corresponding configurations. To give a rough idea of the effectiveness of each model, we show their performance on the Yelp dataset. Auto-Encoder (AE). Auto-encoding is a commonly used method to learn the latent representation z, which first encodes the input sentence x into a latent vector z and then reconstructs a sentence as similar to the input sentence as possible. AE is used in many TST works (e.g., Shen et al. 2017;Hu et al. 2017;Fu et al. 2018;Zhao et al. 2018;Prabhumoye et al. 2018;Yang et al. 2018). To avoid auto-encoding from blindly copying Table 5 Summary of existing disentanglement-based methods and the setting they adopted, with a reference of their performance on the Yelp dataset. For the settings, we include the encoder-decoder training method (Enc-Dec) in Section 5.1.1, the disentanglement method (Disen.) in Section 5.1.2, and the loss types used to control style (Style Control) and content (Content Control) in Section 5.1.3. For the model performance, we report automatic evaluation scores including BLEU with the one human reference (BL-Ref) provided by , accuracy (Acc.), BLEU with the input (BL-Inp), and perplexity (PPL). * marks numbers reported by Liu et al. (2020). Readers can refer to Hu, Lee, and Aggarwal (2020) (Vincent et al. 2010) to replace AE in NLP tasks. Specifically, DAE first passes the input sentence x through a noise model to randomly drop, shuffle, or mask some words, and then reconstructs the original sentence from this corrupted sentence. This idea is used in later TST works, for example, Lample et al. (2019) and Jin et al. (2020a). As pre-trained models became prevalent in recent years, the DAE training method has increased in popularity relative to its counterparts such as GAN and VAE, because pre-training over large corpora can grant models better performance in terms of semantic preservation and fluency (Lai, Toral, and Nissim 2021;Riley et al. 2021).\n\nVariational Auto-Encoder (VAE). Instead of reconstructing data based on the deterministic latent representations by AE, a variational auto-encoder (VAE) (Kingma and Welling 2014; Rezende, Mohamed, and Wierstra 2014) reconstructs data based on the sampled latent vector from its posterior, and uses the regularization by Kullback-Leibler divergence. VAE is also commonly used in TST works (Mueller, Gifford, and Jaakkola 2017;Hu et al. 2017;Liu et al. 2020;Liao et al. 2018;Yi et al. 2020;Tikhonov et al. 2019). The VAE loss is formulated as\n\nwhere \u03bb is the hyper-parameter to balance the reconstruction loss and the KL term, p(z) is the prior drawn from the standard normal distribution of N (0, I), and q E (z|x) is the posterior in the form of N (\u00b5, \u03c3), where \u00b5 and \u03c3 are predicted by the encoder.\n\nGenerative Adversarial Networks (GANs). GANs (Goodfellow et al. 2014) can also be applied to TST (Shen et al. 2017;Zhao et al. 2018;Li et al. 2020). The way GANs work is to first approximate the samples drawn from a true distribution z by using a noise sample s and a generator function G to produce z = G(s). Next, a critic/discriminator f c (z) is used to distinguish real data and generated samples. The critic is trained to distinguish the real samples from generated samples, and the generator is trained to fool the critic. Formally, the training process is expressed as a min-max game played among the encoder E, generator G, and the critic f c :\n\n5.1.2 Latent Representation Manipulation. Based on the general encoder and decoder training method, the core element of disentanglement is the manipulation of latent representation z. Figure 2 illustrates three main methods: latent representation editing, attribute code control, and latent representation splitting. In addition, the \"Disen.\" column of Table 5 shows the type of latent representation manipulation for each work in disentanglement. The first approach, Latent Representation Editing (LRE), shown in Figure 2a, is achieved by ensuring two properties of the latent representation z. The first property is that z should be able to serve as the latent representation for auto-encoding, namely, aligning f c (z) with the input x, where z \u2206 = E(x). The second property is that z should be learned such that it incorporates the new attribute value of interest a . To achieve this, the common practice is to first learn an attribute classifier f c , for example, a multilayer (c) Latent Representation Splitting", "filtered_refids": [[], [], [], [], [], [], [], [], [], ["b216", "b174", "b94", "b90", "b227", "b141", "b156", "b31", "b71", "b195", "b62", "b61", "b106"], ["b103", "b124", "b188", "b62", "b217", "b106"], [], ["b174", "b102", "b227", "b40"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 34, "num_chars": 5490, "num_references": 23}
{"corpusid_sectionid": "229376920-s29", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Style-Oriented Losses", "section": "To achieve Aim 1, many different style-oriented losses have been proposed, to nudge the model to learn a more clearly disentangled a and exclude the attribute information from z.\n\nAttribute Classifier on Outputs (ACO). ACO aims to make sentences generated by the generator G carry the target attribute a according to a pre-trained attribute classifier f c (Hu et al. 2017;Prabhumoye et al. 2018;Yamshchikov et al. 2019). The generator G takes as input the learned attribute vector a , which can be either an attribute code vector trained from scratch (as in the ACC approach) or the attribute representation disentangled from text (by the LRS approach). We denote the generation process to obtain the transferred sentence x as x \u2206 = G(E(x); a ). Correspondingly, ACO minimizes the following learning objective:\n\nIn training, ACO can be trained in two ways: either a normal loss function trained by Gumbel-softmax distribution to approximate the discrete training (Jang, Gu, and Poole 2017), or a negative reward for reinforcement learning by policy gradient training (Williams 1992) as in Luo et al. (2019).\n\nAttribute Classifier on Representations (ACR). Different from the previous ACO objective, whose training signal is from the output sentence x , ACR directly enforces the disentangled attribute representation a to be correctly classified by the attribute classifier, by the following objective (John et al. 2019;Romanov et al. 2019):\n\nAdversarial Learning on Representations (AdvR). As the previous ACR explicitly requires the latent a to be classified by f c , AdvR trains from another perspective-enforcing that no attribute-related information is contained in z Note that by combining ACR and AdvR, we can make attribute information captured fully and exclusively in a. To achieve AdvR, the encoder E is trained to generate the latent representation z \u2206 = E(x) so that z cannot be discriminated by the attribute classifier f c , which is expressed by the following learning objective:\n\nSince AdvR can be imbalanced if the number of samples of each attribute value differs largely, an extension of AdvR is to treat different attribute values with equal weight (Shen et al. 2017):\n\nNote that p(x) is the distribution of sentences of one attribute, and p(x ) is the distribution of sentences of the other attribute.\n\nAdversarial Learning on Outputs (AdvO). Apart from AdvR that adversarially learn the latent representations, we can also use AdvO to perform adversarial training on the outputs, to make them undistinguishable from the real data (Shen et al. 2017;Logeswaran, Lee, and Bengio 2018;Tian, Hu, and Yu 2018). Specifically, for each attribute a i , we train a classifier f (i) c to distinguish between true x i from the mono-style corpus of attribute a i , and the generated sentence x i \u2206 = G(E(x k ); a i ), where k = i, which aims to have the attribute a i . The loss function is\n\nIn the training process, usually we first optimize all attribute classifiers f (i) c , and then train the encoder, generator, and the attribute classifiers together by optimizing the sum the all AdvO training losses:\n\nNote that in order to propagate the gradients, it is feasible to use the sequence of hidden states in the generator instead of discrete text for G(E(x k ); a i ) (Shen et al. 2017).\n\nLanguage Modeling on Outputs (LMO). The above AdvO learns classifiers to distinguish between true samples and generated samples. Such discriminative classification can be alternatively achieved by generative language modeling, namely, LM i for each monostyle corpus with the attribute a i . Specifically, the training objective for each attribute is\n\nwhere \u03b3 is a hyperparameter to weight the two terms. The total training objective sums over the losses of all attributes:", "filtered_refids": [[], ["b215", "b62", "b141"], ["b203", "b110"], ["b74", "b158"], [], ["b174"], [], ["b174", "b109", "b187"], [], ["b174"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3775, "num_references": 12}
{"corpusid_sectionid": "229376920-s30", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Content-Oriented Losses", "section": "The style-oriented losses introduced above ensures the attribute information to be contained in a, but not necessarily putting constraints on the style-independent semantics z.\n\nTo learn the attribute-independent information fully and exclusively in z, the following content-oriented losses are proposed: , and then feeds z to the generator G to obtain the generated sentence G(z). Since the alignment of the input and the generated sentence is to preserve attribute-independent semantic information, the generator can be conditioned on any attribute, namely, a or a . The cycle loss constrains the output x to align with the input x (and, similarly, the output x to align with the input x ) so that the content information can be preserved:\n\nOne way to train the above cycle loss is by reinforcement learning as done by Luo et al. (2019), who use the loss function as a negative for content preservation. Let us denote the vocabulary set as V. We first predict the distribution of BoW features q BoW (z) of the latent representation z using softmax on the 1 \u00d7 |V| BoW features. We then calculate the cross entropy loss of this BoW distribution q BoW (z) against the ground-truth BoW distribution p BoW (x) in the input sentence x. The BoW loss is formulated as follows:\n\nAdversarial BoW Overlap (AdvBoW). BoW ensures the content to be fully captured in z. As a further step, we want to ensure that the content information is exclusively captured in z, namely, not contained in a at all, via the following AdvBow loss on a When disentangling z and a in the LRS framework, we train an adversarial classifier q BoW (a) to predict the BoW features given a by aligning it with the ground-truth BoW distribution p BoW (x), namely, minimizing\n\nThe final min-max objective is\n\nOther Losses/Rewards. There are also other losses/rewards in recent work such as the noun overlap loss (Noun) (Tian, Hu, and Yu 2018), as well as rewards for semantics and fluency (Xu et al. 2018;Gong et al. 2019;Sancheti et al. 2020). We do not discuss them in much detail because they do not directly operate on the disentanglement of latent representations.", "filtered_refids": [[], [], ["b110"], [], [], ["b163", "b209", "b187", "b39"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2130, "num_references": 5}
{"corpusid_sectionid": "229376920-s31", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Prototype Editing", "section": "Despite a plethora of models that use end-to-end training of neural networks, the prototype-based text editing approach still attracts lots of attention, since the proposal of a pipeline method called delete, retrieve, and generate ). Prototype editing is reminiscent of early word replacement methods used for TST, such as synonym matching using a style dictionary (Sheikha and Inkpen 2011), WordNet (Khosmood and Levinson 2010;Mansoorizadeh et al. 2016), hand-crafted rules (Khosmood and Levinson 2008;Castro, Ortega, and Mu\u00f1oz 2017), or using hypernyms and definitions to replace the style-carrying words (Karadzhov et al. 2017).\n\nFeaturing more controllability and interpretability, prototype editing builds an explicit pipeline for TST from x with attribute a to its counterpart x with attribute a :\n\nStep 1. Detect attribute markers of a in the input sentence x, and delete them, resulting in a content-only sentence (Section 5.2.1).\n\nStep 2. Retrieve candidate attribute markers carrying the desired attribute a (Section 5.2.2).\n\nStep 3. Infill the sentence by adding new attribute markers and make sure the generated sentence is fluent (Section 5.2.3).", "filtered_refids": [["b14", "b78", "b80", "b117", "b81"], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1160, "num_references": 5}
{"corpusid_sectionid": "229376920-s32", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Attribute Marker Detection.", "section": "Extracting attribute markers is a non-trivial NLP task. Traditional ways to do it involve first using tagging, parsing, and morphological analysis to select features, and then filtering by mutual information and chi-square testing. In recent deep learning pipelines, there are three major types of approaches to identify attribute markers: frequency-ratio methods, attention-based methods, and fusion methods. Frequency-ratio methods calculate some statistics for each n-gram in the corpora. For example,  detect the attribute markers by calculating its relative frequency of co-occurrence with attribute a versus a , and those with frequencies higher than a threshold are considered the markers of a. Using a similar approach, Madaan et al. (2020) first calculate the ratio of mean TF-IDF between the two attribute corpora for each n-gram, then normalize this ratio across all possible n-grams, and finally mark those n-grams with a normalized ratio p higher than a pre-set threshold as attribute markers.\n\nAttention-based methods train an attribute classifier using the attention mechanism (Bahdanau, Cho, and Bengio 2015), and consider words with attention weights higher than average as markers (Xu et al. 2018). For the architecture of the classifier, Zhang et al. (2018c) use LSTM, andSudhakar, Upadhyay, andMaheswaran (2019) use a BERT classifier, where the BERT classifier has shown higher detection accuracy for the attribute markers.\n\nFusion methods combine the advantages of the above two methods. For example,  prioritize the attribute markers predicted by frequency-ratio methods, and use attention-based methods as an auxiliary back up. One use case is when frequency-ratio methods fail to identify any attribute markers in a given sentence, they will use the attention-based methods as a secondary choice to generate attribute markers. Another case is to reduce false positives. To reduce the number of attribute markers that are wrongly recognized,  set a threshold to filter out low-quality attribute markers by frequency-ratio methods, and in cases where all attribute markers are deleted, they use the markers predicted by attention-based methods.\n\nThere are still remaining limitations of the previous methods, such as imperfect accuracy of the attribute classifier, and unclear relation between attribute and attention scores. Hence, Lee (2020) propose word importance scoring, similar to what is used by Jin et al. (2020b) for adversarial paraphrasing, to measure how important a token is to the attribute by the difference in the attribute probability of the original sentence and that after deleting a token.", "filtered_refids": [["b112"], [null, "b209", "b182", "b4"], [], ["b72", "b95"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2632, "num_references": 7}
{"corpusid_sectionid": "229376920-s33", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Target Attribute Retriever.", "section": "After deleting the attribute markers Marker a (x) of the sentence x with attribute a, we need to find a counterpart attribute marker Marker a (x ) from another sentence x carrying a different attribute a . Denote the sentence template with all attribute markers deleted as Template(x) \u2206 = x\\Marker a (x).\n\nSimilarly, the template of the sentence x is Template(x ) \u2206 = x \\Marker a (x ). A common approach is to find the counterpart attribute marker by its context, because the templates of the original attribute and its counter attribute marker should be similar. Specifically, we first match a template Template(x) with the most similar template Template(x ) in the opposite attribute corpus, and then identify the attribute markers Marker a (x) and Marker a (x ) as counterparts of each other. To match templates with their counterparts, most previous works find the nearest neighbors by the cosine similarity of sentence embeddings. Commonly used sentence embeddings include TF-IDF as used in  and Sudhakar, Upadhyay, and Maheswaran (2019), averaged GloVe embedding distance used in  and Sudhakar, Upadhyay, and Maheswaran (2019), and Universal Sentence Encoder (Cer et al. 2018) used in Sudhakar, Upadhyay, andMaheswaran (2019). Apart from sentence embeddings, Tran, Zhang, and Soleymani (2020) use Part-of-Speech templates to match several candidates in the opposite corpus, and conduct an exhaustive search to fill parts of the candidate sentences into the masked positions of the original attribute markers.  and Sudhakar, Upadhyay, and Maheswaran (2019) feed the content-only sentence template and new attribute markers into a pretrained language model that rearranges them into a natural sentence. This infilling process can naturally be achieved by a masked language model (MLM) (Malmi, Severyn, and Rothe 2020). For example,  use a MLM of the template conditioned on the target attribute, and this MLM is trained on an additional attribute classification loss using the model output and a fixed pre-trained attribute classifier. Because these generation practices are complicated, Madaan et al. (2020) propose a simpler way. They skip Step 2 that explicitly retrieves attribute candidates, and, instead, directly learn a generation model that only takes attribute-masked sentences as inputs. This generation model is trained on data where the attribute-carrying sentences x are paired with their templates Template(x). Training on the pairs of (Template(x), x) constructed in this way can make the model learn how to fill the masked sentence template with the target attribute a.", "filtered_refids": [[], ["b115", "b182", "b112", null, "b190"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2590, "num_references": 5}
{"corpusid_sectionid": "229376920-s35", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Pseudo-Parallel Corpus Construction", "section": "To provide more signals for training, it is also helpful to generate pseudo-parallel data for TST. Two major approaches are retrieval-based and generation-based methods.\n\nRetrieval-Based Corpora Construction. One common way to construct pseudo-parallel data is through retrieval, namely, extracting aligned sentence pairs from two monostyle corpora. Jin et al. (2019) empirically observe that semantically similar sentences in the two mono-style corpora tend to be the attribute-transferred counterparts of each other. Hence, they construct the initial pseudo corpora by matching sentence pairs in the two attributed corpora according to the cosine similarity of pretrained sentence embeddings. Formally, for each sentence x, its pseudo counterpart x is its most similar sentence in the other attribute corpus X , namely, x = argmax x \u2208X Similarity(x, x ). This approach is extended by Nikolov and Hahnloser (2019), who use large-scale hierarchical alignment to extract pseudo-parallel style transfer pairs. Such retrieval-based pseudoparallel data construction is also useful for machine translation (Munteanu and Marcu 2005;Uszkoreit et al. 2010;Marie and Fujita 2017;Gr\u00e9goire and Langlais 2018;Ren et al. 2020).\n\nGeneration-Based Corpora Construction. Another way is through generation, such as iterative back-translation (IBT) (Hoang et al. 2018). IBT is a widely used method in machine translation (Artetxe et al. 2018;Lample et al. 2018aLample et al. , 2018bDou, Anastasopoulos, and Neubig 2020) that adopts an iterative process to generate pseudoparallel corpora.\n\nBefore starting the iterative process, IBT needs to first initialize two style transfer models: M a\u2192a , which transfers from the attribute a to the other attribute a , and M a \u2192a , which transfers from a to a. Then, in each iteration, it executes the following steps:\n\nStep 1. Use the models to generate pseudo-parallel corpora. Specifically, M a\u2192a (x) generates pseudo pairs (x, x ) for all x \u2208 X, and M a \u2192a (x ) generates pairs of ( x, x ) for all x \u2208 X .\n\nStep 2. Re-train these two style transfer models on the datasets generated by 1, that is, re-train M a\u2192a (x) on ( x, x ) pairs and M a \u2192a (x ) on ( x , x) pairs.\n\nFor\n\nStep 1, in order to generate the initial pseudo-parallel corpora, a simple baseline is to randomly initialize the two models M a\u2192a and M a \u2192a , and use them to translate the attribute of each sentence in x \u2208 X and x \u2208 X . However, this simple initialization is subject to randomness and may not bootstrap well. Another way, adopted by Zhang et al. (2018d), borrows the idea from unsupervised machine translation (Lample et al. 2018a) that first learns an unsupervised word-to-word translation table between attribute a and a , and uses it to generate an initial pseudo-parallel corpora. Based on such initial corpora, they train initial style transfer models and bootstrap the IBT process. Another model, Iterative Matching and Translation (IMaT) (Jin et al. 2019), does not learn the word translation table, and instead trains the initial style transfer models on a retrieval-based pseudo-parallel corpora introduced in the retrieval-based corpora construction above.\n\nFor\n\nStep 2, during the iterative process, it is possible to encounter divergence, as there is no constraint to ensure that each iteration will produce better pseudoparallel corpora than the previous iteration. One way to enhance the convergence of IBT is to add additional losses. For example, Zhang et al. (2018d) use the attribute classification loss ACO, as in Equation (3), to check whether the generated sentence by back-translation fits the desired attribute according to a pre-trained style classifier. Alternatively, IMaT (Jin et al. 2019) uses a checking mechanism instead of additional losses. At the end of each iteration, IMaT looks at all candidate pseudo-pairs of an original sentence, and uses WMD (Kusner et al. 2015) to select the sentence that has the desired attribute and is the closest to the original sentence.", "filtered_refids": [[], ["b73", "b118", "b153", "b126", "b193", "b42", "b125"], ["b92", "b56", "b27", "b93", "b2"], [], [], [], [], ["b92", "b73", "b226"], [], ["b89", "b73", "b226"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 4003, "num_references": 18}
{"corpusid_sectionid": "229376920-s37", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Expanding the Scope of Styles", "section": "More Styles. Extending the list of styles for TST is one popular research direction. Existing research originally focused on styles such as simplification (Zhu, Bernhard, and Gurevych 2010), formality (Sheikha and Inkpen 2011), and sentiment transfer (Shen et al. 2017), while the recent two years have seen a richer set of styles such as politeness (Madaan et al. 2020), biasedness (Pryzant et al. 2020), medical text simplification , and so on.\n\nSuch extension of styles is driven by the advancement of TST methods, and also various downstream needs, such as persona-based dialog generation, customized text rewriting applications, and moderation of online text. Apart from the styles that have been researched as listed in Table 3, there are also many other new styles that can be interesting to conduct new research on, including but not limited to the following:\n\n\u2022 Factual-to-empathetic transfer, to improve counseling dialogs (after the first version of this survey in 2020, we gladly found that this direction has now a preliminary exploration by Sharma et al. [2021])\n\n\u2022 Non-native-to-native transfer (i.e., reformulating grammatical error correction with TST)\n\n\u2022 Sentence disambiguation, to resolve nuance in text\n\nMore Difficult Forms of Style. Another direction is to explore more complicated forms of styles. As covered by this survey, the early work on deep learning-based TST explores relatively simple styles, such as verb tenses (Hu et al. 2017) and positive-vs.-negative Yelp reviews (Shen et al. 2017). In these tasks, each data point is one sentence with a clear, categorized style, and the entire dataset is in the same domain. Moreover, the existing datasets can decouple style and style-independent contents relatively well. We propose that TST can potentially be extended into the following settings:\n\n\u2022 Aspect-based style transfer (e.g., transferring the sentiment on one aspect but not the other aspects on aspect-based sentiment analysis data)\n\n\u2022 Authorship transfer (which has tightly coupled style and content)\n\n\u2022 Document-level style transfer (which includes discourse planning)", "filtered_refids": [["b174", "b173", "b112", "b142", "b230"], [], ["b172"], [], [], ["b174", "b62"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 2108, "num_references": 8}
{"corpusid_sectionid": "229376920-s41", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Method Strengths & Weaknesses", "section": "Disentanglement + More profound in theoretical analysis, e.g., disentangled representation learning \u2212 Difficulties of training deep generative models (VAEs, GANs) for text \u2212 Hard to represent all styles as latent code \u2212 Computational cost rises with the number of styles to model Prototype Editing + High BLEU scores due to large word preservation \u2212 Attribute marker detection step can fail if the style and semantics are confounded \u2212 The step target attribute retrieval by templates can fail if there are large rewrites for styles, e.g., Shakespearean English vs. modern English \u2212 Target attribute retrieval step has large complexity (quadratic to the number of sentences) \u2212 Large computational cost if there are many styles, each of which needs a pre-trained LM for the generation step ? Future work can enable matchings for syntactic variation ? Future work can use grammatical error correction to post-edit the output Pseudo-Parallel Corpus Construction + Performance can approximate supervised model performance, if the pseudo-parallel data are of good quality \u2212 May fail for small corpora \u2212 May fail if the mono-style corpora do not have many samples with similar contents \u2212 For IBT, divergence is possible, and sometimes needs special designs to prevent it \u2212 ForIBT,time complexityishigh(duetoiterativepseudo data generation) ? Improve the convergence of the IBT Challenges for Disentanglement. Theoretically, although disentanglement is impossible without inductive biases or other forms of supervision (Locatello et al. 2019), disentanglement is achievable with some weak signals, such as only knowing how many factors have changed, but not which ones (Locatello et al. 2020).\n\nIn practice, some big challenges for disentanglement-based methods include, for example, the difficulty to train deep text generative models such as VAEs and GANs. Also, it is not easy to represent all styles as latent code. Moreover, if targeting multiple styles, the computational complexity linearly increases with the number of styles to model.\n\nChallenges for Prototype Editing. Prototype-editing approaches usually result in relatively high BLEU scores, partly because the output text largely overlaps with the input text. This line of methods is likely to perform well on tasks such as sentiment modification, for which it is easy to identify \"attribute markers,\" and the input and output sentences share an attribute-independent template.\n\nHowever, prototype editing cannot be applied to all types of style transfer tasks. The first step, attribute marker retrieval, might not work if the datasets have confounded style and contents, because they may lead to wrong extraction of attribute markers, such as some content words or artifacts which can also be used to distinguish the stylespecific data.\n\nThe second step, target attribute retrieval by templates, will fail if there is too little word overlap between a sentence and its counterpart carrying another style. An example is the TST task to \"Shakespearize\" modern English. There is little lexical overlap between a Shakespearean sentence written in early modern English and its corresponding modern English expression. In such cases, the retrieval step is likely to fail, because there is a large number of rewrites between the two styles, and the template might be almost hollow. Moreover, this step is also computationally expensive, if there are a large number of sentences in the data (e.g., all Wikipedia text), since this step needs to calculate the pair-wise similarity among all available sentences across style-specific corpora.\n\nThe third step, generation from prototype, requires a separate pretrained LM for each style corpus. When there are multiple styles of interest (e.g., multiple persona), this will induce a large computational cost.\n\nThe last limitation of prototype editing is that it amplifies the intrinsic problem of using BLEU to evaluate TST (Problem 1, namely, the fact that simply copying the input can result in a high BLEU score, as introduced in Section 3.1). For the retrieval-based method, some can argue that there is some performance gain because this method in practice copies more expressions in the input sentence than other lines of methods.\n\nAs future study, there can be many interesting directions to explore, for example, investigating the performance of existing prototype editing models under a challenging dataset that reveals the above shortcomings, proposing new models to improve this line of approaches, and better evaluation methods for prototype editing models.\n\nChallenges for Pseudo-Parallel Corpus Construction. The method to construct pseudoparallel data can be effective, especially when the pseudo-parallel corpora resemble supervised data. The challenge is that this approach may not work if the non-parallel corpora do not have enough samples that can be matched to create the pseudo-parallel corpora, or when the IBT cannot bootstrap well or fails to converge. The time complexity for training IBT is also very high because it needs to iteratively generate pseudo-parallel corpus and re-train models. Interesting future directions can be reducing the computational cost, designing more effective bootstrapping, and improving the convergence of IBT.\n\n6.2.2 Understanding the Evolution from Traditional NLG to Deep Learning Methods. Despite the exciting methodological revolution led by deep learning recently, we are also interested in the merging point of traditional computational linguistics and the deep learning techniques (Henderson 2020). Specific to the context of TST, we will introduce the traditional NLG framework, and its impact on the current TST approaches, especially the prototype editing method.\n\nTraditional NLG Framework. The traditional NLG framework stages sentence generation into the following steps (Reiter and Dale 1997 ", "filtered_refids": [["b107", "b108"], [], [], [], [], [], [], [], [], ["b53"], ["b150"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 35, "num_chars": 5858, "num_references": 4}
{"corpusid_sectionid": "229376920-s42", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "5.", "section": "Referring expression generation 6. Linguistic realization\n\nThe first two steps, content determination and discourse planning, are not applicable to most datasets because the current focus of TST is sentence-level and not discourse-level. Among Steps 3 to 6, sentence aggregation groups necessary information into a single sentence, lexicalization chooses the right word to express the concepts generated by sentence aggregation, referring expression generation produces surface linguistic forms for domain entities, and linguistic realization edits the text so that it conforms to grammar, including syntax, morphology, and orthography. This framework is widely applied to NLG tasks (e.g., Zue and Glass 2000;Mani 2001;McTear 2002;Gatt and Reiter 2009;Androutsopoulos and Malakasiotis 2010).\n\nRe-Viewing Prototype-Based TST. Among the approaches introduced so far, the most relevant for the traditional NLG is the prototype-based text editing, which has been introduced in Section 5.2.\n\nUsing the language of the traditional NLG framework, the prototype-based techniques can be viewed as a combination of sentence aggregation, lexicalization, and linguistic realization. Specifically, prototype-based techniques first prepare an attribute-free sentence template, and supply it with candidate attribute markers that carry the desired attribute, both of which are sentence aggregation. Then, using language models to infill the prototype with the correct expressions corresponds to lexicalization and linguistic realization. Note that the existing TST systems do not explicitly deal with referring expression generation (e.g., generating co-references), leaving it to be handled by language models.\n\nMeeting Point of Traditional and New Methods. Viewing prototype-based editing as a merging point where traditional, controllable framework meets deep learning models, we can see that it takes advantage of the powerful deep learning models and the interpretable pipeline of the traditional NLG. There are several advantages in merging the traditional NLG with the deep learning models. First, sentence planning-like steps make the generated contents more controllable. For example, the template of the original sentence is saved, and the counterpart attributes can also be explicitly retrieved, as a preparation for the final rewriting. Such a controllable, white-box approach can be easy to tune, debug, and improve. The accuracy of attribute marker extraction, for example, is constantly improving across literature (Sudhakar, Upadhyay, and Maheswaran 2019) and different ways to extract attribute markers can be easily fused .\n\nSecond, sentence planning-like steps ensure the truthfulness of information. As most content words are kept and no additional information is hallucinated by the black-box neural networks, we can better ensure that the information of the attribute-transferred output is consistent with the original input.\n\n6.2.3 Inspiration from Tasks with Similar Nature. An additional perspective that can inspire new methodological innovation is insights from other tasks that share a similar nature as TST. We will introduce in this section several closely related tasks, including machine translation, image style transfer, style-conditioned language modeling, counterfactual story rewriting, contrastive text generation, and prototype-based text editing.\n\nMachine Translation. The problem settings of machine translation and TST share much in common: The source and target language in machine translation is analogous to the original and desired attribute, a and a , respectively. The major difference is that in NMT, the source and target corpora are in completely different languages, which have almost disjoint word vocabulary, whereas in TST, the input and output are in the same language, and the model is usually encouraged to copy most content words from input such as the BoW loss introduced in Section 5.1.3.2. Some TST works have been inspired by MT, such as the pseudo-parallel construction (Nikolov and Hahnloser 2019;Zhang et al. 2018d), and in the future there may be more interesting intersections.\n\nData-to-Text Generation. Data-to-text generation is another potential domain that can draw inspiration from and to TST. The data-to-text generation task is to generate textual descriptions from structured data such as tables (Wiseman, Shieber, and Rush 2017;Parikh et al. 2020), meaning representations (Novikova, Dusek, and Rieser 2017), or Resource Description Framework triples (Gardent et al. 2017;Ferreira et al. 2020). With the recent rise of pretrained seq2seq models for transfer learning (Raffel et al. 2020), it is common to formulate data-to-text as a seq2seq task by serializing the structured data into a sequence (Kale and Rastogi 2020;Ribeiro et al. 2020;Guo et al. 2020). Then datato-text generation can be seen as a special form of TST from structured information to text. This potential connection has not yet been investigated but worth exploring.\n\nNeural Style Transfer. Neural style transfer first originates in image style transfer (Gatys, Ecker, and Bethge 2016), and its disentanglement ideas inspired some early TST research (Shen et al. 2017). The difference between image style transfer and TST is that, for images, it is feasible to disentangle the explicit representation of the image texture as the gram matrix of image neural feature vectors, but for text, styles do not have such an explicit representation, but more abstract attributes. Besides this difference, many other aspects of style transfer research can have shared nature. Note that there are style transfer works across different modalities, including images (Gatys, Ecker, and Bethge 2016;Zhu et al. 2017;Chen et al. 2017b), text, voice (Gao, Singh, and Raj 2018;Qian et al. 2019;Yuan et al. 2021), handwriting (Azadi et al. 2018;Zhang and Liu 2013), and videos (Ruder, Dosovitskiy, and Brox 2016;Chen et al. 2017a). Many new advances in one style transfer field can inspire another style transfer field. For example, image style transfer has been used as a way for data augmentation (Zheng et al. 2019;Jackson et al. 2019) and adversarial attack , but TST has not yet been applied for such usage.\n\nStyle-Conditioned Language Modeling. Different from language modeling that learns how to generate general natural language text, conditional language modeling learns how to generate text given a condition, such as some context, or a control code (Pfaff 1979;Poplack 2000). Recent advances of conditional language models (Keskar et al. 2019;Dathathri et al. 2020) also include text generation conditioned on a style token, such as positive or negative. Possible conditions include author style (Syed et al. 2020), speaker identity, persona and emotion , genre, and attributes derived from text, topics, and sentiment (Ficler and Goldberg 2017). They are currently limited to a small set of pre-defined \"condition\" tokens and can only generate from scratch a sentence, but are not yet able to be conditioned on an original sentence for style rewriting. The interesting finding in this research direction is that it can make good use of a pretrained LM and just do some light-weight inference techniques to generate style-conditioned text, so perhaps such approaches can inspire future TST methods and reduce the carbon footprints of training TST models from scratch.\n\nCounterfactual Story Rewriting. Counterfactual story rewriting aims to learn a new event sequence in the presence of a perturbation of a previous event (i.e., counterfactual condition) (Goodman 1947;Starr 2019). Qin et al. (2019) propose the first dataset, each sample of which takes an originally five-sentence story, and changes the event in the second sentence to a new, counterfactual event. The task is to generate the last three sentences of the story based on the newly altered second sentence that initiates the story. The criteria of the counterfactual story rewriting include relevance with the first two sentences, and minimal edits from the original story ending. This line of research is relatively difficult to directly apply to TST, because its motivation and dataset nature is different from the general TST, and more importantly, this task is not conditioned on a predefined categorized style token, but the free-form textual story beginning.\n\nContrastive Text Generation. As neural network-based NLP models more easily learn spurious statistical correlations in the data rather than achieve robust understanding (Jia and Liang 2017), there is recent work to construct auxillary datasets composed of nearmisses of the original data. For example, Gardner et al. (2020) ask crowdsource workers to rewrite the input of the task with minimal changes but matching a different target label. To alleviate expensive human labor, Xing et al. (2020) develop an automatic text editing approach to generate contrast set for aspect-based sentiment analysis. The difference between contrastive text generation and TST is that the former does not require content preservation but mainly aims to construct a slightly textually different input that can result in a change of the ground-truth output, to test the model robustness. So the two tasks are not completely the same, although they have some intersections that might inspire future work, such as aspect-based style transfer suggested in Section 6.1.\n\nPrototype-Based Text Editing. Prototype editing is not unique in TST, but also widely used in other NLP tasks. Knowing the new advances in prototype editing for other tasks can potentially inspire new method innovations in TST. Guu et al. (2018) first proposes the protype editing approach to improve LM by first sampling a lexically similar sentence prototype and then editing it using variational encoder and decoders. This prototypeand-then-edit approach can also be seen in summarization (Wang, Quan, and Wang 2019), machine translation (Cao and Xiong 2018;Wu, Wang, and Wang 2019;Gu et al. 2018;Zhang et al. 2018a;Bult\u00e9 and Tezcan 2019), conversation generation (Weston, Dinan, and Miller 2018;Cai et al. 2019), code generation , and question answering (Lewis et al. 2020). As an extension to the retrieve and edit steps, Hossain, Ghazvininejad, and Zettlemoyer (2020) use an ensemble approach to retrieve a set of relevant prototypes, edit, and finally rerank to pick the best output for machine translation. Such extension can also be potentially applied to TST.", "filtered_refids": [[], ["b120", "b116", "b231", "b0", "b36"], [], [], ["b182"], [], [], ["b126", "b226"], ["b135", "b34", "b204", "b130", "b147", "b28", "b47", null, "b76"], ["b174", "b143", "b18", "b20", null, "b229", "b223", "b228", "b159", "b33", "b218", "b37", "b3"], ["b136", "b184", "b29", "b79", "b137", "b24"], ["b145", "b41", "b181"], ["b70", "b208", "b35"], ["b205", "b10", "b57", "b197", "b9", "b96", "b49", "b45", "b202", "b220", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 57, "num_chars": 10469, "num_references": 53}
{"corpusid_sectionid": "229376920-s43", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Loosening the Style-Specific Dataset Assumptions", "section": "A common assumption for most deep learning-based TST works, as mentioned in Section 2.1, is the availability of style-specific corpora for each style of interest, either parallel or non-parallel. This assumption can potentially be loosened in two ways.\n\nLinguistic Styles with No Matched Data. Because there are various concerns raised by the data-driven definition of style as described in Section 2.1, a potentially good research direction is to bring back the linguistic definition of style, and thus remove some of the concerns associated with large datasets. Several methods can be a potential fit for this: prompt design (Li and Liang 2021;Qin and Eisner 2021;Scao and Rush 2021) that passes a prompt to GPT (Radford et al. 2019;Brown et al. 2020) to obtain a style-transferred text; style-specific template design; or use templates to first generate synthetic data and make models learn from the synthetic data. Prompt design is not yet investigated as a direction for TST research, but it is an interesting direction to explore.\n\nDistinguishing Styles from a Mixed Corpus. It might also be possible to distinguish styles from a mixed corpus with no style labels. For example, Riley et al. (2021) learn a style vector space from text; Xu, Cheung, and Cao (2020) use unsupervised representation learning to separate the style and contents from a mixed corpus of unspecified styles; and Guo et al. (2021) use cycle training with a conditional variational auto-encoder to unsupervisedly learn to express the same semantics through different styles. Theoretically, although disentanglement is impossible without inductive biases or other forms of supervision (Locatello et al. 2019), disentanglement is achievable with some weak signals, such as only knowing how many factors have changed, but not which ones (Locatello et al. 2020). A more advanced direction can be emergent styles (Kang, Wang, and de Melo 2020), since styles can be evolving, for example across dialog turns.", "filtered_refids": [[], ["b146", "b144", "b101", "b7", "b164"], ["b156", "b210", "b107", "b48", "b77", "b108"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1980, "num_references": 11}
{"corpusid_sectionid": "229376920-s44", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Improving Evaluation Metrics", "section": "There has been a lot of attention to the problems of evaluation metrics of TST and potential improvements (Pang and Gimpel 2019;Tikhonov and Yamshchikov 2018;Mir et al. 2019;Fu et al. 2019;Yamshchikov et al. 2021;Jafaritazehjani et al. 2020). Recently, Gehrmann et al. (2021) have proposed a new framework that is a live environment to evaluate NLG in a principled and reproducible manner. Apart from the existing scoring methods, future work can also make use of linguistic rules such as a checklist to evaluate what capabilities the TST model has achieved. For example, there can be a checklist for formality transfer according to existing style guidelines, such as the APA style guide (American Psychological Association 2020). Such a checklistbased evaluation can make the performance of black-box deep learning models more interpretable, and also allow for more insightful error analysis.", "filtered_refids": [["b67", "b133", "b122", "b30", null, "b189", "b214"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 893, "num_references": 7}
{"corpusid_sectionid": "229376920-s46", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Connecting TST to More NLP Tasks", "section": "TST can be applied to other important NLP tasks, such as paraphrase generation, data augmentation, and adversarial robustness probing.\n\nParaphrase Generation. Paraphrase generation is to express the same information in alternative ways (Madnani and Dorr 2010). The nature of paraphrasing shares a lot in common with TST, which is to transfer the style of text while preserving the content. One of the common ways of paraphrasing is syntactic variation, such as \"X wrote Y.\", \"Y was written by X.\", and \"X is the writer of Y.\" (Androutsopoulos and Malakasiotis 2010). Besides syntactic variation, it also makes sense to include stylistic variation as a form of paraphrases, which means that the linguistic style transfer (not the content preference transfer in Table 3) can be regarded as a subset of paraphrasing. The caution here is that if the paraphrasing is for a downstream task, researchers should first check if the downstream task is compatible with the used styles. For example, dialog generation may be sensitive to all linguistic styles, whereas summarization can allow linguistic style-varied paraphrases in the dataset.\n\nThere are three implications of this connection of TST and paraphrase generation. First, many trained TST models can be borrowed for paraphrasing, such as formality transfer and simplification. A second connection is that the method innovations proposed in the two fields can inspire each other. For example, Krishna, Wieting, and Iyyer (2020) formulate style transfer as a paraphrasing task. Thirdly, the evaluation metrics of the two tasks can also inspire each other. For example, Yamshchikov et al. (2021) associate the semantic similarity metrics for two tasks.\n\nData Augmentation. Data augmentation generates text similar to the existing training data so that the model can have larger training data. TST is a good method for data augmentation because TST can produce text with different styles but the same meaning. Image style transfer has already been used for data augmentation (Zheng et al. 2019;Jackson et al. 2019), so it can be interesting to see future works also apply TST for data augmentation.\n\nAdversarial Robustness Probing. Another use of style transferred text is adversarial robustness probing. For example, styles that are task-agnostic can be used for general adversarial attack (e.g., politeness transfer to probe sentiment classification robustness) (Jin et al. 2020b), while the styles that can change the task output can be used to construct contrast sets (e.g., sentiment transfer to probe sentiment classification robustness) (Xing et al. 2020).  apply image style transfer to adversarial attack, and future research can also explore the use of TST in the two ways suggested above.", "filtered_refids": [[], ["b113", "b0"], ["b214", "b87"], [null, "b228"], ["b72", "b208"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2746, "num_references": 8}
{"corpusid_sectionid": "229376920-s47", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Connecting TST to More Specialized Applications", "section": "TST can be applied not only to other NLP tasks as introduced in the previous section, but also can be helpful for specialized downstream applications. In practice, when applying NLP models, it is important to customize for some specific needs, such as generating dialog with a consistent persona, writing headlines that are attractive and engaging, making machine translation models adapt to different styles, and anonymizing the user identity by obfuscating the style.\n\nPersona-Consistent Dialog Generation. A useful downstream application of TST is persona-consistent dialog generation Zhang et al. 2018b;Shuster et al. 2020). Because conversational agents directly interact with users, there is a strong demand for human-like dialog generation. Previously, this has been done by encoding speaker traits into a vector and the conversation is then conditioned on this vector . As future work, TST can also be used as part of the pipeline of personabased dialog generation, where the persona can be categorized into distinctive style types, and then the generated text can be post-processed by a style transfer model.\n\nAttractive Headline Generation. In journalism writing, it is crucial to generate engaging headlines. Jin et al. (2020a) first use TST to generate eye-catchy headlines with three different styles: humorous, romantic, and clickbaity styles.  follow this direction and propose a disentanglement-based model to generate attractive headlines for Chinese news.\n\nStyle-Specific Machine Translation. In machine translation, it is useful to have an additional control of the style for the translated text. Commonly used styles for TST in machine translation are politeness (Sennrich, Haddow, and Birch 2016a) and formality (Niu, Martindale, and Carpuat 2017;Wu, Wang, and Liu 2020). For example, Wu, Wang, and Liu (2020) translate from informal Chinese to formal English.\n\nAnonymization. TST can also be used for anonymization, which is an important way to protect user privacy, especially since there are ongoing heated discussions of ethics in the AI community. Many concerns have been raised about the discriminative task of author profiling, which can mine the demographic identities of the author of a writing, even including privacy-invading properties such as gender and age (Schler et al. 2006). As a potential solution, TST can be applied to alter the text and obfuscate the real identity of the users (Reddy and Knight 2016; Gr\u00f6ndahl and Asokan 2020).", "filtered_refids": [[], ["b177", "b221"], ["b71"], ["b207", "b167", "b128"], ["b165"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2471, "num_references": 7}
{"corpusid_sectionid": "229376920-s48", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Ethical Implications of TST", "section": "Recently, there is more and more attention being paid to the ethics concerns associated with AI research. We discuss in the following two ethics considerations: (1) social impact of TST applications, and (2) data privacy problem of TST.\n\nFields that involve human subjects or direct application to humans work under a set of core principles and guidelines (Beauchamp, Childress et al. 2001). Before initiating a research project, responsible research bodies use these principles as a ruler to judge whether the research is ethically correct to start. NLP research and applications, including TST, that directly involve human users, is regulated under a central regulatory board, the Institutional Review Board (IRB). We also provide several guidelines below to avoid ethical misconduct in future publications on TST.\n\n7.3.1 Social Impact of TST Applications. Technologies can have unintended negative consequences (Hovy and Spruit 2016). For example, TST can facilitate the automation of intelligent assistants with designed attributes, but can also be used to create fake text or fraud.\n\nThus, inventors of a technology should beware how other people very probably adopt this technology for their own incentives. For TST, because it has a wide range of subtasks and applications, we examine each of them with the following two questions:\n\n\u2022 Who will benefit from such a technology?\n\n\u2022 Who will be harmed by such a technology?\n\nAlthough many ethics issues are debatable, we try to categorize the text attribute tasks into three ethics levels: those with beneficial impacts, neutral impacts, and dual use.\n\nBeneficial Impact. An important direction of NLP for social good is to fight against abusive online text. TST can serve as a very helpful tool as it can be used to transfer malicious text to normal language. Shades of abusive language include hate speech, offensive language, sexist and racist language, aggression, profanity, cyberbullying, harassment, trolling, and toxic language (Waseem et al. 2017). There is also other negative text such as propaganda (Bernays 2005;Carey 1997), and others. It is widely known that malicious text is harmful to people. For example, research shows that cyberbullying victims tend to have more stress and suicidal ideation (Kowalski et al. 2014), and also detachment from family and offline victimization (Oksanen et al. 2014). There are more and more efforts put into combating toxic language, such as 30K content moderators that Facebook and Instagram employ (Harrison 2019). Therefore, the automatic malicious-to-normal language transfer can be a helpful intelligent assistant to address such needs. Apart from purifying malicious text on social media, it can also be used on social chatbots to make sure there is no bad content in the language they generate (Roller et al. 2021).\n\nNeutral Impact. Most TST tasks are neutral. For example, informal-to-formal transfer can be used as a writing assistant to help make writing more professional, and formalto-informal transfer can tune the tone of bots to be more casual. Most applications to customize the persona of bots are also neutral with regard to their societal impact.\n\nDual Use. Besides positive and neutral applications, there are, unfortunately, several TST tasks that are double-edged swords. For example, take one of the most popular TST tasks, sentiment modification; although it can be used to change intelligent assistants or robots from a negative to positive mood (which is unlikely to harm any parties), the vast majority of research applies this technology to manipulate the polarity of reviews, such as Yelp (Shen et al. 2017) and Amazon reviews (He and McAuley 2016). This leads to a setting where a negative restaurant review is changed to a positive comment, or vice versa, with debatable ethics. Such a technique can be used as a cheating method for the commercial body to polish its reviews, or harm the reputation of their competitors. Once this technology is used, it will automatically manipulate the online text to contain polarity that the model owner desires. Hence, we suggest the research community raise serious concern against the review sentiment modification task. Another task, political slant transfer, may induce concerns within some specific context. For example, social bots (i.e., autonomous bots on social media, such as Twitter bots and Facebook bots) are a big problem in the United States, even playing a significant role in the 2016 U.S. presidential election (Bessi and Ferrara 2016; Shao et al. 2018). It is reported that at least 400,000 bots were responsible for about 19% of the total Tweets. Social bots usually target to advocate certain ideas, supporting campaigns, or aggregating other sources either by acting as a \"follower\" and/or gathering followers itself. So the political slant transfer task, which transfers the tone and content between Republican comments and Democratic ones, are highly sensitive and may face the risk of being used on social bots to manipulate political views of the mass.\n\nSome more arguable ones are male-to-female tone transfer, which can be potentially used for identity deception. The cheater can create an online account and pretend to be an attractive young woman. There is also the reversed direction (female-to-male tone transfer), which can be used for applications such as authorship obfuscation (Shetty, Schiele, and Fritz 2018), anonymizing the author attributes by hiding the gender of a female author by re-synthesizing the text to use male textual attributes. 7.3.2 Data Privacy Issues for TST. Another ethics concern is the use of data in research practice. Researchers should not overmine user data, such as demographic identities. Such data privacy widely exists in the data science community as a whole, and there have been many ethics discussions (Tse et al. 2015;Russell, Dewey, and Tegmark 2015).\n\nThe TST task needs data containing some attributes along with the text content. Although it is acceptable to use ratings of reviews that are classified as positive or negative, user attributes are sensitive, including the gender of the user's account (Prabhumoye et al. 2018), and age (Lample et al. 2019). The collection and potential use of such sensitive user attributes can have implications that need to be carefully considered.", "filtered_refids": [[], [null], ["b58"], [], [], [], [], ["b12", "b157", "b85", "b50", "b200", null, "b131"], [], ["b174", "b171", "b52"], ["b162", "b176", "b192"], ["b94", "b141"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 46, "num_chars": 6333, "num_references": 17}
{"corpusid_sectionid": "237371890-s1", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Language Pair Speakers (approx) Parallel Sentences", "section": "English-French 267M 280M English-Myanmar 30M 0.7M English-Fon 2M 0.035M Table 1: Examples of language pairs with different levels of resources. The number of speakers is obtained from Ethnologue, and the parallel sentence counts are from Opus.\n\nIn Table 1 we select three language pairs that display a range of resource levels. We show the estimated number of first language speakers 1 of the non-English language, together with the number of parallel sentences available in Opus (Tiedemann 2012), the largest collection of publicly available translated data. 2,3 Although there is typically a correlation between speaker numbers and size of available resources, there are many exceptions where either widely spoken languages have little parallel data, or languages with very small speaker numbers are richly resourced (mainly European languages). For one of the world's most spoken languages, French, there are nearly 280 million parallel sentences of English-French in OPUS. However when we search for English-Myanmar, we find only around 700,000 parallel sentences, despite Myanmar having tens of millions of speakers. If we consider Fon, which has around 2 million speakers, then we find far fewer parallel sentences, only 35,000 4 . Developing MT systems for these three language pairs will require very different techniques.\n\nThe lack of parallel data for most language pairs is only one part of the problem. Existing data is often noisy or from a very specialised domain. Looking at the resources that are available for Fon-English, we see that the only corpus available in Opus is extracted from Jehovah's Witness publications (Agi\u0107 and Vuli\u0107 2019, JW300) 5 . For many language pairs, the only corpora available are those derived from religious sources (e.g. Bible, Koran) or from IT localisation data (e.g. from open-source projects such as GNOME and Kubuntu). Not only is such data likely to be in a very different domain from the text that we would like to translate, but such large-scale multilingual automatically extracted corpora are often of poor quality (Caswell et al. 2021) and this problem is worse for low-resource language pairs. This means that low-resource language pairs suffer from multiple compounding problems: lack of data, out-of-domain data and noisy data. And the difficulty is not just with parallel data, low-resource languages often lack good linguistic tools, and even basic tools like language identification do not exist or are not reliable.\n\nPartly in response to all these challenges, there has been an increasing interest in the research community in exploring more diverse languages, and language pairs that do not include English. This survey paper presents a high-level summary of approaches to low-resource MT, with focus on neural machine translation (NMT) techniques, which should be useful for researchers interested in this broad and rapidly evolving field. There are currently a number of other survey papers in related areas, for example a survey of monolingual data in low-resource NMT (Gibadullin et al. 2019) and a survey of multilingual NMT (Dabre, Chu, and Kunchukuttan 2020). There have also been two very recent surveys of low-resource MT, which have been written concurrently with this survey (Ranathunga et al. 2021;Wang et al. 2021). Our survey aims to provide the broadest coverage of existing research in the field and we also contribute an extensive overview of the tools and techniques validated across 18 low-resource shared tasks that ran between 2018 and 2021.\n\nOne of the challenges of surveying the literature on low-resource MT is how to define what a low-resource language pair is. This is hard, because \"resourced-ness\" is a continuum and any criterion must be arbitrary. We also note that the definition of low-resource can change over time. We could crawl more parallel data, or we could find better ways of using related language data or monolingual data which means that some language pairs are no longer so resource-poor. We maintain that for research to be considered to be on \"low-resource MT\", there should be some way in which the research should either aim to understand the implications of the lack of data, or propose methods for overcoming the lack of data. We do not take a strict view of what to include in this survey though; if the authors consider that they are studying low-resource MT, then that is sufficient. We do feel however that it is important to distinguish between simulated low-resource settings (where a limited amount of data from otherwise highresource language pairs is used) and genuinely low-resource languages (where additional difficulties apply). We also discuss some papers that do not explicitly consider low-resource MT but which present important techniques and we mention methods that we think have the potential to improve low-resource MT. Even though there has been a lot of interest recently in low-resource NLP, the field is limited to languages where some textual data is freely available. This means that so far low-resource MT has only considered 100-odd languages, and there is a long tail of languages that is still unexplored.\n\nIn Figure 1 we show how we structure the diverse research methods addressing low-resource MT, and this paper follows this structure. We start the survey by looking at work that aims to increase the amount and quality of parallel and monolingual data available for low-resource MT (Section 2). We then look at work that uses other types of data: monolingual data (Section 3), parallel data from other language pairs (Section 4), and other types of linguistic data (Section 5). Another avenue of important research is how to make better use of existing, limited resources through better training or modelling (Section 6). In Section 7 we pause our discussion on methods to improve low-resource MT, to consider how to evaluate these improvements. In our final section, we look at efforts in the community to build research capacity through shared tasks and  language-specific collectives (Section 8), providing a practical summary of commonly used approaches and other techniques often used by top-performing shared task systems. This survey aims to provide researchers and practitioners interested in low-resource MT with an overview of the area, and we hope it will be especially useful with those that are new to low-resource MT, and looking to quickly assimilate the recent research directions. We assume that our readers have prior knowledge of MT techniques and are already familiar with basic concepts, including the main architectures used. We therefore do not redefine them in this survey and refer the interested reader to other resources such as (Koehn 2020).", "filtered_refids": [[], ["b191"], [null], [null, "b202", "b34"], [], ["b74"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 41, "num_chars": 6722, "num_references": 6}
{"corpusid_sectionid": "237371890-s4", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Web-crawling for Parallel Data", "section": "Once freely available sources of parallel data have been exhausted, one avenue for improving low-resource NMT is to obtain more parallel data by web-crawling. There is a large amount of translated text available on the web, ranging from small-scale tourism websites to large repositories of government documents. Identifying, extracting and sentence-aligning such texts is not straightforward, and researchers have considered many techniques for producing parallel corpora from web data. The links between source texts and their translations are rarely recorded in a consistent way, so techniques ranging from simple heuristics to neural sentence embedding methods are used to extract parallel documents and sentences.\n\nParacrawl, 11 a recent large-scale open-source crawling effort (Ba\u00f1\u00f3n et al. 2020) has mainly targeted European languages, only a few of which can be considered as lowresource, but it has created some releases for non-European low-resource language pairs, and the crawling pipeline is freely available. Related to this are other broad guages, where it is affected by class imbalance, similar languages, encoding problems and domain mismatches. Further down the crawling pipeline, common techniques for document alignment and sentence alignment rely on the existence of translation systems (Uszkoreit et al. 2010;Volk 2010, 2011) or sentence embeddings (Artetxe and Schwenk 2019a), which again may not be of sufficient quality in lowresource languages and so we often have to fall back on older, heuristic alignment techniques (Varga et al. 2005) (and even this may perform worse if a bilingual lexicon is not available). The consequence is that the resulting corpora are extremely noisy and require extensive filtering before they can be useful for NMT training.\n\nFiltering of noisy corpora is itself an active area of research, and has been explored in recent shared tasks, which particularly emphasised low-resource settings . In an earlier version of the task (Koehn et al. 2018), dual conditional crossentropy (Junczys-Dowmunt 2018, DCCE) was found to be very effective for English-German. In the 2019 and 2020 editions of the task however, DCCE was much less used possibly indicating that it is less effective for low-resource and/or distant language pairs. Instead, we see that all participants apply some heuristic filtering (e.g. based on language identification and length) and then strong submissions typically used a combination of embedding-based methods (such as LASER (Artetxe and Schwenk 2019b), GPT-2 (Radford et al. 2019) and YiSi (Lo 2019)) with feature-based systems such as zipporah (Xu and Koehn 2017) or bicleaner . Whilst the feature-based methods are much faster than sentence-embedding based methods, both types of methods require significant effort in transferring to a new language pair, especially if no pre-trained sentence embeddings or other models are available.\n\nThe conclusion is that all crawled data sources should be treated with care, especially in low-resource settings as they will inevitably contain errors. A large-scale quality analysis (Caswell et al. 2021) of crawled data has highlighted that many contain incorrect language identification, non-parallel sentences, low quality texts, as well as offensive language, and these problems can be more acute in low-resource languages.", "filtered_refids": [[], ["b197", "b176", "b198"], [null, "b149", "b79"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 3344, "num_references": 7}
{"corpusid_sectionid": "237371890-s6", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Test Sets", "section": "Obtaining more training data is important, but we should not forget the role of standardised and reliable test sets in improving performance on low-resource translation. Important contributions have come from shared tasks, such as those organised by the WMT Conference on Machine Translation Barrault et al. 2019Barrault et al. , 2020 (Nakazawa et al. 2018(Nakazawa et al. , 2019(Nakazawa et al. , 2020 and the Workshop on Technologies for MT of Low Resource Languages (LoResMT) (Karakanta et al. 2019;Ojha et al. 2020). The test sets in these shared tasks are very useful, but inevitably only cover a small selection of low-resource languages, and usually English is one of the languages in the pair; non-English language pairs are poorly covered. A recent initiative towards rectifying this situation is the FLORES-101 benchmark , which covers a large number of low-resource languages with multi-parallel test sets, vastly expanding on the original FLORES release . Since FLORES-101 consists of the same set of English sentences, translated into 100 other languages, it can also be used for testing translation between non-English pairs. It has the limitation that, for non-English pairs, the two sides are \"translationese\", and not mutual translations of each other, but there is currently no other data set with the coverage of FLORES-101.\n\nWe summarise the data sets described in this section in Table 2.", "filtered_refids": [["b59", null, "b122", "b128"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1409, "num_references": 4}
{"corpusid_sectionid": "237371890-s8", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Integration of external language models", "section": "For SMT, monolingual data was normally incorporated into the system using a language model, in a formulation that can be traced back to the noisy channel model (Brown et al. 1993). In early work on NMT, researchers drew inspiration from SMT, and several works have focused on integrating external language models into NMT models.\n\nThe first approach, proposed by G\u00fcl\u00e7ehre et al. (2015), was to modify the scoring function of the MT model by either interpolating the probabilities from a language model with the translation probabilities (they call this shallow fusion) or integrating the hidden states from the language model within the decoder (they call this deep fusion). Importantly, they see improved scores for a range of scenarios, including a (simulated) low-resource language direction (Turkish\u2192English), with best results achieved using deep fusion. Building on this, Stahlberg, Cross, and Stoyanov (2018) proposed simple fusion as an alternative method for including a pre-trained LM. In this case, the NMT model is trained from scratch with the fixed LM, offering it a chance to learn to be complementary to the LM. The result is improved translation performance as well as training efficiency, with experiments again on low-resource Turkish-English, as well as on larger data sets for Xhosa\u2192English and Estonian\u2192English.\n\nThe addition of a language model to the scoring function as in the works described above has the disadvantage of increasing the time necessary for decoding (as well as training). An alternative approach was proposed by Baziotis, Haddow, and Birch (2020), who aim to overcome this by using the language model as a regulariser during training, pushing the source-conditional NMT probabilities to be closer to the unconditional LM prior. They see considerable gains in very low-resource settings (albeit simulated), using small data sets for Turkish-English and German-English.", "filtered_refids": [[null], ["b42", "b186"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1909, "num_references": 4}
{"corpusid_sectionid": "237371890-s10", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Self-learning: backtranslation and its variants.", "section": "One of the most successful strategies for leveraging monolingual data has been the creation of synthetic parallel data through translating monolingual texts either using a heuristic strategy or an intermediately trained MT model. This results in parallel data where one side is human generated, and the other is automatically produced. We focus here on backtranslation and its variants, before exploring in the next section how unsupervised MT can be seen as an extension of this idea.\n\nBacktranslation. Backtranslation corresponds to the scenario where target-side monolingual data is translated using an MT system to give corresponding synthetic source sentences, the idea being that it is particularly beneficial for the MT decoder to see wellformed sentences. Backtranslation was first introduced in SMT (Bertoldi and Federico 2009; Bojar and Tamchyna 2011), but since monolingual data could already be incorporated easily into SMT systems using language models, and because inference in SMT was quite slow, backtranslation was not widely used. For NMT however, it was discovered that backtranslation was a remarkably effective way of exploiting monolingual data (Sennrich, Haddow, and Birch 2016a), and it remains an important technique, for both low-resource MT and MT in general.\n\nThere has since been considerable interest in understanding and improving backtranslation. For example, Edunov et al. (2018a) showed that backtranslation improved performance even at a very large scale, but also that it provided improvements in (simulated) low-resource settings, where it was important to use beam-search rather than sampling to create backtranslations (the opposite situation to high-resource pairs). Caswell, Chelba, and Grangier (2019) showed that simply adding a tag to the backtranslated data during training, to let the model know which was back-translated data and which was natural data, could improve performance.\n\nVariants of backtranslation. Forward translation, where monolingual source data is translated into the target language (Zhang and Zong 2016), is also possible but has received considerably less interest than backtranslation for low-resource MT, presumably because of the noise it introduces for the decoder. However, He et al. (2019) actually find it more effective than backtranslation in their experiments for low-resource English\u2192Nepali when coupled with noising (as a kind of dropout), which they identify as an important factor in self-supervised learning. A related (and even simpler) tech-nique related to forward and backtranslation, that of copying from target to source to create synthetic data, was introduced by Currey, Miceli Barone, and Heafield (2017). They showed that this was particularly useful in low-resource settings (tested on Turkish-English and Romanian-English) and hypothesise that it helped particularly with the translation of named entities.\n\nIterative backtranslation. For low-resource NMT, back-translation can be a particularly effective way of improving quality . However one possible issue is that the initial model used for translation (trained on available parallel data) is often of poor quality when parallel data is scarce, which inevitable leads to poor quality backtranslations. The logical way to address this is to perform iterative backtranslation, whereby intermediate models of increasing quality in both language directions are successively used to create synthetic parallel data for the next step. This has been successfully applied to low-resource settings by several authors (Hoang et al. 2018;Dandapat and Federmann. 2018;Bawden et al. 2019;, although successive iterations offer diminishing returns, and often two iterations are sufficient, as has been shown experimentally .\n\nOther authors have sought to improve on iterative backtranslation by introducing a round-trip (i.e. autoencoder) objective for monolingual data, in other words performing backtranslation and forward translation implicitly during training. This was simultaneously proposed by Cheng et al. (2016) and He et al. (2016) and also by Zhang and Zong (2016) who also added forward translation. However, none of these authors applied their techniques to low-resource settings. In contrast, Niu, Xu, and Carpuat (2019) developed a method using Gumbel softmax to enable back-propagation through backtranslation: they tested in low-resource settings but achieved limited success.\n\nDespite the large body of literature on applying back-translation and related techniques, and evidence that it works in low-resource NMT, there are few systematic experimental study of back-translation specifically for low-resource NMT, apart from ), which appears to confirm the findings of (Edunov et al. 2018a) that sampling is best when there are reasonable amounts of data and beam search is better when data is very scarce.", "filtered_refids": [[], ["b176"], ["b15"], ["b49", null, "b221"], [null, "b58"], ["b48", "b2", "b221"], ["b15"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 4857, "num_references": 11}
{"corpusid_sectionid": "237371890-s11", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Unsupervised MT.", "section": "The goal of unsupervised MT is to learn a translation model without any parallel data, so this can be considered an extreme form of low-resource MT. The first unsupervised NMT models (Lample et al. 2018a;Artetxe et al. 2018) were typically trained in a two-phase process: a rough translation system is first created by aligning word embeddings across the two languages (e.g. using bilingual seed lexicons), and then several rounds of iterative backtranslation and denoising autoencoding are used to further train the system. Whilst this approach has been successfully applied to high-resource language pairs (by ignoring available parallel data) it has been shown to perform poorly on genuine low-resource language pairs Marchisio, Duh, and Koehn 2020;Kim, Gra\u00e7a, and Ney 2020), mainly because the initial quality of the word embeddings and their cross-lingual alignments is poor (Edman, Toral, and van Noord 2020). The situation is somewhat improved using transfer learning from models trained on large amounts of monolingual data (Section 3.3), and some further gains can be achieved by adding a supervised training step with the limited parallel data (i.e. semi-supervised rather than unsupervised) (Bawden et al. 2019). However the performance remains limited, especially compared to high-resource language pairs. These negative results have focused researchers' attention on making unsupervised MT work better for low-resource languages. Chronopoulou, Stojanovski, and Fraser (2021) improved the cross-lingual alignment of word embeddings in order to get better results on unsupervised Macedonian-English and Albanian-English. A separate line of work is concerned with using corpora from other languages to improve unsupervised NMT (see Section 4.2.2) 3.2.3 Modification of existing parallel data. Another way in which language models have been used to generate synthetic parallel data is to synthesise parallel examples from new ones by replacing certain words. 15 In translation, it is important to maintain the relation of translation between the two sentences in the parallel pair when modification of the pair occurs. There are to our knowledge few works so far in this area. Fadaee, Bisazza, and Monz (2017) explore data augmentation for MT for a simulated low-resource setting (using English-German). They rely on bi-LSTM language models to predict plausible but rare equivalents of words in sentences. They then substitute in the rare words and replace the aligned word in the corresponding parallel sentence with its translation (obtained through a look-up in an SMT phrase table). They see improved BLEU scores (Papineni et al. 2002) and find that it is a complementary technique to backtranslation. More recently, Arthaud, Bawden, and Birch (2021) apply a similar technique to improve the adaptation of a model to new vocabulary for the low-resource translation direction Gujarati\u2192English. They use a BERT language model to select training sentences that provide the appropriate context to substitute new and unseen words in order to create new synthetic parallel training sentences. While their work explores the trade-off between specialising to the new vocabulary and maintaining overall translation quality, they show that the approach can improve the translation of new words more effectively following data augmentation.", "filtered_refids": [["b135", "b90", "b63", "b14", null, "b110", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3342, "num_references": 7}
{"corpusid_sectionid": "237371890-s12", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Introducing Monolingual Data Using Transfer Learning", "section": "The third category of approaches we explore looks at transfer learning, by which we refer to techniques where a model trained using the monolingual data is used to initialise some or all of the NMT model. A related, but different idea, multilingual models, where the low-resource NMT model may be trained with the help of other (high-resource) languages will be considered in Section 4.\n\nPre-trained embeddings. When neural network methods were introduced to NLP, transfer learning meant using pre-trained word embeddings, such as word2vec (Mikolov et al. 2013) or GloVe (Pennington, Socher, and Manning 2014), to introduce knowledge from large unlabelled monolingual corpora into the model. The later introduction of the multilingual fastText embeddings (Bojanowski et al. 2017) meant that pre-trained embeddings could be tested with NMT (Di Gangi and Federico 2017;Neishi et al. 2017;Qi et al. 2018). Pre-trained word embeddings were also used in the first phase of unsupervised NMT training (Section 3.2.2). Of most interest for low-resource NMT was the study by Qi et al. (2018), who showed that pre-trained embeddings could be extremely effective in some low-resource settings.\n\n15 These techniques are inspired by data augmentation in computer vision, where it is much simpler to manipulate examples to create new ones (for example by flipping and cropping images) whilst preserving the example label. The difficulty in constructing synthetic examples in NLP in general is the fact that the modifying any of the discrete units of the sentence is likely to change the meaning or grammaticality of the sentence.\n\nPre-trained language models. Another early method for transfer learning was to pre-train a language model, and then to use it to initialise either the encoder or the decoder or both (Ramachandran, Liu, and Le 2017). Although not MT per se, Junczys-Dowmunt et al. (2018b) applied this method to improve grammatical error correction, which they modelled as a low-resource MT task. The pre-trained language model approach has been extended with new objective functions based on predicting masked words, trained on large amounts of monolingual data. Models such as ELMo (Peters et al. 2018) and BERT (Devlin et al. 2019) have been shown to be very beneficial to natural language understanding tasks, and researchers have sought to apply related ideas to NMT. One of the blocking factors identified by Yang et al. (2020) in using models such as BERT for pre-training is the problem of catastrophic forgetting (Goodfellow et al. 2014). They propose a modification to the learning procedure involving a knowledge distillation strategy designed to retain the model's capacity to perform language modelling during translation. They achieve increased translation performance according to BLEU, although they do not test on lowresource languages.\n\nDespite the success of ELMo and BERT in NLP, large-scale pre-training in NMT did not become popular until the success of the XLM (Conneau and Lample 2019), MASS (Song et al. 2019b) and mBART ) models. These models allow transfer learning for NMT by initial training on large quantities of monolingual data in several languages, before fine-tuning on the languages of interest. Parallel data can also be incorporated into these pre-training approaches (Tang et al. 2021). Since they use data from several languages, they will be discussed in the context of multilingual models in Section 4.3.", "filtered_refids": [[], ["b147", "b137", null, "b123", "b114", "b8"], [], ["b58", "b215", "b152", null, "b138", "b36"], ["b184", "b190"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3445, "num_references": 14}
{"corpusid_sectionid": "237371890-s14", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Transfer Learning", "section": "In the earliest form of multilingual transfer learning for NMT, a parent model is trained on one language pair, and then the trained parameters are used to initialise a child model, which is then trained on the desired low-resource language pair.\n\nThis idea was first explored by Zoph et al. (2016), who considered a French-English parent model, and child models translating from 4 low-resource languages (Hausa, Turkish, Uzbek and Urdu) into English. They showed that transfer learning could indeed improve over random initialisation, and the best performance for this scenario was obtained when the values of the target embeddings were fixed after training the parent, but the training continued for all the other parameters. Zoph et al. (2016) suggest that the choice of the parent language could be important, but did not explore this further for their low-resource languages.\n\nWhilst Zoph et al. (2016) treat the parent and child vocabularies as independent, Nguyen and Chiang (2017) showed that when transferring between related languages (in this case, within the Turkic family), it is beneficial to share the vocabularies between the parent and child models. To boost this effect, subword segmentation such as BPE (Sennrich, Haddow, and Birch 2016b) can help to further increase the vocabulary overlap. In cases where there is little vocabulary overlap (for example, because the languages are distantly related), mapping the bilingual embeddings between parent and child can help (Kim, Gao, and Ney 2019). In cases where the languages are highly related but are written in different scripts, transliteration may be used to increase the overlap in terms of the surface forms (Dabre et al. 2018;Goyal, Kumar, and Sharma 2020). Interestingly, in the case of transferring from a high-resource language pair into a low-resource one where the target language is a variant of the initial parent language Kumar et al. (2021) found it useful to pretrain embeddings externally and then fix them during the training of the parent, before initialising the embeddings of the low-resource language using those of the high-resource variant. Tested from English into Russian (transferring to Ukranian and Belarusian), Norwegian Bokm\u00e5l (transferring to Nynorsk) and Arabic (transferring to four Arabic dialects), they hypothesise that decoupling the embeddings from training helps to avoid mismatch when transferring from the parent to the child target language.\n\nThe question of how to choose the parent language for transfer learning, as posed by Zoph et al. (2016), has been taken up by later authors. One study suggests that language relatedness is important (Dabre, Nakagawa, and Kazawa 2017). However Kocmi and Bojar (2018) showed that the main consideration in transfer learning is to have a strong parent model, and it can work well for unrelated language pairs. Still, if languages are unrelated and the scripts are different, for example transferring from an Arabic-Russian parent to Estonian-English, transfer learning is less useful. Lin et al. (2019) perform an extensive study on choosing the parent language for transfer learning, showing that data-related features of the parent models and lexical overlap are often more important than language similarity. Further insight into transfer learning for low-resource settings was provided by Aji et al. (2020), who analysed the training dynamics and concluded that the parent language is not important. The effectiveness from transfer learning with strong (but linguistically unrelated) parent models has been confirmed in shared task submissions such as Bawden et al. (2020) -see Section 8.2.4.\n\nMulti-stage transfer learning methods have also been explored. Dabre, Fujita, and Chu (2019) propose a two-step transfer with English on the source side for both parent and child models. First, a one-to-one parent model is used to initialise weights in a multilingual one-to-many model, using a multi-way parallel corpora that includes the child target language. Second, the intermediate multilingual model is fine-tuned on parallel data between English and the child target language.  use a two-parent model and a pivot language. One parent model is between the child source language and the pivot language (e.g. German-English), and the other translates between the pivot and the child target language (e.g. English-Czech). Then, the encoder parameters from the first model and the decoder parameters of the second models are used to initialise the parameters of the child model (e.g. German-Czech).", "filtered_refids": [[], ["b226"], ["b86", "b124", "b122", "b62", "b226", "b176", "b37"], ["b103", null, "b71", "b226"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4551, "num_references": 12}
{"corpusid_sectionid": "237371890-s15", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Multilingual Models", "section": "The goal of multilingual MT is to have a universal model capable of translation between any two languages. Including low-resource language pairs in multilingual models can be seen as means of exploiting additional data from other, possibly related, languages. Having more languages in the training data helps developing a universal representation space, which in turn allows for some level of parameter sharing among the languagespecific model components.\n\nThe degree to which parameters are shared across multiple language directions varies considerably in the literature, with early models showing little sharing across languages (Dong et al. 2015) and some later models exploring the sharing of most or all parameters (Johnson et al. 2017). The amount of parameter sharing can be seen as a trade-off between ensuring that each language is sufficiently represented (has enough parameters allocated) and that low-resource languages can benefit from the joint training of parameters with other (higher-resource) language pairs (which also importantly reduces the complexity of the model by reducing the number of parameters required). Dong et al. (2015) present one of the earliest studies in multilingual NMT, focused on translation from a single language into multiple languages simultaneously. The central idea of this approach is to have a shared encoder and many language-specific decoders, including language-specific weights in the attention modules. By training on multiple target languages (presenting as a multi-task setup), the motivation is that the representation of the source language will not only be trained on more data (thanks to the multiple language pairs), but the representation may be more universal, since it is being used to decode several languages. They find that the multi-decoder setup provides systematic gains over the bilingual counterparts, although the model was only tested in simulated low-resource settings.\n\nAs an extension to this method, Firat, Cho, and Bengio (2016) experiment with multilingual models in the many-to-many scenario. They too use separate encoders and decoders for each language, but the attention mechanism is shared across all directions, which means adding languages increases the number of model parameters linearly (as opposed to quadratic increase when attention is language-direction-specific). In all cases, the multi-task model performed better than the bilingual models according to BLEU scores, although it was again only tested in simulated low-resource scenarios.\n\nMore recent work has looked into the benefits of sharing only certain parts of multilingual models, ensuring language-dependent components. For example, Platanios et al. (2018) present a contextual parameter generator component, which allows finer control of the parameter sharing across different languages. Fan et al. (2021) also include language-specific components by sharing certain parameters across pre-defined language groups in order to efficiently and effectively upscale the number of languages included (see Section 4.2.1).\n\nIn a bid to both simplify the model (also reducing the number of parameters) and to maximally encourage sharing between languages, Ha, Niehues, and Waibel (2016) and Johnson et al. (2017) proposed to use a single encoder and decoder to train all language directions (known as the universal encoder-decoder). Whereas Ha, Niehues, and Waibel (2016) propose language-specific embeddings, Johnson et al. (2017) use a joint vocabulary over all languages included, which has the advantage of allowing shared lexical representations (and ultimately this second strategy is the one that has been retained by the community). The control over the target language was ensured in both cases by including pseudo-tokens indicating the target language in the source sentence. Although not trained or evaluated on low-resource language pairs, the model by Johnson et al. (2017) showed promise in terms of the ability to model multilingual translation with a universal model, and zero-shot translation (between language directions for which no parallel training data was provided) was also shown to be possible. The model was later shown to bring improvements when dealing with translation into several low-resource language varieties (Lakew, Erofeeva, and Federico 2018), a particular type of multilingual MT where the several target languages are very similar. We shall see in the next section (Section 4.2.1) how scaling up the number of languages used for training can be beneficial in the low-resource setting.\n\nCombining multilingual models, with the transfer learning approaches of the previous section, Neubig and Hu (2018) present a number of approaches for adaptation of multilingual models to new languages. The authors consider cold-and warm-start scenarios, depending on whether the training data for the new language was available for training the original multilingual model. They find that multilingual models finetuned with the low-resource language training data mixed in with data from a similar high-resource language (i.e. similar-language regularisation) give the best translation performance.", "filtered_refids": [[], ["b11", "b57"], [], [], ["b57", "b44", "b89"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 5171, "num_references": 5}
{"corpusid_sectionid": "237371890-s16", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Massively multilingual models.", "section": "In the last couple of years, efforts have been put into scaling up the number of languages included in multilingual training, particularly for the universal multilingual model (Johnson et al. 2017). The motivation is that increasing the number of languages should improve the performance for all language directions, thanks to the addition of extra data and to increased transfer between languages, particularly for low-resource language pairs. For example, Neubig and Hu (2018) trained a many-to-English model with 57 possible source languages, and more recent models have sought to include even more languages; Aharoni, Johnson, and Firat (2019) train an MT model for 102 languages to and from English as well as a many-tomany MT model between 59 languages, and Fan et al. (2021), Zhang et al. (2020a) and Arivazhagan et al. (2019) train many-to-many models for over 100 languages.\n\nWhile an impressive feat, the results show that it is non-trivial to maintain high translation performance across all languages as the number of language pairs is increased (Mueller et al. 2020;Aharoni, Johnson, and Firat 2019;Arivazhagan et al. 2019). There is a trade-off between transfer (how much benefit is gained from the addition of other languages) and interference (how much performance is degraded due to having to also learn to translate other languages) (Arivazhagan et al. 2019). It is generally bad news for high-resource language pairs, for which the performance of multilingual models is usually below that of language-direction-specific bilingual models. However, low-resource languages often do benefit from multilinguality, and the benefits are more noticeable for the many-to-English than for the English-to-many (Johnson et al. 2017;Arivazhagan et al. 2019). It has also been shown that for zero-shot translation, the more languages included in the training, the better the results are (Aharoni, Johnson, and Firat 2019;Arivazhagan et al. 2019), and that having multiple bridge pairs in the parallel data (i.e. not a single language such as English being the only common language between the different language pairs) greatly benefits zero-shot translation, even if the amount of non-English parallel data remains small Freitag and Firat 2020).\n\nThere is often a huge imbalance in the amount of training data available across language pairs and for low-resource language pairs, it is beneficial to upsample the amount of data. However, upsampling low-resource pairs has the unfortunate effect of harming performance on high-resource pairs (Arivazhagan et al. 2019), and there is the additional issue of the model overfitting on the low-resource data even before it has time to converge on the high-resource language data. A solution to this problem is the commonly used strategy of temperature-based sampling, which involves adjusting how much we sample from the true data distribution (Devlin et al. 2019;, providing a certain compromise between making sure low-resource languages are sufficiently represented but reducing the deterioration in performance seen in highresource language pairs. Temperature-based sampling can also be used when training the subword segmentation to create a joint vocabulary across all languages so that the low-resource languages are sufficiently represented in the joint vocabulary despite there being little data.\n\nSeveral works have suggested that the limiting factor is the capacity of the model (i.e. the number of parameters). Whilst multilingual training with shared parameters can increase transfer, increasing the number of languages decreases the per-task capacity of the model. Arivazhagan et al. (2019) suggest that model capacity may be the most important factor in the transfer-interference trade-off; they show that larger models (deeper or wider) show better translation performance across the board, deeper models being particularly successful for low-resource languages, whereas wider models appeared more prone to overfitting. Zhang et al. (2020a) show that online backtranslation combined with a deeper Transformer architecture and a special language-aware layer normalisation and linear transformations between the encoder and the decoder improve translation in a many-to-many setup.", "filtered_refids": [["b57", "b219"], [null, "b57", "b116", "b28"], [null], ["b219"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 4242, "num_references": 8}
{"corpusid_sectionid": "237371890-s17", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Multilingual Unsupervised Models.", "section": "As noted in Section 3.2.2, unsupervised MT performs quite poorly in low-resource language pairs, and one of the ways in which researchers have tried to improve its performance is by exploiting data from other languages. Sen et al. (2019b) demonstrate that a multilingual unsupervised NMT model can perform better than bilingual models in each language pair, but they only experiment with high-resource language pairs. Later works (Garcia et al. 2021;Ko et al. 2021) directly address the problem of unsupervised NMT for a low-resource language pair in the case where there is parallel data in a related language. More specifically, they use data from a third language (Z) to improve unsupervised MT between a lowresource language (X) and a high-resource language (Y ). In both works, they assume that X is closely related to Z, and that there is parallel data between Y and Z. As in the original unsupervised NMT models (Lample et al. 2018a;Artetxe et al. 2018), the training process uses denoising autoencoders and iterative backtranslation.", "filtered_refids": [["b90", "b31", null, "b69", "b176"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1041, "num_references": 5}
{"corpusid_sectionid": "237371890-s18", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Large-scale Multilingual pre-training", "section": "The success of large-scale pre-trained language models such as ELMo (Peters et al. 2018) and BERT (Devlin et al. 2019) has inspired researchers to apply related techniques to MT. Cross-lingual language models (XLM; Conneau and Lample 2019) are a direct application of the BERT masked language model (MLM) objective to learn from parallel data. The training data consists of concatenated sentence pairs, so that the model learns to predict the identity of the masked words from the context in both languages simultaneously. XLM was not applied to low-resource MT in the original paper, but was shown to improve unsupervised MT, as well as language modelling and natural language inference in low-resource languages.\n\nThe first really successful large-scale pre-trained models for MT were mBART ) and MASS (Song et al. 2019b), which demonstrated improvements to NMT in supervised, unsupervised and semi-supervised (i.e. with back-translation) conditions, including low-resource language pairs. The idea of these models is to train a noisy autoencoder using large collections of monolingual (i.e. not parallel) data in 2 or more languages. The autoencoder is a transformer-based encoder-decoder, and the noise is introduced by randomly masking portions of the input sentence. Once the autoencoder has been trained to convergence, its parameters can be used to initialise the MT model, which is trained as normal. Using mBART, Liu et al. (2020) were able to demonstrate unsupervised NMT working on the distant low-resource language pairs Nepali-English and Sinhala-English, as well as showing improvements in supervised NMT in lowresource language pairs such as Gujarati-English.\n\nThe original mBART was trained on 25 different languages and its inclusion in HuggingFace (Wolf et al. 2020) makes it straightforward to use for pre-training. It has since been extended to mBART50 (Tang et al. 2021), which is trained on a mixture of parallel and monolingual data, and includes 50 different languages (as the name suggests); mBART50 is also available on HuggingFace. A recent case study (Birch et al. 2021) has demonstrated that mBART50 can be combined with focused data gathering techniques to quickly develop a domain-specific, state-of-the-art MT system for a lowresource language pair (in this case, Pashto-English).\n\nA recent multilingual pre-trained method called mRASP ) has shown strong performance across a range of MT tasks: medium, low and very low-resource. mRASP uses unsupervised word alignments generated by MUSE (Conneau et al. 2018) to perform random substitutions of words with their translations in another language, with the aim of bringing words with similar meanings across multiple languages closer in the representation space. They show gains of up to 30 BLEU points for some very low-resource language pairs such as Belarusian-English. mRASP2 (Pan et al. 2021) extends this work by incorporating monolingual data into the training.\n\nOf course, pre-trained models are useful if the languages you are interested in are included in the pre-trained model, and you have the resources to train and deploy these very large models. On the former point, Muller et al. (2021) have considered the problem of extending multilingual BERT (mBERT) to new languages for natural language understanding tasks. They find greater difficulties for languages which are more distant from those in mBERT and/or have different scripts -but the latter problem can be mitigated with careful transliteration.", "filtered_refids": [["b138"], ["b184", "b104"], [null, "b208", "b190"], [null, "b134"], ["b117"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3498, "num_references": 9}
{"corpusid_sectionid": "237371890-s21", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Morphological segmentation.", "section": "A crucial part of training NMT system is the choice of subword segmentation, a pre-processing technique providing the ability to represent an infinite vocabulary with a fixed number of units and to better generalise over shorter units. For low-resource languages, it is even more important because there is a greater chance of coming across words that were not seen at training time. The most commonly used strategies are statistics-based, such as BPE (Sennrich, Haddow, and Birch 2016b) and sentencepiece (Kudo and Richardson 2018). Not only might these strategies not be optimal from a point of view of linguistic generalisation, but for low-resource languages especially they have also been shown to give highly variable results, depending on what degree of segmentation is selected; this degree is a parameter which therefore must be chosen wisely (Ding, Renduchintala, and Duh 2019;Sennrich and Zhang 2019).\n\nWorks exploring linguistic subword segmentation go back to statistical MT (Oflazer, Durgar El-Kahlout, and Durgar El-Kahlout 2007;Goldwater and McClosky 2005). Much of the focus has been on morphologically rich languages, with high degrees of inflection and/or compounding, for example for German, where minor gains can be seen over standard BPE (Huck, Riess, and Fraser 2017). Specifically for low-resource languages, several works have tested the use of morphological analysers to assist the segmentation of texts into more meaningful units. In their submission to the WMT19 shared task for English\u2192Kazakh, S\u00e1nchez-Cartagena, P\u00e9rez-Ortiz, and S\u00e1nchez-Mart\u00ednez (2019) use the morphological analyser from Apertium (Forcada and Tyers 2016) to segment Kazakh words into stem (often corresponding to the lemma in Kazakh) and the remainder of the word. They then learnt BPE over the morphological segmented data. Ortega, Castro Mamani, and Cho (2020) also use a BPE approach, guided by a list of suffixes, which are provided to the algorithm and are not segmented. They see better performance than using Morfessor or standard BPE. Saleva and Lignos (2021) also test morphologically aware subword segmentation for three low-resource language pairs: Nepali, Sinhala and Kazakh to and from English. They test segmentations using the LMVR (Ataman et al. 2017) and MORSEL (Lignos 2010) analysers, but found no gain over BPE and no consistent pattern in the results. These results go against previous results from Gr\u00f6nroos et al. (2014) that showed that an LMVR segmentation can outperform BPE when handling low-resource Turkish, but they are in accordance with more recent ones for Kazakh-English (Toral et al. 2019) and Tamil-English (Dhar, Bisazza, and van Noord 2020), where it does not seem to improve over BPE. (Garcia-Martinez, Barrault, and Bougares 2016;Sennrich and Haddow 2016;Burlot et al. 2017) were designed as a way of decomposing word units into component parts, which can help to provide some level of composite abstraction from the original wordform. For example, a wordform may be represented by its lemma and its part-of-speech, which together can be used to recover the original surface form. This type of modelling can be particularly useful for morphologically rich languages (many of which are already low-resource), for which the large number of surface forms can result in greater data sparsity and normally necessitate greater quantities of data.", "filtered_refids": [["b177", "b176", "b9", "b85"], ["b176", "b26", "b40", "b161", "b127", "b102", "b194", null, "b7", "b53", "b131", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3377, "num_references": 16}
{"corpusid_sectionid": "237371890-s22", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Factored models. Factored source and target representations", "section": "Factored models originated in SMT , but were notably not easy to scale. The advantage of factored representations in NMT is that the factors are represented in continuous space and therefore may be combined more easily, without resulting in an explosion in the number of calculations necessary. Garcia-Martinez, Barrault, and Bougares (2016), Sennrich and Haddow (2016) and Burlot et al. (2017) evaluate on language pairs involving at least one morphologically rich language and show that improvements in translation quality can be seen, but this is dependent on the language pair and the type of linguistic information included in the factors. N\u0203dejde et al. (2017) use factors to integrate source-side syntactic information in the form of CCG tags (Steedman 2000;Clark and Curran 2007), which they combine with an interleaving approach on the target side (see Section 5.1.4) to significantly improve MT performance, for high-resource (German\u2192English) and mid-low-resource (Romanian\u2192English) language directions.", "filtered_refids": [["b120", null, "b176", "b187"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 1013, "num_references": 4}
{"corpusid_sectionid": "237371890-s24", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Interleaving of linguistic information in the input.", "section": "As well as comparing factored representations and multi-task decoding, N\u0203dejde et al. (2017) also introduce a new way of integrating target-side syntactic information, which they call interleaving. The idea is to annotate the target side of the training data with token-level information (CCG supertags in their case) by adding a separate token before each token containing the information pertaining to it, so that the model learns to produce the annotations along with the translation. They found this to work better than multi-task for the integration of target-side annotations and was also complementary with the use of source factors. Inspired by these results, Tamchyna, Weller-Di Marco, and Fraser (2017) also followed the interleaving approach (for English\u2192Czech and English\u2192German, so not low-resource scenarios), but with the prediction of interleaved morphological tags and lemmas, followed by a deterministic wordform generator. Whereas N\u0203dejde et al. (2017) seek to create representations that are better syntactically informed, the aim of (Tamchyna, Weller-Di Marco, and Fraser 2017) is different: they aim to create a better generalisation capacity for translation into a morphologically rich language by decomposing wordforms into their corresponding tags and lemmas. They see significantly improved results with the two-step approach, but find that simply interleaving morphological tags (similar to N\u0203dejde et al. (2017)) does not lead to improvements. They hypothesise that the morphological tags are less informative than CCG supertags and therefore the potential gain in information is counterbalanced by the added difficulty of having to translate longer target sequences.\n\nIn a systematic comparison with both RNN and transformer architectures and for 8 language directions (and in particular for low-resource languages), S\u00e1nchez-Cartagena, P\u00e9rez-Ortiz, and S\u00e1nchez-Mart\u00ednez (2020) find that interleaving (with part-of-speech information and morphological tags) is beneficial, in line with the conclusions from N\u0203dejde et al. (2017). Interestingly, they find that (i) interleaving linguistic information in the source sentence can help, and morphological information is better than PoS tags and (ii) interleaving in the target sentence can also help, but PoS tagging is more effective than morphological information, despite the translations being more grammatical with added morphological information.", "filtered_refids": [["b120", "b189"], ["b120"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2426, "num_references": 3}
{"corpusid_sectionid": "237371890-s26", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Bilingual lexicons", "section": "Bilingual lexicons are lists of terms (words or phrases) in one language associated with their translations in a second language. The advantage of bilingual lexicons is that they may well provide specialist or infrequent terms that do not appear in available parallel data, with the downside that they do not give information about the translation of terms in context, notably when there are several possible translations of a same term. However, they may be important resources to exploit, since they provide complementary information to parallel data and may be more readily available and cheaper to produce. 16 The approaches developed so far to exploit bilingual lexicons to directly improve NMT can be summarised as follows: (i) as seed lexicons to initialise unsupervised MT (Lample et al. 2018b;Duan et al. 2020) (as described in Section 3.2.2), (ii) as an additional scoring component, particularly to provide coverage for rare of otherwise unseen vocabulary (Arthur, Neubig, and Nakamura 2016; Feng et al. 2017) and (iii) as annotations in the source sentence by adding translations from lexicons just after their corresponding source words (Dinu et al. 2019) 17 or by replacing them in a codeswitching-style setup (Song et al. 2019a).\n\nThe most recent work on using lexicons in pretrained multilingual models (Lin et al. 2020, mRASP) shows the most promise. Here translations of words are substituted into the source sentence in pretraining, with the goal of bringing words with similar meanings across multiple languages closer in the representation space. Please see Section 4.3 for more details.", "filtered_refids": [["b12", "b183", "b91", "b21", null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1608, "num_references": 6}
{"corpusid_sectionid": "237371890-s28", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Meta Learning", "section": "In Section 4 we discussed using multilingual training to improve low-resource MT by combining training sets for different language pairs in joint-learning or transfer learning schemes. A more extreme form of this approach involves the application of meta learning: rather than training a system to directly perform well on a single task or fixed set of tasks (language pairs in our case), a system can be trained to quickly adapt to a novel task using only a small number of training examples, as long as this task is sufficiently similar to tasks seen during (meta-)training.\n\nOne of the most successful meta-learning approaches is Model-Agnostic Meta-Learning (MAML) (Finn, Abbeel, and Levine 2017), which was applied to multilingual MT by Gu et al. (2018). In MAML, we train task-agnostic model parameters\u03b8 so that they can serve as a good initial value that can be further optimised towards a taskspecific parameter vector * \u03b8 m based on a task-specific training set D m . This is accomplished by repeatedly simulating the fine-tuning procedure, evaluating each fine-tuned model on its task-specific evaluation set and then updating the task-agnostic parameters in the direction that improves this score.\n\nOnce training is completed, the fine-tuning procedure can be directly applied to any novel task. Gu et al. (2018) apply MAML by meta-training on synthetic low-resource tasks obtained by randomly subsampling parallel corpora of high-resource language pairs and then fine-tune on true low-resource language pairs, obtaining substantial improvements.\n\nAn alternative approach to meta-learning involves training memory-augmented networks that receive the task-specific training examples at execution time and maintain a representation of them which they use to adapt themselves on the fly Santoro et al. 2016), an approach related to the concept of \"fast weights\" computed at execution time as opposed to \"slow weights\" (the model parameters) computed at training time (Schmidhuber 1992). Lake (2019) applied memory-augmented networks to synthetic sequence-to-sequence tasks in order to evaluate out-of-distribution generalisation under a variety of conditions. Curiously, very large language models such as GPT-2 (Radford et al. 2019) and in particular GPT-3 (Brown et al. 2020) also exhibit this meta-learning capability even without any modification of the network architecture or training procedure, suggesting that meta-learning itself can be learned from a sufficient large amount of data. In fact, GPT-3 achieves near-SOTA quality when translating into English even with a single translation example, for multiple source languages including Romanian, a medium-low resource language.", "filtered_refids": [[], ["b22", "b41"], ["b41"], ["b170", "b149", "b167", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2695, "num_references": 7}
{"corpusid_sectionid": "237371890-s30", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Alternative training objectives", "section": "When an autoregressive model is trained to optimise the cross-entropy loss, it is only exposed to ground-truth examples during training. When this model is then used to generate a sequence with ancestral sampling, beam search or another inference method, it has to incrementally extend a prefix that it has generated itself. Since the model in general cannot learn exactly the \"true\" probability distribution of the target text, the target prefix that it receives as input will be out-of-distribution, which can cause the estimate of the next token probability to become even less accurate. This issue, named exposure bias by Ranzato et al. (2016), can compound with each additional token and might result in the generated text to eventually become completely nonsensical. Exposure bias theoretically occurs regardless of the task, but while its impact has been argued to be small in high-resource settings , in low-resource MT it has been shown to be connected to the phenomenon of hallucination, where the system generates translations that are partially fluent but contain spurious information not present in the source sentence (Wang and Sennrich 2020).\n\nA number of alternatives to cross-entropy training have been proposed in order to avoid exposure bias, which all involve exposing the model during training to complete or partial target sequences generated by itself. Ranzato et al. (2016) explore multiple training strategies and propose a method called MIXER which is a variation of the REINFORCE algorithm (Williams 1992;Zaremba and Sutskever 2015). In practice REINFORCE suffers from high variance, therefore they apply it only after the model has already been pre-trained with cross-entropy, a technique also used by all the other training methods described in this section. They further extend the algorithm by combining cross-entropy training and REINFORCE within a each sentence according to a training schedule which interpolates from full cross-entropy to full REINFORCE. They do not evaluate on a true low-resource language pair, but they do report improvements on German\u2192English translation on the relatively small IWSLT 2014 dataset (Cettolo et al. 2014).\n\nContrastive Minimum Risk Training (CMRT or just MRT) (Och 2003; Shen et al. 2016;Edunov et al. 2018b) is a similar training technique that can be considered a biased variant of REINFORCE that focuses on high-probability translations generated by decoding from the model itself. Wang and Sennrich (2020) apply CMRT to low-resource translation (German\u2192Romansh) as well as German\u2192English IWSLT 2014, reporting improvements in the out-of-domain test case, as well as a reduction of hallucinations.\n\nBoth REINFORCE and CMRT use a reward function that measures the similarity between generated and reference translations, often based on an automatic evaluation metric such as BLEU. However, the exact mechanism that makes such approaches work is not completely clear, Choshen et al. (2020) show that REINFORCE and CMRT also work when the reward function is a trivial constant function rather than a sentence similarity metric, suggesting that their primary effect is to regularise the model pretrained with cross-entropy by exposing it to its own translations, hence reducing exposure bias.\n\nAn alternative training technique involving beam search decoding in the training loop has been proposed by Wiseman and Rush (2016), based on the LaSO (Daum\u00e9 and Marcu 2005) structured learning algorithm. This approach also exposes the model to its own generations during training, and it has the benefit that training closely matches the inference process, reducing any mismatch. The authors report improvements on German\u2192English IWSLT 2014. However they do not evaluate on a true low-resource language pair.\n\nAn even simpler technique that exposes the model's own generations during training is scheduled sampling , which also starts with crossentropy training and progressively replaces part of the ground truth target prefixes observed by the model with its own samples. Plain scheduled sampling is theoretically unsound (Huszar 2015), but it can be made more consistent by backpropagating gradients through a continuous relaxation of the sampling operation, as shown by Xu, Niu, and Carpuat (2019) who report improvements on the low-resource Vietnamese\u2192English language pair. Regularisation techniques have also been applied to low-resource MT. Sennrich and Zhang (2019) evaluated different hyperparameter settings, in particular batch size and dropout regularisation, for German\u2192English with varying amounts of training data and low-resource Korean\u2192English. M\u00fcller, Rios, and Sennrich (2020) experimented with various training and inference techniques for out-of-distribution MT both for a high-resource (German\u2192English) and low-resource (German\u2192Romansh) pair. For the low-resource pair they report improvements by using sub-word regularisation (Kudo 2018), defensive distillation and source reconstruction. An alternate form of subword regularisation, known as BPE dropout has been proposed by Provilkov, Emelianenko, and Voita (2019), reporting improvements on various high-resource and low-resource language pairs. He, Haffari, and Norouzi (2020) apply a dynamic programming approach to BPE subword tokenisation, evaluating during training all possible ways of tokenising each target word into subwords, and computing an optimal tokenisation at inference time. Since their method is quite slow however, they only use it to tokenise the training set and then train a regular Transformer model on it, combining it with BPE dropout on source words, reporting improvements on high-resource and mediumresource language pairs.", "filtered_refids": [["b155", "b201"], ["b155", "b206", null, "b217"], ["b16", "b201", "b178"], ["b3"], ["b207"], ["b214", "b55", "b84"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 5692, "num_references": 14}
{"corpusid_sectionid": "237371890-s31", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Alternative inference algorithms", "section": "In NMT, inference is typically performed using a type of beam search algorithm with heuristic length normalisation 20 (Jean et al. 2015;Koehn and Knowles 2017). Ostensibly, beam search seeks to approximate maximum a posteriori (MAP) inference, however it has been noticed that increasing the beam size, which improves the accuracy of the approximation, often degrades translation quality after a certain point (Koehn and Knowles 2017). It is actually feasible to exactly solve the maximum a posteriori inference problem, and the resulting mode is often an abnormal sentence; in fact, it is often the empty sentence (Stahlberg and Byrne 2019). It is arguably dismaying that NMT relies on unprincipled inference errors in order to generate accurate translations. Various authors have attributed this \"beam search paradox\" to modelling errors caused by exposure bias or other training issues and they have proposed alternative training schemes such as these discussed in section 6.3. Even a perfect probabilistic model, however, could well exhibit this behaviour due to a counter-intuitive property of many high-dimensional random variables that causes the mode of the distribution to be very different from typical samples, which have a log-probability close to the entropy of the distribution. (See Cover and Thomas (2006) for a detailed discussion of typicality from an information theory perspective). Eikema and Aziz (2020) recognise this issue in the context of NMT and tackle it by applying Minimum Bayes Risk (MBR) inference (Goel and Byrne 2000).\n\nMinimum Bayes Risk seeks to generate a translation which is maximally similar, according to a metric such as BLEU or METEOR (Denkowski and Lavie 2011), to other translations sampled from the model itself, each weighted according to its probability. The intuition is that the generated translation will belong to a high-probability cluster of similar candidate translations; highly abnormal translations such as the empty sentence will be excluded. Eikema and Aziz (2019) report improvements over beam search on the low-resource language pairs of the FLoRes dataset (Nepali-English and Sinhala-English)  while they lose some accuracy on English-German. They also evaluate inference though ancestral sampling, the simplest and theoretically least biased inference technique, but they found that it performs worse than both beam search and MBR.\n\nEnergy-based models (EBMs) (LeCun et al. 2006) are alternative representations of a probability distribution which can be used for inference. An ERM of a random variable (a whole sentence, in our case) is a scalar-valued energy function, implemented as a neural network, which represents an unnormalised log-probability. This lack of normalisation means that only probability ratios between two sentences can be computed efficiently, for this reason training and sampling from EBMs requires a proposal distribution to generate reasonably good initial samples to be re-ranked by the model, in the context of MT this proposal distribution is a conventional autoregressive NMT model. Bhattacharyya et al. (2021) define source-conditional or joint EBMs trained on ancestral samples from an autoregressive NMT model using a reference-based metric (e.g. BLEU). During inference they apply the EBM to re-rank a list of N ancestral samples from the autoregressive NMT model. This approximates MAP inference on a probability distribution that tracks the reference-based metric, which would not give high weight to abnormal translations such as the empty sentence. They report improvements on multiple language pairs, in particular for medium-resource and low-resource language pairs such as Romanian, Nepali, and Sinhala to English.\n\nReranking has been also applied under the generalised noisy channel model initially developed for SMT (Koehn, Och, and Marcu 2003), where translations are scored not just under the probability of the target conditional on the source (direct model) but also under the probability of the source conditional on the target (channel model) and the unconditional probability of the target (language model prior), combined by a weighted sum of their logarithms. This reranking can be applied at sentence level on a set of candidate translations generated by the direct model by conventional beam search  or at token level interleaved with beam search (Bhosale et al. 2020), resulting in improvements in multiple language pairs including low-resource ones.", "filtered_refids": [["b80", "b56", "b35", "b17"], [null], ["b94", null], ["b82"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 4470, "num_references": 8}
{"corpusid_sectionid": "237371890-s32", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Rule-based approaches", "section": "Rule-based machine translation (RBMT) consists in analysing and transforming a source text into a translation by applying a set of hand-coded linguistically motivated rules. This was the oldest and the most common paradigm for machine translation before being largely supplanted by corpus-based approaches such as phrase-based statistical machine translation and neural machine translation, which usually outperform it on both accuracy and fluency, especially when translating between language pairs where at least one language is high-resource, such as English. However, rule-based techniques can still be successfully applied to the task of translation between closely related languages.\n\nModern implementations, such as the Apertium system (Forcada et al. 2011;Forcada and Tyers 2016;Khanna et al. 2021), use lexical translation and shallow transfer rules that avoid full parsing and instead exploit the similarities between the source and target language to restructure a sentence into its translation. This approach has been applied to various language pairs, especially in the Western Romance and the South Slavic sub-families. NMT approaches tend to have better fluency than RBMT but they can produce hallucinations in low-resource settings where RBMT can instead benefit from lexical translation with explicit bilingual dictionaries, thus a line of research has developed that attempts to combine both approaches. For instance S\u00e1nchez-Cartagena,  used multi-source Transformer and deep-GRU (Miceli Barone et al. 2017) models to post-edit translations produced by a RBMT system for the Breton-French language pair.\n\nOne of the main drawbacks of RBMT is that it requires substantial language-specific resources and expertise which might not be available for all low-resource languages. See Section 5 for a discussion of other methods to use various linguistic resources that might be more readily available.", "filtered_refids": [[], ["b26", null, "b113", "b25"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1913, "num_references": 4}
{"corpusid_sectionid": "237371890-s33", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Evaluation", "section": "As researchers build different MT systems in order to try out new ideas, how do they know whether one is better than another? If a system developer wants to deploy an MT system, how do they know which is the best? Answering these questions is the goal of MT evaluation -to provide a quantitative estimate of the quality of an MT system's output. MT evaluation is a difficult problem, since there can be many possible correct translations of a given source sentence. The intended use is an important consideration in evaluation; if translation is mainly for assimilation (gisting) then adequacy is of primary importance and errors in fluency can be tolerated; but if the translation is for dissemination (with post-editing) then errors in meaning can be corrected, but the translation should be as close to a publishable form as possible. Evaluation is not specific to low-resource MT; it is a problem for all types of MT research, but low-resource language pairs can present specific difficulties for evaluation.\n\nEvaluation can either be manual (using human judgements) or automatic (using software). Human judgements are generally considered the \"gold standard\" for MT evaluation because, ultimately, the translation is intended to be consumed by humans. The annual WMT shared tasks have employed human evaluation every year since they started in 2006, and the organisers argue that (Callison-Burch et al. 2007):\n\nWhile automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are an imperfect substitute for human assessment of translation quality.\n\nHuman evaluation is of course much more time-consuming (and therefore expensive) than automatic evaluation, and so can only be used to compare a small number of variants; with most of the system selection performed by automatic evaluation. For low-resource MT, the potential difficulty with human evaluation is connecting the researchers with the evaluators. Some low-resource languages have very small language communities, so the pool of potential evaluators is small, whilst in other cases the researchers may not be well connected with the language community -in that case the answer should be for the researchers to engage more with the community (Nekoto et al. 2020).\n\nMost of the evaluation in MT is performed using automatic metrics, but when these metrics are developed they need to be validated against human judgements as the gold standard. An important source of gold standard data for validation of metrics is the series of WMT metrics shared tasks (Freitag et al. 2021b). However this data covers the language pairs used in the WMT news tasks, whose coverage of low-resource languages is limited to those listed in Table 3. It is unclear how well conclusions about the utility of metrics will transfer from high-resource languages to low-resource languages.\n\nAutomatic metrics are nearly always reference-based, in other words they work by comparing the MT hypothesis with a human-produced reference. This means that references need to be available for the chosen language pair, they should be of good quality, and ideally should be established benchmarks used by the research community. Such references are in short supply for low-resource language pairs (Section 2), and when available may be in the wrong domain, small, or of poor quality.\n\nLooking at the automatic metrics in common use in current MT research, we see two main types of metrics being used in current research: string-based metrics (e.g. BLEU (Papineni et al. 2002;Post 2018), ChrF (Popovi\u0107 2015) etc.) and embedding-based metrics (e.g. BERTscore , COMET (Rei et al. 2020), BLEURT (Sellam, Das, and Parikh 2020) etc.). The string-based metrics are \"low-resource metrics\", since they do not require any resources beyond tokenisation/segmentation, whereas the embeddingbased metrics require more significant resources such as pre-trained sentence embeddings, and often need human judgements for fine-tuning. The embedding-based metrics could be considered successors to metrics like METEOR (Denkowski and Lavie 2014), which uses synonym list to improve matching between hypothesis and reference.\n\nRecent comparisons (Freitag et al. 2021b;Kocmi et al. 2021) have suggested that embedding-based metrics have superior performance, in other words that they correlate better with human judgements than string-based metrics. However, since embeddingbased metrics generally rely on sentence embeddings, they only work when such embeddings are available, and if they are fine-tuned on human evaluation data, they may not perform as well when such data is not available. For instance COMET is based on the XML-R embeddings (Conneau et al. 2020), so can support the 100 languages supported by XML-R, but will not give reliable results for unsupported languages.\n\nString-based metrics (such as BLEU and ChrF) will generally support any language for reference-based automatic evaluation. BLEU has been in use in MT evaluation for many years, and its benefits and limitations are well-studied; ChrF has a much shorter history, but recent comparisons suggest that it performs better than BLEU (see references above) and its use of character n-grams probably make it more suited to the morphological complexity found in many low-resource languages. A recent debate in the MT evaluation literature has been about the ability for automatic metrics to discern differences in high-quality MT systems , Figure 6). However in lowresource MT, we may be faced with the opposite problem, i.e. metrics may be less reliable when faced with several low-quality systems (Fomicheva and Specia 2019).\n\nIn conclusion, for evaluation of low-resource MT, we recommend human evaluation as the gold standard, but where automatic evaluation is used, to be especially wary of the lack of calibration of metrics and the potential unreliability of test sets for lowresource language pairs.", "filtered_refids": [[], ["b78"], [], [null], [null], [], ["b135", "b157", "b143", null, "b142"], [null], ["b24"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 31, "num_chars": 5937, "num_references": 10}
{"corpusid_sectionid": "237371890-s37", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Data preparation.", "section": "An important initial step to training an NMT model is to identify available data (See Section 2) and to potentially filter it depending on the noisiness of the dataset and how out-of-domain it is or to use an alternative strategy to indicate domain or data quality (i.e. tagging). So what choices do participants tend to make in terms of using (or excluding) data sources, filtering and cleaning of data and using meta-information such as domain tags?\n\nChoice of data. We focus on constrained submissions only (i.e. where participants can only use the data provided by the organisers), so most participants use all available data. Hesitancy can sometimes be seen with regards to web-crawled data (other than WMT newscrawl, which is generally more homogeneous and therefore of better quality), some choosing to omit the data (Singh 2020) and others to filter it for quality . It is very unusual to see teams do their own crawling (Hernandez and Nguyen (2020) is a counter-example); teams doing so run the risk of crawling data that overlaps with the development set or one side of the test set. Tran et al. (2021) successfully mined an extra million sentences pairs of Hausa-English data from the allowed monolingual data, helping them win the 2021 task.\n\nData cleaning and filtering. Although not exhaustively reported, many of the submissions apply some degree of data cleaning and filtering to the parallel and monolingual data. In its simplest form, this means excluding sentences based on their length (if too long) and the ratio between the lengths of parallel sentences (if too different). Some teams also remove duplicates (e.g. Li et al. (2019a)). More rigorous cleaning includes eliminating sentences containing fewer than a specified percentage of alpha-numeric characters in sentences (depending on the language's script), those identified as belonging to another language (e.g. using language identification) or those less likely to belong to the same distribution as the training data (e.g. using filtering techniques such as Moore-Lewis (Moore and Lewis 2010)). Data filtering is also a commonly used technique for backtranslation data (see the paragraph on data augmentation below), often using similar filtering techniques such as dual conditional cross-entropy filtering (Junczys-Dowmunt 2018) to retain only the cleanest and most relevant synthetic parallel sentences. Unfortunately the effect of data filtering is rarely evaluated, probably because it would involve expensive re-training.\n\nData tagging. Some teams choose to include meta-information in their models through the addition of pseudo-tokens. For example, Dutta et al. (2020) choose to tag sentences according to their quality for the Upper Sorbian-German task, this information being provided by the organisers. Domain tagging (i.e. indicating the type of data), which can be useful to indicate whether data is in-domain or out-of-domain was used by Chen et al. (2020), one of the top-scoring systems for Tamil-English. For the Basque-English task, Scherrer (2018) find that using domain tags gives systematic improvements over not using them, and Knowles et al. (2020a) come to the same conclusion when translating into Inuktitut.", "filtered_refids": [[], ["b180", "b190"], ["b96"], ["b1", "b168", "b13", "b67"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3213, "num_references": 7}
{"corpusid_sectionid": "237371890-s38", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Data pre-processing.", "section": "There is some variation in which data pre-processing steps are used. For example, it has been shown that for high-resource language pairs such as Czech-English, it is not always necessary to applying tokenisation and truecasing steps (Bawden et al. 2019) before apply subword segmentation. We do not observe a clear pattern, with many systems applying all steps, and some excluding tokenisation  for Tamil) and truecasing. Among the different possible pre-processing steps, we review participants choices concerning tokenisation, subword segmentation and transliteration/alphabet mapping (relevant when translating between languages that use different scripts).\n\nTokenisation. If a tokeniser is used before subword segmentation, it is common for it to be language-specific, particularly for the low-resource language in question. For example IndicNLP 22 (Kunchukuttan 2020) is widely used for Indian languages (e.g. for the shared tasks involving Gujarati and Tamil), and many of the Khmer-English submissions also used Khmer-specific tokenisers. For European languages, the Moses tokeniser ) remains the most commonly used option.\n\nSubword segmentation. All participants perform some sort of subword segmentation, with most participants using either sentencepiece (Kudo and Richardson 2018) 23 or subword_nmt toolkits (Sennrich, Haddow, and Birch 2016b). 24 Even though the BPE toolkit is not compatible with the Abugida scripts used for Gujarati, Tamil and Khmer (in these scripts, two unicode codepoints can be used to represent one glyph), we only found one group who modified BPE to take this into account (Shi et al. 2020). BPE-dropout (Provilkov, Emelianenko, and Voita 2020), a regularisation method, was found to be useful by a number of teams (Knowles et al. 2020b;Libovick\u00fd et al. 2020;Chronopoulou et al. 2020). The size of the subword vocabulary is often a tuned parameter, although the range of different values tested is not always reported. Surprisingly, there is significant variation in the subword vocabulary sizes used, and there is not always a clear pattern. Despite the low-resource settings, many of the systems use quite large subword vocabularies (30k-60k merge operations). There are exceptions: a large number of the systems for Tamil-English use small vocabularies (6k-30k merge operations), which may be attributed to the morphologically rich nature of Tamil coupled with the scarcity of data.\n\nJoint subword segmentation is fairly common. Its use is particularly well motivated when the source and target languages are similar where we may expect to see a high amount of lexical overlap (e.g. for the similar language shared tasks such as Upper Sorbian-German) and when 'helper languages' are used to compensate for the lowresource scenario (e.g. addition of Czech and English data). However, it is also used in some cases even where there is little lexical overlap, for example for Tamil-English, where the languages do not share the same script, including by some of the top-scoring systems (Shi et al. 2020;Wu et al. 2018). Although few systematic studies are reported, one hypothesis could be that even if different scripts are used there is no disadvantage to sharing segmentation; it could help with named entities and therefore reducing the overall vocabulary size of the model (Ding, Renduchintala, and Duh 2019).\n\nA few works explore alternative morphology-driven segmentation schemes, but without seeing any clear advantage: Scherrer, Gr\u00f6nroos, and Virpioja (2020) find that, for Upper-Sorbian-German, Morfessor can equal the performance of BPE when tuned correctly (but without surpassing it), whereas S\u00e1nchez-Cartagena (2018) find gains for Morfessor over BPE. Dhar, Bisazza, and van Noord (2020) have mixed results for Tamil-English when comparing linguistically motivated subword units compared to the use of statistics-based sentencepiece (Kudo and Richardson 2018).\n\nTransliteration and alphabet mapping. Transliteration and alphabet mapping has been principally used in the context of exploiting data from related languages that are written in different scripts. This was particularly present for translation involving Indian languages, which often have their own script. For the Gujarati-English task, many of the top systems used Hindi-English data (see below the paragraph on using other language data) and performed alphabet mapping into the Gujarati script (Li et al. 2019b;Bawden et al. 2019;Dabre et al. 2019). For Tamil-English,  found that when using Hindi in a multilingual setup, it helped for Hindi to be mapped into the Tamil script for the Tamil\u2192English direction, but did not bring improvements for English\u2192Tamil. Transliteration was also used in the Kazakh-English task, particularly with the addition of Turkish as higher-resourced language. Toral et al. (2019), a topscoring system, chose to cyrillise Turkish to increase overlap with Kazakh, whereas Briakou and Carpuat (2019) chose to romanise Kazakh to increase the overlap with Turkish, but only for the Kazakh\u2192English direction.", "filtered_refids": [[], ["b87"], ["b6", "b179", "b101", "b176", "b68"], ["b210", "b179", "b9"], ["b7", "b169", "b85"], ["b194", null, "b97", "b122"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 5049, "num_references": 16}
{"corpusid_sectionid": "237371890-s40", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "SMT versus NMT.", "section": "There is little doubt that NMT has overtaken SMT, even in the lowresource tasks. The majority of submissions use neural MT models and more specifically transformers (rather than recurrent models). Some teams compare SMT and NMT (Dutta et al. 2020;Sen et al. 2019a) with the conclusion that NMT is better when sufficient data is available, including synthetic data. Some teams use SMT only for backtranslation, on the basis that SMT can work better (or at least be less susceptible to hallucinating) on the initial training using a very small amount of parallel data. For SMT systems, the most commonly used toolkit is Moses , whereas there is a little more variation for NMT toolkits; the most commonly used being Fairseq ), Marian (Junczys-Dowmunt et al. 2018a, OpenNMT (Klein et al. 2017) and Sockeye (Hieber et al. 2020).\n\nModel size. Although systematic comparisons are not always given, some participants did indicate that architecture size was a tuned parameter , although this can be computationally expensive and therefore not a possibility for all teams. The model sizes chosen for submissions varies, and there is not a clear and direct link between size and model performance. However there are some general patterns worth commenting on. While many of the baseline models are small (corresponding to transformer-base or models with fewer layers), a number of high-scoring teams found that it was possible to train larger models (e.g. deeper or wider) as long as additional techniques were used, such as monolingual pretraining  or additional data from other languages in a multilingual setup or after synthetic data creation through pivoting (Li et al. 2019b) through a higher-resource language or backtranslation Li et al. 2019b). For example the Facebook AI team , who fine-tuned for model architecture, started with a smaller transformer (3 layers and 8 attention heads) for their supervised English\u2192Tamil baseline, but were able to increase this once backtranslated data was introduced (to 10 layers and 16 attention heads). Although some systems perform well with a transformer-base model (Bawden et al. 2019 for Tamil-English), many of the best systems use larger models, such as the transformer-big (Hernandez and Nguyen 2020;Kocmi 2020;Bei et al. 2019;Wei et al. 2020;Chen et al. 2020).\n\nAlternative neural architectures. Other than variations on the basic transformer model, there are few alternative architectures tested. Wu et al. (2020) tested the addition of dynamic convolutions to the transformer model following Wu et al. (2019), which they used along with other wider and deep transformers in model ensembles. However they did not compare the different models. Another alternative form of modelling tested by several teams was factored representations (see Section 5.1.2). Dutta et al. (2020) explored the addition of lemmas and part-of-speech tags for Upper-Sorbian-German but without seeing gains, since the morphological tool used was not adapted to Upper-Sorbian. For Basque-English, Williams et al. (2018) find that source factors indicating the language of the subword can help to improve the baseline system.\n\nTraining parameters. Exact training parameters are often not provided, making comparison difficult. Many of the participants do not seem to choose training parameters that are markedly different from the higher-resource settings (Zhang et al. 2020c;Wu et al. 2020).", "filtered_refids": [["b175", "b13", "b66", null, "b51"], ["b97", "b203", "b50", null, "b213", "b70", "b1"], ["b13", "b205", "b209", "b211"], ["b224", "b211"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3410, "num_references": 18}
{"corpusid_sectionid": "237371890-s41", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Using additional data.", "section": "Much of this survey has been dedicated to approaches for the exploitation of additional resources to compensate for the lack of data for lowresource language pairs: monolingual data (Section 3), multilingual data (Section 4) or other linguistic resources (Section 5). In shared tasks, the following approaches have been shown to be highly effective to boosting performance in low-resource scenarios.\n\nBacktranslation. The majority of high-performing systems carry out some sort of data augmentation, the most common being backtranslation, often used iteratively, although forward translation is also used (Shi et al. 2020;Chen et al. 2020;Zhang et al. 2020c;Wei et al. 2020). For particularly challenging language pairs (e.g. for very low-resource between languages that are not very close), it is important for the initial model that is used to produce the backtranslations to be of sufficiently high quality. For example, some of the top Gujarati-English systems employed pretraining before backtranslation to boost the quality of the initial model (Bawden et al. 2019;Bei et al. 2019). Participants do not always report the number of iterations of backtranslations performed, however those that do often cite the fact that few improvements are seen beyond two iterations . Tagged backtranslation, whereby a pseudo-token is added to sentences that are backtranslated to distinguish then from genuine parallel data have previously shown to provide improvements (Caswell, Chelba, and Grangier 2019). Several participants report gains thanks to the addition of backtranslation tags Chen et al. 2020;Knowles et al. 2020a), although  find that tagged backtranslation under-performs normal backtranslation in a multilingual setup for Tamil-English.\n\nSynthetic data from other languages. A number of top-performing systems successfully exploit parallel corpora from related languages. The two top-performing systems for Gujarati-English use a Hindi-English parallel corpus to create synthetic Gujarati-English data (Li et al. 2019b;Bawden et al. 2019). Both exploit the fact that there is a high degree of lexical overlap between Hindi and Gujarati once Hindi has been transliterated into Gujarati script. Li et al. (2019b) choose to transliterate the Hindi side and then to select the best sentences using cross-entropy filtering, whereas Bawden et al. (2019) choose to train a Hindi\u2192Gujarati model, which they use to translate the Hindi side of the corpus. Pivoting through a higher-resource related language was also found to be useful for other language pairs: for Kazakh-English, Russian was the language of choice (Li et al. 2019b;Toral et al. 2019;Dabre et al. 2019;Budiwati et al. 2019), for Basque-English, Spanish was used as a pivot (Scherrer 2018;, which was found to be more effective than backtranslation by Scherrer (2018), and was found to benefit from additional filtering by .\n\nTransfer-learning using language modelling objectives. The top choices of language modelling objectives are mBART ) (used by Chen et al. (2020) andBawden et al. (2020) for Tamil Kvapil\u00edkov\u00e1, Kocmi, and Bojar (2020) and Dutta et al. (2020) for Upper-Sorbian-German), and MASS (Song et al. 2019b) (used by  and Singh, Singh, and Bandyopadhyay (2020) for Upper Sorbian-German). Some of the top systems used these language modelling objectives, but their use was not across the board, and pretraining using translation objectives was arguably more common. Given the success of pretrained models in NLP, this could be surprising. A possible explanation for these techniques not being used systematically is that they can be computationally expensive to train from scratch and the constrained nature of the shared tasks means that the participants are discouraged from using pretrained language models.\n\nTransfer learning from other MT systems. Another commonly used technique used by participants was transfer learning involving other language pairs. Many of the teams exploited a high-resource related language pair. For example, for Kazakh-English, pretraining was done using Turkish-English (Briakou and Carpuat 2019) and Russian-English (Kocmi and Bojar 2019), Dabre et al. (2019) pretrained for Gujarati-English using Hindi-English, and Czech-German was used to pretrain for Upper-Sorbian-German (Knowles et al. 2020b).\n\nAn alternative but successful approach was to use a high-resource but not necessarily related language pair. For example, the CUNI systems use Czech-English to pretrain Inuktitut (Kocmi 2020) and Gujarati (Kocmi andBawden et al. (2020) found pretraining on English-German to be as effective as mBART training for Tamil-English. Finally, a number of teams opted for multilingual pretraining, involving the language pair in question and a higher-resource language or several higher-resource languages. Wu et al. (2020) use the mRASP approach: a universal multilingual model involving language data for English to and from Pashto, Khmer, Tamil, Inuktitut, German and Polish, which is then fine-tuned to the individual low-resource language pairs. Multilingual models. Other than the pretraining strategies mentioned just above, multilingual models feature heavily in shared task submissions. The overwhelmingly most common framework used was the universal encoder-decoder models as proposed by Johnson et al. (2017). Some participants chose to include select (related) languages. Williams et al. (2018) use Spanish to boost Basque-English translation and find that the addition of French data degrades results. Goyal and Sharma (2019) add Hindi as an additional encoder language for Gujarati-English and for Tamil-English, they test adding Hindi to either the source or target side depending on whether Tamil is the source or target language . Other participants choose to use a larger number of languages. Zhang et al. (2020c) train a multilingual system on six Indian languages for Tamil-English and Hokamp, Glover, and Gholipour Ghalandari (2019) choose to train a multilingual model on all WMT languages for Gujarati-English (coming middle in the results table). Upsampling the lower-resourced languages in the multilingual systems is an important factor, whether the multilingual system is used as the main model or for pretraining (Zhang et al. 2020c;Wu et al. 2020). Recent approaches has seen success using more diverse and even larger numbers of languages. Tran et al. (2021) train a model for 14 diverse language directions, winning 10 of them (although their system is unconstrained). Their two models, many to English and English to many, used a Sparsely Gated Mixture-of-Expert (MoE) models (Lepikhin et al. 2020). The MoE strike a balance between allowing high-resource directions to benefit from increased model capacity, while also allowing transfer to low-resource directions via shared capacity. Microsoft's winning submission to the 2021 large scale multilingual task (Yang et al. 2021) covered 10k language pairs across the FLORES-101 data set. They use the public available DeltaLM-Large model ), a multilingual pre-trained encoderdecoder model, and apply progressive learning (Zhang et al. 2020b) (starting training with 24 encoder layers and adding 12 more) and iterative back-translation.", "filtered_refids": [[], ["b67", "b179", "b203", "b224", null, "b213", "b1"], ["b97", "b122", "b194", null, "b168"], ["b184", "b13", "b181", "b88", null, "b1"], ["b68", "b72", "b122"], ["b216", "b205", "b57", "b39", "b211", "b223", null, "b224", "b95", "b190", "b70"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 42, "num_chars": 7221, "num_references": 32}
{"corpusid_sectionid": "237371890-s42", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Model transformation and finalisation.", "section": "Additional techniques, not specific to lowresource MT, are often applied in the final stages of model construction, and they can provide significant gains to an already trained model. We regroup here knowledge distillation (which we consider as a sort of model transformation) and both model combination and fine-tuning (which can be considered model finalisation techniques).\n\nKnowledge distillation. Knowledge distillation is also a frequently used technique and seems to give minor gains, although is not as frequently used as backtranslation or ensembling. Knowledge distillation (Kim and Rush 2016) leverages a large teacher model to train a student model. The teacher model is used to translate the training data, resulting in synthetic data on the target side. A number of teams apply this iteratively, in combination with backtranslation  or fine-tuning (Li et al. 2019b). Bei et al. (2019) mix knowledge distillation data with genuine and synthetic parallel data to train a new model to achieve gains in BLEU.\n\nModel combination. Ensembling is the combination of several independently trained models and is used by a large number of participants to get gains over single systems. Several teams seek to create ensembles of diverse models, including deep and wide ones. For example Wu et al. (2020) experiment with ensembling for larger models (larger feed forward dimension and then deeper models), including using different sampling strategies to increase the number of different models. Ensembling generally leads to better results but not always. Wu et al. (2020) found that a 9-model ensemble was best for Khmer and Pashto into English, but they found that for English into Khmer and Pashto, a single model was best. A second way of combining several models is to use an additional model to rerank n-best hypothesis of an initial model. Libovick\u00fd et al. (2020) attempted right-to-left rescoring (against the normally produced left-toright hypothesis), but without seeing any gains for Upper-Sorbian-German. Chen et al. (2020) test noisy channel reranking for Tamil-English, but without seeing gains either, although some gains are seen for Inuktitut\u2192English, presumably because of the highquality monolingual news data available to train a good English language model. Fine-tuning. Mentioned previously in the context of pretraining, fine-tuning was used in several contexts by a large number of teams. It is inevitably used after pretraining on language model objectives or on other language pairs (see above) to adapt the model to the language direction in question. It is also frequently used on models trained on backtranslated data, by fine-tuning on genuine parallel data (S\u00e1nchez-Cartagena, P\u00e9rez-Ortiz, and S\u00e1nchez-Mart\u00ednez 2019). A final boost used by a number of top-systems is achieved through fine-tuning to the development set (Shi et al. 2020;Chen et al. 2020;Zhang et al. 2020c;Wei et al. 2020). This was choice not made by all teams, some of which chose to keep it as a held-out set, notably to avoid the risk of overfitting.", "filtered_refids": [[], ["b97", "b61"], ["b211", "b179", "b203", "b101", "b224", "b1"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3053, "num_references": 8}
{"corpusid_sectionid": "21715311-s1", "title": "A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches", "date": "2018-05-01", "section_title": "Background -WordNet Construction", "section": "Generally speaking, WordNets are constructed using one of two approaches (Vossen, 1998):\n\n\u2022 The merge approach -whereby an exhaustive repository of senses (meanings) of each word is compiled, with synsets then created that contain all of the applicable words for a given sense.\n\n\u2022 The expansion approach -whereby existing synsets from a reference WordNet are used as a guide to create corresponding synsets in a new WordNet, by gathering applicable words that represent the meaning of the synset and ordering them by frequency.\n\nSince the introduction of the PWN and the success of early projects such as EuroWordNet that were built around its principles, many projects have focused on building new WordNets in diverse languages using these methods. These endeavours have highlighted various advantages and disadvantages of both the merge and the expansion approaches. Bhattacharyya (2010) describes how the merge approach results in WordNets of high quality, on account of expert lexicographers working in detail on only one language; however, the process is typically very slow. Conversely, the expansion approach can allow the construction of the Word-Net to take place much more quickly, with construction guided by synsets and semantic relationships in the source (or reference) WordNet; however, lexicographers still need to dedicate time to constructing language-specific synsets (meanings or concepts which may not be represented or have a place in the source WordNet), and there is a danger of specific concepts only applicable to the target language being overlooked altogether (Bhattacharyya, 2010). Years of work on constructing new WordNets have also contributed to the development of guidelines and principles for creating them, largely based on leveraging existing knowledge using the expansion approach. The GWA outlines the importance of 'base concepts' 4 -those concepts that occupy a high position in a semantic hierarchy and have many relations to other concepts -as playing a vital role in constructing WordNets. Base concepts are defined by their universality -common (to at least two languages), local (to only one language), or global (across all languages) -with an initial set of 1024 common base concepts being released as part of the EuroWordNet project 5 . As a starting point, the GWA proposes that WordNets be constructed in two steps:\n\n\u2022 A core WordNet of between 5,000 and 10,000 synsets is constructed around the common base concepts,\n\n\u2022 An extended WordNet is built (semi-automatically, given the semantic basis of the core WordNet) to increase the total number of synsets to 20,000 and beyond.\n\nGiven that for many languages these core synsets are readily-constructed, it makes sense to leverage them when constructing new WordNets, and to 'borrow' the semantic relationships that have already been created (Bhattacharyya, 2010). Given the amount of time that can be saved by re-using existing work, there is a tendency to see the expansion approach favoured over the merge approachit also lends itself extremely well to the automatic construction of synsets, where input from lexicographers is minimal to zero. Thus, the research described in this paper (and particularly in section 3.) largely follows the expansion approach, with synsets being constructed by automatically extracting lexical data from a range of resources in order to build a skeleton framework of meanings based on a reference WordNet.", "filtered_refids": [["b16"], [], [], ["b2"], [], [], ["b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 3442, "num_references": 3}
{"corpusid_sectionid": "21715311-s4", "title": "A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches", "date": "2018-05-01", "section_title": "Bilingual Dictionaries", "section": "The most common technique for populating new WordNets automatically has been to leverage the information in bilingual dictionaries in the source and target languages. Use of bilingual dictionaries for this purpose goes back to very early work on building Catalan and Spanish WordNets as part of the EuroWordNet project. In this work, translations of English words in the source WordNet were found, and these translations classified by features such as polysemy (number of translations for each word), structure (the semantic relationships between translations in the source WordNet), and 'conceptual distance' (length of the path between two words in a graph-based representation of the source WordNet) to create a skeleton WordNet in the target language, which could be extended later using bilingual taxonomies (Farreres et al., 1998). Since then, bilingual dictionaries have continued to be a popular resource for the automatic construction of Word-Nets. A Romanian WordNet was built by using a range of heuristics to:\n\n\u2022 Analyse the relationships between synsets in the source (English) WordNet,\n\n\u2022 Identify semantic relationships in various target language resources,\n\n\u2022 Map these relationships to each other in the target (Romanian) WordNet using a bilingual dictionary (Barbu and Barbu Mititelu, 2005).\n\nThe method was evaluated using 9716 synsets from a preexisting Romanian WordNet that also had entries in PWN, from which these 9716 synsets were extracted and used as the source (English) WordNet -the synsets used were limited to hypernymy and meronymy relations, and all 19,624 literal words within the synsets had an entry in the bilingual dictionary. The resulting automatically-constructed Romanian WordNet contained 9610 synsets connected by approximately 11,969 semantic relationships, which were reported to be 91% accurate when compared to the 9716 synsets from the pre-existing Romanian WordNet (Barbu and Barbu Mititelu, 2005). In more recent work on building a Persian WordNet, a bilingual dictionary was used to extract a group of 'candidate' synsets containing English translations of a given Persian word from a source WordNet (PWN). These candidate synsets were then ranked by calculating the Mutual Information of the given Persian word and its English translations in both source and target language corpora, and based on this ranking the most appropriate candidate synset to use for the target (Persian) WordNet was selected (Montazery and Faili, 2010). An extension of this work specifically aimed at lesser-resourced languages was also described, in which a Persian WordNet is constructed by finding the English translations of Persian words in small corpora using a bilingual dictionary. These translations are then used to perform word sense disambiguation (WSD) on a Persian sentence using a source (English) WordNet, and the English synsets returned by the WSD algorithm are mapped to the target (Persian) WordNet (Taghizadeh and Faili, 2016). Again, these techniques have been shown to be able to automatically construct WordNets with a good degree of accuracy. Montazery and Faili (2010) report that a manual evaluation of 500 synsets from their automatically-constructed target WordNet (which in total covered 29,716 synsets from PWN) resulted in an accuracy of 82.6% (95.8% for synsets whose mapping from source to target WordNet was unambiguous and 76.4% for synsets whose correct mapping had to be decided by ranking multiple candidates). Taghizadeh and Faili (2016) manually evaluated 1,750 word/synset pairs from their target WordNet, and describe how a threshold value (between 0 and 1) used by their WSD algorithm to remove low-scoring candidate synsets had a significant impact on their results. Higher threshold values resulted in the WordNet being more precise (90% with a threshold value of 0.1) but with low recall (fewer synsets in the target WordNet), while lower threshold values resulted in a Word-Net with higher recall (more synsets) but with low precision (74% with the threshold value set to 0).", "filtered_refids": [["b3"], [], [], ["b1"], ["b15", "b1", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 4053, "num_references": 5}
{"corpusid_sectionid": "21715311-s7", "title": "A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches", "date": "2018-05-01", "section_title": "Word Embeddings and WordNet Synsets", "section": "Given the increasing popularity of word embeddings (vector space representations of word meanings based on their distribution within large datasets), it should come as no surprise that the links between embeddings and traditional, WordNet-style representations of word senses have recently been explored. Nayak (2015) demonstrated that word embeddings can be classified according to words and can also be used to predict hypernymy relations between them, while Rothe and Sch\u00fctze (2015) report that sets of embeddings trained not just on words but also on synsets (groups of synonyms) and lexemes (word-synset pairs) achieve state-of-the-art performance on WSD and semantic similarity tasks. This kind of research shows the potential of word embeddings for capturing the kinds of relationships (and being useful in the types of tasks) commonly associated with WordNet-style word senses -potential which is further compounded by the reported high precision with which multi-sense word embeddings can be mapped to WordNetstyle synset entries in Babelnet 9 (Panchenko, 2016).", "filtered_refids": [["b12", "b10", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 3, "num_chars": 1071, "num_references": 3}
{"corpusid_sectionid": "21715311-s8", "title": "A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches", "date": "2018-05-01", "section_title": "Extending and Constructing Synsets from Word Embeddings", "section": "Naturally, recent research has explored leveraging the links between word embeddings and synsets in order to automatically construct new synsets from the embeddings themselves. Sand et al. (2017) describe using word embeddings to extend an existing Norwegian WordNet by finding candidate hypernyms for a given word based on its nearest neighbours in the WordNet, and then scoring these candidate hypernyms by distributional similarity (using the vector space of the embeddings) and distance in a graph-based representation of the WordNet. Based on an evaluation of 1388 target words occurring 5 times or more in the news corpus on which the embeddings were trained, an accuracy (percentage of newly-added target words correctly placed under the appropriate hypernym) of 55.80% and an attachment score (percentage of target words actually added to the Norwegian WordNet) of 96.33% were recorded. This 9 http://babelnet.org/ accuracy is increased when only evaluating on target words that occurred more than 100, or more than 500 times in the corpus, but at the cost of diminished coverage (fewer target words available with which to extend the Norwegian Word-Net). An alternative approach to using word embeddings to extend an existing WordNet has been described by Al tarouti and Kalita (2016), who in fact use word embeddings to extend an automatically-constructed Arabic WordNet built using the machine translation / bilingual dictionary method described by Lam et al. (2014). They leverage word embeddings to compute the cosine similarity of a) words within candidate synsets, and b) words within pairs of semantically-related synsets, allowing them to discard candidate synsets (and words within them) whose cosine similarity is below a given threshold value. 600 automaticallyconstructed word pairs (of synonym, hypernym, holonym, and meronym types) were evaluated by Arabic speakers using a 5-point Likert scale, with the average score then converted to a percentage -the resulting precision of the synonyms, hypernyms, holonyms, and meronyms was 78.4%, 84.4%, 90.4%, and 79.6% respectively, slightly higher than the precision (as a percentage) reported by Lam et al. (2014) for Arabic.\n\nA similar method has also been described by Khodak et al. (2017), who report on the automatic construction of whole WordNets in French and Russian from scratch using bilingual dictionaries and word embeddings. After producing candidate synsets by finding the corresponding source language synsets for target language words as given by the bilingual dictionaries, word sense embeddings and word sense induction (WSI) techniques are used to cluster only the most relevant translations of lemmas from the source language synset together, ensuring that the correct target language candidate synset is 'matched' as correct. Evaluating these methods using subsets of 200 nouns, verbs and adjectives from each of the target language WordNets, the resulting F\u02d95 scores -used as a precision-centric alternative to the usual F 1 score -were reported to outperform those yielded using a baseline similarity method by 6% and 10% for French and Russian respectively.", "filtered_refids": [["b14", "b6"], ["b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 3147, "num_references": 3}
{"corpusid_sectionid": "21715311-s9", "title": "A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches", "date": "2018-05-01", "section_title": "Issues for Evaluating Automatically-Constructed WordNets", "section": "One of the biggest issues for the automatic construction of WordNets is how to properly and effectively evaluate their accuracy and/or precision. Across all of the different lexical resource and word embedding-based approaches to automatic synset construction described in the previous two sections, evaluation methods can be split across two types:\n\n\u2022 Comparison against a reference WordNet,\n\n\u2022 Manual evaluations against fixed samples of automatically constructed synsets.\n\nFocusing first on comparisons with reference WordNets, much of the research referenced in the preceding sections reports on problems with this kind of evaluation. Khodak et al. (2017) describe an attempted comparison with their automatically-constructed WordNets and reference Word-Nets being difficult, with the ELRA French WordNet 2 being only around half the size of their new French WordNet and most Russian WordNets being a) even smaller, and b) not easily linked to (or compared with) PWN. Similarly, Taghizadeh and Faili (2016) cite 'the lack of correct links' in the pre-existing FarsNet (an ontology of Persian words mapped to PWN synsets) as being troublesome when attempting to compare their automatically-constructed Persian WordNet to it -they reported after comparing their automatically-constructed Persian WordNet to Farsnet that that the precision of their new WordNet was just 19% and its recall 49%, too low 'to be considered as a reliable resource'. The discrepancies between size and coverage of reference WordNets and the original PWN can be viewed as an issue of granularity: PWN is large enough that its senses are fine-grained, and so several PWN synsets can generally mapped onto one synset in a reference WordNet (such as FarsNet) while other PWN synsets will not be present at all (Khodak et al., 2017). This makes it difficult, when comparing automatically-constructed WordNets to reference WordNets in a target language, to decide whether newly-created synsets are correct or not. For example, Taghizadeh and Faili (2016) use the following criteria to decide whether word and synset pairs in an automaticallyconstructed Persian WordNet are correct, using FarsNet as their reference WordNet:\n\n\u2022 If a Persian word does not exist in FarsNet, it IS NOT correct,\n\n\u2022 If a Persian word exists in FarsNet but is not linked to a PWN synset, it IS NOT correct,\n\n\u2022 If a Persian word exists in Farsnet and and at least one PWN synset is linked to it:\n\n-If the automatically-constructed synset is not one of the linked PWN synsets in FarsNet, it IS NOT correct,\n\n-IF the automatically-constructed synset is one of the linked PWN sysnets in FarsNet, it IS correct.\n\nOut of three options here, two of them lead to the word in the automatically constructed Persian WordNet being classed as incorrect -and even if the word is in both FarsNet and PWN, that word still has to be linked between those resources to be accepted as correct. This approach is therefore totally dependent on the quality of FarsNet, and any words in the automatically constructed WordNet that are in PWN and that should be in FarsNet will, unfortunately, be classed as incorrect. As Oliver and Climent (2014) -who considered an automatically extracted synset correct only if it was also present in a reference WordNethighlight, automatic comparisons with reference WordNets inevitably mean that if the reference WordNets are not complete, then correctly extracted synsets in the automatically constructed WordNet can be evaluated as incorrect -and this is a major problem when reporting on their accuracy and legitimacy as a lexical resource. Sand et al. (2017) also touch on potential discrepancies between automatically extracted synsets and their equivalent synsets in reference WordNets or in PWN, noting that hypernymy relations 'can be right or wrong by varying degrees'. They describe a 'soft accuracy' measure whereby the accuracy of an automatically extracted synset is weighted according to the number of links (or edges) between words in different synsets that separate a given word from what would be its correct position in a graph-based representation of the WordNet. Weighting the accuracy of automatically extracted synsets according to how comparable they are with their fully-formed PWN equivalents is certainly more logical than evaluating strictly on 'correct insertions' -an automatically extracted synset containing 8 of the 10 links to other words present in the same sysnet in PWN, for example, is surely more correct than an automatically extracted synset containing only 2 or 3 of the 10 links.\n\nThe alternative to automatic evaluations of synset correctness is of course manual evaluation, which is widely used both in isolation and in conjunction with automated evaluations in the works cited in Section 3. (Ruiz-Casado et al., 2005;Montazery and Faili, 2010;Lam et al., 2014;Taghizadeh and Faili, 2016). However, as Ruiz-Casado et al. (2005) point out, it is 'difficult to know how accurate manually-evaluated synsets are without some common guidelines. Some works simply describe having manual annotators decide if an automatically extracted is or is not semantically similar to a reference synset (Taghizadeh and Faili, 2016), while others -much more in line with the idea of weighting accuracy according to a degree of correctness (Sand et al., 2017) -have used a Likert scale for conducting manual evaluations (Lam et al., 2014).", "filtered_refids": [[], [], [], ["b15", "b5"], [], [], [], [], [], ["b14"], ["b15", "b6", "b14", "b13", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 5425, "num_references": 8}
{"corpusid_sectionid": "21693867-s0", "title": "The DLDP Survey on Digital Use and Usability of EU Regional and Minority Languages", "date": "2018-05-01", "section_title": "Background and Motivation", "section": "In this paper we present the results of the first survey about the actual usage of four European minority languages and the related needs of their speakers in terms of digital opportunities. The survey is part of the work carried out by the Digital Language Diversity Project (DLDP) (Soria et al., 2016) 1 , a three-year Erasmus+ project started in September 2015. The goal of the DLDP is to help minority language speakers' communities in the acquisition of intellectual and practical skills to create, share, and reuse online digital content in their languages. At the same time we want to define general guidelines and best practices for the promotion of minority languages with poor digital representation, a fact that further prevents their usability on digital media and devices. One of the underlying assumptions of the Digital Language Diversity Project is that the sustainability and preservation of regional and minority languages is closely tied to their being perceived by their speakers as being fully-fledged languages that can be used in any context, the digital one included. Unfortunately, this is far from being a reality not only for regional and minority languages, but for the majority of the world languages. In most cases, the technical or infrastructural impediments for the digital use of European regional and minority languages are modest and fairly easily solvable. Marginalisation and minoritisation of those languages mostly derives from the concurrency of the national and global languages for which digital content and services are more easily available, which further discourages regional and minority language speakers from using those languages digitally. In order to break this vicious circle and make those languages digitally appealing and usable to an extent that can compete with other major lan-guages, it is necessary to approach the problem in terms of \"digital language planning\". In order to be able to plan for digital development, we first need to identify the current and actual extent to which RML are used digitally, the type and frequency of their digital use, the opportunity for their use, and the main obstacles currently preventing it so as to get a clear understanding of the different factors that may affect the digital use of RMLs. Some reports carried out for individual languages and specific media are available, like the Language White Papers (Uszkoreit and Rehm, 2012) published by the META-NET Network that has clearly shown how 30 European languages are at risk of digital extinction because of lack of sufficient support in terms of language technologies. The META-NET work, initially for each of the EU official languages, was then extended to cover as well some regional languages such as Basque (Hern\u00c3\u00a1ez et al., 2012), Catalan (Moreno et al., 2012), Galician (Garc\u00c3a-Mateo and Arza, 2012), and Welsh (Evas, 2013). The reports mostly assessed the status of those languages in terms of language technology support. A general survey covering all regional and minority languages of the EU, the different types of digital media and services available, as well as inquiring about the attitudes and desires to make a digital use of the language is still lacking. The DLDP effort can therefore be seen as a first step towards the design of a survey about digital use of minority languages in both professional and informal contexts, specifically tailored on RMLs in the digital world and structured around a crucial question: is it possible for regional or minority language speakers to have a digital life in those languages? The paper is organised as follows: a description of the methodology underlying the design of the survey; an analysis of the results collected, with a separate section for each language; a summary of the key findings and an indication of the work planned for the future.", "filtered_refids": [[null, "b5", "b8", "b0", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 3859, "num_references": 5}
{"corpusid_sectionid": "258378266-s1", "title": "Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey", "date": 2023, "section_title": "Background", "section": "Neural Ranking (NR) for ODQA Let Q, D and A denote the question, document and answer set. Given a question q \u2208 Q, the NR model assigns a relevance score R(q, d) to each d \u2208 D and selects top-k document D topk \u2208 D with the highest relevance scores. Afterwards, a reader will estimate the score G(a|q, D topk ) to predict the final answer a \u2208 A conditioned on both q and D topk . The NR model can be implemented using various architecture with increasing model complexity. For computational efficiency, normally a bi-encoder architecture (Bromley et al., 1993) is first applied to pre-select top candidates from the whole document set, then a more complex cross-interaction model is applied to provide more accurate relevance scores only for the preselected candidates (Lee et al., 2021). The training objective for the NR model R can be formalized as:\n\nwhere Q \u00d7 D indicates the full set of questiondocument pairs, d + is a positive (relevant) document for q, d \u2212 1\u223cn is the sampled n negative (irrelevant) documents and L is the loss function. A common choice for L is the contrastive loss:\n\nNeural Ranking with Weak Supervision In the standard supervised setting we need relevance annotations for (q, d) \u2192 {+, \u2212} to train R with Eq 1.\n\nObtaining high-quality relevance annotations requires tremendous human labor and is expensive to scale to multiple domains (Del Tredici et al., 2021;Ram et al., 2022). Weak supervision (WS) is a widely-used approach to reduce such cost by leveraging supervision signals from e,g., heuristic rules, knowledge bases or external models . WS signals are cheap to obtain but might contain significant noise which will affect the NR performance. Therefore, understanding their working mechanisms and pros and cons are important to obtain a good NR model. We group WS signals into 3 classes by the resources that they need: (1) Documents: only document collection D is needed;\n\n(2) Documents + Questions: document collection D and question set Q are needed; (3) Documents + QA Pairs: document collection D and QA pairs (Q, A) are needed. In the next section, we will present the three classes of WS signals and discuss their pros and cons.", "filtered_refids": [[null], [], [], [null, "b31"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2169, "num_references": 3}
{"corpusid_sectionid": "258378266-s3", "title": "Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey", "date": 2023, "section_title": "Self Contrastive Learning", "section": "Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q \u2032 , d \u2032+/\u2212 ) from D, then uses them to supervise training of a NR model. The objective is:\n\nwhere L is the ranking loss as in Eq 1. Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q \u2032 , d \u2032+ ). There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based. An overview is in Table 2.\n\nPerturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair. The intuition is that perturbed text should still be relevant to the original text. Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).\n\nProximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other. The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document. They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness. Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.\n\nCooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021). For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus. A term from it is treated as the answer and replaced with a special token. Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents. Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.\n\nHyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant . For example, Chang et al.\n\n(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic. A passage from another page containing hyperlinks to p is treated as a positive document. Yue et al. (2022a) replace an entity word with a question phrase like \"what/when\" to form a pseudo question. A passage from its hyperlinked document that contains the same entity word is treated as a positive sample. Zhou et al. (2022) build positive samples with two typologies: \"dual-link\" where two passages have hyperlinks pointed to each other, and \"co-mention\" where two passages both have a hyperlink to the same third-party document.  ", "filtered_refids": [[], [], ["b16", "b83", "b10"], ["b64", null, "b74"], ["b30", "b31"], [], ["b70", "b80"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3203, "num_references": 10}
{"corpusid_sectionid": "258378266-s5", "title": "Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey", "date": 2023, "section_title": "Choices of Filter", "section": "Filtering is a crucial part of QG since a significant portion of generated questions could be of low quality and would provide misleading signals when used to train the NR model (Alberti et al., 2019). A typical choice is filtering based on round-trip consistency (Alberti et al., 2019;Dong et al., 2019), where a pre-trained QA system is applied to produce an answer based on the generated question. A question is kept only when the produced answer is consistent with the answer from which the question is generated. We can also relax this strict consistency requirement and manually adjust an acceptance threshold based on the probability from the pre-trained QA system (Zhang and Bansal, 2019;Lewis et al., 2021), LM score from the generator itself (Shakeri et al., 2020;Liang et al., 2020), or an entailment score from a model trained on question-context-answer pairs . Influence functions (Cook and Weisberg, 1982) can be used to estimate the effect on the validation loss of including a synthetic example (Yang et al., 2020), but this does not achieve satisfying performances on QA tasks (Bartolo et al., 2021). Bartolo et al. (2021) propose filtering questions based on ensemble consistency, where an ensemble of QA models are trained with different random seeds and only questions agreed by most QA models are selected. When minimal target-domain annotation is available, we can also learn to reweight pseudo samples based on the validation loss , or use RL to select samples that lead to validation performance gains (value estimation) (Yue et al., 2022b).", "filtered_refids": [["b25", "b71", "b66", "b77", null, "b1"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1565, "num_references": 6}
{"corpusid_sectionid": "258378266-s10", "title": "Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey", "date": 2023, "section_title": "Answer as a Document", "section": "As a straightforward way to leverage QA pairs, this method directly treats QA pairs as positive samples and does not distinguish between documents and answers (Lai et al., 2018). These QA pairs can provide direct WS signals to train the NR model:\n\nwhere (q, a + ) \u2208 Q \u00d7 A are question-answer pairs in the target domain, a \u2212 1\u223cn are sampled n negative answers and L is the standard ranking loss.\n\nThough simple, this has been a common practice to \"warm up\" the NR model when no sufficient relevance annotations are available. For largesized models, this can be crucial to fully leverage the model capacity since we often have orders of magnitude more QA pairs than relevance annotations (Ni et al., 2021;Oguz et al., 2021). However, the style, structure and format differ between the document and the answer. The answer is a direct response to the question, and so it is easier to predict due to its strong semantic correlation with the question. Whereas the document can be implicit and may contain fewer obvious clues that can imply an answer; deep text understanding is required to predict the relevance between questions and documents Shen et al., 2022b). Therefore, this approach may be insufficient to reach satisfying results as a standalone method.", "filtered_refids": [[null], [], ["b44", "b22", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1255, "num_references": 4}
{"corpusid_sectionid": "258378266-s12", "title": "Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey", "date": 2023, "section_title": "Latent-Variable Model", "section": "We can still train the NR model on questiondocument pairs as in Answer-Document Mapping. However, instead of relying on a heuristic-based mapping function, we can treat this mapping as a \"latent variable\" within a probabilistic generative process (Lee et al., 2019;Shen, 2022). By this means, the NR model R gets WS signals from the QA reader G by maximizing the marginal likelihood:\n\nwhere Z indicates all possible document combinations. Directly optimizing over Eq 6 is infeasible as it requires enumerating over all documents. A closed-form solution does not exist due to the deep neural network parameterization of R and G. The following section explains popular optimization options. An overview can be seen in Table 4.\n\nTop-k approximation A popular approach is to assume a categorical distribution for R(Z|q); that is, to assume for each question only a single document is selected and the answer is generated from that one document. Eq 6 can be approximated by enumerating over only the top-k documents, assuming the remaining documents having negligibly small contributions to the likelihood:\n\nThis has been a popular choice in end-to-end training of text generation models (Lee et al., 2019;Shen et al., 2019b;Guu et al., 2020;Lewis et al., 2020;Shuster et al., 2021;Ferguson et al., 2022). Despite its simplicity, the top-k approximation has two main drawbacks. (1) The approximation is performed on the top-k documents obtained from the NR model. If the NR model is very weak at the beginning of training, these top-k documents can be a bad approximation to the real joint likelihood and the model might struggle to converge. (2) The assumption that document follow a categorical distribution might be problematic especially if the answer requires evidence from multiple documents (Wang and Pan, 2022).", "filtered_refids": [["b42", null], [], [], ["b62", null, "b50", "b48"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1814, "num_references": 6}
{"corpusid_sectionid": "258378266-s13", "title": "Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey", "date": 2023, "section_title": "Expectation-Maximization (EM) algorithm", "section": "To address the second drawback of the top-k approximation approach, we can assume a multinomial distribution for R(Z|q) so that an answer can be generated from multiple documents. The cost of this relaxation is the increased difficulty of optimization. Approximating the joint likelihood from top-k samples becomes infeasible due to the combinatorial distribution of document. Singh et al. (2021) propose optimizating it with the EM algorithm under an independent assumption about the posterior distribution of R(z|q):\n\nwhere SG means stop-gradient (gradients are not backpropagated through G). As can be seen, the training signal for the NR model is essentially the same as in the Top-k Approximation case, except that the reader is trained by conditioning on all top-k documents to generate the answer. Singh et al. (2021) also find that Eq 7 is quite robust with respect to parameter initialization. Similarly,  apply the hard-EM algorithm to train the NR model, which only treats documents with the highest likelihood estimated by the reader as positive. Izacard et al. (2022) further experiment with using the leave-one-out perplexity from the reader to supervise the ranker.\n\nLearning from attention Another way to optimize the NR model in Eq 6 is to leverage attention scores from the reader G. The assumption is that when training G to generate the answer, its attention score is a good approximation of question-document relevance. The training objective is:\n\nwhere G is trained to generate the right answer based on the question and the top-k document, same as in the EM algorithm. A z is the attention score of G on the document z. L is the loss function to encourage the similarity between distributions of the attention scores and retrieving scores.\n\nIzacard and Grave (2021) propose a training process that optimizes R and G iteratively. R is trained to minimize KL divergence between relevance and attention scores. (Lee et al., 2021) jointly optimize R and G and apply a stop-gradient operation on G when updating R. Sachan et al. (2021) use retriever scores to bias attention scores on the contrary. These can be considered as first-order Taylor series approximations of Eq. 6 by replacing R(Z|q) with attention scores (Deng et al., 2018).\n\nDiscussion Training with latent-variable models can perform close to fully supervised models under certain scenarios Sachan et al., 2021). The main challenge is the training difficulty. In practice, we can often initialize the NR model using the answer as document or answer-document mapping to make the training more stable. If not enough QA pairs are available, we can use heuristics like masked salient entities (Guu et al., 2020) to form pseudo pairs, then apply the same WS techniques in this section. Combining supervision signals from various various optimization techniques such as learning from attention and EM algorithm can also be beneficial (Izacard et al., 2022). If the independence assumption made by Eq 7 does not hold, we need to resort to more complex optimization algorithms. A potential direction is to apply a Dirichlet prior over R(z|q t ), which is a conjugate distribution to the multinomial distribution (Minka, 2000), with the result that the sampled document are not independent individuals but a combination set. Eq 6 can then be estimated by rejection sampling (Deng et al., 2018) or a Laplace approximation (Srivastava and Sutton, 2017) so as to avoid the independence assumption about the posterior distribution. Nonetheless, this will further increase the training complexity, which is already a key bottleneck for training the NR model.", "filtered_refids": [["b51"], [null, "b51"], [], [], [null, "b39"], [null, "b39", "b54", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3628, "num_references": 9}
{"corpusid_sectionid": "258378191-s3", "title": "A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models", "date": 2023, "section_title": "Pre-trained Language Models", "section": "Recently, pre-trained language models have advanced the state-of-the-art in many NLP tasks ranging from textual similarity to text summarization (Zhang et al., 2019;Liu and Lapata, 2019;Zhong et al., 2020) and named entity recognition (Zhou et al., 2021). State-of-the-art pre-trained models include LSTM-based language models (e.g., ELMo (Peters et al., 2018)) and Transformer-based language models (e.g., BERT 2 (Devlin et al., 2019) and RoBERTa ). Specifically, the transformer-based models learn bidirectional representations for words based on a masked language model and sentence adjacency training objective (Devlin et al., 2019). Simply using contextualized embeddings obtained from the transformerbased pre-trained language models in place of traditional embeddings has resulted in state-of-the-art performance on a range of NLP tasks. Therefore, pre-trained language models have been employed as encoders for obtaining word-, sentence-, and document-level representations to assist the downstream tasks.", "filtered_refids": [["b72", "b46", "b13", "b70", "b33", "b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1013, "num_references": 6}
{"corpusid_sectionid": "258378191-s4", "title": "A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models", "date": 2023, "section_title": "Keyphrase Extraction Dataset", "section": "Since the first shared task on KE (Turney, 1999), many shared tasks and benchmark datasets for KE have been created. Specifically, OpenKP (Xiong et al., 2019), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, Se-mEval2017 (Augenstein et al., 2017), and KP20k (Meng et al., 2017) were created from scientific articles in English.\n\nCompared with other datasets, KP20k contains a large amount of annotation data, so it is often used as the dataset to train the neural-based KE models recently. Meanwhile, in recent papers (Sun et al., 2020a;Song et al., 2021), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, and SemEval2017 (Augenstein et al., 2017)  datasets are often used as the zero-shot test sets to verify the robustness of the KE models trained by the KP20k dataset. Furthermore, KE tasks have also been organized on newswire articles in English, e.g., DUC2001 (Wan and Xiao, 2008b). Table 1 summarizes the statistics of several commonly used benchmark datasets.", "filtered_refids": [["b43", "b29", "b66", null, "b22", "b38", "b2"], ["b43", "b52", "b29", null, "b22", "b51", "b61", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1112, "num_references": 15}
{"corpusid_sectionid": "258378191-s7", "title": "A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models", "date": 2023, "section_title": "Two-Stage Unsupervised Keyphrase Extraction Models", "section": "As noted before, unsupervised keyphrase extraction systems generally extract a set of phrases from the source document as candidates by using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum (Hasan and Ng, 2014). The main steps of the commonly used candidate keyphrases extraction methods for the recent unsupervised keyphrase extraction models are as follows, (1) tokenizing the document and tagging the document with partof-speech (POS) tags via the StanfordCoreNLP Tools 3 ; (2) extracting candidate phrases based on part-of-speech tags by the regular expression via the python package NLTK 4 . Furthermore, different pruning heuristics have been designed for pruning candidates that are unlikely to be keyphrases to obtain a better candidate set (Huang et al., 2006;Kumar and Srinathan, 2008;El-Beltagy and Rafea, 2009;Newman et al., 2012;You et al., 2009). After obtaining candidates, keyphrases are determined by estimating the importance of each candidate through various strategies. Here, to facilitate the introduction, we divide the methods of importance estimation into two categories, namely, traditional methods and embedding-based methods. Traditional unsupervised keyphrase extraction systems can be mainly divided into statistics-based (Jones, 2004;Campos et al., 2018b), topic-based (Liu et al., 2009;Jardine and Teufel, 2014), and graph-based (Mihalcea and Tarau, 2004;Wan and Xiao, 2008b;Bougouin et al., 2013;Florescu and Caragea, 2017b) methods. Generally, these models primarily use different features of documents (e.g., word frequency, position, linguistic properties, topic, length, the relationship between words, external knowledge-based information, etc.) to estimate the importance of each candidate phrase and discriminate whether a candidate phrase is a keyphrase (Hasan and Ng, 2014; Papagiannopoulou and Tsoumakas, 2019).\n\nHowever, these traditional unsupervised models estimate the importance scores of candidate phrases based on the surface-level features, ignoring the high-level features (e.g., syntactic and semantic information) of natural languages, which leads to extract wrong keyphrases. Therefore, recent studies focus on embedding-based models (Wang et al., 2015;Mahata et al., 2018a;Papagiannopoulou and Tsoumakas, 2018;Sahrawat et al., 2020;Kulkarni et al., 2022;Song et al., 2022b), which leverage pretrained embeddings (containing high-level features) to obtain phrase and document embeddings and calculate the importance scores of candidate phrases for extracting keyphrases. Wang et al. (2015) is the first work to explore utilizing word embedding and frequency to generate weighted edges between words, then using the weighted PageRank algorithm to compute and rank candidate scores. Key2vec (Mahata et al., 2018a) proposes an effective way of processing text documents for training multi-word phrase embeddings that are used for topic representations of scientific articles and ranking of keyphrases extracted from them using the topic-weighted PageRank algorithm. Mahata et al. (2018b) uses a combination of theme-weighted personalized PageRank algorithm and neural phrase embeddings for extracting and ranking keyphrases. EmbedRank (Bennani-Smires et al., 2018) ranks candidate phrases by measuring the semantic similarity between each candidate phrase and document embeddings.\n\nWith the development of pre-trained language models (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERta ), SIFRank 5 (Sun et al., 2020b) improves candidate phrase and document embeddings from EmbedRank with the pre-trained language model ELMo (Peters et al., 2018) and achieves better performance. JointGL 6 (Liang et al., 2021) integrates boundary-aware phrase centrality (the semantic similarities are calculated between all candidate phrases for identifying which candidate is better) and phrase-document relevance (the semantic similarities are calculated between candidate phrases and their corresponding document) from both local and global views, then used both jointly to determine the importance of each candidate. Attention-Rank 7 (Ding and Luo, 2021) adopts a pre-trained language model to calculate the self-attention of a candidate within the context of a sentence, and the cross-attention between a candidate and sentences within the source document to evaluate the local and global importance of each candidate. MDERank 8  proposes to rank candidates using the similarity between the BERT embeddings of the source document and the masked document. Totally, these models achieve state-ofthe-art performance in the unsupervised keyphrase extraction task, benefiting from the development of representation learning.", "filtered_refids": [["b15", "b67", "b39", "b31", "b41", "b27", "b35", "b21", "b61", "b24", "b8", "b5", "b17"], ["b44", "b47", "b50", "b30", "b62", "b37", "b36"], ["b53", "b13", "b46", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 4752, "num_references": 24}
{"corpusid_sectionid": "258378191-s8", "title": "A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models", "date": 2023, "section_title": "Two-Stage Supervised Keyphrase Extraction Models", "section": "Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincar\u00e9 distance to extract keyphrases.  ", "filtered_refids": [["b52", "b46", "b13", "b66", "b49", null, "b51", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 2493, "num_references": 8}
{"corpusid_sectionid": "258378191-s9", "title": "A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models", "date": 2023, "section_title": "One-Stage Supervised Keyphrase Extraction Models", "section": "A major limitation of the above two-stage supervised approaches is classifying the labels of each candidate phrase independently while ignoring the dependencies that could potentially exist between candidates. Therefore, recent studies (Gollapalli et al., 2017;Basaldella et al., 2018;Wang et al., 2018;Alzaidy et al., 2019;Sun et al., 2019;Mu et al., 2020;Sahrawat et al., 2020) formulated keyphrase extraction as sequence labeling and showed that using linear-chain Conditional Random Fields improved the performance over baseline models for this task. Then, Mu et al. (2020) proposes SKE-BASE-CLS and -RANK, which directly extracts span-based phrase representations from all the document tokens via pre-trained language models and further learn to capture the interaction between them and their corresponding document to get better ranking results. Furthermore, this kind of model can extract overlapped keyphrases (Mu et al., 2020).", "filtered_refids": [["b63", "b54", "b18", "b47", null, "b1", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 936, "num_references": 7}
{"corpusid_sectionid": "246863418-s1", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing", "date": "2022-02-15", "section_title": "Skimming", "section": "Skimming techniques, as summarized in Table 1, skip some time steps or allocate different computation on them. Intuitively, skimming matches how human beings efficiently read text and extract information from it (Li et al., 2019). By em-phasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies. The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.\n\nSkipping and Early Stopping Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early. LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU). At each step, the current states are used to compute a \"jumping softmax\", which decides how many steps to jump forward and whether to stop reading. LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions. The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6\u00d7 speed-up. Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update. If the gate decides to skip a time step, the hidden states will be directly copied without any update.\n\nTo stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level. Jumper  first splits a paragraph to several sub-sentences and encodes them into sentence embeddings. They then apply early stopping at a sentence when the policy network decides to stop reading. Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text. They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early. Yu et al. (2018) add a rereading operation to LSTM-Jump (Yu et al., 2017) which allows the model to stay on the current time step, allocating more computation to important information.\n\nThe aforementioned techniques can only go forward, which makes it impossible to regret if hav-", "filtered_refids": [["b30"], ["b64", "b3"], ["b30", "b50", "b64", "b65"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2627, "num_references": 7}
{"corpusid_sectionid": "246863418-s2", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing", "date": "2022-02-15", "section_title": "Method Decision based on", "section": "Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc. Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)\n\nPoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; \"flush\" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information. LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed. Structural information that naturally exists in sentences can also play a role in skimming. Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).\n\nIn the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers. The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores. The number of tokens to be dropped, referred to as the \"schedule,\" is optimized by combining the sparsity of a soft mask layer with the original loss function. This results in an improved balance between accuracy and processing time. TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained. In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them. The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning. The schedule for LAT is found through an evolutionary search algorithm. LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule. It simply drops tokens with attention scores lower than the learned threshold. Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer. This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.\n\nComputation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely. VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step. Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.\n\nInstead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN. At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token. If the model decides to skim, the small RNN will update only a fraction of the hid-den states. Otherwise, a regular full update will be conducted by the big RNN.\n\nDynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed. HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture. In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.\n\nIn question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed. Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering. It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.", "filtered_refids": [["b15", "b10", "b50", "b30", "b64", "b65", "b3"], ["b15", "b63", "b10", "b4", "b14", "b47", "b13", "b19", "b23", "b24"], ["b24", "b14", "b63", "b13"], ["b19"], ["b47"], ["b4", "b3"], ["b23"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 31, "num_chars": 5405, "num_references": 26}
{"corpusid_sectionid": "246863418-s4", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing", "date": "2022-02-15", "section_title": "MoE Layers with Learned Routing", "section": "A straightforward idea to implement MoE is to learn a router to allocate inputs to experts. Sparsely-Gated MoE layer  contains up to thousands of feed-forward sub-networks with a trainable gating network which determines a sparse combination of these experts to use for each example. There are two major challenges to address: (1) Sparsity. The gating network predicts a softmax weight for the experts based on the input. The gating network is trained by simple back-propagation, together with other parts of the model. Then, only the top-k experts in the layer will be activated based on the softmax prediction of the gating network. They insert one MoE layer between stacked LSTM layers and achieve improvement on language modeling and machine translation tasks.\n\n(2) Load balancing.  observe a self-reinforcing phenomenon that the gating network tends to converge to a state where it always produces large weights for the same few experts. They resolve the problem by defining the importance of an expert relative to a batch of training examples to be batch-wise sum of the gate values for that expert. Then, they introduce an additional loss, the square of the coefficient of variation of the set of importance values, to encourage a more balanced update during training. Besides encouraging a balanced update, the authors also introduce a loss function with a smooth estimator that estimate the number of examples assigned to each expert for a batch of inputs, to encourage experts to receive roughly equal numbers of training examples.\n\nGShard  enables scaling up multilingual neural machine translation Transformer beyond 600 billion parameters. It adapts Sparsely-Gated MoE  to Transformer (Vaswani et al., 2017) by replacing every other feed forward layer with an MoE layer, which routes to top-2 experts. When scaling to multiple devices, the MoE layer is sharded across devices, i.e., each device has different allocated experts, while all other layers are replicated. To achieve workload balance, GShard employs a threshold, namely expert capacity, to limit the maximum number of tokens processed by one single expert. They also introduce a local group dispatching mechanism, which partitions all tokens in a training batch evenly into groups to be processed independently in parallel, to balance the overall workload. Following , they use an additional loss to enforce even top-2 expert capacity; local group dispatching; auxiliary loss; random routing Switch (Fedus et al., 2021) Transformer (T5) top-1 expert capacity; auxiliary loss BASE (Lewis et al., 2021) Transformer (GPT) top-1 linear assignment M6-T  Transformer (M6) k top-1 expert capacity DTS (Nie et al., 2021) Transformer (GPT) dynamic sparsity scheduler\n\nHash (Roller et al., 2021) Transformer hash deterministic hash THOR (Zuo et al., 2022) Transformer (NMT) random random selection allocation for experts. Additionally, they propose a random routing mechanism, which only routes to the second-best expert with probability proportional to its weight, to simplify sparse training. Switch Transformer (Fedus et al., 2021) aims to simplify the Sparsely-Gated MoE  for efficiency and performance. They propose a Switch Layer which only routes to one expert at a time, to reduce gating computation, batch size and communication costs. Switch Transformer inherits expert capacity and an auxiliary load balancing loss from GShard . Combined with low-precision training, compared to T5-Base and T5-Large (Raffel et al., 2020), Switch Transformer obtains up to 7\u00d7 increases in pretraining speed with the same computational resources. They further scale Switch Transformer to more than 1.5 trillion parameters and achieve 4\u00d7 speed-up over T5-XXL.\n\nThe Balanced Assignment of Sparse Experts (BASE) layer (Lewis et al., 2021) formulates token-to-expert allocation as a linear assignment problem and solves it with the auction algorithm (Bertsekas, 1992). This allows an optimal assignment in which each expert receives an equal number of tokens, improving efficiency and getting rid of the expert capacity and auxiliary loss in previous works. The experiments show that BASE layers are more efficient for training compared to Sparsely-Gated MoE layers  and Switch Layers (Fedus et al., 2021), and can successfully learn a good balanced routing without any auxiliary balancing loss.\n\nM6 ) is a multi-modal multitask Transformer, trained in the same way as Switch Transformer (Fedus et al., 2021), scaling up to 100B parameters. Following this, M6-T  splits experts into k prototypes (i.e., groups of experts). In each forward pass, each token is sent to the k prototypes, within which the top-1 routing is done lo-cally. The experiments demonstrate this \"k top-1\" strategy outperforms the top-1 routing in Switch Transformer (Fedus et al., 2021) while being more computation-efficient than \"top-k\" routing. They also claim that the load balancing loss may be ineffective for improving the performance of an MoE model, although it can indeed help balance the workload. They subsequently train a 1 trillion parameter model with the finding.\n\nDense-to-Sparse gate (Nie et al., 2021) begins as a dense gate that routes tokens to all experts then gradually learns to become sparser and route tokens to fewer experts, demonstrating higher training efficiency in experiments. Their experiments confirm the finding in  that an auxiliary load balancing loss does not improve the model performance.\n\nMoE Layer with Unlearnable Routing Although learning-based routing has shown effectiveness only with the help of complicated load balancing mechanisms, recent studies have attempted to get rid of those. Hash Layer (Roller et al., 2021) simplifies routing by using a parameter-free hashing function to route tokens to specific experts. This design eliminates the need for a load balancing loss and sophisticated assignment algorithms. They also study the performance of different hashing techniques, hash sizes and input features, and conclude that balanced and random hashes focused on the most local features work best. The experiments show that a Hash Layer achieves comparable performance with a Switch Layer (Fedus et al., 2021) and BASE Layer (Lewis et al., 2021).\n\nTHOR (Zuo et al., 2022) is a special form of MoE layer, which completely discards the conditional routing mechanism and instead optimizes the consistency between a randomly selected pair of experts. During inference, one expert will be randomly selected to be activated.", "filtered_refids": [[], [], ["b53", "b9", "b28", null], ["b43", "b41", "b9", "b73"], ["b1", "b28", "b9"], ["b9"], [null], ["b43", "b9", "b28"], ["b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 41, "num_chars": 6497, "num_references": 17}
{"corpusid_sectionid": "246863418-s5", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing", "date": "2022-02-15", "section_title": "Applications and Analysis", "section": "GLaM  trains a family of GPT-style language models with up to 1.2 trillion parameters using GShard . CPM-2 (Zhang et al., 2022b) trains a large Chinese language model with 198 billion parameters with BASE layers (Lewis et al., 2021). Artetxe et al. (2021) conduct a detailed empirical study of how autoregressive MoE language models scale compared to dense models. They find MoEs to be substantially more efficient with the exception of fine-tuning. MoE models can match the performance of dense models with 25% of computation in a low-resource setting. Although the advantage fades at scale, their largest MoE model with 1.1 trillion parameters can consistently outperform its dense counterpart with the same amount of computation. Clark et al. (2022) examine the scaling law of BASE Layer (Lewis et al., 2021), Hash Layer (Roller et al., 2021) and earlier Reinforcement Learning-based routing algorithms providing suggestions for best-practices in training MoE models.  propose MoEfication to split feedforward neural networks (FFNN) in a trained large model to experts. They find that a T5-Large (Raffel et al., 2020) model with 700 million parameters only activates 5% neurons for 80% inputs on a downstream task, indicating high redundancy within large pretrained language models. To transform a pretrained language model to an MoE model, they first construct a co-activation graph for each FFNN and then divide the graph into subgraphs with strong internal connections with graph partitioning algorithm. Each subgraph forms an expert. They train a router with oracle best routing for training data. Then, they further fine-tune the resulted model for better performance.", "filtered_refids": [["b43", "b28", "b41", "b0", "b68", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1676, "num_references": 6}
{"corpusid_sectionid": "246863418-s7", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing", "date": "2022-02-15", "section_title": "Confidence-based Early Exit", "section": "Early works for early exit in computer vision (Park et al., 2015;Teerapittayanon et al., 2016;Kaya et al., 2019) often fall into this category. They define a metric as the proxy for confidence of a model prediction. The model exits early when the confidence hits a predefined threshold. DeeBERT (Xin et al., 2020b) applies BranchyNet (Teerapittayanon et al., 2016) to BERT inference. The training for DeeBERT is two-stage: they first train BERT on downstream tasks following standard fine-tuning. Then, they freeze the parameters of the Transformer and insert a linear classifier (i.e., internal classifier) after each Transformer layer. They train the classifiers by minimizing the sum of their cross-entropy loss. For inference, the model exits early when an internal classifier outputs a prediction probability distribution that has an entropy lower than a predefined threshold. RightTool (Schwartz et al., 2020) jointly finetunes BERT with internal classifiers. They use the temperature-calibrated maximum class probability as confidence. FastBERT  first trains the BERT backbone and the final classifier. Then, they distill the final classifier layer to the internal classifiers (Hinton et al., 2015). For inference, the model exits when the entropy of a prediction is below the threshold. Rome-BERT (Geng et al., 2021) provides a simple fix for learning internal classifiers efficiently. Besides self-distillation as in FastBERT, they propose gradient regularization (GR) to facilitate distillation. SkipBERT (Wang et al., 2022) caches pre-computed representation of text chunks to re-", "filtered_refids": [["b11", "b52", "b54", "b39", "b56", null, "b22", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 1591, "num_references": 8}
{"corpusid_sectionid": "246863418-s9", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing", "date": "2022-02-15", "section_title": "Internal classifier training Exit criterion", "section": "DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < \u03b8 RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > \u03b8 FastBERT  two-stage; self-distillation entropy < \u03b8 RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < \u03b8 SkipBERT (2022) joint; weighted sum of CE + KD max class probability > \u03b8 PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > \u03b8 ) Voting  joint; sum of CE + diversity loss accumulated votes > \u03b8 LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > \u03b8 ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < \u03b8 PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > \u03b8)\n\nBERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > \u03b8 CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > \u03b8\n\nCascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > \u03b8 place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.\n\nEnsemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.\n\nThey adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called \"imitation learners\", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.\n\nLearning-based Early Exit Another stream of research is to learn a criterion for early exiting.\n\nBERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a \"meta consistency classifier\" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.\n\nCascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.\n\nApplications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.", "filtered_refids": [["b11", "b72", "b56", null, "b70"], ["b45", "b57"], ["b29"], ["b70", "b72"], [null, "b67"], [], ["b45", "b57"], ["b56", "b29"], ["b56", "b55", null, "b8", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 43, "num_chars": 6105, "num_references": 21}
{"corpusid_sectionid": "254877753-s1", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "What is Reasoning?", "section": "Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although \"reasoning\" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:\n\nDeductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:\n\n\u2022 Premise: All mammals have kidneys. \u2022 Premise: All whales are mammals. \u2022 Conclusion: All whales have kidneys.\n\nInductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:\n\n\u2022 Observation: Every time we see a creature with wings, it is a bird. \u2022 Observation: We see a creature with wings. \u2022 Conclusion: The creature is likely to be a bird.\n\nAbductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:\n\n\u2022 Observation: The car cannot start and there is a puddle of liquid under the engine. \u2022 Conclusion: The most likely explanation is that the car has a leak in the radiator.\n\nOther types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.\n\nFormal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.\n\nReasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term \"reasoning\" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on \"informal deductive reasoning\" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.", "filtered_refids": [["b75", null, "b34", "b76"], [], [], [], [], [], [], [], [null], ["b80", null, "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 3943, "num_references": 8}
{"corpusid_sectionid": "254877753-s3", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "Fully Supervised Finetuning", "section": "Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets. For example, Rajani et al.\n\n(2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019). Talmor et al. (2020) train RoBERTa (Liu et al., 2019) to perform reasoning/inference based on both implicit pre-trained knowledge and explicit free-text statements. Hendrycks et al. (2021) finetune pretrained 2 It is important to note that the term \"reasoning\" in this paper does not necessarily imply that LLMs are truly capable of reasoning or that they are able to reason in the same way that humans do. We will discuss this issue in more detail in \u00a76. language models to solve competition mathematics problems by generating full step-by-step solutions, though the accuracy is relatively low. Nye et al. (2022) train language models to do multi-step reasoning for program synthesis/execution by generating \"scratchpads\", i.e., intermediate computations, before producing the final answers. We refer the reader to Helwe et al. (2021); Bhargava and Ng (2022)'s survey for more studies in this line.\n\nThere are two major limitations of fully supervised finetuning. First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create. Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.", "filtered_refids": [[], ["b40", "b28", "b69", "b51", "b68"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1828, "num_references": 5}
{"corpusid_sectionid": "254877753-s5", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "Chain of Thought and Its Variants", "section": "To encourage LLMs to engage in reasoning rather than simply providing answers directly, we may guide LLMs to generate \"reasoning\" explicitly. One approach for doing this is chain-of-thought prompting, proposed by Wei et al. (2022b). This approach involves providing a few examples of \"chain of thought\" (CoT), which are intermediate natural language reasoning steps, in the prompt to LLMs ( Figure 2). Specifically, in CoT prompting, \u27e8input, output\u27e9 demonstrations are replaced with \u27e8input, chain of thought, output\u27e9 triples, e.g., \"[input] Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? [chain of thought] Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. [output] The answer is 11.\" In this way, given a target question, the model learns to generate explicit ratio- Input* Figure 2: An illustration of Chain-of-Thought Prompting and Rationale Engineering, where asterisk (*) denotes the target problem to be solved.\n\nnale before producing the final answer. Experimental results show that this simple idea can improve LLMs' few-shot performance on arithmetic, symbolic, and commonsense reasoning tasks, sometimes to a striking degree.\n\nThere are several variants of chain-of-thought prompting that have been proposed in the literature, in a different form or to solve a specific problem.\n\nDifferent Form: Kojima et al. (2022) introduce Zero-shot-CoT, in which LLMs are simply prompted with the phrase \"Let's think step by step\" after the input, in order to elicit reasoning without the need for few-shot demonstrations. Madaan et al. Specific Problem/Setting: Before chain of thought, Nye et al. (2022) also try to use intermediate computations, named \"scratchpads\", to improve language models' reasoning performance in both finetuning and few-shot regimes, with a particular focus on programs. Shi et al. (2022) attempt to solve multilingual reasoning tasks with CoT in the native language, CoT in English (regardless of the problem language), and CoT in English (with the problem translated to English). Chen (2022) apply CoT to table-based reasoning, finding that LLMs can achieve strong performance on table tasks with only one exemplar. Prystawski et al. (2022) demonstrate that CoT can improve LLMs' performance on paraphrase selection for metaphors. Lu et al. (2022) apply chain of thought to solve multimodal science questions.", "filtered_refids": [["b78"], [], [], ["b40", "b29", "b49", "b62", "b16"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2460, "num_references": 6}
{"corpusid_sectionid": "254877753-s6", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "Rationale Engineering", "section": "The original version of chain-of-thought prompting, proposed by Wei et al. (2022b), relies on manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation. Rationale engineering aims to more effectively elicit or utilize reasoning in LLMs. This can be achieved through rationale refinement, which involves creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs. A summary of raltionale engineering is illustrated in Figure 2.\n\nRationale refinement. The choice of exemplars can significantly affect the few-shot performance of LLMs, as demonstrated in research such as , which also appears in chain-of-thought prompting. Rationale refinement aims to create and refine rationale examples that are better able to elicit reasoning in LLMs. Fu et al. (2022b) propose complexity-based prompting to create rationales with more reasoning steps. Their experiments show that the performance of LLMs improves with the increased rationale complexity. Similarly,  propose algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations.  design Auto-CoT to automatically construct exemplars by partitioning questions from a given dataset into clusters and then using Zero-Shot-CoT (Kojima et al., 2022) to generate the rationale for a representative question from each cluster. The analysis shows that making exemplars diverse is important in prompting LLMs to produce better rationales.\n\nRationale exploration. In addition to providing better exemplars, we can allow LLMs to fully explore various ways of reasoning to improve their performance on reasoning tasks, named rationale exploration. Based on the idea that complex problems often admit multiple ways of thinking that can lead to their unique correct answer, Wang et al. (2022c) present a decoding strategy called selfconsistency to improve upon the traditional greedy decoding used in chain-of-thought prompting. This strategy involves sampling a diverse set of rationales, rather than just the greedy one, and selecting the most consistent answer by marginalizing out the sampled rationales. The idea is also used in Fu et al. (2022b) to vote over the top complex rationales. To further improve performance,  suggest providing different demonstrations for each question by sampling exemplars from an exemplar base, in order to increase the diversity of the sampled rationales.\n\nRationale verification. Ensuring that the rationales produced by LLMs are valid is critical, as incorrect rationales can lead to incorrect final predictions (Ye and Durrett, 2022). To address this issue, the process of rationale verification aims to verify whether the rationales produced by LLMs lead to the correct final answers. Cobbe et al. (2021) propose augmenting LLMs with a trained verifier that assigns a score to each rationale and solution generated by the LLM, selecting the highest-ranked solution as the final answer when solving math word problems.  also use this technique to guide rationale selection, in conjunction with the process of rationale exploration. Different from the above methods that train an external verifier to verify the rationales, Weng et al. (2022) suggest using LLMs themselves as the verifiers.", "filtered_refids": [["b78"], ["b16", "b23"], ["b74", "b23"], ["b82", "b79"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3430, "num_references": 7}
{"corpusid_sectionid": "254877753-s7", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "Problem Decomposition", "section": "Chain-of-thought prompting, while effective for eliciting reasoning in LLMs, can struggle with complex tasks, e.g., tasks that require compositional generalization (Lake and Baroni, 2018;Keysers et al., 2020). To solve a complex problem, it is helpful to first break it down into smaller, more manageable subproblems. By solving each of these subproblems, we can effectively solve the complex problem. This technique is called problem decom-position or divide and conquer (Talmor and Berant, 2018;Min et al., 2019;Perez et al., 2020).\n\nBased on this idea, Zhou et al. (2022a) propose least-to-most prompting, which consists of two steps: decomposing the complex problem into subproblems and solving these subproblems in a specific order, with each subproblem being facilitated by the answers obtained from previously solved subproblems. As follow-up work, Drozdov et al. (2022) introduce dynamic least-to-most prompting, which is designed to solve more realistic semantic parsing problems by decomposing the problems with prompting-based syntactic parsing and dynamically selecting exemplars based on the decomposition. In addition, Khot et al. (2022) design decomposed prompting, which breaks down a complex problem into subproblems that can be handled by a shared library of prompting-based LLMs, each specialized in a particular subproblem. Furthermore, Dua et al. (2022) develop successive prompting, which iteratively decomposes a complex problem into a simple problem, with the next subproblem prediction having access to the answers to the previous subproblems. While the above methods decompose or solve compositional questions with multiple forward passes, Press et al. (2022) suggest decomposing and solving the input question in one forward pass using CoT prompting. Overall, these techniques show promise for helping LLMs to solve complex tasks by decomposing the problem into more manageable subproblems.", "filtered_refids": [["b17", "b67", "b14", "b45", "b36"], ["b48", "b77", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1917, "num_references": 8}
{"corpusid_sectionid": "254877753-s10", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "Reasoning-Enhanced Training and Prompting", "section": "One approach to improving the reasoning capabilities of LLMs is to pretrain or finetune the models on datasets that include \"reasoning\". ; Taylor et al. (2022) find that LLMs trained on datasets containing scientific and mathematical data can achieve better performance on reasoning tasks like quantitative reasoning problems when using CoT prompting 3 . Pi et al. (2022) show that continually pretraining with SQL data can boost the performance of language models, e.g., T5 (Raffel et al., 2020), on natural language reasoning such as numerical reasoning and logical reasoning. Furthermore, Chung et al. (2022)  finetuning and scratchpad prompting results in a significant improvement in LLMs' ability to generalize to longer problems, while this phenomenon is not observed in the standard fully supervised finetuning paradigm.", "filtered_refids": [["b54", "b46", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 828, "num_references": 3}
{"corpusid_sectionid": "254877753-s11", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "Bootstrapping & Self-Improving", "section": "Instead of finetuning LLMs on pre-built datasets that include reasoning, there are studies that have explored the idea of using LLMs to self-improve their reasoning abilities through a process known as bootstrapping. One example of this is the Self-Taught Reasoner (STaR) introduced by Zelikman et al. (2022), in which a LLM is trained and refined on its own output iteratively. Specifically, with CoT prompting, the model first generates initial rationales. And then, the model is finetuned on rationales that lead to correct answers. This process can be repeated, with each iteration resulting in an improved model that can generate better training data, which in turn leads to further improvements. As a follow-up to this work, Huang et al. (2022a) show that LLMs are able to self-improve their reasoning abilities without the need for supervised data by leveraging the self-consistency of reasoning (Wang et al., 2022c).", "filtered_refids": [["b74", "b26", "b84"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 924, "num_references": 3}
{"corpusid_sectionid": "254877753-s13", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "End Task Performance", "section": "One way to measure reasoning abilities of LLMs is to report their performance, e.g., accuracy, on end tasks that require reasoning. We list some common benchmarks as follows.\n\nArithmetic Reasoning. Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations. This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems. Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015). It is worth mentioning that Anil et al. (2022)  Others. In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning. BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement. Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability. LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015), FetaQA (Nan et al., 2022), as suggested by Chen (2022). In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020;Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d).", "filtered_refids": [[], ["b43", "b57", "b44", "b26", "b25", null, "b24", "b1", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1692, "num_references": 9}
{"corpusid_sectionid": "254877753-s14", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "Analysis on Reasoning", "section": "Although LLMs have demonstrated impressive performance on various reasoning tasks, the extent to which their predictions are based on true reasoning or simple heuristics is not always clear. This is because most existing evaluations focus on their accuracy on end tasks, rather than directly assessing their reasoning steps. While some error analysis has been conducted on the generated rationales of LLMs (Wei et al., 2022b;Kojima et al., 2022, inter alia), this analysis has often been limited in depth.\n\nThere have been some efforts to develop metrics and benchmarks that enable a more formal/deep analysis of reasoning in LLMs. Golovneva et al. (2022) design ROSCOE, a set of interpretable, detailed step-by-step evaluation metrics covering various perspectives including semantic alignment, logical inference, semantic similarity, and language coherence. Saparov and He (2022) create a synthetic dataset called PrOntoQA that is generated from real or fictional ontologies. Each example in the dataset has a unique proof, which can be converted to simple sentences and back again, allowing for a formal analysis of each reasoning step. Han et al. (2022a) introduce a dataset called FO-LIO to test the first-order logic reasoning capabilities of LLMs. FOLIO contains first-order logic reasoning problems that require models to determine the correctness of conclusions given a set of premises. In addition,  conduct ablation experiments on CoT and find that LLMs may also perform reasoning while prompting with invalid rationals. Their study also suggests that being relevant to the query and correctly ordering the reasoning steps are important for CoT prompting.\n\nIn summary, most existing studies primarily report the performance of the models on downstream reasoning tasks, without a detailed examination of the quality of the rationales produced. This leaves open the question of whether the models are actually able to reason in a way that is similar to human reasoning, or whether they are simply able to achieve good performance on the tasks through other means. Further research is needed to more formally analyze the reasoning abilities of LLMs.", "filtered_refids": [["b78", null], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2157, "num_references": 3}
{"corpusid_sectionid": "254877753-s15", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "Findings and Implications", "section": "In this section, we summarize the important findings and implications of studies on reasoning in large language models.\n\nReasoning seems an emergent ability of LLMs. Wei et al. (2022a,b);  show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters). This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks. However, the reason for this emergent ability is not yet fully understood. We refer the reader to Wei et al. (2022a);Fu et al. (2022a) for some potential explanations.\n\nChain of thought elicits \"reasoning\" of LLMs. The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b); . Additionally, Saparov and He (2022) ( \u00a74.2) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual. However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs. Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement. In addition to these benefits, the use of CoT prompts has been shown to improve the out-ofdistribution robustness of LLMs (Wei et al., 2022b;Zhou et al., 2022a;Anil et al., 2022, inter alia), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.\n\nLLMs show human-like content effects on reasoning. According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature. For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions. These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well. This provides some evidence that language models may \"reason\" in a way that is similar to human reasoning.\n\nLLMs are still unskilled at complex reasoning. Although LLMs seem to possess impressive reasoning capabilities with the techniques described in \u00a73, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022);Han et al. (2022a); Ruis et al. (2022). For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively. These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.", "filtered_refids": [[], ["b77", null], ["b78", null, "b77"], ["b18"], [null, "b58", "b71"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3278, "num_references": 9}
{"corpusid_sectionid": "254877753-s17", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "Improving reasoning capabilities of LLMs.", "section": "4 # indicates the finding has not been carefully examined in language models with more than 100 billion parameters.\n\nWhile techniques like chain-of-thought prompting (Wei et al., 2022b) may help to elicit reasoning abilities in large language models, they cannot enable the models to solve tasks beyond their current capabilities. To truly enhance reasoning in LLMs, we need to utilize training data, model architecture, and optimization objectives that are designed to encourage reasoning. For example, finetuning a model with a dataset including CoT data has been shown to improve reasoning , and models can also self-improve through the process of bootstrapping their reasoning (Zelikman et al., 2022;Huang et al., 2022a). There is still much research that needs to be done in this area, and we look forward to future progress in improving reasoning in large language models.", "filtered_refids": [[], ["b78", "b26", "b84"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 878, "num_references": 3}
{"corpusid_sectionid": "254854317-s11", "title": "The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges", "date": "2022-12-19", "section_title": "Linguistic-Driven Approaches", "section": "Equivalence Constraint In a well-formed codeswitched sentence, the switching takes place at those points where the grammatical constraints of both languages are satisfied (Poplack, 1980). Fung (2012, 2013) incorporate this syntactic constraint to a statistical code-switch language model (LM) and evaluate the model on Chinese-English code-switched speech recognition. On the same line of work, Pratapa et al. (2018a); Pratapa and Choudhury (2021) implement the same constraint to Hindi-English CSW data by producing parse trees of parallel sentences and matching the surface order of child nodes in the trees. Winata et al. (2019c) apply the constraint to generate synthetic CSW text and find that combining the real CSW data with synthetic CSW data can effectively improve the perplexity. They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments.\n\nMatrix-Embedded Language Framework (MLF) Myers-Scotton (1997) proposed that in bilingual CSW, there exists an asymmetrical relationship between the dominant matrix language and the subordinate embedded language.\n\nMatrix language provides the frame of the sentence by governing all or most of the most of the grammatical morphemes as well as word order, whereas syntactic elements that bear no or only limited grammatical function can be provided by the embedded language (Johanson, 1999;Myers-Scotton, 2005 Functional Head Constraint Belazi et al. (1994) posit that it is impossible to switch languages between a functional head and its complement because of the strong relationship between the two constituents. Li and Fung (2014) use the constraint of the LM by first expanding the search network with a translation model and then using parsing to restrict paths to those permissible under the constraint.", "filtered_refids": [[null, "b18"], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1809, "num_references": 4}
{"corpusid_sectionid": "254854317-s13", "title": "The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges", "date": "2022-12-19", "section_title": "The Era of Statistical Methods", "section": "The research on CSW is also influenced by the progress and development of machine learning. According to Figure 5, starting in 2006, statistical methods have been adapted to CSW research, while before that year, the approaches were mainly rule-based. There are common statistical methods for text classification used in the literature, such as Naive Bayes (Solorio and Liu, 2008a) and Support Vector Machine (SVM) (Solorio and Liu, 2008b). Conditional Random Field (CRF) (Sutton et al., 2012) is also widely seen in the literature for sequence labeling, such as Part-of-Speech (POS) tagging (Vyas et al., 2014), Named Entity Recognition (NER), and word-level language identification (Lin et al., 2014;Chittaranjan et al., 2014;Jain and Bhat, 2014). HMM-based models have been used in speech-related tasks, such as speech recognition (Weiner et al., 2012a;Li and Fung, 2013) and text synthesis (Qian et al., 2008;Shuang et al., 2010;He et al., 2012).", "filtered_refids": [["b42", null, "b6"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 949, "num_references": 3}
{"corpusid_sectionid": "254854317-s14", "title": "The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges", "date": "2022-12-19", "section_title": "Utilizing Neural Networks", "section": "Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.\n\nNeural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage. RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a). DNN-based and hybrid HMM-DNN models are used in speech recognition models .\n\nPre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer. Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017). A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018). A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b). Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021). Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b). In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.\n\nLanguage Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a). These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages. Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets. Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text. Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context. While XLM-R provides the best result, it is also computationally heavy. There needed to be more exploration of larger models. We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach. Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available. However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations. CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP. Therefore, one future direction is to broaden the language scope of CSW research.", "filtered_refids": [[], [null, "b18", "b17"], [null, "b19"], [null, "b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 3645, "num_references": 7}
{"corpusid_sectionid": "259108815-s1", "title": "Mapping Brains with Language Models: A Survey", "date": "2023-06-08", "section_title": "Datasets", "section": "To infer a mapping between language models and brains, researchers rely on datasets in which brain activity is recorded in response to linguistic stimuli. In some studies, the stimuli are single words (Mitchell et al., 2008;Anderson et al., 2017) or sentences displayed on a screen (Pereira et al., 2018). In others, participants read longer stories (Wehbe et al., 2014a;Bhattasali et al., 2020;Nastase et al., 2021) or listened to speech or podcasts (Huth et al., 2016;Antonello et al., 2021). Table 1 lists publicly available datasets that have been used in the context of mapping language models to and from recordings of brain response. Differences between the datasets -the number of participants, the equipment, the experimental setup, pre-processing steps, and probabilistic corrections -should lead us to expect some variation in what researchers have concluded (Hollenstein et al., 2020).\n\n3 How to predict brain activity?\n\nIn this section, we survey work in which neural responses are predicted from linguistic representations. Such work typically aims to shed light on how language functions in the brain. One of the earliest studies exploring the mapping between brain and language representations is by Mitchell et al. (2008), who trained a linear regression model on a set of word representations extracted from 60 nouns using 115 semantic features based on cooccurrence statistics, to predict the corresponding fMRI representations of the same nouns. They use pair-wise matching accuracy evaluation, extracting two words w and w \u2032 for evaluation, and showed that the predicted fMRI for a word w was closer to the real fMRI image for w than to the real fMRI image for w \u2032 , at above-chance levels. Mitchell et al. (2008) also report percentile rank results, ranking predicted fMRI images by similarity with the real image of w. We discuss how the metrics relate in \u00a76.\n\nThe dataset of Mitchell et al. (2008)is also used by Murphy et al. (2012), who extract linguistic features from part-of-speech taggers, stemmers, and dependency parsers, showing that dependency parsers are the most successful in predicting brain activity. They also use leave-2-out pair-matching as their performance metric.\n\nLater on, Wehbe et al. (2014a) moved on to predicting brain activation patterns for entire sentences rather than for isolated words. They recorded fMRI neural response measurements while participants read a chapter from Harry Potter and the Sorcerer's Stone, then extracted a set of 195 features for each word (ranging from semantic, syntactic properties to visual and discourse-level features) to train a comprehensive generative model that would then predict the time series of the fMRI activity observed when the participants read that passage. Leave-2-out pair-matching accuracy is used for evaluation. Huth et al. (2016), in contrast, use fMRI recordings of participants listening to spoken narrative stories, representing each word in the corpus as a 985-dimensional vector encoding semantic information driven by co-occurrence statistics. They train per-voxel linear regression models and evaluate their predicted per-word fMRI images by their per-voxel Pearson correlation with the real fMRI images, showing that 3-4 dimensions explained a significant amount of variance in the FMRI data. Wehbe et al. (2014b) are among the first to use neural language models, using recurrent models to compute contextualized embeddings, hidden state vectors of previous words, and word probabilities.  of participants reading Harry Potter, obtained in a follow-up study to Wehbe et al. (2014a). From the three sets of representations, they then train linear regression models to predict the MEG vectors corresponding to each word, and the regression models are then evaluated by computing pair-matching accuracy.\n\nSimilarly, S\u00f8gaard (2016) evaluates static word embeddings on the data from Wehbe et al. (2014a), learning linear transformation from word embeddings into an fMRI vector space. The predictions are evaluated through mean squared error (MSE).\n\nJain and Huth (2018) evaluate recurrent language models against the fMRI dataset from Huth et al. (2016). Their findings show that contextual language model representations align significantly better (to brain response) compared to static word embedding models. Their evaluation metric is the total sum of explained variance 1 Following this, Schwartz et al. (2019) use attention-based transformer language models for brain mapping. They finetune BERT (Devlin et al., 2019)to predict neural response measurements from the Harry Potter dataset, showing that the fine-tuned models have representations that encode more brain-activity-relevant language information than the non-finetuned models. They rely on pair-matching accuracy as their performance metric.\n\nAs in S\u00f8gaard (2016), Zhang et al. (2020) map static word embeddings into the vector space of the neural response measurements (fMRI). They introduce a new dataset of such measurements from subjects listening to natural stories. They rely on explained variance as their performance metric.\n\nToneva and Wehbe (2019) evaluate word and sequence embeddings from 4 recurrent and attention-based transformer language models, using the Harry Potter fMRI dataset. They evaluate models across layers, context lengths, and attention types, using pairwise matching accuracy as their performance metric. In a later study, Toneva et al. (2022a) induce compositional semantic representations of \"supra-word meaning\" which they then use to predict neural responses across regions of interest, evaluating their models using Pearson correlation.\n\nAlso using the Harry Potter data,  evaluate five models, one static and four contextualized, relying on a variant of representational similarity analysis (Kriegeskorte et al., 2008). The results suggest that models provide representations of local contexts that are well-aligned to neural measurements. However, as information from further away context is integrated by the models, representations become less aligned to neural measurements.\n\nIn a large-scale study, Schrimpf et al. (2021) examine the relationships between 43 diverse stateof-the-art neural network models (including embedding models, recurrent models, and transformers) across three datasets (two fMRI, one electrocardiography). They rely on a metric they term Brain Score which involves normalising the Pearson correlation by a noise ceiling. Their results show that transformer-based models perform better than recurrent or static models, and larger models perform better than smaller ones.\n\nSimilarly, in , the Schoffelen et al. (2019) fMRI and MEG datasets are used to compare a variety of transformer architectures. They study how architectural details, training settings, and the linguistic performance of these models independently account for the generation of brain correspondent representations. The results suggest that the better language models are at predicting words from context, the better their activations linearly map onto those of the brain. Antonello et al. (2021) evaluate three static and five attention-based transformer models, in combination with four fine-tuning tasks and two machine translation models. They train linear regression models to evaluate their word-level representations against a new fMRI dataset from participants listening to podcast stories. They find a low-dimensional structure in language representations that can predict brain responses. In a similar setting, Antonello and Huth (2022) examine why some features fit the brain data better arguing that the reason is that they capture various linguistic phenomena.\n\nReddy and Wehbe (2021) evaluate syntactic features in conjunction with BERT representations, finding that syntax explains additional variance in brain activity in various parts of the language system, even while controlling for complexity metrics that capture processing load.\n\nIn a series of studies Caucheteux et al. (2021Caucheteux et al. ( , 2022b investigate GPT2's activations in predicting brain signals using the Nastase et al. (2021) dataset. Their evaluation metric is Brain Score (Schrimpf et al., 2018). To determine which factors affect the brain encoding Pasquiou et al. (2022) examine the impact of test loss, training corpus, model architecture, and fine-tuning in various models using the Li et al. (2022) dataset. They evaluate model performance using Pearson Correlation.\n\nOota et al. (2022a) study the impact of context size in language models on how they align with neural response measurements. They use the Nastase et al. (2021) dataset and evaluate recurrent and attention-based transformer architectures. In a later study, Oota et al. (2022b) use the Pereira et al. (2018) dataset and evaluate BERTbase models (fine-tuned for various NLP tasks). They showed that neural response predictions from ridge regression with BERT-base models fine-tuned for coreference resolution, NER, and shallow syntactic parsing explained more variance for Pereira et al. (2018) response measurements. On the other hand, tasks such as paraphrase generation, summarization, and natural language inference led to better encoding performance for the Nastase et al. (2021) data (audio). Using the same dataset, in Oota et al. (2022c) it is shown that the presence of surface, syntactic, and semantic linguistic information is crucial for the alignment across all layers of the language model. They use pairwise matching accuracy and/or Pearson correlation as their performance metrics in these studies.\n\nAw and Toneva (2023) extract feature representations from four attention-based transformer models. They evaluate the impact of fine-tuning on the BookSum dataset (Kryscinski et al., 2021). All models are used to predict brain activity on the Harry Potter data. Pairwise matching accuracy and Pearson correlation are their performance metrics. Merlin and Toneva (2022) focus more narrowly on variants of GPT-2, showing that improvements in alignment with brain recordings are probably not because of the next-word prediction task or wordlevel semantics, but due to multi-word semantics. Their reported metric is Pearson correlation.\n\nIntermediate summary The above studies differ in many respects. Several metrics are used: pairwise-matching accuracy, 2 Pearson correlation (or Brain Score), mean squared error, and representational similarity analysis. Even studies that report the same performance metrics are not directly comparable because they often report on results on different datasets and use slightly different protocols, e.g., Murphy et al. (2012) and Wehbe et al. (2014b). Beinborn et al. (2023) compare various encoding experiments and receive very diverse results for different evaluation metrics. The diversity of metrics and data renders a direct comparison difficult. To remedy this, we consider how the metrics compare in \u00a76.", "filtered_refids": [["b67", "b10", "b40", "b50", "b25", "b27", "b22", "b5", "b3"], [], ["b40"], ["b41", "b40"], ["b67", "b68", "b27"], ["b59", "b67"], [null, "b17"], ["b69"], ["b63"], ["b31"], ["b56"], [null, "b5"], [], ["b12", "b57", "b14", "b48", "b22", "b23"], ["b45", "b22", "b50"], ["b34", "b32"], ["b68", "b8", "b41"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 71, "num_chars": 10929, "num_references": 39}
{"corpusid_sectionid": "259108815-s2", "title": "Mapping Brains with Language Models: A Survey", "date": "2023-06-08", "section_title": "How to predict linguistic stimuli?", "section": "Decoding models work in the other direction and aim to predict linguistic features of the stimuli from recordings of brain response. Pereira et al. (2018) introduce a decoder that predicts stimuli representation of semantic features given fMRI data.They introduce a novel dataset of neural responses aligned with annotation of concrete and abstract semantic categories (such as pleasure, ignorance, cooking etc.). They evaluate static word embeddings by applying ridge regression to predict per-word fMRI vectors. A separate regression model is trained per dimension, allowing for dimension-wise regularization. The model is evaluated in terms of pairwise matching accuracy, but also in terms of percentile rank, adapted to the decoding scenario.\n\nGauthier and Levy (2019) also train linear regression models which map from the response measurements in Pereira et al. (2018), but to representations of the same sentences produced by the BERT language model finetuned on different natural language understanding tasks. The regression models are evaluated using two metrics: mean squared error and average percentile rank. Their results show that fine-tuning with different NLU objectives leads to worse alignment and that, somewhat surprisingly, the only objective which does lead to better alignment is a scrambled language modeling task where the model is trained to predict scrambled sentences.\n\nMinnema and Herbelot (2019) re-examine the work of Pereira et al. (2018) using various metrics (pairwise matching accuracy, percentile rank, cosine distance, R 2 , RSA), comparing decoder models (ridge regression, perceptron, and convolutional neural networks). 3 They show that positive results are only obtained using pairwise matching accuracy. Abdou et al. (2021) investigate whether aligning language models with brain recordings can be improved by biasing their attention with annotations from syntactic or semantic formalisms. They fine-tune the BERT models using several syntactosemantic formalisms and evaluate their alignment with brain activity measurements from the Wehbe et al. (2014a) and Pereira et al. (2018) datasets. Their results -obtained using Pearson correlation as performance metric -are positive for two in three formalisms. Zou et al. (2022) propose a new evaluation method for decoding, a so-called cross-modal cloze task. They generate the data for the task from the neural response measures in Mitchell et al. (2008) and Wehbe et al. (2014a). The task itself amounts to a cloze task in which the context is prefixed by the fMRI image of the masked word. They evaluate models using precision@k. Note how this task is considerably easier than linearly mapping from language model representations into fMRI images, and precision@k results therefore cannot be compared to those obtained in other settings. Their best precision@1 scores are around 0.3, but only marginally (0.03) better than a unimodal LM.\n\nFinally, Pascual et al. (2022) try a more realistic setup by predicting language from fMRI scans of subjects not included in the training. They use the (Pereira et al., 2018) dataset and evaluate the regression models based on pairwise accuracy and precision@k (or top-k accuracy). They propose evaluating with direct classification as a more demanding setup to evaluate and understand current brain decoding models.\n\nIntermediate summary Decoding studies also differ in many respects. Several metrics are used: pairwise-matching accuracy, Pearson correlation, percentile rank, cosine distance, precision@k, and representational similarity analysis; and several datasets are used. Gauthier and Ivanova (2018) criticize the evaluation techniques of decoding studies and suggest adopting task and mechanism explicit models. It is of particular interest to our study that both Minnema and Herbelot (2019) only report positive results for pairwise matching accuracy compared to other metrics. This suggests pairwise matching accuracy is a less conservative metric (and maybe less reliable).", "filtered_refids": [["b50"], ["b50"], ["b67", "b40", "b50", "b70", "b0"], ["b47", "b50"], ["b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 4016, "num_references": 10}
{"corpusid_sectionid": "259108815-s3", "title": "Mapping Brains with Language Models: A Survey", "date": "2023-06-08", "section_title": "Performance Metrics", "section": "We present the evaluation metrics used in the above studies and discuss how they relate. See Table 2 for a summary of metrics and corresponding studies. Mitchell et al. (2008) introduce pairwise matching accuracy. Because of their small sample size, they use a leave-2-out cross-validation, which later work also adopted. The metric is a binary classification accuracy metric on a balanced dataset, so a random baseline converges toward 0.5. Many studies have relied on this metric, both in encoding and decoding (see Table 2). 4 Pearson correlationPearson correlation is another widely used metric in the studies surveyed above, measuring the linear relationship between variables, and providing insight into the strength and direction of their association. Huth et al. (2016), compute Pearson correlation between predicted and actual brain responses using Gaussian random vectors to test statistical significance. Resulting p-values are corrected for multiple comparisons within each subject using false discovery rate (FDR) (Benjamini and Hochberg, 1995). Others have used Bonferroni correction (Huth et al., 2016) or block-wise permutation test (Adolf et al., 2014) to evaluate the statistical significance of the correlation (Zhang et al., 2020). Some report R 2 (explained variance) instead of or in addition to correlation coefficients (Minnema and Herbelot, 2019;Reddy and Wehbe, 2021). Others have adopted a more elaborate extension of Pearson correlation, namely BrainScore (Schrimpf et al., 2018). Brain-Score is estimated on held-out test data, calculating Pearson's correlation between model predictions and neural recordings divided by the estimated ceiling and averaged across voxels and participants.\n\nPercentile rank was first used for encoding (Mitchell et al., 2008), but can also be used for decoding (Pereira et al., 2018;Gauthier and Levy, 2019;Minnema and Herbelot, 2019). In encoding, the predicted brain image for w is ranked along the predicted images for a set of candidate words w \u2032 by their similarity to the real (ground truth) image for w. The average rank is then reported. For decoding, they rank word vectors rather than neural response images. Note the similarity metric is unspecified, but typically cosine distance is used.\n\nMean squared error, the average of the squared differences between word vectors and neural responses, was first used for encoding in S\u00f8gaard (2016) on a held-out test split. It was also used by Gauthier and Levy (2019).\n\nRepresentational similarity analysis (RSA) was introduced in Kriegeskorte et al. (2008) as a non-parametric way to characterize structural alignment between the geometries of representations derived from disparate modalities. RSA abstracts away from activity patterns themselves and instead computes representational similarity matrices (RSMs), which characterize the information carried by a given representation method through global similarity structure. A rank correlation coefficient is computed between RSMs derived from the two spaces, providing a summary statistic indicative of the overall representational alignment between them. Being non-parametric, RSA circumvents many of the various methodological weaknesses (such as over fitting, etc.). Gauthier and Levy (2019), Minnema and Herbelot (2019), and  apply (variations of) RSA to investigate the relations between different model components, and then to study the alignment of these components with brain response.\n\nCosine similarity was used in Mitchell et al. (2008) to select between the candidate images in pairwise matching accuracy, as well as in percentile rank and RSA, but the raw cosine similarities between predicted and real images or embeddings can also be used as a metric. Minnema and Herbelot (2019) use this metric to quantify how close the predicted word vectors are to the target. Finally, Zou et al. (2022) use precision@k, a standard metric in other mapping problems, e.g., cross-lingual word embeddings (S\u00f8gaard et al., 2019).\n\nComparisons Most metrics are used to evaluate both encoding and decoding models (pairwise matching accuracy, Pearson correlation, percentile rank, MSE, RSA, cosine distance). Results for two of the most widely used metrics -pairwise matching accuracy 5 and percentile rank -tend to be around 0.7-0.8 with generally better results for more recent architectures and larger LMs. To draw conclusions across studies relying on different metrics, we need to investigate which metrics are more conservative, and how different metrics relate.\n\nPairwise matching accuracy vs. Pearson correlation It seems that pairwise matching accuracy tends to increase monotonically with Pearson correlation. Consider three sets of distances over corresponding point sets, A, B, and C. If A and B are more strongly linearly correlated than A and C, under an optimal linear mapping \u2126 (minimizing point-wise squared error distance), E[(a \u2212 b\u2126) 2 ] > E[(a \u2212 c\u2126) 2 ]. Even in this conservative setting in our synthetic experiments in Appendix A.1, the correlation between matching accuracy and percentile rank was very high,~0.9.\n\nPairwise matching accuracy vs. percentile rank Both metrics have random baseline scores of 0.5, and they will converge in the limit. If a has a percentile rank of p in a list A, it will be higher than a random member of A p percent of the time. In our experiments in Appendix A.1, the correlation converges toward 1.0, with values consistently higher than 0.8 for N = 100.\n\nPairwise matching accuracy vs. precision@k are also positively correlated. Perfect score in one entails perfect score in the other, but precision@k can of course be very small for very high values of pairwise matching accuracy (especially if the set of candidate words is big). Conversely, we can have 5 When discriminating averages over 20 images (Wehbe et al., 2014b), scores are naturally lower. saturation for high values of k, because matching accuracies higher than n\u2212k n will mean near-perfect precision@k scores. In practice, precision@k (for low values of k) will be much more conservative, however. The correlation coefficient for N = 100 (see Appendix A.1) tends to lie around 0.7.\n\nRelative strength Pairwise Matching Accuracy is a relatively permissible performance metric. To see this, consider the scenario in which all target words can be divided into two equal-sized buckets based on word length (number of characters). Say the neural responses capture nothing but this binary distinction between long and short words, but do so perfectly. Moreover, our mapping method, e.g., linear regression, learns this from training data. Now, from this alone, the pairwise matching accuracy will converge toward \u00b5 = 0.75, since our model will do perfectly (1.0) on half of the data, and exhibit random performance (0.5) on the other half. If the neural responses tracked word length (and not just the distinction between short and long words), performance would be even better. In other words, Pairwise Matching Accuracy scores around 0.7-0.8 (observed in the studies above) may only reflect very shallow processing characteristics. The fact that Minnema and Herbelot (2019) only observed good results with this metric, led them to adopt a rather critical stance, for good reasons.\n\nOther metrics are clearly more conservative. For a set of n candidate words, a random mapping will induce a precision@1-score of 1 n . While hubs may inflate scores for larger values, the metric is extremely conservative for small values of k. However, only Zou et al. (2022) use this metric, and they modify the experimental protocol substantially, making the task much easier by providing additional input to a non-linear model. The small improvement from adding neural response input is interesting, but could potentially be explained by shallow processing characteristics.\n\nThey argue that analogy testing would provide a better evaluation protocol: one would ideally use standard metrics such as semantic relatedness judgment tasks, analogy tasks, etc.\n\n[but] this is not possible due to the limited vocabulary sizes of the available brain datasets Such evaluation is possible on small scale, though, and increasingly larger fMRI datasets are becoming available (see above). Zhang et al. (2020) have identified analogical reasoning in fMRI brain activation spaces. The analogies are computed using vector offset and probe the systematicity of how semantic relations are encoded. If a model encodes the capital-of relation systematically, we can retrieve the capital of Germany by subtracting the fMRI vector for 'Paris' from the sum of our the fMRI vectors for Germany and France. This is the same kind of analogical reasoning found in language models (Mikolov et al., 2013). Garneau et al. (2021) show that the more language models satisfy analogies, the more isomorphic they are.\n\nSo far, it seems that, with the possible exception of Zhang et al. (2020), there is little evidence for structural similarities, beyond what could be induced by shallow processing characteristics, but what about all the studies that report strong Pearson correlations? Per-voxel correlation coefficients are low on average, but across the above studies, typically only around 4-40% of the voxels exhibit significant correlations (Huth et al., 2016;. Since these correlations have been replicated across different datasets, they are generally not disputed, but could still reflect rather shallow processing characteristics.\n\nOn a more positive note, several studies show that larger (and better) language models align better with neural response measurements (Schrimpf et al., 2021;. This suggests that language models in the future may align even better with such measurements, possibly reflecting properties of deep processing. Such correlations with model quality and size are positive, making the results reported above more credible.\n\nGenerally, the conclusions we can draw from the above studies are somewhat vague. There are two reasons for this: (i) Past studies have relied on permissible (pairwise matching accuracy) and ambiguous (Pearson correlation) performance metrics; and (ii) past studies have relied on small-sized datasets. We believe that this calls for a meta-analysis of the above studies. To provide grounds for such a meta-analysis, we have in this section taken steps to compare the metrics used in these studies. We leave it for future work to explore various ways effect sizes can be computed across these studies.", "filtered_refids": [["b57", "b40", "b9", "b27", null, "b69", "b53", "b38", "b2"], ["b20", "b38", "b50", "b40"], ["b20"], ["b20", "b38", "b31"], ["b70", "b38", "b60", "b40"], [], [], [], ["b68"], ["b38"], ["b70"], [], ["b69", "b18", "b35"], ["b69", "b27"], ["b56"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 74, "num_chars": 10491, "num_references": 30}
{"corpusid_sectionid": "264426545-s3", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "Prosodic Errors", "section": "Prosodic features encompass elements that influence the pronunciation of an entire word or sentence, including stress, rhythm, and intonation.Errors related to prosodic features involve the production of larger sound units.For intelligibility, prosodic features particularly play a significant role (Raux and Kawahara, 2002).This is especially true for tonal languages (Dahmen et al., 2023) where variation in the pitch can lead to words with different meanings.Prosodic errors are often languagedependent and categorized by: stress (lexical and sentence), rhythm, and intonation.Accent PCC: 68% (Rasipuram et al., 2015) ERJ (Minematsu et al., 2004) * English Japanese /68,000 200 # Utterance PCC (Luan et al., 2012).Word Intelligibility (Minematsu et al., 2011).Phoneme Errors (Ito et al., 2005) CU-CHLOE (Meng et al., 2007a Stress is the emphasis placed on certain syllables in a word or sentence.It is articulated by increasing the loudness, duration, and pitch of the stressed syllable.It can be categorized as lexical stress, if the stress is placed on syllables within the word, or sentence stress if the stress is placed on words within sentences.Mandarin learners of English have contrastive stress at the word-level that is absent in Korean, Mandarin speakers can have an advantage over Korean speakers in stress processing of English words (Wang, 2022).\n\nRythm is the pattern of stressed and unstressed syllables in a word or sentence.A language can be classified as either stress-timed or syllable-timed (Ohata, 2004;Matthews, 2014).In stress-timed languages, the duration of stressed syllables tends to dominate the overall time required to complete a sentence.Conversely, in syllable-timed languages, each syllable receives an equal amount of time during production.\n\nIntonation refers to the melodic pattern and pitch variations in speech.L2 learners of Vietnamese and Mandarin Chinese encounter significant difficulty in acquiring distinct tones, particularly if their native language lacks tonality.Such tonal languages rely on different pitch patterns to convey distinct meanings, making it challenging for learners to accurately grasp and reproduce these tonal variations (Nguyen et al., 2014;Chen et al., 2015).", "filtered_refids": [["b94", "b97", "b98", "b113", "b26", "b58", "b91", "b112", "b138"], ["b93", "b106"], [null, "b104"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2230, "num_references": 13}
{"corpusid_sectionid": "264426545-s4", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "Pronunciation Constructs", "section": "The motivation behind mastering L2 pronunciation is to communicate properly in the target language.Most of the time, these successes are measured using three pronunciation constructs (Uchihara, 2022) -Intelligibility, Comprehensibility, and Accentedness.These are perceived measures, that are partially independent with overlapping features.\n\nIntelligibility can be defined using the accuracy of the sound, word, and utterance itself along with utterance-level completeness (Abercrombie, 1949;Gooch et al., 2016).Accuracy refers where the learner pronounces each phoneme, or word in the utterance correctly.In contrast, completeness measures the percentage of words pronounced compared to the total number of words.\n\nComprehensibility, on the other hand, is defined based on the perceived ease or difficulty that listeners experience when understanding L2 speech.Fluency, defined by the smoothness of pronunciation and correct usage of pauses (Zhang et al., 2021b), is observed to be one of the key factors that determine the level of comprehensibility, along with good linguistic-knowledge and discourse-level organization (Trofimovich and Isaacs, 2012;Saito et al., 2016).\n\nAmong the three constructs, accentedness, which is defined as \"listeners' perceptions of the degree to which L2 speech is influenced by their native language and/or colored by other non-native features\" (Saito et al., 2016).It is often confused with both comprehensibility and intelligibility, influencing pronunciation assessment.The accent is an inherent trait that defines a person's identity and is one of the first things that a listener notices.It is often observed that most of the unintelligible speech is identified as highly accented whereas highly accented speech is not always unintelligible (Derwing and Munro, 1997;Kang et al., 2018;Munro and Derwing, 1995).Thus accents complicate fine-grained pronunciation assessment as it is harder to pinpoint (supra-)segment-level error.", "filtered_refids": [[], ["b49", "b0"], ["b155", "b130", "b119"], ["b59", "b100", "b31", "b119"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1966, "num_references": 9}
{"corpusid_sectionid": "264426545-s7", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "Classification based on Acoustic Phonetics", "section": "Classifier-based approaches explored both segmental and prosodic aspects of pronunciation.Segmental approaches involve the use of classifiers targeting specific phoneme pair errors, utilizing different acoustic features such as Mel-frequency cepstral coefficients (MFCCs) along with its first and second derivative, energy, zero-cross, and spectral features (Van Doremalen et al., 2009;Huang et al., 2020), with different techniques such as Linear Discriminant Analysis (LDA) (Truong et al., 2004;Strik et al., 2009), decision trees (Strik et al., 2009).Prosodic approaches focus on detecting lexical stress and tones, utilizing features such as energy, pitch, duration, and spectral characteristics, with classifiers like Gaussian mixture models (GMMs) (Ferrer et al., 2015), support vector machines (SVMs) (Chen and Wang, 2010;Shahin et al., 2016), and deep neural network (DNNs) (Shahin et al., 2016), and multi-distribution DNNs (Li et al., 2018a).", "filtered_refids": [["b12", "b57", "b133", "b121", "b77", "b42", "b131", "b125"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 3, "num_chars": 952, "num_references": 8}
{"corpusid_sectionid": "264426545-s9", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "Likelihood-based Scoring and GOP", "section": "The initial likelihood-based MD algorithms aim to detect errors at the phoneme level using pre-trained HMM-GMM ASR models.Notably, Kim et al. (1997) introduced a set of three HMM-based scores, including likelihood scores, log posterior scores, and segment-duration-based scores.Among these three, the log-based posterior scores are widely adopted due to their high correlation with human scores, and are also used to calculate the popular 'goodness of pronunciation' (GOP) measure.The GMM-HMM based GOP scores can be defined by the Equation 1.\n\nO denotes a sequence of acoustic features, p stands for the target phone, and Q represents the set of phones.These scores are further improved using forced alignment framework (Kawai and Hirose, 1998).\n\nMore details are presented in Witt and Young (2000).", "filtered_refids": [["b66"], ["b61"], ["b142"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 800, "num_references": 3}
{"corpusid_sectionid": "264426545-s10", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "Reformulations of GOP", "section": "To enhance further the effectiveness of GOP scoring, (Zhang et al., 2008) are first to propose a logposterior normalized GOP defined as:\n\nBuilding upon this, Wang and Lee (2012) adopted the GOP formulation and incorporate error pattern detectors for phoneme mispronunciation diagnosis tasks.With the emergence of DNN in the field of ASR, Hu et al. (2013Hu et al. ( , 2015a,b) ,b) demonstrated that using a DNN-HMM ASR for GOP yields improved correlation scores surpassing GMM-HMM based GOP.The GOP and its reformulation represent a significant milestone.It leverages pre-trained acoustic models on the target language without the necessitating of speaker's L1 knowledge.Furthermore, it offers the advantage of being computationally efficient to calculate.However, these scores lack context-aware information that is crucial for accurate pronunciation analysis.To overcome this, Sudhakara et al. ( 2019) presented a context-aware GOP formulation by adding phoneme state transition probabilities (STP) extracted from HMM model to the GOP score calculation.Furthermore, Shi et al. (2020) proposed a context-dependent GOP, incorporating a phoneme duration factor \u03b1 i , and phonemes transition factor \u03c4 .The formulated GOP score combines all the contextual scores as illustrated in Equation 3.\n\nFor sentence accuracy evaluation, one common approach is to calculate the average GOP scores across phonemes (Kim et al., 1997;Sudhakara et al., 2019).However, relying solely on averaging GOP scores at the phoneme level is limited.A recent approach in (Sheoran et al., 2023) proposed a combination of phone feature score and audio pitch comparison using dynamic time warping (DTW) with an ideal pronounced speech, as a score to assess prosodic, fluency, completeness, and accuracy at the sentence level.Inspired by GOP, Tong et al. (2015) proposed Goodness of Tone (GOT) based on posterior probabilities of tonal phones.\n\nWhile efforts have been made to improve the GOP formulation, it is important to acknowledge that the GOP score still has limitations, specifically in its ability to identify specific types of mispronunciation errors (deletion, insertion, or substitution), and it also demonstrates a degree of dependency on the language of the acoustic model.", "filtered_refids": [["b153"], ["b54", "b123", null], ["b66", null, "b129", "b126"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2254, "num_references": 8}
{"corpusid_sectionid": "264426545-s11", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "End-to-End Modeling", "section": "In the new era of DNNs and Transformers, there is a significant exploration by researchers in leveraging the power of these models and training endto-end pronunciation systems.Li et al. (2017) introduced LSTM mispronunciation detector leveraging phone-level posteriors, time boundary information, and posterior extracted from trained DNNs models on the classification of phonetic attributes (place, manner, aspiration, and voicing).In contrast, Kyriakopoulos et al. (2018) introduced  2022) enhanced the later model using a triplet of features consisting of acoustic, phonetic, and linguistic embeddings.Subsequently, GOP features extracted from pre-trained ASR are enhanced using a Transformer encoder to predict a range of scores of prosodic and segmental scores (Gong et al., 2022), or using additional SSL representation features, energy, and duration within the same architecture (Chao et al., 2022), or using Conformer encoder explored in (Fan et al., 2023).Moreover, PEPPANET is also a transformer-based mispronunciation model, but can jointly model the dictation process and the alignment process, and it provides corresponding diagnostic feedback (Yan et al., 2023a).A subsequent improvement of PEP-PANET uses knowledge about phone-level articulation traits with a graph convolutional network (GCN) to obtain more discriminative phonetic embeddings (Yan et al., 2023b).Recently, Zhang et al. (2023) proposed recurrent neural network transducer RNN-T for L2 phoneme sequence prediction along with an extended phoneme set and weakly supervised training strategy to differentiate similarsounding phonemes from different languages.\n\nSeveral approaches have also been proposed for supra-segmental features scoring.Yu et al. (2015), proposed a new approach where traditional time-aggregated features are replaced with timesequence features, such as pitch, to preserve more information without requiring manual feature engineering, a BiLSTM model is proposed for fluency predictions.Tao et al. (2016); Chen et al. (2018), studied different DNNs models such as CNN, BiL-STM, Attention BiLSTM to predict the fluency and prosodic scoring.(Lin and Wang, 2021) utilized deep features directly from the acoustic model instead of relying on complex feature computations like GOP scores with a scoring module, incorporating a self-attention mechanism, which is designed to model human sentence scoring.More recently, (Zhu et al., 2023) proposed BiLSTM model trained to predict the intelligibility score of a given phoneme or word segment using an annotated intelligibility L2 speech using shadowing.\n\nTowards lexical stress detection, several methods have been proposed to improve accuracy and performance.Ruan et al. (2019) proposed a sequenceto-sequence approach using the Transformer model upon the need for long-distance contextual information to predict phoneme sequence with stress marks.Furthermore, Korzekwa et al. (2020a) intro-duced an attention-based neural network focusing on the automatic extraction of syllable-level features that significantly improves the detection of lexical stress errors.\n\nTone classification has received significant attention in Mandarin language learning due to the crucial role that tones play in Mandarin Chinese.To address the challenge several methods have been proposed.One approach involves training a DNN to classify speech frames into six tone classes (Ryant et al., 2014).Inspired by this, DNNs have been used to map combined cepstral and tonal features to frame-level tone posteriors.These tone posteriors are then fed into tone verifiers to assess the correctness of tone pronunciation (Lin et al., 2018;Li et al., 2018b).Another study utilizes CNN to classify syllables into four Mandarin tones (Chen et al., 2016a).Similarly, ToneNet, a CNNbased network is introduced for Chinese syllable tone classification using mel-spectrogram as a feature representation (Gao et al., 2019).Additionally, a BiLSTM model is proposed as an alternative to capture long-term dependencies in acoustic and prosodic features for tone classification (Li et al., 2019).", "filtered_refids": [["b10", "b146", "b152", "b79", "b48", "b70", "b38", "b145"], ["b162", "b13", "b150", "b127", "b84"], [null, "b117"], ["b11", "b46", "b118", "b89", "b80", "b81"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 4094, "num_references": 21}
{"corpusid_sectionid": "264426545-s12", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "Self-Supervised Models", "section": "Motivated by the recent success of self-supervised learning methods (Baevski et al., 2020;Hsu et al., 2021;Chen et al., 2022;Mohamed et al., 2022) in speech recognition and related downstream tasks such as emotion recognition, speaker verification, and language identification (Chen and Rudnicky, 2023;Fan et al., 2020), self-supervised approaches is employed also in this field.Xu et al. (2021) explored finetuning wav2vec 2.0 on frame-level L2 phoneme prediction.A pretrained HMM-DNN ASR is used to extract time force-alignment.To overcome the dependency on time alignment, Peng et al. (2021) propose a CTC-based wav2vec 2.0 to predict L2 phonemes sequences.Building upon this work, Yang et al. (2022) propose an approach that leverages unlabeled L2 speech using momentum pseudo-labeling.In a contrasting approach, (Lin and Wang, 2022b) combined wav2vec 2.0 features and phoneme text embeddings in a jointly learning framework to predict frame-level phoneme sequence and detect boundaries.Recently, EL Kheir et al. (2023a) explored the multi-view representation utilizing mono-and multilingual wav2vec 2.0 encoders to capture different aspects of speech production and leveraging articulatory features as auxiliary tasks to phoneme sequence prediction.Furthermore, Kheir et al. (2023b) introduces a novel L1-aware multilingual, L1-MultiMDD, architecture for addressing mispronunciation in multilingual settings encompassing Arabic, English, and Mandarin using wav2vec-large pre-trained model as the acoustic encoder.L1-MultiMDD is enriched with L1-aware speech representation, allowing it to understand the nuances of each speaker's native language.\n\nSSL models have proven to be effective in predicting fluency and prosodic scores assigned by human annotators.Kim et al. (2022); Lin and Wang (2023a); Yang et al. (2022) fine-tuned wav2vec 2.0 and Hubert to predict prosodic and fluency scores.\n\nSimilarly, another research conducted in (Lin and Wang, 2023a) jointly predicts L2 phoneme sequence using CTC loss, and predicts prosodic scores using fused acoustic representations with phoneme embeddings.Subsequently Lin and Wang (2023b) introduced a fusion of language embedding, representation features and build a unified framework for multi-lingual prosodic scoring.Recently, Chao et al. ( 2022", "filtered_refids": [["b39", "b14", "b144", "b147", null, "b19", "b99", "b53", "b109"], ["b147", "b65"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2298, "num_references": 11}
{"corpusid_sectionid": "264426545-s13", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "Unsupervised Approaches", "section": "It is important to note that the aforementioned approaches for studying mispronunciation detection typically involve the need for expert knowledge, laborious manual labeling, or dependable ASR results, all of which come with significant costs.In contrast, recent years have witnessed considerable endeavors in unsupervised acoustic pattern discovery, yielding sub-optimal outcomes.Lee and Glass (2012) initially investigated a comparison-based approach that analyzes the extent of misalignment between a student's speech and a teacher's speech.In subsequent studies Lee and Glass (2015); Lee et al. (2016), explored the discovery of mispronunciation errors by analyzing the acoustic similarities across individual learners' utterances, with a proposed nbest filtering method to resolve ambiguous error candidate hypotheses derived from acoustic similarity clustering.Furthermore, Mao et al. (2018) proposed k-means clustering on phoneme-based phonemic posterior-grams (PPGs) to expand the phoneme set in L2 speech.More recently, Sini et al.\n\n(2023) introduced a weighted DTW alignment as an alternative to the GOP algorithm for predicting probabilities and the sequence of target phonemes.Their proposed method achieves comparable results to the GOP scoring algorithm, likewise Anand et al. (2023) explored alignment distance between wav2vec 2.0 representations of teacher and learner speech using DTW, to distinguish between intelligible and unintelligible speech.", "filtered_refids": [["b74", "b72", "b92"], ["b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1465, "num_references": 4}
{"corpusid_sectionid": "264426545-s14", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "Data Augmentation", "section": "Two major challenges in this field are L2 data scarcity and the imbalanced distribution of negative classes (mispronunciation).To address these challenges, researchers have opted for data augmentation techniques that are proven to be quite effective in pronunciation assessment.Such methods employed strategies like altering the canonical text by introducing mismatched phoneme pairs while preserving the original word-level speech (Fu et al., 2021).Additionally, a mixup technique is utilized in the feature space, leveraging phone-level GOP pooling to construct word-level training data (Fu et al., 2022).Furthermore, the error distance of the clustered SSL model embeddings are employed to substitute the phoneme sound with a similar sound (Zhang et al., 2022b).These latter approaches depend on the reuse of existing information rather than generating novel instances of mispronunciations.In (Fernandez et al., 2017), voice transformations in pitch, vocal-tract, vocal-source characteristics to generate new samples.Furthermore, L2-GEN can synthesize realistic L2 phoneme sequences by building a novel Seq2Seq phoneme paraphrasing model (Zhang et al., 2022a).Korzekwa et al. (2020b) proposed an augmentation technique by generating incorrectly stressed words using Neural TTS.Furthermore, Korzekwa et al. (2022) provided an overview of mispronunciation error generation using three methods, phoneme-2phoneme P2P relies on perturbing phonetic transcription for the corresponding speech audio, text-2-speech create speech signals that match the synthetic mispronunciations, and speech-2-speech S2S to simulate a different aspect of prosodic nature of speech.Recently, SpeechBlender (EL Kheir et al., 2023b) framework is introduced as a fine-grained data augmentation pipeline that linearly interpolates raw good speech pronunciations to generate mispronunciations at the phoneme level.", "filtered_refids": [["b10", "b44", "b41", null, "b69", "b159"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1887, "num_references": 6}
{"corpusid_sectionid": "264426545-s15", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "Evaluation Metrics", "section": "Phoneme Error Rate (PER): is a common metric used in the MD evaluation, measuring the accuracy of the predicted phoneme with the humanannotated sequence.However, PER might not provide a comprehensive assessment of model performance when mispronunciations are infrequent which is the case for MD datasets.Hierarchical Evaluation Structure: The hierarchical evaluation structure developed in (Qian et al., 2010), has also been widely adopted in (Wang and Lee, 2015;Li et al., 2016a;EL Kheir et al., 2023a) among others.The hierarchical mispronunciation detection depends on detecting the misalignment over: what is said (annotated verbatim sequence); what is predicted (model output) along with what should have been said (text-dependent reference sequence).Based on the aforementioned sequences, the false rejection rate, false acceptance rate, and diagnostic error rate are calculated, using:\n\n\u2022 True acceptance (TA): the number of phones annotated and recognized as correct pronunciations.\n\n\u2022 True rejection (TR): the number of phones both annotated and correctly predicted as mispronunciations.The labels are further utilized to measure the diagnostic errors and correct diagnosis based on the prediction output and textdependent canonical pronunciation.\n\n\u2022 False rejection (FR): the number of phones wrongly predicted as mispronunciations.\n\n\u2022 False acceptance (FA): the number of phones misclassified as correct pronunciations.\n\nAs a result, we can calculate the false rejection rate (FRR) that indicates the number of phones recognized as mispronunciations when the actual pronunciations are correct, false acceptance rate (FAR) that indicates phones misclassified as correct but are actually mispronounced, and diagnostic error rate (DER) using the following equations:\n\nPrecision, Recall, and F-measure are also widely used as the performance measures for mispronunciation detection.These metrics are defined as follows:\n\nPearson Correlation Coefficient: PCC is widely used to measure the relation of the predicted score of fluency, stress, and prosody with other suprasegmental and pronunciation constructs with subjective human evaluation for pronunciation assessment.The human scores are typically averaged across all annotators to provide a comprehensive score.", "filtered_refids": [["b111", null, "b140", "b78"], [], [], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2271, "num_references": 4}
{"corpusid_sectionid": "264426545-s20", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "A", "section": "A.8 L2-ARCTIC (Zhao et al., 2018a) The L2-ARCTIC 2 corpus is a specialized speech corpus designed for research in voice conversion, accent conversion, and mispronunciation detection in non-native English.It encompasses a substantial collection of 26867 utterances from 24 non-native speakers (12 males and 12 females) whose L1 languages include Hindi, Korean, Mandarin, Spanish, Arabic, and Vietnamese.The recordings were sourced from a total of 4 speakers per L1 language, consisting of 2 males and 2 females ensuring a balanced distribution in terms of gender and native 2 version 5 released in 2020 avalaible: https://psi.engr.tamu.edu/l2-arctic-corpuslanguages (L1s).Yet, only 150 utterances is manually per speaker to identify three types of segmental mispronunciation errors: substitutions, deletions, and insertions resulting in 3.66 hours.\n\nA.9 VoisTUTOR corpus (Yarra et al., 2019) VoisTUTOR is a pronunciation assessment corpus of Indian second language (L2) learners learning English.The corpus consists of audio recordings of 16 Indian L2 learners reading a set of 1676 sentences.The recordings are accompanied by phonetic transcriptions, human ratings of pronunciation accuracy on a scale of 0 to 10 for each utterance, and binary decisions for seven factors that affect pronunciation quality: intelligibility, phoneme quality, phoneme mispronunciation, syllable stress quality, intonation quality, correctness of pauses, and mother tongue influence.\n\nA.10 SELL-CORPUS (Chen et al., 2019) SELL-CORPUS is a multiple accented speech corpus for L2 English learning in China.The corpus consists of audio recordings of 389 volunteer speakers, including 186 males and 203 females.The speakers are from seven major regional dialects of China, including Mandarin, Cantonese, Wu, Min, Hakka, and Southwestern Mandarin.The corpus contains 31.6 hours of speech recordings.Each recording in the corpus contains a word-level orthographic transcription manually inspected and cleaned by inserting, substituting, or deleting mismatching characters.", "filtered_refids": [["b160"], ["b148"], ["b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2046, "num_references": 3}
{"corpusid_sectionid": "264426545-s21", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "A.11 English Pronunciation by Argentinians", "section": "Database (EpaDB) (Vidal et al., 2019a) EpaDB consists of English phrases recorded by native Spanish speakers with varying levels of English proficiency.The recordings are annotated with ratings indicating the quality of pronunciation at the phrase level.Additionally, detailed phonetic alignments and transcriptions are provided, indicating which phones were actually pronounced by the speakers.\n\nA.12 Speechocean762 (Zhang et al., 2021b) Speechocean762 is an extensive dataset specifically designed for pronunciation assessment.It comprises a total of 5,000 English utterances obtained from 250 non-native speakers.Each utterance in the dataset is associated with five aspect scores at the utterance level, namely accuracy, fluency, completeness, prosody, and a total score ranging from 0 to 10.Additionally, for each word within the ut-terance, three aspect scores are provided, including accuracy, stress, and a total score ranging from 0 to 10. Furthermore, an accuracy score is assigned to each individual phoneme, ranging from 0 to 2. To ensure reliability, each of these scores is annotated by five expert evaluators.\n\nA.13 LATIC (ZHANG, 2021) LATIC primarily targets non-native learners of Mandarin Chinese.The dataset comprises four hours of recordings involving specifically selected non-native Chinese speakers.The participants' L1's vary, including Russian, Korean, French, and Arabic.Following each audio file, annotators transcribed the \"closest\" transcript and provided modern Mandarin annotations after careful listening.\n\nA.14 Arabic-CAPT (Algabri et al., 2022) Arabic-CAPT is an Arabic mispronunciation detection corpus consisting of 62 non-native Arabic speakers from 20 different nationalities, totaling 2.36 hours of speech data.The Arabic non-native speech is annotated following the guidelines in (Zhao et al., 2018a).\n\nA.15 AraVoiceL2 (EL Kheir et al., 2023b) AraVoiceL2 is an Arabic mispronunciation detection corpus comprised of 5.5 hours of data recorded by 11 non-native Arabic speakers.Each speaker recorded a fixed list of 642 words and short sentences, making for a total of 7, 062 recordings.The corpus is annotated at character level including diacritics following (Zhang et al., 2021b) guidelines.", "filtered_refids": [["b135"], ["b155"], [], ["b160", null], ["b155", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2231, "num_references": 6}
{"corpusid_sectionid": "264426545-s24", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "B.1 Segmental Annotation", "section": "Segmental human annotation can be approached from two perspectives.The first and the commonly utilized approach in most available MDD corpora involves linguistics experts transcribing the actual sequence of phonemes spoken by the learner (Bonaventura et al., 2000;Zhao et al., 2018b;Vidal et al., 2019b).The resulted transcription is commonly referred as hypothesis annotation.Additionally, extra tasks can be incorporated, such as providing time boundaries for each pronounced phoneme, to further enhance the annotation process.This approaches may have limitations in capturing non-clear speech instances, such as heavily accented pronunciations that may not be easily detected by human annotators.This leads to the second approach, which incorporates scoringbased methods in addition to hypothesis annotation (Zhang et al., 2021c).In this approach, a score is assigned to each phoneme: 0 represents deleted or mispronounced phonemes, 1 indicates heavily accented pronunciation, and 2 signifies good pronunciation.This scoring-based approach provides a more comprehensive assessment of pronunciation quality, particularly in cases where clear detection by human annotators may be challenging.", "filtered_refids": [["b161", "b7", "b156", "b136"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1193, "num_references": 4}
{"corpusid_sectionid": "264426545-s25", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "B.2 Supra-segmental Annotation", "section": "Limited research has been conducted regarding the annotation of supra-segmental features at the rhythm, stress, and intonation levels such as in (Arvaniti and Baltazani, 2000;Chen et al., 2016b;Cole et al., 2017).However, the ultimate objective of annotating these supra-segmental aspects is to ensure the fluency and intelligibility of L2 learners' speech.Hence the most commonly used annotated datasets at the prosodic level provide human-scored words, and sentences based on overall pronunciation quality and fluency.Multiple tiers of human scoring annotations can be applied in this context.This includes providing the accuracy of pronounced words to assess their intelligibility, assigning scores to evaluate the positioning of stress within individual words or within sentences, and evaluating sentence fluency by considering factors such as pauses, repetitions, and stammering in speech as adapted in (Zhang et al., 2021c", "filtered_refids": [[null, "b23", "b156", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 928, "num_references": 4}
{"corpusid_sectionid": "264451714-s3", "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models", "date": "2023-10-25", "section_title": "Non-optimized Inputs", "section": "Extracting factual knowledge from PLMs depends on providing them with short inputs that indirectly describe the sought-after information.These methods can take various forms (cloze prompts (Taylor, 1953), questions, or entities).Non-optimized inputs represent the simplest case, where the probing 2 For more details refer to Appendix A.1 inputs are not altered in any way.\n\nCloze prompts are widely used across several methods.Petroni et al. (2019) probe PLMs for factual knowledge by manually constructing clozestyle templates for several relations.Onoe et al. (2022) automatically construct cloze prompts from Wikipedia and Wikidata by masking out spans near entities of interest, in order to evaluate PLMs' knowledge about (unseen) entities.Abaho et al. (2022) construct cloze prompts from annotated PubMed abstracts to use PLMs as health outcome predictors.Chen et al. (2022) finetune PLMs using cloze prompts that consist of task descriptions alongside a few examples to elicit more facts.\n\nQuestions are the second input category.Several Question Answering datasets are used to finetune T5 models (Raffel et al., 2020), and evaluate the amount of knowledge implicitly present in their parameters in (Roberts et al., 2020).Multiple choice questions are used in (Hardalov et al., 2020) by providing PLMs with the questions followed by each option individually.The options are masked, and the final answer is selected based on the normalized log probabilities of the predicted tokens for each option.Kalo and Fichtel (2022) present a dataset based on Wikipedia, where inputs consist of several questions and answers, i.e., a few examples to implicitly indicate the task, and a similar question without an answer for evaluation.\n\nEntities are used in methods that infer relational information or generate descriptions based on these entities.Some methods depend on a simple classifier or cosine similarity between the subject and object representations to determine the presence or absence of a relation.For example, to probe for geographical knowledge, Li\u00e9tard et al. ( 2021) use fixed inputs that contain locations (e.g., countries or cities).These inputs are then used to extract representations for the respective locations from PLMs.Using these representations, the authors evaluate based on the ability of a simple classifier to solve certain tasks (e.g., predicting if two countries share border).Dufter et al. (2021) evaluate the amount of knowledge present in static word embeddings by matching a subject entity (the query) to an object entity from a pre-defined set of possible objects based on the cosine similarity between the representations of the subject and object entities.Shi et al. (2021) train generative PLMs to generate entities' descriptions while providing only the en-tities as inputs, and compare them to ground truth descriptions.", "filtered_refids": [["b99"], ["b11", "b77", "b82", "b0"], ["b30", "b86", "b89", "b47"], ["b22", "b95"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2859, "num_references": 11}
{"corpusid_sectionid": "264451714-s4", "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models", "date": "2023-10-25", "section_title": "Optimized Inputs", "section": "Probing inputs contribute substantially to the probing procedure.PLMs are sensitive to the inputs (Petroni et al., 2019;Jiang et al., 2020b;Elazar et al., 2021), and even syntactical variations or distractors, that do not alter the meaning, cause the PLM's predictions to change (Heinzerling and Inui, 2021;Longpre et al., 2021;Pandia and Ettinger, 2021;Podkorytov et al., 2021;Li et al., 2022a).Therefore, depending on the probing inputs, the estimate on factual knowledge we obtain may vary significantly.Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs.\n\nDiversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.This dataset is then used for the final evaluation.\n\nDirect optimization methods aim to directly optimize existing prompts.This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary.Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task.Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradientbased search to sequentially replace the trigger tokens with concrete tokens.These tokens are chosen to increase the probability of predicting the correct object.OPTIPROMPT (Zhong et al., 2021) is sim-ilar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space.In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs.Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts.Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings.These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity.PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs.Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts.(Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts.\n\nGeneration with PLM methods re-write prompts with the help of a secondary PLM.Haviv et al. (2021) re-write manual prompts using another version of the probed model.The re-writing model is trained to produce prompts that help extract more knowledge from the probed one, which is kept unchanged.Zhang et al. (2022) leverage a generative PLM to produce optimized prompts.", "filtered_refids": [["b82", "b79", "b83", "b59", "b23", "b42", "b33", "b65"], ["b42", "b6"], ["b92", "b85", "b61", "b117", "b75", "b53", "b96", "b36"], ["b114", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 4277, "num_references": 20}
{"corpusid_sectionid": "264451714-s6", "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models", "date": "2023-10-25", "section_title": "Vanilla PLMs", "section": "Methods in this category do not induce any changes to the probed PLMs, and depend on pre-training ob-jectives to probe PLMs for factual knowledge.Using the pre-trained parameters is the most straightforward approach and is claimed to preserve the facts learned during pre-training (Elazar et al., 2021;Newman et al., 2022).\n\nMost methods leverage the language modeling objectives from pre-training to probe for factual knowledge (Petroni et al., 2019;Jiang et al., 2020b;Shin et al., 2020;Haviv et al., 2021;Kumar and Talukdar, 2021;Zhong et al., 2021;Kalo and Fichtel, 2022;Newman et al., 2022;Onoe et al., 2022;Saeed and Papotti, 2022).Other methods rely on representations that come from the model's body, discarding task-specific parameters altogether (e.g., the Masked Language Modeling head in BERT-like models) (Li\u00e9tard et al., 2021) or use representations of the subject and object entities in the case of static word embeddings (Dufter et al., 2021).", "filtered_refids": [["b75", "b23"], ["b92", "b47", "b82", "b32", "b42", "b77", "b22", "b62", "b117", "b75", "b53", "b96"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 959, "num_references": 14}
{"corpusid_sectionid": "264451714-s7", "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models", "date": "2023-10-25", "section_title": "Adapted PLMs", "section": "Some works adapt the PLMs under evaluation to enable evaluation tasks, that do not correspond to any pre-training objective.The adaptation, however, is also coupled with risks such as train-test overlap (Lewis et al., 2021;Wang et al., 2021a).Supervised adaptation.Most methods finetune the probed PLMs in a supervised manner to adapt them to the probing task.Roberts et al. (2020) finetune T5 models for closed-book question answering, where models have only questions as inputs, while leaving out any context or external knowledge sources that might contain the answer.Similarly, Wang et al. (2021a) finetune BART to output a related passage, and then the answer.Bouraoui et al. (2020) finetune BERT to classify prompts based on whether the relation between the subject and object entities truly holds or not.Fichtel et al. (2021) finetune a BERT model with its masked language modeling head to predict the masked tokens in the provided prompts.Abaho et al. (2022) propose an additional position-attention layer on top of transformer models, where the position of the masked token is kept constant, and the remaining tokens are given positions relative to the masked token.This approach is considered to put more focus on the masked tokens and its interaction with the remaining tokens in the prompt.Chen et al. (2022) leverage a task description that depends on the relation between the subject and object entity, alongside a few labeled examples to train the probed PLMs.At inference time, the PLMs are kept frozen and are provided with unseen task descriptions and labeled examples to adapt to the task.Elazar et al. (2021) further train BERT with a consistency loss to increase its robustness to paraphrases that describe the same relation.Shi et al. (2021) finetune generative PLMs to generate entity descriptions depending only on their knoweldge from pre-training.Qin and Eisner (2021) do not directly change any parameters in PLMs, but rather introduce additional trainable parameters in each layer that change the hidden representations of the prompts to help make them more suitable for knowledge extraction.\n\nSelf-supervised adaptation.Adaptations in a self-supervised manner can introduce changes to the model without explicitly finetuning the model to the probing task.For example, Meng et al. (2022b) propose to re-wire the probed PLM in a self-supervised manner.Their method depends on using data from the pre-training phase, splitting each sentence into a head part and a tail part, and using a contrastive learning objective to push the representations of the matching head and tail pairs (positives) closer to one another, and that of the non-matching pairs (negatives) to be further apart.The evaluation is based on the similarity between the representations of the prompt and a predefined set of entities that represent potential answers.", "filtered_refids": [["b11", "b6", "b58", "b103", "b85", "b89", "b25", "b23", "b95", "b0"], ["b71"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2860, "num_references": 11}
{"corpusid_sectionid": "264451714-s8", "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models", "date": "2023-10-25", "section_title": "Outputs", "section": "Methods focusing on the outputs of PLMs address restricting the output space of PLMs, debiasing their outputs, and handling multi-token entities.\n\nTyped querying.Kassner et al. (2021) propose to restrict the space of possible values for replacing the masked token (object) from the whole vocabulary to a specific set of tokens whose type matches the type of the ground truth object.For example, if the PLM is queried with the prompt: \"The smallest country in the world is [MASK]\", only entities of type country are considered to replace the [MASK] token.This method has two advantages: it reduces the number of objects under consideration and allows for a better comparison across PLMs with different vocabularies (Kassner et al., 2021).\n\nDebiasing.Zhao et al. (2021) identify biases in the predictions of PLMs towards common and recent tokens, and propose a method that adapts the output probabilities by first estimating these biases using neutral examples and then correcting them.This debiasing method is shown to reduce the variance across prompts and has a positive effect on fact retrieval.Malkin et al. (2022) propose a method to increase the effect of distant tokens on the predictions of PLMs.The method depends on combining two output distributions over the vocabulary.One distribution is based on the full-length input, whereas the other is based on a shortened version of the same input.Wang et al. (2023) identify the problem of object bias in optimized prompts and propose to make all potential objects equally probable when no subject is provided, and increasing the probability of the correct object, when the subject is available.Yoshikawa and Okazaki (2023) output predictions only above a sufficient confidence threshold.This results in a less biased evaluation, and reflects the ability of PLMs in excluding uncertain predictions.To address the problems of multiple valid answers and frequency bias, i.e., the co-occurence of some subject and object entities despite not being in a factual relation to one another, Dong et al. (2022) use two templates, one contains the correct relation while the other contains an erroneous relation between the two entities, and compare the probability for the correct object under both relations.\n\nMulti-token entities.To handle multi-token entities, Jiang et al. (2020a) propose using a predefined number of masked tokens and filling these using different strategies: 1) independent from each other, 2) sequentially (left-to-right for English), 3) starting with the most confident predictions.(Kalinsky et al., 2023) leverage the masked token representation to generate multiple tokens using a small generative model.", "filtered_refids": [[], ["b50"], ["b115", "b105", "b66", "b21", "b110"], ["b41", "b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2675, "num_references": 8}
{"corpusid_sectionid": "264451714-s9", "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models", "date": "2023-10-25", "section_title": "Datasets for Factual Probing", "section": "We found a variety of datasets (44 in our corpus) that have been proposed or used for probing factual knowledge in PLMs: 18 datasets for probing general knowledge, 8 for domain-specific knowledge and 18 datasets that target other aspects, e.g, consistency of PLMs (cf.Table 2).\n\nDatasets for general knowledge probing are used to quantify generic factual knowledge in PLMs with the most prominent being LAMA (Petroni et al., 2019).WIKI-UNI (Cao et al., 2021) is similar to LAMA, but with a uniform distribution of object entities.LAMA-UHN (Poerner et al., 2020) is a subset of LAMA without easy-to-guess examples.DLAMA (Keleg and Magdy, 2023) targets culturally diverse facts.While 16 datasets are solely English, there are three multilingual datasets (mLAMA (Kassner et al., 2021), X-FACTR (Jiang et al., 2020a) and DLAMA (Keleg and Magdy, 2023)).In-dicGLUE (Kakwani et al., 2020) contains 11 Indic languages.Most datasets consist of cloze prompts, while QA datasets (WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019)), PopQA and Enti-tyQuestions (Mallen et al., 2023) are also used to quantify factual knowledge (Roberts et al., 2020).Wang et al. (2021a) adapt SQuAD (Rajpurkar et al., 2018) for closed-book question answering.\n\n6 out of 8 datasets used for probing domainspecific knowledge target the biomedical domain (e.g., MedQA (Jin et al., 2021), Bio-LAMA (Sung et al., 2021) and MedLAMA (Meng et al., 2022b)).The multilingual dataset EX-AMS (Hardalov et al., 2020) focuses on scientific QA, whereas LEFT (Ciosici et al., 2021) contains questions from humanities and social sciences.\n\nThe community has constructed further datasets to investigate other aspects of using PLMs as knowledge bases.PARAREL (Elazar et al., 2021) and its multilingual counterpart mPARA-REL (Fierro and S\u00f8gaard, 2022)", "filtered_refids": [[], ["b52", "b54", "b10", "b67", "b44", "b103", "b89", "b87", "b41", "b82", "b50", "b45", "b84", "b5"], ["b43", "b98", "b14", "b71", "b30"], ["b23", "b26"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1848, "num_references": 21}
{"corpusid_sectionid": "264451714-s11", "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models", "date": "2023-10-25", "section_title": "Factors Affecting Knowledge Retention", "section": "PLMs are diverse with respect to their architectures, pre-training objectives and their pre-training data.A compelling question is: how do all these factors affect knowledge retention in PLMs?\n\nLarge language models are known to perform generally better and hold more knowledge (Brown et al., 2020;Roberts et al., 2020).However, the model's architecture and pre-training objectives are more decisive for knowledge retention than its size (Li et al., 2022a).For example, pre-training with the Salient Span Masking objective (Guu et al., 2020) helps PLMs to absorb more facts (Roberts et al., 2020;Cole et al., 2023).Similarly, Xiong et al. (2020) demonstrate that training the model to predict if the original entities in the text have been replaced with other entities is beneficial for fact retrieval.More generally, Ye et al. (2021) conclude that a masking strategy matching the downstream task, positively affects the performance on that task.\n\nA larger pre-training corpus with an encoderonly model (Liu et al., 2020) leads to higher knowledge retention (Zhang et al., 2021), but with an encoder-decoder model (Lewis et al., 2020), a larger corpus negatively affects knowledge retention Wang et al. (2021a).Recency (Chiang et al., 2020) and frequency (Kandpal et al., 2023), i.e., when and how often the data is observed at training, are also essential for knowledge retention.\n\nLarger models and more pre-training data can improve knowledge retention if combined with the right choices for architecture and pre-training objective(s).However, scaling might not be sufficient (Kandpal et al., 2023).Even though many works propose new architectures and pre-training objectives to increase factual knowledge retention in PLMs and their robustness to prompts (F\u00e9vry et al., 2020;Hosseini et al., 2021;Sadeq et al., 2022;Whitehouse et al., 2022;Min et al., 2023;Zhong et al., 2023), this is a promising future work direction, as there is more room for improvement.", "filtered_refids": [[], ["b15", "b107", "b89", "b28", "b59", "b7", "b109"], ["b12", "b57", "b103", "b49", null, "b64"], ["b91", "b35", "b49", "b116", "b24", "b73", "b106"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1963, "num_references": 20}
{"corpusid_sectionid": "264451714-s12", "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models", "date": "2023-10-25", "section_title": "Should Prompts be Optimized?", "section": "Prompt Optimizing leads to better probing performance (Jiang et al., 2020b;Shin et al., 2020;Kumar and Talukdar, 2021;Newman et al., 2022;Zhang et al., 2022) .However, it remains unclear whether this improvement is due to optimized prompts leaking new knowledge into the probed PLMs.\n\nOptimized prompts can be mere paraphrases of manually created prompts (Bouraoui et al., 2020;Jiang et al., 2020b).These paraphrases might be better fact retrievers because of their similarity to the pre-training corpus (Cao et al., 2022).Other prompt optimization methods find better prompts in discrete or continuous spaces (Shin et al., 2020;Zhong et al., 2021).These prompts are largely uninterpretable, and can even retrieve facts from randomly initialized PLMs (Zhong et al., 2021;Ishibashi et al., 2023).\n\nPerformance improvements for optimized prompts can be attributed either to prompts becoming more similar to the pre-training data or overfitting the facts distribution.Evaluation should take the pre-training corpora and the facts distribution in the probing dataset into account (Cao et al., 2021(Cao et al., , 2022)).Future work should consider adapting prompt optimization methods to produce more interpretable prompts.This would keep the performance gains, and increase the trustworthiness of optimized prompts.", "filtered_refids": [["b42", "b114", "b75", "b53", "b96"], ["b6", "b8", "b117", "b42", "b96", "b37"], ["b10", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1311, "num_references": 13}
{"corpusid_sectionid": "264451714-s13", "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models", "date": "2023-10-25", "section_title": "Obstacles to Adopting PLMs as KBs", "section": "Consistency.A challenge to relying on PLMs as knowledge bases is their sensitivity to the input queries (Fierro and S\u00f8gaard, 2022).PLMs rely on shallow surface features and lexical correlations (Kassner and Sch\u00fctze, 2020;Misra et al., 2020;Poerner et al., 2020;Rogers et al., 2020;Li et al., 2022b), which explains their high sensitivity to the way queries are formulated.Current solutions (Elazar et al., 2021;Newman et al., 2022) train PLMs to be robust to variations in inputs, but further improvements are needed to make PLMs reliable knowledge bases.PLMs are known to be highly sensitive to prompts, especially in languages other than English (Fierro and S\u00f8gaard, 2022), where less resources are available.Making PLMs more robust to prompts in non-English languages is a promising future work direction.\n\nInterpretability.Identifying where facts are stored and how they are retrieved is essential to adopt PLMs as trustworthy knowledge sources.Several approaches locate knowledge in PLMs (Wallat et al., 2020;Podkorytov et al., 2021;Alkhaldi et al., 2022;Dai et al., 2022;Meng et al., 2022a), with different conclusions depending on the architecture (e.g., knowledge is located in the middle layers of GPT-like models (Meng et al., 2022a), or in the upper layers in BERT-like models (Dai et al., 2022)).Another line of work focuses on the data aspect, showing the dependence of PLMs on word co-occurrences and positionally close words (Li et al., 2022b), or tracing back predictions to training data (Akyurek et al., 2022;Park et al., 2023).Knowing how PLMs retrieve facts remains challenging, but necessary to make PLMs transparent fact retrievers.The introduction of a fact tracing benchmark (Akyurek et al., 2022) opens the door for works in this direction.\n\nUpdating Knowledge.PLMs come with a fixed set of pre-trained parameters that encode knowledge about the world.As time passes, this knowledge becomes partially outdated.Hence, editing existing knowledge in PLMs and augmenting them with new knowledge is crucial for their use as knowledge bases (Zini and Awad, 2022).\n\nOne line of research locates the modules responsible for factual predictions and modifies these to update the corresponding facts (Dai et al., 2022;De Cao et al., 2021;Meng et al., 2022a).Other lines of research keep the original PLM unchanged, but augment it with additional parameters to induce the desired changes (Wang et al., 2021b;Lee et al., 2022), or encode facts with time stamps in PLMs to make them \"time-aware\" (Dhingra et al., 2022).\n\nWhen updating facts in PLMs, it is crucial that only the targeted facts are affected and that these facts are retrievable using different paraphrases (De Cao et al., 2021;Hase et al., 2023).However, current methods for facts editing (Meng et al., 2022a(Meng et al., , 2023) ) still do not fulfill these requirements (Hoelscher-Obermaier et al., 2023).Methods that introduce additional parameters should be made more scalable (Jang et al., 2022b).", "filtered_refids": [["b90", "b75", "b26", "b60", "b23", "b84", "b74", "b51"], ["b60", "b1", "b83", "b102", "b80", "b69", "b16", "b3"], ["b118"], ["b18", "b104", "b55", "b20", "b69", "b16"], ["b34", "b39", "b18", "b31", "b69", "b70"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2978, "num_references": 29}
{"corpusid_sectionid": "249204437-s1", "title": "How well do real-time machine translation apps perform in practice? Insights from a literature review", "date": 2022, "section_title": "MT quality assessment", "section": "The quality of MT output has been a hotly debated topic for decades, and a wide variety of methods for its assessment have been proposed (cf. Castilho et al., 2018). When classifying these methods, authors commonly distinguish between automated metrics and human metrics (e.g., Rivera-Trigueros, 2021;Chatzikoumi, 2020). Automated metrics include Word Error Rates (WERs), precision, recall, and BLEU scores, all of which are calculated on the basis of a comparison between MT output and a reference translation created by a professional human translator.\n\nHuman metrics are further subdivided by Chatzikoumi (2020) into metrics in which human experts express a direct judgement concerning the translation quality and metrics in which no direct judgement is expressed. When experts are asked to indicate the adequacy or fluency of a machine translated text on a 5-point scale, for example, they make an explicit quality judgement. When, on the other hand, they classify the translation errors occurring in the MT output, they provide useful information for improving the application without explicitly judging the quality of the output. Measuring the post-editing effort required to reach an acceptable quality level for the target text (e.g. Lacruz et al., 2014) also provides an indirect indication of MT quality.\n\nThere are several reasons why most of the metrics discussed above can be considered less suitable for assessing real-time MT that is used to support synchronous dialogues. First of all, postediting does not occur in such situations, so postediting effort cannot be used as a quality indicator. In the absence of a human-generated reference translation, automated metrics can also not be calculated. Technically speaking, human experts could judge the quality of the output after the dialogue has taken place, but they would be at a disadvantage due to the limited length and disfluent nature of the source texts, particularly when speech input is used (Przybocki et al., 2011).\n\nMoreover, it is important to acknowledge that MT quality assessment can have different purposes. Many of the metrics above were primarily developed to identify areas of improvement for MT applications that are 'under construction' (Dorr et al., 2011). For professionals contemplating the use of real-time MT in their daily professional routines, however, improving the application is not the main priority. They want to know whether using MT will enhance the quality of their interactions with patients, students or business partners who speak a different language. In some cases, they might even wonder whether the use of MT is ethically responsible given the prevalence of errors in MT output and the potentially damaging consequences of such errors in certain contexts (Vieira et al., 2020).\n\nTaken together, these considerations suggest that the evaluation of real-time MT might best be approached from the perspective of 'fitness for purpose', which is achieved when the quality of a translation is 'good enough' for the end user to understand the information content and pragmatic intent of a translated message Directorate General for Translation, 2016). Although this concept has featured prominently in both practical and academic discourse about translation quality for quite some time (Jim\u00e9nez-Crespo, 2018), it is not yet standard practice to ask end users to assess the quality of (post-edited) MT output (cf. Van Egdom & Pluymaekers, 2019).\n\nThis raises the question to what extent existing studies into the performance of real-time MT apps are guided by the concept of fitness for purpose, and how fitness for purpose is operationalized in evaluation methods used in these studies. For the current paper, we are specifically interested in the answers to the following questions:\n\nRQ1: To what extent are real-time MT applications tested in authentic professional situations?\n\nRQ2: Which quality indicators are most commonly used and how are they operationalized? RQ3: Who judges the performance of real-time MT apps? RQ4: Which overall picture concerning the performance of real-time MT apps emerges from the research conducted so far?\n\nWe hope to find these answers by conducting a systematic literature review of prior studies (N = 34) which report an evaluation of a real-time MT app that was or could be used to facilitate a synchronous dialogue between interlocutors who did not speak the same language. More information about our methodology is provided in the next chapter.", "filtered_refids": [["b20", "b2", "b3"], ["b13", "b3"], ["b19"], [null, "b6"], [null, "b27"], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 4490, "num_references": 10}
{"corpusid_sectionid": "249204437-s3", "title": "How well do real-time machine translation apps perform in practice? Insights from a literature review", "date": 2022, "section_title": "Sampling", "section": "In compiling the sample, we followed a multi-step approach (see Figure 1). First, we conducted an initial search in four scientific databases (EBSCOhost, PubMed, Web of Science and Google Scholar), which were selected for reasons of practicality (i.e., accessibility via the university library) as well as quality (cf. Creswell, 2014;Gusenbauer & Haddaway, 2020). In each database, we used the following Boolean combination of search words:\n\n(\"mobile translat*\" OR \"real-time translat*\" OR \"automatic translat*\") OR (\"translat* tool\" OR \"translat* app\") AND (\"quality\" OR \"evaluation\" OR \"usability\") NOT \"knowledge translation\"\n\nDepending on the search functionalities of the database, this query was applied to the abstract, the title and the abstract, or the entire text. The relevance of the articles that came up in the search results was assessed in two steps. On the basis of the abstracts, 23 articles were marked as potentially relevant. After reading the complete articles, we decided that 10 of them indeed corresponded to the inclusion criteria outlined above. In the next step, we expanded the sample by (1) manually adding 4 articles that we had found earlier and (2) investigating studies that were either included in the reference list of one of the articles in the initial set or that referred to one of the articles in the initial set. By doing so, we identified 28 potential additions to the sample, 18 of which met the screening criteria. For the newly added articles (4+18), we repeated the reference check described above, which led to the identification of 2 more articles. After this, saturation was achieved, resulting in a final sample of 34 articles (see Appendix A). More information about the characteristics of these articles (year of publication, the number and types of applications tested, language combinations etc.) will be provided in section 4.1 below.", "filtered_refids": [["b7", "b4"], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1889, "num_references": 3}
{"corpusid_sectionid": "256461385-s2", "title": "Narrative Why-Question Answering: A Review of Challenges and Datasets", "date": 2022, "section_title": "Causality", "section": "Causality is a semantic relationship between events showing that an event occurs or holds due to another event (Mostafazadeh et al., 2016b). Mostafazadeh et al. (2016b) distinguish four types of lexical causality relations: cause, enable, prevent, and cause-to-end based on the works by Wolff and Song (2003), Wolff (2007), and Khemlani et al. (2014). Moreover, causality has temporal implications such that if an event A causes/enables/prevents an event B, then A should start before B, or if an event A causes an event B to end, then B should start before A. Causality relations can hold one of the three temporal implications: before, overlaps, and during (Mostafazadeh et al., 2016b). Thus, while answering a whyquestion, the temporal relation between the events should also be taken into account in addition to the causality relation.\n\nA causal relation is constructed from two components: cause and effect. Based on how the cause and the effect are conveyed in a text, causation can be distinguished into the following categories: explicit vs implicit, marked vs unmarked, and ambiguous vs unambiguous.\n\nExplicit vs Implicit. Causation is explicit if both the cause and the effect are present in the text. Causation is implicit if either the cause or the effect of both are missing from the text (Blanco et al., 2008). For instance, \"She was accepted to a top university after receiving a high score in the state examination\" is explicit, while \"I did not attend the mandatory final exam.\" is implicit because the effect of \"failing the course\" is not explicitly stated.\n\nMarked vs Unmarked. Causation is marked if the text contains the causal signal words that indicate the causal relation (Blanco et al., 2008). For example, \"I was late because of traffic\" is marked, but \"Do not buy any bread. We have already got two at home\" is unmarked.\n\nAmbiguous vs Unambiguous. If the causal relation is presented in the text with causal keywords (e.g., cause, effect, consequence) or with causal signals (e.g., because of, due to, as a result of ), it is considered unambiguous (Girju, 2003). On the other hand, if a causal relation is constructed in the form of an expression containing affect verbs (e.g., affect, change, influence) or link verbs (e.g., link, lead, depend), it is considered ambiguous. Furthermore, if a marked signal always refers to causation (e.g., because), it is unambiguous, while if a marked word occasionally signals causation (e.g., since), it is ambiguous (Blanco et al., 2008).", "filtered_refids": [["b43", "b44", "b16", "b25"], [], ["b2"], ["b2"], ["b12", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2506, "num_references": 8}
{"corpusid_sectionid": "256461385-s5", "title": "Narrative Why-Question Answering: A Review of Challenges and Datasets", "date": 2022, "section_title": "Answer ambiguity", "section": "Answer ambiguity occurs because most questions can have multiple answers belonging to different answer types and because often the desired type is not expressed in the question. Several partially overlapping taxonomies of reasons, which is the cause component of a causal relation, have been proposed (Verberne et al., 2006;Dunietz et al., 2017;Tan et al., 2022). Verberne et al. (2006) distinguish four types of reasons based on Quirk et al. (1985):\n\n\u2022 Cause -a temporal and causal relation without the involvement of the human intention: an event mechanistically leads to another event;\n\n\u2022 Motivation -a temporal and causal relation with an involvement of the human intention: a goal or a motivation of an agent leads to their action;\n\n\u2022 Circumstance -a temporal and causal relation based on conditionality: one event is a condition for another event to occur;\n\n\u2022 Generic purpose -a causal relation stemming from physical functions of the objects.\n\nSimilarly, Dunietz et al. (2017) defines three types of causalities while annotating causal relations: (1) Consequence: similar to the Cause type above, (2) Motivation and (3) Purpose: similar to the Motivation type above. Tan et al. (2022) defines four senses for causality based on Webber et al. (2019) for annotating causal relations: (1) Cause: similar to the Cause type above (2) Purpose: similar to the Motivation type above, (3) Condition and (4) Negative-Condition, which can fit into the Circumstance type above. Although the types of reasons introduced by Verberne et al. (2006) are broader than the taxonomies of Dunietz et al. (2017) and Tan et al. (2022), this list is not complete, as Verberne et al. (2006) demonstrated that not all why-questions can be classified into these categories.\n\nContext: \"He opened the box to take a slice of pizza.\" Question: \"Why did he open the box?\" Answers:\n\n(1) The pizza was in the box.\n\n(2) The box was closed.\n\n(3) He was hungry.\n\n(4) He wanted to eat pizza. (5) He wanted to take a slice of pizza.  (1) and (2) refer to causal reasons, answers (3), (4) and (5) refer to motivational reasons.\n\nValid answers to a why-question about an event or a state can include at least one of the cause, motivation, circumstance, or generic purpose of an event or state according to the above taxonomy. Since a why-question can often be answered with answers falling into several type categories, the necessity to choose the correct answer type creates ambiguity since the desired type is typically not explicitly stated in the question. Furthermore, a whyquestion can be answered with several causes in the causal chain (Verberne et al., 2006), and in that case all these answers can be considered as correct. For instance, consider the example shown in Table 1. For this example question, several potential causes can be the basis for the answer. Consequently, this why-question can be answered according to both mechanistically causal (answers 1, 2) and motivational (answers 3, 4, 5) reasons.", "filtered_refids": [[null, "b9", "b34", "b41"], [], [], [], [], ["b42", null, "b9", "b41"], [], [], [], [], [], ["b41"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2985, "num_references": 9}
{"corpusid_sectionid": "256461385-s6", "title": "Narrative Why-Question Answering: A Review of Challenges and Datasets", "date": 2022, "section_title": "Narratives", "section": "Narratives are texts in which events are causally or thematically linked and develop within a temporal framework (Brewer, 2017). Narratives are generally agent-oriented and their main scope is centered on characters, their actions, and motivations (Sang et al., 2022). In narrative QA, stories, fairytales, books, and (movie) scripts are commonly utilized as narrative texts. Characteristics of narrative texts, such as causality of events and motivations of agents, make narratives a suitable context for asking why-questions. Additionally, fictional narratives can ensure the test of comprehension because they are self-contained, meaning that all elements needed to understand the narrative, such as events, characters, and settings, are present in the text and QA models need to comprehend the narrative in order to answer questions (Dunietz et al., 2020;Richardson et al., 2013;Ko\u010disk\u00fd et al., 2018). Implicitness is a key feature of narratives that makes it different from other types of texts. Length is another characteristic dimension of narratives which is also very important for QA systems. In the following subsections, we will review these characteristics in more detail.", "filtered_refids": [["b18", "b4", "b35", "b8", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1185, "num_references": 5}
{"corpusid_sectionid": "256461385-s7", "title": "Narrative Why-Question Answering: A Review of Challenges and Datasets", "date": 2022, "section_title": "Implicitness: \"Reading between the lines\"", "section": "People often think and communicate with each other in the form of a narrative (story) (Dunietz et al., 2020). They assume that other people with whom they interact share a common ground with them, so they do not have to mention or specify commonly known knowledge (Ostermann et al., 2018a). Similar to the implicitness characteristic of the natural narrative-style communication, narrative texts tend to exclude common knowledge, such as commonsense and script (typical sequences of events to accomplish common tasks) knowledge, and assume that the reader has the background knowledge required to infer relevant implicit information (Schank and Abelson, 1975). For instance, not all causes of events and reasons for actions of agents are explicitly stated in narratives. Thus, the ability to \"read between the lines\" is necessary for properly understanding narratives (Norvig, 1987).", "filtered_refids": [["b28", "b8", "b29", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 883, "num_references": 4}
{"corpusid_sectionid": "256461385-s8", "title": "Narrative Why-Question Answering: A Review of Challenges and Datasets", "date": 2022, "section_title": "Short vs long narratives", "section": "Narratives can be short or long based on the scope of the text stream and the number of events it contains.\n\nShort narratives cover a small number of events and briefly narrate the actions of fewer agents. The local structure of a longer narrative such as an individual scene can be also considered and used as a short narrative. In short narratives, the reader can make inferences by linking local narrative elements and creating a local narrative representation (Sang et al., 2022;Kintsch, 1988).\n\nLong narratives, on the other hand, have large textual content, cover many events, and focus on the actions and interactions of many agents. Long narratives require the readers to comprehend the underlying deep structure of the narrative and analyze the high-level abstractions. Answering questions in this setting requires understanding the global narrative structure, such as the whole story (Sang et al., 2022;Kintsch, 1988) and the integration of various information stated in different parts of the long narrative by connecting individual scenes (McNamara and Magliano, 2009).", "filtered_refids": [[], ["b17", "b36"], ["b17", "b22", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1081, "num_references": 5}
{"corpusid_sectionid": "256461385-s10", "title": "Narrative Why-Question Answering: A Review of Challenges and Datasets", "date": 2022, "section_title": "Datasets", "section": "We selected several multiple-choice, extractive, and free-form QA datasets that utilize narrative as their context. In order to identify why-questions in these datasets, we first extracted all questions including the word why. We then manually removed any nonwhy questions (e.g, \"what did the king's son do after he wondered why the girl was crying\") from the questions that do not start with why. The relevant statistics of all datasets are shown in Table 2.\n\nTellMeWhy (Lal et al., 2021) dataset presents free-form why-questions over events in short narratives. It is the only existing dataset created with the Narrative Why-Question Answering task in mind. The questions were created using templatebased transformations and the answers to questions were crowdsourced. Narratives were collected from ROCStories (Mostafazadeh et al., 2016a) and CATERS (Mostafazadeh et al., 2016b). The dataset has a total of 30,519 why-questions with three golden free-form answers for each question. According to data annotators, 28.82% of questions in the dataset cannot be answered explicitly based on the narrative (context).\n\nMCTest (Richardson et al., 2013) is a multiplechoice MRC dataset based on fictional stories. The dataset is created via crowdsourcing and it is designed for the level of understanding of 7-year-old children. The fictional and basic comprehension nature of the dataset decreases the need for additional world knowledge and makes it possible to find the answer only based on the text.\n\nMCScript (Ostermann et al., 2018a) is a multiple-choice MRC dataset based on stories about daily activities. It is created to evaluate machine comprehension using commonsense (script) knowledge (Ostermann et al., 2018b). Stories are collected by crowdsourcing new texts based on selected scenarios. Questions are crowdsourced based on scenarios independent of narratives and then matched with narratives randomly. Similar to MCTest, texts and questions are created according to the understanding level of a child. In general, 27.4% of questions require commonsense (script) knowledge to correctly infer the answer.  Table 2: Statistics of the narrative why-QA datasets. # of Why shows the number of why-questions in the datasets. % of Why refers to the proportion of why-questions in the datasets. The percentage of implicit questions is taken from the respective dataset papers, except for the NarrativeQA for which this number is due to the analysis done by Bauer et al. (2018) MCScript2.0 (Ostermann et al., 2019) is another multiple-choice MRC dataset focused on script knowledge. The stories were collected by reusing narratives from the MCScript, and crowdsourcing texts based on new scenarios. Questions were collected based on target sentences of stories rather than scenarios or complete stories. Similar to MC-Script and MCTest, the texts and questions are created according to the understanding level of a child. Correct and incorrect answers were crowdsourced by showing questions and hiding the target sentences in the story. In total, 50% of the questions require commonsense knowledge to be answered.\n\nCosmos QA (Huang et al., 2019) is a multiplechoice commonsense-based reading comprehension dataset. 93.8% of the questions in the dataset require contextual commonsense reasoning. Context paragraphs were collected from the spinn3r blog story corpus Burton et al. (2009) and a dataset by Gordon and Swanson (2009). Both questions and answers were crowdsourced. Questions are based on the causes and effects of events, facts about entities, and counterfactuals.\n\nNarrativeQA (Ko\u010disk\u00fd et al., 2018) is a narrative reading comprehension dataset based on books and movies. Books from the Project Gutenberg and movie scripts from the web are used as stories. Moreover, summaries for long narratives are obtained from Wikipedia. Questions and answers are crowdsourced based on summaries only. Since both original long stories and summaries exist for each question, this dataset can be used for two tasks: narrative QA based on long narratives (books and movie scripts) and short narratives (summaries). Manual analysis on the validation set by Bauer et al. (2018) showed that 42% of the questions need commonsense knowledge for inference.\n\nFairytaleQA (Xu et al., 2022) is a narrative com-prehension dataset designed for both question answering and question generation tasks. The narratives were collected from the Project Gutenberg by considering the reading difficulty up to the 10thgrade level. Small sections were extracted from fairytales as context paragraphs. Following the narrative comprehension frameworks by Paris and Paris (2003) and Alonzo et al. (2009), trained annotators created questions and answers for the contexts. The most common questions are about characters' behavior and causal relationships. 25.5% of the questions are implicit (free-form) and 74.5% of the questions are explicit (span-based). The amount of why-questions in the reviewed datasets is reported in Table 2. Among the multiple-choice QA datasets, CosmosQA has a higher number of why-questions compared to others. Among the free-form QA datasets, TellMe-Why dataset contains approximately 4.5 times more why-questions than the other two free-form QA datasets combined. Considering the proportion of why-questions in these datasets (also shown in Table 2), why-questions are well-represented in the CosmosQA and FairytaleQA datasets where they make up a sizeable part of the whole dataset, while in the MCTest, MCScript, MCScript2.0, and Narra-tiveQA datasets, why-questions cover only a small portion of the whole dataset.", "filtered_refids": [[], ["b24", "b25"], ["b35"], ["b30", "b1", "b29", "b31"], ["b15", "b13", "b5"], ["b1", "b18"], [null, "b33", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 49, "num_chars": 5620, "num_references": 15}
{"corpusid_sectionid": "256461385-s11", "title": "Narrative Why-Question Answering: A Review of Challenges and Datasets", "date": 2022, "section_title": "Evaluation measures", "section": "For multiple-choice QA datasets, accuracy is a commonly used metric to measure the performance of a model. For free-form QA datasets, both automatic and human evaluation measures are utilized to evaluate the capabilities of the QA model. Most commonly, ROUGE-L (Lin, 2004), Meteor (Denkowski and Lavie, 2011), BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020) and\n\nBertScore (Zhang et al., 2020) have been used to automatically evaluate the performance of the freeform QA models in narrative setting. Overall, F1 score of the ROUGE-L is the most commonly reported automatic evaluation measure.\n\nIn terms of human evaluation, Lal et al. (2021) proposed to assess the grammaticality and validity of the answers based on a 5-point Likert scale. The scale of the grammaticality ranges from strongly ungrammatical (1) to strongly grammatical (5), where a strongly grammatical answer must follow all the rules of the English grammar and a neutral score (3) is indicated when the meaning of the answer can be still inferred despite clear grammatical mistakes. The validity scale assesses whether the answer is valid and makes sense in the given context.", "filtered_refids": [["b21", "b38", "b7", "b32"], ["b46"], ["b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1155, "num_references": 6}
{"corpusid_sectionid": "256461385-s15", "title": "Narrative Why-Question Answering: A Review of Challenges and Datasets", "date": 2022, "section_title": "Short vs Long narratives", "section": "All reviewed datasets have short narratives as their context. The NarrativeQA short texts have a more complex narrative structure than other datasets, since the short context versions of the NarrativeQA are summaries of the larger narratives, and not single scenes from the long narratives. In short narratives, if there is a common lexical pattern between the question and a part of the narrative, or a large lexical overlap between the answer and the narrative, sophisticated models can treat free-form QA as an extractive task. For example, models trained on the TellMeWhy dataset generally try to find the answer span in the text and copy a part of the narrative as an answer (Lal et al., 2021).\n\nThe NarrativeQA dataset is the only dataset that has long narratives as its context. Linking narrative elements to answer questions in large narratives is harder than in short narratives (see section 3.2). Typically, in order to reason about long narratives, the parts relevant to reasoning are retrieved first (Ko\u010disk\u00fd et al., 2018;Tay et al., 2019;Frermann, 2019;Mou et al., 2020Mou et al., , 2021. The retrieval is difficult even with the state-of-the-art models due to the characteristics of narratives and the necessity of high-level narrative comprehension (Mou et al., 2021).", "filtered_refids": [["b19"], ["b10", "b26", "b18", "b40", "b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1283, "num_references": 6}
{"corpusid_sectionid": "256461385-s21", "title": "Narrative Why-Question Answering: A Review of Challenges and Datasets", "date": 2022, "section_title": "Evaluation", "section": "Evaluation of multiple-choice QA datasets. Evaluating answers of why-questions in multiple-choice datasets is a straightforward process. The presence of only one correct answer in multiple-choice questions helps to correctly assess the performance of the model without having to consider the potential answer ambiguity of why-questions. However, if question ambiguity is not addressed in the dataset, some of the questions can have more than one correct answer. Moreover, general why-questions can affect the accuracy of the overall assessment since these questions remove the necessity of the narrative understanding component of the task. Therefore, general questions should be identified and removed from the datasets in order to correctly assess the models' comprehension ability.\n\nEvaluation of free-form QA datasets. General questions can affect the correct evaluation on freeform QA datasets as well. Thus, in order to increase the accuracy of the overall evaluation process, general questions should be identified and transformed to context-specific by adding contextual information to those answers that make the question general. Moreover, ambiguity in why-questions can cause additional problems with both automatic and human evaluations. Due to the question and answer ambiguities, why-questions can have more valid answers than the collected golden answers in the datasets, and collecting all valid answers to these questions is not feasible and is probably impossible. Consequently, automatic metrics can only evaluate the output of models against the set of golden answers, which is likely only a small subset of all valid answers, and thus these metrics cannot fully measure the capacity of the models.\n\nHuman evaluation is considered the gold standard in all text generation tasks, including freeform QA (Celikyilmaz et al., 2020). However, performing human evaluation is a costly and slow process (Lal et al., 2021), and the reliability of human judgments is questionable (Gatt and Krahmer, 2018), especially in why-question answering that possesses many ambiguities. For example, human evaluators can prefer one interpretation of the question over another in terms of question ambiguity (section 2.2.1) or consider some causes (e.g., motivational) in the causal chain more reasonable than other causes (e.g., mechanistically causal) in case of answer ambiguity (section 2.2.2). Thus, further instructions are needed in the evaluation process to resolve ambiguities in the why-questions.", "filtered_refids": [[], [], ["b19", "b6", "b11"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2505, "num_references": 3}
{"corpusid_sectionid": "245616858-s2", "title": "The Quality of Lexical Semantic Resources: A Survey", "date": 2021, "section_title": "Polysemy", "section": "LSR, e.g, WordNet organizes the relation between terms and synsets through senses (term-synset pair). A term may have many meanings (one or more senses) which is called polysemous term. For example, head has 33 senses in WordNet which indicates that there are 33 relations between the word head and associated synsets. The ambiguity of a term that can be used (in different contexts) to express two or more different meanings is called polysemy. Due to synonymy and polysemy, the relation between terms and synsets is many-to-many relationship. Really, wrong semantic connection can be occurred in WordNet. A misconstruction that results in wrong assignment of a synset to a term is called Sense enumeration (Freihat et al., 2015).\n\nIn WordNet, a compound-noun which contains two-parts (modifier and modified) causes polysemy this is called compound-noun polysemy. It corresponds to \"the polysemy cases, in which the modified noun or the modifier is synonymous to its corresponding noun compound and belongs to more than one synset\". WordNet contains a substantial amount of this type of ploysemy such as: center and medical center in WordNet (Kim and Baldwin, 2013).\n\nAlso in WordNet, a special case is founded when there are related some senses (synsets) with a specific polysemous term and not connected with it. For example, a hierarchical relation between the meanings of a polysemous term (Freihat et al., 2013b). \"In case of abstract meanings, we say that a meaning A is a more general meaning of a meaning B. We say also that the meaning B is a more specific meaning of the meaning A\" which is called specialization polysemy. In this case, synset connections require reorganizing the semantic structure (using semantic relations) to cover and reflect the (implicit) hierarchical relation between all such senses.\n\nSo, the big challenge in WordNet is polysemy, because it may produce OVERLOAD connections (overload of a number of term-synset pairs). For example wrong assignments of a synset to terms in sense enumeration add overload relations in Word-Net which decrease the synset quality implicitly.", "filtered_refids": [["b0"], ["b6"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2109, "num_references": 3}
{"corpusid_sectionid": "245616858-s3", "title": "The Quality of Lexical Semantic Resources: A Survey", "date": 2021, "section_title": "Missing Senses, Lemmas and Relations", "section": "Despite \"the highpolysemous nature of wordNet, there are substantial amount of missing senses (term-synset pairs) in WordNet\" based on Ciaramita and Johnson's work that cause UNDER-LOAD of term synsets problem which is the opposite of the overload of term synsets. For example, new added words in languages cause missing senses (synsets) for some terms in lexical resources (e.g, WordNet). Such as Crypto Mining sense is missing from the synsets of mining term in Word-Net and only two synsets are founded in WordNet for it (Ciaramita and Johnson, 2003).\n\nAlso, WordNet contains synsets with missing lemmas as shown in (Verdezoto and Vieu, 2011). For example, \"the term brocket denotes two synsets in WordNet, the lemmas of the two synsets are incomplete. This is due to the following: the terms red brocket and Mazama americana which are syn-onyms of the lemmas in (b) are missing. The two synsets do not even include the term brocket deer. (a) brocket: small South American deer with unbranched antlers. (b) brocket: male red deer in its second year\" WordNet relations are \"useful to organize the relations between the synsets, while substantial amount of relationships between the synsets remain implicit or sometimes missing as in the case synset glosses relations. For example, the relation between correctness and conformity is implicit. The relation between fact or truth and social expectations in the following two meanings of the term correctness is missing. A human being may understand that correctness is a hyponym of conformity and fact or truth is a hyponym of social expectations, but this is extremely difficult or impossible for a machine because conformity is neither the hypernym of (a) nor (b). The relation between fact or truth and social expectations is missing because social expectations is not defined in WordNet which makes the two synsets are incorrect (Freihat et al., 2013a).\n\nMissing senses, missing terms or missing Relations may cause UNDERLOAD problem whether UNDERLOAD in connections or UNDERLOAD in synset itself. Therfore, to enhance synset quality, you have to solve the two main problems: OVER-LOAD and UNDERLOAD which are caused by polysemy and missing, respectively.", "filtered_refids": [[null], [null, "b17"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2208, "num_references": 3}
{"corpusid_sectionid": "245616858-s6", "title": "The Quality of Lexical Semantic Resources: A Survey", "date": 2021, "section_title": "Lemmas Validation Methods", "section": "The most famous method for lemmas validation is the work of Ramanand in (Nadig et al., 2008). They presented Validate Synset algorithm, its principle depends on \"dictionary definitions to verify that the words present in a synset are indeed synonymous or NOT\". This is due to the availability of synsets in which some members \"do not belong\". To accomplish their work they discussed the following research questions: \"is a given WordNet complete, how to select one lexico-semantic network over another, and are WordNet synsets INCOM-PLETE (may be many words have been omitted from the synset) and are WordNet synsets COR-RECT (the words in a synset indeed synonyms of each other and the combination of words should indicate the required sense)\". To answer the questions they try to validate the available synsets which are the foundations of a WordNet. \"A WordNet synset is constructed by putting together a set of synonyms that together define a particular sense uniquely. This sense is indicated for human readability by a gloss\". To evaluate the quality of a synset, they begin by looking for validating the synonyms that the synset has them. They follow these subtasks in the synset validation: are the words in a synset indeed synonyms of each other? Are there any words which have been omitted from the synset? And does the combination of words indicate the required sense? In their work, they focus on the quality of content embedded in the synsets; this is by attempting to verify a given a set of words/lemmas if they were synonyms and thus correctly belong to that synset or not synonyms based on the following two principles: \"if two words are synonyms, it is necessary that they must share one common meaning out of all the meanings they could possess. And a condition could be showing that the words replace each other in a context without loss of meaning\" (Nadig et al., 2008).\n\nA simple block diagram for a synset synonym validation using the system is shown in Figure 1. As we notice from the block diagram, the input to the system is: \"a WordNet synset which provides the following information: the synonymous words in the synset, the hypernym(s) of the synset and other linked nodes, gloss, example usages\". The output consists of \"a verdict on each word as to whether it fits in the synset, i.e. whether it qualifies to be the synonym of other words in the synset, and hence, whether it expresses the sense represented by the synset\". They used the following hypothesis: \"if a word is present in a synset, there is a dictionary definition for it which refers to its hypernym or to its synonyms from the synset\" (Nadig et al., 2008). However, dictionary definitions include useful clues for validating and verifying synonymy. The results show that: the algorithm is simple to implement and depends on the nature (the depth and the quality) of the used dictionary. Many words in WordNet are not validated, around 0.18 of total words in WordNet and 0.09 of total WordNet synsets that couldn't be validated. Also, the algorithm cannot detect omissions from a synset. To overcome this shortcoming of the algorithm, they proposed that expanding the validation to the synset gloss, and synset relations; using more dictionaries in validation; running the algorithm to other language WordNets, and applying the algorithm on other parts of speech in English. The same team proposed in (Ramanand and Bhattacharyya, 2007) an automatic method for the synset synonyms and the hypernyms validation based on new rules: 8 rules in synonym validation and 3 rules for hypernyms validation which is the first attempt of automatic evaluations for synsets in WordNet. They focus on the synsets because they are the foundational elements of wordnets and focus on the hypernymy hierarchy this is due to its importance in semantic linkages with other synsets. The quality of the synset and its hypernymy ensure the correctness, the completeness and the usability of the resource. They evaluate the quality of a wordnet by \"examining the validity of its constituent synonyms and its hypernym-hyponym pairs\". The authors defined the synonymy validation as \"the inspection of the words in the synset indeed synonyms of each other or NOT\", and they use the following observation: \"If a word w is present in a synset along with other words w 1 , w 2 , . . . , w k , then there is a dictionary definition of w which refers to one or more of w 1 , w 2 , . . . , w k and/or to the words in the hypernymy of the synset\" which was the hypothesis in the (Nadig et al., 2008) work. In the synonymy validation algorithm, the authors apply 8 rules in order which are the basic steps of the algorithm. Also, omissions from synsets aren't considered. Examples of these are synsets such as: Taylor, Zachary Taylor, President Taylor: no definition for the last multiword. Thus the multiword synonyms do share partial words. To validate such multi-words without dictionary entries, they check for the presence of partial words in their synonyms\". They run the algorithm on the noun synsets (39840 from the available 81426) of PWN, the inputs of the algorithm are synsets with more than one lemma, by running the validator which uses the online dictionary service Dictionary.com in validation, the results show that the percentage of the synsets where all words were validated is (0.701), Pushpak algorithm is simple and acts as a backbone for the synset validation models, also, the applied rules such as: Rule1, Rule2 and Rule7 are the most impact among synonym validation rules, on the other hand Rule4, Rule5 and Rule6 are the lowest. They conclude that many of the words present in PWN aren't validated and those with rare meanings and usages. \"The wordnet contains synsets that have outlier words and/or missing words\". The limiting factors are \"the availability of dictionaries and tools like stemmers for those languages\". They plan to summarize the quality of the synsets into a single number. The results could then be correlated with human evaluation, finally converging to a score that captures the human view of the wordnet. \"The presented algorithm is available only for Princeton WordNet. However, the approach could broadly apply to other language wordnets and other knowledge bases as well. And the algorithm has been executed on noun synsets; they can also be run on synsets from other parts of speech\". Also, in the same area and due to the wide-spread usage of lexical semantic resources, the lexicon quality evaluation became more and more important to tell us how well the applica-tions and operations based on these resources perform, for example, the authors in (Giunchiglia et al., 2017) describe a general approach to improve the quality of the lexical semantic resources by proposing an algorithm to classify the ambiguity words (based on their senses) in the lexical semantic resources to three classifications for a: polyseme, homonym or unclassified. Also, they present \"a set of formal quantitative measures of resource incompleteness\". And apply their work and analysis on \"a large scale resource, called the Universal Knowledge Core (UKC)\". The authors define \"two types of incompleteness, i.e., language incompleteness and concept incompleteness\". Language Incompleteness (in a lexical resource): a set of synsets/words/concepts is not lexicalized in a lexical resource (e.g UKC) by a specific language. A model (language incompleteness measurement) that can be used to measure the count (how much) of omitted synsets/words/concepts in the language is described in (Giunchiglia et al., 2017). The notion of \"concept incompleteness can be thought of as the dual of language incompleteness. If the language incompleteness measures how much of the UKC a language does not cover, the concept incompleteness measures how much a single concept is covered across a selected set of languages. Concept incompleteness: is the complement to 1 of its coverage\". A concept incompleteness model that can be used to measure the concept incompleteness is described in (Giunchiglia et al., 2017). Also in the same research, lexical ambiguity is described (it is happened when one word in a language denotes to more than one concept) and they computed the number of ambiguity instances in UKC, e.g., polysemy or homonymy. As an application example they applied the proposed algorithm to \"checks whether any two concepts denoted by a single word are polysemes of homonyms or NOT on the UKC concepts\". They run the algorithm which consists of 4 steps, and the results showed that, \"the UKC contains 2,802,811 ambiguity instances across its pool of 335 languages, these instances were automatically evaluated by the algorithm which, generated 0.32 polysemes among all the ambiguity instances and 0.22 homonyms across all languages\". They concluded that when the language coverage increases then the average ambiguity coverage decreases, and vice versa. Also, \"increasing the minimal required number of ambiguity instances consistently increases the percentage of polysemes (up to the 0.74), decreases the percentage of homonyms (down to the 0.11) as well as the percentage of unclassified instances (down to around the 0.15)\". Giunchiglia's group presented the language incorrectness evaluation method in UKC in (Giunchiglia et al., 2018), the authors proposed that \"the languages in the UKC are far from being complete, i.e., from containing all the words and synsets used in the everyday spoken or written interactions. And far from being correct, i.e., from containing only correct senses, namely, only correct associations from words and concepts to synsets\". These limiting factors impact the lexical resource quality. Language Incorrectness is the number of psycholinguistic mistakes in a language in a lexical resource per the number of total of concepts in that language in the same resource. They proposed a model to measure the language Incorrectness in (Giunchiglia et al., 2018). Furthermore, this work solves the problem of synset incomplete through presenting a model that transforms the semantic relations nodes from synsets to concepts. This is based the fact that is some words have multiple meanings, and each word is codified as a synset, consisting of a (possibly incomplete) set of synonymous words. The proposed approach describes the UKC design as three-layers: words, synsets and concepts. \"Word layer, stores what we call the universal lexicon, the synset layer, stores the world languages, and the concept layer, stores the world (mental) model(s), as represented by the CC\". This work makes an improvement in the UKC that influences on its quality; this due the work that becomes a language independent and handles the problem of each synset is associated with one and only one language.", "filtered_refids": [["b13"], ["b16", "b1", "b13", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 66, "num_chars": 10802, "num_references": 5}
{"corpusid_sectionid": "245616858-s8", "title": "The Quality of Lexical Semantic Resources: A Survey", "date": 2021, "section_title": "Synset Gloss Evaluation Methods", "section": "Measuring lexical semantic relatedness for a synset or a concept generally requires certain background information about the synset. Such information is often described in the synset gloss, which includes a different number of examples. The authors in (Zhang et al., 2011) introduced a new model to measure the semantic relatedness. The model exploits the WordNet gloss and semantic relations as features in building concept vectors. Also, they use other features in the designed model: \"wnsynant merges WordNet synonyms and antonyms. wn-hypoer merges WordNet hypernyms and hyponyms, and wn-assc merges WordNet meronyms, holonyms and related, which are features corresponding to associative relations\". This work participates in the improvement of the quality of Word-Net and Wikipedia operations.\n\nHayashi and his team used a gloss as an indicator in semantic relatedness is their work in (Hayashi, 2016) paper which measures the strength of the evocation relation between the lexicalized concepts. The authors in (Hayashi, 2016) defined the evocation as \"a directed yet weighted semantic relationship between lexicalized concepts\". Evocation relations are \"potentially useful in several semantic NLP tasks, such as the measurement of textual similarity/relatedness and the lexical chaining in discourse, the prediction of the evocation relation between a pair of concepts remains more difficult than measuring conventional similarities (synonymy, as well as hyponymy/hypernymy) or relatednesses (including antonymy, meronymy/holonymy)\" as in (Cramer, 2008). The work in (Hayashi, 2016) made good improvements on evocation relations by applying a novel approach in to prediction of the strength and direction of the evocation relations. For example, PWN dataset includes (39,309) synset pairs. If we compare the work of Y. Hayashi with the results of (Ma, 2013), Y. Hayashi considered \"evocation as a semantic relationship between lexicalized concepts, rather than a relation between words\", which were considered in (Ma, 2013). Also, the authors in (Maziarz and Rudnicka, 2020) worked on the possibility of the WordNet construc-tion based on \"a distance measure which performs better than other knowledge-based features in evocation relations\" (Hayashi, 2016). They used the Dijkstra's algorithm to \"measure the distance between nodes (words/synsets) in WordNet structure using a new method for evocation strength recognition based with four types of relations: \"wn: pure WordNet relations (directed WordNet edges), g: gloss relations (directed gloss relation instances), polyWN: the set of all pairs of polysemous lemma senses taken from WordNet (bidirectional relations between different senses of the same polysemous lemma) and polySC: the set of all pairs of polysemous lemma senses co-occurring in SemCor corpus\" as described in (Chklovski and Mihalcea, 2002). \"Dijkstra's distance measuring algorithm was applied on the four structures (one structure for each relation type) to get the minimum points between lexical concept pairs. Then 3-similarity measures are used in each time in order to obtain the best predictions of evocation strength in all cases\" (Maziarz and Rudnicka, 2020). Marek Maziarz and his team presented a novel approach for evocation relation measurement which based on the combination of three types of relations: \"gloss relations, pairs of polysemous lemma senses and instances derived from the SemCor corpus, and using the proposed inverse Dijkstra's distance for improving lexical WordNet structure for the needs of evocation recognition\". Like the categorization of methods in the preceding subsection, the next group of methods that we present attempt to explain the importance of the synset gloss in the synset quality evaluation by incorporating an additional examples in the gloss.", "filtered_refids": [["b18"], [null, "b9", "b8", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3819, "num_references": 5}
{"corpusid_sectionid": "245616858-s13", "title": "The Quality of Lexical Semantic Resources: A Survey", "date": 2021, "section_title": "Special Relations", "section": "Special types of synset relations are discussed in this section, these relations added more semantic properties on the synset lattice in lexicons, such as the work of Hayashi that was discussed in Section 3.2, they proposed \"a supervised learning approach to predict the strength (by regression) and to determine the directionality (by classification) of the evocation relation that might hold between a pair of lexicalized concepts PWN evocation dataset\" (Hayashi, 2016). The authors used neural network (NN) model for classifying evocation relations into FOUR categories which are: \"outbound\", \"inbound\", \"bidirectional\" and \"no-evocation\". And the forest regression model for the prediction of evocation strength is presented. The features of evocation relations are: Similarity/relatedness features: 4 similarity/relatedness features are utilized; two of them are synset-based such as \"wupSim computes Wu-Palmer similarity which gives the depth of node s from the root\" whereas others are word-based such as \"ldaSim feature provides the cosine similarity between the word vectors\". Lexical resource features: these features have been captured some asymmetric aspects of evocation relationships such as lexNW that finds \"the difference in graph-theoretic influence of the source/target concepts in the underlying PWN lexical-semantic network\". And Semantic relational vectors: in this feature category, they depended on the rule of (Mikolov et al., 2013). \"all pairs of words sharing a particular relation are related by the same constant (vector)\" to implement the features of the evocation relation. This paper proposed \"a supervised learning approach to predict the strength and to determine the directionality of the evocation relation between lexicalized concepts\"; which directly impacts the synset connectivity through improving the strength and directionality measurements. The best case in their experiments was the combination of the proposed features \"Similarity/relatedness features, Lexical resource features and Semantic relational vectors\" which outperformed the individual baselines (Hayashi, 2016). In addition, the authors of the paper (Maziarz and Rudnicka, 2020) focused on a special type of evocation relation which is polysemy, in order to recognize evocation strength. Strong polysemy links participate in constructing a high-quality lexical resource. The framework consisted of three steps. First: they studied the topologies (3-topologies) of the network of polysemy (graphs of senses). All relations of these topologies are polysemy. Spearman's correlation is used for evaluating the similarity measure in the 3-topologies in order to choose the best topology for lexical resource construction. Second: the evocation strength is measured based on the selected topology in step 1 and using the Neural Network and Random Forest models. Also, the authors presented a novel approach which is based on Dijkstra's algorithm to calculate distances between lexical concepts in WordNet structure, and using three types of relations: \"A complete polysemy graph (WN-g-co): for a given lemma-linked all senses together\". SemCor-based polysemy graph (WN-gsc): an incomplete graph built by extracting polysemy links from SemCor. It makes groups for such sense pairs that co-occur in the corpus, giving poor completeness but probably good precision\". And \"the chaining graph (WN-g-ch) tries to connect senses based on contemporary semantic relations between senses of all polysemous words/lemmas that are the closest as in the WordNet graph using the nearest-neighbor chaining algorithm\". The chaining topology is the best one among the three listed topologies for lexical resource construction. Therefore; the polysemy network of the lexical resource structure is constructed using \"chaining procedure executed on individual word senses of polysemous lemmas\". In the measuring of the evocation strength, the work (Maziarz and Rudnicka, 2020) used the inverse of Dijkstra's Distance. For each synset in the evocation set, they \"calculated the dist Dijkstra measure and its inverse to find the evocation strength\" using semantic relational vectors and the lexical resource features (Hayashi, 2016). Third, they applied Neural Network-NN and Random Forest-RF models to measure evocation strength on the chaining topology with the selected features. Good accuracy in the measurements of the evocation strength is resulted from applying both NN and RF models. Therefore, the authors recommended to utilizing the results in the applications of the polysemy such as Word Sense Disambiguation and Information Retrieval.", "filtered_refids": [["b10", "b9", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4626, "num_references": 3}
{"corpusid_sectionid": "32461868-s1", "title": "A Survey on Intelligent Poetry Generation: Languages, Features, Techniques, Reutilisation and Evaluation", "date": "2017-09-01", "section_title": "Languages", "section": "Poetry is an artistic expression of language. Humans have produced poetry in many languages and, due to their specificities, different languages happen to follow different poetic traditions, often focused on different forms. While the majority of poetry generation systems targets English and produces text in this language, there are systems of this kind in other languages, enumerated in this section.\n\nWell-known early attempts to poetry generation included French (Queneau, 1961;Oulipo, 1981), but Spanish was one of the first languages where this topic was explored in the context of AI, and related issues were discussed (Gerv\u00e1s, 2000;Gerv\u00e1s, 2001). For Portuguese, another romance language, song lyrics have been automatically generated for a given melody (Gon\u00e7alo Oliveira et al., 2007), and poetry has been produced according to user-given structures that would set the number of lines, stanzas, syllables per stanza, or the rhyme pattern (Gon\u00e7alo Oliveira, 2012). In an effort to use the same architecture for generating poetry in different languages, the previous system was extended to cover also Spanish and English (Gon\u00e7alo Oliveira et al., 2017). Another poetry generator originally developed for English was also adapted to produce poetry in Spanish (Ghazvininejad et al., 2016).\n\nTraditional eight-line Basque poems, aiming to be sung, have also been produced automatically (Agirrezabal et al., 2013). Although, as Portuguese and Spanish, Basque is spoken in the Iberian Peninsula, it has different origins and is significantly different from romance languages. Toivanen et al. (2012)'s system produced poetry in Finnish, another European language.\n\nAsian languages have also been targeted, some of which with specific tonal and rhythm requirements in poetry generation. This includes the generation of song lyrics in Tamil (Ramakrishnan A et al., 2009;Ramakrishnan A and Devi, 2010), a phonetic language; ancient Chinese classic poetry (Yan et al., 2013;Zhang and Lapata, 2014;Yan, 2016), with strict tonal and rhythm requirements; follow-up lines in Bengali (Das and Gamb\u00e4ck, 2014), matching the rhythm of a user-given line; and poetry inspired by news articles, in Indonesian (Rashel and Manurung, 2014).", "filtered_refids": [[], ["b11", "b12", "b10", "b9", null, "b24", "b8"], ["b0"], ["b28", "b41", "b29", "b30", null, "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2224, "num_references": 14}
{"corpusid_sectionid": "32461868-s2", "title": "A Survey on Intelligent Poetry Generation: Languages, Features, Techniques, Reutilisation and Evaluation", "date": "2017-09-01", "section_title": "Form features", "section": "Despite all the levels of language involved in poetry, form is a key feature for, at the first glance, recognis-ing the resulting text as poetic. Most common formrelated features are, without a doubt, a regular metre and rhymes. When alone, both of them are quite straightforward to handle by computer programs, especially when compared with content features.\n\nMetre is generally modelled with the number of syllables each line has, sometimes also considering the stress patterns (e.g. Manurung (2003), Gerv\u00e1s (2001), Tobing and Manurung (2015)), which indicate the position of the stressed syllables. Rhyme results from the repetition of certain sounds (e.g. in great and mate). End-rhymes, the most typical, occur when two lines end in the same sound. But some systems consider other kinds of rhyme, such as assonance or alliteration, which respectively involve the repetition of the same vowel or of a consonant sound throughout the poem.\n\nFor less phonetic languages, such as Portuguese (Gon\u00e7alo Oliveira et al., 2007) or Spanish (Gerv\u00e1s, 2001), it is often enough to design a set of orthography-based rules to handle metre and rhyme. For English, poetry generators (e.g. Manurung (2003), , Tobing and Manurung (2015)) typically resort to a pronunciation dictionary for this purpose (e.g. CMU's 1 ). Yet, automatic methods for the automatic scansion of poetry have also been developed (Agirrezabal et al., 2016).\n\nMetre and rhymes are often organised according to a well-known poetry form and some systems are designed to produce only poems of specific forms. Haikus traditionally have 3 lines, respectively with 5, 7 and 5 syllables (Manurung, 2003;Netzer et al., 2009), but there are modern haikus with a different number (Wong and Chun, 2008). Limericks have five lines, with lines 1, 2 and 5 generally longer, and rhyme of the kind AABBA (Levy, 2001;Manurung, 2003). The sonnet is a classic form of poem with 14 lines, typically with 10-syllables each. Depending on the tradition, it might have different groupings, stress patterns and rhyming schemes, such as ABAB CDCD EFEF GG (Ghazvininejad et al., 2016). Spanish traditional forms (Gerv\u00e1s, 2000;Gerv\u00e1s, 2001) include the romance, lines of 8 syllables, where all even-numbered rhyme together; the cuarteto, a stanza with four 11-syllable lines, where the two outer lines rhyme together; and tercetos encadenados, stanzas of three 11-syllable lines with the pattern ABA BCB CDC... Bertsolaritza is a Basque traditional verse with metre and rhyme constraints, typically sung (Agirrezabal et al., 2013). The generation of classic Chinese poetry has focused mostly on quartrains, four lines of 5 or 7 characters with a rigid tonal pattern where two kinds of tones are interleaved, and a rhyme scheme where the majority of the lines in the same poem end with the same vowel, but not the same character (Yan et al., 2013;Zhang and Lapata, 2014;Yan, 2016).\n\nThe poetry form can be decided from the initial data (Gerv\u00e1s, 2000), while other systems generate poetry in more or less any form, depending on a user-provided template, which might be strictly structural (Gon\u00e7alo Oliveira, 2012) or a poem, possibly with some words stripped (Toivanen et al., 2014). There are also systems focused on generating song lyrics, which have less traditional forms, but where metre is key for matching the rhythm, while other features should still be present. These include melodies where stressed and weak beats are identified (Gon\u00e7alo Oliveira et al., 2007;Ramakrishnan A et al., 2009;Gon\u00e7alo Oliveira, 2015), pop songs (Barbieri et al., 2012), or rap (Malmi et al., 2016;Potash et al., 2015) where, besides rhyme, assonance is modelled as the repetition of vowel phonemes (e.g. in raps and tax). ered and is often only softly satisfied, for instance, by using words that belong to the same semantic domain. This section describes how different poetry generators select their content in order to transmit a meaningful message or, at least, to be, as much as possible, semantically coherent.\n\nIntelligent poetry generation systems often exploit a model of semantics, either a semantic knowledge base, or a statistical model of distributional semantics. The former is usually a more theoretical view on linguistic knowledge, where words are connected according to labelled relations, with different meanings. Poetry generators have used knowledge bases with verbs and their restrictions and ontological categories (Ramakrishnan A and Devi, 2010); semantic networks extracted from dictionaries, that go beyond synonymy and hypernymy, and cover other relations such as causation, property and others (Gon\u00e7alo Oliveira, 2012); WordNet, a lexical knowledge base Agirrezabal et al., 2013;Tobing and Manurung, 2015); and Con-ceptNet, a common sense knowledge base (Das and Gamb\u00e4ck, 2014). Those have been used not only to restrict the generated words to a common semantic domain, but also for increasing the paraphrasing power, towards higher variation and better covering of different metres.\n\nDistributional models of semantics target how language is actually used, in a collection of documents, and consider that words that occur in similar contexts have similar meanings. These include vector space models, either based on words (Wong and Chun, 2008;McGregor et al., 2016), also including word embeddings learned from collections of poems (Yan, 2016) or from Wikipedia (Ghazvininejad et al., 2016), or based on sentences (Malmi et al., 2016), both used to compute the semantic relatedness with the cosine similarity; or word associations (Netzer et al., 2009;Toivanen et al., 2012) which, according to some authors, capture relations in poetic text better than WordNet-like lexical knowledge bases.\n\nIn some systems, text is generated according to a grammar for handling syntax, possibly also considering semantic features (Manurung, 2003). In Gon\u00e7alo Oliveira (2012)'s system, the grammar is tightly related to the semantics, as each rule transmits a known semantic relation and can be instan-tiated with any pair of words sharing relations of that kind (e.g. vehicle-car or fruit-mango, for hypernymy).\n\nYet, in order to enable some kind of interpretation, the poem must actually be about something or, at least, be different for different stimuli, reflected in its content. Stimuli can be given in different forms, with different degrees of precision, namely: a list of semantic predicates (e.g. love(John, Mary)) (Manurung, 2003); one (Netzer et al., 2009;Toivanen et al., 2013;Ghazvininejad et al., 2016) or more (Wong and Chun, 2008;Gon\u00e7alo Oliveira, 2012;Zhang and Lapata, 2014;Yan, 2016) keywords that will, somehow, set a semantic domain and constraint the generation space; a line of text (Das and Gamb\u00e4ck, 2014) or a sequence of lines (Malmi et al., 2016) to be followed; a textual document, which can either be a single sentence with a message (Gerv\u00e1s, 2001), or a longer text from a blog (Misztal and Indurkhya, 2014) or newspaper (D\u00edaz-Agudo et al., 2002;Rashel and Manurung, 2014;Toivanen et al., 2014;Tobing and Manurung, 2015;Gon\u00e7alo Oliveira and Alves, 2016).\n\nIn order to extract meaningful information to be used in the poem, different systems process the input document differently. For instance, Toivanen et al. (2014) acquire novel associations from the document (e.g. bieber and alcohol, in opposition to pop and star), identified by contrast with well-known associations. Tobing and Manurung (2015) extract dependency relations from the document and use them to constrain the generated poem. They argue that, though not a genuine semantic representation, dependency relations are a useful abstraction of the text and end up conveying its semantics. In fact, some dependency relations include semantic relations (e.g. agent-of, subject-of, object-of ). A final example (Gon\u00e7alo Oliveira and Alves, 2016) extracts concept maps from the input document, and uses them as a semantic network.\n\nTowards an improved interpretation, 's system produces natural language commentaries for each generated poem, providing some generation context. A similar feature is presented by Gon\u00e7alo Oliveira and Alves (2016) or Gon\u00e7alo Oliveira et al. (2017). In this case, semantic relation instances explaining the connection between the input keywords and the words used can be provided either in raw format or, if a grammar exists for this purpose, in natural language.\n\nAdditional semantic features captured by poetry generators include sentiment (Gerv\u00e1s, 2000;Gon\u00e7alo Oliveira et al., 2017), which typically involves exploiting a polarity lexicon; or emotion (Misztal and Indurkhya, 2014), in this case achieved with the help of WordNet Affect.\n\nFigurative language is often implicitly present as a consequence of reusing material from humanproduced poetry, but its presence can also be explicitly handled, for instance, by exploiting similes mined from Google n-grams . Veale (2013) points out the importance of contentfeatures and presents a system more relaxed on form but heavily influenced by figurative language. More precisely, similes (e.g. politicians are crooks) are exploited for generating metaphors (e.g. he is a crook) and conceptual blends (e.g. sweet silence).\n\nPoetry generation systems handle a broad range of features both at the formal and at the content level. Dealing with so many constraints may actually turn out to be computationally impractical (see e.g. Tobing and Manurung (2015)). Yet, this also depends on the techniques adopted for handling all the constrains, surveyed in the following section.", "filtered_refids": [[], ["b9", "b20", "b31"], ["b11", "b1", "b9", "b20"], ["b10", "b9", "b20", "b41", null, "b23", "b8", "b0", "b17"], ["b11", "b15", "b34", "b26", "b18", "b29", "b8", "b2"], ["b7", "b31", "b0"], ["b10", "b18", "b32", "b21", null, "b23"], ["b20"], ["b12", "b10", "b34", "b18", "b9", "b13", "b41", "b31", "b30", null, "b22", "b23", "b7", "b33"], ["b34"], ["b14"], ["b14", "b22", "b8"], ["b37"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 49, "num_chars": 9570, "num_references": 54}
{"corpusid_sectionid": "32461868-s3", "title": "A Survey on Intelligent Poetry Generation: Languages, Features, Techniques, Reutilisation and Evaluation", "date": "2017-09-01", "section_title": "Artificial Intelligence Techniques", "section": "Early poetry generators (e.g. Queneau (1961) or Oulipo (1981)) relied heavily on combinatory processes applied to a set of human-created poems. On the other hand, intelligent poetry generation systems consider semantics when selecting content and take advantage of computational techniques that add value to the generation process, with a more efficient exploration of the space of possible generations, also enabling to handle a larger number of features, often towards a predefined intention, and sometimes resulting in poems with higher novelty. This section enumerates some of those techniques, borrowed from the domain of AI.\n\nThe technique of Case-Based Reasoning exploits past solutions for solving new similar problems, in a four-step approach (retrieve, reuse, revise, retain). In the scope of poetry generation (Gerv\u00e1s, 2001;D\u00edaz-Agudo et al., 2002), it has been instantiated as follows: retrieve vocabulary and line examples that suit fragments of a poem draft; reuse the part-ofspeech (POS) structure of the example lines for producing new lines and combine them with the words in the vocabulary; present the resulting draft to the user, for revision; perform a linguistic analysis of the revised poems and retain it for further generations.\n\nChart Parsing is a known technique that employs dynamic programming for parsing text according to a context-free grammar. Chart Generation, used by some poetry generation systems (Manurung, 1999;Manurung, 2003;Tobing and Manurung, 2015;Gon\u00e7alo Oliveira, 2012), is the inverse of chart parsing. Given a grammar, a lexicon, and a meaning (e.g. as a set of predicates), chart generation produces all syntactically well-formed texts that convey the meaning. Charts store complete generated constituents (inactive edges) as well as incomplete (active edges), with dotted rules marking constituent portions yet to be generated.\n\nPoetry composition can be seen as an incremental task, where initial drafts go through several iterations, each ideally better than the previous, until the final poem. The application of evolutionary algorithms to this task (Levy, 2001;Manurung, 2003) is thus natural. The basic idea is to generate an initial population of poems by a simple method, and then evolve it through several generations, towards more suitable poems, assessed by a fitness function that considers a set of relevant features for poetry. Changes in the population are obtained by the application of crossover and mutation operators. Crossover creates new poems from two other poems in the population. This can be achieved by adopting the syntax of the former but the words or the rhyme of the latter (Levy, 2001), or by swapping parts of the former with parts of the latter (Manurung, 2003). Mutation may involve the replacement of some words in all the poem, only in a certain line, or changing the rhyme (Levy, 2001). It may also consist of adding, deleting or changing contents of the poem, possibly considering the target semantics (Manurung, 2003).\n\nGiven the number of constraints involved in poetry generation, it is also natural to have this problem formulated as a Constraint Satisfaction approach (Toivanen et al., 2013;Rashel and Manurung, 2014). For this purpose, a constraint satisfaction solver explores the search space and produces solutions that match the input properties (how poems can be like), represented as predicates that indicate the poem structure and the vocabulary words to be used. Different constraints and their types can be set for different generations. Hard constraints are mandatory (e.g. number of lines, syllables per line), while soft constraints are optional (e.g. rhymes).\n\nLanguage models have been used to generate poetic text, constrained by both a target style and a predefined form. These include Markov models (Barbieri et al., 2012) and models based on Deep Neural Networks (DNNs), including Recurrent Neural Networks (RNNs). Given a sequence of words, a RNN was used to predict the next word in rap lyrics (Potash et al., 2015). Or given the line history, RNNs can be used for generating new lines incrementally, considering their respective phonetics, structure and semantics (Zhang and Lapata, 2014;Yan, 2016). There may be one neural network (NN) for selecting the structure of lines and another for guiding the generation of single words within a line. Towards better poeticness, Yan (2016) goes further and adds poem refinement iterations to the previous process. The RNN language model may also be guided by a Finite-State Acceptor that controls rhyme and metre (Ghazvininejad et al., 2016). Malmi et al. (2016) use a DNN and the RankSVM algorithm to predict the next full line, from a knowledge base of human-produced lyrics, considering rhyme, structure and semantic similarity. Support Vector Machines (SVMs), trained in a poetry corpus, were also used to predict followup lines with certain syllabic and rhyming properties (Das and Gamb\u00e4ck, 2014). And classic NNs were also used to measure the fitness of poems generated by an evolutionary approach (Levy, 2001). In the latter case, the NN was trained on human judgments of creativity in a selection of limericks, half by humans and another half randomly generated.\n\nMisztal and Indurkhya (2014) adopted a Multi-Agent approach where a set of artificial experts, focused on a particular aspect of poetry generation, interact by sharing results on a blackboard. Experts can contribute with words matching a given topic or emotion (word-generating), arrange words in the common pool into phrases (poem-making), or select the best solutions according to given constraints and heuristics (selection experts), among others. Poetry generation has also been tackled as a Generative Summarization framework that incorporates poetic features as constraints to be optimised (Yan et al., 2013). Candidate poems are retrieved for a set of keywords, they are segmented into constituent terms and clustered given their semantics. Lines that conform the structural constraints, each using terms from the same cluster and with some correlation, are then selected. Suitable term replacements are finally made iteratively, in order to improve structure, rhyme, tonality and semantic coherence.", "filtered_refids": [["b24", null], [null, "b9"], ["b12", "b19", "b20", "b31"], ["b20", "b17"], ["b30", "b33"], ["b10", "b26", "b18", "b41", null, "b7", "b2", "b17"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 6234, "num_references": 21}
{"corpusid_sectionid": "32461868-s5", "title": "A Survey on Intelligent Poetry Generation: Languages, Features, Techniques, Reutilisation and Evaluation", "date": "2017-09-01", "section_title": "Evaluation", "section": "Poetry generation is becoming a mature research field, which is confirmed by several works that go beyond the production and exhibition of a few interesting poems that, to some extent, match the target goals. Despite the subjective aspect that makes poem evaluation far from trivial, it is more and more common to explore different ways for assessing both the obtained results and the generation process.\n\nClaiming that the intended audience of poetry consists of people, the evaluation of computer generated poetry has often resorted to human judges, who assess produced poems according to a set of predefined dimensions. For instance, although, to some extent, the properties of poeticness, grammaticality and meaningfulness can be validated by the methods applied (Manurung, 2003;Misztal and Indurkhya, 2014), they can also be assessed by the observation of the obtained results. Having this in mind, some researchers (Yan et al., 2013;Das and Gamb\u00e4ck, 2014;Zhang and Lapata, 2014;Yan, 2016) evaluated the output of their system based on the opinion of human judges on a set of produced poems, who answered questionnaires designed to capture the aforementioned properties. Still relying on human opinions, other authors took conclusions on the quality of their results with questions that rated slightly different dimensions, though with some overlap. Those include the typicality as a poem, understandability, quality of language, mental images, emotions, and liking (Toivanen et al., 2012); or structure, diction, grammar, unity, message and expressiveness (Rashel and Manurung, 2014).\n\nSome of the previous systems ended up conducting a Turing test-like evaluation, where the scores of the systems produced by their poems were compared to those for human-created poems (Netzer et al., 2009;Toivanen et al., 2012;Agirrezabal et al., 2013;Rashel and Manurung, 2014). Despite also relying on human evaluation, other researchers compared poems produced only by their systems but using different parameters or strategies (Gerv\u00e1s, 2000;Gon\u00e7alo Oliveira et al., 2007;Barbieri et al., 2012;Yan et al., 2013;McGregor et al., 2016); or poems produced by other systems with a very similar purpose (Zhang and Lapata, 2014;Yan, 2016).\n\nEstablished methods to evaluate human creativity, from the psychology domain, have also been proposed to assess computational creativity, including automatically generated poetry. van der Velde et al. (2015) present a map of words related to creativity (e.g. unconventional, spontaneitiy, imagination, planning, craftmanship, art), obtained from an association study. These words were clustered and may be used to define relevant dimensions for evaluating creativity, for instance, in a poem, and in its creation process. Another study employing methods from psychology (Lamb et al., 2016) resorted to human experts for rating the creativity of poems, some written by humans and others generated automatically, by different creative systems. Judges were not informed of this and, despite some consensus on the best and worst poems, they disagreed on the remaining, which made the authors unsure on the suitability of their approach for computer-generated poetry.\n\nThere has been a huge discussion on the suitability of the Turing test for evaluating computational creativity approaches . The main criticism is that it is focused on the resulting products and not on the involved creative process, which encourages the application of simpler processes, some of which might be merely concerned with tricking the human judge into thinking their outputs were produced by a human. The FACE descriptive model  has been proposed to evaluate the creative process and was used in the evaluation of poetry generation systems Misztal and Indurkhya, 2014). To be assessed positively by this model, a creative system must create a concept (C), with several examples (E), include an aesthetic measure (A) for evaluating the concept and its examples, and provide framing information (F) that will explain the context or motivation of the outputs. Yet, other experiments (McGregor et al., 2016) suggest that framing, which can be provided as a natural language commentary, does not make a big difference in human assessment of the creativity, meaningfulness or general quality of computer-generated poems. In many systems, the evaluation of certain features is part of the process, as it happens for Misztal and Indurkhya (2014)'s automatic experts, or with the large number of systems that assess the metre, rhymes and other properties of produced texts during generation time. Few approaches have tried to evaluate poetry generation systems or their results automatically, and these often tackled less subjective dimensions of poems. Those include the application of metrics typically used in the scope of automatic summarization and machine translation, such as ROUGE, to access the performance of a poetry generator based on Generative Summarization (Yan et al., 2013); or BLEU, to assess the ability to generate valid sequences of lines (Zhang and Lapata, 2014;Yan, 2016). ROUGE was also used to assess variation in poems generated by the same system with the same parameters (Gon\u00e7alo Oliveira et al., 2017). Moreover, the perplexity of the learned language model has been compared to human-produced poetry with a similar style (Zhang and Lapata, 2014;Yan, 2016); the average cosine similarity of the lines in automatically-created haikus has been compared to the same value for awarded haikus by humans, to conclude that semantic coherence is similar (Wong and Chun, 2008); and Pointwise Mutual Information, computed on Wikipedia, has been used to measure the association between seeds and words effectively used (Gon\u00e7alo Oliveira, 2015;Gon\u00e7alo Oliveira et al., 2017).\n\nOther systems focused on measuring the novelty of their results, especially those that reuse more material or try to model an artist's style. The main goal is to generate poems that share some similarities with the inspiration set, but are different from any existing poem. Potash et al. (2015) compute the cosine similarity between the automatically generated lines and the original lines by the target artist (the lower cosine, the higher the novelty). To compute similarity, the number of rhyming syllables is divided by the total number of syllables (rhyme density) and the result is compared to the same number for the lyrics of the target artist. Rhyme density is also computed by Malmi et al. (2016) and, to some extent, by Gon\u00e7alo Oliveira et al. (2017. In the latter case, it is given by the number of lines with an end-rhyme divided by the total number of lines.", "filtered_refids": [[], ["b41", "b20", "b32", "b30", null, "b22", "b7"], ["b11", "b41", "b32", "b30", null, "b21", "b23", "b8", "b2", "b0"], ["b16", null, "b36"], ["b15", "b14", "b41", "b21", null, "b22"], [null, "b26", "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 6661, "num_references": 29}
{"corpusid_sectionid": "2395785-s2", "title": "To what extent is Immediate Constituency Analysis dependency-based? A survey of foundational texts", "date": "2017-09-13", "section_title": "Reification", "section": "In graphical trees, nodes and edges are turned into discrete graphical objects. This encoding operation is called reification (from Lat. r\u0113s 'thing'; hence to reifiy 'to turn into a thing'). Theoretical objects can be expressed by graphical objects, in which case, they are indeed reified (Kahane and Mazziotta, 2015;Mazziotta, 2016b). However, as illustrated by the alternative between the use of arrows or the use of vertically ordered strokes, the fact that diagrams are drawn on a bidimensional plane allows for the configurational expression of theoretical objects. Configurational expression competes with reification -e.g. in phrase structure trees (henceforth PST), words are often linearly ordered, which is a configurational means of expression of their precedence relations; this precedence could be reified by arrows instead.\n\nAs an example of linguistic entities that are conceived as distinct notions in the argumentation but not reified in the diagrams, one can introduce S.W. Clark's diagrams. The diagrams in his Practical grammar (1847), a pedagogical handbook on the grammar of English, do not reify the relations between the words -see Mazziotta's comprehensive study (2016a), although the text acknowledges that some words modify or complete others. In the diagrams, words are depicted as labeled bubbles that are but aggregated to one another ( fig. 3).  (Clark, 1847, 23) It is clear in Clark's diagrams that bubbles in contact correspond to word in syntagmatic relation (cf. section 3.2). Their configuration conveys information about the syntactic analysis they encode. It is possible to reify these contacts and we obtain a diagram that, intuitively, is very similar to a classical dependency tree ( fig. 4) -the only difference is that the connection between the verb and the subject and between the verb and the object are not directed. In the diagrams, the choice of what is reified and what is not is closely bound to the theoretical stance chosen, but, as it will appear, some options are not always taken in full awareness.\n\n3 What does dependency-based mean?\n\nThe difference between constituency and dependency is presented through their use of tree structures under 3.1 and the definitional attributes of dependency trees are reviewed under 3.2.", "filtered_refids": [["b15", "b21"], [null], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2279, "num_references": 3}
{"corpusid_sectionid": "2395785-s4", "title": "To what extent is Immediate Constituency Analysis dependency-based? A survey of foundational texts", "date": "2017-09-13", "section_title": "Connection-basedness.", "section": "Words combine pairwise, they are in a syntagmatic relationship in the sense of de Saussure (2013, 170):\n\nWords as used in discourse, strung together one after another, enter into relations based on the linear character of languages. Linearity precludes the possibility of uttering two words simultaneously. They must be arranged consecutively in spoken sequence. Combinations based on sequentiality may be called syntagmas. The syntagma invariably comprises two or more consecutive units: for example, re-lire ('re-read'), contre tous ('against all'), la vie humaine ('the life of man'), Dieu est bon ('God is good'), s'il fait beau temps, nous sortirons ('if it's fine, we'll go out').\n\nSince the term syntagma has been led astray -this is especially the case in French linguistic: Fr. syntagme has been used to translate phrase (Chomsky, 1969) -, we suggest to use the term connection introduced by Tesni\u00e8re (2015, ch. 1, \u00a7 3-5):\n\nEach word in a sentence is not isolated as it is in the dictionary. The mind perceives connections between a word and its neighbors. The totality of these connections forms the scaffold of the sentence. [. . . ] [A] sentence of the type Alfred speaks is not composed of just the two elements, Alfred and speaks, but rather of three elements, the first being Alfred, the second speaks, and the third the connection that unites them -without which there would be no sentence.\n\nElaborating from this quotation, we call connection the undirected relation underlying any dependency. 3 Hence, in a dependency tree, syntagmatic relations are encoded by edges. By contrast, in a PST, edges represent constituency relations -see also (Mel'\u010duk, 1988, 13-14). Analyses and diagrams that make use of connections to describe the syntactic structure of constructions are connection-based.\n\nBinarity. In a dependency tree, a connection always involves exactly two words. In a PST, a phrase can have more than two immediate constituents. Binarity is a central property of ICA until the 60's and still remains preeminent. 4 It seems that binarity is the consequence of the connectionbasedness of these ICAs. Non-binary structures appear later, cf. fig. 6 (Chomsky, 1965, 65). 5 Figure 6: First PST in (Chomsky, 1965) Headedness. Connections are directed, as explained by Tesni\u00e8re (2015, ch. 2, \u00a7 1-3):\n\nStructural connections establish dependency relations between words. In principle, each connection unites a superior term and an inferior term. The superior term is called the governor, and the inferior term the subordinate. Thus in the sentence Alfred speaks (Stemma 1), speaks is the governor and Alfred is the subordinate. We say that the subordinate depends on the governor and that the governor governs the subordinate. Thus in the sentence Alfred speaks (Stemma 1), Alfred depends on speaks, and speaks governs Alfred.\n\nWe call this property headedness.\n\nIt is noteworthy to mention that although the notion of head is absent from , headedness is considered as a central notion in many early ICA-based presentations, and especially in (Bloomfield, 1933). Bloomfield's work emphasizes constituency relations, but connections are also considered: \"Every syntactic constructions shows us two (or sometimes more free forms combined in a phrase, which may call the resultant phrase.\" ( \u00a7 12.10) This last definition allows Bloomfield to oppose endocentric vs. exocentric constructions, according to the fact that the resultant phrase may belong or not to the \"formclass\" (i.e. distributional class) of one of the constituents (called the head). In a dependency tree, every construction is endocentric, i.e. connections are directed from a governor to a dependent. In a PST, endocentric constructions can be encoded by marking one of their constituents as the head.\n\nFlatness (i.e. absence of stratification). In a dependency tree, dependents that have the same governor are not hierarchized. In a PST, phrases are embedded: if a head word has several complements (or specifiers, or adjuncts), each of them can belong to a different stratum (Kahane, 1997;Kahane and Mazziotta, 2015). E.g., the dependency tree of a sentence such as Mary gives Peter a book represents Mary, Peter and a book as co-dependents of gives that belong to the same level, whereas a PST of the same sentence can attach Mary, Peter and a book at different levels. Stratification remains the main difference between dependency syntax and ICA-based syntax. This point will be developed in Section 4.\n\nNode-to-word mapping. Dependency trees do not encode connections by the means of nodes: these are used exclusively to encode words. 6 As a result, one can state:\n\nA dependency structure for a sentence is a one-to-one mapping between the nodes of a tree (the dependency tree) and the words of the sentence. (Kahane, 1996, 45) By contrast, classical PST use nodes to encode words as well as constituents. Thus the mapping between nodes and words is not one-to-one. As it will appear in the next section, node-to-word mapping does not imply flatness.\n\nAs soon as additional nodes are introduced, labels on these nodes can be used to reify other information. E.g., X-bar syntax (Chomsky, 1970) uses XP vs. X labels to express headedness.  ", "filtered_refids": [["b27"], [], ["b31", "b5"], [null], [null], [null, "b31", "b4"], [], [], ["b2"], ["b15", "b18"], [], [null], ["b6"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 45, "num_chars": 5226, "num_references": 13}
{"corpusid_sectionid": "2395785-s8", "title": "To what extent is Immediate Constituency Analysis dependency-based? A survey of foundational texts", "date": "2017-09-13", "section_title": "Barnard, 1836", "section": "To our knowledge, the first diagram representing an ICA ( fig. 7) appears in Frederick A. P. Barnard's Analytic Grammar with Symbolic Illustrations (1836). Syntactic categories of units are represented by special symbols and braces that indicate in a configurational way that a list of units combine together to form another unit. In his text, Barnard compares man and a rational animal or quadruped and a four-footed animal and says (Barnard, 1836, 243-244):\n\nWe thus construct phrases standing in the places of nouns, and answering all their purpose. [. . . ] Contemplating, then, a noun and its adjective, we say that they constitute, together, a compound noun. Contemplating an adjective and its accompanying adverb, we say, in like manner, that they constitute a compound adjective. E.g., in fig. 7, in and disposition form together a unit with the same category as very and who is mild and in disposition form together a unit with the same category as many. 8 Barnard's diagrams have no discrete means to express individual part-whole relations: the brace Figure 7: Barnard's diagram (1836) is equivalent to Chomsky's rewriting operator as well as the \"+\" symbol, linking a phrase with the entire set of its immediate constituents. There is no independent reification for the two operations. Syntagmatic relations are not represented in a discrete way either. The brace inscribes the whole construction. According to our terms (section 3), such a diagram is thus neither exactly connectionbased nor exactly constituency-based.\n\nAs shown in tab. 3, the diagram is very different from a canonical dependency tree: not a single definitional attribute firmly holds.   fig. 7 with respect to definitional attributes of dependency trees.\n\n4. 3 Nida, 1943;1966 It seems that Barnard's diagram was overlooked by his contemporaries. More than one century passed between this attempt and the next ICA diagram. 9 It appears in Nida's Morphology (1949(1943, 87). 10 Fig. 8 shows the first ICA diagram published by Nida and fig. 9 is a diagram from (Nida, 1966).  (1949(1943)) 9 In the mid time, other diagrams, which are much more dependency-based and that will not be discussed here, have been proposed by several authors (Clark, 1847;Reed and Kellogg, 1876;Kern, 1883;Tesni\u00e8re, 1934). 10 We could not access the fist edition of Nida's Morphology (1943). Figure 9: Nida's diagram (1966) At first glance, it would seem that Nida's first diagram could be interpreted as a PST. It is tempting to consider that fig. 8 is completely equivalent to fig. 10, where constituency relations are reified as distinct graphical entities. Figure 10: Nida, 1943's diagram, reified However, fig. 9, which elaborates on the same rationales as fig. 8, demonstrates that it is not the case. Both diagrams consist of arcs between words and arcs between words and other arcs. Every single node in these diagrams corresponds to a word. Thus, the contact point between strokes are not equivalent to reifications, since they are not discrete graphical entities and they possibly allow for several interpretations.\n\nTo fully understand fig. 9, let us recall that Nida's work was preceded by Bloomfield's seminal text on constructions (section 3.1). Hence, in his fig. 9, arcs bear additional symbols (\">\", \"\u00d7\", \"=\") and the accompanying text clearly explains how to interpret them (Nida, 1966, 17):\n\nIn addition to the usual set of lines used to show relationships between immediate constituents, an additional set of symbols has been employed to mark exocentric, endocentric, and paratactic relationships.\n\nConsequently, the labels over the strokes reify the headedness of the connections. Nida's diagrams are connection-based and not constituency-based. Such a diagram is close to a dependency tree. The only difference between classical dependency trees and Nida's diagrams is that the later are not flat, but stratified: connections are ordered and hierarchized. The consequence of such an analysis is that connections can be connected to one another. From a mathematical perspective, this means that edges can have other edges as vertices -see (Kahane and Mazziotta, 2015) for a formalization of such a structure, that can be called a polygraph.\n\nTab. 4 shows that the evolution between fig. 8  and fig. 9 consists in encoding headedness in the diagram. Fig. 9 is almost a dependency tree: the only attribute that does not hold is flatness.", "filtered_refids": [[null], [null], [], ["b25", "b29", null, "b19", "b23", "b24"], [null], [], ["b15"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 40, "num_chars": 4414, "num_references": 10}
{"corpusid_sectionid": "2395785-s10", "title": "To what extent is Immediate Constituency Analysis dependency-based? A survey of foundational texts", "date": "2017-09-13", "section_title": "Wells, 1947", "section": "Rulon S. Wells (1947) is more interested in constituency relations than in constructions seen as wholes. The term construction itself is used in another meaning -\"The reader must constantly bear in mind that our definition of this term is not the same as Bloomfield's\" (Wells, 1947, note 19). He proposes a linear diagram ( fig. 11).\n\nthe || king ||| of |||| England | open ||| ed || Parliament Figure 11: Well's diagram (1947) This diagram (Wells uses this very term to designate this inscription) corresponds to the following analysis (Wells, 1947, 84):\n\nLet us call the ICs of a sentence, and the ICs of those ICs, and so on down to the morphemes, the CONSTITUENTS of the sentence; and conversely whatever sequence is constituted by two or more ICs let us call a CONSTITUTE. Assuming that the ICs of The king of England opened Parliament are the king of England and opened Parliament, that those of the former are the and king of England and those of the latter are opened and Parliament, and that king of England is divided into king and of England, of England is divided into the morphemes of and England, and opened is divided into open and -ed-all of which facts may be thus diagrammed [by fig.  11 ] \"\n\nAlthough this analysis is purely based on the decomposition of wholes (\"constitutes\") into parts (\"constituents\"), the symbols made of \"|\" in Wells's diagrams reify the combination/separation operations (according to the perspective, that can be deductive or inductive) of the elements around them. In a sense, they correspond more to connections than to constituency relations. Tab. 5 shows that Wells's diagram is equivalent to Nida's first diagram ( fig. 8).", "filtered_refids": [[null, "b32"], [null], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1672, "num_references": 3}
{"corpusid_sectionid": "2395785-s12", "title": "To what extent is Immediate Constituency Analysis dependency-based? A survey of foundational texts", "date": "2017-09-13", "section_title": "Gleason", "section": "We may, as a first hypothesis, consider that each of [the words] has some statable relationship to each other word. If we can describe these interrelationships completely, we will have described the syntax of the utterance in its entirety.\n\n[. . . ] At a second step in our procedure, let us assume that these pairs of words function in the utterance as single units. [. . . ] If this procedure is valid, there is no reason why it cannot be repeated as many times as may be useful. Something like the following [diagram] might result.\n\nIn the mentionned diagram ( fig. 12), braces indicates the units that combine together as in Barnard's diagrams (cp. fig. 7).\n\nA characteristic of Gleason's handbook is that it introduces alternate diagrams to inscribe the same Figure 12: Gleason's first ICA diagram analysis. Fig. 13 is similar to Wells's diagrams, but where the hierarchy of frontiers is inverted. Gleason, who starts from the bottom, use thin stroke for the most embedded connection, while Wells, who starts from the top, use them for main segmentation of the sentence.  (Gleason, 1961(Gleason, (1955, ibid.):\n\nThe procedure which we have just sketched will be useful to us, if it serves as a framework within which all the relationships of the utterance can be effectively and economically described. This is done in the following diagram, where the heavier line is \"intended to indicated the most direct relationship between old and house [. . . ] describable in terms of a chain of relationships each of which individually seems significant.\" This last diagram clearly provides both constituency relations (reified by mere strokes) and connections (reified by double arrows). The book does not contain any diagram that is exactly a tree.\n\nThe attributes of Gleason's diagrams are summarized in tab. 6. 4. 6 Hockett, 1958Hockett (1958 formalizes the concept of construction by the means of diagrams consisting of   . 15). Two compartments represent immediate constituents and the lower compartment represents the resultant phrase. These boxes can be embedded to give the whole ICA of a sentence (Hockett, 1958, 160-161): 12 Sentence A consists of only two ultimate constituents (morphemes), which are therefore also the ICs of the whole sentence: 3 and 2 are the ICs of 1. Sentence B consists of more than two ultimate constituents, but, once again, of only two immediate constituents: 3 and 2 as in A, are the ICs of 1. Similar remarks apply to sentences C and D. Furthermore, the relationship between the two ICs of each whole sentence is the same. Thus, if we make just one IC-cut in each sentence, ignoring any smaller constituents for the moment, then all four sentences conform to pattern X.\n\nHockett's boxes can be typed by an additional symbol, \"<\" or \">\", \"placed at each junction of ICs, pointing from attribute to head\" (fig. 16).\n\nWe can observe that, in Hockett's diagrams, constituency relations and connection are indissociable and none of them is favored, although the additional symbols (\"<\" or \">\"), similar to Nida's (1966), are clearly connection-based.", "filtered_refids": [[], [null], [], ["b11"], [], ["b14", null], [], ["b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3081, "num_references": 5}
{"corpusid_sectionid": "43985051-s2", "title": "Notion of Semantics in Computer Science A Systematic Literature Review", "date": "2017-12-01", "section_title": "Background, Context & Motivation", "section": "In human computer interaction, there is obviously 1) a human, 2) a computing system and 3) an engagement or interaction between the two. The engagement could either be passive (as in browsing or viewing), or active, as in querying or selecting something on the system. In such scenarios, humans are said to be deriving meaning from the representation presented by the computer. The modality for representation can be text, image, audio, video etc. More interactive representation(al experiences) can be animation, video, user interfaces etc. In the case of interaction (as in inputting or programming by the human), the computing system is also processing data to derive meaning. Apparently, both the human and the system can be seen as two processing agents. 513\n\nThe notion of meaning and semantics can, therefore, be applied to either of the two agents. Our interest, however, is on the human formulating meaning. From an information delivery pointof-view, the idea of how meaning is extracted, constructed or possessed by the human is studied by Psychologists, Cognitive Scientists and Information Processing researchers. On this side, topics like Sense-making (Russell et al., 1993), User Experience, Semantic Interaction (Endert et al., 2012) etc. emerge.\n\nAs a compliment to the human sense making experience, on the computing side, we may also look at how something can be constructed to deliver a particular meaning. Web Accessibility researchers, claim that currently web content is primarily designed for a majority in mind (Prasad et al., 2014). And that it may not suffice for the individualized needs of a minority of users (Prasad, 2017).\n\nA color blind person, for example, may not benefit in the same way as a non-body disabled user. So, in this regard, on the computing system side of the human computer interaction, does there exist a platform that would enable the creation and simultaneous co-existence of multiple representations for the varying needs of a diverse human end users? Is there sufficient motivation for a system that can renarrate and simultaneously have multiple representations of some source text (Prasad, 2017)? That is, a system equally being able to produce colorful content for the majority of users, high contrast and appropriately rendered visuals for the color blind, braille for the visually impaired, in vernacular for the non-English speakers, in tables, diagrams and scientific explanations for the learned etc. These questions form the background context and motivation for our study of semantics in CSE.", "filtered_refids": [[], ["b19", "b46"], ["b43", "b42"], ["b43"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2555, "num_references": 5}
{"corpusid_sectionid": "43985051-s4", "title": "Notion of Semantics in Computer Science A Systematic Literature Review", "date": "2017-12-01", "section_title": "SLR -A Research Tool", "section": "As already stated, our larger goal is to understand how best to represent either information or data on the system so that it may create the right meaning to the human. To that end we wanted to conduct an exploratory Literature Review for such a social applicable, human oriented web application space. SLRs have been popularized as a Evidence Based Software Engineering (EBSE) research tool by Kitchenham et al. in a seminal paper (Kitchenham et al., 2004) presented at ICSE 2004, which is a prominent conference for Software Engineers. In particular SLRs have been suggested as a systematic way of exploring a problem space and thus have been suggested as valuable first step in a PhD research effort (Kitchenham et al., 2004).\n\nWhile SLRs have been popular in the fields of medical sciences, their use in CSE has been limited. However, we are now beginning to find SLRs in various areas of CSE. SLRs are now being published in Information Systems (Okoli and Schabram, 2010) ", "filtered_refids": [[null, "b27"], ["b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 977, "num_references": 3}
{"corpusid_sectionid": "43985051-s14", "title": "Notion of Semantics in Computer Science A Systematic Literature Review", "date": "2017-12-01", "section_title": "RQ3: Human Vs. Computer Semantics", "section": "Through this RQ3 we wanted to uncover a presumption that most of the notion of semantics in CSE was computer oriented and not human oriented. The SLR results confirmed this. We found that 47 out of 50 papers were indeed meant for computers as the processing agent. Only 3 out of the 50 were designed for human as the processing 517  (Boute, 1988) yes -for SDL comp denotational systems S34 (Papaspyrou, 2001) yes -for C comp denotational prog lang S35 (Lobo et al., 1991) yes -Logic comp logic S36 (Broy and Lengauer, 1991) yes -Logic comp predicative, denotational theoretical, logic S37 (Puntigam, 1997) none comp trace prog lang S38 (Jasmin Christian Blanchette, 2008) yes -alternatives presented comp operational prog lang S39 (Thomas Eiter, 2008) yes -for answer sets comp forgetting, stable model theoretical, logic S40 (Ouksel and Sheth, 1999) none comp general Global Info Systems (GIS) S41 (Millard et al., 2005) yes-for hypertext comp general hypertext, logic S42 (Zeng et al., 2006) yes -compatibility comp compatibility prog lang S43 (Wehrman et al., 2008) yes -for ORC comp operational, denotational, timed theoretical; logic S44 (Zeng et al., 2005) yes -compatibility comp compatibility web services S45 (Benthem, 2005) none comp general logic S46 (Kessing et al., 2012) none hum general game S47 (Baroni and Lenci, 2010) none comp spaces, models, similarity distributed memory; database S48 (Abiteboul and Hull, 1987) yes -IFO database model comp general database S49 (da Silva et al., 2012) none comp general workflows; web services S50 (Titov and Klementiev, 2011) yes-bayesian parsing comp general nlp Table 3: Part two, or the remaining listing of 50 sample studies we used in our SLR.\n\nagent. Upon further investigation, these 3 were either using a specialized concept of semantics or were geared towards a social application. For example S15 had to use human understandable terms like Roof, Window, Gate, Shell, Wall etc to link the graphics to urban planning. S13 used a cell component ontology, and S46 focused on real world physics on game word entities.\n\nThis exposed a potential bias for us. It appears that in CSE, most of the ideas related to semantics have indeed been largely designed for computers, and not humans as the processing agent.", "filtered_refids": [["b11", "b12", "b54", "b39", "b44", "b58", "b26", "b4", "b31", "b35", "b50", "b49", "b59", "b23", "b0", "b5", "b37"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2269, "num_references": 17}
{"corpusid_sectionid": "12719479-s2", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Agreement, Reliability, and Validity", "section": "We begin with a quick recap of the goals of agreement studies, inspired by Krippendorff (2004a, Section 11.1). Researchers who wish to use hand-coded data-that is, data in which items are labeled with categories, whether to support an empirical claim or to develop and test a computational model-need to show that such data are reliable.\n\nThe fundamental assumption behind the methodologies discussed in this article is that data are reliable if coders can be shown to agree on the categories assigned to units to an extent determined by the purposes of the study (Krippendorff 2004a;Craggs and McGee Wood 2005). If different coders produce consistently similar results, then we can infer that they have internalized a similar understanding of the annotation guidelines, and we can expect them to perform consistently under this understanding.\n\nReliability is thus a prerequisite for demonstrating the validity of the coding scheme-that is, to show that the coding scheme captures the \"truth\" of the phenomenon being studied, in case this matters: If the annotators are not consistent then either some of them are wrong or else the annotation scheme is inappropriate for the data. (Just as in real life, the fact that witnesses to an event disagree with each other makes it difficult for third parties to know what actually happened.) However, it is important to keep in mind that achieving good agreement cannot ensure validity: Two observers of the same event may well share the same prejudice while still being objectively wrong.", "filtered_refids": [[null], ["b25", "b56"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1532, "num_references": 3}
{"corpusid_sectionid": "12719479-s5", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Agreement Without Chance Correction", "section": "The simplest measure of agreement between two coders is percentage of agreement or observed agreement, defined for example by Scott (1955, page 323) as \"the percentage of judgments on which the two analysts agree when coding the same data independently.\" This is the number of items on which the coders agree divided by the total number of items. More precisely, and looking ahead to the following discussion, observed agreement is the arithmetic mean of the agreement value agr i for all items i \u2208 I, defined as follows:\n\nagr i = 1 if the two coders assign i to the same category 0 if the two coders assign i to different categories Observed agreement over the values agr i for all items i \u2208 I is then:\n\nFor example, let us assume a very simple annotation scheme for dialogue acts in information-seeking dialogues which makes a binary distinction between the categories statement and info-request, as in the DAMSL dialogue act scheme . Two coders classify 100 utterances according to this scheme as shown in Table 1. Percentage agreement for this data set is obtained by summing up the cells on the diagonal and dividing by the total number of items: A o = (20 + 50)/100 = 0.7. Observed agreement enters in the computation of all the measures of agreement we consider, but on its own it does not yield values that can be compared across studies, because some agreement is due to chance, and the amount of chance agreement is affected by two factors that vary from one study to the other. First of all, as Scott (1955, page 322) points out, \"[percentage agreement] is biased in favor of dimensions with a small number of categories.\" In other words, given two coding schemes for the same phenomenon, the one with fewer categories will result in higher percentage agreement just by chance. If two coders randomly classify utterances in a uniform manner using the scheme of Table 1, we would expect an equal number of items to fall in each of the four cells in the table, and therefore pure chance will cause the coders to agree on half of the items (the two cells on the diagonal: 1 4 + 1 4 ). But suppose we want to refine the simple binary coding scheme by introducing a new category, check, as in the MapTask coding scheme (Carletta et al. 1997). If two coders randomly classify utterances in a uniform manner using the three categories in the second scheme, they would only agree on a third of the items ( 1 9 + 1 9 + 1 9 ). The second reason percentage agreement cannot be trusted is that it does not correct for the distribution of items among categories: We expect a higher percentage agreement when one category is much more common than the other. This problem, already raised by Hsu and Field (2003, page 207) among others, can be illustrated using the following example (Di Eugenio and Glass 2004, example 3, pages 98-99). Suppose 95% of utterances in a particular domain are statement, and only 5% are inforequest. We would then expect by chance that 0.95 \u00d7 0.95 = 0.9025 of the utterances would be classified as statement by both coders, and 0.05 \u00d7 0.05 = 0.0025 as inforequest, so the coders would agree on 90.5% of the utterances. Under such circumstances, a seemingly high observed agreement of 90% is actually worse than expected by chance.\n\nThe conclusion reached in the literature is that in order to get figures that are comparable across studies, observed agreement has to be adjusted for chance agreement. These are the measures we will review in the remainder of this article. We will not look at the variants of percentage agreement used in CL work on discourse before the introduction of kappa, such as percentage agreement with an expert and percentage agreement with the majority; see Carletta (1996) for discussion and criticism. 3", "filtered_refids": [[null], [], ["b94", "b18", null], ["b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 3757, "num_references": 5}
{"corpusid_sectionid": "12719479-s6", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Chance-Corrected Coefficients for Measuring Agreement between Two Coders", "section": "All of the coefficients of agreement discussed in this article correct for chance on the basis of the same idea. First we find how much agreement is expected by chance: Let us call this value A e . The value 1 \u2212 A e will then measure how much agreement over and above chance is attainable; the value A o \u2212 A e will tell us how much agreement beyond chance was actually found. The ratio between A o \u2212 A e and 1 \u2212 A e will then tell us which proportion of the possible agreement beyond chance was actually observed. This idea is expressed by the following formula.\n\nThe three best-known coefficients, S (Bennett, Alpert, and Goldstein 1954), \u03c0 (Scott 1955), and \u03ba (Cohen 1960), and their generalizations, all use this formula; whereas Krippendorff's \u03b1 is based on a related formula expressed in terms of disagreement (see Section 2.6). All three coefficients therefore yield values of agreement between \u2212A e /1 \u2212 A e (no observed agreement) and 1 (observed agreement = 1), with the value 0 signifying chance agreement (observed agreement = expected agreement). Note also that whenever agreement is less than perfect (A o < 1), chance-corrected agreement will be strictly lower than observed agreement, because some amount of agreement is always expected by chance. Observed agreement A o is easy to compute, and is the same for all three coefficients-the proportion of items on which the two coders agree. But the notion of chance agreement, or the probability that two coders will classify an arbitrary item as belonging to the same category by chance, requires a model of what would happen if coders' behavior was only by chance. All three coefficients assume independence of the two coders-that is, that the chance of c 1 and c 2 agreeing on any given category k Table 2 The value of different coefficients applied to the data from Table 1.", "filtered_refids": [[], ["b21", "b94", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1841, "num_references": 3}
{"corpusid_sectionid": "12719479-s11", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "All Categories Are Equally", "section": "Likely: S. The simplest way of discounting for chance is the one adopted to compute the coefficient S (Bennett, Alpert, and Goldstein 1954), also known in the literature as C, \u03ba n , G, and RE (see Zwick 1988;Hsu and Field 2003). As noted previously, the computation of S is based on an interpretation of chance as a random choice of category from a uniform distribution-that is, all categories are equally likely. If coders classify the items into k categories, then the chance P(k|c i ) of any coder assigning an item to category k under the uniformity assumption is 1 k ; hence the total agreement expected by chance is\n\nThe calculation of the value of S for the figures in Table 1 is shown in Table 2. The coefficient S is problematic in many respects. The value of the coefficient can be artificially increased simply by adding spurious categories which the coders would never use (Scott 1955, pages 322-323). In the case of CL, for example, S would reward designing extremely fine-grained tagsets, provided that most tags are never actually encountered in real data. Additional limitations are noted by Hsu and Field (2003). It has been argued that uniformity is the best model for a chance distribution of items among categories if we have no independent prior knowledge of the distribution (Brennan and Prediger 1981). However, a lack of prior knowledge does not mean that the distribution cannot be estimated post hoc, and this is what the other coefficients do.", "filtered_refids": [["b45", "b108", "b7"], ["b45", null, "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1470, "num_references": 6}
{"corpusid_sectionid": "12719479-s14", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "More Than Two Coders", "section": "In corpus annotation practice, measuring reliability with only two coders is seldom considered enough, except for small-scale studies. Sometimes researchers run reliability studies with more than two coders, measure agreement separately for each pair of coders, and report the average. However, a better practice is to use generalized versions of the coefficients. A generalization of Scott's \u03c0 is proposed in Fleiss (1971), and a generalization of Cohen's \u03ba is given in Davies and Fleiss (1982). We will call these coefficients multi-\u03c0 and multi-\u03ba, respectively, dropping the multi-prefixes when no confusion is expected to arise. 5 2.5.1 Fleiss's Multi-\u03c0. With more than two coders, the observed agreement A o can no longer be defined as the percentage of items on which there is agreement, because inevitably there will be items on which some coders agree and others disagree. The solution proposed in the literature is to measure pairwise agreement (Fleiss 1971): Define the amount of agreement on a particular item as the proportion of agreeing judgment pairs out of the total number of judgment pairs for that item.\n\nMultiple coders also pose a problem for the visualization of the data. When the number of coders c is greater than two, judgments cannot be shown in a contingency table like Table 1, because each coder has to be represented in a separate dimension. 5 Due to historical accident, the terminology in the literature is confusing. Fleiss (1971) proposed a coefficient of agreement for multiple coders and called it \u03ba, even though it calculates expected agreement based on the cumulative distribution of judgments by all coders and is thus better thought of as a generalization of Scott's \u03c0. This unfortunate choice of name was the cause of much confusion in subsequent literature: Often, studies which claim to give a generalization of \u03ba to more than two coders actually report Fleiss's coefficient (e.g., Bartko and Carpenter 1976;Siegel and Castellan 1988;Di Eugenio and Glass 2004). Since Carletta (1996) introduced reliability to the CL community based on the definitions of Siegel and Castellan (1988), the term \"kappa\" has been usually associated in this community with Siegel and Castellan's K, which is in effect Fleiss's coefficient, that is, a generalization of Scott's \u03c0. Fleiss (1971) Table 3 lose information because they do not say which coder gave each judgment. This information is not used in the calculation of \u03c0, but is necessary for determining the individual coders' distributions in the calculation of \u03ba. (Agreement tables also add information compared to contingency tables, namely, the identity of the items that make up each contingency class, but this information is not used in the calculation of either \u03ba or \u03c0.) Let n ik stand for the number of times an item i is classified in category k (i.e., the number of coders that make such a judgment): For example, given the distribution in Table 3, n Utt 1 Stat = 2 and n Utt 1 IReq = 1. Each category k contributes ( n ik 2 ) pairs of agreeing judgments for item i; the amount of agreement agr i for item i is the sum of ( n ik 2 ) over all categories k \u2208 K, divided by ( c 2 ), the total number of judgment pairs per item.\n\nFor example, given the results in Table 3, we find the agreement value for Utterance 1 as follows.\n\nThe overall observed agreement is the mean of agr i for all items i \u2208 I.\n\n(Notice that this definition of observed agreement is equivalent to the mean of the two-coder observed agreement values from Section 2.4 for all coder pairs.) If observed agreement is measured on the basis of pairwise agreement (the proportion of agreeing judgment pairs), it makes sense to measure expected agreement in terms of pairwise comparisons as well, that is, as the probability that any pair of judgments for an item would be in agreement-or, said otherwise, the probability that two arbitrary coders would make the same judgment for a particular item by chance. This is the approach taken by Fleiss (1971). Like Scott, Fleiss interprets \"chance agreement\" as the agreement expected on the basis of a single distribution which reflects the combined judgments of all coders, meaning that expected agreement is calculated usingP(k), the overall proportion of items assigned to category k, which is the total number of such assignments by all coders n k divided by the overall number of assignments. The latter, in turn, is the number of items i multiplied by the number of coders c.\n\nAs in the two-coder case, the probability that two arbitrary coders assign an item to a particular category k \u2208 K is assumed to be the joint probability of each coder making this assignment independently, that is (P(k)) 2 . The expected agreement is the sum of this joint probability over all the categories k \u2208 K.\n\nMulti-\u03c0 is the coefficient that Siegel and Castellan (1988) call K.", "filtered_refids": [["b26", "b36"], ["b97", "b36", "b28", "b5", "b17"], [], [], ["b36"], [], ["b97"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4866, "num_references": 9}
{"corpusid_sectionid": "12719479-s16", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Krippendorff's \u03b1 and Other Weighted Agreement Coefficients", "section": "A serious limitation of both \u03c0 and \u03ba is that all disagreements are treated equally. But especially for semantic and pragmatic features, disagreements are not all alike. Even for the relatively simple case of dialogue act tagging, a disagreement between an accept and a reject interpretation of an utterance is clearly more serious than a disagreement between an info-request and a check. For tasks such as anaphora resolution, where reliability is determined by measuring agreement on sets (coreference chains), allowing for degrees of disagreement becomes essential (see Section 4.4). Under such circumstances, \u03c0 and \u03ba are not very useful. In this section we discuss two coefficients that make it possible to differentiate between types of disagreements: \u03b1 (Krippendorff 1980(Krippendorff , 2004a, which is a coefficient defined in a general way that is appropriate for use with multiple coders, different magnitudes of disagreement, and missing values, and is based on assumptions similar to those of \u03c0; and weighted kappa \u03ba w (Cohen 1968), a generalization of \u03ba.\n\n2.6.1 Krippendorff's \u03b1. The coefficient \u03b1 (Krippendorff 1980(Krippendorff , 2004a is an extremely versatile agreement coefficient based on assumptions similar to \u03c0, namely, that expected agreement is calculated by looking at the overall distribution of judgments without regard to which coders produced these judgments. It applies to multiple coders, and it allows for different magnitudes of disagreement. When all disagreements are considered equal it is nearly identical to multi-\u03c0, correcting for small sample sizes by using an unbiased estimator for expected agreement. In this section we will present Krippendorff's \u03b1 and relate it to the other coefficients discussed in this article, but we will start with \u03b1's origins as a measure of variance, following a long tradition of using variance to measure reliability (see citations in Rajaratnam 1960;Krippendorff 1970).\n\nA sample's variance s 2 is defined as the sum of square differences from the mean SS = \u2211(x \u2212x) 2 divided by the degrees of freedom df . Variance is a useful way of looking at agreement if coders assign numerical values to the items, as in magnitude estimation tasks. Each item in a reliability study can be considered a separate level in a single-factor analysis of variance: The smaller the variance around each level, the higher the reliability. When agreement is perfect, the variance within the levels (s 2 within ) is zero; when agreement is at chance, the variance within the levels is equal to the variance between the levels, in which case it is also equal to the overall variance of the data: s 2 within = s 2 between = s 2 total . The ratios s 2 within /s 2 between (that is, 1/F) and s 2 within /s 2 total are therefore 0 when agreement is perfect and 1 when agreement is at chance. Additionally, the latter ratio is bounded at 2: SS within \u2264 SS total by definition, and df total < 2 df within because each item has at least two judgments. Subtracting the ratio s 2 within /s 2 total from 1 yields a coefficient which ranges between \u22121 and 1, where 1 signifies perfect agreement and 0 signifies chance agreement.\n\nWe can unpack the formula for \u03b1 to bring it to a form which is similar to the other coefficients we have looked at, and which will allow generalizing \u03b1 beyond simple numerical values. The first step is to get rid of the notion of arithmetic mean which lies at the heart of the measure of variance. We observe that for any set of numbers x 1 , . . . , x N with a meanx = 1 N \u2211 N n=1 x n , the sum of square differences from the mean SS can be expressed as the sum of square of differences between all the (ordered) pairs of numbers, scaled by a factor of 1/2N.\n\nFor calculating \u03b1 we considered each item to be a separate level in an analysis of variance; the number of levels is thus the number of items i, and because each coder marks each item, the number of observations for each item is the number of coders c.\n\nWithin-level variance is the sum of the square differences from the mean of each item,\n\n. We can express this as the sum of the squares of the differences between all of the judgment pairs for each item, summed over all items and scaled by the appropriate factor. We use the notation x ic for the value given by coder c to item i, andx i for the mean of all the values given to item i.\n\nThe total variance is the sum of the square differences of all judgments from the grand mean, SS total = \u2211 i \u2211 c (x ic \u2212x) 2 , divided by the degrees of freedom df total = ic \u2212 1. This can be expressed as the sum of the squares of the differences between all of the judgments pairs without regard to items, again scaled by the appropriate factor. The notation x is the overall mean of all the judgments in the data.\n\nNow that we have removed references to means from our formulas, we can abstract over the measure of variance. We define a distance function d which takes two numbers and returns the square of their difference.\n\nWe also simplify the computation by counting all the identical value assignments together. Each unique value used by the coders will be considered a category k \u2208 K. We use n ik for the number of times item i is given the value k, that is, the number of coders that make such a judgment. For every (ordered) pair of distinct values k a , k b \u2208 K there are n ik a n ik b pairs of judgments of item i, whereas for non-distinct values there are n ik a (n ik a \u2212 1) pairs. We use this notation to rewrite the formula for the within-level variance. D \u03b1 o , the observed disagreement for \u03b1, is defined as twice the variance within the levels in order to get rid of the factor 2 in the denominator; we also simplify the formula by using the multiplier n ik a n ik a for identical categories-this is allowed because\n\nWe perform the same simplification for the total variance, where n k stands for the total number of times the value k is assigned to any item by any coder. The expected disagreement for \u03b1, D \u03b1 e , is twice the total variance.\n\nBecause both expected and observed disagreement are twice the respective variances, the coefficient \u03b1 retains the same form when expressed with the disagreement values.\n\nNow that \u03b1 has been expressed without explicit reference to means, differences, and squares, it can be generalized to a variety of coding schemes in which the labels cannot be interpreted as numerical values: All one has to do is to replace the square difference function d with a different distance function. Krippendorff (1980Krippendorff ( , 2004a offers distance metrics suitable for nominal, interval, ordinal, and ratio scales. Of particular interest is the function for nominal categories, that is, a function which considers all distinct labels equally distant from one another.\n\nIt turns out that with this distance function, the observed disagreement D \u03b1 o is exactly the complement of the observed agreement of Fleiss's multi-\u03c0, 1 \u2212 A \u03c0 o , and the expected disagreement D \u03b1 e differs from 1 \u2212 A \u03c0 e by a factor of (ic \u2212 1)/ic; the difference is due to the fact that \u03c0 uses a biased estimator of the expected agreement in the population whereas \u03b1 uses an unbiased estimator. The following equation shows that given the correspondence between observed and expected agreement and disagreement, the coefficients themselves are nearly equivalent.\n\nFor nominal data, the coefficients \u03c0 and \u03b1 approach each other as either the number of items or the number of coders approaches infinity. Krippendorff's \u03b1 will work with any distance metric, provided that identical categories always have a distance of zero (d kk = 0 for all k). Another useful constraint is symmetry (d ab = d ba for all a, b). This flexibility affords new possibilities for analysis, which we will illustrate in Section 4. We should also note, however, that the flexibility also creates new pitfalls, especially in cases where it is not clear what the natural distance metric is. For example, there are different ways to measure dissimilarity between sets, and any of these measures can be justifiably used when the category labels are sets of items (as in the annotation of anaphoric relations). The different distance metrics yield different values of \u03b1 for the same annotation data, making it difficult to interpret the resulting values. We will return to this problem in Section 4.4. Cohen (1968). The implementation of weights is similar to that of Krippendorff's \u03b1-each pair of categories k a , k b \u2208 K is associated with a weight d k a k b , where a larger weight indicates more disagreement (Cohen uses the notation v; he does not place any general constraints on the weights-not even a requirement that a pair of identical categories have a weight of zero, or that the weights be symmetric across the diagonal). The coefficient is defined for two coders: The disagreement for a particular item i is the weight of the pair of categories assigned to it by the two coders, and the overall observed disagreement is the (normalized) mean disagreement of all the items. Let k(c n , i) denote the category assigned by coder c n to item i; then the disagreement for item i is disagr i = d k(c 1 ,i)k(c 2 ,i) . The observed disagreement D o is the mean of disagr i for all items i, normalized to the interval [0, 1] through division by the maximal weight d max .", "filtered_refids": [["b54", "b22", "b56"], ["b52", "b54", "b87", "b56"], [], [], [], [], [], [], [], [], [], [], ["b54", "b56"], [], ["b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 51, "num_chars": 9337, "num_references": 10}
{"corpusid_sectionid": "12719479-s23", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Annotator Bias", "section": "The difference between \u03c0 and \u03b1 on the one hand and \u03ba on the other hand lies in the interpretation of the notion of chance agreement, whether it is the amount expected from the the actual distribution of items among categories (\u03c0) or from individual coder priors (\u03ba). As mentioned in Section 2.4, this difference has been the subject of much debate (Fleiss 1975;Krippendorff 1978Krippendorff , 2004bByrt, Bishop, and Carlin 1993;Zwick 1988;Hsu and Field 2003;Di Eugenio and Glass 2004;Craggs and McGee Wood 2005). A claim often repeated in the literature is that single-distribution coefficients like \u03c0 and \u03b1 assume that different coders produce similar distributions of items among categories, with the implication that these coefficients are inapplicable when the annotators show substantially different distributions. Recommendations vary: Zwick (1988) suggests testing the individual coders' distributions using the modified \u03c7 2 test of Stuart (1955), and discarding the annotation as unreliable if significant systematic discrepancies are observed. In contrast, Hsu and Field (2003, page 214) recommend reporting the value of \u03ba even when the coders produce different distributions, because it is \"the only [index] . . . that could legitimately be applied in the presence of marginal heterogeneity\"; likewise, Di Eugenio and Glass (2004, page 96) recommend using \u03ba in \"the vast majority . . . of discourse-and dialogue-tagging efforts\" where the individual coders' distributions tend to vary. All of these proposals are based on a misconception: that single-distribution coefficients require similar distributions by the individual annotators in order to work properly. This is not the case. The difference between the coefficients is only in the interpretation of \"chance agreement\": \u03c0-style coefficients calculate the chance of agreement among arbitrary coders, whereas \u03ba-style coefficients calculate the chance of agreement among the coders who produced the reliability data. Therefore, the choice of coefficient should not depend on the magnitude of the divergence between the coders, but rather on the desired interpretation of chance agreement.\n\nAnother common claim is that individual-distribution coefficients like \u03ba \"reward\" annotators for disagreeing on the marginal distributions. For example, Di Eugenio and Glass (2004, page 99) say that \u03ba suffers from what they call the bias problem, described as \"the paradox that \u03ba Co [our \u03ba] increases as the coders become less similar.\" Similar reservations about the use of \u03ba have been noted by Brennan and Prediger (1981) and Zwick (1988). However, the bias problem is less paradoxical than it sounds. Although it is true that for a fixed observed agreement, a higher difference in coder marginals implies a lower expected agreement and therefore a higher \u03ba value, the conclusion that \u03ba penalizes coders for having similar distributions is unwarranted. This is because A o and A e are not independent: Both are drawn from the same set of observations. What \u03ba does is discount some of the disagreement resulting from different coder marginals by incorporating it into A e . Whether this is desirable depends on the application for which the coefficient is used.\n\nThe most common application of agreement measures in CL is to infer the reliability of a large-scale annotation, where typically each piece of data will be marked by just one coder, by measuring agreement on a small subset of the data which is annotated by multiple coders. In order to make this generalization, the measure must reflect the reliability of the annotation procedure, which is independent of the actual annotators used. Reliability, or reproducibility of the coding, is reduced by all disagreements-both random and systematic. The most appropriate measures of reliability for this purpose are therefore single-distribution coefficients like \u03c0 and \u03b1, which generalize over the individual coders and exclude marginal disagreements from the expected agreement. This argument has been presented recently in much detail by Krippendorff (2004b) and reiterated by Craggs and McGee Wood (2005).\n\nAt the same time, individual-distribution coefficients like \u03ba provide important information regarding the trustworthiness (validity) of the data on which the annotators agree. As an intuitive example, think of a person who consults two analysts when deciding whether to buy or sell certain stocks. If one analyst is an optimist and tends to recommend buying whereas the other is a pessimist and tends to recommend selling, they are likely to agree with each other less than two more neutral analysts, so overall their recommendations are likely to be less reliable-less reproducible-than those that come from a population of like-minded analysts. This reproducibility is measured by \u03c0. But whenever the optimistic and pessimistic analysts agree on a recommendation for a particular stock, whether it is \"buy\" or \"sell,\" the confidence that this is indeed the right decision is higher than the same advice from two like-minded analysts. This is why \u03ba \"rewards\" biased annotators: it is not a matter of reproducibility (reliability) but rather of trustworthiness (validity).\n\nHaving said this, we should point out that, first, in practice the difference between \u03c0 and \u03ba doesn't often amount to much (see discussion in Section 4). Moreover, the difference becomes smaller as agreement increases, because all the points of agreement contribute toward making the coder marginals similar (it took a lot of experimentation to create data for Table 4 so that the values of \u03c0 and \u03ba would straddle the conventional cutoff point of 0.80, and even so the difference is very small). Finally, one would expect the difference between \u03c0 and \u03ba to diminish as the number of coders grows; this is shown subsequently. 6 We define B, the overall annotator bias in a particular set of coding data, as the difference between the expected agreement according to (multi)-\u03c0 and the expected agreement according to (multi)-\u03ba. Annotator bias is a measure of variance: If we take c to be a random variable with equal probabilities for all coders, then the annotator bias B is the sum of the variances of P(k|c) for all categories k \u2208 K, divided by the number of coders c less one (see Artstein and Poesio [2005] for a proof).\n\nAnnotator bias can be used to express the difference between \u03ba and \u03c0.\n\nThis allows us to make the following observations about the relationship between \u03c0 and \u03ba.\n\nObservation 1. The difference between \u03ba and \u03c0 grows as the annotator bias grows: For a constant A o and A \u03c0 e , a greater B implies a greater value for \u03ba \u2212 \u03c0.\n\nObservation 2. The greater the number of coders, the lower the annotator bias B, and hence the lower the difference between \u03ba and \u03c0, because the variance ofP(k|c) does not increase in proportion to the number of coders.\n\nIn other words, provided enough coders are used, it should not matter whether a single-distribution or individual-distribution coefficient is used. This is not to imply that multiple coders increase reliability: The variance of the individual coders' distributions can be just as large with many coders as with few coders, but its effect on the value of \u03ba decreases as the number of coders grows, and becomes more similar to random noise. The same holds for weighted measures too; see the extended version of this article for definitions and proof. In an annotation study with 18 subjects, we compared \u03b1 with a variant which uses individual coder distributions to calculate expected agreement, and found that the values never differed beyond the third decimal point (Poesio and Artstein 2005).\n\nWe conclude with a summary of our views concerning the difference between \u03c0style and \u03ba-style coefficients. First of all, keep in mind that empirically the difference is small, and gets smaller as the number of annotators increases. Then instead of reporting two coefficients, as suggested by Di Eugenio and Glass (2004), the appropriate coefficient should be chosen based on the task (not on the observed differences between coder marginals). When the coefficient is used to assess reliability, a single-distribution coefficient like \u03c0 or \u03b1 should be used; this is indeed already the practice in CL, because Siegel and Castellan's K is identical with (multi-)\u03c0. It is also good practice to test reliability with more than two coders, in order to reduce the likelihood of coders sharing a deviant reading of the annotation guidelines.", "filtered_refids": [["b57", "b53", "b28", "b25", "b101", "b45", null, "b108", "b16", "b37"], ["b108", null, "b9"], ["b57", "b25"], [], [null, "b1"], [], [], [], [], ["b1"], ["b28"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 44, "num_chars": 8490, "num_references": 19}
{"corpusid_sectionid": "12719479-s27", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Generating Data to Measure Reproducibility.", "section": "Krippendorff's recommendations were developed for the field of content analysis, where coding is used to draw conclusions from the texts. A coded corpus is thus akin to the result of a scientific experiment, and it can only be considered valid if it is reproducible-that is, if the same coded results can be replicated in an independent coding exercise. Krippendorff therefore argues that any study using observed agreement as a measure of reproducibility must satisfy the following requirements:\n\n\u2022 It must employ an exhaustively formulated, clear, and usable coding scheme together with step-by-step instructions on how to use it.\n\n\u2022 It must use clearly specified criteria concerning the choice of coders (so that others may use such criteria to reproduce the data).\n\n\u2022 It must ensure that the coders that generate the data used to measure reproducibility work independently of each other.\n\nSome practices that are common in CL do not satisfy these requirements. The first requirement is violated by the practice of expanding the written coding instructions and including new rules as the data are generated. The second requirement is often violated by using experts as coders, particularly long-term collaborators, as such coders may agree not because they are carefully following written instructions, but because they know the purpose of the research very well-which makes it virtually impossible for others to reproduce the results on the basis of the same coding scheme (the problems arising when using experts were already discussed at length in Carletta [1996]). Practices which violate the third requirement (independence) include asking coders to discuss their judgments with each other and reach their decisions by majority vote, or to consult with each other when problems not foreseen in the coding instructions arise. Any of these practices make the resulting data unusable for measuring reproducibility.\n\nKrippendorff's own summary of his recommendations is that to obtain usable data for measuring reproducibility a researcher must use data generated by three or more coders, chosen according to some clearly specified criteria, and working independently according to a written coding scheme and coding instructions fixed in advance. Krippendorff also discusses the criteria to be used in the selection of the sample, from the minimum number of units (obtained using a formula from Bloch and Kraemer [1989], reported in Krippendorff [2004a, page 239] ", "filtered_refids": [[], [], [], [], ["b17"], [null, "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2468, "num_references": 3}
{"corpusid_sectionid": "12719479-s28", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Establishing Significance.", "section": "In hypothesis testing, it is common to test for the significance of a result against a null hypothesis of chance behavior; for an agreement coefficient this would mean rejecting the possibility that a positive value of agreement is nevertheless due to random coding. We can rely on the statement by Siegel and Castellan (1988, Section 9.8.2) that when sample sizes are large, the sampling distribution of K (Fleiss's multi-\u03c0) is approximately normal and centered around zero-this allows testing the obtained value of K against the null hypothesis of chance agreement by using the z statistic. It is also easy to test Krippendorff's \u03b1 with the interval distance metric against the null hypothesis of chance agreement, because the hypothesis \u03b1 = 0 is identical to the hypothesis F = 1 in an analysis of variance.\n\nHowever, a null hypothesis of chance agreement is not very interesting, and demonstrating that agreement is significantly better than chance is not enough to establish reliability. This has already been pointed out by Cohen (1960, page 44): \"to know merely that \u03ba is beyond chance is trivial since one usually expects much more than this in the way of reliability in psychological measurement.\" The same point has been repeated and stressed in many subsequent works (e.g., Posner et al. 1990;Di Eugenio 2000;Krippendorff 2004a): The reason for measuring reliability is not to test whether coders perform better than chance, but to ensure that the coders do not deviate too much from perfect agreement (Krippendorff 2004a, page 237).\n\nThe relevant notion of significance for agreement coefficients is therefore a confidence interval. Cohen (1960, pages 43-44) implies that when sample sizes are large, the sampling distribution of \u03ba is approximately normal for any true population value of \u03ba, and therefore confidence intervals for the observed value of \u03ba can be determined using the usual multiples of the standard error. Donner and Eliasziw (1987) propose a more general form of significance test for arbitrary levels of agreement. In contrast, Krippendorff (2004a, Section 11.4.2) states that the distribution of \u03b1 is unknown, so confidence intervals must be obtained by bootstrapping; a software package for doing this is described in Hayes and Krippendorff (2007).", "filtered_refids": [[null], [null, "b56", "b27"], ["b42", null, "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2280, "num_references": 7}
{"corpusid_sectionid": "12719479-s29", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Interpreting the Value of Kappa-Like Coefficients.", "section": "Even after testing significance and establishing confidence intervals for agreement coefficients, we are still faced with the problem of interpreting the meaning of the resulting values. Suppose, for example, we establish that for a particular task, K = 0.78 \u00b1 0.05. Is this good or bad? Unfortunately, deciding what counts as an adequate level of agreement for a specific purpose is still little more than a black art: As we will see, different levels of agreement may be appropriate for resource building and for more linguistic purposes.\n\nThe problem is not unlike that of interpreting the values of correlation coefficients, and in the area of medical diagnosis, the best known conventions concerning the value of kappa-like coefficients, those proposed by Landis and Koch (1977) and reported in Figure 1, are indeed similar to those used for correlation coefficients, where values above 0.4 are also generally considered adequate (Marion 2004). Many medical researchers feel that these conventions are appropriate, and in language studies, a similar interpretation of the values has been proposed by Rietveld and van Hout (1993). In CL, however, most researchers follow the more stringent conventions from content analysis proposed by Krippendorff (1980, page 147), as reported by Carletta (1996, page 252): \"content analysis researchers generally think of K > .8 as good reliability, with .67 < K < .8 allowing tentative conclusions to be drawn\" (Krippendorff was discussing values of \u03b1 rather than K, but the coefficients are nearly equivalent for categorical labels). As a result, ever since Carletta's influential paper, CL researchers have attempted to achieve a value of K (more seldom, of \u03b1) above the 0.8 threshold, or, failing that, the 0.67 level allowing for \"tentative conclusions.\" However, the description of the 0.67 boundary in Krippendorff (1980) was actually \"highly tentative and cautious,\" and in later work Krippendorff clearly considers 0.8 the absolute minimum value of \u03b1 to accept for any serious purpose: \"Even a cutoff point of \u03b1 = .800 . . . is a pretty low standard\" (Krippendorff 2004a, page 242). Recent content analysis practice seems to have settled for even more stringent requirements: A recent textbook, Neuendorf (2002, page 3), analyzing several proposals concerning \"acceptable\" reliability, concludes that \"reliability coefficients of .90 or greater would be acceptable to all, .80 or greater would be acceptable in most situations, and below that, there exists great disagreement.\" This is clearly a fundamental issue. Ideally we would want to establish thresholds which are appropriate for the field of CL, but as we will see in the rest of this section, a decade of practical experience hasn't helped in settling the matter. In fact, weighted coefficients, while arguably more appropriate for many annotation tasks, make the issue of deciding when the value of a coefficient indicates sufficient agreement even Kappa values and strength of agreement according to Landis and Koch (1977). more complicated because of the problem of determining appropriate weights (see Section 4.4). We will return to the issue of interpreting the value of the coefficients at the end of this article.", "filtered_refids": [[], ["b54", "b92", "b58", null, "b64"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3229, "num_references": 5}
{"corpusid_sectionid": "12719479-s31", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Labeling Units with a Common and Predefined Set of Categories: The Case of Dialogue Act Tagging", "section": "The simplest and most common coding in CL involves labeling segments of text with a limited number of linguistic categories: Examples include part-of-speech tagging, dialogue act tagging, and named entity tagging. The practices used to test reliability for this type of annotation tend to be based on the assumption that the categories used in the annotation are mutually exclusive and equally distinct from one another; this assumption seems to have worked out well in practice, but questions about it have been raised even for the annotation of parts of speech (Babarczy, Carroll, and Sampson 2006), let alone for discourse coding tasks such as dialogue act coding. We concentrate here on this latter type of coding, but a discussion of issues raised for POS, named entity, and prosodic coding can be found in the extended version of the article. Dialogue act tagging is a type of linguistic annotation with which by now the CL community has had extensive experience: Several dialogue-act-annotated spoken language corpora now exist, such as MapTask (Carletta et al. 1997), Switchboard (Stolcke et al. 2000), Verbmobil (Jekat et al. 1995), and Communicator (e.g., Doran et al. 2001), among others. Historically, dialogue act annotation was also one of the types of annotation that motivated the introduction in CL of chance-corrected coefficients of agreement (Carletta et al. 1997) and, as we will see, it has been the type of annotation that has generated the most discussion concerning annotation methodology and measuring agreement.\n\nA number of coding schemes for dialogue acts have achieved values of K over 0.8 and have therefore been assumed to be reliable: For example, K = 0.83 for the 13-tag MapTask coding scheme (Carletta et al. 1997), K = 0.8 for the 42-tag Switchboard-DAMSL scheme (Stolcke et al. 2000), K = 0.90 for the smaller 20-tag subset of the CSTAR scheme used by Doran et al. (2001). All of these tests were based on the same two assumptions: that every unit (utterance) is assigned to exactly one category (dialogue act), and that these categories are distinct. Therefore, again, unweighted measures, and in particular K, tend to be used for measuring inter-coder agreement.\n\nHowever, these assumptions have been challenged based on the observation that utterances tend to have more than one function at the dialogue act level (Traum and Hinkelman 1992;Allen and Core 1997;Bunt 2000); for a useful survey, see Popescu-Belis (2005). An assertion performed in answer to a question, for instance, typically performs at least two functions at different levels: asserting some information-the dialogue act that we called Statement in Section 2.3, operating at what Traum and Hinkelman called the \"core speech act\" level-and confirming that the question has been understood, a dialogue act operating at the \"grounding\" level and usually known as Acknowledgment (Ack). In older dialogue act tagsets, acknowledgments and statements were treated as alternative labels at the same \"level\", forcing coders to choose one or the other when an utterance performed a dual function, according to a well-specified set of instructions. By contrast, in the annotation schemes inspired from these newer theories such as DAMSL , coders are allowed to assign tags along distinct \"dimensions\" or \"levels\".\n\nTwo annotation experiments testing this solution to the \"multi-tag\" problem with the DAMSL scheme were reported in Core and Allen (1997) and Di Eugenio et al. (1998). In both studies, coders were allowed to mark each communicative function independently: That is, they were allowed to choose for each utterance one of the Statement tags (or possibly none), one of the Influencing-Addressee-Future-Action tags, and so forth-and agreement was evaluated separately for each dimension using (unweighted) K. Core and Allen found values of K ranging from 0.76 for answer to 0.42 for agreement to 0.15 for Committing-Speaker-Future-Action. Using different coding instructions and on a different corpus, Di Eugenio et al. observed higher agreement, ranging from K = 0.93 (for other-forward-function) to 0.54 (for the tag agreement).\n\nThese relatively low levels of agreement led many researchers to return to \"flat\" tagsets for dialogue acts, incorporating however in their schemes some of the insights motivating the work on schemes such as DAMSL. The best known example of this type of approach is the development of the SWITCHBOARD-DAMSL tagset by Jurafsky, Shriberg, and Biasca (1997), which incorporates many ideas from the \"multi-dimensional\" theories of dialogue acts, but does not allow marking an utterance as both an acknowledgment and a statement; a choice has to be made. This tagset results in overall agreement of K = 0.80. Interestingly, subsequent developments of SWITCHBOARD-DAMSL backtracked on some of these decisions. For instance, the ICSI-MRDA tagset developed for the annotation of the ICSI Meeting Recorder corpus reintroduces some of the DAMSL ideas, in that annotators are allowed to assign multiple SWITCHBOARD-DAMSL labels to utterances (Shriberg et al. 2004). Shriberg et al. achieved a comparable reliability to that obtained with SWITCHBOARD-DAMSL, but only when using a tagset of just five \"class-maps\". Shriberg et al. (2004) also introduced a hierarchical organization of tags to improve reliability. The dimensions of the DAMSL scheme can be viewed as \"superclasses\" of dialogue acts which share some aspect of their meaning. For instance, the dimension of Influencing-Addressee-Future-Action (IAFA) includes the two dialogue acts Open-option (used to mark suggestions) and Directive, both of which bring into consideration a future action to be performed by the addressee. At least in principle, an organization of this type opens up the possibility for coders to mark an utterance with the superclass (IAFA) in case they do not feel confident that the utterance satisfies the additional requirements for Open-option or Directive. This, in turn, would do away with the need to make a choice between these two options. This possibility wasn't pursued in the studies using the original DAMSL that we are aware of Di Eugenio 2000;Stent 2001), but was tested by Shriberg et al. (2004) and subsequent work, in particular Geertzen and Bunt (2006), who were specifically interested in the idea of using hierarchical schemes to measure partial agreement, and in addition experimented with weighted coefficients of agreement for their hierarchical tagging scheme, specifically \u03ba w .\n\nGeertzen and Bunt tested intercoder agreement with Bunt's DIT++ (Bunt 2005), a scheme with 11 dimensions that builds on ideas from DAMSL and from Dynamic Interpretation Theory (Bunt 2000). In DIT++, tags can be hierarchically related: For example, the class information-seeking is viewed as consisting of two classes, yesno question (ynq) and wh-question (whq). The hierarchy is explicitly introduced in order to allow coders to leave some aspects of the coding undecided. For example, check is treated as a subclass of ynq in which, in addition, the speaker has a weak belief that the proposition that forms the belief is true. A coder who is not certain about the dialogue act performed using an utterance may simply choose to tag it as ynq.\n\nThe distance metric d proposed by Geertzen and Bunt is based on the criterion that two communicative functions are related (d(c 1 , c 2 ) < 1) if they stand in an ancestor-offspring relation within a hierarchy. Furthermore, they argue, the magnitude of d(c 1 , c 2 ) should be proportional to the distance between the functions in the hierarchy. A level-dependent correction factor is also proposed so as to leave open the option to make disagreements at higher levels of the hierarchy matter more than disagreements at the deeper level (for example, the distance between information-seeking and ynq might be considered greater than the distance between check and positive-check).\n\nThe results of an agreement test with two annotators run by Geertzen and Bunt show that taking into account partial agreement leads to values of \u03ba w that are higher than the values of \u03ba for the same categories, particularly for feedback, a class for which Core and Allen (1997) got low agreement. Of course, even assuming that the values of \u03ba w and \u03ba were directly comparable-we remark on the difficulty of interpreting the values of weighted coefficients of agreement in Section 4.4-it remains to be seen whether these higher values are a better indication of the extent of agreement between coders than the values of unweighted \u03ba.\n\nThis discussion of coding schemes for dialogue acts introduced issues to which we will return for other CL annotation tasks as well. There are a number of wellestablished schemes for large-scale dialogue act annotation based on the assumption of mutual exclusivity between dialogue act tags, whose reliability is also well known; if one of these schemes is appropriate for modeling the communicative intentions found in a task, we recommend to our readers to use it. They should also realize, however, that the mutual exclusivity assumption is somewhat dubious. If a multi-dimensional or hierarchical tagset is used, readers should also be aware that weighted coefficients do capture partial agreement, and need not automatically result in lower reliability or in an explosion in the number of labels. However, a hierarchical scheme may not reflect genuine annotation difficulties: For example, in the case of DIT++, one might argue that it is more difficult to confuse yes-no questions with wh-questions than with statements. We will also see in a moment that interpreting the results with weighted coefficients is difficult. We will return to both of these problems in what follows.", "filtered_refids": [["b100", "b18", "b4", "b47", "b32"], ["b100", "b18", "b32"], ["b13", "b104", "b0", "b84"], ["b23", "b29"], ["b39", "b98", "b27", "b48", "b95"], ["b14", "b13"], [], ["b23"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 45, "num_chars": 9759, "num_references": 22}
{"corpusid_sectionid": "12719479-s32", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Marking Boundaries and Unitizing", "section": "Before labeling can take place, the units of annotation, or markables, need to be identified-a process Krippendorff (1995Krippendorff ( , 2004a calls unitizing. The practice in CL for the forms of annotation discussed in the previous section is to assume that the units are linguistic constituents which can be easily identified, such as words, utterances, or noun phrases, and therefore there is no need to check the reliability of this process. We are aware of few exceptions to this assumption, such as Carletta et al. (1997) on unitization for move coding and our own work on the GNOME corpus (Poesio 2004b). In cases such as text segmentation, however, the identification of units is as important as their labeling, if not more important, and therefore checking agreement on unit identification is essential. In this section we discuss current CL practice with reliability testing of these types of annotation, before briefly summarizing Krippendorff's proposals concerning measuring reliability for unitizing.", "filtered_refids": [["b18", "b80", "b56", "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 3, "num_chars": 1015, "num_references": 4}
{"corpusid_sectionid": "12719479-s33", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Segmentation and Topic", "section": "Marking. Discourse segments are portions of text that constitute a unit either because they are about the same \"topic\" (Hearst 1997;Reynar 1998) or because they have to do with achieving the same intention (Grosz and Sidner 1986) or performing the same \"dialogue game\" (Carletta et al. 1997). 7 The analysis of discourse structure-and especially the identification of discourse segments-is the type of annotation that, more than any other, led CL researchers to look for ways of measuring reliability and agreement, as it made them aware of the extent of disagreement on even quite simple judgments (Kowtko, Isard, and Doherty 1992;Passonneau and Litman 1993;Carletta et al. 1997;Hearst 1997). Subsequent research identified a number of issues with discourse structure annotation, above all the fact that segmentation, though problematic, is still much easier than marking more complex aspects of discourse structure, such as identifying the most important segments or the \"rhetorical\" relations between segments of different granularity. As a result, many efforts to annotate discourse structure concentrate only on segmentation.\n\nThe agreement results for segment coding tend to be on the lower end of the scale proposed by Krippendorff and recommended by Carletta. Hearst (1997), for instance, found K = 0.647 for the boundary/not boundary distinction; Reynar (1998), measuring agreement between his own annotation and the TREC segmentation of broadcast news, reports K = 0.764 for the same task; Ries (2002) reports even lower agreement of K = 0.36. Teufel, Carletta, and Moens (1999), who studied agreement on the identification of argumentative zones, found high reliability (K = 0.81) for their three main zones (own, other, background), although lower for the whole scheme (K = 0.71). For intention-based segmentation, Passonneau and Litman (1993) in the pre-K days reported an overall percentage agreement with majority opinion of 89%, but the agreement on boundaries was only 70%. For conversational games segmentation, Carletta et al. (1997) reported \"promising but not entirely reassuring agreement on where games began (70%),\" whereas the agreement on transaction boundaries was K = 0.59. Exceptions are two segmentation efforts carried out as part of annotations of rhetorical structure. Moser, Moore, and Glendening (1996) achieved an agreement of K = 0.9 for the highest level of segmentation of their RDA annotation (Poesio, Patel, and Di Eugenio 2006). Carlson, Marcu, and Okurowski (2003) reported very high agreement over the identification of the boundaries of discourse units, the building blocks of their annotation of rhetorical structure. (Agreement was measured several times; initially, they obtained K = 0.87, and in the final analysis K = 0.97.) This, however, was achieved by employing experienced annotators, and with considerable training.\n\nOne important reason why most agreement results on segmentation are on the lower end of the reliability scale is the fact, known to researchers in discourse analysis from as early as Levin and Moore (1978), that although analysts generally agree on the \"bulk\" of segments, they tend to disagree on their exact boundaries. This phenomenon was also observed in more recent studies: See for example the discussion in Passonneau and Litman (1997), the comparison of the annotations produced by seven coders of the same text in Figure 5 of Hearst (1997, page 55), or the discussion by Carlson, Marcu, and Okurowski (2003), who point out that the boundaries between elementary discourse units tend to be \"very blurry.\" See also Pevzner and Hearst (2002) for similar comments made in the context of topic segmentation algorithms, and Klavans, Popper, and Passonneau (2003) for selecting definition phrases.\n\nThis \"blurriness\" of boundaries, combined with the prevalence effects discussed in Section 3.2, also explains the fact that topic annotation efforts which were only concerned with roughly dividing a text into segments (Passonneau and Litman 1993;Carletta et al. 1997;Hearst 1997;Reynar 1998;Ries 2002) generally report lower agreement than the studies whose goal is to identify smaller discourse units. When disagreement is mostly concentrated in one class ('boundary' in this case), if the total number of units to annotate remains the same, then expected agreement on this class is lower when a greater proportion of the units to annotate belongs to this class. When in addition this class is much less numerous than the other classes, overall agreement tends to depend mostly on agreement on this class.\n\nFor instance, suppose we are testing the reliability of two different segmentation schemes-into broad \"discourse segments\" and into finer \"discourse units\"-on a text of 50 utterances, and that we obtain the results in Table 8. Case 1 would be a situation in which Coder A and Coder B agree that the text consists of two segments, obviously agree on its initial and final boundaries, but disagree by one position on the intermediate boundary-say, one of them places it at utterance 25, the other at utterance 26. Nevertheless, because expected agreement is so high-the coders agree on the classification of 98% of the utterances-the value of K is fairly low. In case 2, the coders disagree on three times as many utterances, but K is higher than in the first case because expected agreement is substantially lower (A e = 0.53).\n\nThe fact that coders mostly agree on the \"bulk\" of discourse segments, but tend to disagree on their boundaries, also makes it likely that an all-or-nothing coefficient like K calculated on individual boundaries would underestimate the degree of agreement, suggesting low agreement even among coders whose segmentations are mostly similar. A weighted coefficient of agreement like \u03b1 might produce values more in keeping with intuition, but we are not aware of any attempts at measuring agreement on segmentation using weighted coefficients. We see two main options. We suspect that the methods proposed by Krippendorff (1995) for measuring agreement on unitizing (see Section 4.3.2, subsequently) may be appropriate for the purpose of measuring agreement on discourse segmentation. A second option would be to measure agreement not on individual boundaries but on windows spanning several units, as done in the methods proposed to evaluate the performance of topic detection algorithms such as  (Beeferman, Berger, and Lafferty 1999) or WINDOWDIFF (Pevzner and Hearst 2002) (which are, however, raw agreement scores not corrected for chance).", "filtered_refids": [["b43", "b90", "b18", "b41", "b51", "b76"], ["b90", "b18", "b91", "b82", "b102", null, "b19", "b69", "b76"], ["b60", "b78", "b50", "b79", null, "b19"], ["b43", "b90", "b18", "b91", "b76"], [], ["b79", "b6", "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 6552, "num_references": 29}
{"corpusid_sectionid": "12719479-s35", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Anaphora", "section": "The annotation tasks discussed so far involve assigning a specific label to each category, which allows the various agreement measures to be applied in a straightforward way. Anaphoric annotation differs from the previous tasks because annotators do not assign labels, but rather create links between anaphors and their antecedents. It is therefore not clear what the \"labels\" should be for the purpose of calculating agreement. One possibility would be to consider the intended referent (real-world object) as the label, as in named entity tagging, but it wouldn't make sense to predefine a set of \"labels\" applicable to all texts, because different objects are mentioned in different texts. An alternative is to use the marked antecedents as \"labels\". However, we do not want to count as a disagreement every time two coders agree on the discourse entity realized by a particular noun phrase but just happen to mark different words as antecedents. Consider the reference of the underlined pronoun it in the following dialogue excerpt (TRAINS 1991 [Gross, Allen, andTraum 1993], dialogue d91-3.2). 8 1.1 M: .... 1.4 first thing I'd like you to do 1.5 is send engine E2 off with a boxcar to Corning to pick up oranges 1.6 as soon as possible 2.1 S: okay 3.1 M: and while it's there it should pick up the tanker Some of the coders in a study we carried out (Poesio and Artstein 2005) indicated the noun phrase engine E2 as antecedent for the second it in utterance 3.1, whereas others indicated the immediately preceding pronoun, which they had previously marked as having engine E2 as antecedent. Clearly, we do not want to consider these coders to be in disagreement. A solution to this dilemma has been proposed by Passonneau (2004): Use the emerging coreference sets as the 'labels' for the purpose of calculating agreement. This requires using weighted measures for calculating agreement on such sets, and consequently it raises serious questions about weighted measures-in particular, about the interpretability of the results, as we will see shortly. Passonneau's Proposal. Passonneau (2004) recommends measuring agreement on anaphoric annotation by using sets of mentions of discourse entities as labels, that is, the emerging anaphoric/coreference chains. This proposal is in line with the methods developed to evaluate anaphora resolution systems (Vilain et al. 1995). But using anaphoric chains as labels would not make unweighted measures such as K a good measure for agreement. Practical experience suggests that, except when a text is very short, few annotators will catch all mentions of a discourse entity: Most will forget to mark a few, with the result that the chains (that is, category labels) differ from coder to coder and agreement as measured with K is always very low. What is needed is a coefficient that also allows for partial disagreement between judgments, when two annotators agree on part of the coreference chain but not on all of it. Passonneau (2004) suggests solving the problem by using \u03b1 with a distance metric that allows for partial agreement among anaphoric chains. Passonneau proposes a distance metric based on the following rationale: Two sets are minimally distant when they are identical and maximally distant when they are disjoint; between these extremes, sets that stand in a subset relation are closer (less distant) than ones that merely intersect. This leads to the following distance metric between two sets A and B.", "filtered_refids": [["b40", "b107", null, "b1", "b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3468, "num_references": 5}
{"corpusid_sectionid": "12719479-s38", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Discourse Deixis.", "section": "A second annotation study we carried out (Artstein and Poesio 2006) shows even more clearly the possible side effects of using weighted coefficients. This study was concerned with the annotation of the antecedents of references to abstract objects, such as the example of the pronoun that in utterance 7.6 (TRAINS 1991, dialogue d91-2.2).\n\n7.3 : so we ship one 7.4 : boxcar 7.5 : of oranges to Elmira 7.6 : and that takes another 2 hours Previous studies of discourse deixis annotation showed that these are extremely difficult judgments to make (Eckert and Strube 2000;Navarretta 2000;Byron 2002), except perhaps for identifying the type of object (Poesio and Modjeska 2005), so we simplified the task by only requiring our participants to identify the boundaries of the area of text in which the antecedent was introduced. Even so, we found a great variety in how these boundaries were marked: Exactly as in the case of discourse segmentation discussed earlier, our participants broadly agreed on the area of text, but disagreed on  its exact boundary. For instance, in this example, nine out of ten annotators marked the antecedent of that as a text segment ending with the word Elmira, but some started with the word so, some started with we, some with ship, and some with one.\n\nWe tested a number of ways to measure partial agreement on this task, and obtained widely different results. First of all, we tested three set-based distance metrics inspired by the Passonneau proposals that we just discussed: We considered discourse segments to be sets of words, and computed the distance between them using Passonneau's metric, Jaccard, and Dice. Using these three metrics, we obtained \u03b1 values of 0.55 (with Passonneau's metric), 0.45 (with Jaccard), and 0.55 (with Dice). We should note that because antecedents of different expressions rarely overlapped, the expected disagreement was close to 1 (maximal), so the value of \u03b1 turned out to be very close to the complement of the observed disagreement as calculated by the different distance metrics.\n\nNext, we considered methods based on the position of words in the text. The first method computed differences between absolute boundary positions: Each antecedent was associated with the position of its first or last word in the dialogue, and agreement was calculated using \u03b1 with the interval distance metric. This gave us \u03b1 values of 0.998 for the beginnings of the antecedent-evoking area and 0.999 for the ends. This is because expected disagreement is exceptionally low: Coders tend to mark discourse antecedents close to the referring expression, so the average distance between antecedents of the same expression is smaller than the size of the dialogue by a few orders of magnitude. The second method associated each antecedent with the position of its first or last word relative to the beginning of the anaphoric expression. This time we found extremely low values of \u03b1 = 0.167 for beginnings of antecedents and 0.122 for endsbarely in the positive side. This shows that agreement among coders is not dramatically better than what would be expected if they just marked discourse antecedents at a fixed distance from the referring expression.\n\nThe three ranges of \u03b1 that we observed (middle, high, and low) show agreement on the identity of discourse antecedents, their position in the dialogue, and their position relative to referring expressions, respectively. The middle range shows variability of up to 10 percentage points, depending on the distance metric chosen. The lesson is that once we start using weighted measures we cannot anymore interpret the value of \u03b1 using traditional rules of thumb such as those proposed by Krippendorff or by Landis and Koch. This is because depending on the way we measure agreement, we can report \u03b1 values ranging from 0.122 to 0.998 for the very same experiment! New interpretation methods have to be developed, which will be task-and distance-metric specific. We'll return to this issue in the conclusions.", "filtered_refids": [["b2"], ["b15", "b70", "b81", "b33"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 4014, "num_references": 5}
{"corpusid_sectionid": "12719479-s39", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Word Senses", "section": "Word sense tagging is one of the hardest annotation tasks. Whereas in the case of partof-speech and dialogue act tagging the same categories are used to classify all units, in the case of word sense tagging different categories must be used for each word, which makes writing a single coding manual specifying examples for all categories impossible: The only option is to rely on a dictionary. Unfortunately, different dictionaries make different distinctions, and often coders can't make the fine-grained distinctions that trained lexicographers can make. The problem is particularly serious for verbs, which tend to be polysemous rather than homonymous (Palmer, Dang, and Fellbaum 2007).\n\nThese difficulties, and in particular the difficulty of tagging senses with a finegrained repertoire of senses such as that provided by dictionaries or by WordNet (Fellbaum 1998), have been highlighted by the three SENSEVAL initiatives. Already during the first SENSEVAL, V\u00e9ronis (1998) carried out two studies of intercoder agreement on word sense tagging in the so-called ROMANSEVAL task. One study was concerned with agreement on polysemy-that is, the extent to which coders agreed that a word was polysemous in a given context. Six naive coders were asked to make this judgment about 600 French words (200 nouns, 200 verbs, 200 adjectives) using the repertoire of senses in the Petit Larousse. On this task, a (pairwise) percentage agreement of 0.68 for nouns, 0.74 for verbs, and 0.78 for adjectives was observed, corresponding to K values of 0.36, 0.37, and 0.67, respectively. The 20 words from each category perceived by the coders in this first experiment to be most polysemous were then used in a second study, of intercoder agreement on the sense tagging task, which involved six different naive coders. Interestingly, the coders in this second experiment were allowed to assign multiple tags to words, although they did not make much use of this possibility; so \u03ba w was used to measure agreement. In this experiment, V\u00e9ronis observed (weighted) pairwise agreement of 0.63 for verbs, 0.71 for adjectives, and 0.73 for nouns, corresponding to \u03ba w values of 0.41, 0.41, and 0.46, but with a wide variety of values when measured per word-ranging from 0.007 for the adjective correct to 0.92 for the noun d\u00e9tention. Similarly mediocre results for intercoder agreement between naive coders were reported in the subsequent editions of SENSEVAL. Agreement studies for SENSEVAL-2, where WordNet senses were used as tags, reported a percentage agreement for verb senses of around 70%, whereas for SENSEVAL-3 (English Lexical Sample Task), Mihalcea, Chklovski, and Kilgarriff (2004) report a percentage agreement of 67.3% and average K of 0.58.\n\nTwo types of solutions have been proposed for the problem of low agreement on sense tagging. The solution proposed by Kilgarriff (1999) is to use professional lexicographers and arbitration. The study carried out by Kilgarriff does not therefore qualify as a true study of replicability in the sense of the terms used by Krippendorff, but it did show that this approach makes it possible to achieve percentage agreement of around 95.5%. An alternative approach has been to address the problem of the inability of naive coders to make fine-grained distinctions by introducing coarser-grained classification schemes which group together dictionary senses (Bruce and Wiebe, 1998;Buitelaar 1998;V\u00e9ronis 1998;Palmer, Dang, and Fellbaum 2007). Hierarchical tagsets were also developed, such as HECTOR (Atkins 1992) or, indeed, WordNet itself (where senses are related by hyponymy links). In the case of Buitelaar and Palmer, Dang, and Fellbaum, the \"supersenses\" were identified by hand, whereas Bruce and Wiebe and V\u00e9ronis used clustering methods such as those from Bruce and Wiebe (1999) to collapse some of the initial sense distinctions. 9 Palmer, Dang, and Fellbaum (2007) illustrate this practice with the example of the verb call, which has 28 fine-grained senses in WordNet 1.7: They conflate these senses into a small number of groups using various criteria-for example, four senses can be grouped in a group they call Group 1 on the basis of subcategorization frame similarities (Table 9). Palmer, Dang, and Fellbaum (2007) achieved for the English Verb Lexical Sense task of SENSEVAL-2 a percentage agreement among coders of 82% with grouped senses, as opposed to 71% with the original WordNet senses. Bruce and Wiebe (1998) found that collapsing the senses of their test word (interest) on the basis of their use by coders and merging the two classes found to be harder to distinguish resulted in an increase of Table 9 Group 1 of senses of call in Palmer, Dang, and Fellbaum (2007, page 149 the value of K from 0.874 to 0.898. Using a related technique, V\u00e9ronis (1998) found that agreement on noun word sense tagging went up from a K of around 0.45 to a K of 0.86. We should note, however, that the post hoc merging of categories is not equivalent to running a study with fewer categories to begin with. Attempts were also made to develop techniques to measure partial agreement with hierarchical tagsets. A first proposal in this direction was advanced by Melamed and Resnik (2000), who developed a coefficient for hierarchical tagsets that could be used in SENSEVAL for measuring agreement with tagsets such as HECTOR. Melamed and Resnik proposed to \"normalize\" the computation of observed and expected agreement by taking each label which is not a leaf in the tag hierarchy and distributing it down to the leaves in a uniform way, and then only computing agreement on the leaves. For example, with a tagset like the one in Table 9, the cases in which the coders used the label 'Group 1' would be uniformly \"distributed down\" and added in equal measure to the number of cases in which the coders assigned each of the four WordNet labels. The method proposed in the paper has, however, problematic properties when used to measure intercoder agreement. For example, suppose tag A dominates two sub-tags A1 and A2, and that two coders mark a particular item as A. Intuitively, we would want to consider this a case of perfect agreement, but this is not what the method proposed by Melamed and Resnik yields. The annotators' marks are distributed over the two sub-tags, each with probability 0.5, and then the agreement is computed by summing the joint probabilities over the two subtags (Equation (4) of Melamed and Resnik 2000), with the result that the agreement over the item turns out to be 0.5 2 + 0.5 2 = 0.5 instead of 1. To correct this, Dan Melamed (personal communication) suggested replacing the product in Equation (4) with a minimum operator. However, the calculation of expected agreement (Equation (5) of Melamed and Resnik 2000) still gives the amount of agreement which is expected if coders are forced to choose among leaf nodes, which makes this method inappropriate for coding schemes that do not force coders to do this.\n\nOne way to use Melamed and Resnik's proposal while avoiding the discrepancy between observed and expected agreement is to treat the proposal not as a new coefficient, but rather as a distance metric to be plugged into a weighted coefficient like \u03b1. Let A and B be two nodes in a hierarchical tagset, let L be the set of all leaf nodes in the tagset, and let P(l|T) be the probability of selecting a leaf node l given an arbitrary node T when the probability mass of T is distributed uniformly to all the nodes dominated by T. We can reinterpret Melamed's modification of Equation (4) in Melamed and Resnik (2000) as a metric measuring the distance between nodes A and B. d M+R = 1 \u2212 \u2211 l\u2208L min(P(l|A), P(l|B)) This metric has the desirable properties-it is 0 when tags A and B are identical, 1 when the tags do not overlap, and somewhere in between in all other cases. If we use this metric for Krippendorff's \u03b1 we find that observed agreement is exactly the same as in Melamed and Resnik (2000) with the product operator replaced by minimum (Melamed's modification).\n\nWe can also use other distance metrics with \u03b1. For example, we could associate with each sense an extended sense-a set es(s) including the sense itself and its grouped sense-and then use set-based distance metrics from Section 4.4, for example Passonneau's d P . To illustrate how this approach could be used to measure (dis)agreement on word sense annotation, suppose that two coders have to annotate the use of call in the following sentence (from the WSJ part of the Penn Treebank, section 02, text w0209):\n\nThis gene, called \"gametocide,\" is carried into the plant by a virus that remains active for a few days.\n\nThe standard guidelines (in SENSEVAL, say) require coders to assign a WN sense to words. Under such guidelines, if coder A classifies the use of called in the above example as an instance of WN1, whereas coder B annotates it as an instance of WN3, we would find total disagreement (d k a k b = 1) which seems excessively harsh as the two senses are clearly related. However, by using the broader senses proposed by Palmer, Dang, and Fellbaum (2007) in combination with a distance metric such as the one just proposed, it is possible to get more flexible and, we believe, more realistic assessments of the degree of agreement in situations such as this. For instance, in case the reliability study had already been carried out under the standard SENSEVAL guidelines, the distance metric proposed above could be used to identify post hoc cases of partial agreement by adding to each WN sense its hypernyms according to the groupings proposed by Palmer, Dang, and Fellbaum. For example, A's annotation could be turned into a new set label {WN1,LABEL} and B's mark into the set table {WN3,LABEL}, which would give a distance d = 2/3, indicating a degree of overlap. The method for computing agreement proposed here could could also be used to allow coders to choose either a more specific label or one of Palmer, Dang, and Fellbaum's superlabels. For example, suppose A sticks to WN1, but B decides to mark the use above using Palmer, Dang, and Fellbaum's LABEL category, then we would still find a distance d = 1/3. An alternative way of using \u03b1 for word sense annotation was developed and tested by Passonneau, Habash, and Rambow (2006). Their approach is to allow coders to assign multiple labels (WordNet synsets) for wordsenses, as done by V\u00e9ronis (1998) and more recently by Rosenberg and Binkowski (2004) for text classification labels and by Poesio and Artstein (2005) for anaphora. These multi-label sets can then be compared using the MASI distance metric for \u03b1 (Passonneau 2006).", "filtered_refids": [["b72"], [null, "b67", "b106"], ["b11", "b12", "b10", "b49", null, "b72", "b3", "b65", "b106"], [null, "b65"], [], [], ["b74", "b75", "b93", "b72", "b1", "b106"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 51, "num_chars": 10658, "num_references": 21}
{"corpusid_sectionid": "12719479-s41", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Methodology", "section": "Our first recommendation is that annotation efforts should perform and report rigorous reliability testing. The last decade has already seen considerable improvement, from the absence of any tests for the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993) or the British National Corpus (Leech, Garside, and Bryant 1994) to the central role played by reliability testing in the Penn Discourse Treebank (Miltsakaki et al. 2004) and OntoNotes (Hovy et al. 2006). But even the latter efforts only measure and report percent agreement. We believe that part of the reluctance to report chance-corrected measures is the difficulty in interpreting them. However, our experience is that chancecorrected coefficients of agreement do provide a better indication of the quality of the resulting annotation than simple percent agreement, and moreover, the detailed calculations leading to the coefficients can be very revealing as to where the disagreements are located and what their sources may be.\n\nA rigorous methodology for reliability testing does not, in our opinion, exclude the use of expert coders, and here we feel there may be a motivated difference between the fields of content analysis and CL. There is a clear tradeoff between the complexity of the judgments that coders are required to make and the reliability of such judgments, and we should strive to devise annotation schemes that are not only reliable enough to be replicated, but also sophisticated enough to be useful (cf. Krippendorff 2004a, pages 213-214). In content analysis, conclusions are drawn directly from annotated corpora, so the emphasis is more on replicability; whereas in CL, corpora constitute a resource which is used by other processes, so the emphasis is more towards usefulness. There is also a tradeoff between the sophistication of judgments and the availability of coders who can make such judgments. Consequently, annotation by experts is often the only practical way to get useful corpora for CL. Current practice achieves high reliability either by using professionals (Kilgarriff 1999) or through intensive training (Hovy et al. 2006;Carlson, Marcu, and Okurowski 2003); this means that results are not replicable across sites, and are therefore less reliable than annotation by naive coders adhering to written instructions. We feel that inter-annotator agreement studies should still be carried out, as they serve as an assurance that the results are replicable when the annotators are chosen from the same population as the original annotators. An important additional assurance should be provided in the form of an independent evaluation of the task for which the corpus is used (cf. Passonneau 2006).", "filtered_refids": [["b59", "b63", "b44", "b68"], ["b44", "b49", null, "b19", "b74"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2702, "num_references": 9}
{"corpusid_sectionid": "12719479-s42", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Choosing a Coefficient", "section": "One of the goals of this article is to help authors make an informed choice regarding the coefficients they use for measuring agreement. While coefficients other than K, specifically Cohen's \u03ba and Krippendorff's \u03b1, have appeared in the CL literature as early as Carletta (1996) and Passonneau and Litman (1996), they hadn't sprung into general awareness until the publication of Di Eugenio and Glass (2004) and Passonneau (2004). Regarding the question of annotator bias, there is an overwhelming consensus in CL practice: K and \u03b1 are used in the vast majority of the studies we reported. We agree with the view that K and \u03b1 are more appropriate, as they abstract away from the bias of specific coders. But we also believe that ultimately this issue of annotator bias is of little consequence because the differences get smaller and smaller as the number of annotators grows (Artstein and Poesio 2005). We believe that increasing the number of annotators is the best strategy, because it reduces the chances of accidental personal biases.\n\nHowever, Krippendorff's \u03b1 is indispensable when the category labels are not equally distinct from one another. We think there are at least two types of coding schemes in which this is the case: (i) hierarchical tagsets and (ii) set-valued interpretations such as those proposed for anaphora. At least in the second case, weighted coefficients are almost unavoidable. We therefore recommend using \u03b1, noting however that the specific choice of weights will affect the overall numerical result.", "filtered_refids": [["b28", "b77", "b1", "b73", "b17"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1531, "num_references": 5}
{"corpusid_sectionid": "12719479-s43", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Interpreting the Values", "section": "We view the lack of consensus on how to interpret the values of agreement coefficients as a serious problem with current practice in reliability testing, and as one of the main reasons for the reluctance of many in CL to embark on reliability studies. Unlike significance values which report a probability (that an observed effect is due to chance), agreement coefficients report a magnitude, and it is less clear how to interpret such magnitudes. Our own experience is consistent with that of Krippendorff: Both in our earlier work (Poesio and Vieira 1998;Poesio 2004a) and in the more recent efforts (Poesio and Artstein 2005) we found that only values above 0.8 ensured an annotation of reasonable quality (Poesio 2004a). We therefore feel that if a threshold needs to be set, 0.8 is a good value.\n\nThat said, we doubt that a single cutoff point is appropriate for all purposes. For some CL studies, particularly on discourse, useful corpora have been obtained while attaining reliability only at the 0.7 level. We agree therefore with Craggs and McGee Wood (2005) that setting a specific agreement threshold should not be a prerequisite for publication. Instead, as recommended by Di Eugenio and Glass (2004) and others, researchers should report in detail on the methodology that was followed in collecting the reliability data (number of coders, whether they coded independently, whether they relied exclusively on an annotation manual), whether agreement was statistically significant, and provide a confusion matrix or agreement table so that readers can find out whether overall figures of agreement hide disagreements on less common categories. For an example of good practice in this respect, see Teufel and Moens (2002). The decision whether a corpus is good enough for publication should be based on more than the agreement score-specifically, an important consideration is an independent evaluation of the results that are based on the corpus.", "filtered_refids": [["b80", "b83", "b1"], ["b28", "b25", "b103"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1957, "num_references": 6}
{"corpusid_sectionid": "8132278-s1", "title": "A Survey of Idiomatic Preposition-Noun-Verb Triples on Token Level", "date": "2010-05-01", "section_title": "Background", "section": "Most of the research on the extraction of idiomatic MWEs focused on the acquisition of MWE types. The procedures made use of several corpus-observable idiosyncratic properties of MWEs: they were identified either based on their co-occurrence frequency (Evert, 2004), their morphosyntactic fixedness -e.g. (Fazly and Stevenson, 2006), (Bannard, 2007) -or their semantics -e.g. (Lin, 1999), (Baldwin et al., 2003), to name only a few examples. However, most of these approaches operate on lexical type level, stating, e.g. that spill+beans is idiomatic, but not on token level. Contrary to this, we intend to take into account whether a text instance of a potentially idiomatic MWE is actually used idiomatically in a given context or not. In fact, there are a number of idiomatic MWEs that can also have a straightforward literal meaning. It is possible to automatically distinguish the idiomatic from the literal use in the way (Katz and Giesbrecht, 2006) did by using latent semantic analysis. In one of their case-studies they found that two thirds of the occurrences of the German idiom ins Wasser fallen (lit.: \"to fall into the water\", idiom.: \"to be cancelled\") were idiomatic uses, as opposed to one third literal uses. In the case of ins Wasser fallen, the two meanings exhibit the same morpho-syntactic surface form. However, sometimes the surface form may help to distinguish the different idiomatic vs. literal uses. Quite often, morpho-syntactic features also support a separation of \"homonymous\" idioms, which have the same lexical items as components, or of different (idiomatic) readings of a \"polysemous\" idiom (see Section 5.2. below) . An example of homography is the German idiom in Gang kommen which means \"to be set in motion\" when it appears in singular form without determiner, while the same used in plural form with definite article in die G\u00e4nge kommen, bear the meaning \"to get organised\". A literal meaning is also thinkable, e.g. in singular with definite article in den Gang kommen, where it would mean something like \"to reach the hallway\". These examples show that it is not sufficient to handle MWEs solely on the basis of the lemmas of their components, but that their context and surface form has also to be taken into account. To our knowledge, (Katz and Giesbrecht, 2006) were so far the only authors who investigated the automatic identification of idiomatic vs. literal uses of German MWEs. For English, however, there has been some more work in this field recently: this includes unsupervised methods like e.g. (Sporleder and Li, 2009) who make use of lexical cohesion in order to recognise different uses of idiomatic MWEs or (Fazly et al., 2009) who use combined knowledge of canonical forms and context information; there have also been supervised methods like (Diab and Bhutada, 2009), who used the MWEs' context and surface form features in a classification approach based on machine learning.", "filtered_refids": [["b6", "b10", "b14", null, "b7", "b5", "b1", "b0", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2937, "num_references": 9}
{"corpusid_sectionid": "8132278-s2", "title": "A Survey of Idiomatic Preposition-Noun-Verb Triples on Token Level", "date": "2010-05-01", "section_title": "Objectives", "section": "In the present work we do not investigate new methods for MWE classification in context. Instead, we take one step back and present a German resource that could be useful for future supervised methods, similar to e.g. (Diab and Bhutada, 2009) and for evaluation of extraction tools. Inspired by the VNC-Tokens Dataset of (Cook et al., 2008), consisting of ca. 3,000 manually annotated corpus sentences of 53 English verb-noun combinations (VNCs), we created a dataset for German: our set contains the manually analysed results of 77 German preposition-noun-verb triples (PNVs: a frequent pattern among German MWEs) in roughly 9,700 sentences. Each instance is provided along with a detailed morpho-syntactic feature description of the MWE and a classification into either literal or idiomatic use. Some cases cannot be decided, as the context given in the sentence is not sufficient to determine the intended reading. Even though we primarily conceived the dataset to serve as a basis for the development of new supervised MWE classification approaches, we will also discuss some examples based on the quantitative distributions of the different readings and their morpho-syntactic feature preferences. We thereby intend to enhance the awareness of literal uses of presumably idiomatic MWEs. Previous work in this field (for German) includes a corpusbased study (H\u00fcmmer, 2007), where the literal vs. idiomatic meaning of 60 German MWEs (of different structural patterns) was investigated from a linguistic and phraseological point of view. Finally, we are aware of one more dataset for English which is (as (Cook et al., 2008) and ours) also conceived to serve future supervised extraction methods, namely the IDIX corpus of (Sporleder et al., 2010). It covers 50 English idioms (mainly V+NP and V+PP) in roughly 5,000 instances and will be available as an add-on to the BNC XML edition.", "filtered_refids": [["b15", "b9", "b2", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1887, "num_references": 4}
{"corpusid_sectionid": "8132278-s6", "title": "A Survey of Idiomatic Preposition-Noun-Verb Triples on Token Level", "date": "2010-05-01", "section_title": "Parsing", "section": "In order to reliably extract PNVs , a deep syntactic analysis is essential, due to the above mentioned non-adjacency phenomena. Furthermore, as a by-product of parsing, we get a full morpho-syntactic analysis of a PNV's constituent words.\n\nIn the past, we successfully used the dependency parser FSPAR (Schiehlen, 2003) for several different MWE extraction tasks, e.g. (Fritzinger, 2009), (Weller and Heid, 2010). FSPAR is highly efficient and relies on a large lexicon. Its output, given in Figure 1 (b), is to be read as follows:\n\n1 st column: position of a word in the sentence 2 nd column: token 3 rd column: part of speech 2 4 th column: base form (lemma) 5 th column: morpho-syntactic information (case, gender, etc.) 6 th column: dependency relation: position of the word's governor 7 th column: grammatical function (subject, object, etc.)\n\nThe dependency tree representation in Figure 1(a) is not provided by the parser; we inserted it here in order to enhance readability of the example. As can be seen from Figure 1, the noun Raum is dependent on the preposition im: the 6 th column in the noun's row (cf. Fig. 1 (b)) points to sentence position 5, where the preposition is located. Analogously, the preposition im is dependent on the verb steht. In case of structural or labelling ambiguities, the parser provides an underspecified output, as e.g. for the attachment of weiter (1||5) in Figure 1: it is either dependent on steht (1) or im (5).", "filtered_refids": [[], ["b16", "b8", "b13"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1455, "num_references": 3}
{"corpusid_sectionid": "16946379-s2", "title": "Tokenization: Returning to a Long Solved Problem A Survey, Contrastive Experiment, Recommendations, and Toolkit", "date": "2012-07-08", "section_title": "A Contrastive Experiment", "section": "To get an overview of current tokenization methods, we recovered and tokenized the raw text which was the source of the (Wall Street Journal portion of the) PTB, and compared it to the gold tokenization in the syntactic annotation in the treebank. 4 We used three common methods of tokenization: (a) the original 2 See http://www.cis.upenn.edu/~treebank/ tokenization.html for available 'documentation' and a sed script for PTB-style tokenization.\n\n3 \u00d8vrelid et al. (2010) observe that tokenizing with the GE-NIA tagger yields mismatches in one of five sentences of the GENIA Treebank, although the GENIA guidelines refer to scripts that may be available on request (Tateisi & Tsujii, 2006). 4 The original WSJ text was last included with the 1995 release of the PTB (LDC #95T07) and required alignment with the treebank, with some manual correction so that the same text is represented in both raw and parsed formats. PTB tokenizer.sed script; (b) the tokenizer from the Stanford CoreNLP tools 5 ; and (c) tokenization from the parser of Charniak & Johnson (2005). Table 1 shows quantitative differences between each of the three methods and the PTB, both in terms of the number of sentences where the tokenization differs, and also in the total Levenshtein distance (Levenshtein, 1966) over tokens (for a total of 49,208 sentences and 1,173,750 gold-standard tokens).\n\nLooking at the differences qualitatively, the most consistent issue across all tokenization methods was ambiguity of sentence-final periods. In the treebank, final periods are always (with about 10 exceptions) a separate token. If the sentence ends in U.S. (but not other abbreviations, oddly), an extra period is hallucinated, so the abbreviation also has one. In contrast, C&J add a period to all final abbreviations, CoreNLP groups the final period with a final abbreviation and hence lacks a sentence-final period token, and the sed script strips the period off U.S. The 'correct' choice in this case is not obvious and will depend on how the tokens are to be used.\n\nThe majority of the discrepancies in the sed script tokenization come from an under-restricted punctuation rule that incorrectly splits on commas within numbers or ampersands within names. Other than that, the problematic cases are mostly shared across tokenization methods, and include issues with currencies, Irish names, hyphenization, and quote disambiguation. In addition, C&J make some additional modifications to the text, lemmatising expressions such as won't as will and n't. Expression-Based Pre-Processing)-essentially a cascade of ordered finite-state string rewriting rules, though transcending the formal complexity of regular languages by inclusion of (a) full perl-compatible regular expressions and (b) fixpoint iteration over groups of rules. In this approach, a first phase of string-level substitutions inserts whitespace around, for example, punctuation marks; upon completion of string rewriting, token boundaries are stipulated between all whitespace-separated substrings (and only these).\n\nFor a good balance of human and machine readability, REPP tokenization rules are specified in a simple, line-oriented textual form. Figure 1 shows a (simplified) excerpt from our PTB-style tokenizer, where the first character on each line is one of four REPP operators, as follows: (a) '#' for group formation; (b) '>' for group invocation, (c) '!' for substitution (allowing capture groups), and (d) ':' for token boundary detection. 6 In Figure 1, the two rules stripping off prefix and suffix punctuation marks adjacent to whitespace (i.e. matching the tab-separated left-hand side of the rule, to replace the match with its right-hand side) form a numbered group ('#1'), which will be iterated when called ('>1') until none of the rules in the group fires (a fixpoint). In this example, conditioning on whitespace adjacency avoids the issues observed with the PTB sed script (e.g. token boundaries within comma-separated numbers) and also protects against infinite loops in the group. 7 REPP rule sets can be organized as modules, typ-6 Strictly speaking, there are another two operators, for lineoriented comments and automated versioning of rule files. 7 For this example, the same effects seemingly could be obtained without iteration (using greatly more complex rules); our actual, non-simplified rules, however, further deal with punctuation marks that can function as prefixes or suffixes, as well as with corner cases like factor(s) or Ca [2+]. Also in mark-up removal and normalization, we have found it necessary to 'parse' nested structures by means of iterative groups. ically each in a file of its own, and invoked selectively by name (e.g. '>wiki' in Figure 1); to date, there exist modules for quote disambiguation, (relevant subsets of) various mark-up languages (HTML, L A T E X, wiki, and XML), and a handful of robustness rules (e.g. seeking to identify and repair 'sandwiched' inter-token punctuation). Individual tokenizers are configured at run-time, by selectively activating a set of modules (through command-line options). An open-source reference implementation of the REPP framework (in C ++ ) is available, together with a library of modules for English.", "filtered_refids": [[null], [null, "b8", "b0"], [], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 5241, "num_references": 5}
{"corpusid_sectionid": "216552915-s2", "title": "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?", "date": "2020-04-25", "section_title": "Belief Measurement in Social Psychology", "section": "A common approach for measuring beliefs about specific identities is to assume a dimensional representation-that is, to assume a set of distinct dimensions of social meaning can be used to characterize how we think and feel about someone that holds a particular identity. From this dimensional perspective, two primary questions arise.\n\nFirst, what are the dimensions along which beliefs form? Social psychologists have identified three classes of important dimensions: traits, affective meanings, and semantic associations. Traits represent visible-although also socioculturally defined-characteristics like age, gender, and race (Freeman and Ambady, 2011). Affective dimensions of social meaning represent how we feel about a given person and/or identity (Todorov et al., 2015;Fiske et al., 2002;Heise, 2007). Here, we use the three affective dimensions proposed by Heise (2007) and that are popular in sociology (Rogers et al., 2013)-Evaluation (goodness/badness), Potency (strength/weakness), and Activity (active/passive). Finally, social psychologists often characterize beliefs about identities in terms of semantic associations to particular concepts (Freeman andAmbady, 2011) or institutions (MacKinnon andHeise, 2010). For example, people link the identities brother and sister together because they are both associated with the family institution. In the present work, we collect beliefs for seventeen different dimensions of social meaning, incorporating age, race, gender, evaluation, potency, activity, and six institutional associations.\n\nSecond, given a theorized dimension of meaning, how should we measure society-wide beliefs about where particular identities lie on that dimension? Here, we adopt perhaps the most common approach, which uses semantic differential scales on surveys (Osgood et al., 1975). The semantic differential technique asks respondents to place an identity on a sliding scale with two opposing concepts (e.g. weak and strong, see the example in Figure 2A). Finally, it is worth noting that here, like in most social psychology research, we assume that responses from survey participants generalize to American culture writ large. This assumption is built on the well-established culture-as-consensus paradigm in psychological anthropology (Karabatsos and Batchelder, 2003;Batchelder and Romney, 1988), and empirical work showing that people tend to agree on the vast majority of their beliefs about people (Heise, 2007). Nonetheless, many counterexamples exist (Berger et al., 1992;Smith-Lovin and Douglas, 1992). We leave questions about how to address these issues to future work.", "filtered_refids": [[], ["b15", null, "b4"], ["b16", "b1", "b10", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2624, "num_references": 7}
{"corpusid_sectionid": "216552915-s3", "title": "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?", "date": "2020-04-25", "section_title": "Measuring beliefs with embeddings", "section": "Embedding-based approaches to measuring beliefs typically follow a three step process of corpus/embedding selection, dimension selection, and word position measurement.\n\nCorpus/Embedding Selection Several recent works have argued that the corpus used can impact measures of beliefs about people derived from word embeddings (Lauscher and Glava\u0161, 2019;Mirzaev et al., 2019;Sweeney and Najafian, 2019). For example, Brunet et al. (2019) show how to reduce gender bias in embeddings by removing particular documents from a corpus. However, several oth-ers have shown that in their analyses, the corpus used does not significantly impact results (Spirling and Rodriguez, 2019;Garg et al., 2018;Kozlowski et al., 2019;Caliskan et al., 2017). Differences in the embedding model used have also been observed to impact measurements (Chaloner and Maldonado, 2019). Again, though, robustness checks from other studies suggest a limited effect beyond the somewhat general hyperparameters of window size and the number of dimensions estimated (Garg et al., 2018;Kozlowski et al., 2019).", "filtered_refids": [[], [null, "b9", "b2", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1074, "num_references": 4}
{"corpusid_sectionid": "216552915-s4", "title": "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?", "date": "2020-04-25", "section_title": "Dimension Selection", "section": "To measure beliefs, one first must select a dimension along which the belief is assumed to be held. Much of the literature has focused on dimensions related to gender or race. Others, however, have seen value in moving beyond these dimensions (Agarwal et al., 2019;Sweeney and Najafian, 2019). Most relevant is the work of Kozlowski et al. (2019), who study the association of 59 concepts across 20 different dimensions of sociocultural meaning, and that of An et al. (2018), who induce 732 different dimensions using WordNet to study contextual effects of linguistic meaning. While neither work focuses heavily on identities, these efforts compliment our goal of studying a broad range of dimensions of social meaning.\n\nScholars then identify a direction within the embedding that represents this dimension. To do so, an approach similar to the semantic differential idea is used. Terms are selected to represent the two ends of the dimension. For example, to identify the gender direction, words at one end might be he and him, and words at the other end, she and her. Scholarship varies on how these dimensioninducing word sets are selected. For example, several scholars have used demographically gendered and/or racialized names (Bolukbasi et al., 2016;Caliskan et al., 2017), while others have relied on careful extraction of concepts from dictionaries and thesauri (Kozlowski et al., 2019). Kozlowski et al. (2019) find that having more words at each end generally provides better measurements, and others have found a need to use frequently occurring terms (Ethayarajh et al., 2019;Brunet et al., 2019). Beyond these observations, however, scholars have generally found stable results as long as reasonable word sets are selected.\n\nWord Position Measurement Finally, the position of each identity along this direction must be identified. Doing so entails two major deci-sions. First, how should one quantify the direction, given the dimension-inducing words? For example, Bolukbasi et al. (2016) identify the direction by taking the first dimension of a PCA on the full set of direction words. Second, how should one define the position of points along this line? For example, several works use the cosine similarity between the identified \"bias direction\" and the embedding of each identity. Scholars have also recently proposed supervised methods for word position measurement (Sweeney and Najafian, 2019;Agarwal et al., 2019). Such approaches are important, but assume the existence of some training data, which may or may not be available in certain measurement contexts. We therefore do not explore these methods further in the present work.\n\nIn sum, using embeddings to measure beliefs requires a series of decisions, the impacts of which are still debated. Below, we provide the most comprehensive study to date on the importance of these decisions on measurement quality.", "filtered_refids": [[null, "b8", "b2"], [null, "b2"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 2887, "num_references": 6}
{"corpusid_sectionid": "216552915-s19", "title": "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?", "date": "2020-04-25", "section_title": "A Embedding Models", "section": "We use twelve publicly available embedding models. We use all public GloVe (Pennington et al., 2014) models 7 , FastText (Mikolov et al., 2018) models 8 , the original Word2Vec (Mikolov et al., 2013) Table 3 outlines the word position measurement models used in the present work. The table provides information on the authors of the measure, whether or not embeddings are normalized before analysis, how words are measured once a direction has been specified, how a direction is specified, and whether or not the method is \"multi-class,\" described further below. Notationally, we have tried to remain as close to the original works as possible. Therefore, w is the identity to be measured, and b is the vector indicating the direction along which it is to be measured. For Garg et al. (2018), b l and b r represent words in the left-hand dimension-inducing word set (e.g. \"man\" and \"him\" for gender) and b r the right-hand of the dimension-inducing word sets (e.g. \"woman\" and \"her\" for gender). In addition, we consider the possibility that the computationally appealing approach from (Ethayarajh et al., 2019) may be improved by using a different direction specification approach. Therefore, we consider two additional word position measurement models, Ethayarajh et al. (2019) ", "filtered_refids": [["b12", null, "b8", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1280, "num_references": 4}
{"corpusid_sectionid": "216552915-s23", "title": "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?", "date": "2020-04-25", "section_title": "D.1 Measurement", "section": "The slider bar for the affective dimensions gives labels at different points, ranging from \"Infinitely\" to \"Slightly\" on both ends, with a \"Neutral\" option  Table 3: Details of the prior work on word position measurement models from which we draw. We use each model listed here, as well as using the approach of Ethayarajh et al. (2019) but using direction specification as described by Garg et al. (2018) and Kozlowski et al. (2019). in the middle. The age slider had the following qualitative labels, spaced equally across the slider bar: \"Baby, Child, Teenager, 20s, 30s, 40s, 50s, 60s, 70s, 80s, 90s, >=100\". The gender slider had the following labels, spaced equally across the slider: \"Always Male, Mostly Male, Equally Male or Female, Mostly Female, Always Female\".\n\nFor the race/ethnicity beliefs, order of the sliders was randomized, and the starting value for each was set to 20%. With respect to discussions about the 2020 census, most importantly, demographers have pushed to include Hispanic or Latino as a racial category rather than to split it out into its own separate question.\n\nFor the associative belief question, presentation order was randomized. The form of the question is drawn from other studies seeking to elicit cognitive associations between a term and a set of other concepts, e.g. from Hill et al. (2015). The specific institutions were originally drawn from the clustering used to determine our identities. However, we added the education and religion institutions after determining they would be necessary for a more complete meaning space, as suggested by the institutional settings with which identities are commonly associated as discussed by MacKinnon and Heise (2010).\n\nFinally, pilot testing suggested that respondents became confused when provided with certain identities that had meanings that were used to construct the question -for example, on the race question, respondents became confused when being asked \"Of all white people, what percentage of them do you think are ... [White]?\" We, therefore, removed the gender question from the identities guy, boy, girl, lady, man, and woman, and removed the race question from the identities Asian, White person, Black person, Arab, and Hispanic.", "filtered_refids": [[null, "b2"], [], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2234, "num_references": 4}
{"corpusid_sectionid": "218487374-s1", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "date": "2020-05-01", "section_title": "Applications", "section": "In Table 1, we gather and summarize applications that use text to adjust for potential confounding. This encompasses both (a) text as a surrogate for confounders, or (b) the language itself as confounders. 2 As an example, consider Kiciman et al. (2018) where the goal is to estimate the size of the causal effect of alcohol use (treatment) on academic success (outcome) for college students. Since randomly assigning college students to binge drink is not feasible or ethical, the study instead uses observational data from Twitter, which also has the advantage of a large sample size of over sixty-three thousand students. They use heuristics to identify  the Twitter accounts of college-age students and extract alcohol mentions and indicators of college success (e.g., study habits, risky behaviors, and emotions) from their Twitter posts. They condition on an individual's previous posts (temporally previous to measurements of treatment and outcome) as confounding variables since they do not have demographic data. They represent text as word counts and use stratified propensity score matching to adjust for the confounding bias. The study finds the effects of alcohol use include decreased mentions of study habits and positive emotions and increased mentions of potentially risky behaviors.\n\nText as a surrogate for confounders. Traditionally, causal research that uses human subjects as the unit of analysis would infer demographics via surveys. However, with the proliferation of the web and social media, social research now includes large-scale observational data that would be challenging to obtain using surveys (Salganik, 2017). This type of data typically lacks demographic information but may contain large amounts of text written by participants from which demographics can be extracted. In this space, some researchers are specific about the confounders they want to extract such as an individual's ideology (Sridhar and Getoor, 2019) or mood (Sridhar et al., 2018). Other researchers condition on all the text they have avail-able and assume that low-dimensional summaries capture all possible confounders. For example, researchers might assume that text encodes all possible confounders between alcohol use and college success (Kiciman et al., 2018) or psychiatric medication and anxiety (Saha et al., 2019). We dissect and comment on this assumption in Section 8.\n\nOpen problems: NLP systems have been shown to be inaccurate for low-resource languages (Duong et al., 2015), and exhibit racial and gender disparity (Blodgett and O'Connor, 2017;Zhao et al., 2017). Furthermore, the ethics of predicting psychological indicators, such as mental health status, from text are questionable (Chancellor et al., 2019). It is unclear how to mitigate these disparities when trying to condition on demographics from text and how NLP errors will propagate to causal estimates.\n\nLanguage as confounders. There is growing interest in measuring language itself (e.g. the sentiment or topical content of text) as causal confounders. For example, Roberts et al. (2020) examine how the perceived gender of an author affects the number of citations that an article receives. However, an article's topics (the confounders) are likely to influence the perceived gender of its author (reflecting an expectation that women write about certain topics) and the number of citations of that article (\"hotter\" topics will receive more Figure 2: This chart is a guide to design decisions for applied research with causal confounders from text.\n\nStep 1: Encode domain assumptions by drawing a causal diagram ( \u00a73). If the application does not use text to measure latent confounders, the causal effects are not identifiable or the application is outside the scope of this review.\n\nStep 2: Use NLP to measure confounders from text ( \u00a74).\n\nStep 3: Choose a method that adjusts for confounding in causal estimates ( \u00a75). Evaluation should include (A) sensitivity analysis ( \u00a74), (B) human evaluation of adjustments when appropriate ( \u00a76), and (C) evaluation of recovering the true causal effects ( \u00a77). citations). Other domains that analyze language as a confounder include news (Johansson et al., 2016), social media (De Choudhury et al., 2016;Olteanu et al., 2017), and loan descriptions (Pham and Shen, 2017). See Section 4 for more discussion on the challenges and open problems of inferring these latent aspects of language.", "filtered_refids": [[null, "b31"], ["b67", "b68", "b31", "b72", "b73"], ["b11", null, "b8", "b82"], [], [], [], [null, "b29", "b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 4420, "num_references": 14}
{"corpusid_sectionid": "218487374-s4", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "date": "2020-05-01", "section_title": "Structural causal models framework", "section": "Structural causal models (SCMs) use a graphical formalism that depicts nodes as random variables and directed edges as the direct causal dependence between these variables. The typical estimand of choice for SCMs is the probability distribution of an outcome variable Y given an intervention on a treatment variable T :\n\nin which the do-notation represents intervening to set variable T to the value t and thereby removing all incoming arrows to the variable T .\n\nIdentification. In most cases, Equation 2 is not equal to the ordinary conditional distribution P (Y | T = t) since the latter is simply filtering to the sub-population and the former is changing the underlying data distribution via intervention. Thus, for observational studies that lack intervention, one needs an identification strategy in order to represent P (Y | do(T = t)) in terms of distributions of observed variables. One such identification strategy (assumed by the applications throughout this review) is the backdoor criterion which applies to a set of variables, S, if they (i) block every backdoor path between treatment and outcome, and (ii) no node in S is a descendant of treatment. Without positive identification, the causal effects cannot be estimated and measuring variables from text is a secondary concern.\n\nDrawing the causal graph. Causal graphs help clarify which variables should and should not be conditioned on. The causal graphs in Figure 3 illustrate how the direction of the arrows differentiates confounder, collider, and mediator variables. Identifying the differences in these variables is crucial since, by d-separation, conditioning on a confounder will block the treatment-confounderoutcome path, removing bias. By contrast, conditioning on a collider can create dependence between treatment-collider-outcome 5 (Pearl, 2009a) potentially introducing more bias (Montgomery et al., 2018;Elwert and Winship, 2014). Mediator variables require a different set of adjustments than confounders to find the \"natural direct effect\" between treatment and outcome (VanderWeele, 2015;Pearl, 2014). A practitioner typically draws a causal graph by explicitly encoding theoretical and domain assumptions as well as the results of prior 5 In Pearl et al. (2016)'s example of a collider, suppose scholarships at a college are only given to two types of students: those with unusual musical talents and high grade point averages. In the general population, musical and academic talent are independent. However, if one discovers a person is on a scholarship (conditioning on the collider) then knowing a person lacks musical talent tells us that they are extremely likely to have a high GPA. data analyses. 6 Open Problems: When could text potentially encode confounders and colliders simultaneously? If so, is it possible to use text to adjust exclusively for confounders?", "filtered_refids": [[], [], [], ["b13", "b41", "b50", "b77", "b48", null, "b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2859, "num_references": 7}
{"corpusid_sectionid": "218487374-s5", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "date": "2020-05-01", "section_title": "Measuring confounders via text", "section": "After drawing the causal graph, the next step is to use available text data to recover latent confounders. Some approaches pre-specify the confounders of interest and measure them from text, P (z | x). Others learn confounders inductively and use a low-dimensional representation of text as the confounding variable z in subsequent causal adjustments.\n\nPre-specified confounders. When a practitioner can specify confounders they want to measure from text (e.g., extracting \"occupation\" from text in our smoking example), they can use either (1) lexicons or (2) 2019) also build machine learning classifiers for users' mental states (e.g., depression and anxiety) and apply these classifiers on Twitter posts that are temporally prior to treatment. If these classifiers accurately recover mental states and there are no additional latent confounders, then conditioning on the measured mental states renders treatment independent of potential outcomes.\n\nOpen problems: Since NLP methods are still far from perfectly accurate, how can one mitigate error that arises from approximating confounding variables? Closely related to this question is effect restoration which addresses error from using proxy variables (e.g., a father's occupation) in place of true confounders (e.g, socioeconomic status) (Kuroki and Pearl, 2014;Oktay et al., 2019). Wood-Doughty et al. (2018) build upon effect restoration for causal inference with text classifiers, but there are still open problems in accounting for error arising from other text representations and issues of calibration (Nguyen and O'Connor, 2015) and prevalence estimation (Card and Smith, 2018; Keith and O'Connor, 2018) in conjunction with NLP. Ideas from the large literature on measurement error models may also be helpful (Fuller, 1987;Carroll et al., 2006;Buonaccorsi, 2010).\n\nInductively derived confounders. Other researchers inductively learn confounders in order to condition on all aspects of text, known and unknown. For example, some applications condition on the entirety of news (Johansson et al., 2016) or scientific articles (Veitch et al., 2019;Roberts et al., 2020). This approach typically summarizes textual information with text representations common in NLP. Ideally, this would encode all aspects of language (meaning, topic, style, affect, etc.), though this is an extremely difficult, open NLP problem. Typical approaches include the following. (1) Bag-of-words representations discard word order and use word counts as representations. (2) Topic models are generative probabilistic models that learn latent topics in document collections and represent documents as distributions over topics (Blei et al., 2003;Boyd-Graber et al., 2014;Roberts et al., 2014). (3) Embeddings are continuous, vector-based representations of text. To create vector representations of longer texts, off-the-shelf word embeddings such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) or combined via variants of weighted averaging (Arora et al., 2017) or neural models (Iyyer et al., 2015;Bojanowski et al., 2017;Yang et al., 2016). (4) Recently, fine-tuned, large-scale neural language models such as BERT (Devlin et al., 2019) have achieved state-of-the-art performance on semantic benchmarks, and are now used as text representations. Each of these text representations is a real-valued vector that is used in place of the confounder, z, in a causal adjustment method ( \u00a75)\n\nOpen problems: Estimates of causal effects are contingent on the \"garden of forking paths\" of data analysis, meaning any \"paths\" an analyst did not take could have resulted in different conclusions (Gelman and Loken, 2013). For settings with causal confounders from text, the first fork is the choice of representation (e.g., topic models or embeddings) and the second fork is the pre-processing and hyperparameter decisions for the chosen representations.\n\nWe highlight that these decisions have been shown to alter results in predictive tasks. For instance, studies have shown that pre-processing decisions dramatically change topic models (Denny and Spirling, 2018;Schofield et al., 2017); embeddings are sensitive to hyperparameter tuning (Levy et al., 2015) and the construction of the training corpus (Antoniak and Mimno, 2018); and fine-tuned language model performance is sensitive to random restarts (Phang et al., 2018). Thus, reporting sensitivity analysis of the causal effects from these decisions seems crucial: how robust are the results to variations in modeling specifications?", "filtered_refids": [[], [], ["b44", "b32", "b45", "b80", "b30", null, "b16"], ["b60", "b40", "b78", "b9", "b29", "b27", null, "b7", "b81", "b53", "b61", "b2"], ["b17"], ["b35", "b55", null, "b70", "b1"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4544, "num_references": 25}
{"corpusid_sectionid": "218487374-s6", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "date": "2020-05-01", "section_title": "Adjusting for confounding bias", "section": "Given a set of variables Z that satisfy the backdoor criterion ( \u00a73.2), one can use the backdoor adjustment to estimate the causal quantity of interest,\n\nConditioning on all confounders is often impractical in high-dimensional settings such as those found in natural language. We provide an overview of methods used by applications in this review that approximate such conditioning, leading to unbiased estimates of treatment effect; however, we acknowledge this is not an exhaustive list of methods and direct readers to more extensive guides (Morgan and Winship, 2015;Athey et al., 2017).\n\nOpen problems: Causal studies typically make an assumption of overlap, also known as common support or positivity, meaning that any individual has a non-zero probability of assignment to each treatment condition for all possible values of the covariates: \u2200z, 0 < P (T = 1 | Z = z) < 1. D' Amour et al. (2017) show that as the dimensionality of covariates grows, strict overlap converges to zero. What are the implications of these results for high-dimensional text data?", "filtered_refids": [[], ["b42", "b3"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1062, "num_references": 3}
{"corpusid_sectionid": "218487374-s8", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "date": "2020-05-01", "section_title": "Matching and stratification", "section": "Matching aims to create treatment and control groups with similar confounder assignments; for example, grouping units by observed variables (e.g., age, gender, occupation), then estimating effect size within each stratum (Stuart, 2010). Exact matching on confounders is ideal but nearly impossible to obtain with high-dimensional confounders, including those from text. A framework for matching with text data is described by Mozer et al. (2020) and requires choosing: (1) a text representation ( \u00a74); (2) a distance metric (cosine, Eucliean, absolute difference in propensity score etc.); and (3) a matching algorithm. As Stuart (2010) describes, the matching algorithm involves additional decisions about (a) greedy vs. optimal matching; (b) number of control items per treatment item; (c) using calipers (thresholds of maximum distance); and (d) matching with or without replacement. Coarsened exact matching (CEM) matches on discretized raw values of the observed confounders (Iacus et al., 2012). Instead of directly matching on observed variables, stratified propensity-score matching partitions propensity scores into intervals (strata) and then all units are compared within a single strata (Caliendo and Kopeinig, 2008). Stratification is also known as interval matching, blocking, and subclassification.\n\nOnce the matching algorithm is implemented, counterfactuals (estimated potential outcomes) are obtained from the matches M i for each unit i: which is plugged into the matching estimator, 8\n\nOpen problems: Ho et al. (2007) describe matching as a method to reduce model dependence because, unlike regression, it does not rely on a parameteric form. Yet, estimated causal effects may still be sensitive to other matching method decisions such as the number of bins in coarsened exact matching, the number of controls to match with each treatment in the matching algorithm, or the choice of caliper. Are causal estimates made using textual covariates particularly sensitive or robust to such choices?", "filtered_refids": [["b43", "b74", null, "b24"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2012, "num_references": 4}
{"corpusid_sectionid": "218487374-s14", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "date": "2020-05-01", "section_title": "Judgements of treatment propensity", "section": "When possible, one can also improve validation by evaluating matched items (posts, sentences, documents etc.) to humans for evaluation. Humans can either (a) use a scale (e.g., a 1-5 Likert scale) to rate items individually on their propensity for treatment, or (b) assess similarity of paired items after matching. A simple first step is for analysts to do \"inhouse\" evaluation on a small sample (e.g., Roberts et al. (2020)), but a larger-sample experiments on crowd-working platforms can also increase the validity of these methods (e.g., Mozer et al. (2020)).\n\nOpen problems: How can these human judgement experiments be improved and standardized? Future work could draw from a rich history in NLP of evaluating representations of topic models and embeddings (Wallach et al., 2009;Bojanowski et al., 2017;Schnabel et al., 2015) and evaluating semantic similarity (Cer et al., 2017;Bojanowski et al., 2017;Reimers and Gurevych, 2019).", "filtered_refids": [["b43", "b60"], ["b69", null, "b58", "b79"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 937, "num_references": 6}
{"corpusid_sectionid": "218487374-s16", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "date": "2020-05-01", "section_title": "Constructed observational studies", "section": "Constructed observational studies collect data from both randomized and non-randomized experiments with similar participants and settings. Evaluations of this kind include job training programs in economics (LaLonde, 1986;Glynn and Kashin, 2013), advertisement marketing campaigns (Gordon et al., 2019), and education (Shadish et al., 2008). For instance, Shadish et al. (2008) randomly assign participants to a randomized treatment (math or vocabulary training) and non-randomized treatment (participants choose their own training). They compare causal effect estimates from the randomized study with observational estimates that condition on confounders from participant surveys (e.g., sex, age, marital status, like of mathematics, extroversion, etc.).\n\nOpen problems: To extend constructed observational studies to text data, one could build upon Shadish et al. (2008) and additionally (a) ask participants to write free-form essays of their past educational and childhood experiences and/or (b) obtain participants' public social media posts. Then causal estimates that condition on these textual representation of confounders could be compared to both those with surveys and the randomized settings. Alternatively, one could find observational studies with both real covariates and text and (1) randomize treatment conditional on the propensity score model (constructed from the covariates but not the text) and (2) estimate causal effect given only text (not the covariates). Then any estimated non-zero treatment effect is only bias.", "filtered_refids": [["b19", "b34", "b20", "b71"], ["b71"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1541, "num_references": 5}
{"corpusid_sectionid": "218487374-s17", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "date": "2020-05-01", "section_title": "Semi-synthetic datasets", "section": "Semi-synthetic datasets use real covariates and synthetically generate treatment and outcome, as in the 2016 Atlantic Causal Inference Competition (Dorie et al., 2019). Several applications in this review use real metadata or latent aspects of text to simulate treatment and outcome: Johansson et al. (2016) simulate treatment and outcome from two centroids in topic model space from newswire text; Veitch et al. (2019) use indicators of an article's \"buzzy\" keywords; Roberts et al. (2020) use \"quantitative methodology\" categories of articles that were hand-coded by other researchers.\n\nOpen problems: Semi-synthetic datasets that use real covariates of text seem to be a better evaluation strategy than purely synthetic datasets. However, with semi-synthetic datasets, researchers could be inadvertently biased to choose metadata that they know their method will recover. A promising future direction is a competition-style evaluation like Dorie et al. (2019) in which one group of researchers generates a causal dataset with text as a confounder and other groups of researchers evaluate their causal methods without access to the data-generating process.", "filtered_refids": [["b78", "b10", "b29"], ["b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1158, "num_references": 4}
{"corpusid_sectionid": "218971825-s5", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "date": "2020-05-28", "section_title": "Techniques", "section": "Papers' techniques are not well grounded in the relevant literature outside of NLP. Perhaps unsurprisingly given that the papers' motivations are often vague, inconsistent, and lacking in normative reasoning, we also found that the papers' proposed quantitative techniques for measuring or mitigating \"bias\" do not effectively engage with the relevant literature outside of NLP. Papers on stereotyping are a notable exception: the Word Embedding Association Test (Caliskan et al., 2017) draws on the Implicit Association Test (Greenwald et al., 1998) from the social psychology literature, while several techniques operationalize the well-studied \"Angry Black Woman\" stereotype Tan and Celis, 2019) and the \"double bind\" faced by women Tan and Celis, 2019), in which women who succeed at stereotypically male tasks are perceived to be less likable than similarly successful men (Heilman et al., 2004). Tan and Celis (2019) also examine the compounding effects of race and gender, drawing on Black feminist scholarship on intersectionality (Crenshaw, 1989).\n\nPapers' techniques are poorly matched to their motivations. We found that although 21% of the papers include allocational harms in their motivations, only four papers actually propose techniques for measuring or mitigating allocational harms.\n\nPapers focus on a narrow range of potential sources of \"bias.\" We found that nearly all of the papers focus on system predictions as the potential sources of \"bias,\" with many additionally focusing on \"bias\" in datasets (e.g., differences in the number of gendered pronouns in the training data ). Most papers do not interrogate the normative implications of other decisions made during the development and deployment lifecycleperhaps unsurprising given that their motivations sometimes include no normative reasoning. A few papers are exceptions, illustrating the impacts of task defnitions, annotation guidelines, and evaluation metrics: Cao and Daum\u00e9 (2019) study how folk conceptions of gender (Keyes, 2018) are reproduced in coreference resolution systems that assume a strict gender dichotomy, thereby maintaining cisnormativity;  focus on the effect of priming annotators with information about possible dialectal differences when asking them to apply toxicity labels to sample tweets, fnding that annotators who are primed are signifcantly less likely to label tweets containing features associated with African-American English as offensive.", "filtered_refids": [["b250", null, "b283"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2452, "num_references": 3}
{"corpusid_sectionid": "218971825-s7", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "date": "2020-05-28", "section_title": "Language and social hierarchies", "section": "Turning frst to (R1), we argue that work analyzing \"bias\" in NLP systems will paint a much fuller picture if it engages with the relevant literature outside of NLP that explores the relationships between language and social hierarchies. Many disciplines, including sociolinguistics, linguistic anthropology, sociology, and social psychology, study how language takes on social meaning and the role that language plays in maintaining social hierarchies. For example, language is the means through which social groups are labeled and one way that beliefs about social groups are transmitted (e.g., Maass, 1999;Beukeboom and Burgers, 2019). Group labels can serve as the basis of stereotypes and thus reinforce social inequalities: \"[T]he label content functions to identify a given category of people, and thereby conveys category boundaries and a position in a hierarchical taxonomy\" (Beukeboom and Burgers, 2019). Similarly, \"controlling images,\" such as stereotypes of Black women, which are linguistically and visually transmitted through literature, news media, television, and so forth, provide \"ideological justifcation\" for their continued oppression (Collins, 2000, Chapter 4).\n\nAs a result, many groups have sought to bring about social changes through changes in language, disrupting patterns of oppression and marginalization via so-called \"gender-fair\" language (Sczesny et al., 2016;Menegatti and Rubini, 2017), language that is more inclusive to people with disabilities (ADA, 2018), and language that is less dehumanizing (e.g., abandoning the use of the term \"illegal\" in everyday discourse on immigration in the U.S. (Rosa, 2019)). The fact that group labels are so contested is evidence of how deeply intertwined language and social hierarchies are. Taking \"gender-fair\" language as an example, the hope is that reducing asymmetries in language about women and men will reduce asymmetries in their social standing. Meanwhile, struggles over language use often arise from dominant social groups' desire to \"control both material and symbolic resources\"-i.e., \"the right to decide what words will mean and to control those meanings\"-as was the case in some white speakers' insistence on using offensive place names against the objections of Indigenous speakers (Hill, 2008, Chapter 3).\n\nSociolinguists and linguistic anthropologists have also examined language attitudes and language ideologies, or people's metalinguistic beliefs about language: Which language varieties or practices are taken as standard, ordinary, or unmarked? Which are considered correct, prestigious, or appropriate for public use, and which are considered incorrect, uneducated, or offensive (e.g., Campbell-Kibler, 2009;Preston, 2009;Loudermilk, 2015;Lanehart and Malik, 2018)? Which are rendered invisible (Roche, 2019)? 3 Language ideologies play a vital role in reinforcing and justifying social hierarchies because beliefs about language varieties or practices often translate into beliefs about their speakers (e.g. Alim et al., 2016;Rosa and Flores, 2017;Craft et al., 2020). For example, in the U.S., the portrayal of non-white speakers' language varieties and practices as linguistically defcient helped to justify violent European colonialism, and today continues to justify enduring racial hierarchies by maintaining views of non-white speakers as lacking the language \"required for complex thinking processes and successful engagement in the global economy\" (Rosa and Flores, 2017).\n\nRecognizing the role that language plays in maintaining social hierarchies is critical to the future of work analyzing \"bias\" in NLP systems. First, it helps to explain why representational harms are harmful in their own right. Second, the complexity of the relationships between language and social hierarchies illustrates why studying \"bias\" in NLP systems is so challenging, suggesting that researchers and practitioners will need to move beyond existing algorithmic fairness techniques. We argue that work must be grounded in the relevant literature outside of NLP that examines the relationships between language and social hierarchies; without this grounding, researchers and practitioners risk measuring or mitigating only what is convenient to measure or mitigate, rather than what is most normatively concerning.\n\nMore specifcally, we recommend that work analyzing \"bias\" in NLP systems be reoriented around the following question: How are social hierarchies, language ideologies, and NLP systems coproduced? This question mirrors Benjamin's (2020) call to examine how \"race and technology are coproduced\"-i.e., how racial hierarchies, and the ideologies and discourses that maintain them, create and are re-created by technology. We recommend that researchers and practitioners similarly ask how existing social hierarchies and language ideologies drive the development and deployment of NLP systems, and how these systems therefore reproduce these hierarchies and ideologies. As a starting point for reorienting work analyzing \"bias\" in NLP systems around this question, we provide the following concrete research questions:\n\n.  (Olteanu et al., 2017)? Are any non-quantitative evaluations performed? . How do NLP systems reproduce or transform language ideologies? Which language varieties or practices come to be deemed good or bad? Might \"good\" language simply mean language that is easily handled by existing NLP systems? For example, linguistic phenomena arising from many language practices (Eisenstein, 2013) are described as \"noisy text\" and often viewed as a target for \"normalization.\" How do the language ideologies that are reproduced by NLP systems maintain social hierarchies? . Which representational harms are being measured or mitigated? Are these the most normatively concerning harms, or merely those that are well handled by existing algorithmic fairness techniques? Are there other representational harms that might be analyzed?", "filtered_refids": [[null], ["b30", "b1", "b47"], [null, "b28", "b20", "b32"], [], [], ["b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 31, "num_chars": 5945, "num_references": 9}
{"corpusid_sectionid": "218971825-s9", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "date": "2020-05-28", "section_title": "Language use in practice", "section": "Finally, we turn to (R3). Our perspective, which rests on a greater recognition of the relationships between language and social hierarchies, suggests several directions for examining language use in practice. Here, we focus on two. First, because language is necessarily situated, and because different social groups have different lived experiences due to their different social positions (Hanna et al., 2020)-particularly groups at the intersections of multiple axes of oppression-we recommend that researchers and practitioners center work analyzing \"bias\" in NLP systems around the lived experiences of members of communities affected by these systems. Second, we recommend that the power relations between technologists and such communities be interrogated and reimagined. Researchers have pointed out that algorithmic fairness techniques, by proposing incremental technical mitigations-e.g., collecting new datasets or training better models-maintain these power relations by (a) assuming that automated systems should continue to exist, rather than asking whether they should be built at all, and ( There are many disciplines for researchers and practitioners to draw on when pursuing these directions. For example, in human-computer interaction, Hamidi et al. (2018) study transgender people's experiences with automated gender recognition systems in order to uncover how these systems reproduce structures of transgender exclusion by redefning what it means to perform gender \"normally.\" Value-sensitive design provides a framework for accounting for the values of different stakeholders in the design of technology (e.g., Friedman et al., 2006;Friedman and Hendry, 2019;Le Dantec et al., 2009;Yoo et al., 2019), while participatory design seeks to involve stakeholders in the design process itself (Sanders, 2002;Muller, 2007;Simonsen and Robertson, 2013;DiSalvo et al., 2013). Participatory action research in education (Kemmis, 2006) and in language documentation and reclamation (Junker, 2018) is also relevant. In particular, work on language reclamation to support decolonization and tribal sovereignty (Leonard, 2012) and work in sociolinguistics focus-ing on developing co-equal research relationships with community members and supporting linguistic justice efforts (e.g., Bucholtz et al., 2014Bucholtz et al., , 2016Bucholtz et al., , 2019 provide examples of more emancipatory relationships with communities. Finally, several workshops and events have begun to explore how to empower stakeholders in the development and deployment of technology (Vaccaro et al., 2019;Givens and Morris, 2020;Sassaman et al., 2020) 4 and how to help researchers and practitioners consider when not to build systems at all (Barocas et al., 2020).\n\nAs a starting point for engaging with communities affected by NLP systems, we therefore provide the following concrete research questions:\n\n. How do communities become aware of NLP systems? Do they resist them, and if so, how? . What additional costs are borne by communities for whom NLP systems do not work well? . Do NLP systems shift power toward oppressive institutions (e.g., by enabling predictions that communities do not want made, linguistically based unfair allocation of resources or opportunities (Rosa and Flores, 2017), surveillance, or censorship), or away from such institutions? . Who is involved in the development and deployment of NLP systems? How do decision-making processes maintain power relations between technologists and communities affected by NLP systems? Can these processes be changed to reimagine these relations?", "filtered_refids": [["b44", "b4", "b40", null, "b81", "b68"], [], ["b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 3596, "num_references": 7}
{"corpusid_sectionid": "218971825-s10", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "date": "2020-05-28", "section_title": "Case study", "section": "To illustrate our recommendations, we present a case study covering work on African-American English (AAE). 5 Work analyzing \"bias\" in the context of AAE has shown that part-of-speech taggers, language identifcation systems, and dependency parsers all work less well on text containing features associated with AAE than on text without these features (J\u00f8rgensen et al., , 2016, and that toxicity detection systems score tweets containing features associated with AAE as more offensive than tweets without them .\n\nThese papers have been critical for highlighting AAE as a language variety for which existing NLP systems may not work, illustrating their limitations. However, they do not conceptualize \"racial bias\" in the same way. The frst four of these papers simply focus on system performance differences between text containing features associated with AAE and text without these features. In contrast, the last two papers also focus on such system performance differences, but motivate this focus with the following additional reasoning: If tweets containing features associated with AAE are scored as more offensive than tweets without these features, then this might (a) yield negative perceptions of AAE; (b) result in disproportionate removal of tweets containing these features, impeding participation in online platforms and reducing the space available online in which speakers can use AAE freely; and (c) cause AAE speakers to incur additional costs if they have to change their language practices to avoid negative perceptions or tweet removal.\n\nMore importantly, none of these papers engage with the literature on AAE, racial hierarchies in the U.S., and raciolinguistic ideologies. By failing to engage with this literature-thereby treating AAE simply as one of many non-Penn Treebank varieties of English or perhaps as another challenging domain-work analyzing \"bias\" in NLP systems in the context of AAE fails to situate these systems in the world. Who are the speakers of AAE? How are they viewed? We argue that AAE as a language variety cannot be separated from its speakersprimarily Black people in the U.S., who experience systemic anti-Black racism-and the language ideologies that reinforce and justify racial hierarchies.\n\nEven after decades of sociolinguistic efforts to legitimize AAE, it continues to be viewed as \"bad\" English and its speakers continue to be viewed as linguistically inadequate-a view called the defcit perspective (Alim et al., 2016;Rosa and Flores, 2017). This perspective persists despite demonstrations that AAE is rule-bound and grammatical (Mufwene et al., 1998;Green, 2002), in addition to ample evidence of its speakers' linguistic adroitness (e.g., Alim, 2004;Rickford and King, 2016). This perspective belongs to a broader set of raciolinguistic ideologies (Rosa and Flores, 2017), which also produce allocational harms; speakers of AAE are frequently penalized for not adhering to dominant language practices, including in the education system (Alim, 2004;Terry et al., 2010), when seeking housing (Baugh, 2018), and in the judicial system, where their testimony is misunderstood or, worse yet, disbelieved (Rickford and King, 2016;Jones et al., 2019). These raciolinguistic ideologies position racialized communities as needing linguistic intervention, such as language education programs, in which these and other harms can be reduced if communities accommodate to dominant language practices (Rosa and Flores, 2017).\n\nIn the technology industry, speakers of AAE are often not considered consumers who matter. For example, Benjamin (2019) recounts an Apple employee who worked on speech recognition for Siri: \"As they worked on different English dialects -Australian, Singaporean, and Indian English -[the employee] asked his boss: 'What about African American English?' To this his boss responded: 'Well, Apple products are for the premium market.\"'\n\nThe reality, of course, is that speakers of AAE tend not to represent the \"premium market\" precisely because of institutions and policies that help to maintain racial hierarchies by systematically denying them the opportunities to develop wealth that are available to white Americans (Rothstein, 2017)an exclusion that is reproduced in technology by countless decisions like the one described above.\n\nEngaging with the literature outlined above situates the system behaviors that are described as \"bias,\" providing a foundation for normative reasoning. Researchers and practitioners should be concerned about \"racial bias\" in toxicity detection systems not only because performance differences impair system performance, but because they reproduce longstanding injustices of stigmatization and disenfranchisement for speakers of AAE. In re-stigmatizing AAE, they reproduce language ideologies in which AAE is viewed as ungrammatical, uneducated, and offensive. These ideologies, in turn, enable linguistic discrimination and justify enduring racial hierarchies (Rosa and Flores, 2017). Our perspective, which understands racial hierarchies and raciolinguistic ideologies as structural conditions that govern the development and deployment of technology, implies that techniques for measuring or mitigating \"bias\" in NLP systems will necessarily be incomplete unless they interrogate and dismantle these structural conditions, including the power relations between technologists and racialized communities.\n\nWe emphasize that engaging with the literature on AAE, racial hierarchies in the U.S., and raciolinguistic ideologies can generate new lines of engagement. These lines include work on the ways that the decisions made during the development and deployment of NLP systems produce stigmatization and disenfranchisement, and work on AAE use in practice, such as the ways that speakers of AAE interact with NLP systems that were not designed for them. This literature can also help researchers and practitioners address the allocational harms that may be produced by NLP systems, and ensure that even well-intentioned NLP systems do not position racialized communities as needing linguistic intervention or accommodation to dominant language practices. Finally, researchers and practitioners wishing to design better systems can also draw on a growing body of work on anti-racist language pedagogy that challenges the defcit perspective of AAE and other racialized language practices (e.g. Flores and Chaparro, 2018; Baker-Bell, 2019; Mart\u00ednez and Mej\u00eda, 2019), as well as the work that we described in section 4.3 on reimagining the power relations between technologists and communities affected by technology.", "filtered_refids": [[null], [], [], ["b26", "b32", null, "b65", "b3"], [], ["b35"], ["b32"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 6624, "num_references": 8}
{"corpusid_sectionid": "253447259-s1", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "The Task", "section": "The definition of a grammatical error is surprisingly difficult. Some types of spelling errors (such as accomodation with a single m) are about equally distributed between native and non-native writers and have no grammatical reflexes, so could be reasonably excluded. Others, such as he eated, are boundary cases as they result from over-regularisation of morphology, whilst he would eated is clearly ungrammatical in the context of a modal auxiliary verb. At the interpretative boundary, infelicitous discourse organisation, such as Kim fell. Sandy pushed him. where the intention is to explain why Kim fell, is not obviously a grammatical error per se but nevertheless can be 'corrected' via a tense change (Sandy had pushed him.) as opposed to a reordering of the sentences. Other tense changes which can span sentences appear more grammatical, such as Kim will We met they talked and left We met, they talked and left Unidiomatic\n\nWe had a big conversation We had a long conversation Multiple I sea the see from the seasoar I saw the sea from the seesaw Table 1 Example error types make Sandy a sandwich. Sandy ate it., as the discourse is incoherent and correction will require a tense change in one or other sentence. In practice, the task has increasingly been defined in terms of what corrections are annotated in corpora used for the shared tasks. These use a variety of annotation schemes but all tend to adopt minimal modifications of errorful texts to create errorfree text with the same perceived meaning. Other sources of annotated data, such as that sourced from the online language learning platform Lang-8 (Mizumoto et al. 2012;Tajiri, Komachi, and Matsumoto 2012), often contain much more extensive rewrites of entire paragraphs of text. Given this resource-derived definition of the task, systems are evaluated on their ability to correct all kinds of mistakes in text, including spelling and discourse level errors that have no or little grammatical reflex. The term 'Grammatical' Error Correction is thus something of a misnomer, but is nevertheless now commonly understood to encompass errors that are not always strictly grammatical in nature. A more descriptive term is Language Error Correction. Table 1 provides a small sample of (constructed) examples that illustrate the range of errors to be corrected and some of the issues that arise with the precise definition and evaluation of the task. Errors can be classified into three broad categories: replacement errors, such as dreamed for dreamt in the second example; omission errors, such as on in the first example; and insertion errors, such as the in the third example. Some errors are complex in the sense that their correction requires a sequence of replacement, omission or insertion steps to correct, as with the syntax example. Sentences may also contain multiple distinct errors that require a sequence of corrections, as in the multiple example. Both the classification and specification of correction steps for errors can be and has been achieved using different schemes and approaches. For instance, correction of the syntax example involves transposing two adjacent words so we could introduce a fourth broad class and correction step of transposition (word order). All extant annotation schemes break these broad classes down into further subclasses based on the part-ofspeech of the words involved, and perceived morphological, lexical, syntactic, semantic or pragmatic source of the error. The schemes vary in the number of such distinctions, ranging from just over two dozen (NUCLE: (Dahlmeier, Ng, and Wu 2013)) to almost a hundred (CLC: (Nicholls 2003)). The schemes also identify different error spans in source sentences and thus suggest different sets of edit operations to obtain the suggested corrections. For instance, the agreement error example might be annotated as She likes him and [kiss \u2192 kisses] him at the token level or simply [\u01eb \u2192 es] at the character level. These differing annotation decisions affected the evaluation of system performance in artefactual ways, so a two-stage automatic standardisation process was developed, ERRANT (Felice, Bryant, and Briscoe 2016;Bryant, Felice, and Briscoe 2017), which maps parallel errorful and corrected sentence pairs to a single annotation scheme using a linguistically-enhanced alignment algorithm and series of error type classification rules. This scheme uses 25 main error type categories, based primarily on part-of-speech and morphology, which are further subdivided into missing (omission), unnecessary (insertion) and replacement errors. This approach allows consistent automated training and evaluation of systems on any or all parallel corpora as well as supporting a more fined-grained analysis of the strengths and weaknesses of systems in terms of different error types.\n\nUltimately however, the correction of errors requires an understanding of the communicative intention of the writer. For instance, the determiner example in Table 1 implicitly assumes a 'neutral' context where the intent is to make a statement about generic icecream rather than a specific instance. In a context where, say, a specific ice-cream dessert is being compared to an alternative dessert, then the determiner is felicitous. Similarly the preposition omission error might not be an error if the writer is describing a context in which a talk was oversubscribed and many attendees had to stand because of a lack of seats. Though annotators will most likely take both the context and perceived writer's intention into account when identifying errors, GEC itself is instead often framed as an isolated sentence-based task that ignores the wider context. This can introduce noise in the task in that errorful sequences in context may appear correct in isolation out of context. A related issue is that correction may not only depend on communicative intent, but also factors such as dialect and genre. For example, correcting dreamed to dreamt may be appropriate if the target is British English, but incorrect for American English.\n\nA larger issue arises with differing possibilities for correction. For example, correcting the tense/aspect example to kissing or to kiss in the context of likes seems equally correct. However, few existing corpora provide more than one possibility which means the true performance of systems is often underestimated. However, the same two corrections are not equally correct as complements of a verb such as try depending on whether the context implies that a kissing event occurred or not. The issue of multiple possible corrections arises with many if not most examples: for instance I haven't the book, We met them, talked and left, We had an important conversation, The sea I see from the seesaw (is calm) are all plausible alternative corrections for some of the examples in Table 1. For this reason, several of the shared tasks have also evaluated performance on grammatical error detection, as this is valuable in some applications. Recently, some work has explored treating the GEC task as one of document-level correction (e.g. Chollampatt, Wang, and Ng (2019); ) which, in principle, could ameliorate some of these issues but is currently hampered by a lack of appropriately structured corpora.", "filtered_refids": [[], ["b98", "b157", "b118", null, "b33"], [], ["b23"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 42, "num_chars": 7287, "num_references": 6}
{"corpusid_sectionid": "253447259-s4", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Annotation Challenges", "section": "As mentioned in Section 1.1, the notion of a grammatical error is hard to define as different errors may have different scope (e.g. local vs. contextual), complexity (e.g. orthographic vs. semantic) and corrections (e.g. [this books \u2192 this book] vs. [this books \u2192 these books]. Human annotation is thus an extremely cognitively demanding task and so clear annotation guidelines are a crucial component of dataset quality. This section briefly outlines three important aspects of data collection: Minimal vs. Fluent Corrections, Annotation Consistency, and Preprocessing Challenges.\n\nMinimal vs. Fluent Corrections. Most GEC corpora have been annotated on the principle of minimal corrections, i.e. annotators should make the minimum number of changes to make a text grammatical. Sakaguchi et al. (2016) argue, however, that this can often lead to corrections that sound unnatural, and so it would be better to annotate corpora on the principle of fluent corrections instead. Consider the following example:\n\nOriginal I want explain to you some interesting part from my experience.\n\nMinimal I want to explain to you some interesting parts of my experience. Fluent I want to tell you about some interesting parts of my experience.\n\nWhile the minimal correction primarily inserts a missing infinitival to before explain to make the sentence grammatical, the fluent correction also changes explain to tell you about because it is more idiomatic to tell someone about an experience rather than explain an experience. One of the main challenges of this distinction, however, is that it is very difficult to draw a line between what constitutes a minimal correction and what constitutes a fluent correction. This is because minimal corrections (e.g. missing determiners) are a subset of fluent corrections, and so there cannot be fluent corrections without minimal corrections. It is also the case that minimal corrections are typically easier to make than fluent corrections (for both humans and machines), although it is undeniable that fluent corrections are the more desirable outcome. Ultimately, although it is very difficult to precisely define a fluent correction, annotation guidelines should nevertheless attempt to make clear the extent to which annotators are expected to edit.\n\nAnnotation Consistency. A significant challenge of human annotation is that corrections are subjective and there is often more than one way to correct a sentence (Bryant and Ng 2015;Choshen and Abend 2018b). It is nevertheless important that annotators attempt to be consistent in their judgements, especially if they are explicitly annotating edit spans. For example the edit [has eating \u2192 was eaten] can also be represented as [has \u2192 was] and [eating \u2192 eaten], and this choice not only affects data exploration and analysis, but can also have an impact on edit-based evaluation. Similarly, the edit [the informations \u2192 information] can also be represented as [the \u2192 \u01eb] and [informations \u2192 information], but the latter may be more intuitive because it represents two independent edits of clearly distinct types. Explicit error type classification is thus another important aspect of annotator consistency, as an error type framework (if any) not only increases the cognitive burden on the annotator, but also might influence an annotator towards a particular correction given the error types that are available . Ultimately, if annotators are tasked with explicitly defining the edits they make to correct a sentence, annotator guidelines must clearly define the notion of an edit.\n\nPreprocessing Challenges. While human annotators are trained to correct natural text, GEC systems are typically trained to correct word tokenised sentences (mainly for evaluation purposes). This mismatch means human annotations typically undergo several preprocessing steps in order to produce the desired output format (Bryant and Felice 2016). The first of these transformations involves converting character-level edits to token-level edits. While this is often straightforward, it can sometimes be the case that a human-annotated character span does not map to a complete token; e.g. [ing \u2192 ed] to denote the edit [dancing \u2192 danced]. Although such cases can often (but not always) be resolved automatically, e.g., by expanding the character spans of the edit or calculating token alignment, they can also be reduced by training annotators to explicitly annotate longer spans rather than sub-words.\n\nThe second transformation involves sentence tokenisation, which is potentially more complex given human edits may change sentence boundaries; e.g. [A. B, C. \u2192 A, B. C.]. Sentences are nevertheless typically tokenised based solely on the original text, with the acknowledgement that some may be sentence fragments (to be joined with the following sentence) and that edits which cross sentence boundaries are ignored (e.g. [. Because \u2192 ,  because]. It is worth noting that this issue only affects sentence-based GEC systems (the vast majority) but paragraph or document-based systems are unaffected.", "filtered_refids": [[], ["b140"], [], [], [], [null, "b25"], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 5068, "num_references": 5}
{"corpusid_sectionid": "253447259-s7", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "FCE.", "section": "The First Certificate in English (FCE) corpus (Yannakoudakis, Briscoe, and Medlock 2011) is a public subset of the Cambridge Learner Corpus (CLC) (Nicholls 2003) that consists of 1,244 scripts (\u223c531k words) written by international learners of English as a second language (L2 learners). Each script typically contains two answers to a prompt in the style of a short essay, letter, or description, and each answer has been corrected by a single annotator who has identified and classified each edit according to a framework of 88 error types (Nicholls 2003 2001)) and the data is split into a standard training, development and test set. The FCE was used as the official dataset of the HOO-2012 shared task (Dale, Anisimoff, and Narroway 2012), one of the official training datasets of the BEA-2019 shared task (Bryant et al. 2019), and has otherwise commonly been used for grammatical error detection (Rei and Yannakoudakis 2016;Bell, Yannakoudakis, and Rei 2019;). It also contains essay level scores, as well as other limited metadata about the learner, and has been used for automatic essay scoring (AES) (e.g. Ke and Ng (2019)).\n\nNUCLE/CoNLL. The National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier, Ng, and Wu 2013) consists of 1,397 argumentative essays (\u223c1.16m words) written by NUS undergraduate students who needed L2 English language support. The essays, which are approximately C1 level, are written on a diverse range of topics including technology, healthcare, and finance, and were each corrected by a single annotator who identified and classified each edit according to a framework of 28 error types. NUCLE was used as the official training corpus of the CoNLL-2013 and CoNLL-2014 shared tasks  as well as one of the official training datasets of the BEA-2019 shared task (Bryant et al. 2019). The CoNLL-2013 and CoNLL-2014 test sets were annotated under similar conditions to NUCLE and respectively consist of 50 essays each (\u223c30k words) on the topics of i) surveillance technology and population aging, and ii) genetic testing and social media. The CoNLL-2014 test set was also doubly annotated by 2 independent annotators, resulting in 2 sets of official reference annotations; Bryant and Ng (2015) and Sakaguchi et al. (2016) subsequently collected another 8 sets of annotations each for a total of 18 sets of reference annotations. The CoNLL-2013 dataset is now occasionally used as a development set, while the CoNLL-2014 dataset is one of the most commonly used benchmark test sets. One limitation of the CoNLL-2014 test set is that it is not very diverse given that it consists entirely of essays written by a narrow range of learners on only two different topics.\n\nLang-8. The Lang-8 Corpus of Learner English (Mizumoto et al. 2012;Tajiri, Komachi, and Matsumoto 2012) is a preprocessed subset of the multilingual Lang-8 Learner Corpus (Mizumoto et al. 2011), which consists of 100,000 submissions (\u223c11.8m words) to the language learning social network service,  The texts are wholly unconstrained by topic, and hence include the full range of ability levels (A1-C2), and were written by international L2 English language learners with a bias towards Japanese L1 speakers. Although Lang-8 is one of the largest publicly available corpora, it is also one of the noisiest as corrections are provided by other users rather than professional annotators. A small number of submissions also contain multiple sets of corrections, but all annotations are provided as parallel text and so do not contain explicit edits or error types. Lang-8 was also one of the official training datasets of the BEA-2019 shared task (Bryant et al. 2019).\n\nJFLEG. The Johns Hopkins Fluency-Extended GUG corpus (JFLEG) (Napoles, Sakaguchi, and Tetreault 2017) is a collection of 1,501 sentences (\u223c28.1k words) split roughly equally into a development and test set. The sentences were randomly sampled from essays written by L2 learners of English of an unspecified ability level (Heilman et al. 2014) and corrected by crowdsourced annotators on Amazon Mechanical Turk (Crowston 2012). Each sentence was annotated a total of 4 times, resulting in 4 sets of parallel reference annotations, but edits were not explicitly defined or classified. The main innovation of JFLEG is that sentences were corrected to be fluent rather than minimally grammatical (Section 2.1). The main criticisms of JFLEG are that it is much smaller than other test sets, the sentences are presented out of context, and it was not corrected by professional annotators (Napoles, N\u0103dejde, and Tetreault 2019).\n\nW&I+LOCNESS. The Write & Improve (W&I) and LOCNESS corpus (Bryant et al. 2019) respectively consist of 3,600 essays (\u223c755k words) written by international learners of all ability levels (A1-C2) and 100 essays (\u223c46.2k words) written by native British/American English undergraduates. It was released as the official training, development and test corpus of the BEA-2019 shared task and was designed to be more balanced than other corpora such that there are roughly an equal number of sentences at each ability level: Beginner, Intermediate, Advanced, Native. The W&I essays come from submissions to the Write & Improve online essay-writing platform 3 (Yannakoudakis et al. 2018) and the LOCNESS essays, which only comprise part of the development and test sets, come from the LOCNESS corpus (Granger 1998). The training and development set essays were each corrected by a single annotator, while the test set essays were corrected by 5 annotators resulting in 5 sets of parallel reference annotations. Edits were explicitly defined, but not manually classified, so error types were added automatically using the ERRANT framework (Bryant, Felice, and Briscoe 2017). The test set references are not currently publicly available, so all evaluation on this dataset is done via the BEA-2019 Codalab competition platform, 4 which ensures all systems are evaluated in the same conditions.", "filtered_refids": [["b10", "b34", "b118", "b177", "b71", null, "b148"], ["b140", null, "b33", "b148"], ["b99", "b148", "b98", "b157"], ["b115", "b110", "b56"], [null, "b178"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 5987, "num_references": 20}
{"corpusid_sectionid": "253447259-s10", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "WikEd.", "section": "The Wikipedia Edit Error Corpus (WikEd) (Grundkiewicz and Junczys-Dowmunt 2014) consists of tens of millions of sentences of revision histories from articles on English Wikipedia. The texts are written and edited by native speakers rather than L2 learners and not all changes are grammatical edits; e.g. information updates. A preprocessed version of the corpus is available 5 (28.5m sentences, 626m words) which filters and modifies sentences such that they only contain edits similar to those in NUCLE. The corpus also includes tools to facilitate the collection of similar Wiki-based corpora for other languages.\n\nAESW. The Automatic Evaluation of Scientific Writing (AESW) dataset consists of 316k paragraphs (\u223c35.5m words) extracted from 9,919 published scientific journal articles and split into a training, development and test set for the AESW shared task (Daudaravicius et al. 2016). A majority of the paragraphs come from Physics, Mathematics and Engineering journals and were written by advanced or native speakers. The articles were edited by professional language editors who explicitly identified the required edits but did not classify them by error type. Although large, one of the main limitations of the AESW dataset is that the texts come from a very specific domain and many sentences contain placeholder tokens for mathematical notation and reference citations which do not generalise to other domains.\n\nGMEG. The Grammarly Multi-domain Evaluation for GEC (GMEG) dataset (Napoles, N\u0103dejde, and Tetreault 2019) consists of 5,919 sentences (\u223c122.4k words) split approximately equally across 3 different domains: formal native, informal native, and learner text. Specifically, the formal text is sampled from the WikEd corpus (Grundkiewicz and Junczys-Dowmunt 2014), the informal text is sampled from Yahoo Answers, and the learner text is sampled from the FCE (Yannakoudakis, Briscoe, and Medlock 2011). The sentences were sampled at the paragraph level (except for WikEd) to include some context and were annotated by 4 professional annotators to produce 4 sets of alternative references. One of the goals of GMEG was to diversify researchers away from purely L2 learner-based corpora.\n\nCWEB. The Corrected Websites (CWEB) dataset (Flachs et al. 2020) consists of 13.6k sentences (297k words) sampled from random paragraphs on the web in the Common-Crawl dataset. 6 Paragraphs were filtered to reduce noise (e.g. non-English and duplicates) and loosely defined as formal (\"sponsored\") and informal (\"generic\") based on the domain of the URL. The paragraphs, which are split equally between a development set and a test set, were doubly annotated by 2 professional annotators and edits were extracted and classified automatically using ERRANT (Bryant, Felice, and Briscoe 2017). Like GMEG, one of the aims of CWEB was to introduce a dataset that extended beyond learner corpora.\n\nGHTC. The GitHub Typo Corpus (GHTC) (Hagiwara and Mita 2020) consists of 353k edits from 203k commits to repositories in the GitHub software hosting website. 7 All the edits were gathered from repositories that met certain conditions (e.g. a permissive license) and from commits that contained the word 'typo' in the commit message. The  intuition behind the corpus was that developers often make small commits to correct minor spelling/grammatical errors and that these annotations can be used for GEC. The main limitation of GHTC is that the majority of edits are spelling or orthographic errors from a specific domain (i.e. software documentation) and that the context of the edit is not always a full sentence.", "filtered_refids": [[null], ["b35"], ["b177", null, "b110"], ["b42", null], ["b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3613, "num_references": 8}
{"corpusid_sectionid": "253447259-s11", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Non-English Datasets", "section": "Although most work on GEC has focused on English, corpora for other languages are also slowly being created and publicly released for the purposes of developing GEC models. This section introduces some of the most prominent (Table 4), along with other relevant resources, but is again by no means an exhaustive list. These resources are ultimately helping to pave the way for research into multilingual GEC (N\u00e1plava and Straka 2019; Katsumata and Komachi 2020;Rothe et al. 2021).\n\nArabic. The Qatar Arabic Language Bank (QALB) project ) is an initiative that aims to collect large corpora of annotated Arabic for the purposes of Arabic GEC system development. A subset of this corpus was used as the official training, development and test data of the QALB-2014 and QALB-2015 shared tasks on Arabic text correction Rozovskaya et al. 2015). In particular, QALB-2014 released 21.3k documents (1.1m words) of annotated user comments submitted to the Al Jazeera news website by native speakers, while QALB-2015 released 622 documents (90.8k words) of annotated essays written by the full range of Arabic L2 learners (A1-C2) (Zaghouani et al. 2015) along with an additional 920 documents (48.5k words) of unreleased Al Jazeera comments. QALB-2015 thus had 2 test sets: one on native Al Jazeera data and one on Arabic L2 learner essays. In all cases, files were provided at the document level (rather than the sentence level) and edits were explicitly identified by trained annotators and classified automatically using a framework of 7 error types.\n\nChinese. The Test of Chinese as a Foreign Language (TOCFL) corpus (Lee, Tseng, and Chang 2018) and the Hanyu Shuiping Kaoshi (HSK: Chinese Proficiency Test) corpus 8 (Zhang 2009) respectively consist of 2.8k essays (1m characters) and 11k essays (4m characters) written by the full range of language learners (A1-C2) who took Mandarin Chinese language proficiency exams. Various subsets of these corpora were used as the official training and test sets in the NLPTEA series of shared tasks on Chinese Grammatical Error Diagnosis (i.e. error detection) between 2014-2020 (Yu, Lee, and Chang 2014;Rao, Yang, and Zhang 2020). The most recent of these shared tasks, NLPTEA-2020, released a total of 2.6k paragraphs (92.1k characters, 1-5 sentences each), which were annotated by a single annotator according to a framework of 4 error types: Redundant (R), Missing (M), Word Selection (S) or Word Order (W). The NLPCC-2018 shared task (Zhao et al. 2018), which was the first shared task on full error correction in Mandarin Chinese, released a further 717k training sentences (14.1m characters) which were extracted from a cleaned subset of Lang-8 user submissions (Mizumoto et al. 2011). Like the Lang-8 Corpus of Learner English, the ability level of the authors in this dataset is unknown and corrections were provided by other users. The test data for this shared task came from the PKU Chinese Learner Corpus and consists of 2000 sentences (61.3k characters) written by foreign college students. All test sentences were first annotated by a single annotator, who also classified edits according to the same 4-error-type framework as NLPTEA, and subsequently checked by a second annotator who was allowed to make changes to the annotations if necessary.\n\nThe Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction (MuCGEC) Zhang et al. (2022b) is a new corpus that is intended to be a more robust test set for Chinese GEC. It contains a total of 7063 sentences (\u223c278k characters) sampled approximately equally from the NLPCC-2018 training set (Lang-8), the NLPCC-2018 test set (PKU Chinese Learner Corpus) and the NLPTEA-2018/2020 test sets (HSK Corpus). All sentences were annotated by multiple annotators, but identical references were removed, so we report an average of 2.3 references per sentence (90% of all sentences have 1-3 references). Edits were also classified according to a scheme of 19 error types, including 5 main error types and 14 minor sub-types.\n\nCzech. The AKCES-GEC corpus (N\u00e1plava and Straka 2019) consists of 47.3k sentences (505k words) written by both learners of Czech as a second language (from both Slavic and non-Slavic backgrounds) and Romani children who speak a Czech ethnolect as a first language. The essays and exam-style scripts come from the Learner Corpus of Czech as a Second Language (CzeSL) (Rosen 2016) which falls under the larger Czech Language Acquisition Corpora (AKCES) project (\u0160ebesta 2010). The essays in the training set were annotated once (1 set of annotations) and the essays in the development and test sets were annotated twice (2 sets of annotations), all with explicit edits that were classified according to a framework of 25 error types.\n\nThe Grammar Error Correction Corpus for Czech (GECCC) (N\u00e1plava et al. 2022) is an extension of AKCES-GEC that includes both formal texts written by native Czech primary and secondary school students as well as informal website discussions on Facebook and Czech news websites, in addition to the texts written by Czech language learners and Romani children. The total corpus consists of 83k sentences (949k words), all of which were manually annotated (or re-annotated in order to preserve annotation style) by 5 experienced annotators who explicitly identified edits. Edits were then classified automatically by a variant of ERRANT (Bryant, Felice, and Briscoe 2017) for Czech which included a customised tagset of 65 errors types. GECCC is currently one of the largest non-English corpora and is also larger than most popular English benchmarks.\n\nGerman. The Falko-MERLIN GEC corpus (Boyd 2018) consists of 24k sentences (381k words) written by learners of all ability levels (A1-C2). Approximately half the data comes from the Falko corpus (Reznicek et al. 2012), which consists of minimallycorrected advanced German learner essays (C1-C2), while the other half comes from the MERLIN corpus (Boyd et al. 2014), which consists of standardised German language exam scripts from a wide range of ability levels (A1-C1). Edits were not explicitly annotated, but extracted and classified automatically using a variation of ERRANT (Bryant, Felice, and Briscoe 2017) which was adapted for German and included a customised tagset for German error types.\n\nJapanese. The TMU Evaluation Corpus for Japanese Learners (TEC-JL) (Koyama et al. 2020) consists of 1.9k sentences (41.5k characters) written by language learners of unknown level (A1-C2?) and submitted to the language learning social network service Lang-8. TEC-JL is a subset of the multilingual Lang-8 Learner Corpus (Mizumoto et al. 2011) and was doubly annotated by 3 native Japanese university students (2 sets of annotations) to be a more reliable test set than the original Lang-8 Learner Corpus which can be quite noisy.\n\nRussian. The Russian Learner Corpus of Academic Writing (RULEC) (Alsufieva, Kisselev, and Freels 2012) consists of essays written by L2 university students and heritage Russian speakers in the United States. A subset of this corpus, 12.5k sentences (206k words), was annotated by 2 native speakers of Russian with backgrounds in linguistics and released as the RULEC-GEC corpus (Rozovskaya and Roth 2019). Edits were explicitly annotated and classified according to a framework of 23 error types. Another corpus of annotated Russian errors, the Russian Lang-8 corpus (RU-Lang8) (Trinh and Rozovskaya 2021), which is a subset of the aforementioned multilingual Lang-8 Learner Corpus (Mizumoto et al. 2011), was also recently announced, however the data has not yet been publicly released.\n\nUkrainian. The UA-GEC corpus (Syvokon and Nahorna 2021) consists of 20.7k sentences (329k words) written by almost 500 authors from a wide variety of backgrounds (mostly technical and humanities) and ability levels (two-thirds native). The texts cover a wide range of topics including short essays (formal, informal, fictional or journalistic) and translated works of literature, and were annotated by two native speakers with degrees in Ukrainian linguistics. Edits were explicitly annotated and classified according to a scheme of 4 error types: Grammar, Spelling, Punctuation or Fluency.", "filtered_refids": [["b70", null], [null], ["b196", "b184", "b202", "b99", "b132"], ["b199"], [null], ["b109"], ["b12", null], ["b78", "b99"], ["b99", "b163", "b2"], ["b156"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 8237, "num_references": 19}
{"corpusid_sectionid": "253447259-s15", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Statistical Machine Translation", "section": "In contrast with statistical classifiers, one of the main advantages treating GEC as a statistical machine translation (SMT) problem is that SMT can theoretically correct all error types simultaneously without expert knowledge or feature engineering. This also includes interacting errors, which are problematic for rule-based systems and classifiers. Despite originally being developed for translation between different languages, SMT has been successfully applied to GEC, which can be seen as a translation problem from errorful to correct sentences. More specifically, although both the source and target sentences are in the same language, i.e. monolingual translation, the source may contain grammatical errors which should be 'translated' to appropriate corrections. SMT is inspired by the noisy channel model (Shannon 1948) and is mathematically formulated using Bayes' rule:\n\nwhere a correct sentence C is said to have passed through a noisy channel to produce an erroneous sentence E, and the goal is to reconstruct the correct sentence\u0108 using a language model (LM) P (C) and a translation model (TM) P (E|C) -see Figure 1. Candidate sentences are generated by means of a decoder, which normally uses a beam search strategy. The denominator P (E) in Equation 1 is ignored since it is constant across all Cs. The use of SMT for GEC was pioneered by Brockett, Dolan, and Gamon (2006), who built a system to correct errors involving 14 countable and uncountable nouns. Their training data comprised a large corpus of sentences extracted from news articles which were deliberately modified to include artificial mass noun errors. Mizumoto et al. (2011) applied the same techniques to Japanese error correction but improved on them by not only considering a wider set of error types, but also training on real learner examples extracted from the language learning social network website Lang-8. Yuan and Felice (2013) subsequently trained a POS-factored SMT system to correct five types of errors in learner text for the CoNLL-2013 shared task, and revealed the potential of using SMT as a general approach for correcting multiple error types and interacting errors simultaneously. In the following year, the two top-performing systems in the CoNLL-2014 shared task demonstrated that SMT yielded state-of-the-art performance on general error correction in contrast with other methods (Felice et al. 2014;Junczys-Dowmunt and Grundkiewicz 2014). This success led to SMT becoming a dominant approach in the field and inspired other researchers to adapt SMT technology for GEC, including:\n\n\u2022 Adding GEC-specific features to the model to allow for the fact that most words translate into themselves and errors are often similar to their correct forms. Two types of these features include the Levenshtein distance ( \u2022 Introducing neural network components, such as a neural network global lexicon model (NNGLM) and neural network joint model (NNJM) (Chollampatt, Taghipour, and Ng 2016;Chollampatt and Ng 2017).\n\nDespite their success in GEC, SMT-based approaches suffer from a few shortcomings. In particular, they i) tend to produce locally well-formed phrases with poor overall grammar, ii) exhibit a predilection for changing phrases to more frequent versions even when the original is correct, resulting in unnecessary corrections, iii) are unable to process long-range dependencies and iv) are hard to constrain to particular error types (Felice 2016;Yuan 2017). Last but not least, the performance of SMT systems depends heavily on the amount and quality of parallel data available for training, which is very limited in GEC. A common solution to this problem is to generate artificial datasets, where errors are injected into well-formed text to produce pseudo-incorrect sentences, as described in Section 5.", "filtered_refids": [["b145"], [null, "b99", "b190"], ["b22", "b18"], ["b186", "b188"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3814, "num_references": 8}
{"corpusid_sectionid": "253447259-s16", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Neural Machine Translation", "section": "With the advent of deep learning and promising results reported in machine translation and other sequence-to-sequence tasks, neural machine translation (NMT) was naturally extended to GEC. Compared to SMT, NMT uses a single large neural network to model the entire correction process, eliminating the need for complex GEC-specific feature engineering. Training an NMT system is furthermore an end-to-end process and so does not require separately trained and tuned components as in SMT. Despite its simplicity, NMT has achieved state-of-the-art performance on various GEC tasks (Flachs, Stahlberg, and Kumar 2021;Rothe et al. 2021).\n\nNMT employs the encoder-decoder framework (Cho et al. 2014). An encoder first reads and encodes an entire input sequence x = (x 1 , x 2 , ..., x T ) into hidden state representations, and a decoder then generates an output sequence y = (y 1 , y 2 , ..., y T \u2032 ) by predicting the next word y t based on the input sequence x and all the previously generated words {y 1 , y 2 , ..., y t\u22121 }:\n\nDifferent network architectures have been proposed for building the encoders and decoders; three commonly used sequence-to-sequence models are RNNs (Bahdanau, Cho, and Bengio 2015), CNNs (Gehring et al. 2017), and Transformers (Vaswani et al. 2017).", "filtered_refids": [["b43", null], ["b13"], ["b48", "b8", "b164"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1274, "num_references": 6}
{"corpusid_sectionid": "253447259-s17", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Recurrent Neural Networks", "section": "Recurrent Neural Networks (RNN) are a type of neural network that is specifically designed to process sequential data. RNNs are used to transform a variablelength input sequence to another variable-length output sequence (Cho et al. 2014;Sutskever, Vinyals, and Le 2014). To handle long-term dependencies, gated units are usually used in RNNs (Goodfellow, Bengio, and Courville 2016). The two most effective RNN gates are Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber 1997) and Gated Recurrent Units (GRU) (Cho et al. 2014). Bahdanau, Cho, and Bengio (2015) introduced an attention mechanism to implement variable-length representations, which eased optimisation difficulty and resulted in improved performance.  presented the first work on NMT-based approach for GEC. Their model consists of a bidirectional RNN encoder and an attention-based RNN decoder. Xie et al. (2016) proposed the use of a character-level RNN sequence-to-sequence model for GEC. Following their work, a hybrid model with nested attention at both the word and character level was later introduced by Ji et al. (2017).", "filtered_refids": [["b155", "b174", "b13", "b59", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1102, "num_references": 5}
{"corpusid_sectionid": "253447259-s19", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Transformers", "section": "The Transformer (Vaswani et al. 2017) is the first sequence transducer network that entirely relies on a self-attention mechanism to compute the representations of its input, without the need for recurrence or convolution. Its architecture allows better parallelisation on multiple GPUs, overcoming the weakness of RNNs.\n\nThe Transformer has become the architecture of choice for machine translation since its inception (Edunov et al. 2018;Wang et al. 2019;Liu et al. 2020). Previous work has investigated the adaptation of NMT to GEC, such as optimising the model with editweighted loss (Junczys-Dowmunt et al. 2018) and adding a copy mechanism (Zhao et al. 2019;Yuan et al. 2019). A copy mechanism allows the model to directly copy tokens from the source sentence, which often has substantial overlap with the target sentence in GEC. The Copy-Augmented Transformer has become a popular alternative architecture for GEC (Hotate, Kaneko, and Komachi 2020;Wan, Wan, and Wang 2020). Another modification to the Transformer architecture is altering the encoder-decoder attention mechanism in the decoder to accept and make use of additional context. For example, Kaneko et al. (2020) added the BERT representation of the input sentence as additional context for GEC, while  added the previous sentences in the document, and Zhang et al. (2022c) added a tree-based syntactic representation of the input sentence.\n\nAs the Transformer architecture has a large number of parameters, yet parallel GEC training data is limited, pre-training has become a standard procedure in building GEC systems. The first Transformer-based GEC system (Junczys-Dowmunt et al. 2018) pre-trained the Transformer decoder on a language modeling task, but it has since become more common to pre-train on synthetic GEC data. The top two systems in the BEA-2019 shared task (Grundkiewicz, Junczys-Dowmunt, and Heafield 2019;Choe et al. 2019) and a recent state-of-the-art GEC system  both pre-trained their Transformer models with synthetic data, but they generated their synthetic data in different ways. We discuss different techniques for generating synthetic data in Section 5.1. More recently, with the advances in large pre-trained language models, directly fine-tuning large pre-trained language models with GEC parallel data has been shown to achieve comparable performance with synthetic data pre-training (Katsumata and Komachi 2020), even reaching state-of-the-art performance (Rothe et al. 2021;Tarnavskyi, Chernodub, and Omelianchuk 2022).\n\nIrrespective of the type of NMT architecture however (RNN, CNN, Transformer), NMT systems share several weaknesses with SMT systems, most notably in terms of data requirements. In particular, although NMT systems are more capable at correcting longer range and more complex errors than SMT, they also require as much training data as possible, which can lead to extreme resource and time requirements: it is not uncommon for some models to require several days of training time on a cluster of GPUs. Moreover, neural models are almost completely uninterpretable (which furthermore makes them difficult to customise) and it is nearly impossible for a human to determine the reasoning behind a given decision; this is particularly problematic if we also want to explain the cause of an error to a user rather than just correct it. Ultimately however, a key strength of NMT is that it is an end-to-end approach, and so does not require feature engineering or much human intervention, and it is undeniable that it produces some of the most convincing output to date.", "filtered_refids": [["b164"], ["b86", "b67", "b40", "b165", "b167", "b61", "b191", "b201"], ["b16", "b159", "b50", null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3585, "num_references": 13}
{"corpusid_sectionid": "253447259-s20", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Edit-based approaches", "section": "While most GEC approaches generate a corrected sentence from an input sentence, the edit generation approach generates a sequence of edits to be applied to the input sentence instead. As GEC has a high degree of token copying from the input to the output, Stahlberg and Kumar (2020) argued that generating the full sequence is wasteful. By generating edit operations instead of all tokens in a sentence, the edit generation approach typically has a faster inference speed, reported to be five to ten times faster than GEC systems that generate the whole sentence. One limitation of this approach, however, is that edit operations tend to be token-based, and so sometimes fail to capture more complex, multi-token fluency edits (Lai et al. 2022). Edit generation has been cast as a sequence tagging task (Malmi et al. 2019;Awasthi et al. 2019;Omelianchuk et al. 2020;Tarnavskyi, Chernodub, and Omelianchuk 2022) or a sequence-to-sequence task (Stahlberg and Kumar 2020).\n\nIn the sequence tagging approach, for each token of an input sentence, the system predicts an edit operation to be applied to that token (Table 5). This approach requires the user to define a set of tags representing the edit operations to be modelled by the system. Some edits can be universally modelled, such as conversion of verb forms or conversion of nouns from singular to plural form. Some others such as word insertion and word replacement are token-dependent. Token-dependent edits need a different tag for each possible word in the vocabulary, resulting in the number of tags growing linearly with the number of unique words in the training data. Thus, the number of token-dependent tags to be modelled in the system becomes a trade-off between coverage and model size.  Table 5 Example task formulation of edit generation in the sequence tagging approach from (Omelianchuk et al. 2020). APP_x denotes an operation of appending token x, and REP_x denotes replacing the current token with x.\n\nOn the other hand, the sequence-to-sequence approach is more flexible as it does not limit the output to pre-defined edit operation tags. It produces a sequence of edits, each consisting of a span position, a replacement string, and an optional tag for edit type (Table 6). These tags add interpretability to the process and have been shown to improve model performance. As generation in the sequence-to-sequence approach has a left-to-right dependency, the inference procedure is slower than that in the sequence tagging approach. It is still five times faster than that in the whole sentence generation approach as the edit sequence generated is much shorter than the sequence of all tokens in the sentence (Stahlberg and Kumar 2020).\n\nSource After many years he still dream to become a super hero . Target After many years , he still dreams of becoming a super hero . Edits (SELF,3,SELF), (PUNCT,3,','), (SELF,5,SELF), (SVA,6,'dreams'), (PART,7,'of'), (FORM,8,'becoming'), (SELF,12,SELF) Table 6 Example task formulation of edit generation in the sequence-to-sequence approach from (Stahlberg and Kumar 2020). Each tuple represents a tag, a span's ending position, and a replacement string.\n\nThe main advantages of edit-based approaches to GEC are thus that they not only add much needed transparency and explainability to the correction process, but they are also much faster at inference time than NMT. Their main disadvantages, however, are that they generally require human engineering to define the size and scope of the edit label set, and that it is more difficult to represent interacting and complex multi-token edits with token-based labels. Like all neural approaches, they also depend on as much training data as possible, but when data is available, edit-based approaches are very competitive with state-of-the-art NMT models.", "filtered_refids": [["b6", "b92", "b120", null, "b149", "b159"], ["b120"], ["b149"], [null, "b149"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3816, "num_references": 10}
{"corpusid_sectionid": "253447259-s21", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Language Models for Low-Resource and Unsupervised GEC", "section": "Unlike previous strategies, language model based GEC does not require training a system with parallel data. Instead, it employs various techniques using n-gram or Transformer language models. LM-based GEC was a common approach before machine translationbased GEC became popular (Dahlmeier and Ng 2012a; Lee and Lee 2014), but has experienced a recent resurgence with low-resource GEC and unsupervised GEC due to the effectiveness of large Transformer-based language models (Alikaniotis and Raheja 2019; Grundkiewicz and Junczys-Dowmunt 2019; Flachs, Lacroix, and S\u00f8gaard 2019). Recent advances have enabled Transformer-based language models to more adequately capture syntactic phenomena (Jawahar, Sagot, and Seddah 2019;Wei et al. 2021), making them capable GEC systems when little or no data is available. These systems can, however, become even more capable when exposed to a small amount of parallel data (Mita and Yanaka 2021).", "filtered_refids": [["b65", "b170", "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 932, "num_references": 3}
{"corpusid_sectionid": "253447259-s22", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Language models as Discriminators", "section": "The traditional LM-based approach to GEC makes the assumption that low probability sentences are more likely to contain grammatical errors than high probability sentences, and so a GEC system must determine how to transform the former into the latter based on language model probabilities (Bryant and Briscoe 2018). Correction candidates can be generated from confusion sets (Dahlmeier and Ng 2011a;Bryant and Briscoe 2018), classification-based GEC models (Dahlmeier and Ng 2012a), or finite-state transducers (Stahlberg, Bryant, and Byrne 2019). Yasunaga, Leskovec, and Liang (2021) proposed an alternative method using the break-it-fix-it (BIFI) approach , with a language model as the critic (LM-critic). Specifically, BIFI trains a breaker (noising channel) and a fixer (GEC model) on multiple rounds of feedback loops. An initial fixer is used to correct erroneous text, then the sentence pairs are filtered using LM-critic. Using this filtered data, the breaker is trained and used to generate new synthetic data from a clean corpus. These new sentence pairs are then also filtered using LM-critic and subsequently used to train the fixer again. The BIFI approach can be used for unsupervised GEC by training the fixer on synthetic data.", "filtered_refids": [[null, "b148", "b31", "b180"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1244, "num_references": 4}
{"corpusid_sectionid": "253447259-s23", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Language models as Generators", "section": "A more recent LM-based approach to GEC is to use a language model as a zeroshot or few-shot generator to generate a correction given a prompt and noisy input sentence. For example, given the prompt \"Correct the grammatical errors in the following text:\" followed by an input sentence, the language model is expected to generate a corrected form of the input sentence given the prompt as context. This approach has become possible largely due to the advent of Large Language Models (LLMs), such as GPT-2 (Radford et al. 2019), GPT-3 (Brown et al. 2020), OPT (Zhang et al. 2022a) and PaLM (Chowdhery et al. 2022), which have been trained on up to a trillion words and parameterised using tens or hundreds of billions of parameters. These models have furthermore been shown to be capable of generalising to new unseen tasks or languages by being fine-tuned on a wide variety of other NLP tasks (Sanh et al. 2022;Wei et al. 2022;Muennighoff et al. 2022), and so it is possible, for the first time, to build a system that is capable of carrying out multilingual GEC without having been explicitly trained to do so.\n\nDespite their potential however, there have not yet been any published studies that have formally benchmarked generative LLMs against any of the standard GEC test sets. Although a number of studies were beginning to appear at the time of final submission of this survey paper, most only evaluated LLM performance on a small sample (100 sentences) of the official test sets (Wu et al. 2023;Coyne and Sakaguchi 2023). These studies generally conclude, however, that LLMs have a tendency to overcorrect for fluency, which causes them to underperform on datasets that were developed for minimal corrections (Fang et al. 2023). We expect further investigation of this phenomenon in the coming year.\n\nRegardless of the type of language model, the main advantage of language model based approaches is that they only require unannotated monolingual data and so are much easier to extend to other languages than all other approaches. While discriminative LMs may not perform as well as state-of-the-art models and generative LLMs models have not been formally benchmarked, LMs have nevertheless proven themselves capable and can theoretically correct all types of errors, including complex fluency errors. The main disadvantage of language model approaches, however, is that it can be hard to adequately constrain the model, and so models sometimes replace grammatical words with other words that simply occur more frequently in a given context. An additional challenge in generative LLM-based GEC is that prompt engineering is important (Liu et al. 2023) and output may vary depending on whether a system was asked to 'correct' a grammatical error or 'fix' a grammatical error (Coyne and Sakaguchi 2023). Ultimately, all LM-based approaches suffer from the limitation that probability is not grammaticality, and so rare words may be mistaken for errors.", "filtered_refids": [[null, "b142", "b169"], [null, "b172"], ["b85"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2956, "num_references": 6}
{"corpusid_sectionid": "253447259-s25", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Re-ranking", "section": "Machine translation based (both SMT and NMT) systems can produce an n-best list of alternative corrections for a single sentence. This has led to much work on n-best list re-ranking, which aims to determine whether the best correction for a sentence is not the most likely candidate produced by the system (i.e. n = 1), but is rather somewhere further down the top n most likely candidates (Yuan, Briscoe, and Felice 2016;Mizumoto and Matsumoto 2016;Hoang, Chollampatt, and Ng 2016). As a separate postprocessing step, candidates produced by an SMT-based or NMT-based GEC system can be re-ranked using a rich set of features that have not been explored by the decoder before, so that better candidates can be selected as 'optimal' corrections. During re-ranking, GEC-specific features can then be easily adapted without worrying about fine-grained model smoothing issues. In addition to the original model scores of the candidates, useful features include: \u2022 error detection information which has been used in a binary setting (Yannakoudakis et al. 2017;Yuan et al. 2019), as well as a multi-class setting .\n\nN -best list reranking has traditionally been one of the simplest and most popular methods of boosting system performance. An alternative form of reranking is to collect all the edits from the N -best corrections and filter them using an edit-scorer (Sorokin 2022).", "filtered_refids": [["b100", "b58", "b179", "b188", "b191"], ["b146"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1374, "num_references": 6}
{"corpusid_sectionid": "253447259-s26", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Ensembling and System Combination", "section": "Ensembling is a common technique in machine learning to combine the predictions of multiple individually trained models. Ensembles often generate better predictions than any of the single models that are combined (Opitz and Maclin 1999). In GEC, ensembling usually refers to averaging the probabilities of individually trained GEC models when predicting the next token in the sequence-to-sequence approach or the edit tag in the edit-based approach. GEC models that are combined into ensembles usually have similar properties with only slight variations, which can be the random seed , the pre-trained model (Omelianchuk et al. 2020), or the architecture (Choe et al. 2019).\n\nOn the other hand, different GEC approaches have different strengths and weaknesses. Susanto, Phandi, and Ng (2014) have shown that combining different GEC systems can produce a better system with higher accuracy. When combining systems that have substantial differences, training a system combination model is preferred over ensembles. A system combination model allows the combined system to properly integrate the strengths of the GEC systems and has been shown to produce better results than ensembles (Kantor et al. 2019;Qorib, Na, and Ng 2022). The combination model can be trained through learning the characteristic of the GEC systems (Kantor et al. 2019;Lin and Ng 2021;Qorib, Na, and Ng 2022) or learning how to score a correction by supplying the model with examples of good and bad corrections for different kinds of student sentences (Sorokin 2022). Moreover, most system combination methods for GEC work on a black-box setup (Kantor et al. 2019;Lin and Ng 2021;Qorib, Na, and Ng 2022), only requiring the systems' outputs without any access to the systems' internals and the prediction probabilities. When the individual component systems are not different enough, encouraging the individual systems to be more diverse before combining them can also improve performance (Han and Ng 2021).", "filtered_refids": [["b16", "b120", "b121"], ["b146", "b126", "b84", "b69", "b53", "b154"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1978, "num_references": 9}
{"corpusid_sectionid": "253447259-s27", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Multi-task learning", "section": "Multi-task learning allows systems to use information from related tasks and learn from multiple objectives via shared representations, leading to performance gains on individual tasks. Rei and Yannakoudakis (2017) was the first to investigate the use of different auxiliary objectives for the task of error detection in learner writing through a neural sequence-labelling model. In addition to predicting the binary error labels (i.e. correct or incorrect), they experimented with also predicting specific error type information, including the learner's L1, token frequency, POS tags and dependency relations. Asano et al. (2019) employed a similar approach in which their error correction model additionally estimated the learner's language proficiency level and performed sentence-level error detection simultaneously. Token-level and sentence-level error detection have also both been explored as auxiliary objectives in NMT-based GEC (Yuan et al. 2019;Zhao et al. 2019), where systems have been trained to jointly generate a correction and predict whether the source sentence (or any token in it) is correct or incorrect. Labels for these auxiliary error detection tasks can be extracted automatically from existing datasets using automatic alignment tools like ERRANT (Bryant, Felice, and Briscoe 2017).", "filtered_refids": [["b191", "b179", "b201"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1309, "num_references": 3}
{"corpusid_sectionid": "253447259-s28", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Custom inference methods", "section": "Various inference techniques have been proposed to improve the quality of system output or speed up inference time in GEC. The most common of these, which specifically improves output quality, is to apply multiple rounds of inference, known as iterative decoding or multi-turn decoding. Since the input and output of GEC are in the same language, the output of the model can be passed through the model again to produce a second iteration of output. The advantage of this is that the model gets a second chance to correct errors it might have missed during the first iteration. Lichtarge et al. (2019) thus proposed an iterative decoding algorithm that allows a model to make multiple incremental corrections. In each iteration, the model is allowed to generate a different output only if it has high confidence. This technique proved effective for GEC systems trained on noisy data such as Wikipedia edits, but not as effective on GEC systems trained on clean data. Ge, Wei, and Zhou (2018) proposed an alternative iterative decoding technique called fluency boost, in which the model performs multiple rounds of inference until a fluency score stops increasing, while Lai et al. (2022) proposed an iterative approach that investigated the effect of correcting different types of errors (missing, replacement, unnecessary words) in different orders. Iterative decoding is commonly employed in sequence-labelling GEC systems which cannot typically correct all errors in a single pass. In these systems, iterative decoding is applied until the model stops making changes to the output or the number of iterations reaches a limit (Awasthi et al. 2019;Omelianchuk et al. 2020;Tarnavskyi, Chernodub, and Omelianchuk 2022).\n\nOther inference techniques have been proposed to speed up inference time in GEC. As many tokens in GEC are copied from the input to the output, standard left-toright inference can be inefficient. Chen et al. (2020a) thus proposed a two-step process that only performs correction on text spans that are predicted to contain grammatical errors. Specifically, their system first predicts erroneous spans using an erroneous span detection (ESD) model, and then corrects only the detected spans using an erroneous span correction (ESC) model. They reported reductions in inference time of almost 50% compared to a standard sequence-to-sequence model. In contrast, Sun et al. (2021) proposed a parallelisation technique to speed up inference, aggressive decoding, which can be applied to any sequence-to-sequence model. Specifically, aggressive decoding first decodes as many tokens as possible in parallel and then only re-decodes tokens one-by-one at the point where the input and predictions differ (if any). When the input and predicted tokens start to match again, aggressive decoding again decodes the remainder in parallel until either the tokens no longer match or the end-of-sentence token is predicted. Since the input and output sequences in GEC are often very similar, this means most tokens can be decoded aggressively, yielding an almost ten time speedup in inference time.", "filtered_refids": [["b6", "b46", "b159", "b120", "b83"], ["b152"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3101, "num_references": 6}
{"corpusid_sectionid": "253447259-s31", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Data Augmentation", "section": "A common problem in GEC is that the largest publicly-available high-quality parallel corpora only contain roughly 50k sentence pairs, and larger corpora, such as Lang-8, are noisy Rothe et al. 2021). This data sparsity problem has motivated a lot of research into synthetic data generation, especially in the context of resourceheavy NMT approaches, because synthetic data primarily requires a native monolingual source corpus rather than a labour-intensive manual annotation process. In this section, we introduce several different types of data augmentation methods, including rule-based noise injection and back-translation, but also noise reduction which aims to improve the quality of existing datasets by removing/down-weighting noisy examples. It is an open question as to how to best evaluate the quality of synthetic data (Htut and Tetreault 2019;White and Rozovskaya 2020). An effort has been made by (Kiyono et al. 2019) to compare the noise injection method and back-translation, but it is hard to comprehensively compare synthetic data generation methods directly, so most research evaluates it indirectly in terms of its impact on the performance of previous experiments. Data augmentation has nevertheless contributed greatly to GEC system improvement and has become a staple component of recent models.", "filtered_refids": [["b72", null, "b171", "b62"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1318, "num_references": 4}
{"corpusid_sectionid": "253447259-s33", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Noise Injection", "section": "One way to artificially generate grammatical errors to clean monolingual corpora is by perturbing a clean text to make it grammatically incorrect. The perturbations can be in the form of rule-based noising operations or error patterns that usually appear in GEC parallel corpora.\n\nRule-based. The most intuitive way of adding noise to a clean corpus is by applying a series of perturbation operations based on some pre-defined rules. The rules are applied based on a probability, which can be decided arbitrarily, empirically, or through some observations of available data. Ehsan and Faili (2013) apply one error to each sentence from predefined error templates that include omitting prepositions, repeating words, and so on. Lichtarge et al. (2019) introduce spelling errors to Wikipedia edit history by performing deletion, insertion, replacement, and transposition of characters. Zhao et al. (2019) also apply a similar noising strategy but at the word level, that is deleting, adding, shuffling, and replacing words in a sentence. Grundkiewicz, Junczys-Dowmunt, and Heafield (2019) combine both approaches, character-level and word-level noising, but word substitution is limited to pairs from a confusion set made from an inverted spellchecker. Similarly, Xu et al. (2019) also combine both approaches but with a more complex word substitution strategy by making use of part-of-speech (POS) tags. The rule-based injection technique can also be applied dynamically during training to increase the error rate in a parallel corpus instead of creating additional training data (Zhao and Wang 2020).\n\nError patterns. Another way of generating synthetic data is through injecting errors that frequently occur in GEC parallel corpora. In this way, the errors are more similar to the ones that humans usually make. Rozovskaya and Roth (2010b) proposed three different methods of injecting article errors, based on the error distribution in English as a Second Language (ESL) data. They proposed adding article errors based on the distribution of articles in a text before correction, the distribution of articles in the corrected text, and the distribution of article corrections themselves. Felice and Yuan (2014a) later improved the method by taking into consideration the morphology, POS tag, semantic concept, and word sense information of a text when generating the artificial errors. Rei et al. (2017) further extended it to all types of errors. Another direction of emulating human errors is by extracting the correction patterns from GEC parallel corpora and applying the inverse of those corrections on grammatically correct sentences, as done by Yuan and Felice (2013) using the corrections from the NUCLE corpus and by Choe et al. (2019) using the corrections from the W&I training data. The correction patterns are extracted both in lexical form (an \u2192 the) and POS (NN \u2192 NNS).", "filtered_refids": [[], ["b83", "b176", "b201", "b203"], ["b133", "b179", null, "b190", "b16"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2886, "num_references": 9}
{"corpusid_sectionid": "253447259-s34", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Back-translation", "section": "Emulating human errors can be made in a more automated and dynamic way via a noisy channel model. The noisy channel model is trained with the inverse of a GEC parallel corpus, treating the learner's sentence as the target and the reference sentence as the source. This technique is commonly called back-translation. The technique was originally proposed for generating additional data in machine translation (Sennrich, Haddow, and Birch 2016), but it is also directly applicable to GEC. Rei et al. (2017) were the first to apply back-translation to grammatical error detection (GED) and Xie et al. (2018) were the first to apply it to GEC. Yuan et al. (2019) add a form of quality control to Rei et al. (2017) based on language model probabilities in an effort to make sure that the generated synthetic sentences are less probable (and hence hopefully less grammatical) than the original input sentences. Between the rulebased and back-translation strategy, Kiyono et al. (2019) report that the back-translation strategy has better empirical performance. They also compare back-translation with a noisy beam-search strategy (Xie et al. 2018) and back-translation with sampling strategy (Edunov et al. 2018), and report that both achieve competitive performance. Koyama et al. (2021) furthermore compare the effect of using different architectures (e.g. CNN, LSTM, Transformer) for back-translation, and find that interpolating multiple generation systems tends to produce better synthetic data to train a GEC system on. Another variant of back-translation was proposed by Stahlberg and Kumar (2021) to generate more complex edits. They found that generating a sequence of edits using Seq2Edit (Stahlberg and Kumar 2020) works better than generating the corrupted sentences directly. They also reported that back-translation with sampling worked better than beam search in their experiments.", "filtered_refids": [["b72", "b175", "b40", "b144", "b179", "b150", null, "b77", "b149", "b191"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1890, "num_references": 10}
{"corpusid_sectionid": "253447259-s36", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Augmenting Official Datasets", "section": "Besides generating synthetic data to address the data sparsity problem in GEC, other works focus on augmenting official datasets, via noise reduction or model enhancement.\n\nNoise reduction aims to reduce the impact of wrong corrections in the official GEC datasets. One direction focuses on correcting noisy sentences. Mita et al. (2020) and Rothe et al. (2021) achieve this by incorporating a well-trained GEC model to reduce wrong corrections. The other direction attempts to down-weight noisy sentences. Lichtarge, Alberti, and Kumar (2020) introduce an offline re-weighting method to score each training sentence based on delta-log perplexity, \u2206ppl, which measures the model's log perplexity difference between checkpoints for a single sentence. Sentences with lower \u2206ppl are preferred and assigned a higher weight during training.\n\nModel enhancement augments official datasets to address the model's weakness. Parnow, Li, and Zhao (2021) aim to enhance performance by reducing the error density mismatch between training and inference. They use a generative adversarial network (GAN) (Goodfellow et al. 2014) to produce an ungrammatical sentence that could better represent the error density at inference time. Lai et al. (2022) also address the mismatch between training and inference, but specific to multi-round inference. They propose additional training stages that make the model consider edit type interdependence when predicting the corrections. Cao, Yang, and Ng (2021) aim to enhance model performance in low-error density domains. The augmented sentences are generated by beam search to capture wrong corrections that the model tends to make. Supervised contrastive learning (Chen et al. 2020b) is then applied to enhance model performance. Cao, Yang, and Ng (2023) use augmented sentences generated during beam search to address the exposure bias problem in seq2seq GEC models. A dynamic data reweighting method through reinforcement learning is used to select an optimal sampling strategy for different beam search candidates.", "filtered_refids": [[], ["b94", "b82", null], ["b123", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2044, "num_references": 5}
{"corpusid_sectionid": "253447259-s37", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Evaluation", "section": "A core component of any NLP system is the ability to measure model performance. This section hence first introduces the most commonly-used evaluation metrics in GEC, namely the MaxMatch (M 2 ) scorer (Dahlmeier and Ng 2012b), ER-RANT (Bryant, Felice, and Briscoe 2017;Felice, Bryant, and Briscoe 2016) and GLEU (Napoles et al. 2015, as well as other reference-based and reference-less metrics that have been proposed. It next discusses the problem of metric reliability, particularly in relation to correlation with human judgements, and explains why it is difficult to draw any robust conclusions. The section concludes with a discussion of best practices in GEC evaluation, including defining standard experimental settings and highlighting their limitations. To date, almost all evaluation in GEC has been carried out at the sentence level.", "filtered_refids": [["b111", null, "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 843, "num_references": 3}
{"corpusid_sectionid": "253447259-s40", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "ERRANT", "section": "The ERRANT scorer 10 (Bryant, Felice, and Briscoe 2017) is similar to the M 2 scorer, in that it is a reference-based metric that measures performance in terms of an edit-based F-score, but differs primarily in that it is also able to calculate error types scores. Specifically, unlike the M 2 scorer, it uses a linguistically-enhanced Damerau-Levenshtein alignment algorithm to extract edits from the hypothesis text (Felice, Bryant, and Briscoe 2016), and then classifies them according to a rule-based error type framework. This facilitates the calculation of F-scores for each error type rather than just overall, which can be invaluable for a detailed system analysis. For example, System A might outperform System B overall, but system B might outperform System A on certain error types, and this information can be used to improve System A.\n\nERRANT was the first scorer to be able to evaluate GEC systems in terms of error types and is moreover able to do so at three different levels of granularity: It is also able to carry out this analysis in terms of both error detection and correction.\n\nERRANT currently only supports English, but other researchers have independently extended it for German (Boyd 2018), Greek (Korre, Chatzipanagiotou, and Pavlopoulos 2021), Arabic (Belkebir and Habash 2021) and Czech (N\u00e1plava et al. 2022).", "filtered_refids": [[null], [], ["b12", "b9", "b76", "b109"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1339, "num_references": 5}
{"corpusid_sectionid": "253447259-s43", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Reference-based Metrics", "section": "I-measure. The I -measure (Felice and Briscoe 2015) was designed to overcome certain shortcomings of the M 2 scorer, e.g. the M 2 scorer is unable to differentiate between a bad system (TP=0, FP>0) and a do-nothing system (TP=0, FP=0) which both result in F=0, and instead measure system performance in terms of relative textual Improvement. The I -measure is calculated by carrying out a 3-way alignment between the original, hypothesis and reference texts and classifying each token according to an extended version of the Writer-Annotator-System (WAS) evaluation scheme . This ultimately enables the calculation of accuracy, which Felice and Briscoe (2015) modify to weight TPs and FPs differently to more intuitively reward or punish a system. Having calculated a weighted accuracy score for a system, a baseline weighted accuracy score is computed in the same manner using a copy of the original text as the hypothesis. The difference between these scores is then normalised to fall between -1 and 1, where I < 0 indicates text degradation and I > 0 indicates text improvement.\n\nGMEG. The GMEG metric (Napoles, N\u0103dejde, and Tetreault 2019) is an ensemble metric that was designed to correlate with human judgements on three different datasets. It was motivated by the observation that different metrics correlate very differently with human judgements in different domains, and so a better metric would be more consistent. As an ensemble metric, GMEG depends on features (e.g. precision and recall) from several other metrics, including M 2 , ERRANT, GLEU, and the I -measure (73 features in total). The authors then use these features to train a ridge regression model that was optimised to predict the human scores for different systems.\n\nGoToScorer. The GoToScorer (Gotou et al. 2020) was motivated by the observation that some errors are more difficult to correct than others yet all metrics treat them equally. The GoToScorer hence models error difficulty by weighting edits according to how many different systems were able to correct them; e.g., edits that were successfully corrected by all systems would yield a smaller reward than those successfully corrected by fewer systems. Although this methodology confirmed the intuition that some errors types were easier to correct than others, e.g. spelling errors (easy) vs. synonym errors (hard), one disadvantage of this approach is that the difficulty weights depend entirely on the type and number of systems involved. Consequently, results do not generalise well and error difficulty (or gravity) remains an unsolved problem.\n\nSErCl/SERRANT. SErCl (Choshen et al. 2020) is not a metric per se, but rather a method of automatically classifying grammatical errors by their syntactic properties using the Universal Dependencies formalism (Nivre et al. 2020). It is hence similar to ERRANT except it can more easily support other languages. The main disadvantage of SErCl is that it is not always meaningful to classify errors entirely based on their syntactic properties, e.g. spelling and orthography errors, and some error types are not very informative, e.g. \"VERB\u2192ADJ\". SERRANT (Choshen et al. 2021) is hence a compromise that attempts to combine the advantages of both SErCl and ERRANT.\n\nPT-M 2 . The pretraining-based MaxMatch (PT-M 2 ) metric (Gong et al. 2022) is a hybrid metric that combines traditional edit-based metrics, such as M 2 , with recent pretraining-based metrics, such as BERTScore ). The main advantage of pretraining-based metrics over edit-based metrics is that they are more capable of measuring the semantic similarity between pairs of sentences, rather than just comparing edits. Since Gong et al. (2022) found that off-the-shelf pretraining metrics correlated poorly with human judgements on GEC at the sentence level, they instead proposed measuring performance at the edit level. This approach ultimately produced the highest correlation with human judgements on the CoNLL-2014 test set to date, but should be considered with caution, as Hanna and Bojar (2021) also highlight some of the limitations of pretraining metrics and cite sources that claim correlation with human judgements may not be the best way to evaluate a metric (see Section 6.5).", "filtered_refids": [[null], ["b110"], [], ["b28", "b119", "b27"], [null, "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 4241, "num_references": 7}
{"corpusid_sectionid": "253447259-s44", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Reference-less Metrics", "section": "GBMs. The first work to explore the idea of a reference-less metric for GEC  was inspired by similar work on quality estimation in machine translation (e.g. Specia et al. (2020)). Specifically, the authors proposed three Grammaticality-Based Metrics (GBMs) that either use a benchmark GEC system to count the errors in the output produced by other GEC systems or else predict a grammaticality score using a pretrained ridge regression model (Heilman et al. 2014). The main limitation of these metrics is that they i) require an existing GEC system to evaluate other GEC systems and ii) are insensitive to changes in meaning. The authors thus proposed interpolating reference-less metrics with other reference-based metrics.\n\nGFM. Asano, Mizumoto, and Inui (2017) extended the work on GBMs by introducing three reference-less metrics for Grammaticality, Fluency and Meaning preservation (GFM). Specifically, the Grammaticality metric combines Napoles, Sakaguchi, and Tetreault's 2016 GBMs into a single model, the Fluency metric computes a score using a language model, and the Meaning preservation metric computes a score using the METEOR metric from machine translation (Denkowski and Lavie 2014). A weighted linear sum of the three scores is then used as the final score. The main weaknesses of the GFM metric are that the Grammaticality and Fluency metrics suffer from the same limitations as GBMs, and the Meaning preservation metric only models shallow text similarity in terms of overlapping content words.\n\nUSim. The USim metric (Choshen and Abend 2018c) was motivated by the fact that no other metric takes deep semantic similarity into account and it is possible that a GEC system might change the intended meaning of the original text; e.g., by inserting/deleting 'not' or replacing a content word with an incorrect synonym. It is calculated by first automatically annotating the original and hypothesis texts as semantic graphs using the UCCA semantic scheme (Abend and Rappoport 2013) and then measuring the overlap between the graphs (in terms of matching edges) as an F-score. USim was thus designed to operate as a complementary metric to other metrics. SOME. Sub-metrics Optimised for Manual Evaluation (SOME) (Yoshimura et al. 2020) is an extension of GFM that was designed to optimise each Grammaticality, Fluency and Meaning preservation metric to more closely correlate with human judgements. The authors achieved this by annotating the system output of five recent systems on a 5point scale for each metric and then fine-tuning BERT (Devlin et al. 2019) to predict these human scores. This differs from GFM in that GFM was fine-tuned to predict the human ranking of different systems rather than explicit human scores. While the authors found SOME correlates more strongly with human judgements than GFM, both metrics nevertheless suffer from the same limitations.\n\nScribendi Score. The Scribendi Score (Islam and Magnani 2021) was designed to be simpler than other reference-less metrics in that it requires neither an existing GEC system nor fine-tuning. Instead, it calculates an absolute score (1=positive, -1=negative, 0=no change) from a combination of language model perplexity (GPT2: Radford et al. (2019)) and sorted token/Levenshtein distance ratios, which respectively ensure that i) the corrected sentence is more probable than the original and ii) both sentences are not significantly different from each other. While it is intuitive that these scores correlate with the grammaticality of a sentence, they are not, however, a robust way of evaluating a GEC system. For example, the sentence 'I saw the cat\" is more probable than \"I saw a cat\" in GPT2 (160.8 vs 156.4), and both sentences are moreover very similar, yet we would not want to always reward this as a valid correction since both sentences are grammatical. We observe the same effect in \"I ate the cake.\" (130.2) vs. \"I ate the pie.\" (230.7) and so conclude that the Scribendi Score is highly likely to erroneously reward false positives.\n\nIMPARA. The Impact-based Metric for GEC using Parallel data (IMPARA) (Maeda, Kaneko, and Okazaki 2022) is a hybrid reference-based/reference-less metric that requires parallel data to train an edit-based quality estimation and semantic similarity model, but can be used as a reference-less metric after training. It is sensitive to the corpus it is trained on (i.e., it does not generalise well to unseen domains) but shows comparable or better performance to SOME in terms of correlation with human judgements. Its main advantage is that it only requires parallel data for training (i.e., not system output or human judgements), but its main disadvantage is that IMPARA scores are not currently interpretable by humans.", "filtered_refids": [["b147", "b56"], ["b38", "b5"], ["b183", "b39", "b26", "b0"], ["b129"], ["b91"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4756, "num_references": 10}
{"corpusid_sectionid": "253447259-s45", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Metric Reliability", "section": "Given the number of metrics that have been proposed, it is natural to wonder which metric is best. This is not straightforward to answer, however, as all metrics have different strengths and weaknesses. There has nevertheless been a great deal of work based on the assumption that the \"best\" metric is the one that correlates most closely with groundtruth human judgements.\n\nWith this in mind, the first work to compare metric performance with human judgements was by Napoles et al. (2015) and Grundkiewicz, Junczys-Dowmunt, and Gillian (2015), who independently collected human ratings for the 13 system outputs from the CoNLL-2014 shared task (including the unchanged original text) using the Ap-praise evaluation framework (Federmann 2010) commonly used in MT. This framework essentially asks humans to rank randomly chosen samples of 5 system outputs (ties are permitted) in order to build up a collection of pairwise judgements that can be used to extrapolate an overall system ranking. A metric can then be judged in terms of how well it correlates with this extrapolated ranking. The judgements collected by Grundkiewicz, Junczys-Dowmunt, and Gillian (2015) in particular proved especially influential (their dataset was much larger than Napoles et al. (2015)) and were variously used to justify GLEU as a better metric than M 2 (Napoles et al. 2015;Sakaguchi et al. 2016) and motivate almost all reference-less metrics to date (except USim).\n\nUnfortunately however, this methodology was later found to be problematic and many of the conclusions drawn using these datasets were thrown into doubt. Notable observations included:\n\n\u2022\n\nThe correlation coefficients reported by Napoles et al. (2015) and Grundkiewicz, Junczys-Dowmunt, and Gillian (2015) were very different even though they essentially carried out the same experiment (albeit on different samples) (Choshen and Abend 2018a).\n\n\u2022 This method of human evaluation was abandoned in machine translation due to unreliability (Choshen and Abend 2018a;Graham, Baldwin, and Mathur 2015).\n\n\u2022 Chollampatt and Ng (2018b) found no evidence of GLEU being a better metric than M 2 for ranking systems. Choshen and Abend (2018a) surmise that one of the reasons these metric correlation experiments proved unreliable is that rating sentences for grammaticality is a highly subjective task which often shows very low inter-annotator agreement (IAA); e.g. it is difficult to determine whether a sentence containing one major error should be considered \"more grammatical\" than a sentence containing two minor errors. Napoles, N\u0103dejde, and Tetreault (2019) nevertheless carried out a follow-up study which not only used a continuous scale to judge sentences (rather than rank them) (Sakaguchi and Van Durme 2018), but also collected judgements on all pairs of sentences to overcome sampling bias. They furthermore reported results on different datasets from different domains, rather than just CoNLL-2014, in an effort to determine the most generalisable metric. Their results, partially recreated in Table 7, hence found that dataset does indeed have an effect on metric performance, most likely because different error type distributions are judged inconsistently by humans. In fact, although Napoles, N\u0103dejde, and Tetreault (2019) reported very high IAA at the corpus level (0.9-0.99 Pearson/Spearman), IAA at the sentence level was still low to average (0.3-0.6 Pearson/Spearman).\n\nUltimately, although ground-truth human judgements may be an intuitive way to benchmark metric performance, they are also highly subjective and should be considered with caution. Nothing demonstrates this sentiment better than the conclusions drawn about the I -measure, which was initially found to have a weak negative correlation with human judgements (Napoles et al. 2015;Grundkiewicz, Junczys-Dowmunt, and Gillian 2015;Sakaguchi et al. 2016), subsequently found to have good correlation at the sentence level    Table 7 Pearson r and Spearman \u03c1 correlation coefficients for different metrics across three different datasets. This is a subset of the results reported in Napoles, N\u0103dejde, and Tetreault (2019) Table 8.\n\nmetric across multiple domains (Napoles, N\u0103dejde, and Tetreault 2019). Reliable methods of evaluating automatic metrics thus remain an active area of research.", "filtered_refids": [[], ["b49", "b111", "b140"], [], [], ["b49", "b111", "b24"], ["b24", null], ["b24", "b110", "b20", "b141"], ["b49", "b111", "b140"], ["b110"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 4315, "num_references": 16}
{"corpusid_sectionid": "253447259-s54", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "System Description", "section": "We first note that many of the systems in Table 8 are extensions of 3 other systems: Omelianchuk et al. (2020), Sun et al. (2021), and Kiyono et al. (2019). Specifically, Omelianchuk et al. (2020) built a sequence tagging model (Section 3.4) using a pre-trained language model (e.g. BERT) and 9 million synthetic sentence pairs, Sun et al. (2021) used a rule-based approach to generate 300 million synthetic sentence pairs (Section 5.1.1) to train a modified BART model which contains 12 encoders and 2 decoders, and Kiyono et al. (2019) used 70 million synthetic sentence pairs generated through back-translation (Section 5.1.2) to train a Transformer-big model.\n\nMany of these systems specifically build on top of Omelianchuk et al. (2020), including systems from Sorokin (2022);Lai et al. (2022);Parnow, Li, and Zhao (2021); Yasunaga, Leskovec, and Liang (2021). Specifically, Sorokin (2022) and Tarnavskyi, Chernodub, and Omelianchuk (2022) upgraded the pre-trained language model from base to large (e.g., RoBERTa-base vs. RoBERTa-large) and employed an additional mechanism to select the final edits by means of edit-scoring or majority voting (VT) respectively. Parnow, Li, and Zhao (2021) and Lai et al. (2022) address the problem of edit interdependence, i.e. when the correction of one error depends on another, by means of GANs and multi-turn training respectively. Yasunaga, Leskovec, and Liang (2021) applied the break-it-fix-it (BIFI) framework  to Omelianchuk et al. (2020) (Section 3.5) to gradually train a system that iteratively generates and learns from more realistic synthetic data. In contrast, Sun and Wang (2022) add a single hyperparameter to Sun et al. (2021) to control the trade-off between precision and recall (PRT), Kaneko et al. (2020) ", "filtered_refids": [["b72", "b120", "b152"], ["b67", "b146", "b152", "b153", "b120", "b123", null, "b159", "b180"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1769, "num_references": 12}
{"corpusid_sectionid": "253447259-s56", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Analysis", "section": "Despite all these enhancements, we first observe that it is very difficult to draw conclusions about the efficacy of different techniques in Table 8, because different systems were trained using different amounts/types of data (both real and artificial) and developed using different pre-trained models and performance-boosting techniques. Consequently, the systems are rarely directly comparable and we can only infer the relative advantages of different approaches from the wider context. With this in mind, the general trend in the past couple of years has been to scale models up using i) more artificial data, ii) multiple pre-trained models/architectures, and iii) multiple performance-boosting techniques.\n\nIn terms of artificial data, the trend is somewhat mixed, as on the one hand, Stahlberg and Kumar (2021) introduced a system trained on more than half a billion synthetic sentences, but on the other hand, they were still outperformed by systems that used orders of magnitude less data (Lai et al. 2022;Tarnavskyi, Chernodub, and Omelianchuk 2022). This pattern has been consistent for several years now and reveals a delicate trade-off between artificial data quantity and quality. There is ultimately no clear relationship between data quantity and performance, and some systems still achieve competitive performance without artificial data (Rothe et al. 2021;Katsumata and Komachi 2020).\n\nThe use of several pre-trained model architectures, however, tells a different story and it is generally the case that using multiple architectures improves performance: the top 3 latest state-of-the-art systems all use at least 2 different pre-trained models (Qorib, Na, and Ng 2022;Lai et al. 2022;Tarnavskyi, Chernodub, and Omelianchuk 2022). This suggests that different pre-training tasks capture different aspects of natural language that complement each other in different ways in GEC. In contrast, approaches that rely on a single pre-trained model typically perform slightly worse than those that combine architectures, although it is worth keeping in mind that there is also a trade-off between model complexity and run-time which is seldom reported (Omelianchuk et al. 2020;Sun et al. 2021).\n\nFinally, adding more performance-boosting techniques also tends to result in better performance, and the systems that incorporate the most techniques typically score highest. Among these techniques, the use of model ensembling or system combination (Section 4.2) mitigates the instability of neural models and allows a final system to make use of the strengths of several other systems. However, this comes at a cost to model complexity and run-time.", "filtered_refids": [[], ["b70", null, "b159"], ["b152", "b120", null, "b126", "b159"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2659, "num_references": 8}
{"corpusid_sectionid": "221970053-s4", "title": "A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English", "date": "2020-09-27", "section_title": "Rule and Feature Based Methods", "section": "Before the deep learning era, human-designed rules (Hobbs, 1978;Raghunathan et al., 2010), knowledge (Ponzetto and Strube, 2006;Versley et al., 2016), and features (Ng, 2005;Wiseman et al., 2016) dominated the general coreference resolution and PCR tasks. Some rules and features are crucial for correctly resolving pronouns (Lee et al., 2013). For example, 'he' typically refers to males and 'she' typically refers to females; 'it' typically refers to singular objects and 'them' typically refers to plural objects. The performances of these methods heavily rely on the coverage and quality of the manually defined rules and features. Based on these designed features (Bengtson and Roth, 2008), a few more advanced machine learning models were applied to the coreference resolution task. For example, instead of identifying coreference relation pair-wisely, (Clark and Manning, 2015) proposes an entity-centric coreference system that can learn an effective policy for building coreference chains incrementally. Besides that, a novel model was also proposed to predict coreference relations with a deep reinforcement learning framework (Clark and Manning, 2016). Moreover, heuristic rules based on linguistic knowledge can also be incorporated into constraints for machine learning models .", "filtered_refids": [["b44", "b4", "b14", "b29", "b32", "b45", "b21", "b5", "b0", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1291, "num_references": 10}
{"corpusid_sectionid": "221970053-s6", "title": "A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English", "date": "2020-09-27", "section_title": "Further Improvements", "section": "Recently, on top of the end-to-end model, a few improved works were proposed to address different limitations of the original end-to-end model 5 :\n\n1. Higher-order Information: One limitation of the original end-to-end model is that all predictions are based on pairs, which is not sufficient for capturing higher-order coreference relations. To fix this issue, a differentiable approximation module was proposed in  to provide the higher-order coreference resolution inference ability (i.e., leveraging the coreference cluster to better predict the coreference relations). Moreover, this work first incorporates ELMo (Peters et al., 2018), a kind of deep contextualized word representations, as part of the word representation, which is proven very effective.\n\n2. Structured Knowledge: Another limitation of the end-to-end model is that its success heavily relies on the quality and coverage of the training data. However, in real applications, it is labor-intensive and almost impossible to annotate a large-scale dataset to contain all scenar-  (Clark and Manning, 2015) 25.8 62.1 36.5 28.9 64.9 40.0 9.8 6.3 7.6 25.4 59.3 36.5 Deep-RL (Clark and Manning, 2016) 78.6 63.9 70.5 73.3 68.9 71.0 3.7 2.9 5.5 76.4 61.2 68.0\n\nEnd-to-end (Lee et al., 2017) (Zhang et al., 2019c) 80.0 75.6 77.7 81.7 72.2 76.7 50.8 64.6 56.9 77.9 74.0 75.9 + SpanBERT (Joshi et al., 2020) 82   ios. To solve this problem, two works (Zhang et al., 2019b,c) were proposed to inject external structured knowledge into the end-to-end model. Among these two, (Zhang et al., 2019b) requires converting external knowledge into features while (Zhang et al., 2019c) directly uses external knowledge in the format of triplets.", "filtered_refids": [[], ["b30"], ["b5", "b4"], ["b15", "b50", null, "b22", "b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1694, "num_references": 8}
{"corpusid_sectionid": "221970053-s8", "title": "A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English", "date": "2020-09-27", "section_title": "Performances and Analysis", "section": "We follow the experimental setting of (Zhang et al., 2019c) and test the performance 6 of representative models (Raghunathan et al., 2010;Manning, 2015, 2016;Lee et al., 2017;Zhang et al., 6 We use the released codes of different models along with their default hyper-parameters to finish the experiments. For the end2end model, we also include ELMo (Peters et al., 2018) as part of the representation and achieve better performance than the original one in Table 1. 2019c; Joshi et al., 2020) on the CoNLL-2012 dataset (Pradhan et al., 2012). The experiment setting (both detection the mentions and resolving the coreference relations) and evaluation metric are the same as these previous works on CoNLL-2012. From the results in Table 2, we can observe that with the help of the end-to-end model and further modifications, the community has made great progress on the standard evaluation set. For example, the end-to-end model achieves an F1 score over 70 and adding external knowledge (either in a structured way or a representation way) further boost the performance. Among all pronoun types, all models perform better on third personal and possessive pronouns, and relatively poorly on demonstrative ones. This is mainly because of the imbalanced distribution of the dataset (i.e., third personal and possessive pronouns appear much more than demonstrative ones).", "filtered_refids": [["b30", null, "b22", "b33", "b51", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1368, "num_references": 6}
{"corpusid_sectionid": "221970053-s16", "title": "A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English", "date": "2020-09-27", "section_title": "Reasoning with Structured Knowledge", "section": "At first, people tried to leverage different commonsense knowledge resources to solve WSC questions in an explainable way. For example, Liu et al. (2016) first leveraged the commonsense triplets from ConceptNet (Liu and Singh, 2004) to train the word embeddings and then applied the embeddings to solve the WSC task. Knowledge hunter (Emami et al., 2018) proposed to leverage search engines (e.g., Google) to acquire needed commonsense knowledge. It first searched WSC questions in search engines and then used the returned searching results to solve WSC questions. SP-10K (Zhang et al., 2019a) conducted experiments to show that selectional preference (SP) knowledge such as human beings are more likely to eat 'food' rather than 'rock' can also be helpful for solving WSC questions. Last but not least, ASER (Zhang et al., 2020) tried to use knowledge about eventualities (e.g., 'being hungry' can cause 'eat food') to solve WSC questions. In general, structured commonsense knowledge can help solve one-third of the WSC questions, but their overall performance is limited due to their low coverage. There are mainly two reasons: (1) coverage of existing commonsense resources are not large enough;\n\n(2) lack of a principled way to use structured knowledge for NLP tasks. Current methods (Emami et al., 2018;Zhang et al., 2019aZhang et al., , 2020 mostly rely on string match. However, for many WSC questions, it is hard to find supportive knowledge in such way.", "filtered_refids": [["b26", "b25", "b49", "b48", "b8"], ["b49", "b48", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1464, "num_references": 8}
{"corpusid_sectionid": "221970053-s17", "title": "A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English", "date": "2020-09-27", "section_title": "Language Representation Models", "section": "Another approach is leveraging language models to solve WSC questions (Trinh and Le, 2018), where each WSC question is first converted into two sentences by replacing the target pronoun with the two candidates respectively and then the language models can be employed to compute the probability of both sentences. The sentence with a higher probability will be selected as the final prediction. As this method does not require any string match, it can make prediction for all WSC questions and achieve better overall performance. Recently, a more advanced transformer-based language model GPT-2 (Radford et al., 2019) achieved better performance due to its stronger language representation ability. The success of language models demonstrates that rich commonsense knowledge can be indeed encoded within language models implicitly.\n\nAnother interesting finding about these language model based approaches is that they proposed two settings to predict the probability: (1) Full: use the probability of the whole sentence as the final prediction;\n\n(2) Partial: only consider the probability of the partial sentence after the target pronoun. Experiments show that the partial model always outperforms the full model. One explanation is that the influence of the imbalanced distribution of candidate words is relieved by only considering the sentence probability after them. Such observation also explains why GPT-2 can outperform unsuper-  (Emami et al., 2018) 119 79 75 60.1% 57.3% SP (Human) (Zhang et al., 2019a) 15 0 258 100% 52.7% SP (PP) (Zhang et al., 2019a) 50 26 197 65.8% 54.4% ASER (String Match) (Zhang et al., 2020) 63 27 183 70.0% 56.6% LM (Single) (Trinh and Le, 2018) 149 124 0 54.5% 54.5% LM (Ensemble) (Trinh and Le, 2018) 168 105 0 61.5% 61.5% GPT-2 (Radford et al., 2019) 193 80 0 70.7% 70.7%\n\nFinetuning BERT (Devlin et al., 2019) +ASER (Zhang et al., 2020) 177 96 0 64.5% 64.5% BERT (Devlin et al., 2019) +DPR (Rahman and Ng, 2012) 195 78 0 71.4% 71.4% BERT (Devlin et al., 2019) +WinoGrande (Sakaguchi et al., 2020) 210 63 0 76.9% 76.9% RoBERTa (Liu et al., 2019) +DRP (Rahman and Ng, 2012) 227 46 0 83.1% 83.1% RoBERTa (Liu et al., 2019) +WinoGrande (Sakaguchi et al., 2020) 246 27 0 90.1% 90.1%", "filtered_refids": [[], [], ["b35", "b49", "b48", "b42", "b8"], ["b6", "b39", "b27", "b49", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2216, "num_references": 10}
{"corpusid_sectionid": "221970053-s21", "title": "A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English", "date": "2020-09-27", "section_title": "Other PCR Tasks", "section": "Besides the ordinary and hard PCR tasks, PCR is also an important research topic for many special purposes (e.g., gender bias) or in some special settings (e.g., Visual-aware PCR). In this section, we briefly introduce these tasks:\n\n1. PCR in the Medical Domain: I2b2 (Uzuner et al., 2012) is a dataset that focuses on identify-10 The original batch size is 16 and our batch size is 4 due to the GPU memory limitation, so the experimental result is slightly different from the one reported in the original paper.\n\ning coreference relations in electronic medical records. As reported in (Zhang et al., 2019c), the training set of I2b2 contains 2,024 third personal pronouns, 685 possessive pronouns, and 270 demonstrative pronouns. Its test set contains 1,244 third personal pronouns, 367 possessive pronouns, and 166 demonstrative pronouns. As a dataset in a relatively narrow domain, the usage of domain knowledge becomes important. As shown in (Zhang et al., 2019c), i2b2 can be used as an additional dataset to evaluate models' cross-domain abilities. 3. PCR for Chatbots: CIC (Chen and Choi, 2016) is a dataset focusing on identifying coreference relations in multi-party conversations. Compared with the ordinary PCR tasks, which are mostly annotated on formal textual data (e.g., newswire), identifying coreference relation in conversation is more challenging.\n\n4. PCR for Studying Gender Bias: Nowadays, gender bias has been a hot research topic in the NLP community (Rudinger et al., 2018;Zhao et al., 2018). WinoGender (Rudinger et al., 2018) is among the most popular works. The setting of WinoGender is similar to the setting of WSC (Levesque et al., 2012), where each sentence contains one target pronoun and two candidate noun phrases and the models are required to select the correct antecedent from the two candidates. But the purpose is different. WSC aims at evaluating models' abilities to understand commonsense knowledge, while Wino-Gender aims at evaluating how well models can predict without the influence of gender bias. The experiments show that some gender bias (e.g., 'he' is more likely to be predicted to be the doctor rather than the nurse by the machine) indeed exists in pre-trained language representation models. Such observation is astonishing and motivates the community to think about how to minimize the influence of such gender bias.", "filtered_refids": [[], ["b43"], ["b51", "b2"], ["b24", "b52", "b38"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2372, "num_references": 6}
{"corpusid_sectionid": "237941087-s1", "title": "Coreference Resolution for the Biomedical Domain: A Survey", "date": "2021-09-25", "section_title": "Background", "section": "Coreference resolution in the general domain has a long history of being studied from early heuristicbased and rule-based approaches to recent learningbased approaches. Lee et al. (2017) proposed the first end-to-end neural coreference resolution model which uses LSTM encoder. Based on the end-to-end model, many extensions to the model have been proposed. BERT and SpanBERT were proposed to replace the LSTM encoder and achieved better performance on OntoNotes dataset (Joshi et al. 2019, Joshi et al. 2020. Wu et al. (2020) adapted questionanswering framework on coreference resolution, and achieved the state-of-the-art result with 83.1% F1 score on OntoNotes dataset. Ye et al. (2020) proposed a novel language representation model CorefBERT, which can capture the coreferential relations in context. However, these general coreference systems do not work well in the biomedical domain due to the lack of domain knowledge. For example, the end-to-end model (Lee et al., 2017) only achieved 33.85% and 61.25% F1 scores on CRAFT-CR and BioNLP datasets respectively (Trieu et al., 2018), but achieved 68.8% F1 score on OntoNotes dataset (Hovy et al., 2006), which covers multiple genres, such as newswire, broadcast news and web data.", "filtered_refids": [["b52", "b54", "b56", "b18", "b20", "b27", "b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1236, "num_references": 7}
{"corpusid_sectionid": "237941087-s2", "title": "Coreference Resolution for the Biomedical Domain: A Survey", "date": "2021-09-25", "section_title": "Biomedical Coreference Datasets", "section": "Several biomedical datasets with coreference annotations exist, but different document selection criteria, annotation schemes, domains and coreference types were used. The best known include:\n\nMEDSTRACT  is a corpus consisting of MEDLINE abstracts with coreference annotation. It is mainly concerned with two forms of anaphora: pronominal and sortal (definite noun phrase) anaphora. This corpus adapted the MUC-7 annotation scheme (Hirschman, 1997); in addition, semantic types from UMLS (Bodenreider, 2004) were also annotated.\n\nFlySlip (Gasperin et al., 2007) contains anaphoric links among noun phrases, including coreferent and associative relations. Different from MEDSTRACT, full-text biomedical articles were annotated in this corpus. FlySlip was annotated according to a domain-specific annotation scheme.\n\nGENIA-MedCo (Su et al., 2008) is a coreferentially annotated version of the GENIA corpus (Kim et al., 2003), which in turn consists of 1999 MED-LINE abstracts. This corpus follows the MUC-7 annotation scheme, but adds more linguistic based relations.\n\nDrugNerAR (Segura-Bedmar et al., 2010) was created to study anaphoric expressions in the task of extracting drug-drug interactions in pharmacological literature. This corpus consists of 49 fulltext from the DrugBank database, which contains 4900 drug entries.\n\nBioNLP-ST'11 COREF (Nguyen et al., 2011) was created in support of one of the tasks of the BioNLP 2011 shared task, focusing on finding anaphoric protein references, and based on the observation that one of major difficulties in event extraction is coreference resolution. This corpus was derived from three resources: MedCo coreference annotation (Su et al., 2008), Genia event annotation (Kim et al., 2008), and Genia Treebank (Tateisi et al., 2005).\n\nHANAPIN (Batista-Navarro and Ananiadou, 2011) is comprised of 20 full-text articles from biochemistry literature. In addition to nominal and pronominal anaphora, this corpus also annotated abbreviation/acronyms and numerical anaphora. CRAFT-CR (Cohen et al., 2017) consists of 97 full-text biomedical journal articles. Similar to the general domain, this corpus was annotated with coreferent chains in full-text articles, while most other biomedical coreference datasets focuse on annotating the pairwise coreference relation between an anaphor and its antecedent. In addition, all coreference expressions were annotated regardless of semantic type.\n\nThese datasets are summarized in Table 1.", "filtered_refids": [[], ["b3", "b17"], ["b12"], ["b47", "b22"], [], ["b49", "b47", "b23", "b37"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2474, "num_references": 10}
{"corpusid_sectionid": "237941087-s4", "title": "Coreference Resolution for the Biomedical Domain: A Survey", "date": "2021-09-25", "section_title": "Pre-training on biomedical corpora", "section": "Following the success of large-scale pre-training language models (PLMs) in the general domain, several biomedical-domain PLMs have been developed in recent years by pre-training on large-scale biomedical corpora. Most biomedical PLMs conduct continual pretraining of the general domain PLMs and still use vocabulary trained on the general domain text. BioBERT  is the first transformerbased biomedical PLM, pre-trained on PubMed abstracts and PubMed Central full-text articles. Clini-calBERT and Bio_ClinicalBERT (Alsentzer et al., 2019) are pre-trained on MIMIC-III Clinical Notes, whereas BlueBERT (Peng et al., 2019) uses both PubMed and MIMIC-III for pre-training. All these models are pre-trained based on general BERT, except Bio_ClinicalBERT which is initialized from BioBERT.\n\nIn addition to initializing from general BERT, some biomedical PLMs are directly pre-trained on biomedical text from scratch and use domainspecific custom vocabulary. SciBERT (Beltagy et al., 2019) is pre-trained on biomedical and computer science papers from scratch and achieved good performance on many scientific NLP tasks. PubMedBERT (Gu et al., 2020) and BioELECTRA (raj Kanakarajan et al., 2021) are both pre-trained on PubMed abstract and PubMed Central full text articles, but the latter adopts ELECTRA architecture (Clark et al., 2019). BioMegatron (Shin et al., 2020) is a large-scale model based on Megatron (Shoeybi et al., 2019) architecture. It also investigated the effect of vocabulary and corpora domain on the performance of biomedical tasks.", "filtered_refids": [["b38", "b0"], ["b44", "b45", null, "b7", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1547, "num_references": 7}
{"corpusid_sectionid": "237941087-s5", "title": "Coreference Resolution for the Biomedical Domain: A Survey", "date": "2021-09-25", "section_title": "Integrating biomedical knowledge bases", "section": "Although the biomedical PLMs, such as BioBERT, have achieved good performance on many biomedical tasks, however, these models can be further enhanced by integrating biomedical knowledge bases, such as UMLS (Bodenreider, 2004).\n\nSeveral models enhance biomedical PLMs by integrating synonym knowledge from UMLS. Each mention in the biomedical text can be linked to a Concept Unique Identifier (CUI) in UMLS, and each CUI has a synonym set. SAPBERT , UMLSBERT (Michalopoulos et al., 2021) and BIOSYN (Sung et al., 2020) further pre-trained PubMedBERT, Bio_Clinical BERT and BioBERT on UMLS synonyms, using multi-similarity loss, multi-label loss and synonym marginalization algorithm respectively.\n\nIn addition to synonym knowledge, Clinical KB-BERT (Hao et al., 2020) injects UMLS relation knowledge into BioBERT. Whereas CODER  learns both synonym and relation knowledge based on PubMedBERT or mBERT (Devlin et al., 2019) via contrastive learning. Also, some research focus on fusing the UMLS entity embeddings with contextual embeddings to improve biomedical PLMs (He et al., 2020;Yuan et al., 2021).\n\nThis paper selected some of the models above to evaluate the ability of biomedical-specific representation for biomedical coreference task, detailed in Section 6.", "filtered_refids": [["b3"], ["b48", "b34"], ["b15", "b16", "b9", "b57"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1265, "num_references": 7}
{"corpusid_sectionid": "237941087-s7", "title": "Coreference Resolution for the Biomedical Domain: A Survey", "date": "2021-09-25", "section_title": "Rule-based models", "section": "Early approaches to biomedical coreference resolution are primarily rule-based. These models rely on syntactic parsers to extract hand-crafted features and rules. Nguyen et al. (2012) implemented a protein coreference system that makes use of syntactic information from the parser output, and proteinindicated information. The results showed that domain-specific semantic information is important for coreference resolution.  developed a rule-based coreference system, as a part of the EventMine event extraction system. A set of rules was developed based on syntactic trees and predicate-argument structures. The system achieved 55.9% F1 score on BioNLP 2011 protein coreference task. Kilicoglu and Demner-Fushman (2016) developed a new corpus of structured drug labels and proposed a general framework based on a smorgasbord architecture for finegrained biomedical coreference resolution. The framework adopted different strategies for each coreference type and mention type, and combined them to reach desired performance, like selecting dishes from a smorgasbord. Li et al. (2018) presented two methods for bio-entity coreference resolution: a rule-based method and a recurrent neural network (RNN) model. The rule-based model created a set of syntactic rules or semantic constraints for coreference and achieved a state-of-the-art performance with 62.0% F1 score on BioNLP 2011 protein coreference task.\n\nThese rule-based models mostly designed rules for specific type of coreference relation and even specific corpus, which limits the scope of the resolution.", "filtered_refids": [["b21", "b29", "b36"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1565, "num_references": 3}
{"corpusid_sectionid": "237941087-s8", "title": "Coreference Resolution for the Biomedical Domain: A Survey", "date": "2021-09-25", "section_title": "Machine learning-based models", "section": "In the early years, due to the lack of publicly available annotated corpora, researchers have to annotate their own corpora for developing machine learning approaches (Yang et al., 2004, Torii and Vijay-Shanker, 2005, Su et al., 2008, Gasperin, 2009.\n\nAfter the BioNLP 2011 protein coreference dataset was made publicly available, several machine learning-based models were developed for this task.  adapted a general coreference system Reconcile (Stoyanov et al., 2010) for the biomedical domain by modifying several components to biomedical texts. It trained two separate classifiers for detecting anaphora and antecedent mentions.\n\nIn addition to using machine learning-based methods only, several models adopted hydrid approach, i.e., combining both machine learning-  based and rule-based methods. D'Souza and Ng (2012) proposed a hybrid approach that used a classifier with syntactic path-based features. It investigated five different learning-based methods, and a rule-based approach for anaphora resolution. This model achieved a superior performance than previous either rule-based or learning-based models on BioNLP 2011 protein coreference task. Li et al. (2014) later also used a hybrid approach, adopting the rule-based method or the machine learning method for three types of anaphora. As the method of D'Souza and Ng (2012), they also used different rules for different types of anaphora. The system achieved better performance with 68.6% F1 score than previous methods on BioNLP 2011 protein coreference development data.", "filtered_refids": [["b47", "b13", "b50", "b55"], ["b46"], ["b30"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1538, "num_references": 6}
{"corpusid_sectionid": "237941087-s9", "title": "Coreference Resolution for the Biomedical Domain: A Survey", "date": "2021-09-25", "section_title": "Deep learning-based models", "section": "In recent years, much effort has been made on using deep learning methods for biomedical coreference. Trieu et al. (2018) applied general domain endto-end neural coreference resolution system (Lee et al., 2017) to biomedical text, integrating the domain specific features to enhance the system. The model was evaluated on BioNLP 2011 protein coreference dataset and CRAFT-CR dataset. The results indicated that in-domain embeddings and domain-specific features helped improve the performance. Then, Trieu et al. (2019) proposed a system to address the challenge of coreference resolution in the full-text articles in the CRAFT-CR dataset. The model also applied end-to-end system (Lee et al., 2017), but enhanced the system by utilizing a syntax-based mention filtering method and replacing LSTM with BERT. This model achieved better performance on the CRAFT-CR dataset.\n\nDifferent from the models above, Li et al. (2021) integrated external knowledge to enhance the neural coreference system for biomedical texts. A knowledge attention module was developed to select the most related and helpful knowledge triplets. This model achieved the state-of-the-art perfor-   53.5 69.8 60.5 50.4 62.7 55.9 (D' Souza and Ng, 2012) 59.9 77.1 67.4 55.6 67.2 60.9 (Li et al., 2014) 69.8 67.5 68.6 ---Simple system (Choi et al., 2016) 64  (Trieu et al., 2018) 56.7 71.7 63.1 47.5 55.6 51.2 KB-attention (Li et al., 2021) 63.4 68.1 65.6 69.4 69.6 69.5 ", "filtered_refids": [["b52", "b51", "b27"], ["b52", "b6", "b31", "b30", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1438, "num_references": 8}
{"corpusid_sectionid": "237941087-s16", "title": "Coreference Resolution for the Biomedical Domain: A Survey", "date": "2021-09-25", "section_title": "Experimental setup", "section": "We conduct experiment using following models:\n\nbiomedical PLMs+c2f-coref: we refer to the higher-order coreference model (Lee et al., 2018) as c2f-coref. We build the c2f-coref system on top of different biomedical PLMs respectively, including BioBERT, SciBERT, Bio_ClinicalBERT, PubMedBERT, UMLS-BERT, and Clinical KB-BERT. Among these models, UMLSBERT and Clinical KB-BERT integrate external biomedical knowledge base, i.e., UMLS, while other models are pretrained on large-scale biomedical datasets.\n\n-BERT_base+c2f-coref (Joshi et al., 2019): the c2f-coref system on top of BERT representation.\n\n-SpanBERT_base+c2f-coref (Joshi et al., 2020): the c2f-coref system on top of Span-BERT_base, which pre-trained span representations to better represent and predict spans of text.\n\nWe run these models on the CRAFT-CR dataset of latest released version 4.0.1 3 . CRAFT-CR con-  sists of 97 full-text journal articles from PMC. As shown in Table 2, 60 documents are used for finetuning these models. These models are fine-tuned using learning rate of 1\u00d710 \u22125 for PLMs parameters and 2\u00d710 \u22124 for task parameters with Adam optimizer, a dropout of 0.3, and max_training_len of 384 for Span-BERT_base and 128 for other PLMs respectively. For SciBERT and PubMedBERT, we use the specific domain vocabulary, while general BERT vocabulary is used for other models.\n\nFor evaluation, we calculate F1 scores on six common metrics including B 3 , BLANC, CEAFE, CEAFM, LEA and MUC using the official evaluation script 4 provided by the CRAFT shared task organizers, which is also used by previous models (Trieu et al. 2018, Trieu et al. 2019, Li et al. 2021.", "filtered_refids": [[], ["b28"], ["b20"], ["b19"], [], ["b52", "b51", "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1643, "num_references": 6}
{"corpusid_sectionid": "11591301-s3", "title": "A Survey on the Role of Negation in Sentiment Analysis", "date": "2010-07-10", "section_title": "Negation and Bag of Words in Supervised Machine Learning", "section": "Several research efforts in polarity classification employ supervised machine-learning algorithms, like Support Vector Machines, Na\u00efve Bayes Classifiers or Maximum Entropy Classifiers. For these algorithms, already a low-level representation using bag of words is fairly effective (Pang et al., 2002). Using a bag-of-words representation, the supervised classifier has to figure out by itself which words in the dataset, or more precisely feature set, are polar and which are not. One either considers all words occurring in a dataset or, as in the case of Pang et al. (2002), one carries out a simple feature selection, such as removing infrequent words. Thus, the standard bag-of-words representation does not contain any explicit knowledge of polar expressions. As a consequence of this simple level of representation, the reversal of the polarity type of polar expressions as it is caused by a negation cannot be explicitly modeled.\n\nThe usual way to incorporate negation modeling into this representation is to add artificial words: i.e. if a word x is preceded by a negation word, then rather than considering this as an occurrence of the feature x, a new feature NOT x is created. The scope of negation cannot be properly modeled with this representation either. Pang et al. (2002), for example, consider every word until the next punctuation mark. Sentence 2 would, therefore, result in the following representation:\n\n8. I do not NOT like NOT this NOT new NOT Nokia NOT model.\n\nThe advantage of this feature design is that a plain occurrence and a negated occurrence of a word are reflected by two separate features. The disadvantage, however, is that these two contexts treat the same word as two completely different entities.\n\nSince the words to be considered are unrestricted, any word -no matter whether it is an actual polar expression or not -is subjected to this negation modification. This is not only linguistically inaccurate but also increases the feature space with more sparse features (since the majority of words will only be negated once or twice in a corpus). Considering these shortcomings, it comes to no surprise that the impact of negation modeling on this level of representation is limited. Pang et al. (2002) report only a negligible improvement by adding the artificial features compared to plain bag of words in which negation is not considered. Despite the lack of linguistic plausibility, supervised polarity classifiers using bag of words (in particular, if training and testing are done on the same domain) offer fairly good performance. This is, in particular, the case on coarse-grained classification, such as on document level. The success of these methods can be explained by the fact that larger texts contain redundant information, e.g. it does not matter whether a classifier cannot model a negation if the text to be classified contains twenty polar opinions and only one or two contain a negation. Another advantage of these machine learning approaches on coarsegrained classification is their usage of higher order n-grams. Imagine a labeled training set of documents contains frequent bigrams, such as not appealing or less entertaining. Then a feature set using higher order n-grams implicitly contains negation modeling. This also partially explains the effectiveness of bigrams and trigrams for this task as stated in (Ng et al., 2006). The dataset used for the experiments in (Pang et al., 2002;Ng et al., 2006) has been established as a popular benchmark dataset for sentiment analysis and is publicly available 1 .", "filtered_refids": [["b17"], ["b17"], [], [], ["b15", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3571, "num_references": 4}
{"corpusid_sectionid": "11591301-s5", "title": "A Survey on the Role of Negation in Sentiment Analysis", "date": "2010-07-10", "section_title": "Contextual Valence Shifters", "section": "The first computational model that accounts for negation in a model that includes knowledge of polar expressions is (Polanyi and Zaenen, 2004). The different types of negations are modeled via contextual valence shifting. The model assigns scores to polar expressions, i.e. positive scores to positive polar expressions and negative scores to negative polar expressions, respectively. If a polar expression is negated, its polarity score is simply inverted (see Example 1).\n\nIn a similar fashion, diminishers are taken into consideration. The difference is, however, that the score is only reduced rather than shifted to the other polarity type (see Example 2).\n\nBeyond that the model also accounts for modals, presuppositional items and even discourse-based valence shifting. Unfortunately, this model is not implemented and, therefore, one can only speculate about its real effectiveness.\n\nKennedy and Inkpen (2005) evaluate a negation model which is fairly identical to the one proposed by Polanyi and Zaenen (2004) (as far as simple negation words and diminishers are concerned) in document-level polarity classification. A simple scope for negation is chosen. A polar expression is thought to be negated if the negation word immediately precedes it. In an extension of this work (Kennedy and Inkpen, 2006) a parser is considered for scope computation. Unfortunately, no precise description of how the parse is used for scope modeling is given in that work. Neither is there a comparison of these two scope models measuring their respective impacts.\n\nFinal results show that modeling negation is important and relevant, even in the case of such simple methods. The consideration of negation words is more important than that of diminishers. Wilson et al. (2005) carry out more advanced negation modeling on expression-level polarity classification. The work uses supervised machine learning where negation modeling is mostly encoded as features using polar expressions. The features for negation modeling are organized in three groups:", "filtered_refids": [["b18"], [], [], ["b7", "b18"], ["b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2039, "num_references": 4}
{"corpusid_sectionid": "11591301-s6", "title": "A Survey on the Role of Negation in Sentiment Analysis", "date": "2010-07-10", "section_title": "Features for Negation Modeling", "section": "\u2022 negation features \u2022 shifter features \u2022 polarity modification features Negation features directly relate to negation expressions negating a polar expression. One feature checks whether a negation expression occurs in a fixed window of four words preceding the polar expression. The other feature accounts for a polar predicate having a negated subject. This frequent long-range relationship is illustrated in Sentence 9. All negation expressions are additionally disambiguated as some negation words do not function as a negation word in certain contexts, e.g. not to mention or not just. Shifter features are binary features checking the presence of different types of polarity shifters. Polarity shifters, such as little, are weaker than ordinary negation expressions. They can be grouped into three categories, general polarity shifters, positive polarity shifters, and negative polarity shifters. General polarity shifters reverse polarity like negations. The latter two types only reverse a particular polarity type, e.g. the positive shifter abate only modifies negative polar expressions as in abate the damage. Thus, the presence of a positive shifter may indicate positive polarity. The set of words that are denoted by these three features can be approximately equated with diminishers. Finally, polarity modification features describe polar expressions of a particular type modifying or being modified by other polar expressions. Though these features do not explicitly contain negations, language constructions which are similar to negation may be captured. In the phrase [disappointed \u2212 hope + ] \u2212 , for instance, a negative polar expression modifies a positive polar expression which results in an overall negative phrase. Adding these three feature groups to a feature set comprising bag of words and features counting polar expressions results in a significant improvement. In (Wilson et al., 2009), the experiments of Wilson et al. (2005) are extended by a detailed analysis on the individual effectiveness of the three feature groups mentioned above. The results averaged over four different supervised learning algorithms suggest that the actual negation features are most effective whereas the binary polarity shifters have the smallest impact. This is consistent with Kennedy and Inkpen (2005) given the similarity of polarity shifters and diminishers.\n\nConsidering the amount of improvement that is achieved by negation modeling, the improvement seems to be larger in (Wilson et al., 2005). There might be two explanations for this. Firstly, the negation modeling in (Wilson et al., 2005) is considerably more complex and, secondly, Wilson et al. (2005) evaluate on a more fine-grained level (i.e. expression level) than Kennedy and Inkpen (2005) (they evaluate on document level). As already pointed out in \u00a73.1, document-level polarity classification contains more redundant information than sentence-level or expression-level polarity classification, therefore complex negation modeling on these levels might be more effective since the correct contextual interpretation of an individual polar expression is far more important 2 . The fine-grained opinion corpus used in (Wilson et al., 2005;Wilson et al., 2009) and all the resources necessary to replicate the features used in these experiments are also publicly available 3 .", "filtered_refids": [["b24", "b6", "b25"], ["b24", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3354, "num_references": 5}
{"corpusid_sectionid": "11591301-s8", "title": "A Survey on the Role of Negation in Sentiment Analysis", "date": "2010-07-10", "section_title": "Semantic Composition", "section": "In (Moilanen and Pulman, 2007), a method to compute the polarity of headlines and complex noun phrases using compositional semantics is presented. The paper argues that the principles of this linguistic modeling paradigm can be successfully applied to determine the subsentential polarity of the sentiment expressed, demonstrating it through its application to contexts involving sentiment propagation, polarity reversal (e.g. through the use of negation following Polanyi and Zaenen (2004) and Kennedy and Inkpen (2005)) or polarity conflict resolution. The goal is achieved through the use of syntactic representations of sentences, on which rules for composition are defined, accounting for negation (incrementally applied to constituents depending on the scope) using negation words, shifters and negative polar expressions. The latter are subdivided into different categories, such that special words are defined, whose negative intensity is strong enough that they have the power to change the polarity of the entire text spans or constituents they are part of. A similar approach is presented by Shaikh et al. (2007). The main difference to Moilanen and Pulman (2007) lies in the representation format on which the compositional model is applied. While Moilanen and Pulman (2007) use syntactic phrase structure trees, Shaikh et al. (2007) consider a more abstract level of representation being verb frames. The advantage of a more abstract level of representation is that it more accurately represents the meaning of the text it describes. Apart from that, Shaikh et al. (2007) design a model for sentence-level classification rather than for headlines or complex noun phrases. The approach by Moilanen and Pulman (2007) is not compared against another established classification method whereas the approach by Shaikh et al. (2007) is evaluated against a non-compositional rule-based system which it outperforms.", "filtered_refids": [["b11", "b6", "b19", "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1919, "num_references": 4}
{"corpusid_sectionid": "11591301-s9", "title": "A Survey on the Role of Negation in Sentiment Analysis", "date": "2010-07-10", "section_title": "Shallow Semantic Composition", "section": "Choi and Cardie (2008) present a more lightweight approach using compositional semantics towards classifying the polarity of expressions. Their working assumption is that the polarity of a phrase can be computed in two steps:\n\n\u2022 the assessment of polarity of the constituents NP2 in rural areas. The advantage of these rules is that they restrict the scope of negation to specific constituents rather than using the scope of the entire target expression. Such inference rules are very reminiscent of polarity modification features (Wilson et al., 2005), as a negative polar expression is modified by positive polar expression. The rules presented by Choi and Cardie (2008) are, however, much more specific, as they define syntactic contexts of the polar expressions. Moreover, from each context a direct polarity for the entire expression can be derived. In (Wilson et al., 2005), this decision is left to the classifier. The rules are also similar to the syntactic rules from Moilanen and Pulman (2007). However, they involve less linguistic processing and are easier to comprehend 4 . The effectiveness of these rules are both evaluated in rule-based methods and a machine learning based method where they are anchored as constraints in the objective function. The results of their evaluation show that the compositional methods outperform methods using simpler scopes for negation, such as considering the scope of the entire target expression. The learning method incorporating the rules also slightly outperforms the (plain) rule-based method.", "filtered_refids": [[], ["b24", "b11", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1548, "num_references": 3}
{"corpusid_sectionid": "11591301-s10", "title": "A Survey on the Role of Negation in Sentiment Analysis", "date": "2010-07-10", "section_title": "Scope Modeling", "section": "In sentiment analysis, the most prominent work examining the impact of different scope models for negation is (Jia et al., 2009). The scope detection method that is proposed considers:\n\n\u2022 static delimiters \u2022 dynamic delimiters \u2022 heuristic rules focused on polar expressions Static delimiters are unambiguous words, such as because or unless marking the beginning of another clause. Dynamic delimiters are, however, ambiguous, e.g. like and for, and require disambiguation rules, using contextual information such as their pertaining part-of-speech tag. These delimiters suitably account for various complex sentence types so that only the clause containing the negation is considered. The heuristic rules focus on cases in which polar expressions in specific syntactic configurations are directly preceded by negation words which results in the polar expression becoming a delimiter itself. Unlike Choi and Cardie (2008), these rules require a proper parse and reflect grammatical relationships between different constituents. The complexity of the scope model proposed by Jia et al. (2009) is similar to the ones of the compositional models (Moilanen and Pulman, 2007;Shaikh et al., 2007;Choi and Cardie, 2008) where scope modeling is exclusively incorporated in the compositional rules. Apart from scope modeling, Jia et al. (2009) also employ a complex negation term disambiguation considering not only phrases in which potential negation expressions do not have an actual negating function (as already used in (Wilson et al., 2005)), but also negative rhetorical questions and restricted comparative sentences. On sentence-level polarity classification, their scope model is compared with \u2022 a simple negation scope using a fixed window size (similar to the negation feature in (Wilson et al., 2005)) \u2022 the text span until the first occurrence of a polar expression following the negation word\n\n\u2022 the entire sentence\n\nThe proposed method consistently outperforms the simpler methods proving that the incorporation of linguistic insights into negation modeling is meaningful. Even on polarity document retrieval, i.e. a more coarse-grained classification task where contextual disambiguation usually results in a less significant improvement, the proposed method also outperforms the other scopes examined. There have only been few research efforts in sentiment analysis examining the impact of scope modeling for negation in contrast to other research areas, such as the biomedical domain (Huang and Lowe, 2007;Morante et al., 2008;Morante and Daelemans, 2009). This is presumably due to the fact that only for the biomedical domain, publicly available corpora containing annotation for the scope of negation exist (Szarvas et al., 2008). The usability of those corpora for sentiment analysis has not been tested.", "filtered_refids": [["b5"], ["b11", "b19", "b24", "b5", "b3"], [], ["b14", "b13", "b4", "b21"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2816, "num_references": 10}
{"corpusid_sectionid": "11591301-s17", "title": "A Survey on the Role of Negation in Sentiment Analysis", "date": "2010-07-10", "section_title": "Limits of Negation Modeling in Sentiment Analysis", "section": "So far, this paper has not only outlined the importance of negation modeling in sentiment analysis but it has also shown different ways to account for this linguistic phenomenon. In this section, we present the limits of negation modeling in sentiment analysis. Earlier in this paper, we stated that negation modeling depends on the knowledge of polar expressions. However, the recognition of genuine polar expressions is still fairly brittle. Many polar expressions, such as disease are ambiguous, i.e. they have a polar meaning in one context (Sentence 12) but do not have one in another (Sentence 13).\n\n12. He is a disease to every team he has gone to.\n\n13. Early symptoms of the disease are headaches, fevers, cold chills and body pain.\n\nIn a pilot study (Akkaya et al., 2009), it has already been shown that applying subjectivity word sense disambiguation in addition to the featurebased negation modeling approach of Wilson et al. (2005) results in an improvement of performance in polarity classification. Another problem is that some polar opinions are not lexicalized. Sentence 14 is a negative pragmatic opinion (Somasundaran and Wiebe, 2009) which can only be detected with the help of external world knowledge.\n\n14. The next time I hear this song on the radio, I'll throw my radio out of the window.\n\nMoreover, the effectiveness of specific negation models can only be proven with the help of corpora containing those constructions or the type of language behaviour that is reflected in the models to be evaluated. This presumably explains why rare constructions, such as negations using connectives (Sentence 6 in \u00a72), modals (Sentence 7 in \u00a72) or other phenomena presented in the conceptual model of Polanyi and Zaenen (2004), have not yet been dealt with.", "filtered_refids": [[], [], [], ["b24", "b20", "b0"], [], ["b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1770, "num_references": 4}
{"corpusid_sectionid": "231839511-s3", "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries", "date": "2021-02-04", "section_title": "Sampling or selection bias", "section": "Sampling bias occurs when observations are drawn from an unrepresentative subset of the population being studied (Marshall, 1996) and applied more widely. In our context, this might arise when selecting communities from which to collect language data, or specific individuals within each community. When sampling communities, bias can be introduced if convenience is prioritized. Communities which are easier to access may not produce language data representative of a larger area or group. This can be illustrated through Uganda's refugee response, which consists of 13 settlements (including the 2nd largest in the world) hosted in 12 districts (UNHCR, 2020). Data collection may be easier in one of the older, established settlements; however, such data cannot be generalised over the entire refugee response due to different cultural backgrounds, length of stay of refugees in different areas, and the varied stages along the humanitarian chain -emergency, recovery or developmentfound therein (Winter, 1983;OECD, 2019). Prioritizing convenience in this case may result in corpora which over-represents the cultural and economic contexts of more established, longer-term refugees. When sampling interviewees, bias can be introduced when certain sub-sets of a community have more data collected than others (Bryman, 2012). This is seen when data is collected only from men in a community due to cultural norms (Nadal, 2017), or only from wealthier people in cell-phone-based surveys (Labrique et al., 2017).", "filtered_refids": [["b28", "b41", "b80", null, "b19", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1510, "num_references": 6}
{"corpusid_sectionid": "231839511-s4", "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries", "date": "2021-02-04", "section_title": "Observer bias", "section": "Observer bias occurs when there are systematic errors in how data is recorded, which may stem from observer viewpoints and predispositions (Gonsamo and D'Odorico, 2014). We identify three key observer biases relevant to our context. Firstly, confirmation bias, which refers to the tendency to look for information which confirms one's preconceptions or hypotheses (Nickerson, 1998). Researchers collecting data in LICs may expect interviewees to express needs or hardships based on their preconceptions. As Kumar (1987) points out, \"often they hear what they want to hear and ignore what they do not want to hear\". A team conducting a needs assessment for a rural electrification project, for instance, may expect a need for electricity, and thus consciously or subconsciously seek data which confirms this, interpret potentially unrelated data as electricity-motivated (Hirmer and Guthrie, 2017), or omit data which contradicts their hypothesis (Peters, 2020). Using such data to train NLP models may introduce unintentional bias towards the original expectations of the researchers instead of accurately representing the community.\n\nSecondly, the interviewer's understanding and interpretation of the speaker's utterances might be influenced by their class, culture and language. Note that, particularly in countries without strong language standardisation policies, consistent semantic shifts can happen even between varieties spoken in neighboring regions (Gordon, 2019), which may result in systematic misunderstanding (Sayer, 2013). For example, in the neighboring Ugandan tribes of Toro and Bunyoro, the same word omunyoro means respectively husband and a member of the tribe. Language data collected in such contexts, if not properly handled, may contain inaccuracies which lead to NLP models that misrepresent these tribes. Rich information communicated through gesture, expression, and tone (i.e. nonverbal data, Oliver et al. (2005)) may also be systematically lost during verbatim transcription, causing inadvertent inconsistencies in the corpora.\n\nThirdly, interviewer bias, which refers to the subjectivity unconsciously introduced into data gathering by the worldview of the interviewer (Frey, 2018). For instance, a deeply religious interviewer may unintentionally frame questions through religious language (e.g. it is God's will, thank God, etc.), or may perceive certain emotions (e.g. thankfulness) as inherently religious, and record language data including this perception. The researcher's attitude and behaviour may also influence responses (Silverman, 2013); for instance, when interviewers take longer to deliver questions, interviewees tend to provide longer responses (Matarazzo et al., 1963). Unlike in internetbased language data collection, where all speakers are exposed to uniform, text-based interfaces, collecting data from illiterate communities necessitates the presence of an interviewer, who cannot always be the same person due to scalability constraints, introducing this inevitable variability and subsequent data bias.", "filtered_refids": [["b6", "b39", "b47", null, "b17"], ["b42"], ["b30", null, "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 3061, "num_references": 9}
{"corpusid_sectionid": "231839511-s5", "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries", "date": "2021-02-04", "section_title": "Response bias", "section": "Response bias occurs when speakers provide inaccurate or false responses to questions. This is particularly important when working in rural settings, where the majority of data collection is currently related to SD projects. The majority of existing data is biased by the projects for which it has been collected, and any newly collected data for NLP uses is also likely to be used in decision making for SD. This inherent link of data collection to material development outcomes inevitably affects what is communicated. There are five key response biases relevant to our context.\n\nFirstly, recall bias, where speakers recall only certain events or omit details (Coughlin, 1990). This is often as a result of external influences, such as the presence of a data collector who is new to the community. Recall can also be affected by the distortion or amplification of traumatic memories (Strange and Takarangi, 2015); if data is collected around a topic a speaker may find traumatic, recall bias may be unintentionally introduced.\n\nSecondly, social desirability bias, which refers to the tendency of interviewees to provide socially desirable/acceptable responses rather than honest responses, particularly in certain interview contexts (Bergen and Labont\u00e9, 2020). In tight-knit rural communities, it may be difficult to deviate from traditional social norms, leading to biased data. As an illustrative example, researchers in Nepal found that interviewer gender affected the detail in responses to some sensitive questions (e.g. sex and contraception): participants provided less detail to male interviewers (Axinn, 1991). Social desirability bias can produce corpora which misrepresent community social dynamics or under-represent sensitive topics.\n\nThirdly, recency effect or serial-position, which is the tendency of a person to recall the first and last items in a series best, and the middle items worst (Troyer, 2011). This can greatly impact the content of language data. For instance, in the context of data collection to guide development work, it is important to understand current needs and values (Hirmer and Guthrie, 2016); however, if only the most recent needs are discussed, long-term needs may be overlooked. To illustrate, while a community which has just experienced a poor agricultural season may tend to express the importance of improving agricultural output, other needs which are less top-of-mind (i.e. healthcare, education) may be equally important despite being expressed less frequently. If data containing recency bias is used to develop NLP models, particularly for sustainable development applications (such as for Automatic UPV Classifi-cation, Conforti et al. (2020)), these may amplify current needs and under-represent long-term needs.\n\nFourthly, acquiescence bias, also known as \"yea\" saying (Laajaj and Macours, 2017), which can occur in rural developing contexts when interviewees perceive that certain (possibly false) responses will please a data collector and bring benefits to their community. For example, if data collection is being undertaken by a group with a stated desire to build a school may be more likely to hear about how much education is valued.\n\nFinally, priming effect, or the ability of a presented stimulus to influence one's response to a subsequent stimulus (Lavrakas, 2008). Priming is problematic in data collection to inform SD projects; it can be difficult to collect data on the relative importance of simultaneous (or conflicting) needs if the community is primed to focus on one (Veltkamp et al., 2011). An example is shown in Figure 2a; respondents may be drawn to speak more about the most dominant prompts presented in the chart. This is typical of a broader failure in SD to uncover beneficiary priorities without introducing project bias (Watkins et al., 2012). Needs assessments, like the one referenced above linked to a rural electrification project, tend to focus explicitly on project-related needs instead of more broadly identifying what may be most important to communities (Masangwi, 2015;USAID, 2006). As speakers will usually know why data is being collected in such cases, they may be biased towards stating the project aim as a need, thereby skewing the corpora to over-represent this aim.", "filtered_refids": [[], [null, "b58"], [null], ["b65", "b5"], ["b18"], ["b29", "b79", "b22", "b75", "b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 4274, "num_references": 11}
{"corpusid_sectionid": "231839511-s6", "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries", "date": "2021-02-04", "section_title": "Ethical Considerations", "section": "Certain ethical codes of conduct must be followed when collecting data from illiterate speakers in rural communities in LICs (Musoke et al., 2020). Unethical data collection may harm communities, treat them without dignity, disrupt their lives, damage intra-community or external relationships, and disregard community norms (Thorley and Henrion, 2019). This is particularly critical in rural developing regions, as these areas are home to some of the world's poorest and most vulnerable to exploitation (Christiaensen and Subbarao, 2005;de Cenival M., 2008). Unethical data collection can replicate extractive colonial relationships whereby data is extracted from communities with no mutual benefit or ownership (Dunbar and Scrimgeour, 2006). It can lead to a lack of trust between data collec-tor and interviewees and unwillingness to participate in future research (Clark, 2008). These phenomena can bias data or reduce data availability. Ethical data collection practices in rural developing regions with high illiteracy include: obtaining consent (McAdam, 2004), accounting for cultural differences (Silverman, 2013), ensuring anonymity and confidentiality (Bryman, 2012), respecting existing community or leadership structures (Harding et al., 2012), and making the community the owner of the data. While the latter is not often currently practiced, it is an important consideration for community empowerment, with indigenous data sovereignty efforts (Rainie et al., 2019) already setting precedent.", "filtered_refids": [["b31", "b55", "b35", null, "b1", "b61"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1505, "num_references": 6}
{"corpusid_sectionid": "231839511-s8", "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries", "date": "2021-02-04", "section_title": "Preparation", "section": "Here, we outline practical preparation steps for careful planning, which can minimise error and reduce fieldwork duration (Tukey, 1980). Local Context. A thorough understanding of local context is key to successful data collection (Hentschel, 1999;Bukenya et al., 2012;Launiala and Kulmala, 2006). Local context is broadly defined as facts, concepts, beliefs, values, and perceptions used by local people to interpret the world around them, and is shaped by their surroundings (i.e. their worldview, Vasconcellos and Vasconcellos Sobrinho (2014)). It is important to consider local context when preparing to collect data in rural developing areas, as common data collection methods may be inappropriate due to contextual linguistic differences and deep-rooted social and cultural norms (Walker and Hamilton, 2011;Mafuta et al., 2016;Nikulina et al., 2019;Wang et al.). Selecting a contextually-appropriate data collection method is critical in mitigating social desirability bias in the collected data, among other challenges. Re-searchers should review socio-economic surveys and/or consult local stakeholders who can offer valuable insights on practices and social norms. These stakeholders can also highlight current or historical matters of concern to the area, which may be unfamiliar to researchers, and reveal local, traditional, and indigenous knowledge which may impact the data being collected (Wu, 2014) and result in recency effect. It is good practice to identify local conflicts and segmentation within a community, especially in a rural context, where the population is vulnerable and systematically unheard (Dudwick et al., 2006;Mallick et al., 2011).\n\nCase sampling. In qualitative research, sample cases are often strategically selected based on the research question (i.e. systematic or purposive sampling, Bryman (2012)), and characteristics or circumstances relevant to the topic of study (Yach, 1992). If data collected in such research is used beyond its original scope, sampling bias may result. So, while data collected in previous research should be re-used to expand NLP corpora where possible, it is important to be cognizant of the purposive sampling underlying existing data. A comprehensive dataset characterisation (Bender and Friedman, 2018;Gebru et al., 2018) can help researchers understand whether an existing dataset is appropriate to use in new or different research, such as in training new NLP models, and can highlight the potential ethical concerns of data re-use.\n\nParticipant sampling. Interviewees should be selected to represent the diverse interests of a community or sampling group (e.g. occupation, age, gender, religion, ethnicity or male/female household heads (Bryman, 2012)) to reduce sampling bias (Kitzinger, 1994). To ensure representativity in collected data, sampling should be random, i.e. every subject has equal probability to be included (Etikan et al., 2016). There may be certain societal subsets that are concealed from view (e.g. as a result of embarrassment from disabilities or physical differences) based on cultural norms in less inclusive societies (Vesper, 2019); particular care should be exercised to ensure such subsets are represented.\n\nGroup composition. Participant sampling best practices vary by data collection method, with particular care being necessary in group settings. In traditional societies where strong power dynamics exist, attention should be paid to group composition and interaction to prevent some voices from", "filtered_refids": [["b26", "b40", "b78", "b25", "b82", "b66", "b21", null, "b77", "b2"], ["b83", null], ["b16", "b76", null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3505, "num_references": 15}
{"corpusid_sectionid": "231839511-s11", "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries", "date": "2021-02-04", "section_title": "\u2022 Select interviewers & design interview processes to account for known norms which might skew responses", "section": "Recency effect: Tendency to recall first or last items in a series best, & middle items worst.\n\n\u2022 Minimise external influence on participants throughout data gathering (e.g. technologies, people, perceptions). Acquiescence: Respondents perceive certain, perhaps false, answers may please data collectors, bringing community benefits.\n\n\u2022 Gather non-sectoral holistic insights (e.g. from socioeconomic data or local stakeholders)\n\nPriming effect: Ability of a presented stimulus to influence one's response to a subsequent stimulus \u2022 Use appropriate visual prompts (graphically similar), language and technology Table 1: Sources of potential bias in data collection when operating in rural and illiterate settings in developing countries, and key countermeasures that can help mitigating them.\n\nbeing silenced or over-represented (Stewart et al., 2007). For example, in Uganda, female interviewees may be less likely to voice opinions in the presence of male interviewees (FIDH, 2012;Axinn, 1991), introducing a form of social desirability bias in resulting corpora. To minimise this risk of data bias, relations and power dynamics must be considered during data collection planning (Hirmer, 2018). It may be necessary to exclude, for instance, close relatives, governmental officials, and village leaders from group discussions where data is being collected, and instead engage such stakeholders in separate activities to ensure that their voices are included in the corpora without biasing the data collected from others.\n\nInterviewer selection. The interviewer has a significant opportunity to introduce observer and response biases in collected data (Salazar, 1990). Interviewers familiar with local language, including community-specific dialects, should be selected wherever possible. Moreover, to reduce misunderstanding and recall biases in collected data, it is useful to have the same person who conducts the interviews also transcribe them. This minimizes the layers of linguistic interpretation affecting the final dataset and can increase accuracy through familiarity with the interview content. If the interviewer is unavailable, the transcriber must be properly trained and briefed on the interviews, and made aware of the level of detail needed during transcription (Parcell and Rafferty, 2017).\n\nStudy design. In rural LIC communities, qualitative data like natural language is usually collected by observation, interview, and/or focus group discussion (or a combination, known as mixed methods) which are transcribed verbatim (Moser and Korstjens, 2018). Prompts are often used to spark discussion. Whether visual prompts (Hirmer, 2018) or verbalised question prompts are used during data collection, these should be designed to: (i) accommodate illiteracy, (ii) account for disabilities (e.g. visually impairment; both could cause sampling bias), and (iii) minimise bias towards a topic or sector (e.g. minimising acquisition bias and confirmation bias). For instance, visual prompts should be graphically similar and contain only visuals familiar to the respondents. This is analogous to the uniform interface with which speakers interact during text-based online data collection, where the platform used is graphically the same to all users inputting data. Using varied graphical styles or unfamiliar images may result in priming (Figure 2a). To minimise recall bias or recency effect in collected data, socio-economic data can be integrated in data analysis to better understand if the assertions made in collected data reference recent events, for example. These should be non-sector specific, to gain holistic insights and to minimise acquisition bias and confirmation bias.", "filtered_refids": [[], [], [], [], [null, "b57", "b4"], ["b45", "b52"], ["b33", "b4"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3696, "num_references": 7}
{"corpusid_sectionid": "231839511-s12", "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries", "date": "2021-02-04", "section_title": "Engagement", "section": "Here, we outline practical steps for successful community engagement to achieve ethical and highquality data collection.\n\nDefining community. Defining a community in an open and participatory manner is critical to meaningful engagement (Dyer et al., 2014). By understanding the community the way they understand themselves, misunderstandings and tensions that affect data quality can be minimized. The definition of the community (MacQueen et al., 2001) coupled with the requirements and use-cases for the collected data determines the data collection methodology and style which will be most appropriate (e.g. interview-based community consultation vs. collaborative co-design for mutual learning).\n\nFollow formal structures. Researchers entering a community where they have no background to collect data should endeavour to know the community prior to commencing any work (Diallo et al., 2005). This could entail visiting the community and mapping its hierarchies of authority and decision-making pathways, which can guide the research team on how to interact respectfully with the community (Tindana et al., 2011). This process should also illuminate whether knowledgeable community members should facilitate entry by performing introductions and assisting the external data collection team. Following formal community structures is vital, especially in developing communities, where traditional rules and social conventions are strongly held yet often not articulated explicitly or documented. Approaching community leaders in the traditional way can help to build a positive long-term relationship, removing suspicion about the nature and motivation of the researchers' activities, explaining their presence in the community, and most importantly building trust as they are granted permission to engage the community by its leadership (Tindana et al., 2007).\n\nVerbalising consent. Data ethics is paramount for research involving human participants (Accenture, 2016;Tindana et al., 2007), including any collection of personal and identifiable data, such as natural language. Genuine (i.e. voluntary and informed) consent must be obtained from interviewees to prevent use of data which is illegal, coercive, or for a purpose other than that which has been agreed (McAdam, 2004). The Nuffield Council on Bioethics (2002) caution that in LICs, misunderstandings may occur due to cultural differences, lower social-economic status, and illiteracy (McMillan et al., 2004) which can call into question the legitimacy of consent obtained. Researchers must understand that methods such as long information forms and consent forms which must be signed may be inappropriate for the cultural context of LICs and can be more likely to confuse than to inform (Tekola et al., 2009). The authors advise that consent forms should be verbal instead of written, with wording familiar to the interviewees and appropriate to their level of comprehension (Tekola et al., 2009). For example, to speak of data storage on a password protected computer while obtaining consent in a rural community without access to electricity or information technology is unfitting. Innovative ways to record consent can be employed in such contexts (e.g. video taping or recording), as signing an official document may be \"viewed with suspicion or even outright hostility\" (Upjohn and Wells, 2016), or seen as \"committing ... to something other than answering questions\". Researchers new to qualitative data collection should seek advice from experienced researchers and approval from their ethics committee before implementing consent processes.\n\nApproaching participants. Despite having gained permission from community authorities and obtained consent to collect data, researchers must be cautious when approaching participants (Irabor and Omonzejele, 2009;Diallo et al., 2005) to ensure they do not violate cultural norms. For example, in some cultures a senior family member must be present for another household member to be interviewed, or a female must be accompanied by a male counterpart during data collection. Insensitivity to such norms may compromise the data collection process; so, they should be carefully noted when researching local context ( \u00a74.1) and interviews should be designed to accommodate them where possible. Furthermore, researchers should investigate the motivations of the participants to identify when inducements become inappropriate and may lead to either harm or data bias (McAdam, 2004).\n\nMinimise external influence. Researchers must be aware of how external influences can affect data collection (Ramakrishnan et al., 2012). We find three main levels of external influence: (i) technologies unfamiliar to a rural developing country context may induce social desirability bias or priming (e.g. if a researcher arrives to a community in an expensive vehicle or uses a tablet for data collection); (ii) intergroup context, which according to Abrams (2010) refers to when \"people in different social groups view members of other groups\" and may feel prejudiced or threatened by these differences. This can occur, for instance, when a newcomer arrives and speaks loudly relative to the indigenous community, which may be perceived as overpowering; (iii) there is the risk of a researcher over-incentivizing the data collection process, using leading questions and judgemental framing (interviewer bias or confirmation bias). To overcome these influences, researchers must be cognizant of their influence and minimise it by hiring local mediators where possible alongside employing appropriate technology, mannerisms, and language.", "filtered_refids": [[], ["b24", null], [null, "b63", "b62"], ["b63", "b60", "b31", "b32", null, "b72"], [null, "b10", "b31"], ["b49"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 5629, "num_references": 15}
{"corpusid_sectionid": "231839511-s13", "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries", "date": "2021-02-04", "section_title": "Undertaking Interviews", "section": "Here, we detail practical steps to minimise challenges during the actual data collection.\n\nInterview settings. People have personal values and drivers that may change in specific settings. For example, in the Ugandan Buganda and Busoga tribes, it is culturally appropriate for the male head if present to speak on behalf of his wife and children. This could lead to corpora where input from the husband is over-represented compared to the rest of the family. To account for this, it is important to collect data in multiple interview settings (e.g. individual, group male/female/mixed; Figures 2b, 2c). Additionally, the inputs of individuals in group settings should be considered independently to ensure all participants have an equal say, regardless of their position within the group (Barry et al., 2008;Gallagher et al., 1993). This helps to avoid social desirability bias in the data and is particularly important in various developing contexts where stereotypical gender roles are prominent (Hirmer, 2018). During interviews, verbal information can be supplemented through the observation of tone, cadence, gestures, and facial expressions (Narayanasamy, 2009;Hess et al., 2009), which could enrich the collected data with an additional layer of annotation.\n\nWorking with multiple interviewers. Arguably, one of the biggest challenges in data collection is ensuring consistency when working with multiple interviewers. Some may report word-forword what is being said, while others may summarise or misreport, resulting in systematic misunderstanding. Despite these risks, employing multiple interviewers is often unavoidable when collecting data in rural areas of developing countries, where languages often exhibit a high number of regional, non-mutually intelligible varieties. This is particularly prominent across SSA. For example, 41 languages are spoken in Uganda (Nakayiza, 2016); English, the official language, is fluently spoken by only \u223c5% of the population, despite being widely used among researchers and NGOs (Katushemererwe and Nerbonne, 2015). To minimise data inconsistency, researchers should: (i) undertake interviewer training workshops to communicate data requirements and practice data collection processes through mock field interviews; (ii) pilot the data collection process and seek feedback to spot early deviation from data requirements; (iii) regularly spot-check interview notes; (iv) support written notes with audio recordings 3 ; and (v) offer quality based incentives to data collectors.\n\nParticipant remuneration. While it is common to offer interviewees some form of remuneration for their time, the decision surrounding payment is ethically-charged and widely contested (Hammett and Sporton, 2012). Rewards may tempt people to participate in data collection against their judgement. They can introduce sampling bias or create power dynamics resulting in acquiescence bias (Largent and Lynch, 2017). Barbour (2013) offers three practical solutions: (i) not advertise payment; (ii) omit the amount being offered; or (iii) offer non-financial incentives (e.g. products that are desirable but difficult to get in an area). The decision whether or not to remunerate should not be based upon the researcher's own ethical beliefs and resources, but instead by considering the specific context 4 , interviewee expectations, precedents set by previous researchers, and local norms (Hammett and Sporton, 2012). Representatives from local organisations (such as NGOs or governmental authorities) may be able to offer advice. (a) (b) (c) Figure 2: Collecting oral data in rural Uganda. 2a Priming effect (note the word \"Energy\" in the poster's title and the visual prompts differences between items). On the contrary, 2b and 2c show minimal priming; note also that different demographics are separately interviewed (women group, single men) to avoid social desirability bias.", "filtered_refids": [[], [null, "b38", "b4", "b3"], ["b13", "b37"], [null, "b20", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 3906, "num_references": 9}
{"corpusid_sectionid": "231839511-s14", "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries", "date": "2021-02-04", "section_title": "Post-interviewing", "section": "Here, we discuss practical strategies to mitigate ethical issues surrounding the management and stewardship of collected data. Anonymisation. To protect the participants' identity and data privacy, locations, proper names, and culturally explicit aspects (such as tribe names) of collected data should be made anonymous (Sweeney, 2000;Kirilova and Karcher, 2017). This is particularly important in countries with security issues and low levels of democracy.\n\nSafeguarding data. A primary responsibility of the researcher is to safeguard participants' data (Kirilova and Karcher, 2017). In addition to anonymizing data, mechanisms for data management include in-place handling and storage of data (UKRI, 2020a). Whatever data management plan is adopted, it must be clearly articulated to participants before the start of the interview (i.e. as part of the consent process (Silverman, 2013)), as was discussed in \u00a74.2 (Verbalising consent).\n\nWithdrawing consent. Participants should have the ability to withdraw from research within a specified time frame. This is known as withdraw consent and is commonly done by phone or email (UKRI, 2020b). As people in rural illiterate communities have limited means and technology access, a local phone number and contact details of a responsible person in the area should be provided to facilitate withdraw consent.\n\nCommunication and research fatigue. While researchers frequently extract knowledge and data from communities, only rarely are findings fed back to communities in a way that can be useful to them. Whatever the research outcomes, researchers should share the results with participating communities in an appropriate manner. In illiterate communities, for instance, murals (Jimenez, 2020), artwork, speeches, or song could be used to communicate findings. Not communicating findings may result in research fatigue as people in over-studied communities are no longer willing to participate in data collection. This is common \"where repeated engagements do not lead to any experience of change [...]\" Clark (2008). Patel et al.\n\n(2020) offers practical guidance to minimise research fatigue by: (i) increasing transparency of research purpose at the beginning of the research, and (ii) engaging with gatekeeper or oversight bodies to minimise number of engagements per participant. Failure to restrict the number of times that people are asked to participate in studies risks poor future participation (Patel et al., 2020) which can also lead to sampling bias.", "filtered_refids": [["b59", "b15"], ["b15", "b67", "b55"], [], ["b12"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2511, "num_references": 6}
{"corpusid_sectionid": "265158335-s0", "title": "Exploring undergraduate translation students' perceptions towards machine translation: A qualitative questionnaire survey", "date": 2023, "section_title": "Research background", "section": "The term 'machine translation' (MT) includes various activities related to translation that are performed by computers.A widely used definition provided by Hutchins and Sommers (1992, p. 3) is that MT systems are 'computerised systems responsible for the production of translations from one natural language into another, with or without human assistance'.\n\nThe increasing accuracy and fluency of MT have recently led to MT being included in translation programmes in higher education institutions, with specialised courses for students being provided.However, such courses are often offered at the postgraduate level or towards the last year of an undergraduate programme (e.g., Arenas & Moorkens, 2019;Doherty et al., 2012).One of the concerns is that technologies such as MT might be too difficult for undergraduate translation students to learn.The other concern is that the students' translation performances and their translation competence could be negatively impacted by MT because they might not have the ability to evaluate the output of the technology (Bowker, 2015).Therefore, MT training is not usually available to undergraduate students.In addition, teachers or management may formulate policies that forbid students from using MT in their assignments.\n\nThere is a lack of sufficient evidence in academia to conclude that MT has a negative impact on novice translation students.Most of the previous studies have focused on postgraduate students or undergraduate students in the last year of their programmes (e.g., Jia et al., 2019;Wang et al., 2021;Zaretskaya et al., 2016).Little research has targeted undergraduate translation learners in the early stages of their training.\n\nHowever, it has been observed that students have been interacting with MT in contravention of official instructions.With MT systems and abundant information about them being freely available on the internet, it is unlikely that students would be unaware of MT or would not be interested in experimenting with it.As the quality of MT increases, students might have strong intentions to use MT when learning to translate.The author thus argued that novice translation students' knowledge about and experience of MT could be of value in the curriculum design and pedagogical development of MT courses.\n\nIn previous studies, the participants' perceptions were often solicited via closed-ended questions (e.g., Liu et al., 2022;Yang et al., 2021;Yang & Wang, 2019), the answers to which were later analysed as quantitative data.However, as the participants answered questions on a scale or according to the available choices, their views were limited.For example, Yang and Wang (2019)'s study focused exclusively on students' intentions to use MT; their study was based on a technology acceptance model in which an individual's intention to engage with technology was linked directly to their attitude.A questionnaire using a 5-point scale was developed in accordance with the model and was answered by 109 Chinese student translators.The results supported and verified the technology acceptance model in that the students' perceived ease of use and perceived usefulness of MT were correlated positively with their intentions to use MT.\n\nBased on the above discussion, little is known about how undergraduate students in the early stages of translator training perceive and use MT or what their training needs may be.Therefore, this research intended to survey translation students in the early stages of their translator training to solicit their attitudes towards and perceptions of MT via a qualitative questionnaire survey.This research included open-ended questions, thus allowing the participants to express their opinions freely.Furthermore, unlike interviews, questionnaires with open-ended questions could be self-administered without the researcher's presence, thus avoiding researcher bias.", "filtered_refids": [[null], ["b4", "b1", "b0"], ["b11", "b9", "b7"], [], [null, "b10"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3890, "num_references": 9}
{"corpusid_sectionid": "265158335-s2", "title": "Exploring undergraduate translation students' perceptions towards machine translation: A qualitative questionnaire survey", "date": 2023, "section_title": "Methods and data", "section": "After ethical clearance was obtained from the university, 20 students from an Applied Translation Studies programme at a university based in China were recruited.\n\nThe participants were all in the second year of an undergraduate translation programme; they had similar educational backgrounds and hence comparable language proficiency and translation competencies.They had taken three translation courses covering fundamental translation theories and practices.No specialised training in MT, post-editing (PE) or translation technology was provided in the classroom.\n\nThe survey included ten open-ended questions to solicit the participants' knowledge, experience, perceptions of and attitudes to MT.As few previous studies have used open-ended questions, the design of this questionnaire mainly drew on Gonz\u00e1lez Pastor (2021)'s paper, which had a similar design and goal as the current project, and referenced three other relevant papers (\u00c7etiner & \u0130\u015fisa\u011f, 2019;de Faria Pires, 2020;Schmidhofer & Mair, 2018).Gonz\u00e1lez Pastor (2021) investigated students' attitudes to and perceptions of translation technology before and after being taught about translation technology.The questions were adapted and narrowed down to MT-and PE-specific questions.Questions regarding the students' understanding of translation concepts, processes and products were added as answers to these questions provide an alternative perspective to understand the impact of MT and PE on students' translation processes and products.\n\nThis paper mainly analysed answers to questions regarding the students' knowledge, attitudes to and perceptions of MT.Therefore, only the answers to the following six (out of 10) questions were analysed and presented.6. Do you think there are ethical concerns related to MT, such as legal or moral issues, biases, justice or privacy?Can you explain your thoughts in detail?\n\nThe students were told that they could answer the survey in Chinese or English, whichever they felt most comfortable using.The answers in Chinese were translated by the author, a certified translator in China and Australia with over 15 years of professional experience.\n\nAll the data were de-identified with students' names being indicated as \"s + participant number\" and were imported into NVivo 14 for thematic analysis.The author conducted the analysis twice at two different times to ensure the reliability of the results.", "filtered_refids": [[], [], [null, "b2", "b3"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2408, "num_references": 3}
{"corpusid_sectionid": "5738018-s3", "title": "Annotating genericity: a survey, a scheme, and a corpus", "date": "2015-06-01", "section_title": "ACE entity class annotations", "section": "The research objective of the ACE program (1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008) was the detection and characterization of entities, relations and events in natural text (Linguistic Data Consortium, 2000). All entity mentions receive an entity class label indicating their genericity status. Of the corpora described here, the ACE corpora have been the most widely used for recent research on automatically identifying generic NPs (Reiter and Frank, 2010). The annotation guidelines developed over time; we describe both the initial guidelines of ACE-2 and those from ACE-2005.\n\nThe ACE-2 corpus (Mitchell et al., 2003) includes 40106 annotated entity mentions in 520 newswire and broadcast documents. The annotation guidelines give no formal definition of genericity; annotators are asked to determine whether each entity refers to \"any member of the set in question\" (generic) or rather \"some particular, identifiable member of that set\" (specific/non-generic). 1 This leads to a mix of constructions being marked as generic: types of entities (Good students do all the reading), generalizations across a set of entities (Purple houses are really ugly), hypothetical entities (If a person steps over the line,...) and negated mentions (I saw no one). Suggested attributes of entities are marked as generic (John seems to be a nice person), but a 'positive assertion test' leads to marking both NPs (Joe and a nice guy) as specific in examples like (Joe is a nice guy). Neither of these two cases (be a nice person / be a nice guy) is in fact an entity mention; they are rather predicative uses.\n\nThe guidelines for genericity were redefined for annotation of the ACE-2005 Multilingual Training Corpus (Walker et al., 2006), which contains news, broadcast news, broadcast conversation, forum and weblog texts as well as transcribed conversational telephone speech. In contrast to ACE-2, the ACE-2005 annotation manual 2 clearly defines mentions as kind-referring or not, using the labels GEN (generic) and SPC (specific/non-generic) respectively. The new guidelines also introduce two additional entity class labels for non-attributive mentions. Negatively quantified entities that refer to the empty set of the kind mentioned (There are no confirmed suspects yet) receive the label NEG. The label USP (underspecified) is used for non-generic nonspecific reference, these cases include quantified NPs in modal, future, conditional, hypothetical, negated, uncertain or question contexts. USP also covers 'truly ambiguous cases' that have both generic and non-generic readings (The economic boom is providing new opportunities for women in New Delhi), and cases where the author mentions an entity whose identity would be 'difficult to locate' (Officials reported ...). In our opinion, the latter interferes with the definition of SPC as marking cases where the entity referred to is a particular object in the real world, even if the author does not know its identity (At least four people were injured). The breadth of the USP category causes problems with consistency of application (see Section 3).\n\nThe ACE annotation scheme has also been applied in the Newsreader project. 3 The ECB+ corpus (Cybulska and Vossen, 2014) is an extension of EventCorefBank (ECB), a corpus of news articles marked with event coreference information (Bejan and Harabagiu, 2010). ECB+ annotates entity mentions according to ACE-2005, but collapses the three non-GEN labels into a single category. Roughly 12500 event participant mentions are annotated, some doubly and some singly. Agreement statistics for genericity are not reported. 3 www.newsreader-project.eu", "filtered_refids": [["b30", null], ["b26"], ["b32"], [null, "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3667, "num_references": 6}
{"corpusid_sectionid": "5738018-s4", "title": "Annotating genericity: a survey, a scheme, and a corpus", "date": "2015-06-01", "section_title": "Other corpora annotated at the NP-level", "section": "The resources surveyed here apply carefullydefined notions of genericity but are too small to be feasible machine learning training data.\n\nThe question of whether an NP is generic or not arises in the research context of coreference resolution. Some approaches mark coreference only for non-generic mentions (Hovy et al., 2006;Hinrichs et al., 2004); others include generic mentions (Poesio, 2004), or take care not to mix coreference chains between generic and non-generic mentions (Bj\u00f6rkenstam and Bystr\u00f6m, 2012). Bj\u00f6rkelund et al. (2014) mark genericity in a corpus of German with both coreference and information-status annotations. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they provide a complete overview. In short, they argue that a consistent definition of genericity is lacking and report on their annotation scheme for Czech as applied to the Prague Dependency TreeBank (B\u00f6hmov\u00e1 et al., 2003).\n\nThe GNOME corpus (Poesio, 2004) is a coreference corpus with genericity annotations; NPs are marked with the attributes generic-yes or generic-no. Poesio et al. report that their annotators found it hard to decide how to mark references to substances (A table made of wood) and quantified NPs. Similar to our experience, they found it helpful to have annotators first try to identify generic sentences, and then determine this attribute of the NP. They report an agreement of \u03ba = 0.82 on their corpus, which consists of 900 finite clauses from descriptions of museum objects, pharmaceutical leaflets and dialogues.\n\nComing from a formal semantic perspective, Herbelot and Copestake (2010) and Herbelot and Copestake (2011) describe an approach to treating ambiguously quantified NPs. This annotation effort aims to produce resources for the task of determining the extent to which the semantic properties ascribed to a given NP in context apply to the members of that class. For example, the statement Cats are mammals describes a property of all cats, where Cats have four legs is true only for most cats. The scheme, which includes the labels ONE, SOME, MOST, ALL and QUANT (for explicitly quantified NPs), is applied to 300 subject-verb-object triples from sentences randomly extracted from Wikipedia. Annotators are shown the sentence and the triple. \u03ba ranges from 0.88 and 0.81 for QUANT and ONE to values between 0.44 and 0.51 for the other classes. Bhatia et al. (2014b) present an annotation scheme for Communicative Functions of Definiteness, intended to cover the many semantic and pragmatic functions conveyed by choices regarding definiteness across languages of the world. The scheme has been applied to 3422 English NPs contained in texts from four genres. Their typology includes two categories relevant to our survey: GENERIC KIND LEVEL applies to utterances predicating over an entire class, like Dinosaurs are extinct.\n\nGENERIC INDIVIDUAL LEVEL is for predications applying to the individual members of a class or kind, such as Cats have fur. Across 1202 annotated NPs for an interannotator agreement study, the two annotators used the GENERIC INDIVIDUAL LEVEL label 45 times and 30 times, respectively, with agreement in 29 cases. Neither used the GENERIC KIND LEVEL. The entire corpus contains just 131 NPs labeled with GENERIC INDIVIDUAL LEVEL and none with GENERIC KIND LEVEL (Bhatia et al., 2014a).\n\nThe question of genericity has also been addressed in cognitive science (Prasada, 2000). Gelman and Tardif (1998) study the usage of generic NPs cross-linguistically for English and Chinese in child-directed speech. They annotate kind-referring NPs as generic. They report agreement as the fraction of items on which the annotators agreed at over 99%, but given that their data set has fewer than 1% generic NPs, this statistic does not allow us to estimate how well annotators agreed.", "filtered_refids": [[], ["b17", "b18", "b28", "b27", "b5", "b3"], ["b28"], ["b15", "b16", "b2"], ["b1"], ["b29"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3875, "num_references": 12}
{"corpusid_sectionid": "5738018-s6", "title": "Annotating genericity: a survey, a scheme, and a corpus", "date": "2015-06-01", "section_title": "ACE-2005: an agreement study", "section": "In this section we investigate some problems with the ACE annotation scheme via a study of annotator agreement. The data was first labeled by two annotators independently, then adjudicated by a senior annotator. To our knowledge, agreement numbers on this task have not been published to date. In order to assess both the quality of the data and the difficulty of the task, we compute inter-annotator agreement as follows. Using the 533 documents from the adjudicated data set that were marked by two annotators in the first step, we compute Cohen's \u03ba (Cohen, 1960) for entity class annotations over the four labels SPC, GEN, USP and NEG.\n\nIntuitions about NP genericity are most reliable for subject position as other argument positions involve additional difficulties (Link, 1995). To get a better sense of the difficulty of annotating subjects compared to that for other argument positions, we compute agreement over mentions whose (manually marked) head is the grammatical subject of some other node in a dependency graph (including any dependency type containing subj). We obtain dependency graphs using the Stanford parser (Klein and Manning, 2002).\n\nAn additional complication in entity mention annotation is determining the mention span. Because spans are not pre-marked in the ACE corpora but identified independently by each annotator, we compute \u03ba only over all exactly-matching entity mention spans for the two annotators. For all mentions, annotators mark about 90% of spans marked by the other annotator. For subject mentions, this number is even higher, at about 95%. The spans of the remaining mentions overlap for the two annotators. We exclude them from this study as we cannot be sure that the two mention spans refer to the same entity.\n\nDiscussion. Table 2 shows the confusion matrices of labels for the all-mentions-case and the subjectsonly case. In both cases, confusion between SPC and GEN is acceptable, but confusion between USP and both SPC and GEN is rather high. For example, in the case of subjects, annotator 1 tags 652 mentions as GEN that annotator 2 marks USP, but the two of them only agree on 597 mentions to be GEN. Although it may be useful to create a separate category for unclear or underspecified cases, the definition of USP is not yet clear-cut and compounded with lack of specificity, which refers to whether the speaker presumably knows the referent's identity or not. Even if the identity of a referent may be 'difficult to locate' (as in Officials reported...). The clause certainly does not make a statement about the kind 'official'; instead, it expresses an existential statement (There are officials who reported...). The definition of SPC states that the reader does not necessarily have to know the identity of the entity, possibly making the distinction hard for annotators.\n\nAnother difficult case are noun modifiers in compounds (e.g. a subway system); these are marked as GEN in the corpus. Using the automatic parses,  we find that 9.5% of all mentions marked GEN in the adjudicated corpus are one-token mentions modifying another noun via an nn dependency relation. Genericity as reference to kinds is a discourse phenomenon and thus defined as an attribute of referring expressions. Because nominal modifiers do not introduce discourse referents, they should not be treated on the genericity annotation layer. The data shows moderate agreement for the first two passes of entity class annotation (\u03ba = 0.53 for all mentions and \u03ba = 0.50 for subject mentions). Note that \u03ba scores are not directly comparable across different annotation projects (see also Section 5), we give the above scores for the sake of completeness. Observed and expected agreement are 0.83 and 0.65 for the all-mentions case and 0.79 and 0.58 for subject mentions. This indicates that the all-mentions case may contain some trivial cases, one of which is the case of nominal modifiers described above.\n\nIn summary, the ACE scheme problematically fails to treat subject NPs differently from NPs in other syntactic positions, and 'fuzzy' points in the guidelines, particularly concerning the USP label, contribute to disagreements between annotators.", "filtered_refids": [["b8"], ["b23", "b20"], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 4181, "num_references": 3}
{"corpusid_sectionid": "10690796-s0", "title": "A Survey of Ellipsis in Chinese", "date": "2015-08-01", "section_title": "Sluicing", "section": "Sluicing (Ross 1969, Merchant 2001) typically elides everything from a clause except an interrogative expression (wh-element), e.g.\n\n(9) They are hiding something, but While there is a preference for the b-questions, in which the verb is repeated, the a-questions are not clearly bad. This situation clouds the picture, since the marginal a-questions look like the sluicing in direct questions that is frequent in those languages that have sluicing. One might, however, assume that what has actually been elided from the a-questions is the auxiliary sh\u00ec 'be'. On such an account, such examples would, strictly speaking, not count as instances of sluicing as it is commonly understood. Further data speak more clearly against the presence of sluicing in Mandarin. Cases of so-called multiple sluicing are bad in Mandarin. Multiple sluicing occurs when the sluiced clause contains two or more wh-remnants. The following example illustrates multiple sluicing in English:\n\n(13) A: Somebody has a crush on somebody?\n\nhas Who crush a on whom B: Who has a crush on whom?\n\nThe sluiced clause contains the two wh-remnants, who and on whom, identifying it as an instance of multiple sluicing.  (11a) and (12a). This confirms that sluicing as it is commonly understood in English and related languages does not exist in Mandarin.\n\nA number of accounts of sluicing-like data in Mandarin have acknowledged that what at times looks like sluicing is in fact a different mechanism, this mechanism being called pseudosluicing (see for instance Wei 2004, andAdams andTamioka 2014). Pseudosluicing involves the auxiliary sh\u00ec -but at times sh\u00ec can be omitted. The analysis of pseudosluicing put forth in the literature (Adams and Tamioka 2014) is that it involves zero anaphora; a subject pronoun has been dropped, e.g. \u2026w\u01d2men b\u00f9 zh\u012bd\u00e0o (t\u0101) sh\u00ec shu\u00ed, lit. 'we not know it be who' -more about zero anaphora below in Section 8.\n\nThe absence of sluicing in Mandarin is consistent with the absence of sluicing in wh-in-situ languages in general (Merchant 2001: 84f.).", "filtered_refids": [["b13", "b18"], [], [], [], [], [null, "b20", "b0"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2044, "num_references": 6}
{"corpusid_sectionid": "10690796-s5", "title": "A Survey of Ellipsis in Chinese", "date": "2015-08-01", "section_title": "Null complement anaphora", "section": "Null complement anaphora (Hankamer andSag 1976, Depiante 2000) is a mechanism that elides a complement clause, to-phrase, or prepositional phrase, e.g.\n\n(36) Jim promised he would help, and promised Bill also would he help\n\nBill also promised he would help.\n\n(37) Sam refuses to help, and refuses Sue also to help\n\nSue also refuses to help.\n\nThe predicates that license null complement anaphora in English (e.g. ask, know, promise, refuse, try) are limited. Similar predicates that one might expect to also license null complement anaphora fail to do so (e.g. imagine, intend, pretend, say, think, etc. These two examples suggest that the similar predicates across the languages allow for the ellipsis of a complement clause or phrase. However, concluding that Mandarin has null complement anaphora in the same way that English does is difficult. The difficulty is due to the fact that Mandarin seems to freely allow the ellipsis of most all complements that can be easily recovered from context. When the elided complement is a verb phrase, one can acknowl-edge VP-ellipsis as discussed above, and when the elided complement can be interpreted as a definite or indefinite noun phrase, an analysis in terms of zero anaphora is available (see the next section). Thus the extent to which null complement anaphora is present in Mandarin is unclear.", "filtered_refids": [[null, "b5"], [], [], [], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1345, "num_references": 3}
{"corpusid_sectionid": "260899983-s3", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Linguistic Knowledge Competency", "section": "Linguistic knowledge includes grammatical, semantic, and pragmatic knowledge (Fromkin et al., 2018). The grammar of a natural language is its set of structural constraints on speakers' or writers' composition of clauses, phrases, and words. The term can also refer to the study of such constraints, a field that includes domains such as phonology, morphology, and syntax, often complemented by phonetics, semantics, and pragmatics. Semantic (Austin, 1975) studies the meaning of words, phrases, and sentences, focusing on general meanings rather than on what an individual speaker may want them to mean. Pragmatics (Austin, 1975) studies language use and how listeners bridge the gap between sentence meaning and the speaker's meaning. It is concerned with the relationship between semantic meaning, the context of use, and the speaker's meaning.  (Warstadt et al., 2020) evaluates what language models (LMs) know about major grammatical phenomena. Linguistic mappings 3 task aims to explore the depth of linguistic knowledge in enormous language models trained on word prediction. It aims to discover whether such knowledge is structured so as to support the use of grammatical abstractions, both morphological (past tense formation and pluralization) and syntactic (question formation, negation, and pronominalization). The minute mysteries qa 4 is a reading comprehension task focusing on short crime and mystery stories where the goal is to identify the perpetrator and to explain the reasoning behind the deduction and the clues that support it. The metaphor boolean 5 task presents a model with a metaphoric sentence and asks it to identify whether a second sentence is the correct interpretation of the first. The last three are selected from BIG-Bench (Srivastava et al., 2022), containing diverse task topics including linguistics.", "filtered_refids": [[null, "b138", "b40", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1840, "num_references": 4}
{"corpusid_sectionid": "260899983-s4", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "World Knowledge Competency", "section": "World knowledge is non-linguistic information that helps a reader or listener interpret the meanings of words and sentences (Ovchinnikova, 2012). It is also referred to as extra-linguistic knowledge. In this paper, we categorize world knowledge into general knowledge and domain knowledge. The general knowledge includes commonsense knowledge (Davis, 2014) and prevalent knowledge. The commonsense knowledge consists of world facts, such as \"Lemons are sour\", or \"Cows say moo\", that most humans are expected to know. The prevalent knowledge exists at a particular time or place. For example, \"Chinese people are used to drinking boiled water.\" is only known by a part of human beings; \"There were eight planets in the solar system\" is prevalent knowledge until it is overthrown. The domain knowledge (Alexander, 1992) is of a specific, specialized discipline or field, in contrast to general or domain-independent knowledge. People who have domain knowledge, are often considered specialists or experts in the field.\n\nThe bottom group of Table 1 shows some task examples that are used for testing world knowledge. For example, the LexGLUE (Chalkidis et al., 2022) tests whether LLMs perform well in the legal domain; WikiFact  is a fact completion scenario that tests language models' factual knowledge based on Wikipedia. The input will be a partial sentence such as \"The capital of France is \", and the output will be the continuation of the sentence such as \"Paris\"; TruthfulQA (Lin et al., 2022b) comprises questions spanning numerous categories including economics, science, and law. The questions are strategically chosen so humans may also incorrectly answer them based on misconceptions and biases; language models should ideally return accurate and truthful responses; HellaSwag (Zellers et al., 2019) tests commonsense inference and was created through adversarial filtering to synthesize wrong answers. The World knowledge competency, along with linguistic knowledge, serves as the foundation for solving different NLP tasks and is one of the core competencies of LLMs.", "filtered_refids": [["b94", "b26"], ["b15", "b157", "b71"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2081, "num_references": 5}
{"corpusid_sectionid": "260899983-s5", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Reasoning", "section": "Reasoning competency is a crucial skill for LLMs to solve complex problems. What's more, from the perspective of intelligent agents, reasoning ability is also one of the core capabilities towards achieving  AGI (Bubeck et al., 2023;Qiao et al., 2022). However, there remains no consensus whether LLMs can really reason, or just simply produce a larger context that increases the likelihood of correctly predicting the missing tokens (Mialon et al., 2023). Although \"reasoning\" itself may currently be an excuse of language, we can still objectively verify the reasoning performance of LLMs through various reasoning competencies. Previous methods mainly focus on the division of reasoning tasks.  divides existing evaluation tasks into three major categories, namely knowledge reasoning, symbolic reasoning, and mathematical reasoning, based on the type of logic and evidence involved in the reasoning process. Zhao et al. (2023) divides reasoning tasks into deductive reasoning and defeasible reasoning according to the reasoning form. In this section, we decompose the reasoning competency into 6 sub-parts from the perspective of model competency, providing a comprehensive overview of existing research efforts and suggesting potential future directions. And Table 2 presents some datasets for evaluating LLM's reasoning competency using this categorization approach.", "filtered_refids": [["b11", null, "b105", "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1371, "num_references": 4}
{"corpusid_sectionid": "260899983-s6", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Causal Reasoning Competency", "section": "Causal reasoning competency is a highly significant cognitive ability aimed at inferring causality through the observation of cause-effect relationships (Vowels et al., 2023;D\u00fcndar-Coecke, 2022;Chan et al., 2023). It enables us to comprehend and explain the relationships between events, variables, and actions, ultimately empowering us to make informed predictions and decisions .\n\nThe benchmarks Causal-TimeBank (Mirza et al., 2014), StoryLine (Caselli and Vossen, 2017), and MAVEN-ERE (Wang et al., 2022c) aim to test the existence of causal relationships between two events in sentences. COPA (Gordon et al., 2012) and XCOPA (Ponti et al., 2020) are evaluation benchmarks for extracting causal relationships in sentences, consisting of a set of premises and possible causes or effects. Tested systems are required to apply commonsense knowledge to identify the correct answers. e-CARE (Du et al., 2022) and CALM-Bench (Dalal et al., 2023) introduce a set of causal querying tasks to evaluate models, which include a cause and several potential effect sentences. Additionally, an annotated and interpretable causal reasoning dataset is provided for these tasks.", "filtered_refids": [[null, "b34"], ["b135", "b85", "b14", "b45", null, "b23", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1164, "num_references": 9}
{"corpusid_sectionid": "260899983-s7", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Deduction Reasoning Competency", "section": "In the era of Large Language Models (LLMs), deductive reasoning abilities serve as the foundational skills for logical reasoning (Evans, 2002). Unlike traditional rule-based deductive reasoning systems, it involves deriving specific conclusions or answers from general and universally applicable premises using given rules and logic. Specifically, it manifests as a process of Zero-Shot Chain-of-Thought utilizing given rules (Lyu et al., 2023;Kojima et al., 2022). For instance, (Kojima et al., 2022) introduced the \"Let's think step by step\" prompt technique to better evaluate the Deduction Reasoning Competency.\n\nCurrent testing of this ability often intertwines with other skills and still lacks an independent evaluation on typical text (Clark et al., 2020) and symbol-related (Wu et al., 2021) deductive datasets. However, in general, almost all QA tasks can be explicitly evaluated for Deduction Reasoning using the Chain-of-Thought (CoT) approach. Therefore, the effectiveness of models' Deduction Reasoning Competency can be to some extent reflected by evaluating the performance of QA tasks after applying the CoT method.", "filtered_refids": [["b78", null, "b37"], ["b21", "b143"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1132, "num_references": 5}
{"corpusid_sectionid": "260899983-s8", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Induction Reasoning Competency", "section": "In contrast to deductive reasoning, inductive reasoning aims to derive conclusions from specific observations to general principles Olsson et al., 2022). In recent years, a new paradigm of Induction Reasoning has been proposed by (Cheng et al., 2023), which requires models to generate general-purpose program code to solve a class of problems based on given contextual questions and a specific question. For example, Cheng et al. (2023), Jiang et al. (2023) and Sur\u00eds et al. (2023) induced general principle-based solutions by generalizing each question into a universal executable language.\n\nTherefore, for competency evaluation, while DEER  and Mathematical Induction (BIGBench Split (Srivastava et al., 2022)) took the first step in inductive reasoning, we still hope to establish a more systematic and comprehensive benchmark for evaluating this capability. Recently, Bills et al. (2023) has tested the inductive ability of GPT-4 (OpenAI, 2023) to evaluate its effectiveness in inducing patterns that are difficult for humans to express clearly. Intriguingly, Mankowitz et al. (2023) used some techniques to evaluate the extent to which LLM can mine previously unknown patterns.", "filtered_refids": [["b121", null, "b20", "b55"], ["b80", null, "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1183, "num_references": 7}
{"corpusid_sectionid": "260899983-s9", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Abduction Reasoning Competency", "section": "Abduction Reasoning Competency encompasses the task of providing explanations for the output generated based on given inputs (Kakas and Michael, 2020). This form of reasoning is particularly critical in scenarios where uncertainty or incomplete information exists, enabling systems to generate hypotheses and make informed decisions based on the available evidence. Notably, the research conducted by LIREx (Zhao and Vydiswaran, 2021) and STaR (Zelikman et al., 2022) delved into the Abduction Reasoning Competency of models and demonstrated the effectiveness of rationales provided during the Abduction Reasoning process in facilitating improved learning in downstream models.\n\nIn terms of datasets within the LLM setting, the benchmarks HUMMINGBIRD (Mathew et al., 2021) and HateXplain (Hayati et al., 2021) require models to output word-level textual segments as explanations for sentiment classification results. On the other hand, benchmarks such as Wik-iQA (Yang et al., 2015), HotpotQA (Yang et al., 2018), and SciFact (Wadden et al., 2020) provide sentence-level coarse-grained textual segments as explanations for model classification results. ERASER (DeYoung et al., 2020) and FineIEB  provide benchmarks for evaluating Abduction Reasoning with diverse granularity explanations. Based on previous research, Synthetic Reasoning  provides a comprehensive evaluation of both Deduction Reasoning and Abduction Reasoning Competency. Moreover, Hessel et al. (2022) introduced the first comprehensive multimodal benchmark for testing Abduction Reasoning capabilities, providing a solid foundation for future advancements in this domain. Recently, Bills et al. (2023) evaluate GPT-4 by observing the activation of neurons in GPT-2 and offering explanations for the GPT-2's outputs. This research avenue also presents a novel approach for exploring the future evaluation of Abduction Reasoning Competency.", "filtered_refids": [["b162", "b58", "b156"], ["b147", "b130", "b28", "b47", null, "b81", "b148", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1906, "num_references": 11}
{"corpusid_sectionid": "260899983-s10", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Analogical Reasoning Competency", "section": "Analogy reasoning competency encompasses the ability of reasoning by identifying and applying similarities between diverse situations or domains. It is based on the assumption that similar cases or objects tend to exhibit common attributes or behaviors. By recognizing these similarities, analogy reasoning enables systems to transfer knowledge or experience from one context to another (Sinha et al., 2019;Wei et al., 2022b). This type of reasoning plays a vital role in problem-solving, decision-making, and learning from past experiences. A typical example is In-Context-Learning (Dong et al., 2023), where the model is required to perform analogical reasoning based on given contexts, which are evaluated based on the final analogical results.\n\nFor a better assessment and understanding of the model's analogical reasoning ability, Brown et al. (2020) introduces SAT Analogies as a test to evaluate LLM's analogical reasoning capabilities. In recent years, Authorship Verification and ARC datasets (Srivastava et al., 2022) have also proposed evaluation benchmark that involve presenting contextual examples and requiring the model to produce induced pattern-compliant results. However, it should be noted that In-Context Learning (ICL) can be utilized for almost all tasks, enabling the evaluation of models' Analogical Reasoning Competency to some extent through the assessment of their performance after undergoing ICL.", "filtered_refids": [["b113", "b55", "b141"], [null, "b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1426, "num_references": 5}
{"corpusid_sectionid": "260899983-s11", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Multi-hop Reasoning Competency", "section": "Multi-hop reasoning refers to the ability to combine and integrate information from multiple sources or contexts to arrive at logical conclusions. This competency of reasoning enables systems to retrieve coherent and comprehensive answers by traversing multiple pieces of information, thus performing complex tasks of information retrieval, comprehension, and reasoning (Wang et al., 2022a;Qiu et al., 2019).\n\nCurrently, HotpotQA (Yang et al., 2018) serves as a commonly used dataset for multi-hop question answering tasks. Expanding on this, Ye and Durrett (2022) introduced a new and demanding subset that aimed to achieve a balance between accurate and inaccurate predictions using their model. Similarly, StrategyQA (Geva et al., 2021) is another widely used benchmark for multi-hop question answering (Wei et al., 2022b), where the required reasoning steps are implicit in the questions and should be inferred using strategies.", "filtered_refids": [["b133", "b106"], ["b148", "b44", "b141"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 932, "num_references": 5}
{"corpusid_sectionid": "260899983-s12", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Reasoning in Scenarios", "section": "Commonsense Reasoning Commonsense reasoning is crucial for machines to achieve human-like understanding and interaction with the world in the field of machine intelligence (Storks et al., 2019;Bhargava and Ng, 2022). The ability to comprehend and apply commonsense knowledge enables machines to make accurate predictions, engage in logical reasoning, and navigate complex social situations.\n\nOpenBookQA (Mihaylov et al., 2018) provides a foundational test for evaluating Commonsense Reasoning abilities in the form of an open-book exam. Building upon this, Common-senseQA (Talmor et al., 2019) requires models to employ rich world knowledge for reasoning tasks. PIQA (Bisk et al., 2020) introduces a dataset for testing models' understanding of physical world commonsense reasoning. StrategyQA (Geva et al., 2021) presents a complex benchmark that requires commonsense-based multi-step/multi-hop reasoning, enabling a better exploration of the upper limits of models' Commonsense Reasoning Competency. Currently, due to early research on LLM (Wei et al., 2022b), CommonsenseQA (Talmor et al., 2019) remains the most widely used benchmark for commonsense reasoning.\n\nMathematical Reasoning Mathematical reasoning competency is crucial for general intelligent systems. It empowers intelligent systems with the capability of logical reasoning, problem-solving, and data manipulation and analysis, thereby facilitating the development and application of intelligent systems (Qiao et al., 2022;Mishra et al., 2022b;Mishra et al., 2022a).\n\nEarly evluation studies focused on small datasets of elementary-level mathematical word problems (MWPs) (Hosseini et al., 2014), but subsequent research aimed to increase complexity and scale (Srivastava et al., 2022;Brown et al., 2020).\n\nFurthermore, recent benchmarks (Mishra et al., 2022b;Mishra et al., 2022a) have provided comprehensive evaluation platforms and benchmarks for mathematical reasoning abilities. GSM8K (Cobbe et al., 2021) aims to evaluate elementary school MWPs. Currently, due to early research efforts on LLMs (Wei et al., 2022b), it remains the most widely used benchmark for mathematical reasoning in the LLM evaluation. Moreover, There have been recent advancements in evaluation research that explore mathematical reasoning competency integrating external knowledge, leveraging language diversity for multilingual evaluation , and testing mathematical reasoning on multi-modal setting (Lindstr\u00f6m and Abraham, 2022), aiming to judge the broader data reasoning capabilities of large language models (LLMs).\n\nStructured Data Reasoning Structured data reasoning involves the ability to reason and derive insights and answers from structured data sources, such as structured tabular data (Qiao et al., 2022;Li et al., 2023b;Xie et al., 2022).\n\nWikiSQL (Zhong et al., 2017) and WikiTQ (Pasupat and Liang, 2015) provide tables as input and answer questions based on the additional input of questions.\n\nHybridQA (Chen et al., 2020b) and MultiModalQA (Talmor et al., 2021) propose benchmarks for hybrid Structure Reasoning by combining structured table inputs with text (and even other modalities).\n\nSimilarly, Multi-WoZ (Budzianowski et al., 2018), KVRET (Eric et al., 2017) and SQA (Iyyer et al., 2017) integrate table data into task-oriented dialogue systems to generate more complex structures and output dialog-related classifications. Unlike traditional QA, FeTaQA (Nan et al., 2021) requires free-form answers instead of extracting answer spans from passages. ToTTo (Parikh et al., 2020) introduces an open-domain English table-to-text dataset for Structured Data Reasoning. Additionally, benchmarks such as Tab-Fact (Chen et al., 2020a) and FEVEROUS (Aly et al., 2021) evaluate whether model statements are consistent with facts mentioned in structured data. In recent years, with a deeper focus on testing models' mathematical abilities, TabMWP  introduces a grade-level dataset of table-based mathematical word problems that require mathematical reasoning using both text and table data.", "filtered_refids": [["b7", "b119"], ["b44", "b141", "b9", "b123", "b84"], ["b105", "b86", "b87"], ["b49", null, "b10"], ["b86", null, "b87", "b141"], ["b105", "b145", "b68"], ["b98", "b165"], ["b124", "b18"], ["b12", "b97", "b35", null, "b53", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 4049, "num_references": 30}
{"corpusid_sectionid": "260899983-s13", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Reliability", "section": "Reliability measures to what extent a human can trust the contents generated by a LLM. It is of vital importance for the deployment and usability of the LLM, and attracts tons of concerns along with the rapid and astonishing development of recent LLMs (Weidinger et al., 2021;Wang et al., 2022d;Ji et al., 2023;Zhuo et al., 2023). Lots of concepts are closely related to reliability under the context of LLM, including but not limited to hallucination, truthfulness, factuality, honesty, calibration, robustness, interpretability (Lee et al., 2018;Belinkov et al., 2020;Evans et al., 2021;Mielke et al., 2022;Lin et al., 2022b). Reliability also overlaps with the safety and generalization of a LLM (Weidinger et al., 2021). In this section, we will give an overview of two most concerned directions: Hallucination, Uncertainty and Calibration.", "filtered_refids": [["b6", "b136", "b55", "b71", null, "b142", "b83", "b65", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 844, "num_references": 9}
{"corpusid_sectionid": "260899983-s14", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Hallucination", "section": "Hallucination is a term often used to describe LLM's falsehoods, which is the opposite side of truthfulness or factuality OpenAI, 2023;Bubeck et al., 2023). Hallucination is always categorized into intrinsic (close domain) hallucination and extrinsic (open domain) hallucination OpenAI, 2023). Intrinsic hallucination refers to the unfaithfulness of the model output to a given context, while extrinsic hallucination refers to the untruthful contents about the world generated by the model without reference to a given source.\n\nEarly research on hallucination mainly focused on the intrinsic hallucination and lots of interesting metrics were proposed to evaluate the intrinsic hallucination level of a PTM . However, Bang et al. (2023) claimed that intrinsic hallucination was barely found after conducting a comprehensive analysis of ChatGPT's responses. Hence for LLM, the extrinsic hallucination is of the greatest concern. To evaluate the extrinsic hallucination potential of a LLM, a common practice is to leverage knowledge-intensive tasks such as Factual Question Answering (Joshi et al., 2017;Zheng et al., 2023) or Knowledge-grounded Dialogue (Dinan et al., 2019b;Das et al., 2022). TruthfulQA (Lin et al., 2022b) is the most popular dataset used to quantify hallucination level of a LLM. This dataset is adversarially constructed to exploit the weakness of LLM, which contained 817 questions that span 38 categories. OpenAI (2023) leveraged real-world data flagged as non-factual to construct an adversarial dataset to test GPT-4's hallucination potential. BIG-bench (Srivastava et al., 2022), a famous benchmark to evaluate LLM's capabilities, also contains many sub-tasks on factual correctness including TruthfulQA. Although most of these tasks are multiple choices or classification in a fact verification (Thorne et al., 2018) manner, they are closely associated with truthfulness and can be regarded as a generalized hallucination evaluation.", "filtered_refids": [["b11", null], ["b56", "b71", "b30", null, "b164", "b24", "b125", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1959, "num_references": 10}
{"corpusid_sectionid": "260899983-s15", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Uncertainty and Calibration", "section": "A reliable and trustworthy Language model must have the capability to accurately articulate its level of confidence over its response, which requires the model to be aware of its uncertainty. A model that can precisely measure its own uncertainty is sometimes called self-aware, honesty or known-unknown (Kadavath et al., 2022;. In general deep learning applications, calibration concerns about the uncertainty estimation of a classifier. Output probability from a well-calibrated classifier are supposed to be consistent with the empirical accuracy in real world (Vaicenavicius et al., 2019). HELM  treated calibration as one of general metrics and comprehensively evaluated the calibration degree of many prevailing models on multiple choice and classification tasks. (OpenAI, 2023) also showed that GPT-4 before RLHF was well-calibrated on multiple choice tasks, although the decent calibration degree was compromised significantly by post-training.  when it comes to free-form generation, it's a different story. Kuhn et al. (2023) pointed out that semantic nature of language and intractable output space guaranteed the uniqueness of free-form generation. They proposed an algorithm to cluster model outputs and then estimate the model uncertainty. Mielke et al. (2022) claimed that models always express confidence over incorrect answers and proposed the notion of linguistic calibration, which teached models to verbally express uncertainty rather than estimating a probability. Lin et al. (2022a) trained models to directly generate predicted uncertainty probability in natural language.  proposed the SelfAware dataset which contains unanswerable questions and used the accuracy of model rejection as a measure of uncertainty.", "filtered_refids": [[null, "b70", "b126", "b64", "b83"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1735, "num_references": 5}
{"corpusid_sectionid": "260899983-s16", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Safety", "section": "As the LLMs rapidly penetrate into the manufactural and interactive activities of human society, such as LLM-based poem-template generators and chatting robots, the safety concerns for LLMs gain much attention nowadays. The rationales of LLMs are statistics-based, and this inherent stochasticity brings limitations and underlying risks, which deeply affect the real-world deployment of LLMs. Some datasets are proposed to evaluated the safety of LLMs (Table 3), however, the corresponding validity and authority of the safety judgement are inadequate as the current evaluative dimensions are not sufficient (Waseem et al., 2017;Weidinger et al., 2021) and the perception of safety is highly subjective (Koco\u0144 et al., 2021;Weidinger et al., 2021). To this end, based on our survey on relevant papers, we propose a comprehensive perspective on the safety competency of LLMs, ranging from harmful contents to the ethical consideration, to inspire the further developments towards the techniques and evaluations of LLMs safety.", "filtered_refids": [[null, "b142", "b139"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 1024, "num_references": 3}
{"corpusid_sectionid": "260899983-s17", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Harmfulness", "section": "The harmful contents include the offensive language or others that have the explicit harm towards the specific object, such content that has been widely discussed. However, there is not a unified definition of the constitution of harmful contents, based on our surveys, we conclude the relevant themes into five aspects, including offensiveness, violence, crime, sexual-explicit, and unauthorized expertise. Many researches focus on the language detection for the outputs of LLMs to ensure the harmlessness (Wulczyn et al., 2017;Zampieri et al., 2019;Dinan et al., 2019a), while other techniques are proposed to stimulate LLMs to generate safe outputs directly (Krause et al., 2021;Atwell et al., 2022). For the unauthorized expertise, a general LLM should avoid any unauthorized expertise before the establishment of accountability system , which involves the psychological orientation and any medical advice. Besides, the impact of conversation context on safety gains more attention recently, as a results, detective and generative algorithms base on the context are proposed successively (Dinan et al., 2019a;Baheti et al., 2021;. RealToxicityPrompts (Gehman et al., 2020) is a dataset derived from English web texts, where prompts are automatically truncated from sentences classified as toxicity from a widely-used toxicity classifier. RealToxicityPrompts consists of 100K natural prompts, with average 11.7 tokens in length. BAD (Xu et al., 2021) is a dataset collected by the human-in-the-loop strategy, where crowdworkers are ask to prob harmful model outputs. BAD consist of 5k conversations with around 70k utterances in total, which could be used in both non-adversarially and adversarially testing the model weakness.", "filtered_refids": [["b155", "b4", "b146", "b144", "b29", "b62", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1730, "num_references": 7}
{"corpusid_sectionid": "260899983-s18", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Unfairness and Social Bias", "section": "Unfairness and social bias present more covertly and widely for LLMs. Following the previous studies, we conclude that social bias is an inherent characteristic of a LLM, which mainly embody in the dis-tribution difference of a LLM in language selection based on different demographic groups. Compared to the social bias, unfairness is the external form, which reflected in the output performance of specific tasks, for example, the African American English (AAE) is frequently mis-classified as the offensive language by some language detector (Lwowski et al., 2022). However, issues of unfairness and social bias are inevitable as they are widely distributed in human languages, and LLMs are required to memorize language as accurately as possible in the training stage (Weidinger et al., 2021). With respect to evaluate this important aspect, CrowS-Pairs (Nangia et al., 2020) is benchmark proposed to evaluating social bias. There are 1508 examples in CrowS-Pairs that involves nine types of social bias, like gender, race, and Nationality. StereoSet (Nadeem et al., 2021) is a dataset that could be used to evaluate social bias level in both word-level and sentence level, which examples are in four domains: race, gender,religion, and profession. For the StereoSet, the bias level is computed by the difference between model generation probabilities of biased and anti-biased sentence.", "filtered_refids": [["b77", "b142", "b88", "b90"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1391, "num_references": 4}
{"corpusid_sectionid": "260899983-s19", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Others", "section": "As current algorithms for model safety based on the human perception, there is still no golden standardized judgement for LLMs to refer to, especially when a judgement is highly various across societies. It is necessary to align LLMs with the morality, ethics, and values of human society. More and more works focus on reifying this abstract concept into textual data recently, for example,  proposal an implicit reasoning frame to explain the underlying harm of the target language. Besides, other works leverage rule-of-thumb (RoT) annotations of texts to support the judgement (Forbes et al., 2020;Ziems et al., 2022). However, current works in this area are neonatal, and we could expect more related works in the future.\n\nBesides, we are also concerned about the privacy and political risks of LLMs. Since the LLMs are trained on vast corpus collected from books, conversations, web texts and so on, the privacy safety of LLMs arouses people's concern. These training texts might contain the private or sensitive information such as personal physical information, home address, etc. Many studies indicate LLMs are brittle under attacks, leaking the sensitive information unintentionally (Carlini et al., 2020;Li et al., 2022). Therefore, it is essential to test the privacy protection ability of a LLM. Moreover, the politics ignorance is also intractable for a LLM. The politics-related risk mainly stems from the composition of the training corpus. Texts in the corpus are derived from different language and social environments (usually the larger the more diversified), and different countries have different political prudence and stance, which brings additional risks to the wide deployment of a LM.", "filtered_refids": [["b170", "b39"], [null, "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1710, "num_references": 4}
{"corpusid_sectionid": "260899983-s22", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Sentiment Understanding", "section": "Sentiment understand mainly involves the understanding of opinions, sentiments and emotions in the text (Liu, 2015). Representative tasks that reflect this competency include sentiment classification (SC), aspect-based sentiment analysis (ABSA), and multifaceted analysis of subjective texts (MAST). SC aims at assigning pre-defined sentiment classes to given texts. The typical datasets include IMDB (Maas et al., 2011), SST (Socher et al., 2013), Twitter (Rosenthal et al., 2017), Yelp (Zhang et al., 2015). ABSA focuses on identifying the sentiments of specific aspects in a sentence , and the most widely used datasets are the SemEval series (Pontiki et al., 2014;Pontiki et al., 2015;Pontiki et al., 2016). MAST are tasks that involve the finer-grained and broader range of human subjective feelings (emotions (Sailunaz et al., 2018), stance (K\u00fc\u00e7\u00fck and Can, 2021), hate (Schmidt and Wiegand, 2017), irony (Zeng and Li, 2022), offensive (Pradhan et al., 2020), etc.) (Poria et al., 2023). Given that MAST includes a wide range of tasks, the datasets are not listed here in detail. Among them, the commonly used evaluation metrics for the above tasks are accuracy and F1 score (micro or macro). Some preliminary empirical studies  indicate that LLMs can significantly improve performance on these tasks in few-shot learning settings. LLMs have the potential to be a general solution without designing different models for various tasks. Therefore, the sentiment understand competency of different LLMs deserves comprehensive exploration and empirical evaluation. To evaluate the performance of this competency, we can utilize multiple domainspecific datasets or choose the comprehensive benchmark (Srivastava et al., 2022;.", "filtered_refids": [["b111", "b100", "b63", "b103", "b159", "b79", "b101", "b104", "b158", "b102", null, "b114", "b75", "b108", "b109"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1726, "num_references": 15}
{"corpusid_sectionid": "260899983-s23", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Sentiment Generation", "section": "We categorize sentiment generation into two manifestations. One is to generate text that contains sentiments, and the other is to generate text that elicits sentiments. The former requires specifying the desired sentiment, and the latter requires a combination of commonsense knowledge (Speer et al., 2017;Hwang et al., 2021) or theory of mind (Sodian and Kristen, 2010). A classic application scenario is in open-domain dialogue, specifically, emotional dialogue (Zhou et al., 2018), empathetic dialogue (Rashkin et al., 2019), and emotional support conversation (Liu et al., 2021). To measure the quality of the generated text, it is necessary to employ both automatic metrics (such as sentiment accuracy, BLEU (Papineni et al., 2002), perplexity) and human evaluations (human ratings or preference tests). Currently, no work has comprehensively explored this aspect, but it is an essential path towards artificial general intelligence (AGI) (Bubeck et al., 2023).", "filtered_refids": [["b115", "b52", "b11", "b107", "b167", "b116", "b96", "b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 966, "num_references": 8}
{"corpusid_sectionid": "260899983-s24", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Planning", "section": "Planning is the thinking before the actions take place. Given a specific goal, planning is the process to decide the means to achieve the goal. There're few works (Valmeekam et al., 2023;Valmeekam et al., 2022;Pallagani et al., 2023;) that look at the planning ability of LLMs. Some of them focus on commonsense areas  like wedding or menu planning. Others adopted automated planning problems, formal language translators, and verifiers to automatically evaluate LLMs' competency (Valmeekam et al., 2023). With PDDL 6 represented problem descriptions and the translation of such problems into text and back, LLMs can thus sequence a series of actions to reach the planning goal. Whether the planning purpose is achieved can be easily verified via automatic verifiers. Possessing web-scale knowledge, LLMs have great potential for executing planning tasks or assisting planners.", "filtered_refids": [["b127", "b95", "b128"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 877, "num_references": 3}
{"corpusid_sectionid": "2044324-s2", "title": "A Survey of Current Datasets for Vision and Language Research", "date": "2015-06-23", "section_title": "Language Quality", "section": "We define the following criteria for evaluating the captions or instructions of the datasets:\n\n\u2022 Vocabulary Size (#vocab), the number of unique vocabulary words.\n\n2 http://visionandlanguage.net \u2022 Syntactic Complexity (Frazier, Yngve) measures the amount of embedding/branching in a sentence's syntax. We report mean Yngve (Yngve, 1960) and Frazier measurements (Frazier, 1985); each provides a different counting on the number of nodes in the phrase markers of syntactic trees.\n\n\u2022 Part of Speech Distribution measures the distribution of nouns, verbs, adjectives, and other parts of speech.\n\n\u2022 Abstract:Concrete Ratio (#Conc, #Abs, %Abs) indicates the range of visual and non-visual concepts the dataset covers. Abstract terms are ideas or concepts, such as 'love' or 'think' and concrete terms are all the objects or events that are mainly available to the senses. For this purpose, we use a list of most common abstract terms in English (Vanderwende et al., 2015), and define concrete terms as all other words except for a small set of function words.\n\n\u2022 Average Sentence Length (Sent Len.) shows how rich and descriptive the sentences are.\n\n\u2022 Perplexity provides a measure of data skew by measuring how expected sentences are from one corpus according to a model trained on another corpus. We analyze perplexity (Ppl) for each dataset against a 5-gram language model learned on a generic 30B words English dataset. We further analyze pair-wise perplexity of datasets against each other in Section 4.", "filtered_refids": [[], [], ["b10", "b36"], [], ["b32"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1502, "num_references": 3}
{"corpusid_sectionid": "2044324-s7", "title": "A Survey of Current Datasets for Vision and Language Research", "date": "2015-06-23", "section_title": "Crowd-sourced Captions", "section": "\u2022 UIUC Pascal Dataset (Farhadi et al., 2010) is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image.\n\n\u2022 Flickr 30K Images (Young et al., 2014) extends previous Flickr datasets , and includes 158,915 crowd-sourced captions that describe 31,783 images of people involved in everyday activities and events.\n\n\u2022 Microsoft COCO Dataset (MS COCO) (Lin et al., 2014) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled using per-instance segmentations. In total, this dataset contains photos of 91 basic object types with 2.5 million labeled instances in 328k images, each paired with 5 captions. This dataset gave rise to the CVPR 2015 image captioning challenge and is continuing to be a benchmark for comparing various aspects of vision and language research. \u2022 Abstract Scenes Dataset (Clipart) (Zitnick et al., 2013) was created with the goal of representing real-world scenes with clipart to study scene semantics isolated from object recognition and segmentation issues in image processing. This removes the burden of low-level vision tasks. This dataset contains 10,020 images of children playing outdoors associated with total 60,396 descriptions.", "filtered_refids": [["b8"], ["b37"], ["b40", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1293, "num_references": 4}
{"corpusid_sectionid": "2044324-s10", "title": "A Survey of Current Datasets for Vision and Language Research", "date": "2015-06-23", "section_title": "Beyond Visual Description", "section": "Recent work has demonstrated that n-gram language modeling paired with scene-level understanding of an image trained on large enough datasets can result in reasonable automatically generated captions (Fang et al., 2014;Donahue et al., 2014). Some works have proposed to step beyond description generation, towards deeper AI tasks such as question answering (Ren et al., 2015;Malinowski and Fritz, 2014). We present two of these attempts below:\n\n\u2022 Visual Madlibs Dataset (VML) (Yu et al., 2015) is a subset of 10,783 images from the MS COCO dataset which aims to go beyond describing which objects are in the image. For a given image, three Amazon Turkers were prompted to complete one of 12 fill-in-the-blank template questions, such as 'when I look at this picture, I feel -', selected automatically based on the image content. This dataset contains a total of 360,001 MadLib question and answers.\n\n\u2022 Visual Question Answering (VQA) Dataset (Antol et al., 2015) is created for the task of openended VQA, where a system can be presented with an image and a free-form natural-language question (e.g., 'how many people are in the photo?'), and should be able to answer the question. This dataset contains both real images and abstract scenes, paired with questions and answers. Real images include 123,285 images from MS COCO dataset, and 10,000 clip-art abstract scenes, made up from 20 'paperdoll' human models with adjustable limbs and over 100 objects and 31 animals. Amazon Turkers were prompted to create 'interesting' questions, resulting in 215,150 questions and 430,920 answers.\n\n\u2022 Toronto COCO-QA Dataset (CQA) (Ren et al., 2015) is also a visual question answering dataset, where the questions are automatically generated from image captions of MS COCO dataset. This dataset has a total of 123,287 images with 117,684 questions with one-word answer about objects, numbers, colors, or locations.", "filtered_refids": [["b6", "b7", "b18", "b27"], ["b39"], ["b0"], ["b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1903, "num_references": 7}
{"corpusid_sectionid": "2044324-s11", "title": "A Survey of Current Datasets for Vision and Language Research", "date": "2015-06-23", "section_title": "Analysis", "section": "We analyze the datasets introduced in Section 3 according to the metrics defined in Section 2, using the Stanford CoreNLP suite to acquire parses and part-of-speech tags (Manning et al., 2014). We also include the Brown corpus (Francis and Kucera, 1979;Marcus et al., 1999) as a reference point. We find evidence that the VQA dataset captures more abstract concepts than other datasets, with almost 20% of the words found in our abstract concept resource. The Deja corpus has the least number of abstract concepts, followed by COCO and VDC. This reflects differences in col-  Table 2: Perplexities across corpora, where rows represent test sets (20k sentences) and columns training sets (remaining sentences). To make perplexities comparable, we used the same vocabulary frequency cutoff of 3. All models are 5-grams.  Figure 1: Simplified part-of-speech distributions for the eight datasets. We include the POS tags from the balanced Brown corpus (Marcus et al., 1999) to contextualize any very shallow syntactic biases. We mapped all nouns to \"N,\" all verbs to \"V,\" all adjectives to \"J\" and all other POS tags to \"O.\" lecting the various corpora: For example, the Deja corpus was collected to find specifically visual phrases that can be used to describe multiple images. This corpus also has the most syntactically simple phrases, as measured by both Frazier and Yngve; this is likely caused by the phrases needing to be general enough to capture multiple images. The most syntactically complex sentences are found in the Flickr30K, COCO and CQA datasets. However, the CQA dataset suffers from a high perplexity against a background corpus relative to the other datasets, at odds with relatively short sentence lengths. This suggests that the automatic caption-to-question conversion may be creating unexpectedly complex sentences that are less reflective of general language usage. In contrast, the COCO and Flickr30K dataset's relatively high syntactic complexity is in line with their relatively high sentence length. Table 2 illustrates further similarities between datasets, and a more fine-grained use of perplexity to measure the usefulness of a given training set for predicting words of a given test set. Some datasets such as COCO, Flickr30K, and Clipart are generally more useful as out-domain data compared to the QA datasets. Test sets for VQA and CQA are quite idiosyncratic and yield poor perplexity unless trained on in-domain data. As shown in Figure 1, the COCO dataset is balanced across POS tags most similarly to the balanced Brown corpus (Marcus et al., 1999). The Clipart dataset provides the highest proportion of verbs, which often correspond to actions/poses in vision research, while the Flickr30K corpus provides the most nouns, which often correspond to object/stuff categories in vision research.\n\nWe emphasize here that the distinction between a qualitatively good or bad dataset is task dependent. Therefore, all these metrics and the obtained results provide the researchers with an objective set of criteria so that they can make the decision whether a dataset is suitable to a particular task.", "filtered_refids": [["b21", "b9", "b20"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3132, "num_references": 3}
{"corpusid_sectionid": "258740687-s2", "title": "A Survey on Zero Pronoun Translation", "date": "2023-05-17", "section_title": "Linguistic Phenomenon", "section": "Definition of Zero Pronoun Cohesion is a significant property of discourse, and it occurs whenever \"the interpretation of some element in the discourse is dependent on that of another\" (Halliday and Hasan, 1976). As one of cohesive devices, anaphora is the use of an expression whose inter-pretation depends specifically upon antecedent expression while zero anaphora is a more complex scenario in pro-drop languages. A ZP is a gap in a sentence, which refers to an entity that supplies the necessary information for interpreting the gap (Zhao and Ng, 2007). ZPs can be categorized into anaphoric and non-anaphoric ZP according to whether it refers to an antecedent or not. In pro-drop languages such as Chinese and Japanese, ZPs occur much more frequently compared to nonpro-drop languages such as English. The ZP phenomenon can be considered one of the most difficult problems in natural language processing (Peral and Ferr\u00e1ndez, 2003).", "filtered_refids": [[null, "b10", "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 938, "num_references": 3}
{"corpusid_sectionid": "258740687-s7", "title": "A Survey on Zero Pronoun Translation", "date": "2023-05-17", "section_title": "Zero Pronoun Resolution", "section": "The task contains three steps: ZP detection, anaphoricity determination and reference linking. Earlier works investigated rich features using traditional ML models (Zhao and Ng, 2007;Kong and Zhou, 2010;Chen andNg, 2013, 2015). Recent studies exploited neural models to achieve the better performance (Chen and Ng, 2016;Yin et al., 2018;Song et al., 2020). The CoNLL2011 and CoNLL2012 2 are commonlyused benchmarks on modeling unrestricted coreference. The corpus contains 144K coreference instances, but dropped subjects only occupy 15%.\n\nZero Pronoun Recovery Given a source sentence, this aims to insert omitted pronouns in proper positions without changing the original meaning (Yang and Xue, 2010;Yang et al., 2015Yang et al., , 2019a. It is different from ZP resolution, which identifies the antecedent of a referential pronoun (Mitkov, 2014). Previous studies regarded ZP recovery as a classification or sequence labelling problem, which only achieve 40\u223c60% F1 scores on closed datasets Song et al., 2020), indicating the difficulty of generating ZPs. It is worth noting that ZP recovery models can work for ZPT task in a pipeline manner: input sentences are labeled with ZPs using an external recovery system and then fed into a standard MT model (Chung and Gildea, 2010;Wang et al., 2016a).\n\nZero Pronoun Translation When pronouns are omitted in a source sentence, ZPT aims to generate ZPs in its target translation. Early studies have investigate a number of works for SMT models (Chung and Gildea, 2010;Le Nagard and Koehn, 2010;Taira et al., 2012;Xiang et al., 2013;Wang et al., 2016a). Recent years have seen a surge of interest in NMT Wang et al., 2018a), since the problem still exists in advanced NMT systems. ZPT is also related to pronoun translation, which aims to correctly translate explicit pronoun in terms of feminine and masculine. The DiscoMT 3 is a commonly-cited benchmark on pronoun translation, however, there was no standard ZPT benchmarks up until now.", "filtered_refids": [["b51", null, "b19", "b55"], ["b47", "b49", "b48", "b19", null, "b38", "b5"], ["b34", "b46", null, "b23", "b38"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1984, "num_references": 16}
{"corpusid_sectionid": "258740687-s10", "title": "A Survey on Zero Pronoun Translation", "date": "2023-05-17", "section_title": "Overview", "section": "Modeling ZPs has so far not been extensively explored in prior research, largely due to the lack of publicly available data sets. Existing works mostly focused on human-annotated, small-scale and single-domain corpora such as OntoNotes (Pradhan et al., 2012;Aloraini and Poesio, 2020) and Treebanks (Yang and Xue, 2010;Chung and Gildea, 2010). We summarize representative corpora as:\n\n\u2022 OntoNotes. 5 This is annotated with structural information (e.g. syntax and predicate argument structure) and shallow semantics (e.g. word sense linked to an ontology and coreference). It comprises various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in English, Chinese, and Arabic languages. ZP sentences are extracted for ZP resolution task (Chen and Ng, 2013, 2016). \u2022 TVSub. 6 This extracts Chinese-English subtitles from television episodes. Its source-side sentences are automatically annotated with ZPs by a heuristic algorithm (Wang et al., 2016a), which was generally used to study dialogue translation and zero anaphora phenomenon (Wang et al., 2018a;Tan et al., 2021). \u2022 CTB. 7 This is a part-of-speech tagged and fully bracketed Chinese language corpus. The text are extracted from various domains including newswire, government documents, magazine articles, various broadcast news and broadcast conversation programs, web newsgroups and weblogs. Instances with empty category are extracted for ZP recovery task (Yang and Xue, 2010;Chung and Gildea, 2010). \u2022 BaiduKnows. The source-side sentences are collected from the Baidu Knows website, 8 which were annotated with ZP labels with boundary tags. It is widely-used the task of ZP recovery Song et al., 2020).  (Yang and Xue, 2010) ZH Human News 10.6K \u2717 \u2713 \u2717 KTB (Chung and Gildea, 2010) KO Human News 5.0K \u2717 \u2713 \u2717 BaiduKnows  ZH Human Baidu Knows 5.0K \u2717 \u2713 \u2717 TVsub (Wang et al., 2018a) ZH, EN Auto Movie Subtitles 2.2M \u2717 \u2717 \u2713 ZAC (Pereira, 2009) PT Human Mixed Sources 0.6K \u2713 \u2717 \u2717 Nagoya (Zhan and Nakaiwa, 2015) JA Auto Scientific Paper 1.2K \u2713 \u2717 \u2717 SKKU (Park et al., 2015) KO Human Dialogue 1.1K \u2713 \u2717 \u2717 UPENN (Prasad, 2000) HI Human News 2.2K \u2713 \u2717 \u2717 LATL (Russo et al., 2012) IT, ES Human Europarl 2.0K \u2713 \u2717 \u2713 UCFV (Bacolini, 2017) HE Human Dialogue 0.1K \u2713 \u2717 \u2717 Table 1: A summary of existing datasets regarding ZP. We classify them according to language (Lang.), annotation type (Anno.) and text domain. We also report the number of sentences (Size). \"Reso.\", \"Reco.\" and \"Trans.\" indicate whether a dataset can be used for specific ZP tasks. The symbol \u2713 or \u2717 means \"Yes\" or \"No\".", "filtered_refids": [["b49", "b12", null], ["b11", "b34", "b9", "b13", "b25", "b49", null, "b19", "b53", "b38", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2586, "num_references": 14}
{"corpusid_sectionid": "258740687-s12", "title": "A Survey on Zero Pronoun Translation", "date": "2023-05-17", "section_title": "Become An Independent Research Problem.", "section": "Early works extracted ZP information from closed annotations (e.g. OntoNotes and Treebanks) (Yang and Xue, 2010;Chung and Gildea, 2010), which were considered as a sub-problem of coreference or syntactic parsing. With further investigation on the problem, MT community payed more attention to it by manually or automatically constructing ZP recovery and translation datasets (e.g. BaiduKnows and TVsub) (Wang et al., 2018a;. 4. Coping with Data Scarcity. The scarcity of ZPT data remains a core issue (currently only 2.2M \u223c 0.1K sentences) due to two challenges:\n\n(1) it requires experts for both source ZP annotation and target translation (Wang et al., 2016c(Wang et al., , 2018a; (2) annotating the training data manually spends much time and money. Nonetheless, it is still necessary to establish testing datasets for validating/analyzing the model performance.\n\nBesides, pre-trained modes are already equipped with some capabilities on discourse (Chen et al., 2019;Koto et al., 2021). This highlights the importance of formulating the downstream task in a manner that can effectively leverage the capabilities of the pre-trained models.", "filtered_refids": [["b49", null, "b34"], ["b42", "b34"], ["b30", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1141, "num_references": 7}
{"corpusid_sectionid": "258740687-s14", "title": "A Survey on Zero Pronoun Translation", "date": "2023-05-17", "section_title": "Overview", "section": "Early researchers have investigated several approaches for conventional statistical machine translation (SMT) (Le Nagard and Koehn, 2010; Xiang et al., 2013;Wang et al., 2016a). Modeling ZPs for advanced NMT models, however, has received more attention, resulting in better performance in this field (Wang et al., 2018a;Tan et al., 2021;Hwang et al., 2021). Generally prior works fall into three categories: (1) Pipeline, where input sentences are labeled with ZPs using an external ZP recovery system and then fed into a standard MT model (Chung and Gildea, 2010;Wang et al., 2016a); (2) Implicit, where ZP phenomenon is implicitly resolved by modelling document-level contexts Ri et al., 2021); (3) Endto-End, where ZP prediction and translation are jointly learned in an end-to-end manner Tan et al., 2021).\n\nPipeline The pipeline method of ZPT borrows from that in pronoun translation (Le Nagard and Koehn, 2010;Pradhan et al., 2012) due to the strong relevance between the two tasks. Chung and Gildea (2010) systematically examine the effects of empty category (EC) 9 on SMT with pattern-, CRF-and parsing-based methods. The results show that this can really improve the translation quality, even though the automatic prediction of EC is not highly accurate. Besides, Wang et al. (2016aWang et al. ( ,b, 2017b proposed to integrate neural-based ZP recovery with SMT systems, showing better performance on both ZP recovery and overall translation. When entering the era of NMT, ZP recovery is also employed as an external system. Assuming that no-pro-drop languages can benefit pro-drop ones, Ohtani et al. (2019) tagged the coreference information in the source language, and then encoded it using a graph-based encoder integrated with NMT model. Tan et al. (2019) recovered ZP in the source sentence via a BiLSTM-CRF model (Lample et al., 2016). Different from the conventional ZP recovery methods, the label is the corresponding translation of ZP around with special tokens. They then trained a NMT model on this modified data, letting the model learn the copy behaviors. Tan et al. (2021) used ZP detector to predict the ZP position and inserted a special token. Second, they used a attention-based ZP recovery model to recover the ZP word on the corresponding ZP position.\n\nEnd-to-End Due the lack of training data on ZPT, a couple of studies pay attention to data augmentation. Sugiyama and Yoshinaga (2019) employed the back-translation on a context-aware NMT model to augment the training data. With the help of context, the pronoun in no-pronoun-drop language can be translated correctly into pronoundrop language. They also build a contrastive dataset to filter the pseudo data. Besides, Kimura et al.\n\n(2019) investigated the selective standards in detail to filter the pseudo data. Ri et al. (2021) deleted the personal pronoun in the sentence to augment the training data. And they trained a classifier to keep the sentences that pronouns can be recovered without any context. About model architecture, Wang et al. (2018a) first proposed a reconstruction-based approach to reconstruct the ZP-annotated source sentence from the hidden states of either encoder or decoder, or both. The central idea behind is to guide the corresponding hidden states to embed the recalled source-side ZP information and subsequently to help the NMT model generate the missing pronouns with these enhanced hidden representations. Although this model achieved significant improvements, there nonetheless exist two drawbacks: 1) there is no interaction between the two separate reconstructors, which misses the opportunity to exploit useful relations between encoder and decoder representations; and 2) testing phase needs an external ZP prediction model and it only has an accuracy of 66% in F1-score, which propagates numerous errors to the translation model. Thus, Wang et al. (2018b) further proposed to improve the reconstruction-based model by using shared reconstructor and joint learning. Furthermore, relying on external ZP models in decoding makes these approaches unwieldy in practice, due to introducing more computation cost and complexity.\n\nAbout learning objective, contrastive learning is often used to let the output more close to golden data while far away from negative samples. Yang et al. (2019b) proposed a contrastive learning to reduce the word omitted error. To construct the negative samples, they randomly dropped the word by considering its frequency or part-of-speech tag. Hwang et al. (2021) further considered the coreference information to construct the negative sample. According to the coreference information, they took place the antecedent in context with empty, mask or random token to get the negative samples. Besides, Jwalapuram et al. (2020) served the pronoun mistranslated output as the negative samples while golden sentences as positive sample. To get the negative samples, they aligned the word between model outputs and golden references to get the sentences with mistranslated pronoun.\n\nImplicit Some works consider not just the ZPT issue but rather focus on the overall discourse problem. The document-level NMT models (Wang et al., 2017a;Werlen et al., 2018;Ma et al., 2020;Lopes et al., 2020) are expected to have strong capabilities in discourse modelling such as translation consistency and ZPT. Another method is the round-trip translation, which is commonly-used in automatic post-editing (APE) (Freitag et al., 2019), quality estimation (QE) (Moon et al., 2020) to correct of detect the translation errors. Voita et al. (2019) served this idea on context-aware NMT to correct the discourse error in the output. They employed the round-trip translation on monolingual data to get the parallel corpus in the target language. They then used the corpus to train a model to repair discourse phenomenon in MT output.  proposed a fully unified ZPT model, which absolutely released the reliance on external ZP models at decoding time. Besides, they exploited to jointly learn inter-sentential con-  Table 2: A comparison of representative ZPT methods with different benchmarks. The ZPT methods are detailed in Section 5.1. The Baseline is a standard Transformer-big model while ORACLE is manually recovering ZPs in input sentences and then feeding them into the Baseline (Wu et al., 2020). As detailed in Section 4.1, TVSub (both translation and ZP training data) and BaiduKnows (ZP training data) are widely-used benchmarks in movie subtitle and Q&A forum domains, respectively. The Webnovel is our in-house testing data (no training data) in web fiction domain. As detailed in Section 6.1, BLEU is a general-purpose evaluation metric while APT is a ZP-targeted one.\n\ntext (Sordoni et al., 2015) to further improve ZP prediction and translation. Table 1 shows that only the TVsub is suitable for both training and testing in ZPT task, while others like LATL is too small and only suitable for testing. To facilitate fair and comprehensive comparisons of different models across different benchmarkss, we expanded the BaiduKnows by adding human translations and included in-house dataset 10 . As shown in Table 2, we re-implemented three representative ZPT methods and conducted experiments on three benchmarks, which are diverse in terms of domain, size, annotation type, and task. As the training data in three benchmarks decrease, the difficulty of modelling ZPT gradually increases.", "filtered_refids": [["b34", "b46", "b25", null, "b16", "b38"], ["b12", "b25", null, "b7", "b24", "b38"], ["b22"], ["b16", "b34", "b37"], [null, "b50"], ["b6", "b44", "b4", "b29", "b45", "b1", "b36"], ["b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 50, "num_chars": 7429, "num_references": 26}
{"corpusid_sectionid": "258740687-s18", "title": "A Survey on Zero Pronoun Translation", "date": "2023-05-17", "section_title": "Data-Level Methods Do Not Change Model", "section": "Architecture. This is more friendly to NMT. Some researchers targeted making better usage of the limited training data (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021). They trained an external model on the ZP data to recover the ZP information in the input sequence of the MT model (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021) or correct the errors in the translation outputs (Voita et al., 2019). Others aimed to up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019;Kimura et al., 2019;Ri et al., 2021). They preferred to  improve the ZPT performance via a data augmentation without modifying the MT architecture (Wang et al., 2016a;Sugiyama and Yoshinaga, 2019). Kimura et al. (2019); Ri et al. (2021) verified that the performance can be further improved by denoising the pseudo data. 4. Multitask and Multi-Lingual Learning. ZPT is a hard task to be done alone, researchers are investigating how to leverage other related NLP tasks to improve ZPT by training models to perform multiple tasks simultaneously (Wang et al., 2018a). Since ZPT is a cross-lingual problem, researchers are exploring techniques for training models that can work across multiple languages, rather than being limited to a single language (Aloraini and Poesio, 2020).\n\n6 Evaluation Methods", "filtered_refids": [["b34", "b29", "b25", null, "b22", "b7", "b24", "b16", "b38"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1307, "num_references": 9}
{"corpusid_sectionid": "258740687-s19", "title": "A Survey on Zero Pronoun Translation", "date": "2023-05-17", "section_title": "Overview", "section": "There are three kinds of automatic metrics to evaluate performances of related models:\n\n\u2022 Accuracy of ZP Recovery: this aims to measure model performance on detecting and predicting ZPs of sentences in one pro-drop language. For instance, the micro F1-score is used to evaluating Chinese ZPR systems Song et al. (2020). 11 \u2022 General Translation Quality: there are a number of automatic evaluation metrics for measuring general performance of MT systems (Snover  Table 4: Correlation between the manual evaluation and other automatic metrics, which are applied on different ZPT benchmarks, which are same as in Table 2. et al., 2006). BLEU (Papineni et al., 2002) is the most widely-used one, which measures the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations. ME-TEOR (Banerjee and Lavie, 2005) incorporates semantic information by calculating either exact match, stem match, or synonymy match. Furthermore, COMET (Rei et al., 2020) is a neural framework for training multilingual MT evaluation models which obtains new SOTA levels of correlation with human judgements. \u2022 Pronoun-Aware Translation Quality: Previous works usually evaluate ZPT using the BLEU metric (Wang et al., 2016a(Wang et al., , 2018aRi et al., 2021), however, general-purpose metrics cannot characterize the performance of ZP translation. As shown in Table 3, the missed or incorrect pronouns may not affect BLEU scores but severely harm true performances. To fix this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; L\u00e4ubli et al., 2018).", "filtered_refids": [[], ["b15", "b34", null, "b19", "b16", "b38", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1639, "num_references": 7}
{"corpusid_sectionid": "258557362-s1", "title": "Large Language Models Meet NL2Code: A Survey", "date": "2022-12-19", "section_title": "Large Language Models for NL2Code", "section": "Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.\n\nEarly works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.\n\nThese models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.", "filtered_refids": [[null, "b8", "b25"], [null, "b27"], ["b12", null, "b6", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4198, "num_references": 9}
{"corpusid_sectionid": "258557362-s3", "title": "Large Language Models Meet NL2Code: A Survey", "date": "2022-12-19", "section_title": "Large Model Size", "section": "As shown in Figure 2 and Table 2, recent LLMs for NL2Code exhibit larger sizes and superior performance. This is consistent with prior findings that an increased number of model parameters can enhance model capabilities (Radford et al., 2019;Thoppilan et al., 2022;Chowdhery et al., 2022). We further demonstrate the correlation between model size and performance in Figure 3a, which compares the pass@1 results of 10 representative models on the HumanEval benchmark. It is clear that larger models generally result in better performance. Furthermore, we also find that current models, regardless of size, still have the potential for improvement through further increases in size. Additional results on the HumanEval and MBPP benchmarks can be found in Appendix Figure 7, which also support this conclusion.\n\nAdditionally, we conduct an experiment on the HumanEval benchmark to examine the syntax error rates of the code generated by different models of varying sizes. Specifically, we make the models predict 10 code samples for each programming problem, and then calculate the percentage of code samples that have syntax errors. As shown in Figure 3b, results indicate that larger models tend to have lower syntax error rates. It is noteworthy that the largest version of the CodeGen-Mono model exhibits a remarkably low rate of syntax errors, i.e., 6%. However, as evidenced by Figure 3a and Table 2, the CodeGen-Mono model with 16 billion parameters still has unsatisfactory performance in terms of pass@k , e.g., pass@1 to be 29%. This highlights the fact that the current limitation for large pre-trained models is the generation of semantically correct code.", "filtered_refids": [["b24", "b14", null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1666, "num_references": 3}
{"corpusid_sectionid": "258557362-s4", "title": "Large Language Models Meet NL2Code: A Survey", "date": "2022-12-19", "section_title": "Large and Premium Data", "section": "As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. This highlights the importance of selecting and pre-processing high-quality data. In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.\n\nEarly models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet (Husain et al., 2019), CoST (Zhu et al., 2022b), and XL-CoST (Zhu et al., 2022a). However, manual annotation is labour-intensive and time-consuming. There are also models like GPT-3 (Brown et al., 2020), GPT-Neo (Black et al., 2021), and GPT-J (Wang and Komatsuzaki, 2021)  In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities. One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.", "filtered_refids": [[], ["b43", "b27", null, "b42", "b38", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1629, "num_references": 6}
{"corpusid_sectionid": "258557362-s6", "title": "Large Language Models Meet NL2Code: A Survey", "date": "2022-12-19", "section_title": "Benchmarks and Metrics", "section": "To evaluate the NL2Code task, high-quality benchmarks and reliable metrics are fundamental and essential. In this section, we provide a brief overview of current benchmarks and metrics, as well as our observations and the open challenges.\n\nWe summarize 17 well-studied NL2Code benchmarks in Table 3, where we can find that each of these benchmarks has its own characteristics regarding size, language, complexity, and scenario. We observe that most benchmarks contain a limited number of instances. For example, the widely used HumanEval and MBPP have 164 and 974 instances, respectively. This is because these benchmarks are typically hand-written to ensure that LLMs have not seen them during training. In the era of large language models, it is crucial to avoid data leak-age when creating new benchmarks. Additionally, most current benchmarks have their problem descriptions in English and code solutions in Python.\n\nRecently, several multi-lingual benchmarks have been proposed, such as MBXP (Athiwaratkun et al., 2022), HumanEvalX (Zheng et al., 2023), and Mul-tiPL (Cassano et al., 2022), which cover multiple programming languages, and ODEX (Wang et al., 2022c), which covers multiple natural languages. Details of multi-lingual benchmarks are listed in Appendix Table 7. Furthermore, benchmarks have been proposed for other practical scenarios, such as data science (Lai et al., 2022), public library (Zan et al., 2022b), private library (Zan et al., 2022a), multi-turn program synthesis (Nijkamp et al., 2023), and code security (Siddiq and msiddiq, 2022). For execution-based benchmarks, comprehensive test cases with complete coverage of the generated program can ensure the trustworthiness of evaluation results. As a reference, the average number of test cases for each benchmark, as well as the length statistics of the problem descriptions and solutions are also provided in Table 3.\n\nManually evaluating the generated code is impractical, which calls for the need for automatic metrics. The above mentioned benchmarks all provide test cases for execution-based evaluation, where metrics such as pass@k (Chen et al., 2021), n@k (Li et al., 2022b), test case aver-age (Hendrycks et al., 2021), and execution accuracy (Rajkumar et al., 2022) can be used. However, this approach has stringent requirements for the quality of test cases and can only evaluate executable code. For non-executable code, metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), andCodeBLEU (Ren et al., 2020) are used, while they can not precisely evaluate the correctness of the code. So far, there are many open challenges in designing metrics to evaluate various aspects of code, such as vulnerability, maintainability, clarity, execution complexity, and stability.", "filtered_refids": [[], [], [null, "b38", "b8", "b37"], ["b15", null, "b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2765, "num_references": 7}
{"corpusid_sectionid": "258557362-s7", "title": "Large Language Models Meet NL2Code: A Survey", "date": "2022-12-19", "section_title": "Challenges and Opportunities", "section": "Our investigations have revealed that advances in LLMs for NL2Code have a considerable impact on both academia and industry. Despite this progress, there are still numerous challenges that need to be addressed, offering ample opportunities for further research and applications. In this section, we explore the challenges and opportunities in terms of the ability gap between LLMs and humans.\n\nUnderstanding Ability The inherent flexibility of natural language allows for a variety of expressions to convey functional requirements. Humans are able to understand various descriptions at different levels of abstraction. In contrast, current LLMs tend to be sensitive to the given context, which may cause unexpected performance degradation . In addition, LLMs may struggle when faced with complex problems that have numerous conditions and requirements (Barke et al., 2022;Imai, 2022). We believe exploring the understanding abilities of LLMs is a crucial research direction. One potential solution is to break down complex problems into multiple steps, as is commonly done in reasoning tasks (Wei et al., 2022).\n\nJudgement Ability Humans have the ability to determine whether they can solve a programming problem or not. While current models will always return a solution even if there is no answer to the problem, due to the fact that they are trained by unsupervised causal language modeling objective. This can cause problems in practical applications. To improve the judgment ability of LLMs, researchers have employed reinforcement learning to leverage user feedback, as seen in models like InstructGPT  and ChatGPT 8 . However, collecting high-quality feedback for code is costly and challenging. There are also ongoing studies (Chen et al., 2023;Key et al., 2022) exploring the possibility of self-validation for LLMs, which is also a promising research direction.\n\nExplanation Ability It is widely acknowledged that human developers possess the ability to interpret the meaning of the code they write, which is crucial for educational purposes and software maintenance. Recent studies showed that LLMs have the potential to automatically generate code explanations. MacNeil et al. (2022a) proposed using LLMs to generate code explanations for students during their learning process, and MacNeil et al. (2022b) proposed explaining numerous aspects of a given code snippet using Copilot. Further research and explorations are necessary to fully realize the potential of LLMs in this regard.\n\nAdaptive Learning Ability A fundamental difference between current large language models and humans is their ability to adapt to new and updated knowledge. Human developers possess a unique ability to quickly search and learn new materials, such as programming documentation, and adapt to changes in APIs with relative ease. However, re-training or fine-tuning LLMs requires significant effort and resources. This issue has inspired a number of recent studies, such as DocCoder  and APICoder (Zan et al., 2022a), which utilize retrieval-based methods to provide extra or updated knowledge during model inference. Despite these advancements, it remains an open challenge to endow LLMs with the powerful learning capabilities humans possess.\n\nMulti-tasking Ability Large language models have been applied to a variety of code-related tasks, such as code repair (Joshi et al., 2022; Prenner and Robbes, 2021), code search (Neelakantan et al., 2022), and code review (Li et al., 2022c) as well as non-code tasks that can be formatted in a code-like manner, such as mathematics (Drori and Verma, 2021; Drori et al., 2021) and chemistry (Krenn et al., 2022;Hocky and White, 2022). However, there are differences between LLMs and human abilities in terms of multi-tasking. Humans can seamlessly switch between tasks, while LLMs may require sophisticated prompt engineering (Liu et al., 2023). Another evidence is that LLMs lack the ability to quickly master multiple programming languages (Zheng et al., 2023) as humans do. These limitations highlight areas for future research.\n\nIn this paper, we survey 27 existing large language models for NL2Code, and draw a thorough analysis of the underlying reasons for their success. We also provide a detailed review of benchmarks and metrics. Regarding the gap between models and humans, we present ongoing challenges and opportunities. In addition, we have developed a website to track the latest findings in this field. We hope this survey can contribute to a comprehensive overview of the field and promote its thriving evolution.", "filtered_refids": [[], [null, "b32"], [null], [null], ["b37"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 34, "num_chars": 4568, "num_references": 6}
{"corpusid_sectionid": "254877175-s1", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Mathematical Reasoning Tasks", "section": "In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?\n\nRationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5\n\nAnswer: (B) 6.5 Figure 2: An example of geometry problems.\n\nMath Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as \"Which kicker kicked the most field goals?\" over the content of paragraphs.\n\nIn this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?\n\nRationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5\n\nAnswer: (B) 6.5 Figure 2: An example of geometry problems.\n\nMath Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as \"Which kicker kicked the most field goals?\" over the content of paragraphs.", "filtered_refids": [[null], [null, "b6"], [], [], [null], [null, "b6"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 36, "num_chars": 4182, "num_references": 6}
{"corpusid_sectionid": "254877175-s3", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Seq2Seq-based Networks for Math", "section": "Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).\n\nSequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).", "filtered_refids": [["b21", null, "b0", "b27"], ["b21", null, "b0", "b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 2088, "num_references": 8}
{"corpusid_sectionid": "254877175-s4", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Graph-based Networks for Math", "section": "Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).\n\nSeq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).", "filtered_refids": [["b10", "b26", "b9", "b29", null, "b19"], ["b10", "b26", "b9", "b29", null, "b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2422, "num_references": 12}
{"corpusid_sectionid": "254877175-s5", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Attention-based Networks for Math", "section": "The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).\n\nThe attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).", "filtered_refids": [["b9", "b20", "b21", null, "b8"], ["b9", "b20", "b21", null, "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2142, "num_references": 10}
{"corpusid_sectionid": "254877175-s6", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Other Neural Networks for Math", "section": "Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.\n\nOther deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.\n\nDeep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.\n\nOther deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.", "filtered_refids": [[null], [null], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3706, "num_references": 4}
{"corpusid_sectionid": "254877175-s7", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Pre-trained Language Models for Mathematical Reasoning", "section": "Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.\n\nPre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.", "filtered_refids": [[null, "b38"], [null, "b38"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3074, "num_references": 4}
{"corpusid_sectionid": "254877175-s8", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Self-Supervised Learning for Math", "section": "Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: \"all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters\". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.\n\nSelf-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: \"all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters\". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.", "filtered_refids": [[null, "b18", "b17"], [null, "b18", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4022, "num_references": 6}
{"corpusid_sectionid": "254877175-s9", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Task-specific Fine-tuning for Math", "section": "Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).\n\nTask-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).", "filtered_refids": [[null, "b38", "b3"], [null, "b38", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1464, "num_references": 6}
{"corpusid_sectionid": "254877175-s10", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "In-context Learning for Mathematical Reasoning", "section": "Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).\n\nChain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.\n\nApart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the \"Let's think step by step!\" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.\n\nLarge language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).\n\nChain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.\n\nApart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the \"Let's think step by step!\" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.", "filtered_refids": [[null], ["b1"], [null], [null], ["b1"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 4344, "num_references": 6}
{"corpusid_sectionid": "254877175-s11", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "In-context Example Selection", "section": "Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. \n\nEarly chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. ", "filtered_refids": [[null, "b36", "b25", "b3"], [null, "b36", "b25", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1272, "num_references": 8}
{"corpusid_sectionid": "254877175-s12", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "High-quality Reasoning Chains", "section": "Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).\n\nProcess-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.\n\nOutcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through \"selfteaching\", as a complementary solution to produce a higher degree of diversity.\n\n6 Discussion and Findings\n\nEarly chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).\n\nProcess-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.\n\nOutcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through \"selfteaching\", as a complementary solution to produce a higher degree of diversity.\n\n6 Discussion and Findings", "filtered_refids": [["b1", "b25", null, "b36"], [null], [null, "b25"], [], ["b1", "b25", null, "b36"], [null], [null, "b25"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 31, "num_chars": 5746, "num_references": 14}
{"corpusid_sectionid": "254877175-s14", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Analysis of Deep Learning Methods", "section": "Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an \"UNK\" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as \"15\" and \"98\" in GPT-3, while another format like 1, 598 is split as three different tokens: \"1\", \",\", and \"598\". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but\n\nIs the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an \"UNK\" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as \"15\" and \"98\" in GPT-3, while another format like 1, 598 is split as three different tokens: \"1\", \",\", and \"598\". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but", "filtered_refids": [["b14", null, "b33"], ["b14", null, "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3094, "num_references": 6}
{"corpusid_sectionid": "254877175-s15", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Problems", "section": "GPT-3 (text-davinci-002)\n\nJohn had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.\n\nJohn had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?\n\nMary has 5 balls.\n\nJohn had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.\n\nJohn had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.\n\nJohn had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.\n\nJohn had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.\n\nAre deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an \"F\" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work\n\nGPT-3 (text-davinci-002)\n\nJohn had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.\n\nJohn had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?\n\nMary has 5 balls.\n\nJohn had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.\n\nJohn had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.\n\nJohn had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.\n\nJohn had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.\n\nAre deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an \"F\" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work", "filtered_refids": [[], [], [], [], [], [], [], [], [null, "b5"], [], [], [], [], [], [], [], [], [null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 47, "num_chars": 4364, "num_references": 4}
{"corpusid_sectionid": "254877175-s16", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Generalization and Robustness", "section": "Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.\n\nAnother aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.\n\nDespite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.\n\nAnother aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.", "filtered_refids": [[null], [null], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2544, "num_references": 4}
{"corpusid_sectionid": "254877175-s18", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Learning from Feedback", "section": "Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).\n\nAnother important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).", "filtered_refids": [[null, "b31", "b18", "b17"], [null, "b31", "b18", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1784, "num_references": 8}
{"corpusid_sectionid": "254877175-s19", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Multi-modal Mathematical Reasoning", "section": "In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.\n\nIn recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.", "filtered_refids": [[null, "b38"], [null, "b38"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 2094, "num_references": 4}
{"corpusid_sectionid": "254877175-s25", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "A.1 Math Word Problem Solving", "section": "Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.\n\nExisting MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.\n\nMost MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.\n\nDeveloping algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.\n\nExisting MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.\n\nMost MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.", "filtered_refids": [[null], [null, "b37"], [null], [null], [null, "b37"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 36, "num_chars": 4954, "num_references": 8}
{"corpusid_sectionid": "254877175-s26", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "A.2 Theorem Proving", "section": "Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating \"proof steps\" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.\n\nData sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.\n\nOther resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).\n\nInformal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in \"standard\" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.\n\nAn emerging area of research aims to combine elements of informal and formal theorem proving. \n\nRecently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating \"proof steps\" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.\n\nData sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.\n\nOther resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).\n\nInformal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in \"standard\" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.\n\nAn emerging area of research aims to combine elements of informal and formal theorem proving. ", "filtered_refids": [[], ["b21", null, "b40"], ["b15", null], ["b2", "b3"], [], [], ["b21", null, "b40"], ["b15", null], ["b2", "b3"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 4938, "num_references": 14}
{"corpusid_sectionid": "254877175-s27", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "A.3 Geometry Problem Solving", "section": "Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.\n\nSome early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.\n\nAutomated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.\n\nSome early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.", "filtered_refids": [[null, "b6", "b22"], [null], [null, "b6", "b22"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3068, "num_references": 8}
{"corpusid_sectionid": "254877175-s29", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "A.5 Other Quantitative Problems", "section": "Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  \n\nNumbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  ", "filtered_refids": [[null, "b23", "b32"], [null, "b23", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3162, "num_references": 6}
{"corpusid_sectionid": "254877175-s40", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Mathematical Reasoning Tasks", "section": "In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?\n\nRationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5\n\nAnswer: (B) 6.5 Figure 2: An example of geometry problems.\n\nMath Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as \"Which kicker kicked the most field goals?\" over the content of paragraphs.\n\nIn this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?\n\nRationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5\n\nAnswer: (B) 6.5 Figure 2: An example of geometry problems.\n\nMath Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as \"Which kicker kicked the most field goals?\" over the content of paragraphs.", "filtered_refids": [[null], [null, "b6"], [], [], [null], [null, "b6"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 36, "num_chars": 4182, "num_references": 6}
{"corpusid_sectionid": "254877175-s42", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Seq2Seq-based Networks for Math", "section": "Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).\n\nSequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).", "filtered_refids": [["b21", null, "b0", "b27"], ["b21", null, "b0", "b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 2088, "num_references": 8}
{"corpusid_sectionid": "254877175-s43", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Graph-based Networks for Math", "section": "Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).\n\nSeq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).", "filtered_refids": [["b10", "b26", "b9", "b29", null, "b19"], ["b10", "b26", "b9", "b29", null, "b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2422, "num_references": 12}
{"corpusid_sectionid": "254877175-s44", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Attention-based Networks for Math", "section": "The attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).\n\nThe attention mechanism has been successfully applied to NLP (Bahdanau et al., 2015) and vision problems (Xu et al., 2015;Woo et al., 2018), taking into account the hidden vectors of the inputs during the decoding processing. Recently, researchers have been exploring its usefulness in mathematical reasoning tasks, as it can be used to identify the most important relationships between mathematical concepts. For instance, MATH-EN (Wang et al., 2018a) is a math word problem solver which benefits from long-distance dependency information learned by self-attention. Attention-based methods have also been applied to other mathematical reasoning tasks such as geometry problems solving (Robaidek et al., 2018;Chen et al., 2021a) and theorem proving (Yang and Deng, 2019). Various attention mechanisms have been studied to extract better representations, such as Group-ATT (Li et al., 2019) which uses different multi-head attention to extract various types of MWP features, and graph attention which is applied to extract knowledgeaware information in (Wu et al., 2020).", "filtered_refids": [["b9", "b20", "b21", null, "b8"], ["b9", "b20", "b21", null, "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2142, "num_references": 10}
{"corpusid_sectionid": "254877175-s45", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Other Neural Networks for Math", "section": "Deep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.\n\nOther deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.\n\nDeep learning approaches to mathematical reasoning tasks can also make use of other neural networks, such as convolutional neural networks (CNN) and multimodal networks. Some work encodes the input text using a convolutional neural network architecture, giving the model the ability to capture long-term relationships between symbols in the input (Gehring et al., 2017;Wang et al., 2018a,a;Robaidek et al., 2018;Alemi et al., 2016;Loos et al., 2017). For example, the first application of deep neural networks for theorem proving is proposed in (Alemi et al., 2016), which relies on convolutional networks for premise selection. Multimodal mathematical reasoning tasks, such as geometry problem solving and diagram-based mathematical reasoning, are formalized as visual question answer (VQA) problems (Kafle et al., 2018;Chen et al., 2021a;Lu et al., 2021b). In this domain, visual inputs are encoded using ResNet (He et al., 2016) or Faster- RCNN (Ren et al., 2015), while textual representations are obtained via GRU or LTSM. Subsequently, the joint representation is learned using multimodal fusion models, such as BAN (Kim et al., 2018), FiLM (Perez et al., 2018), and DAFA (Gao et al., 2019.\n\nOther deep neural network structures can also be used in mathematical reasoning. A Graph Neural Network (GNN) is employed for geometry problem parsing in , taking advantage of its success in spatial reasoning. WaveNet has been applied to theorem proving (Loos et al., 2017;Bansal et al., 2019), due to its ability to address longitudinal time-series data. Furthermore, Transformers are found to outperform GRU in generating mathematical equations in DDT (Meng and Rumshisky, 2019). Finally, MathDQN (Wang et al., 2018b) is the first work to explore reinforcement learning for math word problem solving, taking advantage of its strong search capabilities.", "filtered_refids": [[null], [null], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3706, "num_references": 4}
{"corpusid_sectionid": "254877175-s46", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Pre-trained Language Models for Mathematical Reasoning", "section": "Pre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.\n\nPre-trained language models (Devlin et al., 2019;Radford et al., 2020;Brown et al., 2020) have demonstrated remarkable performance gains on a wide range of NLP tasks. By pre-training on a large corpus of text, the models learn valuable world knowledge (Guu et al., 2020), which could be applied to downstream tasks. Similar ideas can be applied to math-related problems, and previous work has shown the promising performance of pretrained language models in answering math word problems (Kim et al., 2020), assisting with theorem proving (Wu et al., 2022b), as well as solving other mathematical tasks (Charton, 2022). However, though large language models excel in modeling natural language, there are several challenges to using them for mathematical reasoning. First, pre-trained language models are not specifically trained on mathematical data. This likely contributes to them being less proficient in math-related tasks compared to natural language tasks. There is also less mathematical or scientific data available for large-scale pre-training compared to text data. Second, the size of pre-trained models continues to grow, making it expensive to train the entire model from scratch for specific downstream tasks. Additionally, downstream tasks may deal with different input formats or modalities, such as structured tables (Zhao et al., 2022) or diagrams (Lu et al., 2021b). To address these challenges, researchers have to adjust pre-trained models by finetuning them on downstream tasks or adapting the neural architectures.", "filtered_refids": [[null, "b38"], [null, "b38"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3074, "num_references": 4}
{"corpusid_sectionid": "254877175-s47", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Self-Supervised Learning for Math", "section": "Self-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: \"all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters\". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.\n\nSelf-supervised learning is a machine learning approach in which an algorithm learns to perform a task without being explicitly provided with labeled training data. Table 2 provides a list of language models pre-trained with self-supervised tasks for mathematical reasoning. Model scale. There is a clear trend that pre-trained language models have become increasingly larger in the past few years (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020;Radford et al., 2020;Brown et al., 2020). A recent study (Liang et al., 2022a) shows that model scale within a model family reliably predicts model accuracy. The study also mentions an interesting thresholding effect: \"all models that win head-to-head model comparisons for accuracy at a rate well above chance are at least 50B parameters\". A similar size-growing trend can be observed in the field of mathematical reasoning with pre-trained language models.    (Wu et al., 2021d;Krishna et al., 2021;Ri and Tsuruoka, 2022;Anderson and Farrell, 2022;Wu et al., 2022c) shows that pre-training on data that is fully synthetically generated-synthetic pre-training can actually provide substantial gains.  guage Modeling (CLM), where the model is trained to predict the next token in a sequence of tokens. Following the same paradigm, researchers pre-train language models with MLM and CLM tasks on mathematical or scientific corpora for downstream tasks (Polu and Sutskever, 2020;Hendrycks et al., 2021b;Jiang et al., 2022b). There is also recent work that designs customized tasks to inject mathematical reasoning capabilities into language models. For instance, Liang et al. (2022b) pre-train language models with a suite of 8 numeracy-augmented tasks with consideration of reasoning logic and numerical properties. LIME (Wu et al., 2021d) proposes synthetic pretraining tasks to learn three reasoning primitives: deduction, induction, and abduction before learning more complex reasoning skills, which also be regarded as a form of curriculum learning.", "filtered_refids": [[null, "b18", "b17"], [null, "b18", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4022, "num_references": 6}
{"corpusid_sectionid": "254877175-s48", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Task-specific Fine-tuning for Math", "section": "Task-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).\n\nTask-specific fine-tuning is a technique to improve the performance of a pre-trained language model on a specific task. This is also a common practice when there is not enough data for training the large models from scratch. As shown in Table 3, existing work fine-tunes pre-trained language models on a variety of downstream tasks, such as math word problems (Kim et al., 2020;Shen et al., 2021), MathQA (Zhao et al., 2022), geometry problem solving (Lu et al., 2021a), linear algebra (Charton, 2022), and theorem proving (Welleck et al., 2022a). Apart from fine-tuning the model parameters, some work also uses pre-trained language models as encoders and ensembles them with other modules for downstream tasks (Lu et al., 2021b).", "filtered_refids": [[null, "b38", "b3"], [null, "b38", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1464, "num_references": 6}
{"corpusid_sectionid": "254877175-s49", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "In-context Learning for Mathematical Reasoning", "section": "Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).\n\nChain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.\n\nApart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the \"Let's think step by step!\" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.\n\nLarge language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).\n\nChain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.\n\nApart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the \"Let's think step by step!\" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.", "filtered_refids": [[null], ["b1"], [null], [null], ["b1"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 4344, "num_references": 6}
{"corpusid_sectionid": "254877175-s50", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "In-context Example Selection", "section": "Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. \n\nEarly chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. ", "filtered_refids": [[null, "b36", "b25", "b3"], [null, "b36", "b25", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1272, "num_references": 8}
{"corpusid_sectionid": "254877175-s51", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "High-quality Reasoning Chains", "section": "Early chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).\n\nProcess-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.\n\nOutcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through \"selfteaching\", as a complementary solution to produce a higher degree of diversity.\n\n6 Discussion and Findings\n\nEarly chain of thought work (e.g., Wei et al. (2022)) mainly relies on a single human-annotated reasoning chain as a prompt. However, manually creating reasoning chains has two disadvantages. First, as tasks become more complex, current models may not be sufficient to learn to perform all necessary reasoning steps and cannot easily generalize to different tasks. Second, a single decoding process is vulnerable to incorrect inference steps, leading to an incorrect prediction as the final answer. To address this limitation, recent studies mainly fo- Hand-crafted -Self-Consistency-CoT (Wang et al., 2023) Codex (175B) Random Language Hand-crafted Self-consistency Least-to-most CoT (Zhou et al., 2023) Codex (175B) Random Language Hand-crafted -PromptPG- CoT (Lu et al., 2022b) GPT-3 (175B) RL Language Hand-crafted -Retrieval-CoT (Zhang et al., 2023) GPT-3 (175B) Retrival Language Auto-generated -Auto-CoT (Zhang et al., 2023) Codex (175B) Clustering Language Auto-generated -Complexity- CoT (Fu et al., 2023) GPT-3 (175B) Complexity Language Hand-crafted Self-consistency Few-shot- PoT (Chen et al., 2022b) GPT-3 (175B) Random Code Hand-crafted - cus on two aspects, (i) hand-crafting more complex demonstrations, which we refer to as process-based approaches (Zhou et al., 2023;Chen et al., 2022b), (ii) leveraging ensemble-like methods, which we refer to as outcome-based approaches (Wang et al., 2023;Li et al., 2022a).\n\nProcess-based approaches aim to improve the chain-of-thought reasoning quality, especially for complex reasoning tasks. In least-to-most prompting (Zhou et al., 2023), the problem-solving process is implemented through two-stage prompting: (i) reducing a complex problem into a list of subproblems; (ii) solving these sub-problems sequentially, so that solving a given sub-problem is facilitated by the answers to previously solved subproblems. Similarly , an alternative solution that uses large language models to express the reasoning process as a program. The computation is then relegated to an external computer, which executes the generated programs to derive the answer. A more recent work, Chameleon (Lu et al., 2023), integrates different tools to enhance the abilities of LLMs for compositional reasoning.\n\nOutcome-based approaches acknowledge the potential incorrectness of an individual reasoning path, and instead use multiple reasoning paths (Wang et al., 2023;Li et al., 2022a). Selfconsistency (Wang et al., 2023) generates a set of reasoning paths by sampling from the language model, and marginalizes out the reasoning paths by choosing the most common answer. In addition to using sampling with a single prompt to produce multiple reasoning paths, Li et al. (2022a) propose to introduce diverse prompts through \"selfteaching\", as a complementary solution to produce a higher degree of diversity.\n\n6 Discussion and Findings", "filtered_refids": [["b1", "b25", null, "b36"], [null], [null, "b25"], [], ["b1", "b25", null, "b36"], [null], [null, "b25"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 31, "num_chars": 5746, "num_references": 14}
{"corpusid_sectionid": "254877175-s53", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Analysis of Deep Learning Methods", "section": "Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an \"UNK\" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as \"15\" and \"98\" in GPT-3, while another format like 1, 598 is split as three different tokens: \"1\", \",\", and \"598\". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but\n\nIs the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an \"UNK\" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as \"15\" and \"98\" in GPT-3, while another format like 1, 598 is split as three different tokens: \"1\", \",\", and \"598\". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but", "filtered_refids": [["b14", null, "b33"], ["b14", null, "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3094, "num_references": 6}
{"corpusid_sectionid": "254877175-s54", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Problems", "section": "GPT-3 (text-davinci-002)\n\nJohn had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.\n\nJohn had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?\n\nMary has 5 balls.\n\nJohn had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.\n\nJohn had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.\n\nJohn had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.\n\nJohn had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.\n\nAre deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an \"F\" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work\n\nGPT-3 (text-davinci-002)\n\nJohn had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.\n\nJohn had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?\n\nMary has 5 balls.\n\nJohn had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.\n\nJohn had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.\n\nJohn had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.\n\nJohn had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.\n\nAre deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an \"F\" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work", "filtered_refids": [[], [], [], [], [], [], [], [], [null, "b5"], [], [], [], [], [], [], [], [], [null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 47, "num_chars": 4364, "num_references": 4}
{"corpusid_sectionid": "254877175-s55", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Generalization and Robustness", "section": "Despite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.\n\nAnother aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.\n\nDespite impressive progress, neural models commonly display generalization and robustness fail-ures on reasoning tasks. For example, above we discussed difficulties in generalizing to larger numbers (Table 5) or remaining robust to nearby problems (Table 6), while others identify failures in generalizing to longer problems than those observed in training (e.g., Anil et al. (2022)). One direction is to explore new inference-time (Jung et al., 2022;Mitchell et al., 2022) or fine-tuning (Anil et al., 2022) strategies.\n\nAnother aspect of generalization relates to the role of memorization. For example, is the ability to produce a complex solution dependent on seeing many similar solutions during training, or even on memorizing the solution? Term frequency in the pretraining corpus is known to impact accuracy in simple arithmetic tasks (Razeghi et al., 2022) or factual question answering (Kandpal et al., 2022). On the other hand, Lewkowycz et al. (2022) did not find evidence of memorization in complex outputs, yet their training set and model are not available for inspection. Gaining a full understanding of these factors for complex problems and outputs (e.g., multi-step solutions or proofs) requires more analysis, as well as accessible datasets and models.", "filtered_refids": [[null], [null], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2544, "num_references": 4}
{"corpusid_sectionid": "254877175-s57", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Learning from Feedback", "section": "Another important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).\n\nAnother important direction to further improve language models for mathematical reasoning is to let the model learn from feedback. Such a process makes the continual improvement of models' output quality and safety possible. An example is us-ing reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) to align language models with instructions. The idea is to let humans rank the generated outputs of language models and use the learned reward function to finetune the language model with policy gradient (Ouyang et al., 2022;Glaese et al., 2022;Qiu et al., 2022a). In the context of mathematical reasoning, feedback does not necessarily come from humans directly. The outcome of a theorem-proof engine (Jiang et al., 2021;Wu et al., 2021dWu et al., , 2022c or the execution result of model-generated scripts can also be used as the reward source (Polu and Sutskever, 2020).", "filtered_refids": [[null, "b31", "b18", "b17"], [null, "b31", "b18", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1784, "num_references": 8}
{"corpusid_sectionid": "254877175-s58", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Multi-modal Mathematical Reasoning", "section": "In recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.\n\nIn recent years, there has been growing interest in multi-modal mathematical reasoning, which involves using multiple sources of information, such as text, tables, natural images, and diagrams (Kahou et al., 2018;Kafle et al., 2018;Lu et al., 2021bLu et al., , 2022b. However, currently available datasets in this domain tend to be small (Zhao et al., 2022), generated from templates (Kahou et al., 2018), or focus on specific topics (Lu et al., 2021a;Chen et al., 2022a). One line of current research involves applying VQA-based frameworks to analyze figures and plots, but this approach can result in significant semantic gaps due to the fact that most VQA models are trained on natural images. One potential direction for future work is to enhance the ability of multi-modal mathematical reasoning systems to tackle more complex and realistic problems. This may involve creating unified models for interpreting and integrating different modalities, as well as developing better evaluation benchmarks to assess the performance of these systems.", "filtered_refids": [[null, "b38"], [null, "b38"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 2094, "num_references": 4}
{"corpusid_sectionid": "254877175-s64", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "A.1 Math Word Problem Solving", "section": "Developing algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.\n\nExisting MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.\n\nMost MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.\n\nDeveloping algorithms to solve math word problems (MWPs) automatically has been an interest of NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). A math word problem (also termed an algebraic or arithmetic word problem) describes a brief narrative that involves characters, entities, and quantities. The mathematical relationship of an MWP can be modeled with a set of equations whose solution reveals the final answer to the question. A typical example is shown in Table 1. A question involves the four basic arithmetic operations of addition, subtraction, multiplication, and division with single or multiple operation steps. The challenge of MWPs for NLP systems lies in the need for language comprehension, semantic parsing, and multiple mathematical reasoning skills.\n\nExisting MWP datasets cover grade school problems, which are crawled from online learning websites (Koncel-Kedziorski et al., 2015), collected from textbooks, or manually annotated by human workers (Patel et al., 2021). Early math word problem datasets are relatively small or limited to a small number of operation steps (Hosseini et al., 2014;Kushman et al., 2014;Roy et al., 2015). Some recently curated datasets aim to increase problem diversity and difficulty levels. For example, Ape210K (Zhao et al., 2020) consists of 210k elementary math word problems, which is the largest publicly available. The problems in GSM8K (Cobbe et al., 2021) can involve up to 8 steps to solve. SVAMP (Patel et al., 2021) is a benchmark that tests the robustness of deep learning models to math word problems with simple variations. More recently built datasets involve modalities beyond text. For example, IconQA (Lu et al., 2021b) provides an abstract diagram as a visual context, while TabMWP (Lu et al., 2022b) provides a tabular context for each problem.\n\nMost MWP datasets provide annotated equations as a rationale for the solution (e.g., Table 1). To improve the performance and interpretability of the learned solvers, MathQA (Tafjord et al., 2019) is annotated with precise operation programs, and MathQA-Python (Austin et al., 2021) is provided with specific Python programs instead. Another line of datasets annotates the problems with multistep natural language solutions that are regarded as more human-readable (Ling et al., 2017;Cobbe et al., 2021;Lu et al., 2022b). Lila (Mishra et al., 2022a) annotates many of the previously mentioned MWP datasets with Python program rationales.", "filtered_refids": [[null], [null, "b37"], [null], [null], [null, "b37"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 36, "num_chars": 4954, "num_references": 8}
{"corpusid_sectionid": "254877175-s65", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "A.2 Theorem Proving", "section": "Recently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating \"proof steps\" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.\n\nData sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.\n\nOther resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).\n\nInformal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in \"standard\" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.\n\nAn emerging area of research aims to combine elements of informal and formal theorem proving. \n\nRecently, there has been increased interest in using language models for theorem proving in formal interactive theorem provers (ITP) (e.g., Polu and Sutskever (2020) To prove a theorem in an ITP, the theorem is stated in the ITP's programming language, then simplified by generating \"proof steps\" until it is reduced to known facts. The result is a sequence of steps that constitutes a verified proof.\n\nData sources for neural theorem proving in ITPs include interactive learning environments that interface with ITPs, and datasets derived from proofs in ITP libraries. For example, CoqGym (Yang and Deng, 2019) provides an interactive environment and 71K human-written proofs for the Coq ITP. For Isabelle, PISA (Jiang et al., 2021) enables interaction and provides a dataset of 183k proofs mined from the Isabelle standard library and Archive of Formal Proofs. For Lean, LeanStep (Han et al., 2022) provides a dataset of proof-steps from Lean's mathematical library along with auxiliary tasks, while Lean-Gym (Polu et al., 2023) provides an interactive REPL. The miniF2F (Zheng et al., 2022) benchmark aims to provide a shared benchmark across ITPs, consisting of 488 problem statements sourced from mathematical competitions.\n\nOther resources provide proxy environments or tasks. For example, INT (Wu et al., 2021c) provide a synthetic proving environment to measure six different types of generalization. Li et al. construct IsarStep using the Isabelle Archive of Formal Proofs, and propose a task of filling in a missing intermediate proposition. Early applications of deep learning for formal theorem proving focus on selecting relevant premises (Alemi et al., 2016).\n\nInformal theorem proving presents an alternative medium for theorem proving, in which statements and proofs are written in the mixture of natural language and symbols used in \"standard\" mathematics (e.g., in L A T E X), and are checked for correctness by humans. Early work focuses on selecting relevant premises (Ferreira and Freitas, 2020b,a). Welleck et al. (2021) develop NaturalProofs, a large-scale dataset of 32k informal mathematical theorems, definitions, and proofs, and provide a benchmark for premise selection via retrieval and generation tasks. Welleck et al. (2022a) adapt NaturalProofs for full proof generation, and provide a human evaluation protocol and proxy automatic metrics.\n\nAn emerging area of research aims to combine elements of informal and formal theorem proving. ", "filtered_refids": [[], ["b21", null, "b40"], ["b15", null], ["b2", "b3"], [], [], ["b21", null, "b40"], ["b15", null], ["b2", "b3"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 4938, "num_references": 14}
{"corpusid_sectionid": "254877175-s66", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "A.3 Geometry Problem Solving", "section": "Automated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.\n\nSome early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.\n\nAutomated geometry problem solving (GPS) is also a long-standing AI task in mathematical reasoning research (Gelernter et al., 1960;Wen-Tsun, 1986;Chou et al., 1996;Ye et al., 2008) and has attracted much attention in recent years. Different from a math word problem, a geometry problem consists of a textual description in natural language and a geometric diagram. As shown in Figure 2, the multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. GPS is a challenging task for deep learning methods due to the complex skills it requires. It involves the ability to parse multimodal informa-tion, perform symbolic abstraction, utilize theorem knowledge, and conduct quantitative reasoning.\n\nSome early datasets are proposed to facilitate research in this domain (Seo et al., 2015;Alvin et al., 2017;Sachan et al., 2017;Sachan and Xing, 2017). However, these datasets are relatively small or not publicly available, which limits the development of deep learning methods. In response to this limitation, Lu et al. create the Geometry3K dataset, which consists of 3,002 multi-choice geometry problems with unified logic form annotations for the multimodal inputs. More recently, largerscale datasets such as GeoQA (Chen et al., 2021a), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022a) have been introduced and are annotated with programs that can be learned by neural solvers and executed to obtain the final answers.", "filtered_refids": [[null, "b6", "b22"], [null], [null, "b6", "b22"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3068, "num_references": 8}
{"corpusid_sectionid": "254877175-s68", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "A.5 Other Quantitative Problems", "section": "Numbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  \n\nNumbers are an integral part of our daily lives, and we humans reason with numbers in a variety of tasks, such as understanding news, reports, elections, and markets. This has led many in the community to question whether AI systems can effectively perform quantitative reasoning in everyday scenarios. To this end, various benchmarks have been developed to evaluate the capabilities of AI systems in this area. Diagrams, such as figures, charts, and plots, are essential media that convey large amounts of information in a concise way. FigureQA (Kahou et al., 2018), DVQA (Kafle et al., 2018), MNS (Zhang et al., 2020c), PGDP5K (Hao et al., 2022), and GeoRE (Yu et al., 2021a), are released to investigate models' abilities to reason about quantitative relationships among entities grounded in diagrams. NumerSense (Lin et al., 2020), instead, examines whether and to what extent existing pre-trained language models can induce numerical commonsense knowledge. EQUATE (Ravichander et al., 2019) formalizes aspects of quantitative reasoning in a natural language inference framework. Quantitative reasoning can appear frequently in specific domains like finance, science, and programming. For instance, the ConvFinQA (Chen et al., 2022c) targets numerical reasoning over financial reports in a conversational question answering format. Sci-enceQA (Lu et al., 2022a) involves numerical reasoning in scientific domains, while P3 (Schuster et al., 2021) studies the function inference ability of deep learning models to find a valid input which makes the given program return True.  ", "filtered_refids": [[null, "b23", "b32"], [null, "b23", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3162, "num_references": 6}
{"corpusid_sectionid": "237532535-s1", "title": "A Survey of Online Hate Speech through the Causal Lens", "date": "2021-09-16", "section_title": "Sociological Causal Impact", "section": "Digital profanity is, first and foremost, a sociological phenomenon that affects many aspects of the virtual world. There are some pronounced directions which require thorough examination, such as the consequences of such actions on the affected communities as well as their targets, or the impact of banning policies aiming at fighting against it. Moreover, there are several other underlying effects that could get in the spotlight, including the ramifications of interventions or the reasons that drive toxic actors. As a result, it is of paramount importance to examine hate speech in a holistic way and concretely quantify the drivers of these outcomes. However, it is impossible to achieve this task without considering causality, because non-causal inferences can never be conclusive. Surprisingly, despite the broad interest of the research community in this topic, very little work has been done on attempting causal links, even on the most prominent tasks related to online hate speech (OHS).\n\nIn the present survey, we classify fundamental sociological outcomes related to OHS and outline the most distinguished body of literature. The classification results into three major pillars, with respect to the following:\n\n\u2022 Digital misbehaviours versus the physical world: we summarise studies concerning the propagation of online hate speech to real life Schwarz, 2018, 2020) as well as the influence of offline events to the dissemi-nation of the issue online (Olteanu et al., 2018;Thomas et al., 2021) 1 .\n\n\u2022 Harmful content versus the individuals: we outline research concerning the impact of toxic behaviours on the targets or passive readers (Saha et al., 2019a) as well as the by-products of web characteristics (such as anonymity) on hate speech producers (von Essen and Jansson, 2020).\n\n\u2022 Effect of interventions: finally, we review works that revolve around quantifying the effect of combating strategies which various platforms adopt. Existing research has focused on limiting policies, censoring and counter-speaking (\u00c1lvarez-Benjumea and Winter, 2018), social sanctioning (Munger, 2017), quarantining (Chandrasekharan et al., 2020) and banning hateful communities (Chandrasekharan et al., 2017;Thomas et al., 2021).\n\nIn all the of the cases, we can consider the role of hate speech both from the position of a phenomenon which is the result of (potentially) multiple causes, but also from the position of the causal root and study its effects. Therefore, in the following sections we review both directions.", "filtered_refids": [[], [], [null, "b22", "b36"], ["b29"], ["b21", "b1", null, "b36"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2526, "num_references": 8}
{"corpusid_sectionid": "237532535-s2", "title": "A Survey of Online Hate Speech through the Causal Lens", "date": "2021-09-16", "section_title": "Digital and Physical World", "section": "The first and most apparent pillar concerns the interrelation of online hate speech with the physical world, in a range of simple dissemination to absolute influence. Towards this direction, Schwarz (2018, 2020) have conducted research on the causal effect of social media on the propagation of hate sentiments offline, whether antirefugee (M\u00fcller and Schwarz, 2018) or more specifically anti-Muslim (M\u00fcller and Schwarz, 2020).\n\nIn (M\u00fcller and Schwarz, 2018), the authors provide evidence that there is significant association between negative content against refugees existing on Facebook and offline hate crimes in Germany on a municipal level, while controlling for multiple potentially confounding factors such as German municipalities' characteristics and overall social media usage. To reach to this conclusion they combine a variety of data sources; online antirefugee sentiment is represented by content from a widespread Facebook page of a German right-wing party, which hosts plenty of far-right content; controlling for the network's popularity in Germany, the authors measure Facebook outages and internet disruptions; finally, to further measure user activity on the network and create controls based on a neutral subject, they explore another broadly popular page of a famous commercial product. The causal framework they implement is a fixed-effects regression model (inspired by Bartik (1991)), which considers the aforementioned panel data combined with a range of controls. They discover that the effect is stronger in areas with higher Facebook usage and demonstrate a robustly strong connection between the activity of the right-wing group and severe hate crimes. Similarly, in (M\u00fcller and Schwarz, 2020), they study the causal impact of Islamophobic social media content on registered crimes and overall negative sentiment against Muslims, and whether former US president Donald Trump's Twitter campaign has contributed to the propagation of Islamophobia. To ensure validity and robustness of their findings, they fuse a number of different data sources and employ a difference-in-differences approach. The data originate mainly from Twitter for the social media information, a survey by FBI to discover hate crimes, data from mass media, demographic information about US counties etc. Their findings provide evidence to associate a 38% larger increase in hate crimes, between 2010 and 2017, with higher exposure to social media. Moreover, consistently with previous research, they also provide evidence which shows a connection amid the start of Trump's presidential campaign with an increase of anti-Muslim sentiments in USA. Both projects illustrate there is strong evidence linking OHS with offline occurrences of hate-related crimes, with the former having a causal effect on the latter. 2 Looking at the opposite direction, online hate speech is frequently affected by events taking place in the offline world. For example, following the September 11 th , 2001 attack to the twin towers of 2 It goes without saying that results from papers that employ causality need to be interpreted with great care. As M\u00fcller and Schwarz (2018) emphasise, for instance, their findings do not indicate that social media can cause crimes against minorities, but rather \"social media can act as a propagating mechanism [...]\" so that \"shifts in exposure to anti-refugee sentiment on social media can increase the number of anti-refugee attacks\". New York City, there seems to be an increase in Islamic terrorist attacks as much as an increase in Islamophobia. It is very well expected that this will affect the online world and the way Muslims are perceived, which seems to actually be the case according to Olteanu et al. (2018). In (Olteanu et al., 2018) they analyse the influence of offline events on online hate speech. More specifically, they study the impact of several attacks related to the Islamic State -both Islamic terrorist attacks and Islamophobic ones -in terms of online hate speech and counter-hate speech. Their findings indicate that terrorist attacks show an increase in OHS, especially towards Muslims and Arabs.\n\nTo calculate the causal effect, they construct time series from Reddit and Twitter -representing the number of posts and unique users involved with the event -and then synthesize counterfactual time series for the same period of time, such as they would be produced had the attacks not happened. The counterfactual data are created by composing timelines of the same event, while adjusting for a temporal shift prior to the event, so that the time series will reflect a similar period of time but in different time windows. The produced timelines, put together with other external data sources, are then fit into a state space model (Brodersen et al., 2015) using maximum likelihood estimation, to predict the synthesized control time series. Comparing the treatment and control series, they calculate relative effects for a number of manually curated terms, which are also annotated across four hate speech dimensions (stance, targets, severity, and framing).", "filtered_refids": [[null, "b19", "b20"], [null, "b19", "b20", "b22"], ["b6"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 5106, "num_references": 8}
{"corpusid_sectionid": "237532535-s3", "title": "A Survey of Online Hate Speech through the Causal Lens", "date": "2021-09-16", "section_title": "Actors and Targets", "section": "Beyond broadly looking at the overall impact of OHS, the next step would be to concentrate on the individuals and speculate how OHS affects and is affected by the participating members, whether these are at the producing or receiving end of such content. For example, in the aforementioned works, Olteanu et al. (2018) and M\u00fcller and Schwarz (2020) have made some general remarks regarding OHS, but in order to effectively focus on the task they narrow the type of hate speech to be racism and, more specifically, Islamophobism. Emphasis, however, is given on understanding the dynamics of diffusion and not on studying the impact on individuals who support Islam.\n\nOn the these grounds, Saha et al. (2019a) study the effect of hate prevalence on the stress levels of US college students, within Reddit college communities. To measure the levels of toxicity, they employ hate lexicons, with the help of which they compute the College Hate Index (CHX), as fractions of hateful keywords in each community compared to other subreddits banned for violating the hate-limiting policies of Reddit. Similarly, they quantify the exposure of users to hateful content based on the threads they have participated and to account for their stress levels they use a binary classifier based on existing models. They examine numerous observable confounders -such as the subreddit and user activity -and apply propensity score matching to calculate the causal effect whilst controlling for the covariates. Their results demonstrate an increase in stress expression caused by exposure to hateful speech.\n\nAdditionally, inherent characteristics of online environments, such as anonymity, ease of access, and size of audience (Brown, 2018) are highly likely to affect the behaviour of online social networks' users and sometimes make it easier for them to misbehave. von Essen and Jansson (2020) discuss the outcomes of the by-products of online world characteristics -in this case anonymityon hate speech actors. In particular, they compare the degree of hatefulness before and after the identities of a large set of users from Flashback, an anonymous Swedish discussion platform similar to Reddit, have been publicly exposed. Their hypothesis suggests that once running the risk of exposure, users decrease the volume of hateful content they post. To detect hate, they implement a machine learning model and make predictions on the data, which will afterwards be used with a difference-in-differences approach to make causal claims. According to their estimates, the reduction of anonymity, as in risk of exposure, leads to a decline in general hate and overall activity, and even more on xenophobic content. Surprisingly, levels of misogyny increase. These empirical findings mostly support the author's original hypothesis.", "filtered_refids": [["b22", "b20"], ["b29"], ["b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2806, "num_references": 4}
{"corpusid_sectionid": "237532535-s4", "title": "A Survey of Online Hate Speech through the Causal Lens", "date": "2021-09-16", "section_title": "Interventions", "section": "Last but not least, perhaps the most extensively studied of all three pillars is the effect of interventions, possibly due to its relatively close connection to causality. Interventions here refer to actions that are taken towards the elimination of the phenomenon; such strategies include quarantining (Chandrasekharan et al., 2020), banning (Chan-drasekharan et al., 2017;Thomas et al., 2021), censoring (\u00c1lvarez-Benjumea and Winter, 2018), sanctioning (Munger, 2017), and counter speaking (\u00c1lvarez-Benjumea and Winter, 2018). Reverberations of each policy vary, depending on the platform and the methodology followed.\n\nChandrasekharan et al. (2017) first studied the effect of banning an entire hateful community using causal inference methods. More specifically, they investigate through a quasi-experiment how banning targeted communities influenced hate speech levels on Reddit. Initially, they investigate the activity of the participants, post-banning the examined subreddits, with respect to activity level and hateful content volume. To control for potential confounders related to user characteristics -such as the activity, popularity, or age of user accounts -they employ Mahalanobis Distance Matching (Rubin et al., 2006) between treatment and control users, which is then further enhanced with a differencein-differences analysis (Abadie, 2005) of the two groups over time. Subsequently, they inspect the level of hate propagation to other invaded (as they call them) subreddits. In this case, matching needs to be applied on a subreddit level, rather than user level, so instead they employ the Interrupted Time Series approach (Bernal et al., 2017). Their results show that the ban worked for Reddit, meaning that having targeted a particular area of the platform, they have successfully eliminated hateful content without conveying the problem elsewhere within Reddit.\n\nIn addition to this work, the authors also studied the causal effect of quarantining Reddit communities, in a similar experimental framework (Chandrasekharan et al., 2020). Quarantining is a form of intervention that Reddit applies, where communities are indicated as potentially problematic and users have to deliberately choose to enter them, after being warned about toxicity levels within. This approach is less stern than banning, however according to the findings of this study, it is still effective to restrict the influx of users in hostile communities, while preserving the freedom of speech. In this case, the causal inference strategy used is the Interrupted Time Series regression (Bernal et al., 2017), which models interruptions caused by the treatment variable. ", "filtered_refids": [["b21", null, "b1", "b36"], ["b5", "b0", "b27"], ["b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2666, "num_references": 8}
{"corpusid_sectionid": "237532535-s6", "title": "A Survey of Online Hate Speech through the Causal Lens", "date": "2021-09-16", "section_title": "Confounding Bias", "section": "To establish true causation among a target variable X and an outcome Y, there should not be any indirect connections influencing the effect of X on Y. In reality it is rarely the case that this will hold organically true, hence being aware of any confounding factors is fundamental in order to intervene accordingly and eliminate their impact. Many confounders can be detected after careful exploration of the domain and quantified through unadjusted versus adjusted estimates. These are called observable confounders and are largely discovered based on domain expertise or previous research. It is well possible, however, that not all such factors can be anticipated; there are cases where they might not be discoverable or their computations might not be feasible.\n\nOne very relevant work on treating latent confounders is the one of Cheng et al. (2019), who attempt to create a somewhat causally-aware cyberbullying detection model for observed data, making it robust on confounding bias by controlling for latent (or plausible, as they call them) confounders. To achieve that, they look for pairs of variables that demonstrate Simpson's paradox and then employ a clustering algorithm to group the data based on the discovered p-confounders. They afterwards perform classification within the clusters. Their proposed framework of detecting latent confounding factors is potentially generalisable outside the scope of this topic. Nevertheless, to the best of our knowledge, this is the only research project considering cyberbullying detection from a causal prism.\n\nSuggestions While controlling for confounders, and in case they are categorical or numerical variables, it is possible to follow well established techniques such as matching (De Graaf et al., 2011). In simple terms, matching is a method to create pairs of samples from the same category and of different outcomes, to approximate randomised conditions and obliterate confounding bias so that any effect that remains must be realistic. For example, Zhang et al. (2018) perform their task while controlling for topical confounding, by creating pairs of \"good\" and \"bad\" conversations from the same Wikipedia page. This way, they ensure that any differences caused by the nature of the Wikipage topic are removed. However, it is not always possible to know the confounding factors. In that case, it is more useful to consider ways of eradicating confounding in a holistic way, such as the clustering approach of (Cheng et al., 2019).\n\nAdditionally, it is highly likely for features of this language-dominated topic to be textual. In that case, treating the covariates can be fairly challenging. Drawing inspiration from the textual causality literature (Keith et al., 2020), there are a number of approaches both for observable and latent textual confounders. For the former, it is possible to follow the strategies that will be described in Section 3.2 and extract features that best represent each specific factor. The case of latent confounders, though, is more elaborate. One suggestion proposed by Landeiro et al. (2019) is the use of adversarial learning as a combating method to confounding shift, although it has yet to be examined within the context of OHS.", "filtered_refids": [[], [], ["b42", null], ["b15", "b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3230, "num_references": 4}
{"corpusid_sectionid": "237532535-s7", "title": "A Survey of Online Hate Speech through the Causal Lens", "date": "2021-09-16", "section_title": "Linguistic Representations", "section": "Most of the body of literature described in Section 2 base their research on numerical features, regression, and time series analysis and do not look at the actual content of the social media platforms or question the validity of hatefulness. Contrary to traditional approaches to hate speech detection, which mostly work with text, these causal analyses largely overlook language. One possible explanation for this could be the inherent difficulty of constructing representative vectors that will capture the verbal essence. Language as a means of communication is intrinsically high-dimensional and reducing it in lower dimensions to allow for processing requires strenuous effort. Saha et al. (2019a) and Cheng et al. (2019) are the only previous papers, to the best of our knowledge, to exploit the texts and attempt to represent them with some feature vectors, to produce causal representations of OHS: frequency of hate keywords in the first case, and Linguistic Inquiry and Word Count (LIWC -Tausczik and Pennebaker (2010)) in the second. There is great need for experimentation and improvement to attempt more holistic approaches to the problem.\n\nSuggestions Since this task is significantly understudied in the context of online hate speech, there are a lot of possible directions to follow. For example, it might prove meaningful to extract high-level representations like sentiment features or toxicity scores. The latter is supported by von Essen and Jansson (2020), who claim that \"hate begets hate\", meaning that hateful content triggers replies of the same style. Furthermore, one could use the framework implemented by Pryzant et al. (2018), which automatically induces representative lexicons for social science tasks, while controlling for potential confounding factors. Having previously established some factors as such, it is possible to employ this framework and observe the performance of the lexicon. Lastly, it could prove fruitful to explore pre-trained language representation models that are focused on capturing profanity (e.g. HateBERT (Caselli et al., 2020)) or causality (e.g. Causal- BERT (Khetan et al., 2020;Veitch et al., 2020)), or even attempt to fine-tune a hate-and causal-specific version of the original BERT model (Devlin et al., 2018), for a combination of the two. The latter would probably be very demanding with respect to the need for training data, albeit promising.", "filtered_refids": [[null, "b29"], ["b9", null, "b24", "b38", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2415, "num_references": 7}
{"corpusid_sectionid": "254564474-s4", "title": "A Survey on Natural Language Processing for Programming", "date": "2022-12-12", "section_title": "Classification Tasks", "section": "The classification task detects whether given programs have specific characteristics, e.g., being cloned (clone detection), or being vulnerable (vulnerability identification). They are essential in protecting software from the effects of adhoc reuse (Svajlenko et al., 2014) and cyber attacks (Zhou et al., 2019). The granularity of the input ranges from a coarse-grained software repository (Hovsepyan et al., 2012) to a fine-grained function (Russell et al., 2018;Zhou et al., 2019). Despite the fact that NL does not explicitly occur in either input or output, we include tasks of such form for two reasons. First, PL has been demonstrated to contain abundant statistical properties similar to NL (Mou et al., 2016). Second, most of the ways that PL is processed are derived from NLP, like machine translation techniques (Tufano et al., 2019) in the transcription task ( \u00a7 2.5).", "filtered_refids": [["b63", "b56", "b60", "b44", "b20", "b70"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 881, "num_references": 6}
{"corpusid_sectionid": "254564474-s5", "title": "A Survey on Natural Language Processing for Programming", "date": "2022-12-12", "section_title": "Synthesis Tasks", "section": "The synthesis task generates a program given a context (which can be NL, PL, or their mixture), thus can accelerate the development process. It can be further divided into program synthesis and code completion by the formal completeness of the output. The output of program synthesis is a relatively independent unit, such as a function and a class, while the output of code completion is less restricted, ranging from tokens to code snippets.\n\nProgram synthesis is also called code generation. It is the systematic derivation of a program from a given specification (Manna and Waldinger, 1980). Conventional deductive approaches (Manna and Waldinger, 1980;Polozov and Gulwani, 2015) take logical specifications, which are logically complete but hard to write. Inductive approaches (Lieberman, 2001) list inputoutput examples as specifications, which are more accessible but incomplete. In contrast, an NL specification is sufficient to describe the logic of a program. Meanwhile, it is compatible with inputoutput examples by including them in a docstring. Therefore, it can take advantage of both the deductive and inductive approaches.\n\nCode completion is also called code suggestion in early research (Tu et al., 2014;Hindle et al., 2016). It suggests the next program token given a context and has been widely applied to IDEs . The application scenario includes the completion of method calls, keywords, variables, and arguments. With the bloom of the pre-trained models, the scenario has been extended to punctuations, statements, and even code snippets , further blurring the line between program synthesis and code completion.", "filtered_refids": [[], ["b42", "b33", "b50"], ["b62", "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1634, "num_references": 5}
{"corpusid_sectionid": "254564474-s6", "title": "A Survey on Natural Language Processing for Programming", "date": "2022-12-12", "section_title": "Transcription Tasks", "section": "The transcription task converts a given program to meet a specific requirement. Concretely, program translation aims to convert between highlevel PL (Roziere et al., 2020;Zhu et al., 2022), e.g., C# and Java. It can accelerate the update of projects written by deprecated PL, and the migration of algorithms implemented by various PLs. Code refinement aims to convert a buggy program into correct one (Wang et al., 2021). It is closely related to vulnerability identification but is required to fix the detected bugs simultaneously. The transcription task differs from the synthesis task in two aspects. First, its input program is formally complete (input program is None or a function header in program synthesis, a partial code snippet in code completion). Second, its output can be strictly aligned with the input in both the format and the content.", "filtered_refids": [["b65", "b55", "b71"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 853, "num_references": 3}
{"corpusid_sectionid": "254564474-s8", "title": "A Survey on Natural Language Processing for Programming", "date": "2022-12-12", "section_title": "General vs. Dedicated", "section": "There are two primary sources of general datasets: 1) open-source platforms such as GitHub, GitLab, and SeCold, 2) community-based spaces like Stack Overflow. The datasets are automatically collected and large in scale, thus can be applied to pretraining to ensure generated PL is grammatically correct and logically valid. However, sometimes they are noisy and non-informative. For instance, a commit message like \"update\" is of little substantial content; a code snippet answer might be irrelevant to its question (Iyer et al., 2018).\n\nStructure-based datasets are specially formatted to support particular tasks. Concrete structure information is available via open-source parsers, e.g., Tree-sitter. 1 Most functionality-oriented datasets contain a number of test cases for each sample to verify the functional correctness of synthesized programs. Therefore, the datasets are typically handcrafted (Chen et al., 2021;Austin et al., 2021) or collected from online judge websites (Iyer et al., 2018;Puri et al., 2021;Hendrycks et al., 2021;, including AIZU, AtCoder, Codeforces, Codewars, and Kattis.", "filtered_refids": [["b24"], ["b24", "b51", "b7", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1102, "num_references": 5}
{"corpusid_sectionid": "254564474-s11", "title": "A Survey on Natural Language Processing for Programming", "date": "2022-12-12", "section_title": "NL Evaluation", "section": "NL evaluation can refer to NLP and be conducted by the following two complementary approaches.\n\nAutomatic Evaluation is usually implemented by comparing the n-grams between the predicted output and given references. Concrete metric includes BLEU (Papineni et al., 2002), MENTOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004). However, limited by the number of references, they might correlate weakly with the real quality (Liu et al., 2016b). Hence, it is crucial to conduct a human evaluation simultaneously.\n\nHuman Evaluation consists of several independent dimensions, such as naturalness, diversity, and informativeness. Common annotation methods include point-wise mode (Iyer et al., 2016;Shi et al., 2021a) and pair-wise mode (Panthaplackel et al., 2020). Human evaluation is more accurate, finegrained, and comprehensive than automatic evaluation. However, it is also time-consuming and labor-intensive, and thus can only be conducted on a small subset of the test set.", "filtered_refids": [[], ["b49", "b34", "b8", "b37"], ["b48", "b57", "b23"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 979, "num_references": 7}
{"corpusid_sectionid": "254564474-s13", "title": "A Survey on Natural Language Processing for Programming", "date": "2022-12-12", "section_title": "Reference based Evaluation", "section": "Regarding a program as a sequence of tokens, PL can also be evaluated by n-gram based NL metrics, such as BLEU (Wang et al., 2021) and exact match (EM, Guo et al., 2022). To further capture the structure-based property, Ren et al. (2020) propose the CodeBLEU metric, which takes AST and data flow graph into consideration. Similar to NL, PL is expressive in that a program can be implemented differently, leading to the same weak correlation issue with a limited number of references. Actually, we can generate more than one (e.g., K) program for each sample to improve the performance. In this way, Strict Accuracy regards a sample as accepted if any of the K programs pass all test cases. Therefore, it is also called p@k in some literature. The sampling size could be huge, but the number of submissions sometimes is limited, like the competition scenario. To highlight the difference between the sampling and submission,  further propose the n@k metric, which computes the acceptance ratio when sampling k and submitting n programs.\n\nThe test case based evaluation is a remarkable progress, which has already in turn improve the training process via reinforcement learning (Le et al., 2022). Currently, the associated datasets are only available in program synthesis. Extending it to other functionality-oriented tasks is expected to gain similar improvement.", "filtered_refids": [["b13", "b65"], ["b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1363, "num_references": 3}
{"corpusid_sectionid": "254564474-s15", "title": "A Survey on Natural Language Processing for Programming", "date": "2022-12-12", "section_title": "Structure-based Understanding", "section": "Compared with NL, PL has more sophisticated structures, such as hierarchy, loops, and recursions. Generally, it would benefit the performance by explicitly representing the structures with appropriate data structure, including relative distance, abstract syntax tree, control flow graph, program dependence graph, and code property graph.\n\nRelative Distance typically refers to the distance between two tokens in the source code sequence. In this way, it can be easily combined into token representations as a feature. Ahmad et al. (2020) represent the relative distance as a learnable embedding and introduce it into transformer models by biasing the attention mechanism. Results show that the relative distance is an effective alternative to AST to capture the structure information. Based on that, Zugner et al. (2021) further extend the concept of relative distance from textual context to AST. Jointly training a model with the two types of relative distance achieves further improvement.\n\nAbstract Syntax Tree (AST) is a tree representation that carries the syntax and structure information of a program (Shi et al., 2021b). It simplifies inessential parts (e.g., parentheses) of the parse tree by implying the information in its hierarchy. Each node of AST has arbitrary number of children organized in a specific order. Therefore, a lossless representation of AST should capture the two characteristics simultaneously. Despite that, some AST can be complex with a deep hierarchy , which delays the parsing time and increases the input length (up to 70%) (Guo et al., 2022).\n\nControl Flow Graph (CFG) represents a program as a graph. Its node (also called a basic block) contains a sequence of successive statements executed together. Edges between nodes are directed, denoting the order of execution (Allen, 1970). CFG makes it convenient to locate specific syntactic structures (such as loops and conditional statements) and redundant statements.\n\nProgram Dependence Graph (PDG) is another graphical representation of a program. Nodes in PDG are statements and predicate expressions, and edges denote both data dependencies and control dependencies (Ferrante et al., 1987). The data dependencies describe the partial order between definitions and usages of variables, and have been demonstrated to be beneficial for program understanding (Krinke, 2001;Allamanis and Brockschmidt, 2017;Allamanis et al., 2018;. Similar to CFG, control dependencies also model the execution order, but it highlights a statement or a predicate itself by determining edges according to its value (Liu et al., 2020a).\n\nCode Property Graph (CPG) is a joint graph that merges AST, CFG, and PDG (Yamaguchi et al., 2014). In this way, it takes advantage of all the representations, and thus can comprehensively represent a program for structure-based tasks, such as vulnerability identification (Zhou et al., 2019) and comment summarization (Liu et al., 2020a).\n\nIn summary, a structure-based representation benefits program understanding. Among the representations, relative distance takes the most concise form but has the minimum structure information, while CPG is the other extreme. AST, CFG, and PDG are a balance between conciseness and information capacity. As a tree representation, AST can be more easily integrated by a backbone model than the graphical CFG and PDG, and thus is the most widely used structure-based representation.", "filtered_refids": [[], ["b72", "b0"], ["b13", "b58"], ["b6"], ["b10", "b4", "b25", "b38", "b3"], ["b69", "b70", "b38"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 3425, "num_references": 13}
{"corpusid_sectionid": "254564474-s18", "title": "A Survey on Natural Language Processing for Programming", "date": "2022-12-12", "section_title": "Backbone Models", "section": "Most of the functionality-oriented algorithms are model-agnostic and have little impact on the choice of a backbone model. In this section, we focus on the match between the structure-based representation (e.g., AST) and backbone models.\n\nRecurrent Neural Network (RNN, Mikolov et al., 2010) and its variant LSTM (Hochreiter and Schmidhuber, 1997) are capable of processing variable-length inputs. Therefore, it is well-suited for representing NL description (Liu et al., 2016a;Weigelt et al., 2020) and PL token sequence (Wei et al., 2019). Meanwhile, it accepts the structurebased representation formatted as a sequence, e.g., the pre-order traversal of AST. However, such transforms are lossy in that the AST cannot be recovered. To this end, Hu et al. (2018) propose a structurebased traversal (SBT) approach, adding parentheses into the sequence to mark hierarchical relationships. Distinct from SBT that adapts data to a model, Shido et al. (2019) propose a Multi-way Tree-LSTM, which directly takes input as AST. It first encodes the children of a node with a standard LSTM, and subsequently integrates the results into the node with a Tree-LSTM.\n\nConvolutional Neural Network (CNN, LeCun et al., 1989) extracts the features by scanning an input with a sliding window and applying stacked convolution and pooling operations on the window. Both two operations can be parallelized, making CNN more time-efficient than RNN. CNN in NLP4P usually takes input as execution traces (Gupta et al., 2020), input-output pairs (Bunel et al., 2018), and encodes them into an embedding as the output. Similar to Tree-LSTM,   CNN can also be adapted to the structure-based representation. For instance, Mou et al. (2016) propose a tree-based convolutional neural network (TBCNN), which encodes AST by a weight-base and positional features.\n\nTransformer (Vaswani et al., 2017) has a similar interface to RNN. The difference lies in the following two aspects. First, it is more time-efficient by solely depending on the attention mechanism, rather than the recurrent unit. Second, it can better capture long-term dependencies, which is essential for processing PL since programs can be pretty long (Ahmad et al., 2020).\n\nDespite these approaches, some studies explore the usage of feed-forward neural network (Iyer et al., 2016;Loyola et al., 2017), recursive neural network (Liang and Zhu, 2018), and graph neural network (Liu et al., 2020a). The architecture of the models, as well as RNN and CNN, can be flexibly adapted to customized data, e.g., the primitive AST. While for large-scale general data, the transformer is suggested due to its high capacity and easy access to pre-training.", "filtered_refids": [[], ["b43", "b67", "b66", "b21", null, "b19", "b59"], [null, "b9", "b44"], ["b64", "b0"], ["b38", "b23", "b40", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 2681, "num_references": 16}
{"corpusid_sectionid": "254564474-s24", "title": "A Survey on Natural Language Processing for Programming", "date": "2022-12-12", "section_title": "Multilingual NLP4P", "section": "As the bloom of the open source software platform, e.g., GitHub, source code, along with their NL descriptions, has accumulated to a considerable amount, making it possible to learn a data-driven NLP4P model. However, the distribution of these data is highly unbalanced. Most of the NL part is English, and the PL part is Java and Python. As a result, the performance of low-resource NL and PL is much worse than the average performance. For example, Ruby only takes a minor proportion in CodeSearchNet dataset and is inferior to other PL in both code search and comment generation tasks (Feng et al., 2020).\n\nTo bridge the gap between different languages, the simplest way is to translate a low-resource language into its high-resource counterpart. For tasks whose input is low-resource NL, we can translate it into English before sending it to the model. For tasks whose output is low-resource PL, we can first generate a Java program and subsequently translate it into the desired PL. However, it introduces extra effort and cascading errors during the translation. Multilingual learning approaches (Conneau et al., 2020;Liu et al., 2020b;Xue et al., 2021) provide access to address the issue. It can efficiently utilize the data presented in various languages, representing them in a unified semantic space and avoiding cascading errors.", "filtered_refids": [["b14"], [null, "b39"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1341, "num_references": 3}
{"corpusid_sectionid": "261100760-s1", "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4", "date": "2023-08-24", "section_title": "Multilingualism", "section": "Chinese Linguistic Test. SuperCLUE benchmarks (Xu et al., 2023) compared ChatGPT-like foundation models in regard to their basic language ability, professional ability, and Chinese-featured ability. By 18th May 2023, they reported that GPT-4 and ChatGPT achieved the second (76.67) and third best (66.18) results after humans (96.50), exceeding other LLMs. GPT-4 and ChatGPT had better basic language ability than the other two metrics in Chinese. Both models achieved human-like accuracy on role-playing, chit-chatting, and coding. However, their ability to understand Chinese poetry, literature, classical Chinese, and couplets was far inferior to that of humans. Huang et al. (2023) also ranked GPT-4 and ChatGPT as top-2 on a multi-discipline (52 subjects) Chinese evaluation.\n\nMultilingual NLP Tasks were examined by Lai et al. (2023), e.g., multilingual part-of-speech (PoS) tagging, NER, relation classification, NLI, QA, commonsense reasoning, and summarization. The researchers analyzed 36 languages and discovered that task-specific fine-tuned models outperformed ChatGPT in the majority of examined tasks, except PoS tagging. ChatGPT exhibited superior performance in English tasks compared to tasks in other languages; for low-and extremely low-resource languages, ChatGPT performed significantly worse than baselines. Noticeably, despite the use of non-English languages in the target tasks, ChatGPT improved its performance with English prompts. Wei et al. (2023) found that direct usage of Chat-GPT could not yield satisfying results in Chinese information extraction. Wang et al. (2023b) tested ChatGPT and GPT-4 on English-to-Chinese and English-to-German summarization, showing that although ChatGPT and GPT-4 exceeded other LLM baselines on a zero-shot setup, they fell behind a fine-tuned mBART-50 (Tang et al., 2021) on most of the examined datasets. Bang et al. (2023) argued that ChatGPT generally yielded weak performance on low-resource languages in language understanding and generation , while achieving higher profi-ciency in comprehending non-Latin scripts compared to its proficiency in generating them.", "filtered_refids": [[null], [null, "b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2133, "num_references": 3}
{"corpusid_sectionid": "261100760-s2", "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4", "date": "2023-08-24", "section_title": "Reasoning", "section": "Logical Reasoning was tested by Bang et al. (2023). They found 56 out of 60 answers correct (with appropriate prompts) for deductive reasoning, i.e. applying general rules to specific situations or cases. This was stronger than other types of reasoning. 26 out of 30 were scored for abductive reasoning, i.e. forming plausible explanations or hypotheses, based on limited evidence or incomplete information. 33 out of 60 were scored for inductive reasoning, i.e., drawing generalized conclusions from examples or specific observations. Commonsense Reasoning. Bang et al. (2023) tested ChatGPT via three commonsense datasets, showing that 80 out of 90 of ChatGPT's predictions were correct. ChatGPT was able to give good explanations of the reasoning steps to support its answer. However, Qin et al. (2023); Laskar et al. (2023) showed that the commonsense reasoning accuracy of ChatGPT was lower than fine-tuned baselines by a large margin. Davis (2023) found significant flaws in common benchmarks for common sense, including the CommonsenseQA dataset used by Bang et al. (2023), which he explicitly addressed. Davis (2023) listed several examples of commonsense and particularly physical reasoning failures that had been found shortly after the release of ChatGPT, and pointed to others. However, there does not exist a thorough assessment of the GPT models' commonsense reasoning ability. More generally, Davis (2023) pointed out that \"many important aspects of commonsense reasoning and commonsense knowledge are not tested in any existing benchmark\". Bubeck et al. (2023) probed a small number of their own real-world physical reasoning tasks with GPT-4, finding that it had good knowledge. They concluded that the model was able to learn an understanding of the real-world environment through the medium of text.\n\nCausal Reasoning. Bang et al. (2023) found that 24 out of 30 causes or effects could be correctly identified.  systematically evaluated event causality identification, causal discovery, and causal explanation generation. Compared to SOTA models, ChatGPT and GPT-4 yielded lower scores in causality identification. They outperformed baseline models on the causal discovery, although the compared models, e.g., BERT-(Devlin et al., 2019) and RoBERTa-base (Liu et al., 2019) were relatively weak. The generation of causal explanations yielded inconsistent findings in terms of AVG-BLEU and ROUGE-l metrics, while the human evaluation affirmed that both GPT models attained a level of accuracy comparable to that of human performance. K\u0131c\u0131man et al. (2023) examined ChatGPT and GPT-4 on the causal discovery, counterfactual reasoning, and actual causality inferring, finding that they outperformed other LLMs and SOTA models largely on the first two tasks.\n\nPsychological Reasoning is the ability of humans to reason about other's unobservable mental states (a.k.a Theory of Mind (ToM)). Kosinski (2023) and Moghaddam and Honey (2023) designed sets of False-Belief questions and quantified results suggested that both ChatGPT and GPT-4 had ToM ability, but that was still inferior to a human's. However, Marcus and Davis (2023) pointed out flaws in the Kosinski (2023) study because the test material was in the training data. Holterman and van Deemter (2023) further tested GPT-3 and GPT-4 on more ToM tasks summarized in Kahneman (2000). They acknowledged the potential problem of the test material being in the training data, so they substituted various nouns in the scenario. However, this is unlikely to be adequate to stop a neural model from generalising from those examples. Borji (2023) found that chatGPT failed on a variant of a classic 'Sally-Anne Test' (used to test children).  found that chatGPT answered correctly on a similar variant Sally-Anne, and they further tested on a range of more advanced ToM scenarios, with probing questions, e.g. to infer the counterfactual impact of actions on mental states. They found that GPT-4 had superior abilities and suggested that GPT-4 had a very advanced level of ToM.\n\nTask-Oriented Reasoning. Qin et al. (2023) evaluated dialogue, logical reasoning, complex yes/no QA, symbolic reasoning (last letter concatenation and coin flip), date understanding-, and tracking shuffled objects-oriented logical reasoning. However, ChatGPT underperformed finetuned baselines on most of the tasks, excluding the logical reasoning tasks. To ascertain whether Chat-GPT relies on profound comprehension of truth and logic in their reasoning or merely exploits shallow memorized patterns, Wang et al. (2023a) proposed a dialectical evaluation task, finding that despite dis-playing high confidence, ChatGPT demonstrated an inability to hold its belief in the truth in a wide range of reasoning tasks, e.g., mathematics, firstorder logic, commonsense, and generic reasoning.\n\nNatural Language Inference (NLI) aims to examine if a statement can be inferred, contradicted, or neutral, compared to another statement. Liu et al. (2023b) compared ChatGPT and GPT-4 with RoBERTa. However, both models encountered difficulties when dealing with novel and out-ofdistribution data. They yielded relatively modest performance on NLI that needed logical reasoning. Qin et al. (2023) also proved that the NLI ability of ChatGPT was lower than that of supervised models. Ambiguity is one of the difficulties of NLI. For example, whether \"John and Anna are not a couple\" contradicts \"John and Anna are married\" depends on whether \"married\" means \"both married\" or \"married to each other\". Given an ambiguous NLI premise,  asked ChatGPT and GPT-4 to either generate disambiguations of a premise with respect to the hypothesis or recognize disambiguation (i.e., deciding whether the disambiguation is an interpretation of an ambiguous premise). Their human evaluation showed that for the first task, GPT-4 achieved correctness at 32%, while, for the second task it was at the level of random guessing, suggesting that resolving tricky ambiguity remained challenging for ChatGPT.", "filtered_refids": [[null, "b10"], [null, "b10"], [null], [], ["b11", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 39, "num_chars": 6019, "num_references": 7}
{"corpusid_sectionid": "261100760-s5", "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4", "date": "2023-08-24", "section_title": "Natural Science", "section": "Physics. ChatGPT was evaluated as if it is a college student who needs to finish homework, clicker questions, programming exercises, and exams in the first-year Calculus-based Physics (Kortemeyer, 2023). Overall, ChatGPT achieved 53.05% after weighing different testing modules. This score met the minimum requirement for course credit, yet it adversely affected the overall grade-point average, falling below the necessary threshold for graduation. ChatGPT showed outstanding performance in clicker and programming questions, achieving scores higher than 90%. However, its performance in homework and exams was subpar. Additionally, ChatGPT's mathematical difficulties in the field of physics lowered its overall score.\n\nChemistry. Clark (2023) asked ChatGPT to finish two real chemistry exams with closed-and open-response questions. 44% closed-response questions were correctly answered by ChatGPT, although this is lower than the student average score (69%). Conversely, when it came to open-response questions, ChatGPT's performance was even lower than that of the least successful student.  (2023) suggested that ChatGPT achieved an impressive final diagnosis accuracy (76.9%), while its initial diagnosis accuracy was just 60.3%.", "filtered_refids": [["b17"], ["b15", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1236, "num_references": 3}
{"corpusid_sectionid": "261100760-s7", "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4", "date": "2023-08-24", "section_title": "Social Science", "section": "Education. Dahlkemper et al. (2023) aimed to investigate the extent to which students possessed accurate perception regarding the scientific accuracy and the linguistic quality of ChatGPT's responses. They provided 102 physics students with ChatGPT answers and (masked) expert answers to evaluate the scientific accuracy and linguistic quality of both parties, perceived by the students, based on three physics questions. Despite the fact that all responses generated by ChatGPT in their study were incorrect, imperfect, or misleading, the evaluation results indicated that when confronted with a difficult question, students perceived Chat-GPT's scientific accuracy to be on par with that of the expert solution, attributing this to the higher linguistic quality of ChatGPT. However, in cases where the questions were relatively easy, both the scientific accuracy and linguistic quality of the expert's answers surpassed that of ChatGPT.\n\nLaw. ChatGPT was tested with four law class exams by Choi et al. (2023), including Constitutional Law, Employee Benefits, Taxation, and Torts. ChatGPT performed better on 12 essay questions than on the 95 multiple-choice questions. Although it passed all four exams, it ranked almost the lowest among law school students in each class. Liu et al. (2023b) examined ChatGPT and GPT-4 in Law School Admission Test (LSAT) and the Chinese Civil Service Examination (CCSE). Compared with RoBERTa, the advantage of GPT-4 was much larger than that of ChatGPT. GPT-4 even exceeded the average level of a human on LSAT. Nevertheless, when compared to the human ceiling, significant disparities persisted in both models.\n\nEconomics. ChatGPT was examined with the Test of Understanding of College Economics (TUCE, Geerling et al., 2023). It demonstrated proficiency by providing accurate responses to 19/30 microeconomics questions and 26/30 macroeconomics questions. Such performance placed it in the upper echelons, ranking within the top 9% and top 1% among 3,255 and 2,789 college students who had successfully finished a full semester of microeconomics and macroeconomics. Xie et al. (2023) tested ChatGPT in stock predictions. ChatGPT showed weaker performance than traditional machine learning algorithms in numerical feature-based predictions. Through error analysis, it was determined that ChatGPT was also weak in comprehending investor sentiment expressed in text.", "filtered_refids": [["b15"], ["b11", "b18"], [null, "b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2403, "num_references": 5}
{"corpusid_sectionid": "261100760-s8", "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4", "date": "2023-08-24", "section_title": "Ethical Considerations", "section": "Fairness asks models to be fair in terms of gender, race, language, culture, and more. Seghier (2023); Yong et al. (2023) found that ChatGPT's responses were much worse in languages other than English (see \u00a7 2.2 ). Zhuo et al. (2023) assessed two types of social biases, namely gender bias and race bias. They concluded that, for text generation, although biases still existed, ChatGPT had mitigated them to a large extent compared to its predecessors, and that, for dialogue generation, ChatGPT could generate unbiased responses.\n\nRobustness requires models to maintain their performance when the inputs are different from the training data. Such inputs could be noisy data, outliers and attacks. For noisy data, Zhuo et al. assessed whether ChatGPT would suffer from SQL injection if it served as a text-to-SQL interface (Li et al., 2023a), while the answer was positive.\n\nReliability requires the generated text to be faithful. Neural text generators hallucinate (Ji et al., 2023) and, therefore, produce unfaithful texts. Toxicity asks models not to generate harmful, offensive and pornographic content. The GPT models were designed to normally refuse to generate toxic content. Nevertheless, Derner and Batisti\u010d (2023) found that through role-playing ChatGPT still produced offensive content. A quantitative study by Zhuo et al. (2023) showed that, by feeding toxic prompting, only 0.5% of the ChatGPT responses were toxic. Nonetheless, this ability was not very robust. In line with Derner and Batisti\u010d (2023), they also found that it was susceptible to prompt injections achieved by role-playing.", "filtered_refids": [["b2"], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 1603, "num_references": 3}
{"corpusid_sectionid": "261100760-s10", "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4", "date": "2023-08-24", "section_title": "Comparing GPT versus Humans", "section": "In specific NLP tasks with rich training resources, ChatGPT and GPT-4 may not perform as well as expert models (see Tables 1 and 3). There is also a certain distance from humans (see Tables 2 and 4). However, when it comes to scientific knowledge, extensive multidisciplinary testing has showcased their superiority compared to earlier models (see Table 5). Notably, in computer science and law exams, GPT-4 has achieved a level of accuracy that closely matches or even surpasses the average human performance (see Table 6). However, when we are quoted a study that shows an AI system performing at a similar level to humans, these are typically based on average performance over many cases. This average hides the fact that the AI is typically outperforming on some specialist knowledge that is difficult for humans, and underperforming on some examples that are relatively easy for humans. The same has been shown in computer vision (Russakovsky et al., 2015). Thus, human-level average accuracy achieved by AI is not equivalent to human performance and intelligence.\n\nThe GPTs may exhibit instances of hallucinations and false information (Cabrera and Neubig, 2023;Bang et al., 2023). This may be attributed to the fact that the \"next word prediction\"-based pre-training only taught GPTs \"what is right\" while neglecting \"what is wrong\". To learn to generate a correct last word, e.g., \"bird\" after the context \"if an animal has wings and can fly, it is likely a\", GPTs have to learn logic, commonsense, linguistics, and science. However, without learning \"if an animal has wings and can fly, it is a penguin\" is wrong, GPTs may struggle to distinguish penguins from other birds by flight ability and yield penguins' flying hallucinations, because GPTs likely have learned \"penguins are birds, having wings\" from corpora. In reality, incorrect examples are far more than the negations we can see from corpora. False information may exhibit in ambiguous cases if GPTs do not know the boundary between positive and negative examples. In contrast, humans learn knowledge from both right and wrong applications to sidestep obvious fallacies (NASEM, 2018).\n\nThe inference process of GPT models is also dissimilar to humans. Humans are known to have two types of reasoning: \"thinking fast\" and \"thinking slow\" (Kahneman, 2000). However, GPT models seem to only have thinking fast; they do a feedforward propagation and perform relatively fast inference . Humans in contrast can spend longer deliberating for hard questions. This requires a longer process of iterative inference, not necessarily dictated by a fixed number of iterations, but instead iterating until the system achieves a satisfactory state (van Bergen and Kriegeskorte, 2020). Nevertheless, it is clear that GPT models can do a certain type of reasoning. Especially, it has been shown that adding \"Let's think step by step\" to a prompt can allow GPT to use its own output as a sort of scratchpad to help it chain together multiple steps to arrive at a solution . In this way GPT is simulating \"thinking slow\", however it is limited to \"linear\" sequences of thought and has severe limitations in tasks that require planning (ibid.); it does not backtrack to try other possible alternatives. Such backtracking would require some sort of short-term memory or workspace to remember what has been tried and what is yet to be tried. These are well-known shortcomings of neural models (Minsky, 1991).  noted the contextdependence of GPT's mathematical knowledge; \"changes in the wording of the question can alter the knowledge that the model displays.\" The general phenomenon of sensitivity to input phrasing is well known from other language models also (Mao et al., 2022). Similarly, Lai et al. (2023) found a huge drop in commonsense knowledge when testing in languages other than English. This also explains why assessments by different papers can arrive at contradictory conclusions about its knowledge. It demonstrates that the internal representation in GPT suffers from severe entanglement (Bengio et al., 2013). While more extensive training can cause it to give correct responses in more contexts, it cannot change the fundamental fact that knowledge is not disentangled from the text contexts in which it appears; therefore there will always be the possibility that a change to the text of a question, that preserves the semantics, will elicit a factually incorrect response. Fixing this issue may require a hybrid system with separate facts. A commonsensebased neurosymbolic AI framework, such as the one proposed by Cambria et al. (2022) for sentiment analysis, moreover, can help increase the explainability of the reasoning processes required for decision-making, which is crucial for sensitive applications involving ethics, privacy and health.", "filtered_refids": [[], [null, "b10"], [null, "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 33, "num_chars": 4816, "num_references": 4}
{"corpusid_sectionid": "261100760-s11", "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4", "date": "2023-08-24", "section_title": "Evaluation", "section": "Concerns about the validity of NLP evaluations in general (Belz et al., 2023) are applicable to this paper also. The following points are worth noting: 1) ChatGPT as a commercial product is updated periodically. On one hand, this means the \"flaws\" identified in early studies are fixed in later versions and different studies about the same task are not fully comparable. On the other hand, since the repair may be done by including datasets from previous assessments in fine-tuning, the data leakage may make the assessment unfair (Min et al., 2022;OpenAI, 2023a). This has been well-evidenced in Aiyappa et al. (2023) andde Wynter et al. (2023).\n\n2) The design of prompts highly influences the results, leading to biased comparisons. Take MT as an example, by seeking better prompts, Hendy et al. (2023) and Jiao et al. (2023) had very different conclusions compared to Bang et al. (2023). In future studies, it is necessary to ensure transparency in prompt design and facilitate a fair comparison between LLMs and baselines.\n\n3) The factors that matter in previous NLP evaluations are still valid, which may include the choice of evaluation corpora and metrics, the design of human evaluations, the task formulation and so on. For instance, Mart\u00ednez (2023) argued that OpenAI's assessment of GPT-4 (OpenAI, 2023a) on the Uniform Bar Exam is misleading because they incorrectly included testtakers who re-took the exam in their comparison. Zhang et al. (2023b) misconcluded that GPT-4 had solved all MIT math and computer science curricula because they improperly used GPT-4 as the judge. 4) The assessments of some key abilities, e.g., creativity and logical reasoning, lack either objective criteria or large-scale benchmarks. Consequently, the evaluation can only capture a portion of the overall capabilities of GPT. People seem to commonly believe that AI will cause mass unemployment (Hatzius et al., 2023). However, in these fields, the evaluation is also very limited. It would be expected to see what kind of leading results the GPT models achieved compared with humans in the field of work that can be replaced by AI.", "filtered_refids": [[null], [null, "b10"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2129, "num_references": 4}
{"corpusid_sectionid": "41310809-s8", "title": "Trends in HLT Research: A Survey of LDC's Data Scholarship Program", "date": "2016-05-01", "section_title": "Successes", "section": "Despite the challenges involved in the LDC Data Scholarship program, we believe it makes a valuable contribution to multiple fields. A goal for the program has been to aid new developments in language-related research and technology, and based on feedback from award recipients, we think that goal has been met. 6 For instance, most reported that they used the data as they intended and received the results they expected. Three students have graduated from their programs and two more expect to graduate in 2016. There have been at least six published papers based on data received in the program. (E.g., Guven, 2012;Harrat, et al., 2013;George, et al., 2015). Most awardees described the data they received as vital to their work. In one case, data awarded through the program was used to build a state-of-the-art speaker recognition system (AMRITATCS) that the awardee and his colleagues submitted to The Speakers in the Wild (SITW) Speaker Recognition Challenge hosted by SRI International. 7 There were some who found that they could not use the data, for instance, because they expected it to contain something it did not, the data set was too small, or their dissertation topic changed. Overall, however, as indicated above, students report positive experiences in the program. We think that the data scholarship program has helped potential new entrants to the field by giving them the experience of working with data as they will be expected to in their careers. It is clear to us, however, that metric-driven evaluation has not spread throughout the field. This is something we as a community should consider addressing to ensure that new entrants are prepared to make meaningful contributions and further progress.", "filtered_refids": [[null, "b1", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1725, "num_references": 3}
{"corpusid_sectionid": "233476148-s3", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Tasks", "section": "Most papers in Table 1 focus on text classification with single input (TC) for a variety of specific problems such as e-mail categorization , topic classification (Kulesza et al., 2015;Teso and Kersting, 2019), spam classification (Koh and Liang, 2017), sentiment analysis (Ribeiro et al., 2018b) and auto-coding of transcripts (Kulesza et al., 2010). By contrast, Zylberajch et al. (2021) targeted natural language inference (NLI) which is a type of text-pair classification, predicting whether a given premise entails a given hypothesis. . Ghai et al. (2021) suggested that most researchers work on TC because, for this task, it is much easier for lay participants to understand explanations and give feedback (e.g., which keywords should be added or removed from the list of top features) 4 . Meanwhile, some other NLP tasks require the feedback providers to have linguistic knowledge such as part-of-speech tagging, parsing, and machine translation. The need for linguists or experts renders experiments for these tasks more difficult and costly. However, we suggest that there are several tasks where the trained models are prone to be buggy but the tasks are underexplored in the EBHD setting, though they are not too difficult to experiment on with lay people. NLI, the focus of (Zylberajch et al., 2021), is one of them. Indeed, McCoy et al. (2019) and Gururangan et al. (2018) showed that NLI models can exploit annotation artifacts and fallible syntactic heuristics to make predictions rather than learning the logic of the actual task. Other tasks and their bugs include: QA, where Ribeiro et al. (2019) found that the answers from models are sometimes inconsistent (i.e., contradicting previous answers); and reading comprehension, where Jia and Liang (2017) showed that models, which answer a question by reading a given paragraph, can be fooled by an irrelevant sentence being appended to the paragraph. These non-TC NLP tasks would be worth exploring further in the EBHD setting.", "filtered_refids": [["b67", "b77", "b22", "b23", null, "b48", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1994, "num_references": 7}
{"corpusid_sectionid": "233476148-s5", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Bug Sources", "section": "Most of the papers in Table 1 experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability. Out of the 15 papers we surveyed, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study, since it has lots of natural artifacts. For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions. However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild. Apart from classification performance drops, natural artifacts can also cause model biases, as shown in (De-Arteaga et al., 2019;Park et al., 2018) and debugged in (Lertvittayakumjorn et al., 2020;Yao et al., 2021).\n\nIn the absence of strong natural artifacts, bugs can still be simulated using several techniques. First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance (Kulesza et al., 2010). Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017). Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020;Yao et al., 2021). All of these techniques give rise to undesirable model behaviors, requiring debugging. Another technique, not found in Table 1 but suggested in related work (Idahl et al., 2021), is contaminating input texts in the training data with decoys (i.e., injected artifacts) which could deceive the model into predicting for the wrong reasons. This has been experimented with in the computer vision domain (Rieger et al., 2020), and its use in the EBHD setting in NLP could be an interesting direction to explore.", "filtered_refids": [["b43", "b28", "b30", null, "b72"], ["b30", null, "b19", "b23", "b72", "b53"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2172, "num_references": 11}
{"corpusid_sectionid": "233476148-s7", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Providing Explanations", "section": "The main role of explanations here is to provide interpretable insights into the model and uncover its potential misbehavior or irrationality, which sometimes cannot be noticed by looking at the model outputs or the evaluation metrics.\n\nExplanation scopes. Basically, there are two main types of explanations that could be provided to feedback providers. Local explanations (L) explain the predictions by the model for individual inputs. In contrast, global explanations (G) explain the model overall, independently of any specific inputs. It can be seen from Table 1 that most existing work use local explanations. One reason for this may be that, for complex models, global explanations can hardly reveal details of the models' inner workings in a comprehensible way to users. So, some bugs are imperceptible in such highlevel global explanations and then not corrected by the users. For example, the debugging framework FIND, proposed by Lertvittayakumjorn et al. (2020), uses only global explanations, and it was shown to work more effectively on significant bugs (such as gender bias in abusive language detection) than on less-obvious bugs (such as dataset shift between product types of sentiment analysis on product reviews). Otherwise, Ribeiro et al. (2018b) presented adversarial replacement rules as global explanations to reveal the model weaknesses only, without explaining how the whole model worked.\n\nOn the other hand, using local explanations has limitations in that it demands a large amount of effort from feedback providers to inspect the explanation of every single example in the training/validation set. With limited human resources, efficient ways to rank or select examples to explain would be required (Idahl et al., 2021). Recently, some work in explainable AI considers generating explanations for a group of predictions (Johnson et al., 2020; Chan et al., 2020) (e.g., for all the false positives of a certain class), thus staying in the middle of the two extreme explanation types (i.e., local and global). This kind of explanation is not too fine-grained, yet it can capture some suspicious model behaviors if we target the right group of examples. So, it would be worth studying in the context of EBHD (to the best of our knowledge, no existing study experiments with it).\n\nGenerating explanations. To generate explanations in general, there are two important questions we need to answer. First, which format should the explanations have? Second, how do we generate the explanations?\n\nFor the first question, we see many possible answers in the literature of explainable NLP (e.g., see the survey by Danilevsky et al. (2020)). For instance, input-based explanations (so called feature importance explanations) identify parts of the input that are important for the prediction. The explanation could be a list of importance scores of words in the input, so called attribution scores or relevance scores (Lundberg and Lee, 2017; Arras et al., 2016). Example-based explanations select influential, important, or similar examples from the training set to explain why the model makes a specific prediction (Han et al., 2020;Guo et al., 2020). Rule-based explanations provide interpretable decision rules that approximate the prediction process (Ribeiro et al., 2018a). Adversarial-based explanations return the smallest changes in the inputs that could change the predictions, revealing the model misbehavior (Zhang et al., 2020a). In most NLP tasks, inputbased explanations are the most popular approach for explaining predictions (Bhatt et al., 2020). This is also the case for EBHD as most selected studies use input-based explanations (Kulesza et al., 2009(Kulesza et al., , 2010Teso and Kersting, 2019;Cho et al. For the second question, there are two ways to generate the explanations: self-explaining methods and post-hoc explanation methods. Some models, e.g., Naive Bayes, logistic regression, and decision trees, are self-explaining (SE) (Danilevsky et al., 2020), also referred to as transparent (Adadi and Berrada, 2018) or inherently interpretable (Rudin, 2019). Local explanations of self-explaining models can be obtained at the same time as predictions, usually from the process of making those predictions, while the models themselves can often serve directly as global explanations. For example, feature importance explanations for a Naive Bayes model can be directly derived from the likelihood terms in the Naive Bayes equation, as done by several papers in Table 1 (Kulesza et al., 2009;Smith-Renner et al., 2020). Also, using attention scores on input as explanations, as done in (Cho et al., 2019), is a self-explaining method because the scores were obtained during the prediction process.\n\nIn contrast, post-hoc explanation methods (PH) perform additional steps to extract explanations after the model is trained (for a global explanation) or after the prediction is made (for a local explanation). If the method is allowed to access model parameters, it may calculate word relevance scores by propagating the output scores back to the input words (Arras et al., 2016) or analyzing the derivative of the output with respect to the input words (Smilkov et al., 2017;Sundararajan et al., 2017). If the method cannot access the model parameters, it may perturb the input and see how the output changes to estimate the importance of the altered parts of the input (Ribeiro et al., 2016;Jin et al., 2020). The important words and/or the relevance scores can be presented to the feedback providers in the EBHD workflow in many forms such as a list of words and their scores (Teso and Kersting, 2019;Ribeiro et al., 2016), word clouds (Lertvittayakumjorn et al., 2020), and a parse tree (Yao et al., 2021). Meanwhile, the influence functions method, used in (Koh and Liang, 2017;Zylberajch et al., 2021), identifies training examples which influence the prediction by analyzing how the prediction would change if we did not have each training point. This is another post-hoc explanation method as it takes place after prediction. It is similar to the other two example-based explanation methods used in (Khanna et al., 2019;Han and Ghosh, 2020).\n\nPresenting explanations. It is important to carefully design the presentation of explanations, taking into consideration the background knowledge, desires, and limits of the feedback providers. In the debugging application by Kulesza et al. (2009), lay users were asked to provide feedback to email categorizations predicted by the system. The users were allowed to ask several Why questions (inspired by Myers et al. (2006)) through either the menu bar, or by right-clicking on the object of interest (such as a particular word). Examples include \"Why will this message be filed to folder A?\", \"Why does word x matter to folder B?\". The system then responded by textual explanations (generated using templates), together with visual explanations such as bar plots for some types of questions. All of these made the interface become more user-friendly. In 2015, Kulesza et al. proposed, as desirable principles, that the presented explanations should be sound (i.e., truthful in describing the underlying model), complete (i.e., not omitting important information about the model), but not overwhelming (i.e., remaining comprehensible). However, these principles are challenging especially when working on non-interpretable complex models.", "filtered_refids": [[], ["b30", "b51"], [null], [], ["b67", "b50", "b25", "b55", null, "b23", "b62", "b24", "b74", "b1", "b8", "b5"], ["b67", "b49", "b30", null, "b19", "b77", "b72", "b16", "b65", "b5"], ["b24", null, "b40"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 44, "num_chars": 7431, "num_references": 28}
{"corpusid_sectionid": "233476148-s8", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Collecting Feedback", "section": "After seeing explanations, humans generally desire to improve the model by giving feedback (Smith-Renner et al., 2020). Some existing work asked humans to confirm or correct machine-computed explanations. Hence, the form of feedback fairly depends on the form of the explanations, and in turn this shapes how to update the model too (discussed in section 2.2.3). For text classification, most EBHD papers asked humans to decide which words (WO) in the explanation (considered important by the model) are in fact relevant or irrelevant (Kulesza et al., 2010;Ribeiro et al., 2016;Teso and Kersting, 2019). Some papers even allowed humans to adjust the word importance scores (WS) (Kulesza et al., 2009(Kulesza et al., , 2015. This is analogous to specifying relevancy scores for example-based explanations (ES) in (Zylberajch et al., 2021). Meanwhile, feedback at the level of learned features (FE) (i.e., the internal neurons in the model) and learned rules (RU) rather than individual words, was asked in (Lertvittayakumjorn et al., 2020) and (Ribeiro et al., 2018b), respectively. Additionally, humans may be asked to check the predicted labels (Kulesza et al., 2009;Smith-Renner et al., 2020) or even the ground truth labels (collectively noted as LB in Ta It is likely that identifying important parts in the input is sufficient to make the model accomplish simple text classification tasks. However, this might not be enough for complex tasks which require reasoning. Recently, Yao et al. (2021) asked humans to provide, as feedback, compositional explanations to show how the humans would reason (RE) about the models' failure cases. An example of the feedback for a hate speech detection is \"Because X is the word dumb, Y is a hateful word, and X is directly before Y , the attribution scores of both X and Y as well as the interaction score between X and Y should be increased\". To acquire richer information like this as feedback, their framework requires more expertise from the feedback providers. In the future, it would be interesting to explore how we can collect and utilize other forms of feedback, e.g., natural language feedback (Camburu et al., 2018), new training examples (Fiebrink et al., 2009), and other forms of decision rules used by humans (Carstens and Toni, 2017).", "filtered_refids": [["b11", "b12", "b67", "b49", "b30", "b77", "b22", "b23", "b62", null, "b24", "b72", "b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2292, "num_references": 13}
{"corpusid_sectionid": "233476148-s9", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Updating the Model", "section": "Techniques to incorporate human feedback into the model can be categorized into three approaches.\n\n(1) Directly adjust the model parameters (M). When the model is transparent and the explanation displays the model parameters in an intelligible way, humans can directly adjust the parameters based on their judgements. This idea was adopted by Kulesza et al. (2009Kulesza et al. ( , 2015 where humans can adjust a bar chart showing word importance scores, corresponding to the parameters of the underlying Naive Bayes model. In this special case, steps 2 and 3 in Figure 1 are combined into a single step. Besides, human feedback can be used to modify the model parameters indirectly. For example, Smith-Renner et al. (2020) increased a word weight in the Naive Bayes model by 20% for the class that the word supported, according to human feedback, and reduced the weight by 20% for the opposite class (binary classification). This choice gives good results, however, it is not clear why and whether 20% is the best choice here.\n\nOverall, this approach is fast because it does not require model retraining. However, it is important to ensure that the adjustments made by humans generalize well to all examples. Therefore, the system should update the overall results (e.g., performance metrics, predictions, and explanations) in real time after applying any adjustment, so the humans can investigate the effects and further adjust the model parameters (or undo the adjustments) if necessary. This agrees with the correctability principles proposed by Kulesza et al. (2015) that the system should be actionable and reversible, honor user feedback, and show incremental changes.\n\n(2) Improve the training data (D). We can use human feedback to improve the training data and retrain the model to fix bugs. This approach includes  Zylberajch et al., 2021). As this approach modifies the training data only, it is applicable to any model regardless of the model complexity.\n\n(3) Influence the training process (T). Another approach is to influence the (re-)training process in a way that the resulting model will behave as the feedback suggests. This approach could be either model-specific (such as attention supervision) or model-agnostic (such as user co-training). Cho et al. (2019) used human feedback to supervise attention weights of the model. Similarly, Yao et al. (2021) added a loss term to regularize explanations guided by human feedback.  proposed (i) constraint optimization, translating human feedback into constraints governing the training process and (ii) user co-training, using feedback as another classifier working together with the main ML model in a semi-supervised learning setting. Lertvittayakumjorn et al. (2020) disabled some learned features deemed irrelevant, based on the feedback, and re-trained the model, forcing it to use only the remaining features. With many techniques available, however, there has not been a study testing which technique is more appropriate for which task, domain, or model architecture. The comparison issue is one of the open problems for EBHD research (to be discussed in section 4).", "filtered_refids": [[], ["b24", "b22"], ["b22"], ["b77"], ["b72", "b30"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3139, "num_references": 6}
{"corpusid_sectionid": "233476148-s11", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Experimental Setting", "section": "To conduct experiments, some studies in Table 1 selected human participants (SP) to be their feedback providers. The selected participants could be people without ML/NLP knowledge (Kulesza et al., 2010(Kulesza et al., , 2015 or with ML/NLP knowledge (Ribeiro et al., 2018b;Zylberajch et al., 2021) depending on the study objectives and the complexity of the feedback process. Early work even conducted experiments with the participants in-person Kulesza et al., 2009Kulesza et al., , 2015. Although this limited the number of participants (to less than 100), the researchers could closely observe their behaviors and gain some insights concerning humancomputer interaction.\n\nBy contrast, some used a crowdsourcing platform, Amazon Mechanical Turk 5 in particular, to collect human feedback for debugging the models. Crowdsourcing (CS) enables researchers to conduct experiments at a large scale; however, the quality of human responses could be varying. So, it is important to ensure some quality control such as specifying required qualifications (Smith-Renner et al., 2020), using multiple annotations per question (Lertvittayakumjorn et al., 2020), having a training phase for participants, and setting up some obvious questions to check if the participants are paying attention to the tasks (Egelman et al., 2014).\n\nFinally, simulation (SM), without real humans involved but using oracles as human feedback instead, has also been considered (for the purpose of testing the EBHD framework only). For example, Teso and Kersting (2019) set 20% of input words as relevant using feature selection. These were used to respond to post-hoc explanations, i.e., top k words selected by LIME. Koh and Liang (2017) simulated mislabeled examples by flipping the labels of a random 10% of the training data. So, when the explanation showed suspicious training examples, the true labels could be used to provide feedback. Compared to the other settings, simulation is faster and cheaper, yet its results may not reflect the effectiveness of the framework when deployed with real humans. Naturally, human feedback is sometimes inaccurate and noisy, and humans could also be interrupted or frustrated while providing feedback (Amershi et al., 2014). These factors, discussed in detail in the next section, cannot be thoroughly studied in only simulated experiments.", "filtered_refids": [["b77", "b22", "b23", "b24", "b51"], ["b30", null], ["b67", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2352, "num_references": 9}
{"corpusid_sectionid": "233476148-s14", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Willingness", "section": "We would like humans to provide feedback for improving models, but do humans naturally want to? Prior to the emerging of EBHD, studies found that humans are not willing to be constantly asked about labels of examples as if they were just simple oracles (Cakmak et al., 2010;Guillory and Bilmes, 2011). Rather, they want to provide more than just data labels after being given explanations (Amershi et al., 2014;Smith-Renner et al., 2020). By collecting free-form feedback from users, Ghai et al. (2021) discovered various feedback types. The most prominent ones include removing-adding features (words), tuning weights, and leveraging feature combinations.  further analyzed categories of background knowledge underlying the feedback and found, in their experiment, that it was mainly based on commonsense knowledge and English language knowledge. Such knowledge may not be efficiently injected into the model if we exploit human feedback which contains only labels. This agrees with some participants, in (Smith-Renner et al., 2020), who described their feedback as inadequate when they could only confirm or correct predicted labels.\n\nAlthough human feedback beyond labels contains helpful information, it is naturally neither complete nor precise. Ghai et al. (2021) observed that human feedback usually focuses on a few features that are most different from human expectation, ignoring the others. Also, they found that humans, especially lay people, are not good at correcting model explanations quantitatively (e.g., adjusting weights). This is consistent with the findings of Miller (2019) that human explanations are selective (in a biased way) and rarely refer to probabilities but express causal relationships instead.", "filtered_refids": [[null, "b10", "b62", "b3"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1728, "num_references": 5}
{"corpusid_sectionid": "233476148-s15", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Trust", "section": "Trust (as well as frustration and expectation, discussed next) is an important issue when the system end users are feedback providers in the EBHD framework. It has been discussed widely that explanations engender human trust in AI systems (Pu and Chen, 2006;Lipton, 2018;Toreini et al., 2020). This trust may be misplaced at times. Showing more detailed explanations can cause users to over rely on the system, leading to misuse where users agree with incorrect system predictions (Stumpf et al., 2016). Moreover, some users may over trust the explanations (without fully understanding them) only because the tools generating them are publicly available, widely used, and showing appealing visualizations (Kaur et al., 2020).\n\nHowever, recent research reported that explanations do not necessarily increase trust and reliance. Cheng et al. (2019) found that, even though explanations help users comprehend systems, they cannot increase human trust in using the systems in high-stakes applications involving lots of qualitative factors, such as graduate school admissions. Smith-Renner et al. (2020) reported that explanations of low-quality models decrease trust and system acceptance as they reveal model weaknesses to the users. According to Schramowski et al. (2020), despite correct predictions, the trust still drops if the users see from the explanations that the model relies on the wrong reasons. These studies go along with a perspective by Zhang et al. (2020b) that explanations should help calibrate user perceptions to the model quality, signaling whether the users should trust or distrust the AI. Although, in some cases, explanations successfully warned users of faulty models (Ribeiro et al., 2016), this is not easy when the model flaws are not obvious (Zhang et al., 2020b;Lertvittayakumjorn and Toni, 2019).\n\nBesides explanations, the effect of feedback on human trust is quite inconclusive according to some (but fewer) studies. On one hand, Smith-Renner et al. (2020) found that, after lay humans see explanations of low-quality models and lose their trust, the ability to provide feedback makes human trust and acceptance rally, remedying the situation. In contrast, Honeycutt et al. (2020) reported that providing feedback decreases human trust in the system as well as their perception of system accuracy no matter whether the system truly improves after being updated or not.", "filtered_refids": [[null, "b33", "b63", "b46"], ["b15", "b58", "b31", "b49", "b62", "b75"], [null, "b62"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2400, "num_references": 12}
{"corpusid_sectionid": "233476148-s16", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Frustration", "section": "Working with explanations can cause frustration sometimes. Following the discussion on trust, explanations of poor models increase user frustration (as they reveal model flaws), whereas the ability to provide feedback reduces frustration. Hence, in general situations, the most frustrating condition is showing explanations to the users without allowing them to give feedback (Smith-Renner et al., 2020).\n\nAnother cause of frustration is the risk of detailed explanations overloading users (Narayanan et al., 2018). This is especially a crucial issue for inherently interpretable models where all the internal workings can be exposed to the users. Though presenting all the details is comprehensive and faithful, it could create barriers for lay users (Gershon, 1998). In fact, even ML experts may feel frustrated if they need to understand a decision tree with a depth of ten or more. Poursabzi-Sangdeh et al. (2018) found that showing all the model internals undermined users' ability to detect flaws in the model, likely due to information overload. So, they suggested that model internals should be revealed only when the users request to see them.", "filtered_refids": [["b62"], ["b45", null, "b41"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1152, "num_references": 4}
{"corpusid_sectionid": "233476148-s20", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Beyond English Text Classification", "section": "All papers in Table 1 conducted experiments only on English datasets. We acknowledge that qualitatively analyzing explanations and feedback in languages at which one is not fluent is not easy, not to mention recruiting human subjects who know the languages. However, we hope that, with more multilingual data publicly available (Wolf et al., 2020) and growing awareness in the NLP community (Bender, 2019), there will be more EBHD studies targeting other languages in the near future.\n\nAlso, most existing EBHD works target text classifiers. It would be interesting to conduct more EBHD work for other NLP tasks such as reading comprehension, question answering, and natural language inference (NLI), to see whether existing techniques still work effectively. Shifting to other tasks requires an understanding of specific bug characteristics in those tasks. For instance, unlike bugs in text classification which are usually due to word artifacts, bugs in NLI concern syntactic heuristics between premises and hypotheses (Mc-Coy et al., 2019). Thus, giving human feedback at word level may not be helpful, and more advanced methods may be needed. Lakkaraju et al. (2020) remarked that the evaluation setup of existing EBHD work is often too easy or unrealistic. For example, bugs are obvious artifacts which could be removed using simple text pre-processing (e.g., removing punctuation and redacting named entities). Hence, it is not clear how powerful such EBHD frameworks are when dealing with real-world bugs. If bugs are not dominant and happen less often, global explanations may be too coarse-grained to capture them while many local explanations may be needed to spot a few appearances of the bugs, leading to inefficiency. As reported by Smith-Renner et al. (2020), feedback results in minor improvements when the model is already reasonably good.", "filtered_refids": [["b70", "b7"], [null, "b62", "b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1855, "num_references": 5}
{"corpusid_sectionid": "233476148-s21", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Tackling More Challenging Bugs", "section": "Other open problems, whose solutions may help deal with challenging bugs, include the following. First, different people may give different feedback for the same explanation. As raised by Ghai et al. (2021), how can we integrate their feedback to get robust signals for model update? How should we deal with conflicts among feedback and training examples (Carstens and Toni, 2017)? Second, confirming or removing what the model has learned is easier than injecting, into the model, new knowledge (which may not even be apparent in the explanations). How can we use human feedback to inject new knowledge, especially when the model is not transparent? Lastly, EBHD techniques have been proposed for tabular data and image data Ghai et al., 2021;Popordanoska et al., 2020). Can we adapt or transfer them across modalities to deal with NLP tasks?", "filtered_refids": [["b12", null, "b44"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 843, "num_references": 3}
{"corpusid_sectionid": "233476148-s22", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Analyzing and Enhancing Efficiency", "section": "Most selected studies focus on improving correctness of the model (e.g., by expecting a higher F1 or a lower bias after debugging). However, only some of them discuss efficiency of the proposed frameworks. In general, we can analyze the efficiency of an EBHD framework by looking at the efficiency of each main step in Figure 1. Step 1 generates the explanations, so its efficiency depends on the explanation method used and, in the case of local explanation methods, the number of local explanations needed.\n\nStep 2 lets humans give feedback, so its efficiency concerns the amount of time they spend to understand the explanations and to produce the feedback.\n\nStep 3 updates the model using the feedback, so its efficiency relates to the time used for processing the feedback and retraining the model (if needed). Existing work mainly reported efficiency of step 1 or step 2. For instance, approaches using example-based explanations measured the improved performance with respect to the number of explanations computed (step 1) (Koh and Liang, 2017;Khanna et al., 2019;Han and Ghosh, 2020). Kulesza et al. (2015) compared the improved F1 of EBHD with the F1 of instance labelling given the same amount of time for humans to perform the task (step 2). Conversely, Yao et al. (2021) compared the time humans need to do EBHD versus instance labelling in order to achieve the equivalent degree of correctness improvement (step 2).\n\nNone of the selected studies considered the efficiency of the three steps altogether. In fact, the efficiency of step 1 and 3 is important especially for black box models where the cost of post-hoc explanation generation and model retraining is not negligible. It is even more crucial for iterative or responsive EBHD. Thus, analyzing and enhancing efficiency of EBHD frameworks (for both machine and human sides) require further research.", "filtered_refids": [[], [], [null, "b19", "b22", "b72", "b16"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1870, "num_references": 5}
{"corpusid_sectionid": "16442276-s1", "title": "Content Models for Survey Generation: A Factoid-Based Evaluation", "date": 2015, "section_title": "Data", "section": "Prior research in automatic survey generation has explored using text from different parts of scientific papers. Some of the recent work has treated survey generation as a direct extension of single paper summarization (Qazvinian and Radev, 2008) and used citing sentences to a set of relevant papers as the input for the summarizer (Mohammad et al., 2009;Qazvinian et al., 2013). However, in our prior work, we have observed that it's difficult to generate coherent and readable summaries using just citing sentences and have proposed the use of sentences from introductory texts of papers that cite a number of important papers on a topic (Jha et al., 2015). The use of full text allows for the use of discourse structure of these documents in framing coherent and readable surveys. Since the content models we explore are meant to be part of a larger system that should be able to generate coherent and readable survey articles, we use the introduction sentences for our experiments as well.\n\nThe corpus we used for extracting our experimental data was the ACL Anthology Network, a comprehensive bibliographic dataset that contains full text and citations for papers in most of the important venues in natural language processing . An oracle method is used for selecting the initial set of papers for each topic. For each topic, the bibliographies of at least three human-written surveys were extracted, and any papers that appeared in more than one survey were added to the target document set for the topic.\n\nThe text for summarization is extracted from introductory sections of papers that cite papers in the target document set. The intuition behind this is that the introductory sections of papers that cite these target document summarize the research in papers from the target document set as well as the relationships between these papers. Thus, these introductions can be thought of as mini-surveys for specific aspects of the topic; combining text from these introductory sections should allow us to generate good comprehensive survey articles for the topic 1 . For our experiments, we sort the citing papers based on the number of papers they cite Input sentence Factoids According to [1] , the corpus based supervised machine learning methods are the most successful approaches to WSD where contextual features have been used mainly to distinguish ambiguous words in these methods. supervised wsd, corpus based wsd Compared with supervised methods, unsupervised methods do not require tagged corpus, but the precision is usually lower than that of the supervised methods.\n\nsupervised wsd, unsupervised wsd Word sense disambiguation (WSD) has been a hot topic in natural language processing, which is to determine the sense of an ambiguous word in a specific context. definition of word sense disambiguation Improvement in the accuracy of identifying the correct word sense will result in better machine translation systems, information retrieval systems, etc.\n\nwsd for machine translation, wsd for information retrieval The SENSEVAL evaluation framework ( Kilgarriff 1998 ) was a DARPA-style competition designed to bring some conformity to the field of WSD, although it has yet to achieve that aim completely. senseval Table 3: Sample input sentences from the topic of word sense disambiguation annotated with factoids.\n\nin the target document set, pick the top 20 papers, and extract sentences from their introductions to form the input text for the summarizer. The seven topics used in our experiments and input size for each topic are shown in Table 2.\n\nOnce the input text for each topic has been extracted, we annotate the sentences in the input text with factoids for that topic. Some annotated sentences in the topic of word sense disambiguation are shown in Table 3. Given this new annotated data, we can compare how the factoids are distributed across different citing sentences (as annotated by Jha et al. (2013)) and introduction sentences that we have annotated. For this, we divide the factoids into five categories: definitions, venue, resources, methodology, and applications. The fractional distribution of factoids in these categories is shown in Table 4. We can see that the distribution of factoids relating to venues, methodology and applications is similar for the two datasets. However, factoids related to definitional sentences are almost completely missing in the citing sentences data. This lack of background information in citing sentences is one of the motivations for using introduction sentences for survey article generation as opposed to previous work.\n\nThe complete set of factoids as well as annotated sentences for all the topics is available for download at http: //clair.si.umich.edu/corpora/ Surveyor_CM_Data.tar.gz.", "filtered_refids": [["b21", "b16", "b22", "b9"], [], [], [], [], [], ["b8"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4771, "num_references": 5}
{"corpusid_sectionid": "225062337-s4", "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios", "date": "2020-10-23", "section_title": "How Low is Low-Resource?", "section": "On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.\n\n(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.\n\nGiven the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.", "filtered_refids": [[], ["b13", "b0"], [null, "b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1450, "num_references": 4}
{"corpusid_sectionid": "225062337-s6", "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios", "date": "2020-10-23", "section_title": "Data Augmentation", "section": "New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019). Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018).\n\nTo go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (\u015eahin and Steedman, 2018;Vania et al., 2019;Dehouck and G\u00f3mez-Rodr\u00edguez, 2020), simplification of sentences by removal of sentence parts (\u015eahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020). For whole sentences, paraphrasing through backtranslation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018). An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019). Back-translation has also been leveraged for text classification (Xie et al., 2020;Hegde and Patil, 2020). This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020). It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. Ding et al. (2020) extend this idea for token level tasks.\n\nAdversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020). They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020). Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score). This approach is often applied on the level of vector representations. For instance, Grundkiewicz et al. (2019) reverse the augmentation setting by applying transformations that flip the (binary) label. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.\n\nOpen Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing. A reason might be that several of the approaches require an indepth understanding of the language. There is not yet a unified framework that allows applying data augmentation across tasks and languages. Recently, Longpre et al. (2020) hypothesised that data augmentation provides the same benefits as pretraining in transformer models. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited.", "filtered_refids": [["b49", null, "b46", "b18"], ["b55", null, "b9", "b46"], [null, "b60"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3788, "num_references": 11}
{"corpusid_sectionid": "225062337-s7", "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios", "date": "2020-10-23", "section_title": "Distant & Weak Supervision", "section": "In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).\n\nWhile distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with \"It was great/bad\" for obtaining binary sentiment labels.\n\nOpen Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.\n\nDistant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.\n\nWhile distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).", "filtered_refids": [["b67", "b20", null, "b22", "b23", "b1", "b38", "b61", "b2"], [null, "b5"], ["b54", null], [null], ["b14", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4273, "num_references": 16}
{"corpusid_sectionid": "225062337-s8", "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios", "date": "2020-10-23", "section_title": "Cross-Lingual Annotation Projections", "section": "For cross-lingual projections, a task-specific classifier is trained in a high-resource language. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (T\u00e4ckstr\u00f6m et al., 2013;Wisniewski et al., 2014;Plank and Agi\u0107, 2018;Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014;Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agi\u0107 and Vuli\u0107, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019;Zhang et al., 2019a;Fei et al., 2020;Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015;Marasovi\u0107 et al., 2016;Friedrich and Gateva, 2017).\n\nOpen issues: Cross-lingual projections set high requirements on the auxiliary data needing both labels in a high-resource language and means to project them into a low-resource language. Especially the latter can be an issue as machine translation by itself might be problematic for a specific low-resource language. A limitation of the parallel corpora is their domains like political proceedings or religious texts. Mayhew et al. (2017), Fang and Cohn (2017) and Karamanolakis et al. (2020) propose systems with fewer requirements based on word translations, bilingual dictionaries and taskspecific seed words, respectively.", "filtered_refids": [["b52", "b12", "b39", "b41", "b59", null, "b69", "b65"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2124, "num_references": 9}
{"corpusid_sectionid": "225062337-s9", "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios", "date": "2020-10-23", "section_title": "Learning with Noisy Labels", "section": "The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.\n\nNoise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Sch\u00fctze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).\n\nThe noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a \"cleaned\" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.\n\nIn NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).", "filtered_refids": [[], ["b1", null], ["b10", "b67", "b21", null, "b48", "b22", "b61"], ["b11", "b1", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2587, "num_references": 12}
{"corpusid_sectionid": "225062337-s10", "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios", "date": "2020-10-23", "section_title": "Non-Expert Support", "section": "As an alternative to an automatic annotation process, annotations might also be provided by nonexperts. Similar to distant supervision, this results in a trade-off between label quality and availability. For instance, Garrette and Baldridge (2013) obtain labeled data from non-native-speakers and without a quality control on the manual annotations. This can be taken even further by employing annotators who do not speak the low-resource language (Mayhew and Roth, 2018;Mayhew et al., 2019;Tsygankova et al., 2020).\n\nNekoto et al. (2020) take the opposite direction, integrating speakers of low-resource languages without formal training into the model development process in an approach of participatory research. This is part of recent work on how to strengthen low-resource language communities and grassroot approaches (Alnajjar et al., 2020;Adelani et al., 2021).", "filtered_refids": [[null, "b44"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 869, "num_references": 3}
{"corpusid_sectionid": "225062337-s12", "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios", "date": "2020-10-23", "section_title": "Pre-Trained Language Representations", "section": "Feature vectors are the core input component of many neural network-based models for NLP tasks. They are numerical representations of words or sentences, as neural architectures do not allow the processing of strings and characters as such. Collobert et al. (2011) showed that training these models for the task of language-modeling on a large-scale corpus results in high-quality word representations, which can be reused for other downstream tasks as well. Subword-based embeddings such as fastText n-gram embeddings (Bojanowski et al., 2017) and byte-pair-encoding embeddings (Heinzerling and Strube, 2018) addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word.  showed that these embeddings leveraging subword information are beneficial for lowresource sequence labeling tasks, such as named entity recognition and typing, and outperform wordlevel embeddings. Jungmaier et al. (2020) added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings. In addition, pre-trained embeddings were published for more than 270 languages for both embedding methods. This enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence. This includes pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b). These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019).\n\nOpen Issues: While pre-trained language models achieve significant performance increases compared to standard word embeddings, it is still questionable if these methods are suited for real-world low-resource scenarios. For example, all of these models require large hardware requirements, in particular, considering that the transformer model size keeps increasing to boost performance (Raffel et al., 2020). Therefore, these large-scale methods might not be suited for low-resource scenarios where hardware is also low-resource. Biljon et al. (2020) showed that low-to mediumdepth transformer sizes perform better than larger models for low-resource languages and Schick and Sch\u00fctze (2020) managed to train models with three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT-3 on few-shot task by reformulating the training task and using ensembling. Melamud et al. (2019) showed that simple bag-of-words approaches are better when there are only a few dozen training instances or less for text classification, while more complex transformer models require more training data. Bhattacharjee et al. (2020) found that crossview training (Clark et al., 2018) leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages. Alabi et al. (2020) found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources.", "filtered_refids": [[null, "b47"], ["b16", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3503, "num_references": 4}
{"corpusid_sectionid": "225062337-s14", "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios", "date": "2020-10-23", "section_title": "Multilingual Language Models", "section": "Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages. This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) . These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.\n\nIn cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language. Instead, labeled data from a high-resource language is leveraged. A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.\n\nThe transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages. This is useful for standard word embeddings  as well as pre-trained language models. For example, by aligning the languages inside a single multilin- This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018). This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020). For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.\n\nOpen Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold. For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier. Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT. Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 . In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages. This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.\n\n(2020) showed.", "filtered_refids": [[null], [], ["b66", null], ["b53"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3008, "num_references": 4}
{"corpusid_sectionid": "256231532-s1", "title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks", "date": "2022-10-11", "section_title": "Social Influence Dialogue Systems", "section": "\"Social influence is a fact of everyday life\" (Gass, 2015). It is the change in thoughts, feelings, attitudes, or behaviors resulting from interaction with an individual or a group (Rashotte, 2007). Influence is measured by quantifiable proxies of the observed change, like the interest to indulge in physical exercise before or after the interaction with a system, or the final deal in a negotiation as opposed to one person taking it all. Social influence dialogue systems act interactively and influence their partners in decision-making and behavioral contexts (Zhang et al., 2020a;Lee et al., 2020). This calls for an active role by the system, distinguishing them from other well-studied scenarios, such as purely task-oriented, where systems passively assist their partners to complete tasks, and opendomain, that target social companionship. Key social influence tasks include persuasion , aiming to change users' attitudes or behaviors, and negotiation, aiming to change the users' perspective to achieve a common ground (Lewis et al., 2017). Conceptual overview: Figure 1 distinguishes between the kinds of conversational content in social influence interactions. The task-oriented content focuses on influencing for a domain-specific goal, like persuading for donation, bargaining with tradeoffs, or encouraging healthier habits. These interactions may also contain social content, such as small talk, empathy, or self-disclosure. The task-oriented content provides a context for social interactions. Depending on the task, social content is optional, but if present, can in turn build rapport and enhance user-system relationship for improved task outcomes (Liao et al., 2021). Connections with task-oriented and opendomain systems: Similar to a task-oriented or an open-domain scenario, social influence dialogue can also be seen as a sequential decision making process with the goal of maximizing the expected reward Gao et al., 2018). Our proposed category is not meant to be disjoint from these traditional categories. However, it still uniquely brings together the tasks that capture social influence, which is fundamentally absent from how we primarily define dialogue tasks in the community. Defining a new category that captures social influence dialogue would foster a dedicated effort towards this important aspect of real-world conversations.\n\nTask-oriented scenarios focus on collaborative information exchange for a common goal of task completion. In social influence tasks, the goals of the system and the user can be different and even conflicting, leading to collaborative or noncollaborative interactions. Further, the goals can go beyond the current task (e.g. multiple therapy interactions, repeated negotiations), leading to social interactions for long-term relationships. If a scenario involves the system's goal to influence its partner, we consider it under social influence in this paper. For instance, He et al. (2018) studied buyerseller price negotiations. The task of the buyer is to negotiate for a reasonable price (arguably making it task-oriented), but achieving it requires social influence skills of engaging in trade-offs and building a rapport with the seller so as to reach an agreement. Measures of Success: The above discussion indicates that a comprehensive evaluation of social influence systems must draw from both task-oriented and open-domain dialogue research. Since there exist surveys that discuss the evaluation in these settings (Deriu et al., 2021;Li et al., 2021), we don't cover them here in detail. However, we define three essential axes for evaluation: 1) Linguistic Performance, or the system's linguistic sophistication based on automatic (e.g. perplexity, BLEU) and human (e.g. fluency, consistency, coherency) evaluation. 2) Influence Outcome, or the ability to influence defined by objective goals like the negotiated price or weight loss after therapy. 3) Partner Perception, or the subjective evaluation of the user, for instance, the user's satisfaction, likeness towards the system, and interest in interacting again. In a buyer-seller negotiation, if the seller hates the buyer in the end, no matter how favorable the deal is for the buyer, one might argue that this is still a failed negotiation for the buyer. Hence, we encourage future work to take all three dimensions into account collectively.", "filtered_refids": [["b60", "b18", "b1", "b25", "b16", "b38", "b0"], ["b52", null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 4377, "num_references": 10}
{"corpusid_sectionid": "256231532-s2", "title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks", "date": "2022-10-11", "section_title": "Social Influence Across Diverse Application Areas", "section": "We now illustrate social influence across numerous domains and application areas. In total, we curated 22 datasets from prior work that capture social influence in various forms, spanning 12 publication venues, 4 languages, and 7 application domains (see Appendix A for details on the compilation process). In general, the datasets capture the following information about an interaction: the non-conversational context for the participants (e.g. negotiation preferences or other role-specific information), the conversation between them, and outcome assessment. Optionally, some datasets also gather participant demographics and personality traits, utterance-level annotations, and subjective evaluations via post-surveys.\n\nTo understand the structural similarities and differences between these datasets, we design a taxonomy with two primary dimensions: Task Structure (Symmetric vs Asymmetric), and Context Definition (Global vs Local). Task Structure captures whether the participant roles are defined in a sym-metric or an asymmetric manner. For instance, a typical multi-issue negotiation is symmetric, in the sense that both parties have their own preferences and goals based on which they actively try to reach a favorable agreement (Lewis et al., 2017). On the other hand, a counseling session between a therapist and a patient is asymmetric, where the therapist attempts to emotionally support the patient by employing social influence skills (Althoff et al., 2016). Context Definition relates to whether the input context before each interaction is defined globally or locally. For instance, the PersuasionFor-Good dataset globally defines the context of persuasion for charity donation, which is kept the same throughout . On the contrary, in a typical debate, although the rules are defined globally, the conversation topic and arguments are local and can vary for each conversation (Durmus and Cardie, 2019). We present this categorization in Table 1. We further categorize the datasets according to their Domain, Source, and the # of parties. We provide key statistics and the available metadata in Appendix B. We now briefly discuss the datasets in each domain.\n\nGames: Strategy games involve social influence dynamics of trust and deception. Diplomacy captures deception in long-lasting relationships, where players forge and break alliances to dominate Europe (Peskov et al., 2020). Catan revolves around the trade of resources for acquiring roads, settlements, and cities (Asher et al., 2016;Boritchev and Amblard, 2021). The players have access to only a subset of resources that they would need, which encourages strategic influence and trade.\n\nMulti-Issue Bargaining Tasks (MIBT): MIBT is a tractable closed-domain abstraction of a typical negotiation (Fershtman, 1990). It is based on a fixed set of issues each with a predefined priority for each player, which essentially governs the goals of the players. If the priorities of the players align, this leads to competitive negotiations, where each party attempts to convince their partner with tradeoffs and persuasive arguments. If they don't, this allows cooperative interactions where the negotiators try to find optimal divisions that benefit everyone. DealOrNoDeal (Lewis et al., 2017) involves negotiations over three issues: books, balls, and hats. Other datasets define a more grounded scenario, such as symmetric CaSiNo (Chawla et al., 2021b) negotiations between two campsite neighbors and asymmetric JobInterview (Yamaguchi et al., 2021)   negotiations between recruiters and applicants. Social Good: Social influence is critical for social good applications. The tactics must be personalized using knowledge that is both relevant and appealing. PersuasionForGood  involves asymmetric interactions led by a persuader who attempts to convince the other participant for charity donation by employing a variety of tactics. For instance, Logical Appeal uses reason and evidence to support the argument, while Emotional Appeal elicits specific emotions. E-commerce: These tasks are typically asymmetric. A buyer influences the seller towards a reasonable price, while the seller tries to maximize their own profit. An effective system must combine price-related reasoning with language realization. CraigslistBargain (He et al., 2018) involves openended price negotiations with rich influence strategies like embellishments, side offers, emotional appeals, and using world knowledge. Another example is customer support interactions in AntiScam dataset (Li et al., 2020), where users defend themselves against attackers who try to steal sensitive personal information with convincing arguments.\n\nTherapy & Support: Effective therapy using social influence aids in the treatment of mental disorders, and substance use disorders, along with changing undesirable behaviors like unhealthy diets. A counselor needs to be adaptive, personalized, should understand the core issues, and should facilitate a change in patient's perspective (Althoff et al., 2016). In SMS counseling, Althoff et al. (2016) found that linguistic influence like pushing the conversation in the desired direction is associated with perspective change. Similar scenarios were captured in other datasets as well (Demasi et al., 2019;Liang et al., 2021). Tanana et al. (2016) collected the Motivational Interviewing dataset where the goal is to elicit and explore the patient's own motivations for behavior change. EmpatheticDialogues (Rashkin et al., 2019) captured empathetic support interactions, which has been associated with rapport and better task outcomes (Kim et al., 2004;Norfolk et al., 2007;Fraser et al., 2018).\n\nArgumentation: In addition to factuality and social proof, a convincing argument must also consider the intensity, valence, authoritativeness, and framing (Chaiken, 1987;Althoff et al., 2014). Tan et al. (2016) released the ChangeMyView logs from Reddit, involving discussions on numerous controversial topics. Other datasets include Debate Dot Org (DDO) debates on diverse topics (Durmus and Cardie, 2019), congressional proceedings (Thomas et al., 2006), and court hearings (Fornaciari and Poesio, 2012;D.-N.-M. et al., 2012;Ji et al., 2020). Conversational Recommendation: Everyday scenarios naturally hold potential for influence via recommendations, for instance, a movie fan per-suading their friends to watch a movie that they adore.  and Dodge et al. (2016) collected movie recommendation datasets. Instead of guiding the conversation towards a specific movie, the goal is simply to provide recommendations based on facts and personal experiences. Nevertheless, they still provide interesting examples of scenarios that can involve social influence.\n\nMiscellaneous: The Target-Guided dataset (Tang et al., 2019) was constructed from the PersonaChat corpus (Zhang et al., 2018).\n\nInstead of being openended, the Target-Guided scenario defines a concrete goal of naturally guiding the conversation to a designated target subject, thereby, making it a social influence setting.", "filtered_refids": [[], [null, "b18"], [null, "b33"], ["b54", "b18", null, "b23", "b5"], ["b47", "b13", "b30", null, "b24"], ["b49", null, "b10", "b46"], ["b48", "b61"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 48, "num_chars": 7055, "num_references": 20}
{"corpusid_sectionid": "256231532-s3", "title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks", "date": "2022-10-11", "section_title": "Methodological Progress", "section": "Having summarized the datasets that capture social influence, we now discuss the modeling approaches developed for social influence dialogue systems. Most domains have seen efforts in analyzing human dialogue behaviors and their impact on task outcomes. Examples include analyzing deception in games (Peskov et al., 2020), the impact of persuasive strategies and dialogue acts on charity donations ( In addition, researchers have targeted various domain-specific subtasks that can be crucial for the eventual development of dialogue systems in this space. This involves research in lie detection methods (Yeh and Ku, 2021;Yu et al., 2015), discourse parsing (Shi and Huang, 2019;Ouyang et al., 2021), strategy prediction (Chawla et al., 2021b;, breakdown detection (Yamaguchi et al., 2021), outcome prediction (Sinha and Dasgupta, 2021;Chawla et al., 2020;Dutt et al., 2020), and argument mining (Dutta et al., 2022).\n\nResearch that directly targets the development of dialogue systems in this space is still nascent. Among other challenges like limited cross-cultural diversity and relatively smaller dataset size, social influence dialogue settings pose a unique challenge: an average human often exhibits sub-optimal strategic behaviors in social influence tasks (Wunderle, 2007;Babcock and Laschever, 2009). This means that standard seq2seq approaches trained on these collected datasets using supervised learning are fundamentally insufficient for developing effective dialogue systems with influence capabilities. Hence, prior work has put a special attention to the system strategy, employing different ways to model the strategy and language together.\n\nWe design a taxonomy of methods developed for social influence tasks, assisting readers to comprehend the progress and reflect on future directions. We organize them based on the system strategy, language generation, partner model, architecture, learning process, and the use of pretrained language models. We present annotations for all the surveyed methods in Table 2 and discuss the common categories in brief below.", "filtered_refids": [["b43", "b54", "b58", "b44", "b32", "b59", null, "b33"], ["b53", null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2080, "num_references": 10}
{"corpusid_sectionid": "256231532-s4", "title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks", "date": "2022-10-11", "section_title": "Strategy Representation", "section": "Implicit: The most obvious way to represent the system strategy is implicitly, without any intended decoupling between system strategy and response realization. This corresponds to the usual sequenceto-sequence framework that has been a standard baseline for the methods developed in this space. An important example is the work by Lewis et al. (2017), who were one of the first works to train endto-end dialogue models that exhibit social influence. The authors employed a neural network based on GRUs, one for encoding the negotiation context, one to encode the dialogue utterances, and two recurrent units to generate the output agreement in a bidirectional manner. Latent vectors: Yarats and Lewis (2018) explored latent vectors to decouple utterance semantics from its linguistic aspects. Their hierarchical approach first constructs a latent vector from the input message, which is then used for response generation and planning. These latent vectors are trained to maximize the likelihood of future dialogue messages and actions, which enables the decoupling between semantics and realization. Dialogue Acts (DAs): Dialogue Acts, such as greeting, offer propose, agreement, or disagreement, are effective at capturing a high-level structure of the dialogue flow in social influence settings, reducing the model strategy to first predicting the dialogue act for the next response. The use of DAs makes it convenient to apply reinforcement learning approaches (Zhang et al., 2020b;Yang et al., 2021), while also aiding in developing a modular dialogue system design (He et al., 2018). Semantic Strategies: The structural properties ex-  ", "filtered_refids": [["b5", "b56", "b62", "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1642, "num_references": 4}
{"corpusid_sectionid": "256231532-s5", "title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks", "date": "2022-10-11", "section_title": "Language Generation", "section": "An important aspect of the system design is an effective way to realize the language, that is, to generate the next response so that it portrays the desired strategic behaviors. Borrowing from task-oriented and open-domain research, existing dialogue models for social influence use a variety of methods to generate the final system response.\n\nTemplates and retrieval methods: Predefined templates and response retrieval from the training data simplify the generation pipeline, improving controllability and modularity. He et al. (2018) used templates in their generator which are later filled by retrieving similar responses from the data. This allowed the authors to explore supervised and reinforcement learning at the level of DAs for the influence strategy of the system. Conditional Generation: Text generation methods result in more diverse responses, but negatively impact the controllability and interpretability. Prior work relies on autoregressive text generation conditioned on the dialogue history, non-conversational context, and additional annotations. These are either encoder-decoder networks (Lewis et al., 2017;Li et al., 2020;Joshi et al., 2020) or use a decoderonly design (Li et al., 2020). A useful future direction is to combine generation with retrieval for knowledge-grounded settings like argumentation. Similar methods have been explored for other NLP tasks like open-domain question answering and question generation (Lewis et al., 2020).", "filtered_refids": [[], ["b12", "b18", "b19", "b23", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1467, "num_references": 5}
{"corpusid_sectionid": "256231532-s7", "title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks", "date": "2022-10-11", "section_title": "Training", "section": "Architecture Choices: One crucial aspect is the architecture design: End-to-end (Lewis et al., 2017;Radford et al., 2019) vs Modular (He et al., 2018). While end-to-end methods improve the diversity and need less manual effort, a modularized design enhances controllability and explainability. Perhaps, this is why modular methods are popular in large-scale models (Hadfi et al., 2021). Improving the control of desired variables such as topics, strategy, or emotion in the end-to-end methods is an open area of research and is yet to be explored for social influence dialogue systems. The performance was later improved by Joshi et al. (2020), who replaced FSTs with Graph Neural Networks to better model the interdependencies. Others have relied on RL to explicitly optimize the model on task-specific objective outcomes. While SL trains the model to mimic the average human behavior, RL techniques, such as those based on REINFORCE (Williams, 1992), allow the system to explore its own strategies in the wild while being guided by one or more overall reward metrics. Lewis et al. (2017) used RL in negotiations, with the final points scored in the agreed deal as the reward. More recent work employed RL to incorporate simplistic partner models into the decisionmaking process of the dialogue system, showing improvements in negotiation tasks (Zhang et al., 2020b;Yang et al., 2021).\n\nMulti-tasking and Pretraining: Limited efforts have also explored multi-tasking and pretrained language models for social influence dialogue systems, which provide promising ways to deal with the challenge of insufficient training data. Liu (2021) trained a sequence-to-sequence transformer on a mix of Cornell Movie Dialogue corpus (Danescu-Niculescu-Mizil and Lee, 2011) and psychotherapy data. Li et al. (2020) fine-tuned the GPT model (Radford et al., 2018), while employing multi-tasking to incorporate intents and slots for both the human and the system. Wu et al. (2021) recently introduced ARDM which uses GPT2 (Radford et al., 2019) to separately encode the utterances of the human and the dialogue system, reducing the reliance on additional annotations.", "filtered_refids": [["b12", "b56", "b4", "b18", "b62", "b5", "b36"], ["b52", "b23", "b26", "b35"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2152, "num_references": 11}
{"corpusid_sectionid": "256231532-s9", "title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks", "date": "2022-10-11", "section_title": "Task Evaluation:", "section": "Another key limitation of existing work is the lack of a comprehensive evaluation. Prior work majorly focused on objective metrics which only provides a limited view of the model performance. A comprehensive evaluation is challenging since it must consider partner perception along with objective outcomes. Building user simulators could potentially alleviate this problem (Li et al., 2016;Jain et al., 2018;. Most existing simulators are developed for task-oriented systems which follow a certain agenda. Future research should study how to use partner modeling to build social influence user simulators for more efficient and accurate task evaluation (He et al., 2018;Yang et al., 2020). For instance, one could potentially design different user personalities and simulate the change in user's beliefs, opinions, and attitudes accordingly (Yang et al., 2021).\n\nMultimodal systems: Being a core function of human communication, social influence occurs not just through text, but through all possible modalities. Schulman and Bickmore (2009) showed that embodied agents achieve better persuasion results than text-only agents. Other studies have recognized the importance of emotion in social influence tasks (Asai et al., 2020;Chawla et al., 2021a). Nguyen et al. (2021) proposed a speech dataset in debates and study the influence of spoken tactics on persuasiveness across genders. Given these findings, we encourage interdisciplinary efforts in the future to explore the developement of multimodal social influence agents.\n\nKnowledge-enriched systems: Social influence tasks often involve constantly-changing world knowledge such as organization facts and news. Often, the system's internal state (e.g., the change of task setting from one set of products to a different set) needs to be updated. Retraining the entire system is costly to maintain after the initial development. Recent work has proposed to augment the dialogue system with internet-search ability to generate more factual and updated responses in open-domain dialogues (Komeili et al., 2021). Future efforts in this direction will benefit social influence dialogue systems as well.", "filtered_refids": [["b56", "b55", "b22", "b8", "b5"], [null, "b28", "b41"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2152, "num_references": 9}
{"corpusid_sectionid": "256231532-s11", "title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks", "date": "2022-10-11", "section_title": "Broader Impact and Ethical Considerations", "section": "Social influence is ubiquitous in everyday life. Research on how we use influence in all aspects of our lives spans a number of fields, including social psychology, communication, consumer behavior, behavioral change, and behavioral economics. This research has led to crucial findings about the strategies of social influence and how they impact our decision-making. Over the past few decades, research has accumulated and demonstrated the effectiveness of using various strategies across contexts and domains. Prominent examples include core principles of social influence by Cialdini from social psychology: reciprocity, commitment and consistency, social proof, liking and attractiveness, authority, and scarcity (Cialdini, 2009). Further, communication strategies used in persuasion and general social influence contexts include credibility appeals, two-sided argumentation, emotional tactics, and appeals to social norms, among others (Cameron, 2009;O'keefe, 2015). First, the well-studied principles in social influence research can guide the development of effective dialogue systems with influence capabilities. In fact, many of the strategies found in the datasets developed for social influence tasks (Section 3) directly map to the principles laid out by Cialdini, for instance, credibility and emotional appeal in PersuasionForGood dataset  and reciprocity observed in CaSiNo negotiation dataset (Chawla et al., 2021b). Second, research in social influence dialogue systems provides novel datasets on human-human and humanmachine communication, and therefore, holds a great potential to advance theories of human cogni-tion and influence processes (Gratch et al., 2015). The datasets and subsequent analyses can further contribute new theoretical insights to social influence research.\n\nAlthough dialogue systems have already been used in a number of applications involving chatbots and AI assistants, advancements in social influence dialogue systems can help to bridge the gap between our existing task definitions and a number of other real-world applications. For instance, realistic customer support interactions often involve active behaviors from both the support agent and the user where the agent uses social cues for improved customer satisfaction and retention, while the user attempts to address their queries. These settings naturally involve aspects of social influence, unlike traditional task-oriented definitions where the dialogue system plays a passive role to assist the human users. As discussed earlier, social influence dialogue systems can positively help to advance other areas as well. In therapy domain, these systems can assist in various psychological treatments such as by increasing the willingness to disclose (Lucas et al., 2014). In pedagogy, they can help to make social skills training more accessible (Johnson et al., 2019).\n\nWhile we think about these applications, it is crucial to also lay out proper ethical guidelines to avoid any misuse of these systems. Primary concerns are around the use of deception (e.g. in Diplomacy and other negotiation tasks), emotional appeals (e.g. in persuasion), and behavior change (e.g. in conversational recommendations).\n\nTo mitigate possible misuse scenarios or unintended harms, we now lay out a few ethical guidelines which also apply to dialogue research in general. First, rigorous attempts must be made to ensure that the data collection, design processes, and evaluations, strictly abide by the guidelines and regulations laid out by the relevant Institutional Review Board (IRB). Second, the research team needs to develop a thorough plan to monitor and understand the behaviors of the developed systems before deployment. This includes identifying the goals of the dialogue system, identifying potential toxic language use, and any discriminatory behaviors. Third, investment into improved data collection practices, along with explainable and controllable dialogue systems can help identify these issues early on and allow manipulation to avoid them. Fourth, we argue that transparency is the key.\n\nAll stakeholders must be made aware of the goals and design objectives of the system, along with any known misbehaviors or potential risks. The users must also be informed of any data collected during the deployment phase. Lastly, we believe that continuous monitoring of dialogue systems is necessary to ensure that the system performs consistently and does not diverge to unexpected conditions that may incur offensive or discriminative actions. We hope that our work promotes a more systematic study of social influence dialogue systems, which in turn will help to tackle the ethical concerns in a more principled way.", "filtered_refids": [[null, "b31", "b3"], ["b11", "b27"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 4720, "num_references": 5}
{"corpusid_sectionid": "237421091-s2", "title": "(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys", "date": "2021-09-04", "section_title": "Stance Detection and Annotation", "section": "While early efforts to use social media to extract public opinion focused on sentiment analysis (O'Connor et al., 2010;Mitchell et al., 2013;Conrad et al., 2019), more recent work has shifted towards stance detection, which more directly aligns with the goal of public opinion modeling (Sen et al., 2020;Mohammad et al., 2017). In stance detection, the task moves from estimating the positive or negative sentiment of a given text to evaluating whether the authoring individual is for, against, or neutral towards some target concept. In principle, this aligns well with traditional public opinion polling in which respondents self-report along a similar stance scale.\n\nHowever, the growth of stance detection methods has come with many questions about their validity and foundational premises. Sen et al. (2020) showed that a variety of existing stance detection tools for Twitter do not generalize well, even when the target is held constant and test data are reasonably similar to training data. Joseph et al. (2017) showed that how one constructs annotation tasks can significantly impact (supervised) model performance and one's assessment of it. Further, as demonstrated by Shen and Rose (2021) on the closely related task of inferring political ideology, annotator expertise and subjectivity also play an important role in the quality of annotated data.\n\nThe present work complements these prior efforts by delving into other questions of annotator disagreement and inference. Whereas prior work has considered disagreement arising from task differences (Joseph et al., 2017), or properties of the annotators (Shen and Rose, 2021), we control for both of these factors, taking a single task and a relatively homogenous set of expert annotators. Instead, extending recent work studying prediction on multiple targets (van den Berg et al., 2019;, we study how agreement varies depending on the target selected, and how even within a single task design, annotators can come to rely on distinct subsets of information. Second, prior work has largely only assumed that it is possible for annotators to accurately infer a user's stance. The present work tests this assumption by comparing these annotations to self-report data.", "filtered_refids": [["b15", "b19", "b7", "b24", "b17"], ["b24", "b13"], ["b29", "b13", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2228, "num_references": 10}
{"corpusid_sectionid": "233219920-s1", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Preliminaries", "section": "We briefly review preliminaries and assumptions necessary for our survey.\n\nKnowledge bases We use the term \"knowledge base\" (KB) to refer to a relational data structure comprising a set of entities E, relation types R, and triples (s, r, o) \u2208 E \u00d7 R \u00d7 E, where s, o \u2208 E are subject and object entities, respectively. 1 We consider two types of KBs under the umbrella of \"relational world knowledge.\" Encyclopedic KBs store facts about typed, disambiguated entities; a well-known example is the Wikidata KB (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), which, like its sister project Wikipedia, is publicly accessible and collaboratively constructed. By contrast, in commonsense KBs, \"entities\" are typically represented by non-canonicalized free-text phrases. Examples include the publicly accessible, crowdsourced Con-ceptNet (Liu and Singh, 2004;Speer et al., 2017) and ATOMIC (Sap et al., 2019) KBs.\n\nLanguage models Following the contemporary NLP literature, we use the term \"language model\" (LM) to refer to a deep neural network that is trained to learn contextual text representations. LMs generally come pretrained, with parameters pre-initialized for generic text representation via self-supervised training on large corpora, and may be used as-is after pretraining, or further finetuned with supervision on downstream task(s). This work considers LMs based on the Transformer architecture (Vaswani et al., 2017), examples of which include the encoder-only BERT family (Devlin et al., 2019;, the decoder-only GPT family (Brown et al., 2020), and the encoder-decoder T5  and BART  families.", "filtered_refids": [[], [null, "b13", "b35", "b39"], ["b19", null, "b47"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1583, "num_references": 7}
{"corpusid_sectionid": "233219920-s2", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Word-level supervision", "section": "The standard language modeling task is to predict the n-th word in a sequence of n words-that is, a conditional probability estimation task (Radford et al., 2019). While many variants of this task have been proposed to allow LMs to condition their predictions on different inputs (Devlin et al., 2019;, a notable feature of all such approaches is that they operate at the word (and subword) level. If these supervision techniques do not incorporate KBs at all, how are they relevant when considering LMs as relational knowledge representations? The answer is simple. Typical language Prompt handcrafting (Petroni et al., 2019;Dufter et al., 2021) Automatic prompt engineering (Jiang et al., 2020b;Shin et al., 2020;Zhong et al., 2021;Qin and Eisner, 2021) Adversarial prompt modification Poerner et al., 2020; Varying base prompts (Elazar et al., 2021;Heinzerling and Inui, 2021;Jiang et al., 2020a;Kassner et al., 2021) Symbolic rule-based prompting Talmor et al., 2020a) Statement scores ( \u00a7 3.2) Single-LM scoring (Tamborrino et al., 2020;) Dual-LM scoring (Davison et al., 2019Shwartz et al., 2020) modeling corpora like Wikipedia are known to contain KB-like assertions about the world (Da and Kasai, 2019). LMs trained on enough such data can be expected to acquire some KB-like knowledge, even without targeted entity-or relation-level supervision. Therefore, in order to motivate the necessity (if at all) of KB supervision, it is crucial to first understand what relational world \"knowledge\" LMs acquire from word-level pretraining.\n\nIn this section, we cover strategies to extract and utilize this knowledge under the cloze prompting ( \u00a7 3.1) and statement scoring ( \u00a7 3.2) protocols. Table 1 provides a taxonomy for this section, with representative examples and evaluation tasks.", "filtered_refids": [["b43", "b63", "b26", "b28", "b27", "b30", null, "b19", "b38", "b37"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1791, "num_references": 10}
{"corpusid_sectionid": "233219920-s3", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Cloze prompting", "section": "The cloze prompting protocol (Taylor, 1953 and Figure 2) is a direct approach for extracting and evaluating KB-like knowledge in pretrained LMs. Under this protocol, KB triples are first converted to natural language assertions using (e.g.) relation templates. For each assertion, the token(s) corresponding to the object entity are held out. A frozen pretrained LM then ranks candidate tokens within its vocabulary by the probability that they fill in the empty slot(s). Accuracy is typically measured by the proportion of prompts for which the correct answer appears in the LM's top-k predictions, with the assumption that better performance implies more pretrained knowledge within the LM.\n\nHandcrafted prompts in English with singletoken answers make up LAMA (Petroni et al., 2019), one of the earliest and most widely-used LM cloze probes. LAMA, which is mapped primarily to Wikidata and ConceptNet triples, was initially used to compare pretrained LMs' knowledge to offthe-shelf KB question answering systems. Petroni et al. (2019) showed that pretrained BERT is com- petitive with a supervised relation extraction model that has been provided an oracle for entity linking, particularly for 1-1 queries. Subsequent work has experimented with handcrafted templates for probing the knowledge of both very large (hundredbillion parameter) LMs (Brown et al., 2020) as well as non-contextual word embeddings, i.e., as a simple control baseline for LMs (Dufter et al., 2021). Both studies demonstrate some success, particularly in cases where the probed model is provided a small amount of extra context in the form of conditioning examples (Brown et al., 2020) or entity type information (Dufter et al., 2021).\n\nAutomatic prompt engineering is a promising alternative to prompt handcrafting for knowledge extraction in LMs (Liu et al., 2021a), as prompts engineered using discrete (Jiang et al., 2020b;Shin et al., 2020;Haviv et al., 2021) and continuous (Zhong et al., 2021;Qin and Eisner, 2021;Liu et al., 2021b) optimization have improved LMs' lower-bound performance on LAMA's underlying queries. Note, however, that optimized prompts are not always grammatical or intelligible (Shin et al., 2020). Prompt optimization methods may also confound knowledge probes by overfitting to the probes' answer distributions during train-ing (Zhong et al., 2021;, and often require large validation sets for tuning, which may not be feasible in practice (Perez et al., 2021).\n\nAdversarial modification of LAMA prompts has uncovered weaknesses in pretrained LMs' world \"knowledge,\" for example that BERT's accuracy drops precipitously when irrelevant statements or negation words are added to prompts Lin et al., 2020;, and that it can \"guess\" answers using shallow lexical cues or benchmark artifacts (Poerner et al., 2020;. However, the adversarial robustness of LM knowledge improves greatly with supervision in both the pretraining  and fine-tuning  stages, suggesting that explicit KB-level supervision is a viable remedy to input sensitivity. For the former, it has been found that pretrained BERT-based LMs typically do not output consistent answers for prompt paraphrases, although their consistency can again be greatly improved by targeted pretraining (Elazar et al., 2021;Heinzerling and Inui, 2021). For the latter, initial results on prompts beyond English indicate high variability in pretrained LM performance across languages and poor performance on prompts with multi-token answers (Jiang et al., 2020a;Kassner et al., 2021).\n\nPrompts generated with symbolic rules have been used to test pretrained LMs' abilities to learn, e.g., equivalence, implication, composition, and conjunction. Existing studies vary the degrees of experimental control: Talmor et al. (2020a) use BERT-based models with their publicly-available pretrained weights, whereas  pretrain BERT from scratch on synthetic KB triples only. Both studies observe mixed results, concluding that word-level pretraining alone (at least on BERT) does not lead to strong \"reasoning\" skills.", "filtered_refids": [[], [null, "b26"], ["b52", "b63", "b14", "b28", null, "b22", "b37"], ["b11", null, "b27"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 4057, "num_references": 13}
{"corpusid_sectionid": "233219920-s4", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Statement scoring", "section": "Beyond probing, pretrained LM \"knowledge\" can be purposed toward downstream KB-level tasks in a zero-shot manner via statement scoring. Here, a pretrained LM is fed natural language statements corresponding to KB triples, and its token probabilities across each statement are pooled to yield statement scores. These scores are then treated as input to a downstream decision, mirroring the way that supervised LMs can be trained to output probabilities for triple-level prediction tasks ( \u00a7 5). We categorize statement scoring strategies as single-or dual-LM approaches. The single-LM approach pools the pretrained LM's token scores over a candidate set of sequences, then takes the highest-scoring sequence as the LM's \"prediction\" or choice (Tamborrino et al., 2020;Bouraoui et al., 2020;Brown et al., 2020). The dual-LM framework first uses one pretrained LM to generate useful context (e.g., clarification text) for the task, then feeds this context to another, possibly different pretrained LM to obtain a final score (Davison et al., 2019;Shwartz et al., 2020). Both categories have shown promise over comparable unsupervised (and, under some conditions, supervised) methods for tasks like multiple-choice QA (Tamborrino et al., 2020;Shwartz et al., 2020;Brown et al., 2020) and commonsense KB completion (Davison et al., 2019). However, LM scores have also shown to be sensitive to small perturbations in text , so this approach may be less effective on noisy or long-tail inputs.", "filtered_refids": [["b43", null, "b38"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1486, "num_references": 3}
{"corpusid_sectionid": "233219920-s7", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Modeling entities without linking", "section": "The \"least symbolic\" entity supervision approaches that we consider input textual contexts containing entity mention-spans to the LM, and incorporate these mention-spans into their losses. However, they do not require the LM to link these mentions to the KB's entity set, so the LM is never directly exposed to the KB. Figures 3a and 3b provide examples of input and output for this class of approaches.\n\nMasking tokens in mention-spans and training LMs to predict these tokens may promote knowledge memorization .  investigate this strategy using a simple masking strategy whereby an LM is trained to predict the tokens comprising named entities and dates in text (Figure 3a, originally proposed by Guu et al., 2020). The authors find that the largest (11 billion parameter) version of T5 generates exact-match answers on open-domain question answering (QA) benchmarks with higher accuracy than extractive systems-even without access to external context documents, simulating a \"closed-book\" exam.\n\nContrastive learning techniques, which have been used for LM supervision at the word and sentence level (Devlin et al., 2019), have also been devised for supervision on entity mentions (Shen et al., 2020). For example, Xiong et al. (2020) replace a proportion of entity mentions in the pretraining corpus with the names of negatively-sampled entities of the same type, and train an LM to predict whether the entity in the span has been replaced ( Figure 3b). Although the previously discussed closed-book T5 model    .", "filtered_refids": [[], [null], ["b57", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1518, "num_references": 3}
{"corpusid_sectionid": "233219920-s8", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Linking with late fusion", "section": "The next-strongest level of entity supervision is to train the LM to link entity-centric textual contexts to a KB's entity set E. Here, we cover late fusion approaches, which operate at the word level in terms of input to the LM and incorporate entities at the LM's output layer only, as exemplified in Figure 3c. The simplest representatives of this category train LMs to match individual tokens (Broscheit, 2019) or mentions  in a text corpus to an entity bank, without any external resources. The minimally \"entity-aware\" BERT proposed by Broscheit (2019), which adds a single classification layer on top of a pretrained BERT encoder, achieves competitive results with a state-of-the-art specialized entity linking architec- ture (Kolitsas et al., 2018).\n\nEntity meta-information such as names and descriptions are viable external resources for LMpowered entity linking (Botha et al., 2020). For example, in zero-shot entity linking (Logeswaran et al., 2019), textual mentions must be linked to entities unseen during training using only entity descriptions as additional data. Here, competitive solutions train separate BERT models to select and rank candidate entities by encoding their descriptions (Logeswaran et al., 2019;. More recently, encoder-decoder LMs have been trained to retrieve entities by generating their unique names (De Cao et al., 2021), which has the advantage of scaling with the LM's vocabulary size (usually tens of thousands) instead of the KB entity set size (potentially tens of millions). De Cao et al.\n\n(2021) achieve results competitive to discriminative approaches on entity linking and QA, suggesting the potential of generative entity-aware LMs.\n\nExternal entity embeddings pretrained by a separate model have been used as strong sources of inductive bias for LMs. For example, several variants of BERT further pretrain the base model by linearly fusing external entity embeddings with contextual word representations at the output of the BERT encoder (Zhang et al., 2019;He et al., 2020). BERT has also been fine-tuned to match its output token representations to external entity embeddings for the task of end-to-end entity linking . Such approaches rely heavily on the quality of the externally-learned embeddings, which is both a strength and a drawback: Such embeddings may contain useful implicit structural information about the KB, but on the other hand may propagate errors into the LM (Shen et al., 2020).", "filtered_refids": [["b5"], [null, "b19", "b25"], [], ["b62", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2452, "num_references": 6}
{"corpusid_sectionid": "233219920-s9", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Linking with middle or early fusion", "section": "The last and strongest category of entity supervision techniques that we consider are also linking-based, but fuse entity information at earlier stages of text encoding. Mid-fusion approaches retrieve external entity representations in between hidden layers and re-contextualize them into the LM, whereas early fusion approaches simply treat entity symbols as tokens in the vocabulary. Figure 3d provides an example of input/output for early fusion.\n\nRetrieving entity embeddings and integrating them into an LM's hidden word representations is a middle-fusion technique that has the advantage of modeling flexibility: It allows the practitioner to choose where (i.e., at which layer) the entity embeddings are integrated, and how the entity embeddings are learned and re-contextualized into the LM. Peters et al. (2019) integrate externally pre-trained, frozen entity embeddings into BERT's final hidden layers using a word-to-entity attention mechanism. F\u00e9vry et al. (2020) learn the external entity embeddings jointly during pretraining, and perform the integration in BERT's earlier hidden layers using an attention-weighted sum. The latter approach is competitive with a 30\u00d7 larger T5 LM in closed-book QA ( \u00a7 4.1), suggesting that LMs and KB embeddings can be trained jointly to enhance and complement each other.\n\nTreating entities as \"tokens\" by appending special reserved entity symbols to the LM's vocabulary is the earliest of entity fusion approaches (Figure 3d). For instance, Yamada et al. (2020) input entity \"tokens\" alongside textual contexts that mention these entities to RoBERTa, and use specialized word-to-entity and entity-to-entity attention matrices within its hidden layers. Other approaches leave the base LM's internal architecture completely unchanged and focus only on aligning the LM's word and entity embedding spaces at the input level (Rosset et al., 2020;Poerner et al., 2020). Note, however, that this approach may significantly enlarge the LM's vocabulary. For example, plain BERT's vocabulary is around 30k tokens, whereas English Wikipedia has around 6 million entities. This can make pretraining on a larger vocabulary expensive in terms of both time and memory usage (Yamada et al., 2020;Dufter et al., 2021).", "filtered_refids": [[], ["b23"], [null, "b58", "b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2250, "num_references": 4}
{"corpusid_sectionid": "233219920-s12", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Relations as templated assertions", "section": "Template-based lexicalization is a popular relation supervision strategy that does not directly expose the LM to the KB. Similar to how KB queries are converted to cloze prompts for knowledge probing ( \u00a7 3.1), triples are first converted to natural language assertions using relation templates, usually handcrafted. These assertions are then fed as input to the LM, which is trained with any number of task-specific losses. Figure 4 provides an input/output example for this class of approach.\n\nLexicalized triples from Wikidata have been used as LM training data in proof-of-concept studies demonstrating that LMs can serve as natural language querying interfaces to KBs under controlled conditions (Heinzerling and Inui, 2021). A promising approach in this direction uses encoder-decoder LMs to generate answer sets to natural language queries over lexicalized Wikidata triples (Thorne et al., 2020, toward handling multi-answer KB queries with LMs-thus far an understudied task in the LM knowledge querying literature. Other approaches convert KB triples to sentences using relation templates in order to construct taskspecific training datasets for improved performance in, e.g., story generation (Guan et al., 2020), commonsense QA (Ye et al., 2020;Ma et al., 2021), and relation classification (Bouraoui et al., 2020). While most of these approaches rely on template handcrafting, a few automatically mine templates using distant supervision on Wikipedia, achieving competitive results in tasks like relation classification (Bouraoui et al., 2020) and commonsense QA (Ye et al., 2020).\n\nCompositional paths spanning multiple atoms of symbolic knowledge may also be lexicalized and input to an LM (Lauscher et al., 2020;Talmor et al., 2020a) in order to train LMs for soft com- positional reasoning (Clark et al., 2020;Talmor et al., 2020b). Notably, when RoBERTa is finetuned on sentences expressing (real or synthetic) facts and rules from a KB, it can answer entailment queries with high accuracy (Clark et al., 2020;Talmor et al., 2020b). However, as Clark et al. (2020) note, these results do not necessarily confirm that LMs can \"reason,\" but rather that they can at least emulate soft reasoning-raising an open question about how to develop probes and metrics to verify whether LMs can actually reason compositionally.", "filtered_refids": [[], ["b45", "b20", "b60", null], ["b42", null, "b2", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2330, "num_references": 8}
{"corpusid_sectionid": "233219920-s13", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Linearizing KB triples", "section": "The main advantage of templating is that it converts symbolic triples into sequences, which can be straightforwardly input to LMs. However, handcrafting templates is a manual process, and distant supervision can be noisy. To maintain the advantage of templates while avoiding the drawbacks, triples can alternatively be fed to an LM by linearizing them-that is, flattening the subject, relation, and object into an input sequence (Figure 4). With linearization, relation-level supervision becomes as simple as feeding the linearized sequences to the LM and training again with task-specific losses (Yao et al., 2019;Kim et al., 2020;Ribeiro et al., 2020;Wang et al., 2021a) or injecting the sequences into the pretraining corpus . A notable recent example of the former approach (Agarwal et al., 2021) trains T5 on linearized Wikidata triples in order to generate fully natural language versions of those triples. These verbalized triples are used as retrieval \"documents\" for improved LM-based QA over traditional document corpora; note, however, that they can also be used as LM training data for other downstream tasks in place of handcrafted templates ( \u00a7 5.1).", "filtered_refids": [["b59", "b51", "b7", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1165, "num_references": 4}
{"corpusid_sectionid": "233219920-s14", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Relations as dedicated embeddings", "section": "The strategies discussed thus far treat KB triples and paths as natural language sequences. A \"more symbolic\" approach is to represent KB relation types with dedicated embeddings, and integrate these embeddings into the LM using late, middle, or early fusion approaches. Figures 5a and 5b provide input/output examples for late fusion, whereby relation textual contexts are input to the LM, and relation embeddings are constructed or integrated at the LM's output. Figure 5c exemplifies early fusion, whereby relations are treated as input tokens.\n\nContextual representations of entity mentionspans may be pooled at an LM's output layer to represent a relation (Wang et al., 2021c;Yu et al., 2020). For example, Baldini Soares et al. (2019) concatenate the contextual representations of special entity-start markers inserted adjacent to textual entity mentions, and fine-tune BERT to output similar relation representations for statements ranging over the same entity pairs (Figure 5a). This approach, which proved highly successful for relation classification, has been applied to the same task in languages beyond English (K\u00f6ksal and \u00d6zg\u00fcr, 2020;Ananthram et al., 2020), and as an additional LM pretraining objective .\n\nNon-contextual relation embeddings may be learned by defining a separate relation embedding matrix with |R| rows and fusing this matrix into the LM. One advantage of this approach, similar to methods for retrieving external entity embeddings ( \u00a7 4.3), is that it supports fusion at both the late (Wang et al., 2021d;Daza et al., 2021) and middle (Liu et al., 2021c) stages. As an example of the former, Wang et al. (2021d) propose an LM pretraining objective whereby textual descriptions of KB entities are input to and encoded by an LM, then combined with externally-learned relation embeddings at the output using a link prediction loss (Figure 5b). Combined with standard word-level language modeling objectives, this approach enables generalization across both sentencelevel tasks like relation classification, and graphlevel tasks like KB completion.\n\nTreating relations as \"tokens,\" toward early fusion of relations in LMs, is achieved by appending the KB's relation types to the LM's vocabulary (Figure 5c). A notable instantiation of this approach is the COMET commonsense KB construction framework (Bosselut et al., 2019;Hwang et al., 2021;Jiang et al., 2021). Given a subject phrase/relation token as input, COMET fine-tunes an LM to generate object phrases. COMET demon- strates promising improvements over 400\u00d7 larger LMs not trained for KB construction (Hwang et al., 2021). However, templating ( \u00a7 5.1) may yield better results than adding special tokens to the vocabulary when the COMET framework is trained and tested in a few-shot setting (Da et al., 2021).", "filtered_refids": [[], ["b4", null, "b40"], ["b54", null, "b17"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2796, "num_references": 7}
{"corpusid_sectionid": "243865366-s1", "title": "Natural Language Processing Meets Quantum Physics: A Survey and Categorization", "date": 2021, "section_title": "Quantum Physics Preliminaries", "section": "The simplest quantum mechanical system is a qubit, which has two possible states: |0 and |1 , where '|\u00b7 ' is called the Dirac notation, and a ket |\u03c8 denotes a unit column vector. Similarly, the row vector \u03c8 \u2020 is expressed as a bra \u03c8|, where the dagger ( \u2020) corresponds to the conjugate transpose. A qubit can be represented by the linear combination of states, often called superposition:\n\n(1)\n\nwhere a and b are complex numbers and |a| 2 + |b| 2 = 1. Thus the state of a qubit is a unit vector in a two-dimensional complex vector space. When we measure a qubit we obtain either 0, with probability |a| 2 , or 1, with probability |b| 2 . The superposition state can be used for representing multiple meanings of a word. For example, think of a mouse again as a small rodent and a hand-held pointing device. This two independent latent concepts can be denoted as |rodent and |device . Then, the word 'mouse' can be modeled as a superposition state, i.e. |mouse = a |rodent + b |device .\n\nEntanglement is another elementary and unique resource of quantum mechanics which plays a key role in many interesting applications of quantum computing. Consider the following two-qubit entangled Bell state (Nielsen and Chuang, 2002):\n\nAs discussed earlier, when we measure the first qubit, we obtain two possible results: 0 with probability 1/2 and 1 with probability 1/2. According to Eq.2, a measurement of the second qubit always gives the same outcome as the measurement of the first qubit, because the measurement results of these two entangled qubits are correlated. Coecke et al. (2020) proposed that if words are encoded into quantum states, then the grammatical structure is to entangle these states. Because grammar is what correlates meanings between words. We will explain this in Section 4.2.2.\n\nProjective measurements are the most general form of measurement in quantum mechanics, where the measurement operators are projectors P that satisfy P 2 = P . If the state is |\u03c8 before projective measurement then the probability that result m occurs is given by p(m) = \u03c8| P m |\u03c8 . The state after measurement is:\n\nProjective measurement can be applied to calculate cosine similarity in NLP, which measures the similarities between two vectors. Suppose |A and |B represent word A and B, respectively. Then, the cosine similarity of these two word vectors is\n\nwhere P B = |B B| is a projective measurement operator.\n\nIn addition to state vectors, quantum mechanics can also be formulated using density matrix, which is mathematically equivalent. Suppose that a quantum system is in one of the states |\u03c8 i , where i is an index, with probability p i . The definition of the density matrix is:\n\nMore information about quantum computing can be found in (Nielsen and Chuang, 2002).", "filtered_refids": [[], [], [], ["b32"], [null], [], [], [], [], ["b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 2773, "num_references": 3}
{"corpusid_sectionid": "243865366-s3", "title": "Natural Language Processing Meets Quantum Physics: A Survey and Categorization", "date": 2021, "section_title": "Quantum Algorithms", "section": "In quantum computing, a quantum algorithm is an algorithm that runs on real quantum computers. With regard to representation, Coecke et al.    (Coecke and Kissinger, 2018;Coecke et al., 2020) to demonstrate: (a) A ket |\u03c8 , (b) A bra \u03c8|, (c) Bell state in Eq. 2, (d) (g(|\u03d5 1 \u2297 |\u03d5 2 )) \u2297 I, where matrix multiplication looks like connecting up the inputs and outputs of boxes and tensor product looks like placing boxes side by side.\n\n(2010) constituted a graphical framework (DisCo-Cat) for natural language that combines words and builds the meaning of a sentence instead of thinking of a sentence as a bag of words. They devised a graphical framework from previous work which represents quantum mechanics pictorially by using lines, triangles, and so on (Coecke and Kissinger, 2018). As an example, in Figure 1, we use this graphical framework to demonstrate the ket, bra, and two-qubit entangled states introduced in Section 2.\n\nZeng and Coecke (2016) first discussed whether a quantum computer can be applied to process natural language, showing a quantum algorithm for calculating sentence similarity that, under certain conditions, achieves a quadratic speedup over classical methods (see Table 2). This quadratic speedup, however, requires quantum random access memory (QRAM), which is expensive and remains unrealized (Biamonte et al., 2017). Considering this problem, Meichanetzidis et al. (2020a) and Coecke et al. (2020) proposed quantum algorithms that can potentially be implemented in existing NISQ computers. Wiebe et al. (2019) presented a representation for the linguistic structure which can encode NLP problems into small quantum devices. As a proof-of-concept experiment, Meichanetzidis et al. (2020b) performed the first quantum NLP task using a small dataset on NISQ hardware. To present larger-scale experiments, Lorenz et al. (2021) implemented models that solve sentence classification tasks on NISQ computers for datasets of size \u2265 100 sentences. These works pave the way for practical quantum NLP in the NISQ era. ", "filtered_refids": [[null, "b7"], ["b7"], ["b44", "b4", "b29", "b27", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2040, "num_references": 8}
{"corpusid_sectionid": "243865366-s5", "title": "Natural Language Processing Meets Quantum Physics: A Survey and Categorization", "date": 2021, "section_title": "Classical Algorithms", "section": "Quantum-inspired or quantum-like NLP algorithms have been designed for classical computers, and some of them achieve comparable performance to state-of-the-art models (Jiang et al., 2020;. For the sake of applicability, these classical algorithms borrow mathematical frameworks from quantum mechanics but are not constrained by the quantum computing operations when processing the data.\n\nVan Rijsbergen (2004) first proposed to unify information retrieval models into the mathematical framework of quantum mechanics in Hilbert space. Sordoni et al. (2013) proposed a quantum language model, which models term dependencies using the density matrix. This work indicates that the density matrix may be a more general representation of texts. Based on this, Basile and Tamburini (2017) presented a language model using the evolution of the state which can be implemented in speech recognition.  encoded words as quantum states and sentences as mixed systems.\n\nRecently, in order to improve practicality, some quantum-inspired neural networks for natural language problems have been proposed.  use a density matrix based convolutional network to capture interactions within each utterance, outperforms a number of state-of-the-art sentiment analysis algorithms. Jiang et al. (2020) proposed a quantum interference inspired neural matching model with application to ad-hoc retrieval. The main difference between these quantum-inspired neural models for NLP and the existing neural based models is that the former models use the mathematical framework of quantum theory to describe language features. These features described by quantum theory are then used as the input of the neural network. Using quantum mechanics concepts to describe features have better interpretability, because they have more transparent physical explanations. It is also more beneficial to the subsequent neural network to extract useful information.\n\nThe above quantum-inspired neural networks are mainly for improving end-to-end performance, but still lack a theoretical foundation for the connection between quantum-inspired language model and neural network. Tensor networks, which factorize very large tensors into networks of smaller tensors, can help the theoretical understanding of existing neural networks (Levine et al., 2018). Based on tensor decomposition, Zhang et al. (2018b) proposed a quantum many-body wave function (QMWF) inspired language modeling and showed a mathematical understanding of using convolutional neural network (CNN). More recently,  proposed a tensor network method (namely TextTN) for natural language representation. Tensor network can not only run on a classical computer but also can be transformed into a quantum circuit. In addition, the hyper-parameters of TextTN can be well interpreted by the entanglement entropy .", "filtered_refids": [["b15"], ["b38", "b3"], ["b15"], ["b21", "b50"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2829, "num_references": 6}
{"corpusid_sectionid": "243865366-s8", "title": "Natural Language Processing Meets Quantum Physics: A Survey and Categorization", "date": 2021, "section_title": "Modeling Word Ambiguity", "section": "Word ambiguity is a combination of distinct known meanings. , Li et al. (2018) and Coecke et al. (2020) adopted superposition state and complex number to formulate this combination. The latent concepts of a word form a set of pure orthonormal states of the space {|C i }. This word t is modeled as a superposition state\n\nin which the amplitude {a i } n i=1 are complex numbers and n i=1 |a i | 2 = 1. As mentioned in Section 2, if the superposition state is measured, it will collapse into the basis vector. This means that when a word is observed within a certain context it will collapse to one of its known meanings.  and  showed that we can benefit from the complex-valued word embedding and the phases can be linked to some important features such as word positions. Moreover, the computational space increase exponentially with the size of the system (Coecke et al., 2020). If we consider a system of n qubits, then a quantum state of this system can represent a word that has 2 n latent concepts and is specified by 2 n amplitude. Trying to store all these complex numbers and vectors can be challenging on classical computers.\n\nMeyer and Lewis (2020), Bankova et al. (2018) and Piedeleu et al. (2015) adopted density matrices to model lexical ambiguity. Unlike commonly-used methods which map words into vectors, they map words into matrices.", "filtered_refids": [[null, "b23"], [null], ["b34", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1350, "num_references": 5}
{"corpusid_sectionid": "243865366-s11", "title": "Natural Language Processing Meets Quantum Physics: A Survey and Categorization", "date": 2021, "section_title": "Modeling Term Dependencies", "section": "Quantum-inspired algorithms have been considered for dependencies between terms in frequently occurring multiword expressions. The quantum language model (QLM) proposed by Sordoni et al. (2013) first applies quantum theory to model term dependencies, argue that there may be a situation in which classical probability fails and need to switch to a more general probabilistic theory. They map words w to projectors:\n\nwhere w \u2208 V and |e w is the one-hot encoding of the word w. For example, consider V = Figure 2: Diagrammatic form of the reduction n(n r sn l )n \u2192 (nn r )s(n l n) \u2192 1s1 \u2192 s, where n is noun, s is declarative statement, and cup denote the grammar reductions. According to pregroup grammar (Lorenz et al., 2021;Lambek, 2008), Jack likes Rose is grammatical because of above reduction.\n\n{natural, language}. Then \u03a0 language is:\n\nThe relationship linking two or more words is represented by a subset of the vocabulary \u03ba = {w 1 , w 2 , ..., w n } and encoded into a new projector:\n\nwhere {a i } n i=1 are real numbers and n i=1 a 2 i = 1. For example, we can model the dependency between natural and language, \u03ba nl = {natural, language}, by K nl = |\u03ba nl \u03ba nl |, where |\u03ba nl = 2 5 |e natural + 3 5 |e language . Then\n\n|\u03ba nl is a superposition state and K nl is a density matrix. In quantum mechanics, elements of the density matrix K nl contain the correlation between quantum states |e natural and |e language , thus dependency between natural and language is modeled. This method of modeling term dependency is interpretable and has physical meaning.\n\nSome algorithms have been proposed based on above QLM. Xie et al. (2015) took entanglement into consideration which is not considered in original QLM,  adopted word embedding instead of one-hot encoding, and so on. The basic and important idea behind these algorithms is to treat word vectors as quantum states from which we can obtain the density matrix of the sentence or document. Then this density matrix naturally contains the correlation of these quantum states, which means the dependence between words is modeled.", "filtered_refids": [["b38"], ["b18", "b27"], [], [], [], [], ["b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2085, "num_references": 4}
{"corpusid_sectionid": "243865366-s15", "title": "Natural Language Processing Meets Quantum Physics: A Survey and Categorization", "date": 2021, "section_title": "Modeling Grammar", "section": "Pregroup gramma (Lambek, 1997) is used for analyzing the structure of natural languages. As an algebraic gadget, pregroup grammar can be denoted using having cup-shaped wires (Lambek, 2008). We show an example sentence in Figure 2. From Figure 1 and Figure 2, we can see that diagrammatic frameworks used for quantum mechanics and pregroup grammar are partially similar. Coecke et al. (2010) introduced a model based on tensor product composition, which uses pregroup grammar to compute the meaning of sentences and phrases. Coecke et al. (2020) recast this model in quantum computational terms and showed that pregroup can always be made using only Bell-effect and identities.\n\nHere is an example of how to use Bell-effect and identities to represent applying an adjective to a noun. Assuming the meaning of story is a 1-qubit state |\u03c8 story \u2208 C 2 and the meaning of adjective happy is a 2-qubit state |\u03c8 love \u2208 C 2 \u2297 C 2 . In happy story, happy modifies the noun story. Coecke et al. (2020) model this modification using \n\nwhere Bell| = 00| + 11| and I is the identity. The mapping (I \u2297 Bell|) shows the interaction between the meaning of words. Using diagrammatic notation (Coecke and Kissinger, 2018;Coecke et al., 2020), our example is illustrated in Figure 3. The pentagon represents the quantum state, the straight line represents the identity matrix, and the cup-shaped wire represents the Bell-effect. Coecke et al. (2020) also showed that this type of wire structure and pregroup grammar can be equivalent, and thus to some extent NLP is quantum native.", "filtered_refids": [[null, "b18", "b17"], [null], [null, "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1563, "num_references": 6}
{"corpusid_sectionid": "243865366-s16", "title": "Natural Language Processing Meets Quantum Physics: A Survey and Categorization", "date": 2021, "section_title": "Applications", "section": "Quantum NLP shows comparable or better performance compared with strong baselines for some tasks. We summarize the results of these algorithms in Table 3.\n\nInformation retrieval (IR). Sordoni et al. (2013) first proposed a quantum language model for IR, representing terms in queries and documents as superposition events attached with quantum probability, which has no classical analog. Extensions of the quantum language model have also been proposed for IR. Xie et al. (2015) advanced the QLM framework by taking into account quantum entanglement, which has a significant cognitive implication. Li et al. (2018) proposed an algorithm to help improve convergence. Jiang et al. (2020) took interference into account, which produces additional contributions to the total probability beyond classical cases. Based on this new contributions, they proposed a matching model for ad-hoc retrieval. The quantum matching models outperform some traditional models.\n\nQuestion answering (QA). Zhang et al. (2018a) used density matrices to represent questions and answers and introduced a joint representation to model the similarities between the question and answers. This joint representation is then used as an input to a neural network.  proposed a complex-valued network for QA, which is interpretable and shows comparable performance to strong CNN and RNN baselines. Coecke et al. (2020) mentioned that QA tasks can be executed on quantum computers. After mapping a question to a vector, QA tasks become the task to find the closest vector in the answer vectors pool. They exploited quantum advantage for finding the closest vector (Wiebe et al., 2015) and showed quantum speedup. Meichanetzidis et al. (2020b) showed the first-ever quantum NLP experiment on quantum hardware through a QA task. Although this is a proof-ofconcept experiment, it paved the way for the future use of quantum computers to deal with practical NLP problems.", "filtered_refids": [[], ["b15", "b38", "b23", "b46"], ["b49", "b45", null, "b29"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 1931, "num_references": 8}
{"corpusid_sectionid": "243865366-s19", "title": "Natural Language Processing Meets Quantum Physics: A Survey and Categorization", "date": 2021, "section_title": "Benefits", "section": "In this section, we make a summary of potential benefits of quantum NLP, and discuss the most salient directions that remain under-explored due to various reasons.\n\nLowering computational cost. Some articles have demonstrated quantum speedup for specific NLP tasks, such as question answering (Coecke et al., 2020;Zeng and Coecke, 2016). Quantum search algorithm (Grover, 1996), quantum nearestneighbor algorithm (Wiebe et al., 2015) and other quantum algorithms which achieve speedup over classical algorithms could be used after classical language features are encoded into quantum states. As mentioned in Section 4, the quantum superposition is suitable for modeling uncertainties in language, such as word ambiguity . And entanglement can describe the composition of lexical semantic units (Coecke et al., 2020;Meichanetzidis et al., 2020b). It's possible that, by adaptations to quantum algorithms and deployments to quantum computers, a family of NLP tasks can enjoy quantum speedup.\n\nEnhancing learning ability. Quantum mechanics is well-known to generate counter-intuitive patterns (Biamonte et al., 2017). It is reasonable to hope that quantum computers can recognize some patterns that cannot be recognized by classical computers. As shown in Table 3, some quantum NLP models have shown comparable or better performance over strong baselines. And the framework of quantum mechanics can be applied to model some features that are difficult to model with classical probabil-ity. For example, quantum theory is used to model interference phenomenon in information retrieval (Jiang et al., 2020) and term dependencies (Sordoni et al., 2013). It's more consistent with human cognition. Li et al. (2021) demonstrate that neural machine translation models fail badly on compositional generalization. According to existing paper, we believe quantum NLP models have potential advantages in compositional generalization problem.\n\nIncreasing storage capacity. Quantum computers have strong storage capabilities. As mentioned before, Coecke hold the view that NLP is quantumnative (Meichanetzidis et al., 2020b;Coecke et al., 2020) such that the exponentially large vector space required to represent sentences can only be naturally and feasibly realized in quantum computers. From this point of view, developments of quantum language models will be beneficial also in terms of storage efficiency.", "filtered_refids": [[], ["b12", "b29", "b45", null, "b48"], ["b15", "b38", "b25", "b4"], [null, "b29"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2395, "num_references": 11}
{"corpusid_sectionid": "231709697-s2", "title": "English Machine Reading Comprehension Datasets: A Survey", "date": "2021-01-25", "section_title": "Answer Type", "section": "Cloze The question is formulated as a sentence with a missing word or phrase which should be inserted into the sentence or should complete the sentence. The answer candidates may be included as in (1) from ReciteQA (Yagcioglu et al., 2018), and may not, as in (2)  We distinguish cloze multiple choice datasets from other multiple choice datasets. The difference is the form of question: in the cloze datasets, the answer is a missing part of the question context and, combined together, they form a grammatically correct sentence, whereas for other multiple choice datasets, the question has no missing words.\n\nBoolean A Yes/No answer is expected, e.g. (4) from the BoolQ dataset (Clark et al., 2019). Some datasets which we include here have a third \"Cannot be answered\" or \"Maybe\" option, e.g. (5) from PubMedQuestions (Jin et al., 2019).\n\n(4) P: The series is filmed partially in Prince Edward Island as well as locations in ... Q: Is anne with an e filmed on pei? A: Yes (5) P: ... Young adults whose families were abstainers in 2000 drank substantially less across quintiles in 2010 than offspring of nonabstaining families. The difference, however, was not statistically significant between quintiles of the conditional distribution. Actual drinking levels in drinking families were not at all or weakly associated with drinking in offspring. ... Q: Does the familial transmission of drinking patterns persist into young adulthood? A: Maybe\n\nExtractive or Span Extractive The answer is a substring of the passage. In other words, the task is to determine the answer character start and end index in the original passage, as shown in (6)  Generative or Free Form Answer The answer must be generated based on information presented in the passage. Although the answer might be in the text, as illustrated in (7) from Narra-tiveQA (Ko\u010disk\u00fd et al., 2018), no passage index connections are provided.\n\n(7) P: ...Mark decides to broadcast his final message as himself. They finally drive up to the crowd of protesting students, .... The police step in and arrest Mark and Nora.... Q: What are the students doing when Mark and Nora drive up? A: Protesting.", "filtered_refids": [["b4"], [null, "b8"], [], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2154, "num_references": 4}
{"corpusid_sectionid": "231709697-s8", "title": "English Machine Reading Comprehension Datasets: A Survey", "date": "2021-01-25", "section_title": "Quantitative Analysis", "section": "Each dataset's size is shown in Table 1. About onethird of datasets contain 100k+ questions which makes them suitable for training and/or fine tuning a deep learning model. A few datasets contain fewer than 10k samples: MultiRC (9.9k), ARC (7,8k), Shmoop (7.2k), ReClor (6.1k), Open-BookQA(6k), QAngaroo MedHop (2.5k), WikiQA (2k). Every dataset has its own structure and data format. We processed all datasets extracting lists of questions, passages, and answers, including answer candidates, and then use the spaCy 10 tokenizer.\n\nQuestion/Passage/Answer Length The graphs in Fig. 4 provide more insight into the differences between the datasets in terms of answer, question, and passage length, as well as vocabulary size. The outliers are highlighted. 11 The majority of datasets have a passage length under 1500 tokens with the median being 329 tokens but due to seven outliers, the average number of tokens is 1250 ( Fig. 4 (a)). Some datasets (MS MARCO, SearchQA, AmazonYesNo, AmazonQA, MedQA) have a collection of documents as a passage but others contain just a few sentences. 12 The number of tokens in a question lies mostly between 5 and 20. Two datasets, ChildrenBookTest and WhoDid-What, have on average more than 30 tokens per question while WikiReading, QAngaroo MedHop, and WikiHope have only 2 -3.5 average tokens per question ( Fig. 4 (b)). The majority of datasets contain fewer than 8 tokens per answer with the average being 3.5 tokens per answer. The Natu-ralQuestions is an outlier with average 164 tokens per answer 13 (Fig. 4 (c)).\n\nVocabulary Size To obtain a vocabulary size we calculate the number of unique lower-cased token lemmas. A vocabulary size distribution is presented in Fig. 4 (d). There is a moderate correlation 14 between the number of questions in a dataset and its vocabulary size (see Fig. 5). 15 WikiReading has the largest number of questions as well as the richest vocabulary. bAbI is a synthetic dataset with 40k questions but only 152 lemmas in its vocabulary.\n\nLanguage Detection We ran a language detector over all datasets using the pyenchant for American and British English, and langid li- 12 We consider facts (sentences) from ARC and QASC corpora as different passages. 13 We focus on short answers, considering long ones only if the short answer is not available.\n\n14 As the data has a non-normal distribution, we calculated the Spearman correlation: coefficient=0.58, p-value=1.3e-05. 15 The values for the BookTest (Bajgar et al., 2017) and WhoDidWhat (Onishi et al., 2016) are taken from the papers.  braries. 16 In 37 of the 60 datasets, more than 10% of the words are reported to be non-English. 17 We inspected 200 randomly chosen samples from a subset of these. For Wikipedia datasets (Hot-PotQA, QAngoroo WikiHop), around 70-75% of those words are named entities; 10-12% are specific terms borrowed from other languages such as names of plants, animals, etc.; another 8-10% are foreign words, e.g. the word \"dialetto\" from Hot-PotQA \"Bari dialect (dialetto barese) is a dialect of Neapolitan ...\"; about 1.5-3% are misspelled words and tokenization errors. In contrast, for the user-generated dataset, AmazonQA, 67% are tokenization and spelling errors. This aspect of a dataset's vocabulary is useful to bear in mind when, for example, fine-tuning a pre-trained language model which has been trained on less noisy text.\n\nFirst Question Word A number of datasets come with a breakdown of question types based on the first token (Nguyen et al., 2016;Ostermann et al., 2018Ostermann et al., , 2019Ko\u010disk\u00fd et al., 2018;Clark et al., 2019;Xiong et al., 2019;Jing et al., 2019;Bjerva et al., 2020). We inspect the most frequent first word in a dataset's questions excluding cloze-style questions. Table 1 shows the most frequent first word per dataset and Table 2 ", "filtered_refids": [[], [null], [null], [null], [null, "b1"], [null, "b8", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 3825, "num_references": 8}
{"corpusid_sectionid": "231709697-s10", "title": "English Machine Reading Comprehension Datasets: A Survey", "date": "2021-01-25", "section_title": "Evaluation Metrics", "section": "Accuracy is defined as the ratio of correctly answered questions out of all questions. For those datasets where the answer should be found or generated (extractive or generative tasks) accuracy is the same as Exact Match (EM), implying the system answer is exactly the same as the gold answer.\n\nIn contrast with selective and boolean tasks, extractive or generative tasks can have ambiguous, incomplete, or redundant answers. In order to assign credit when the system answer does not exactly match the gold answer, Precision and Recall, and their harmonic mean, F1, can be calculated over words or characters. Accuracy is used for all boolean, multiple choice, and cloze datasets. 19 For extractive and generative tasks it is common to report EM (accuracy) and F1. For cloze datasets, the metrics depends on the form of answer. If there are options available, accuracy can be calculated. If words have to be generated, the F1 measure can also be applied.\n\nOne can view the MRC task from the perspective of Information Retrieval, providing a ranked list of answers instead of one definitive answer. In this case, a Mean Reciprocal Rank (Craswell, 2009) (MRR) and Mean Average Precision (MAP) can be used, as well as the accuracy of the top hit (Hits@1) (single answer) over all possible answers (all entities). 20 All metrics mentioned above work well for welldefined answers but might not reflect performance for generative datasets as there could be several alternative ways to answer the same question. Some datasets provide more than one gold answer. A number of different automatic metrics used in language generation evaluation are also used: Bilingual Evaluation Understudy Score (BLEU) (Papineni et al., 2002), Recall Oriented Understudy for Gisting Evaluation (ROUGE-L) (Lin, 2004), and Metric for Evaluation of Translation with Explicit 19 Except MultiRC as there are multiple correct answers and all of them should be found, and CliCR and ReCoRD which use exact match and F1. This is because even though the task is cloze, the answer should be generated (in case of CliCR) or extracted (ReCoRD). 20 MRR and MAP are used only by (Yang et al., 2015) in the WikiQA dataset, as well as precision, recall and F1. (Miller et al., 2016) in the WikiMovies datasets used the accuracy of the top hit (Hits@1).\n\nORdering (METEOR) (Lavie and Agarwal, 2007). MSMarco, NarrativeQA, and TweetQA are generative datasets which use these metrics. Choi et al. (2018) introduced the human equivalence score (HEQ). It measures the percentage of examples where the system F1 matches or exceeds human F1, implying a system's output is as good as that of an average human. There are two variants: HEQ-Q based on questions and HEQ-D based on dialogues.", "filtered_refids": [[], [null], [null, "b5"], [null, "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 2737, "num_references": 5}
{"corpusid_sectionid": "231709697-s14", "title": "English Machine Reading Comprehension Datasets: A Survey", "date": "2021-01-25", "section_title": "A Other Datasets", "section": "There are a number of datasets we did not include in our analysis as we focus on the Question Answering Machine Reading Comprehension task. In this section we mention some of these and explain why they are excluded. CLOTH (Xie et al., 2018) and Story Cloze Test, (Mostafazadeh et al., 2016(Mostafazadeh et al., , 2017 are cloze-style datasets with a word missing from the context and without a specific query. In contrast, cloze question answering datasets considered in this work have a passage and a separate sentence which can be treated as question with a missing word.\n\nAs well as this, we did not include a number of MRC datasets where the story should be completed such as ROCStories (Mostafazadeh et al., 2016), CODAH (Chen et al., 2019), SWAG (Zellers et al., 2018), and HellaSWAG (Zellers et al., 2019) because there are no questions.\n\nQBLink (Elgohary et al., 2018) is technically a MRC QA dataset but for every question there is only the name of a wiki page available. The \"lead in\" information is not enough to answer the question without additional resources. In other words, QBLink is a more general QA dataset, like CommonSenseQA (Talmor et al., 2019). Another general dataset is MKQA (Longpre et al., 2020) which contains 260k question-answer pairs in 26 typologically diverse languages.\n\nTextbook Question Answering (TQA) (Kembhavi et al., 2017) is a multi-modal dataset requiring not only text understanding but also picture processing. MCQA is a Multiple Choice Question Answering dataset in English and Chinese based on examination questions introduced as a Shared Task in IJCNLP 2017 by (Shangmin et al., 2017). The authors do not provide any supportive documents which can be considered as a passage so it is not a reading comprehension task.", "filtered_refids": [[null], [null, "b8"], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1765, "num_references": 5}
{"corpusid_sectionid": "222133962-s4", "title": "A Survey of Unsupervised Dependency Parsing", "date": "2020-10-04", "section_title": "Models", "section": "A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.\n\nBased on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).\n\nSimilar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.", "filtered_refids": [[], ["b6", "b60", "b44", "b4", "b30", "b2", "b3"], ["b26"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 2939, "num_references": 8}
{"corpusid_sectionid": "222133962-s8", "title": "A Survey of Unsupervised Dependency Parsing", "date": "2020-10-04", "section_title": "Intermediate Representation Encoder Decoder", "section": "Autoencoder CRFAE (Cai et al., 2017) Z P (z|x) P (x|z) D-NDMV (Han et al., 2019a) Deterministic Variant S P (s|x) P (z,x|s)\n\nVariational Autoencoder (Li et al., 2019) Z P (z|x) P (z, x) D-NDMV (Han et al., 2019a) Variational Variant S P (s|x) P (z, x|s) (Corro and Titov, 2018) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation.x is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x.\n\nIn addition to the EM algorithm, the learning objective can also be optimized with gradient descent. Yang et al. (2020) recently observe that gradient descent can sometimes significantly outperform EM when learning neural DMV.\n\nBetter learning results can also be achieved by manipulating the training data. Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences. Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing. Spitkovsky et al. (2013) propose to treat different learning algorithms and configurations as modules and connect them to form a network. Some approaches discussed above, such as Lateen EM and curriculum learning, can be seen as special cases of this approach.", "filtered_refids": [["b20", "b5"], ["b33", "b10", "b20"], ["b60"], ["b57", "b58"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1467, "num_references": 8}
{"corpusid_sectionid": "222133962-s12", "title": "A Survey of Unsupervised Dependency Parsing", "date": "2020-10-04", "section_title": "Variational Autoencoder-Based Approaches", "section": "As mentioned in Section 3.1, the training objective of a generative model is typically the probability of the training sentence and the dependency tree is marginalized as a hidden variable. However, the marginalized probability cannot usually be calculated accurately for more complex models that do not make strict independence assumption. Instead, a variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability. Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.\n\nThree unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1). There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.\n\nRecurrent Neural Network Grammars (RNNG) ) is a transition-based constituent parser, with a discriminative and a generative variant. Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing. Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence. The probability of each operation is calculated by a neural network. Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder respectively. However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting. Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.\n\nThe model proposed by Corro and Titov (2018) is also based on a variational autoencoder. It is designed for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised dependency parsing. The encoder of this model is a conditional random field model while the decoder generates a sentence based on a graph convolutional neural network whose structure is specified by the dependency tree. Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation. Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.\n\nThe variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector.", "filtered_refids": [[], [], ["b33"], ["b10", "b25"], ["b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3173, "num_references": 4}
{"corpusid_sectionid": "222133962-s13", "title": "A Survey of Unsupervised Dependency Parsing", "date": "2020-10-04", "section_title": "Other Discriminative Approaches", "section": "Apart from the approaches based on autoencoder and variational autoencoder, there are also a few other discriminative approaches based on discriminative clustering (Grave and Elhadad, 2015), self-training (Le and Zuidema, 2015), or searching (Daum\u00e9 III, 2009). Because of space limit, below we only introduce the approach based on discriminative clustering called Convex MST (Grave and Elhadad, 2015).\n\nConvex MST employs a first-order graph-based discriminative parser. It searches for the parses of all the training sentences and learns the parser simultaneously, with a learning objective that the searched parses are close to the predicted parses by the parser. In other words, the parses should be easily predictable by the parser. The objective function can be relaxed to become convex and then can be optimized exactly.", "filtered_refids": [["b11", "b31", "b18"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 826, "num_references": 3}
{"corpusid_sectionid": "222133962-s17", "title": "A Survey of Unsupervised Dependency Parsing", "date": "2020-10-04", "section_title": "Neural Parameterization", "section": "Traditional generative approaches either directly learn or use manually-designed features to compute dependency rule probabilities. Following the recent rise of deep learning in the field of NLP, Jiang et al. (2016) propose to predict dependency rule probabilities using a neural network that takes as input the vector representations of the rule components such as the head and child tokens. The neural network can automatically learn features that capture correlations between tokens and rules. Han et al. (2019a) extend this generative approach to a discriminative approach by further introducing sentence information into the neural network in order to compute sentence-specific rule probabilities. Compared with generative approaches, it is more natural for discriminative approaches to use neural networks to score dependencies or parsing actions, so recent discriminative approaches all make use of neural networks (Li et al., 2019;Corro and Titov, 2018).", "filtered_refids": [["b20", "b33", "b10", "b26"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 962, "num_references": 4}
{"corpusid_sectionid": "222133962-s18", "title": "A Survey of Unsupervised Dependency Parsing", "date": "2020-10-04", "section_title": "Lexicalization", "section": "In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a;He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010 use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007, Pate andSpitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance,  use neural networks to predict dependency probabilities that are automatically smoothed.\n\nIn principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similarities between words. Recently proposed contextual word embeddings  Table 2: Reported directed dependency accuracies on section 23 of the WSJ corpus, evaluated on sentences of length \u2264 10 and all lengths. *: without gold POS tags. \u2020: with more training data in addition to WSJ. (Devlin et al., 2019) are even more informative, capturing contextual information. However, word embeddings have not been widely used in unsupervised dependency parsing. One concern is that word embeddings are too informative and may make unsupervised models more prone to overfitting. One exception is He et al. (2018), who propose to use invertible neural projections to map word embeddings into a latent space that is more amenable to unsupervised parsing.", "filtered_refids": [["b63", "b57", "b4", "b45", "b48", "b22", "b24", "b53"], ["b12", "b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1910, "num_references": 10}
{"corpusid_sectionid": "222133962-s20", "title": "A Survey of Unsupervised Dependency Parsing", "date": "2020-10-04", "section_title": "Unsupervised Multilingual Parsing", "section": "To tackle the lack of supervision in unsupervised dependency parsing, some previous work considers learning models of multiple languages simultaneously Liu et al., 2013;Jiang et al., 2019;Han et al., 2019b). Ideally, these models can learn from each other by identifying shared syntactic behaviors of different languages, especially those in the same language family. For example,  propose to utilize the similarity of different languages defined by a phylogenetic tree and learn several dependency parsers jointly. Han et al. (2019b) propose to learn a unified multilingual parser with language embeddings as input. Jiang et al. (2019) propose to guide the learning process of unsupervised dependency parser from the knowledge of another language by using three types of regularization to encourage similarity between model parameters, dependency edge scores, and parse trees respectively.", "filtered_refids": [["b21", "b28", "b35"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 890, "num_references": 3}
{"corpusid_sectionid": "222133962-s22", "title": "A Survey of Unsupervised Dependency Parsing", "date": "2020-10-04", "section_title": "Utilization of Syntactic Information in Pretrained Language Modeling", "section": "Pretrained language modeling (Peters et al., 2018;Devlin et al., 2019;Radford et al., 2019), as a new NLP paradigm, has been utilized in various areas including question answering, machine translation, grammatical error correction, and so on. Pretrained language models leverage a large-scale corpus for pretraining and then small data sets of specific tasks for finetuning, reducing the difficulty of downstream tasks and boosting their performance. Current state-of-the-art approaches on supervised dependency parsing, such as Zhou and Zhao (2019), adopt the new paradigm and benefit from pretrained language modeling. However, pretrained language models have not been widely used in unsupervised dependency parsing. One major concern is that pretrained language models are too informative and may make unsupervised models more prone to overfitting. Besides, massive syntactic and semantic information is encoded in pretrained language models and how to extract the syntactic part from them is a challenging task.", "filtered_refids": [["b12", "b47", "b46", "b64"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1015, "num_references": 4}
{"corpusid_sectionid": "219177284-s2", "title": "Conversational Machine Comprehension: a Literature Review", "date": "2020-06-01", "section_title": "What is Conversational Machine Comprehension?", "section": "The task of CMC is defined as: Given a passage P , the conversation history in the form of questionanswer pairs {Q 1 , A 1 , Q 2 , A 2 , ..., Q i\u22121 , A i\u22121 } and a question Q i , the model needs to predict the answer A i . The answer A i can either be a text span (s i , e i ) (Choi et al., 2018) or a free-form text {a i,1 , a i,2 , ..., a i,j } with evidence R i (Reddy et al., 2019). Single-turn MRC models cannot directly cater to CMC, as the latter is much more challenging to address. The major challenges being:\n\n\u2022 The encoding module needs to encode not only P and A i but also the conversational history.\n\n\u2022 General observation about information-seeking dialog in humans suggests that the starting dialogturns tend to focus on the beginning chunks of the passage and shift focus to the later chunks as the conversation progresses (Choi et al., 2018). The model is thus expected to capture these focal shifts during a conversation and reason pragmatically, instead of only matching lexically or via paraphrasing.\n\n\u2022 Multi-turn conversations are generally incremental and co-referential. These conversational dialogs are either drilling down (the current question is a request for more information about the topic), shifting topic (the current question is not immediately relevant to something previously discussed), returning topic (the current question is asking about a topic again after it had previously been shifted away from), clarification of topic, or definition of an entity (Yatskar, 2019). The model should, therefore, be able to take context from history which may or may not be immediate.", "filtered_refids": [["b6", "b36"], [], ["b6"], ["b42"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1609, "num_references": 4}
{"corpusid_sectionid": "219177284-s4", "title": "Conversational Machine Comprehension: a Literature Review", "date": "2020-06-01", "section_title": "CoQA", "section": "Conversational QA (CoQA) dataset consists of 126k questions sourced from 8k conversations.\n\n\u2022 Dataset preparation: Conversations are prepared over passages collected across 7 different domains, each with its source dataset, such as news articles derived from CNN (Hermann et al., 2015). Amongst the 7 domains, two are used for out-of-domain evaluation (only for evaluation, not training), while the other five aid in-domain evaluation (both training and evaluation). The dialog is prepared in a two annotator setting with one questioning and another answering, both referring to the entire context.\n\n\u2022 Questions: Questions are factoid but require sufficient co-referencing and pragmatic reasoning (Bell, 1999).\n\n\u2022 Answers: Answers are free-form, with their corresponding rationale highlighted in the passage. However, Yatskar (2019) identified that the answers are slightly modified versions of the rationale, and therefore optimizing an extractive model to predict the answer span with maximum F1 overlap to the gold answer can achieve up to 97.8 F1.\n\n\u2022 Dialog features: The dialogs mostly involve drilling-down for details (about 60% of all questions) but lack other dialog features like topic-shift, clarification, or definition.\n\n\u2022 Evaluation: Macro-average F1 score of word overlap is used as an evaluation metric and is computed separately for in-domain and out-of-domain.", "filtered_refids": [[], [null], ["b2"], ["b42"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1378, "num_references": 3}
{"corpusid_sectionid": "219177284-s6", "title": "Conversational Machine Comprehension: a Literature Review", "date": "2020-06-01", "section_title": "Generic Framework of a CMC Model", "section": "(2) reasoning in the neural space to identify the answer vector and (3) decoding the answer vector into a natural language output. Huang et al. (2018a) adapted these steps in CMC by adding conversational history modeling. Qu et al. (2019c) proposed a ConvQA model with separate modules for history selection and modeling. Based on these prior works, we synthesize a generic framework for a CMC model. A typical CMC model is provided with context C, current question Q i and the conversation history\n\n, and needs to generate an output set O i . The CMC framework is provided in Fig. 1. There are four major components of the framework, based on their contribution to the overall CMC flow.\n\n1. History Selection module: With complicated dialog behaviors like topic shift or topic return (Yatskar, 2019), simply selecting immediate turns may not work well. A history selection module, therefore, chooses a subset H i of the history turns H i based on a policy (dynamic or static) that is expected to be more helpful than the others. If the history selection module is based on a dynamic learned policy (e.g. Qu et al. (2019b)), then feedback from the other modules can guide its update.", "filtered_refids": [["b15", "b33"], [], ["b42", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1183, "num_references": 4}
{"corpusid_sectionid": "219177284-s7", "title": "Conversational Machine Comprehension: a Literature Review", "date": "2020-06-01", "section_title": "Encoder:", "section": "The lexical tokens of the context passage C, selected conversational turns H i , and the current question Q i need to be transformed into input embeddings for the reasoning module. Encoder facilitates this transition. The encoder steps may vary with every approach and reasoning inputs, at a high level, encoding involves transformation and combination of context-independent word embeddings called lexical embeddings such as GloVE (Pennington et al., 2014), intra-sequence contextual  Figure 1: Generic framework of a CMC model. A typical CMC model would consist of (1) History selection module, that selects a subset H i of conversational history H i relevant to the current question Q i ;\n\n(2) Encoder, that encodes the lexical tokens of context C, Q i and H i into input embeddings for contextual integration layer; (3) Reasoning module, that performs contextual integration of input embeddings into contextualized embeddings; and finally, (4) Output predictor, that predicts the output set O i based on contextualized embeddings. embeddings e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) or RNN, question-aware embeddings, and additional feature embeddings like POS tags , history embedding (Qu et al., 2019c) or conversation count. Conversational history H i is generally integrated with this module into any or all of the contextual input embeddings. This process is called History modeling and is the most significant aspect of a CMC encoder.\n\n3. Contextual Integration layer: Contextual information accumulated in the passage, query, and/or history embeddings individually must be fused to generate query-aware and/or history-aware contextualized output embeddings. This process may involve a single layer (single-step reasoning) or repetition across multiple layers (multi-step reasoning). Input for this module generally consists of two (or more) sequence sets for every history turn, or aggregated across all turns, which are then fused in each layer and often inter-weaved (Huang et al., 2018b) with attention.", "filtered_refids": [["b28"], ["b33", "b7", "b29"], ["b16"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2036, "num_references": 5}
{"corpusid_sectionid": "219177284-s11", "title": "Conversational Machine Comprehension: a Literature Review", "date": "2020-06-01", "section_title": "Trends in History Modeling", "section": "How conversational history is integrated or used in the encoding process of contextual input embeddings can be used to classify CMC models. Different trends observed in this respect are described below. Some models may use a combination of these approaches.\n\n1. Appending selected history questions and/or answers (in raw form or text span indices) to the current question before encoding. QA tokens across turns should be distinguishable or separated when appending. Models DrQA+PGNet (Reddy et al., 2019), SDNet  and RoBERTa + AT + KD (Ju et al., 2019) append all history QA pairs separated by tokens like symbols\n\nOn the other hand, QuAC baseline model BiDAF++ w/ 2-ctx (Ohsugi et al., 2019) and GraphFlow  append only the history questions to the current question and encode relative dialogturn number within each question embedding to differentiate. Choi et al. (2018) validate that this dialog-turn encoding strategy performs better in practice.\n\n2. Encoding context tokens with history answer marker embeddings (HAE) before passing on for reasoning. These embeddings indicate if the context token is present in any conversational history answer or not, such as in BiDAF++ w/ 2-ctx (Choi et al., 2018), GraphFlow , BERT+HAE (Qu et al., 2019a) and HAM (Qu et al., 2019b). HAM encodes a dialog-turn encoded variant of HAE called Positional HAE. It maintains a lookup table of history embeddings for every relative position from the current conversation and embeds the corresponding embedding if the token is found in that history answer, e.g. for the current question q k if a token is found in history answer a k\u22122 then Positional HAE embedding at index 2 is encoded, otherwise embedding at index 0 is encoded. This setting is illustrated in Fig. 3b.\n\n3. Integrating intermediate representations generated in the reasoning modules of selected history conversation turns to grasp the deep latent semantics of the history, rather than acting on raw inputs. This approach is also called the FLOW based approach. The models that follow this approach are FlowQA (Huang et al., 2018a), FlowDelta (Yeh and Chen, 2019), and GraphFlow . GraphFlow encodes conversational histories into context graphs which are used by the reasoning module for contextual analysis.\n\nFor contextual encoding, most of the models utilize one of the two types of encoders: (a.) Bidirectional sequential language models such as BiDAF (Seo et al., 2017) or ELMo (Peters et al., 2018) (b.) Deep bidirectional transformer-based models such as BERT (Devlin et al., 2019) or RoBERTa .", "filtered_refids": [[], ["b18", "b36"], ["b6", "b25"], ["b6", "b31", "b32"], ["b15", "b43"], ["b38", "b29", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2552, "num_references": 12}
{"corpusid_sectionid": "219177284-s12", "title": "Conversational Machine Comprehension: a Literature Review", "date": "2020-06-01", "section_title": "Trends in Contextual Reasoning", "section": "While every CMC model has its unique flavor in integrating encoded representations of the query, history, and text contextually, some recurrent themes in reasoning can still be drawn. It is important to note that some of these themes will reflect state-of-the-art techniques around their release, which may now be obsolete. However, having their knowledge would prevent the re-exploration of those ideas. Following are the commonly observed themes:\n\nA. Attention-based Reasoning with Sequence Models This was a common theme across MRC models until transformers (Vaswani et al., 2017) were introduced and got rid of sequence modeling. Consequently, initial baseline models were based on this approach. CoQA baseline (Reddy et al., 2019) first involves DrQA (Chen et al., 2017), which performs BiLSTM based contextual integration over encoded tokens for extractive span, and later PGNet, that uses attentionbased neural machine translation (Bahdanau et al., 2015) for abstractive answer reasoning. QuAC baseline (Choi et al., 2018) combines self-attention with BiDAF (Seo et al., 2017) that performs reasoning via multi-layered bidirectional attention followed by multi-layered BiLSTM (BiDAF++). SDNet  applies both inter-attention and self-attention in multiple layers, interleaved with BiLSTM, to comprehend conversation context.", "filtered_refids": [[], ["b6", "b39", "b36", "b38", "b0", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1329, "num_references": 6}
{"corpusid_sectionid": "219177284-s15", "title": "Conversational Machine Comprehension: a Literature Review", "date": "2020-06-01", "section_title": "C. Contextual Integration using Pre-trained Language Models", "section": "Large-scale pre-trained LMs such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018) and RoBERTa , have become the current state-of-the-art approaches for contextual reasoning in CMC models, with leaderboards of both datasets stacked with these models or their variants. The approach is based on the fine-tune BERT-based MRC modeling outlined by Devlin et al. (2019), in which question and context are packed together (with marker embeddings to distinguish) in an input sequence to BERT that outputs contextualized question-aware embeddings for each input token.\n\nUsing pre-trained models for reasoning is advantageous in two aspects: Firstly, it simplifies the architecture by fusing encoding and reasoning modules into a single module. Secondly, it provides a ready-to-tune architecture that abstracts out complex contextual interactions between query and context while providing sufficient flexibility to control interactivity via augmentation of input embeddings i.e. concatenation of special embeddings to input tokens that signal the model to incorporate a desirable characteristic in contextualization.\n\n(a) HAM uses a dynamic attention-based history selection policy. Contextualized representations are generated by the model's encoder (BERT+PosHAE) for every history turn at word and sequence levels. Sequence-level embeddings are used to compute attention weights via scaled-dot product, and aggregate representations are generated by a weighted combination of embeddings of each turn in the proportion of their attention weights. Thus, attention weights help in determining the degree of selection (relevance) of each history turn.\n\n(b) HAM's BERT based Encoder (Reasoning Architecture) for every conversation turn. The encoder is provided with input sequence consisting of query tokens (yellow) and context tokens (green) separated by [SEP]. It outputs contextualized representations Ti corresponding to aligned question/passage tokens. The Token embeddings are augmented with segment embeddings(to differentiate query and context), positional embeddings (for distinct position in the sequence), and Positional HAE embeddings (for encoding history answer and relative conversational turn). Figure 3: Illustration of (a) history selection module and (b) encoder/reasoning module of History Attention Mechanism (HAM) model (Qu et al., 2019b).\n\nHowever, incorporating history into these models is a key challenge in this approach as most of the transformer models such as BERT only accept 2 segments ids in the input sequence. Based on recent research in CMC, two main trends in solving the history integration issue are discussed below:\n\n1. Modify the input embeddings for a single-turn MRC model to incorporate history. This is done by either appending the entire conversation to the question, such as Ju et al. (2019) which uses RoBERTa ) as the base model and truncates query if it exceeds the limit, or add special embeddings to highlight conversational history for the model, such as HAE (Qu et al., 2019a) embeds history answer embeddings with each context token if it is present in any of the history turns (detailed in section 6.2). This approach does not effectively use the model to capture interactions between every dialog-turn and context.\n\n2. Use separate model for each conversational turn to capture one-to-one interaction between history and context, and merge the per-turn contextualized embeddings into aggregated history-aware embeddings. Two models follow this trend. Ohsugi et al. (2019) uses BERT models to capture contextual interaction for every question (history and current) and answer (2N+1 sequences for N turns) and concatenates all sequences together. Finally, it runs Bi-GRU (Cho et al., 2014) over the aggregated sequence to capture inter-turn interactions before sending for prediction. On the other hand, HAM (Qu et al., 2019b) ignores the history questions and uses the current question as a query with positional History Answer Embeddings (section 6.2), thus generating one output sequence per conversation turn. Fig. 3b illustrates HAM encoder. The final sequence is generated using token-level soft-attention based aggregation across all per-turn contextualized sequences.", "filtered_refids": [["b34", "b7"], [], [], [null, "b32"], [], ["b31", "b18"], ["b25", "b5", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4227, "num_references": 9}
{"corpusid_sectionid": "219177284-s20", "title": "Conversational Machine Comprehension: a Literature Review", "date": "2020-06-01", "section_title": "Conversation setting", "section": "Questioner-Answerer setting where both have access to the entire context.\n\nTeacher-Student setting where the teacher has access to the full context for answering, while the student has only the title and summary of the article.   (Reddy et al., 2019) and QuAC (Choi et al., 2018) based on different characteristics defined in their papers and the analysis paper by Yatskar (2019). Figure 4: Architecture of the Reasoning Layer of GraphFlow. Context graph-based flow sequence is processed using GNNs and alternated with bi-LSTM and co-attention mechanisms. Source :  (a) A QA dialog example in the CoQA dataset. Every dialog is based on a context and each turn of the dialog contains a question (Qi), an answer (Ai) and a rationale (Ri) that supports the answer. There is sufficient co-referencing between dialog turns as seen in this example -'Where' in Q2 follows on the candidature mentioned in Q1, 'his' in Q4 points to A3, 'he' in Q5 references A4, and 'them' in Q6 refers to people mentioned in both A3 and A4. Source: (Reddy et  The questions are open-ended due to the asymmetric nature of dataset. There is also sufficient co-referencing -'she' in Q3 refers to the protagonist and is a succession of Q2, similarly Q7 is a follow-up on Q5, 'it' in Q6 refers to song mentioned in A5. C A summary of the common CMC models", "filtered_refids": [[], [null, "b6", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1325, "num_references": 3}
{"corpusid_sectionid": "227230405-s1", "title": "A Survey on Approaches to Computational Humor Generation", "date": 2020, "section_title": "Humor Generation Systems", "section": "Humor generation can be viewed as a special case of automatic text generation. Accordingly, the existing systems can be categorized as belonging to one of two major approaches to text generation: Templates and neural networks. Until Yang and Sheng's publication on an LSTM RNN for joke production in 2017, all of the previous published systems were template-based. While the early template-based systems had to be equipped with handcrafted lexicons, more recent systems have a variety of external lexical resources as source of material for generation.\n\nIt is also worth mentioning that the nature of the humorous texts produced by different systems is rather diverse. While a number of systems was focused on the generation of question-answer jokes (Raskin and Attardo, 1994;Binsted and Ritchie, 1994;Ritchie et al., 2007;Sj\u00f6bergh and Araki, 2008;Hong and Ong, 2009;Labutov and Lipson, 2012), others aimed at creating narrative jokes (Sj\u00f6bergh and Araki, 2009;Yang and Sheng, 2017;Yu et al., 2018). Furthermore, three systems generate humor through lexical replacement, in acronyms (Stock and Strapparava, 2005), proverbs (Sj\u00f6bergh and Araki, 2008) or in SMS (Valitutti et al., 2016), and one system creates witty analogies (Petrovi\u0107 and Matthews, 2013).\n\nIn the following, we will give an overview of existing humor generation systems. Since neural networks have recently achieved state-of-the-art results on many tasks in natural language processing, we will start with the group of systems that apply them for the creation of jokes. This group, however, contains only two systems, whereas the vast majority (ten systems) belongs to the group of templatebased approaches.", "filtered_refids": [[], ["b15", "b51", "b28", "b50", "b35", "b32", "b41", "b45", "b19", "b38", "b37", "b3"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1674, "num_references": 12}
{"corpusid_sectionid": "227230405-s4", "title": "A Survey on Approaches to Computational Humor Generation", "date": 2020, "section_title": "Ontologies for variable selection", "section": "Early generators make use of handcrafted ontologies that are tailored to the specific system. This includes the Light Bulb Joke Generator (LIBJOG, Raskin and Attardo, 1994) that produces jokes of the pattern How many <group name> does it take to screw in a light bulb? <NumberX>. One to <activity 1> and <numberY> to <activity2>. LIBJOG uses a lexicon of social groups and their stereotypical activities to produce jokes such as 'How many Californians does it take to change a light bulb? Twelve. One to screw it in and eleven to share in the experience.'. Over several phases from LIBJOG-1 to LIBJOG-4, the system became incrementally more complex, allowing for more activity fields and less restricted number relations. Binsted and Ritchie (1994) follow a similar approach by basing their punning riddle generator on a manually edited lexicon that stores lexical relationship information, such as synonymy, hyponymy and associated verbs and the phonetical feature homophony. Their system called JAPE-1 uses question-answer templates, as for example What is <adjective> and <verb>?-<noun phrase>. With <noun phrase> as user input, JAPE fills the two remaining slots based on the words' homophony, exploiting phonological ambiguity to induce humorousness. An example of such a joke is: 'What's green and bounces? A spring cabbage!'. Figure 1 illustrates the mechanism of that joke. In total, JAPE has 15 joke templates with schemes.\n\nAnother line of systems uses freely-available ontologies and databases. Resources used by the existing systems are WordNet (Princeton University, 2010) for lexical relationships, ConceptNet (Speer et al., 2017) for semantic relationships and UniSyn (Fitt, 2002) or CMU pronouncing dictionary (Lenzo, 2007) for phonetical relationships. Furthermore, systems make use of thematic word lists for slang words or profanity. With STANDUP, Ritchie et al. (2007) present an extension of JAPE that has a user interface makes use of WordNet and UniSyn instead of the handcrafted lexicon.\n\nHAHAcronym (Stock and Strapparava, 2005) is a system that writes out common acronyms in a humorous manner. For example, the acronym FBI (Federal Bureau of Investigation) becomes Fantastic Bureau of Intimidation. HAHAcronym makes use of WordNet, augmented with domain labels for the entries and the CMU pronunciation dictionary to take into account word rhymes and rhythms. The systems parses a given acronym to detect the syntactical form and the highest-ranking noun phrase, which remains unchanged. The other words get substituted while preserving the initial letter, the word class as well as the overall rhyme and rhythm. A substitution is chosen by exploiting semantic field opposition.\n\nSj\u00f6bergh and Araki's (2009) ambiguous compound generator also leverages WordNet to create jokes, this time however focusing less on inter-word relationships, but more on the provided definitions and example sentences. The system consists of two modules, creating two types of jokes, each making use of possibly ambiguous compound nouns. The first module creates jokes with the template 'I saw a <noun1> <noun2>. She (<noun1>) <WordNet example sentence>.' For the two noun slots, the system selects compound nouns from WordNet where both parts also occur as single entries, the first as noun and the second as noun and verb. For the verb, one of the example sentences is extracted in order to complete the template to a joke such as 'I saw a fish stick. She (the fish) stuck her thumb in the crack. ' The audience initially assumes that fish stick is a composite, as this is the more obvious interpretation. However, this expectation is disrupted in the second sentence, when the rear part of the supposed composite turns out to be a verb. The first sentence is now reanalyzed, and the incongruity is resolved. The Figure 1: Illustrated example of the joke mechanism in JAPE. From: Binsted and Ritchie, 1994 second type of joke Sj\u00f6bergh an Araki's system produces, follows the template '<WordNet definition <noun1> <noun2>>.' For this, compounds are selected when they contain at least one word that is included in WordNet itself, and by changing one letter that word must become another word present in WordNet. This compound is combined with one entry that has the original compound in its WordNet definition. From the exemplary compound 'god of war' this would render 'Ares: (Greek mythology) Greek god of car'. To reduce the number of jokes, the authors implement a measure of funniness and only output jokes above a certain threshold. The measure is calculated by the relation of the frequency of the selected compound between a joke corpus and a non-humorous corpus.\n\nHong and Ong's (2009) system uses WordNet's relation synonymy and UniSyn's pronunciation information alongside with any semantic relationship retrieved from ConceptNet. Their system creates punning riddles similar to Binsted and Ritchie's JAPE. This time, however, the algorithm is designed to first learn question-answer joke patterns from examples and then create new jokes according to the patterns. To create joke patterns, the algorithm called T-PEG marks nouns, adjectives and verbs in every joke from a POS-tagged joke corpus as candidate variables. For every tuple of candidate variables in a joke, T-PEG determines the lexical, phonetical or semantical relationship between the two variables. Those candidate variables with at least one relationship with another candidate are the final variables in the learned template. For example, from the source pun 'Which bird can lift the heaviest weights? The crane.', T-PEG extracts the sentence template 'Which <X1> can <X3> the heaviest <X6>? The <Y1>.' and the word relationships 'X1 ConceptuallyRelatedTo X6', 'X6 ConceptuallyRelatedTo X1', 'Y1 IsA X1', 'X6 CapableOfReceivingAction X3', 'Y1 CapableOf X3' and 'Y1 UsedFor X3'. From the 39 templates the algorithm extracted, the authors found 27 (69 %) to be usable. To generate jokes, T-PEG uses the library of patterns and WordNet, Unisyn and ConceptNet to find fitting words for the slots and a keyword input from the user as a starting point.", "filtered_refids": [["b32", "b3"], ["b11", "b39", "b20", "b35"], ["b41"], [null, "b3"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 51, "num_chars": 6130, "num_references": 9}
{"corpusid_sectionid": "227230405-s7", "title": "A Survey on Approaches to Computational Humor Generation", "date": 2020, "section_title": "Natsu to ieba, natsukashii Nose de, kyuuyuu to waiwai! (Speaking of summer, it will be fun to meet some old friends in my beloved Nose [a town near Osaka]!)", "section": "Similar to Stock and Strapparava's (2005) HAHAcronym, Valitutti et al. (2016) generate humorous texts by word substitution. They substitute a single word in a given short text message (SMS) that serves as a template. Their 2016 study is an enhanced version of a 2013 paper by the same authors (Valitutti et al., 2013). They assume different kinds of lexical constraints that determine the funniness of a substitution: (1) Taboo: The substitution is a taboo word (dirty word e.g.) or connoted with a taboo. (2) Coherence: The substitute forms a coherent compound with its neighbors. (3) Position: The substitution takes place among the last words of the text. (4) Form: The substitute is phonetically or orthographically similar to the original word. They use WordNet, the CMU pronunciation dictionary, the Google n-gram corpus after 1990, two online dictionaries of slang words and an example list of funny autocorrection mistakes from the internet, to find suitable candidates and check them for compliance with the constraints. As input for the manipulation they use an SMS Corpus. An example of a joke that leverages the constraints form and position is 'Tmr u going to school? I meet u in pool?' (school/pool replacement).", "filtered_refids": [["b45", "b41", "b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1226, "num_references": 3}
{"corpusid_sectionid": "227230405-s8", "title": "A Survey on Approaches to Computational Humor Generation", "date": 2020, "section_title": "Evaluation of Humor Generation Systems", "section": "In order to compare the existing systems to each other, some basic measures for the quality of humor generation systems are needed. Although a multitude of systems has been developed so far, a standardized methodology of assessing the quality of such systems is still missing. Most previous systems chose the humorousness of the output as an evaluation criterion. The majority of approaches was evaluated by means of user studies where actual humans rated the generated jokes on a Likert scale that typically ranges from 0 (not a joke) to 5 (really funny). The first authors to introduce this method were Binsted and Ritchie (1994) and many followed their example, such as Sj\u00f6berg and Araki (2008)  However, in most of the papers it remains unclear how the samples for evaluation were selected. Petrovic and Matthews (2013) for example disclose that they hand-picked the best jokes for the human assessment. Obviously, it matters a lot whether the average humor of a system is calculated on the basis of random jokes or on the basis of jokes that were pre-selected by the authors.\n\nThe main challenge in the design of humor evaluation measures is that humor is highly subjective and what makes one person laugh might not have the same effect on another person. Winters et al. (2018) show that the funniness judgements of jokes differ significantly among the test persons. Circumventing this issue, Valitutti (2011) proposes the HF (humorous frequency) as measure for the evaluation of computational humor generators. He defines HF as the fraction of funny items in a randomly generated set of jokes. Any item is either funny or not funny, i.e. in this context humorousness has a Boolean value rather than a score. To calculate the HF of a system, Boolean humorousness ratings have to be assigned to every item of a randomly selected set of jokes generated by the system. However, how a single joke is assessed remains highly subjective. Binsted and Ritchie (1997) suggest to classify a computer-generated joke as successful when people cannot tell it apart from a human-generated joke, funniness left aside. Yu et al. (2018) conduct such an evaluation. The authors design what they call a Soft Turing Test to let test persons rate the humanness of the jokes generated by their system on a three-score Likert-scale (2-definitely human/0-definitely machine).\n\nWhat all these approaches have in common, however, is that they do not consider the humor capability of the systems as a whole and rather evaluate individual jokes, or, as in the case of the HF, derive an overall evaluation of the system from the scores of the individual jokes. Consequently, the diversity of the produced material is completely disregarded in these evaluations. This overlooks the fact that a system that produces a row of almost similar jokes would not be very enjoyable for a human. Taking this factor into account, the humor capability of a system with a low complexity, i.e. a system that produces similar jokes with almost the same wording and the same production principle, should be rated lower than a system that produces a variety of jokes and joke mechanisms.\n\nTo our knowledge, none of the evaluation procedures yet has attempted to apply objective criteria to the decision of whether a given piece of text is humorous or not. However, from the insights of humor theory and research, criteria for what constitutes a joke can be concluded. Consequently, a useful measure would be not if a text is humorous or how funny a joke is, but rather whether a text is identifiable as joke or not, taking into account objective criteria.", "filtered_refids": [["b3"], ["b48", "b51", "b44", "b4"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 3613, "num_references": 5}
{"corpusid_sectionid": "7384097-s2", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "Phrase-Based SMT", "section": "Phrase-based SMT (PSMT) is the currently dominant approach in string-based SMT. PSMT ruled out the early word-based SMT framework (Brown et al. 1990(Brown et al. , 1993Berger et al. 1996) thanks to two important novelties: the use of multi-word translation units (Och 1999;Zens, Och, and Ney 2002;Koehn, Och, and Marcu 2003), and the move from a generative to a discriminative modeling framework . The search process (1) in PSMT is guided by the target string e, built from left to right, and the alignment variable b that embeds both segmentation and reordering of the source phrases. This is defined as\n\nsuch that K 1 , . . . , K I are consecutive intervals partitioning the target word positions, and J 1 , . . . , J I are corresponding but not necessarily consecutive intervals partitioning the source word positions. A phrase segmentation for our running example is shown in Figure 2. The use of phrases mainly results in a better handling of ambiguous words and many-to-many word equivalences, but it also makes it possible to capture a considerable amount of local reordering phenomena within a translation unit (intra-phrase 3 Automatic measures of translation quality are discussed in Section 3. ", "filtered_refids": [["b115", "b6", "b161", "b21", "b22", "b95"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1205, "num_references": 6}
{"corpusid_sectionid": "7384097-s6", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "n-gram Based SMT", "section": "n-gram based SMT (Casacuberta and Vidal 2004;Mari\u00f1o et al. 2006) is a string-based alternative to PSMT. In this framework, smoothed n-gram models are learned over sequences of minimal translation units (called tuples), which, like phrase pairs, are pairs of word sequences extracted from word-aligned parallel sentences. Tuples, however, are typically shorter than phrase pairs and are extracted from a unique, monotonic segmentation of the sentence pair. Thus, the problem of spurious phrase segmentation is avoided but non-local reordering becomes an issue. For instance, in Figure 2, a monotonic phrase segmentation could be achieved only by treating the large block [jdd ... AlsAds]-[The ... renewed] as a single tuple. Reordering is then addressed by \"tuple unfolding\" (Crego, Mari\u00f1o, and de Gispert 2005): that is, during training the source words of each translation unit are rearranged in a target-like order so that more, shorter tuples can be extracted. At test time, input sentences have to be pre-ordered for translation. To this end, Crego and Mari\u00f1o (2006) propose to precompute a number of likely permutations of the input using POS-based rewrite rules learned during tuple unfolding. The reorderings thus obtained are used to extend the search graph of a monotonic decoder. 8 Reordering is often considered as a shortcoming of n-gram-based SMT as reordering decisions are largely decoupled from decoding and mostly based on source-side information.", "filtered_refids": [["b105", "b38", "b39", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1464, "num_references": 4}
{"corpusid_sectionid": "7384097-s8", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "Syntax-Based", "section": "SMT. An important motivation for using syntax in SMT is that reordering among natural languages very often involves the permutation of whole syntactic constituents (e.g., Fox 2002). For instance, in our running example (Figure 2), knowing the span of the Arabic subject would be enough to predict the reordering of the verb for translation into English.\n\nSyntax-based SMT encompasses a variety of frameworks that use syntactic annotation either on the source or on the target language, or both. So-called tree-to-string methods (Huang, Knight, and Joshi 2006;Liu, Liu, and Lin 2006) use a given input sentence parse tree to restrict the application of translation/reordering rules to word spans that coincide with syntactic constituents of specific categories. For instance, the swap of Alr}ys Alfrnsy may only be dictated by a rule applying to noun phrases composed of a noun and an adjective. On the other hand, string-to-tree methods (Yamada and Knight 2002;Galley et al. 2004;Marcu et al. 2006;Shen, Xu, and Weischedel 2010) use syntax as a way to restrict translation hypotheses to well-formed target language sentences-ruling out, for instance, a translation that fails to reorder the translated verb renewed with respect to its subject. Using syntax on both source and target sides (treeto-tree) (Imamura, Okuma, and Sumita 2005;Ding and Palmer 2005;Smith and Eisner 2006;Watanabe, Tsukada, and Isozaki 2006;Zhang et al. 2008) has proven rather difficult in practice due to the complexity of aligning potentially very different tree topologies and to the large size of the resulting translation grammars. Moreover, the need for highquality parsers in both language sides seriously limits the applicability of this approach.\n\nSyntax-based SMT approaches also differ in the formalism they use to represent the trees. Those based on phrase structure (constituency) grammars typically comply with the principle that each translation/reordering rule should match a complete constituent, whereas those based on dependency grammars opt for a more flexible use of structure. For example, in string-to-dependency SMT (Shen, Xu, and Weischedel 2010) rules can correspond to partial constituents but must be either a single rooted tree, with each child being a complete sub-tree, or a sequence of siblings, each being a complete subtree. Partial dependency rules are then combined during decoding, which means that not all reordering decisions are governed by the translation model.\n\nAn even more flexible use of structure is advocated by the treelet-based SMT framework (Quirk, Menezes, and Cherry 2005), where translation rules can correspond to any connected subgraph of the dependency tree (i.e., treelet). As illustrated by Figure 4, treelet pairs are extracted from pairs of source dependency parse tree and target-side projected trees. Treelets can be seen as phrases that are not limited to sets of adjacent words, but rather to sets of words that are connected by dependency relations, which in turn make it possible to learn non-local reordering patterns. As reordering jdd AlEAhl Almgrby Almlk mHmd AlsAds dEm -h l-m$rwE Alr}ys Alfrnsy! ", "filtered_refids": [["b59"], ["b43", "b141", "b104", "b153", "b79", "b82", "b127", "b102", null, "b163", "b61"], ["b127"], ["b119"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 3144, "num_references": 14}
{"corpusid_sectionid": "7384097-s13", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "Main Pre-ordering Strategies.", "section": "A large number of pre-ordering strategies have been proposed. As a first classification, we divide them into deterministic, non-deterministic, and hybrid. Deterministic pre-ordering aims at finding a single optimal permutation of the input sentence, which is then translated monotonically or with a low distortion limit (Nie\u00dfen and Ney 2001; Xia and McCord 2004;Collins, Koehn, and Kucerova 2005;Popovi\u0107 and Ney 2006;Costa-juss\u00e0 and Fonollosa 2006;Wang, Collins, and Koehn 2007;   the opposite direction by placing Japanese syntactic heads in the middle is not a trivial problem. We utilize the Head-Finalization rules to generate intermediate head-finalized English sentences called Head-Final English (HFE) and decompose Japanese-to-English translation into 1) Japaneseto-HFE translation and 2) HFE-to-English postordering. We achieved significant improvements from baseline (phrase-based, hierarchical phrasebased, and string-to-tree) translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively, in the experiment of patent translation.\n\nThe remainder of this paper is organized as follows. Section 2 briefly reviews related studies on the reordering problem and another related technology called post-editing. Section 3 presents the proposed method in detail taking Japanese-to-English translation as a test case. Section 4 reports our experiments and discusses the results. Section 5 concludes this paper with our prospects for future work.", "filtered_refids": [["b34", "b118", "b139", "b148", "b36"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1458, "num_references": 5}
{"corpusid_sectionid": "7384097-s16", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "Linguistic", "section": "Knowledge-Based Pre-ordering. In these approaches, manually written rules determine the transformation of input syntax trees (Collins, Koehn, and Kucerova 2005;Wang, Collins, and Koehn 2007;Xu et al. 2009;Isozaki et al. 2010b;Yeniterzi and Oflazer 2010;Gojun and Fraser 2012;Andreas, Habash, and Rambow 2011) or the permutation of shallow syntactic chunks in a sentence (Hardmeier, Bisazza, and Federico 2010;Durgar El-Kahlout and Oflazer 2010;Bisazza, Pighin, and Federico 2012). In an early example of syntax-based pre-ordering, Collins, Koehn, and Kucerova (2005) propose a set of six rules aimed at arranging German sentences in English-like order. The rules address the position of verbs, verb particles, and negation particles, and they are applied to constituency parse trees. Following a similar approach, Gojun and Fraser (2012) develop a set of rules for the opposite translation direction (English-to-German). Xu et al. (2009) instead propose a simple set of dependency-based rules to pre-order English for translation into subject-object-verb (SOV) languages, which is shown to be effective for Korean, Japanese, Hindi, Urdu, and Turkish. Isozaki et al. (2010b) obtain even better results in an English-to-Japanese task using only one pre-ordering rule (i.e., head finalization) with a parser annotating syntactic heads.", "filtered_refids": [["b155", "b34", "b18", "b20", "b150", "b48", "b139", "b84", "b74", "b1"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1332, "num_references": 10}
{"corpusid_sectionid": "7384097-s17", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "Data-Driven", "section": "Pre-ordering. This kind of model is learned from sets of pairs (f, f ) where f is a source sentence and f is its reference permutation (pre-ordering) inferred from a reference translation e via a word-level alignment. 11 These approaches typically require some form of linguistic annotation of the source language, such as syntactic parse trees (Xia and McCord 2004;Habash 2007;Li et al. 2007;Elming and Habash 2009;Genzel 2010;Khalilov and Fonollosa 2011;Khalilov and Sima'an 2011;Yang et al. 2012;Lerner and Petrov 2013;Jehl et al. 2014), shallow syntax chunks (Zhang, Zens, and Ney 2007;Crego and Habash 2008), or POS labels Rottmann and Vogel 2007;Niehues and Kolss 2009;Tromble and Eisner 2009;Visweswariah et al. 2011).\n\nAmong the first examples of data-driven tree-based pre-ordering, Xia and McCord (2004) propose a method to automatically learn reordering patterns from a dependencyparsed French-English bitext, using a number of heuristics. While source-side parses are required by their method, target-side parses are optionally used to provide additional constraints during rule extraction. Habash (2007) extracts pre-ordering rules from an Arabic-English parallel corpus dependency-parsed on the source side. In both these works, pre-ordering rules are applied in a deterministic way to preprocess both training and test data. Following a discriminative modeling approach, Li et al. (2007) train a maximum-entropy classifier to pre-order each node with at most three children in the source constituency parse, using a rich set of lexical and syntactic features. Lerner and Petrov (2013) extend this work to pre-order nodes with more children (up to seven on either side of the head) using a cascade of classifiers: first, decide the order of each child relative to the head, then decide the order of left children and that of the right children. As training separate classifiers for each number of children is prone to sparsity issues, Jehl et al. (2014) build a single logistic regression model to predict whether any two sibling nodes should be swapped or not. Then, for each node in the tree, they search for the best permutation of all its children given the pairwise scores produced by the model, using a depth-first procedure. Yang et al. (2012) treat the permutation of each node's children as a ranking problem and model it with ranking support vector machines. As an alternative to deterministic pre-ordering, they also propose using the predicted source permutation to generate soft constraints for the SMT decoder: that is, a penalty that fires whenever the decoder violates the predicted pre-ordering. A tighter integration between source pre-ordering and source-to-target translation is proposed by Dyer and Resnik (2010). In their approach, optimal source pre-orderings (f') are treated as a latent variable in an end-to-end translation model and the parameters of the tree permutation model are learned directly from parallel data. At test time, alternative permutations of the input tree are encoded as a source reordering forest, which is then translated by a finite-state phrase-based translation model.\n\nExamples of pre-ordering based on shallow syntax include Zhang, Zens, and Ney (2007) and Crego and Habash (2008). In these approaches, automatically extracted chunk pre-ordering rules are used to generate a word reordering lattice of the input sentence, which is then translated by a monotonic phrase or n-gram-based decoder.\n\nIn Costa-juss\u00e0 and Fonollosa (2006), pre-ordering is learned by training a monolingual n-gram based SMT system at the level of word clusters. In Tromble and Eisner (2009), pre-ordering is cast as a permutation problem and solved by a model that estimates the probability of reversing the relative order of any two input words based on their distance as well as lexicalized and POS-based features. In a related work, Visweswariah et al. (2011) obtain smaller models and better results by learning the cost of a given input word appearing right after another, as opposed to anywhere after it (cf. source word-after-word reordering models described in Section 2.1).", "filtered_refids": [["b65", "b97", "b113", "b85", "b89", "b137", "b88", "b120", null, "b154", "b164", "b53", "b148", "b96", "b73", "b37"], ["b52", "b97", "b85", "b154", "b148", "b96", "b73"], ["b164", "b37"], ["b137"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 4125, "num_references": 26}
{"corpusid_sectionid": "7384097-s20", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "General-Purpose Metrics", "section": "BLEU (Papineni et al. 2001) is a lexical match-based score that represents the de facto standard for SMT evaluation. Here, proximity between candidate and reference translations is measured in terms of overlapping word n-grams, with n typically ranging from 1 to 4. For each order n a modified precision score (see Papineni et al. [2001] for details) is computed on the whole test set and combined in a geometric mean. The resulting score is then multiplied by a brevity penalty that accounts for length mismatches between reference and candidate translations. Al-Onaizan and Papineni (2006) use BLEU to measure word order similarity between two languages: that is, by computing the BLEU score between the original target sentence e and a source-like permutation of e. Using n-grams, though, is a limited solution to the problem of word ordering evaluation. First, because only exact surface matches are counted, without any consideration of morphology or synonymy. Second, because the absolute positioning of words in the sentence is not captured, but only their proximity within a small context.\n\nThe former issue is addressed to some extent by METEOR (Banerjee and Lavie 2005), which relies on language-specific stemmers and synonymy modules to go beyond the surface-level similarity. As for word order, METEOR treats it separately with a fragmentation penalty proportional to the smallest number of chunks that the hypothesis must be divided into to align with the reference translation. This quantity can be interpreted as the number of times that a human reader would have to \"jump\" between words to recover the correct translation order. However, no distinction is made between short and long-range reordering errors.\n\nThe weakness of BLEU and METEOR with respect to word order was demonstrated by Birch, Osborne, and Blunsom (2010) with a significant example that we report in Table 2. For simplicity, the example assumes that the reference order is monotonic and that hypotheses and reference translations contain exactly the same words. According to both metrics, hypothesis (a) is worse than (b), although in (a) only two adjacent words are swapped whereas in (b) the two halves of the sentence are swapped.", "filtered_refids": [["b0", "b117"], ["b4"], ["b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2218, "num_references": 4}
{"corpusid_sectionid": "7384097-s22", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "(a) (b)", "section": "Permutation distance metrics are calculated by comparing permutations extracted from a source sentence aligned to different target sentences. We invert the distance metrics by subtracting from one, so that an increase in the metrics represents an increase in the quality of word order. We define and discuss the different metrics below.\n\n-The Hamming Distance measures the number of disagreements between two permutations (Ronald 1998): relative order differs in the two permutations) normalized by the total number of ordered element pairs:\n\nBirch, Osborne, and Blunsom (2010) further suggest extract the square root of K to obtain a function that is more discriminative on lower distance ranges, (i.e., for translations that are closer to the reference word ordering). Finally, the Kendall Reordering Score (KRS)-a positive measure of quality ranging from 0 to 1-is computed by subtracting the latter quantity from one, and by multiplying the result by a brevity penalty (BP) that accounts for length mismatches between reference and candidate translations:\n\nThe BP definition corresponds to that of BLEU (Papineni et al. 2001) with the difference that, for KRS, it is computed at the sentence level. In case of multiple references, the one that yields the highest score for each test sentence is retained. Finally, the average of all sentence-level KRS scores gives the global KRS of the test set. The linear interpolation of KRS and BLEU (LRscore) can be successfully used to optimize the feature weights of a PSMT system, leading to translation outputs that are preferred by human annotators according to Birch and Osborne (2011). In a related work, Bisazza and Federico (2013a) observe that some word classes, like verbs, are typically more important than others to determine the general structure of a sentence. Hence, they develop a word-weighted KRS variant that is more sensitive to the positioning of specific input words. Assuming that each input word f i is assigned a weight \u03bb i , the original KRS formula is modified as follows:\n\n\u03bb i +\u03bb j if \u03c0 i < \u03c0 j and \u03c3 i > \u03c3 j 0 otherwise\n\nFor their evaluation of long reordering errors in Arabic-English and German-English, Bisazza and Federico (2013a) set the weights to 1 for verbs and 0 for all other words to only capture verb reordering errors. The resulting metric, KRS-V, rates a translation hypothesis as perfect when the translations of all source verbs are located in their correct position, regardless of the ordering of other words. In a different approach called RIBES, Isozaki et al. (2010a) propose directly measuring the reordering occurring between the words of the hypothesis and those of the reference translation, thereby eliminating the need to word-align input and output sentence. A limitation of this approach is that only identical words contribute to the score. As a solution, the permutation distance is multiplied by a word precision score that penalizes hypotheses containing few reference words. Nevertheless, the resulting metric assigns different scores to hypotheses that differ in their lexical choice, but not in their word reordering. Talbot et al. (2011) introduce yet another reordering-specific metric, called fuzzy reordering score (FRS) which, like the KRS, is independent from lexical choice and measures the similarity between a sentence's reference reordering and the reordering produced by an SMT system (or by a pre-ordering technique). However, whereas Birch, Osborne, and Blunsom (2010) used Kendall's tau between the two sentence permutations, Talbot et al. count the smallest number of chunks that the hypothesis permutation must be divided into to align with the reference permutation. This corresponds precisely to the fragmentation penalty of METEOR except that the alignment is performed between permutations and not between translations. Like METEOR, FRS makes no difference between short and long-range reordering errors (cf. Table 2).\n\nStanojevi\u0107 and Sima'an (2014b) argue for a hierarchical treatment of reordering evaluation, where word sequences can be grouped recursively into larger blocks. To this end, they factorize the output-reference reordering into a Permutation Tree (Zhang and Gildea 2007), whose nodes represent atomic permutations. Given this factorization, the counts of monotone (1 2) versus other permutation nodes-(2 1), (3 1 4 2), and so onare used as features in a linear model of translation quality (BEER) trained to correlate with the human ranking of a set of MT system outputs. With reference to Table 2, the permutation trees of both hypotheses (a) and (b) would contain only one swapping node leading to the same reordering score. Stanojevi\u0107 and Sima'an (2014a) extend this work with a stand-alone reordering metric that considers all possible tree factorizations of a permutation (permutation forest) and that gives recursively less importance to lower nodes in the tree (i.e., covering smaller spans). Hierarchical permutation metrics are shown to better correlate with human judgments than string-based permutation metrics like Kendall's tau distance K.", "filtered_refids": [[], [], [], ["b16", "b9", "b117"], [], ["b16", "b136", "b83"], ["b130", "b162"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 5097, "num_references": 8}
{"corpusid_sectionid": "7384097-s23", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "Reordering Phenomena in Natural Languages", "section": "Understanding the complexity of reordering in a given language pair is key to selecting the right SMT models and to improving them. To date, word reordering phenomena in natural languages have mainly been analyzed from a quantitative perspective (Birch, Osborne, and Koehn 2008;Birch, Blunsom, and Osborne 2009). While measuring the amount of reordering is certainly important, understanding which kinds of reordering occur in a given language pair is also essential. To this end, we present a qualitative analysis of word reordering based on linguistic knowledge. More specifically, we draw on a large body of syntactic information collected by linguists from more than 1500 languages, and systematized in the World Atlas of Language Structures (WALS) (Dryer and Haspelmath 2011). 13 Following the seminal work of language typologist Matthew S. Dryer, we describe the word order profile of a language by the canonical orders of its constituent sets (word order features). The resulting language pair classification is primarily based on the order of subject, object and verb, and further refined according to the order of several other element pairs, such as noun-adjective, verb-negation, and so forth. We then compare the word order features of several languages that were studied in the SMT field, and show that empirical results generally confirm the existing theoretical knowledge.", "filtered_refids": [["b11", "b47", null, "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1387, "num_references": 4}
{"corpusid_sectionid": "7384097-s28", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "Clause-Level Order Features.", "section": "r Subject, Object, Verb [WALS feature 81A]\n\nThe first and most important feature is the \"ordering of subject, object, and verb in a transitive clause, more specifically declarative clauses in which both the subject and object involve a noun (and not just a pronoun)\" (Dryer 2011). For instance, English and French are SVO languages, whereas Turkish is SOV. The distribution of main word order types in a large sample of world languages is given in Table 4. This feature is often used alone to denote the word order profile of a language, because it can be a good predictor of several other features.\n\nr Oblique or Adpositional Phrase [WALS feature 84A] This feature refers to the position of a phrase functioning as an adverbial modifier of the verb, relative to the position of the object and verb. For instance, English is VOX because it places oblique phrases after the verb and object. Content questions are characterized by the presence of an interrogative word or phrase (e.g., who, which one). In some languages, like English, these are always placed at the beginning of the sentence. In some others, like Turkish, they take the position of the constituent they replace: For instance, the word 'ne/what' replacing the object naturally occurs between subject and verb.\n\nr Negation and Verb [WALS feature 143A] Order of the negative word or morpheme 15 with respect to the main verb. Note that more than one word or morpheme may be necessary to express negation (e.g., 'ne ... pas' in French). Order of demonstrative words (e.g., this, that) or affixes with respect to the noun they modify.", "filtered_refids": [[], ["b46"], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1595, "num_references": 3}
{"corpusid_sectionid": "7384097-s33", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "Word Order Differences", "section": "Linguistically motivated word order profiles can be very helpful anticipating the kind of word reordering problems that an SMT system will have to face. Clearly, these will also vary in relation to the text genre (written news, speeches, etc.) and to the translation's style and degree of literality. However, we can reasonably expect the syntactic properties of the two languages to determine the general reordering characteristics of that pair.\n\nWe will now analyze the reordering characteristics of seven language pairs: English paired with the other six languages presented in Table 3, as well as the French and Arabic pair. To this end, we propose the following analysis procedure. As a first indication of reordering complexity, we look at the main word order feature (subject, object, verb). A difference at this level typically results in poor SMT performances. Then, we count the total number of discordant features. To simplify, if a particular element does not exist in a language (e.g., polar question particles in English) we count zero difference for that feature, and when one of the languages has a mixed order we count a half difference. We insist, however, on the qualitative nature of our analysis: Numbers are only meaningful in combination with the list of specific discordant features, as these have a different impact on word reordering. In particular, we find it essential for SMT to distinguish between clause-level and phrase-level differences (CDiff and PDiff) because the former account for most longer-range word movements, and the latter for the shorter. Thus, a language pair with only phrase-level discordant features is likely to be suitable for a PSMT approach, where reordering is managed through local distortion or inside translation units. On the contrary, the presence of many clauselevel differences typically calls for a tree-based solution, either at preprocessing or at decoding time. As we will see, some pairs lie on the borderline, with only one or few clause-level differences. Finally, it should be noted that, even among features of the same group, some have more impact on SMT than others due to their frequency or to the average length of their constituents. For instance, the order of noun and genitive is more important than that of adjective and degree word.\n\nEnglish and German [ Main order: different; CDiff: 1.5; PDiff: 0.5 ]\n\nThe main word order of German is SVO or SOV, according to the syntactic context (cf. Section 4.2). German also differs from English with respect to the position of oblique phrases and that of the negation: Both are fixed in English but mixed in German. At the phrase level, German predominantly places the genitive after the noun, while English displays both orders. Thus, despite belonging to the same family branch (Indo-European/Germanic), this pair displays complex reordering patterns. Indeed, German-English reordering has been widely studied in SMT and is still an open topic. At the Workshop on Statistical Machine Translation 2014 (Bojar et al. 2014), a syntaxbased string-to-tree SMT approach (Williams et al. 2014) won in both language directions (official results excluding online systems). At the International Workshop on Spoken Language Translation 2014 (Cettolo et al. 2014), the best submission was a combination of PSMT with POS-and syntax-based preordering (Slawik et al. 2014), string-to-tree syntax-based SMT, and factored PSMT (Birch et al. 2014).\n\nEnglish and French [ Main order: same; CDiff: 0.5; PDiff: 1.5 ] Most clause-level features have the same values in French as in English, except for the negation, which is typically expressed by two words in French: one preceding and one following the verb. 17 At the phrase level, differences are found in the location of genitives and adjectives. Thus, English and French have very similar clause-level orders, but reordering is abundant at the local level. This is a case where reordering is mostly well handled by string-based PSMT. As a reference, the three top English-to-French WMT14 systems (official results excluding online systems) were all phrase-based. A similar trend was observed in the French-to-English track.\n\nEnglish and Arabic [ Main order: different; CDiff: 0.5; PDiff: 2.5 ]\n\nThe dominant Arabic order is VSO, followed by SVO (cf. Section 4.2). Apart from this important difference, all other clause-level features agree between Arabic and English. At the phrase level, differences are found in genitives, adjectives, and degree words. As a result, reordering is overwhelmingly local but few crucial long-range reorderings also regularly occur. Thus, this pair is challenging for PSMT but, at the same time, not well suited for a tree-based approach. As shown by Zollmann et al. (2008) and Birch, Blunsom, and Osborne (2009), PSMT performs similarly or better than HSMT for the Arabic-to-English language pair. However, HSMT was shown to better cope with the reordering of VSO sentences (Bisazza 2013). Pre-ordering of Arabic VSO sentences for translation into English has proved to be a particularly difficult task (Green, Sathi, and Manning 2009;Carpuat, Marton, and Habash 2010) and has inspired work on hybrid pre-ordering where multiple verb pre-orderings are fed to a PSMT decoder Andreas, Habash, and Rambow 2011); see also Section 2.4. [ Main order: different;CDiff: 5.5;PDiff: 1.5 ] Turkish is a good example of head-final language, except for the fact that it can use both clause-final and clause-initial subordinators. 18 As a result, almost all clause-level features are discordant in this pair. At the phrase level, Turkish mainly differs from English for the use of postpositions instead of prepositions. Among our language pairs, this is one of the most difficult to reorder for an SMT system. The complex nature of its reordering phenomena suggests a good fit for tree-based SMT approaches, and indeed, HSMT was shown to significantly outperform PSMT between Turkish and English in both language directions (Ruiz et al. 2012;Y\u0131lmaz et al. 2013). However, state-of-the-art SMT quality in this language pair is still very low, mostly because of the agglutinative nature of Turkish, which makes it difficult to tear apart word reordering issues from rich morphology issues. Attempting to address both issues in an English-to-Turkish factored PSMT system, Yeniterzi and Oflazer (2010) pre-process the parsed English side with a number of syntax-to-morphology mapping rules and constituent pre-ordering rules dealing with local and global reordering phenomena, respectively. Only the former, though, resulted in better translation quality.", "filtered_refids": [[], [], [], ["b26", "b143", "b9", "b19", "b128"], [], [], ["b155", "b12", "b156", "b121", "b9", "b71", null, "b23", "b165", "b1"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 46, "num_chars": 6626, "num_references": 15}
{"corpusid_sectionid": "7384097-s34", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "English and Turkish", "section": "English and Japanese [ Main order: different; CDiff: 6; PDiff: 1.5 ] Japanese is the prototypical example of head-final language. In this pair all clause-level features are discordant, whereas at the phrase level, Japanese differs from English for the use of postpositions and the strictly head-final genitive construction. This pair, like the previous one, is extremely challenging for PSMT because of the hierarchical nature of its reordering phenomena and the high frequency of long-range word movements. Indeed, translation between English and Japanese has spurred a remarkable amount of work on pre-ordering, post-ordering, and decoding-time reordering. In 2013 the PatentMT evaluation campaign of the NTCIR conference (Goto et al. 2013a) saw rule-based and hybrid systems largely outperform the purely statistical ones in Japanese-to-English. The highest-ranked SMT submission was actually a combination of three SMT systems, including a baseline PSMT method, a rule-based pre-ordering method, and a post-ordering method based on string-to-tree syntax-based SMT . Interestingly, the trends were different in the opposite translation direction, English-to-Japanese, where all rule-based MT systems were significantly outperformed by a PSMT system that performed pre-ordering of the English input with few manual rules for head finalization based on dependency parse trees .\n\nEnglish and Chinese [ Main order: same; CDiff: 3.5; PDiff: 1 ] Despite belonging to the same main order type, these two languages differ in the positioning of oblique phrases, relative clauses, interrogative phrases, and subordinating words. 19 Moreover, word order variations are quite common in Chinese to mark the topic of a sentence, (i.e., what is being talked about). Comparing the two languages at the phrase level, we find partial disagreement in the use of genitive and adpositions (Chinese has both prepositions and postpositions). Thus, this pair too is characterized by very complex reordering, hardly manageable by a PSMT system. This is confirmed by a number of empirical results showing that tree-based approaches (particularly HSMT) consistently outperform PSMT in Chinese-to-English evaluations (Zollmann et al. 2008;Birch, Blunsom, and Osborne 2009). It is worth noting that translation between Chinese and English has been the main motivation and test bed for the development of HSMT.\n\nFrench and Arabic [ Main order: different; CDiff: 1.5; PDiff: 1 ] At the clause level, this pair differs in main word order (SVO versus VSO or SVO) like the English-Arabic pair, but also in the order of negation and verb. On the other hand, phrase-level order is notably more similar, with only one discordant feature of minor importance (adjective and degree word). Less research was published on this language pair. Nevertheless, Hasan and Ney (2008) and Schwenk and Senellart (2009) chose a PSMT approach to experiment with an Arabic-to-French task. Figure 8 illustrates the reordering characteristics of three language pairs by means of sentence examples that were automatically word-aligned with GIZA++ (Och and Ney 2003) (intersection of direct and inverse alignments). In the first row, we see two English-German sentence pairs; in both cases, most of the points lie close to the diagonal representing an overall monotonic translation, whereas few isolated points denote the very long-range reordering of verbs. Similarly, in the two English-Arabic sentence pairs, we mostly observe local reorderings, with the exception of few isolated points corresponding to the Arabic clause-initial verbs. Finally, the two Turkish-English examples display global reordering, due to the high number of clause-level order differences.\n\nWhere possible, it is interesting to relate our analysis with previously published measures of reordering based on parallel data. To our knowledge, the most comprehensive results of this kind are reported by Birch (2011), who formulates reordering as a binary process occurring between two blocks that are adjacent in the source (cf. ITG constraints in Section 2.1). Here, the general amount of reordering in a language pair is estimated by the RQuantity, defined as the sum of the spans of all the reordered blocks on the target side, normalized by the length of the target sentence and averaged over a corpus. Based on the Europarl corpus (Koehn 2002) and automatic word alignments, Birch (2011) reports average RQuantity values of 0.586/0.608 in English-to-German/German-to-English, versus only 0.402/0.395 in English-to-French/French-to-English. The manually aligned GALE corpus (LDC2006E93) is instead used to measure the distribution of reordering widths, defined as the sum of the swapped blocks' target spans. Widths are binned into short (2-4 words), medium (5-8), and long (>8). In Chinese-to-English there are about 0.8/0.9/0.9 short/medium/long reordered blocks per sentence, whereas in Arabic-to-English there are 1.1/0.4/0.2 short/medium/long reordered blocks per sentence. These figures align nicely with our classification of phrase-and clause-level differences, which we have related to longer and shorter-range English and German:\n\nEnglish and Arabic: English and Turkish:", "filtered_refids": [[null], [null, "b9", "b165"], ["b75", "b122"], ["b93", "b8"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 31, "num_chars": 5204, "num_references": 8}
{"corpusid_sectionid": "10205309-s7", "title": "When Conset meets Synset: A Preliminary Survey of an Ontological Lexical Resource based on Chinese Characters", "date": "2006-07-17", "section_title": "Hanzi-grounded Lexicon and Ontology", "section": "The current lexicon contains over 5000 characters, and 30,000 derived words in total. 3 The building of the lexical specification of the entries in HanziNet includes various aspects of Hanzi:\n\n1. Conset(s): The conceptual code is the core part of the MRD lexicon in HanziNet. Concepts in HanziNet are indicated by means of a label (conset name) with a code form. In order to increase the efficiency, an ideal strategy is to adopt the Huffmann-coding-like method, by encoding the conceptual structure of Hanzi as a pattern of bits set within a bit string. 4 The coding thus refers to the assignment of code sequences to an character. The sequence of edges from the root to any character yields the code for that character, and the number of bits varies from one character to another. Currently, for each conset (309 in total) there are 12 characters assigned on the average; for each character, it is assigned to 3 Since this lexicon aims at establishing an knowledge resource for modern Chinese NLP, characters and words are mostly extracted from the Academia Sinica Balanced Corpus of Modern Chinese (http://www.sinica.edu.tw/SinicaCorpus/), those characters and words which have probably only appeared in classical literary works, (considered ghost words in the lexicography), will be discarded. 4 This is inspired by Chu (1999)'s works. 3. Shallow parts of speech (mainly Nominal(N) and Verbal(V) tags) 4. Gloss of prototypical meaning 5. List of combined words with statistics calculated from corpus, and 6. Further aspects such as character types and cognates: According to ancient study, characters can be compartmentalized into six groups based on the six classical principles of character construction. Character type here means which group the character belongs to. And the term cognate here is defined as characters that share the same CSH or CSM.  The second core component of the proposed resource is a set of hierarchically related Top Concepts called Top-level Ontology (or Upper ontology). This is similar to EuroWordnet 1.2, which is also enriched with the Top Ontology and the set of Base Concepts (Vossen 1998).\n\nAs mentioned, a tentative set of 309 conset, a kind of ontological categories in contrast with synset has been proposed 7 , and over 5000 characters have been used as instances in populating the character ontology.\n\nMethodologically, following the basic line of OntoClear approach (Guarino and Welty (2002)), we use simple monotonic inheritance in our ontology design, which means that each node inherits properties only from a single ancestor, and the inherited value cannot be overwritten at any point of the ontology. The decision to keep the relations to one single parent was made in order to guarantee that the structure would be able to grow indefinitely and still be manageable, i.e. that the transitive quality of the relations between the nodes would not degenerate with size. Figure 4 shows a snapshot of the character ontology. Figure 4: The character ontology: a snapshot", "filtered_refids": [[null], [null, "b3"], [], ["b4"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3015, "num_references": 4}
{"corpusid_sectionid": "239015827-s14", "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models", "date": "2021-10-16", "section_title": "B Bias Attribute Words", "section": "Below, we list the bias attribute words we use for CDA, SentenceDebias, and INLP.\n\nGender (Zhao et al., 2018).\n\n(actor, actress), (actors, actresses), (airman, airwoman), (airmen, airwomen), (uncle, aunt), (uncles, aunts), (boy, girl), (boys, girls), (groom, bride), (grooms, brides), (brother, sister), (brothers, sisters), (businessman, businesswoman), (businessmen, businesswomen), (chairman, chairwoman), (chairmen, chairwomen), (dude, chick), (dudes, chicks), (dad, mom), (dads, moms), (daddy, mommy), (daddies, mommies), (son, daughter), (sons, daughters), (father, mother), (fathers, mothers), (male, female), (males, females), (guy, gal), (guys, gals), (gentleman, lady), (gentlemen, ladies), (grandson, granddaughter), (grandsons, granddaughters), (guy, girl), (guys, girls), (he, she), (himself, herself), (him, her), (his, her), (husband, wife), (husbands, wives), (king, queen), (kings, queens), (lord, lady), (lords, ladies), (sir, maam), (man, woman), (men, women), (sir, miss), (mr., mrs.), (mr., ms.), (policeman, policewoman), (prince, princess), (princes, princesses), (spokesman, spokeswoman), (spokesmen, spokeswomen)\n\nRace. (black, caucasian, asian), (african, caucasian, asian), (black, white, asian), (africa, america, asia), (africa, america, china), (africa, europe, asia)\n\nReligion (Liang et al., 2020).\n\n(jewish, christian, muslim), (jews, christians, muslims), (torah, bible, quran), (synagogue, church, mosque), (rabbi, priest, imam), (judaism, christianity, islam) ", "filtered_refids": [[], [null], [], [], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 3, "num_chars": 1495, "num_references": 3}
{"corpusid_sectionid": "247627890-s2", "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions", "date": "2022-03-22", "section_title": "Initial Instruction", "section": "In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as \"Go upstairs and pass the table in the living room. Turn left and go through the door in the middle.\" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.\n\nSome work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.", "filtered_refids": [["b16", null, "b19", "b25"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1916, "num_references": 5}
{"corpusid_sectionid": "247627890-s3", "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions", "date": "2022-03-22", "section_title": "Coarse-grained Navigation", "section": "In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle). Usually, instructions are more concise and contain merely information of the target goal.\n\nRoomNav (Wu et al., 2018) requires agent navigate according to instruction \"go to X\", where X is a predefined room or object.\n\nIn Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question. The instructions in REVERIE (Qi et al., 2020b) are annotated by humans, and thus more complicated and diverse. The agent navigates through the rooms and differentiates the object against multiple competing candidates. In SOON (Zhu et al., 2021a), an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.\n\nNavigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available). In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple). Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question. Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment. CHAI (Misra et al., 2018) requires the agent to navigate and simply interact with the environments.", "filtered_refids": [[], ["b16"], [null, "b25"], [null, "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1652, "num_references": 5}
{"corpusid_sectionid": "247627890-s9", "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions", "date": "2022-03-22", "section_title": "Semantic Understanding", "section": "Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.\n\nIntra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).\n\nInter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).", "filtered_refids": [[], ["b4", "b28", "b21", null, "b22"], ["b21", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1361, "num_references": 7}
{"corpusid_sectionid": "247627890-s14", "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions", "date": "2022-03-22", "section_title": "Reinforcement Learning", "section": "VLN is a sequential decision-making problem and can naturally be modeled as a Markov decision process. So Reinforcement Learning (RL) methods are proposed to learn better policy for VLN tasks. A critical challenge for RL methods is that VLN agents only receive the success signal at the end of the episode, so it is difficult to know which actions to attribute success to, and which to penalize. To address the ill-posed feedback issue, Wang et al. (2019) propose RCM model to enforces cross-modal grounding both locally and globally, with goal-oriented extrinsic reward and instructionfidelity intrinsic reward. He et al. (2021) propose to utilize the local alignment between the instruction and critical landmarks as the reward. Evaluation metrics such as CLS (Jain et al., 2019) or nDTW (Ilharco et al., 2019) can also provide informative reward signal (Landi et al., 2020), and natural language may also provide suggestions for reward (Fu et al., 2019).\n\nTo model the dynamics in the environment, Wang et al. (2018) leverage model-based reinforcement learning to predict the next state and improve the generalization in unseen environment. Zhang et al. (2020a) find recursively alternating the learning schemes of imitation and reinforcement learning improve the performance.", "filtered_refids": [["b11", null], ["b12", "b21"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1279, "num_references": 4}
{"corpusid_sectionid": "247627890-s17", "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions", "date": "2022-03-22", "section_title": "Asking for Help", "section": "An intelligent agent asks for help when uncertain about the next action. Action probabilities or a separately trained model (Chi et al., 2020;Zhu et al., 2021c;Nguyen et al., 2021a) can be leveraged to decide whether to ask for help. Using natural language to converse with the oracle covers a wider problem scope than sending a signal. Both rule-based methods (Padmakumar et al., 2021) and neural-based methods (Roman et al., 2020;Nguyen et al., 2021a) have been developed to build navigation agents with dialog ability. Meanwhile, for tasks (Thomason et al., 2019b;Padmakumar et al., 2021) that do not provide an oracle agent to answer question in natural language, researchers also need to build a rule-based (Padmakumar et al., 2021) or neural-based (Roman et al., 2020) oracle. Dial-FRED (Gao et al., 2022) uses a language model as an oracle to answer questions.", "filtered_refids": [[null, "b29", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 867, "num_references": 3}
{"corpusid_sectionid": "247627890-s19", "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions", "date": "2022-03-22", "section_title": "Data Augmentation", "section": "Trajectory-Instruction Augmentation Augmented path-instruction pairs could be used in VLN directly. Currently the common practice is to train a speaker module to generate instructions given a navigation path (Fried et al., 2018). This generated data have varying quality (Zhao et al., 2021). Therefore an alignment scorer (Huang et al., 2019) or adversarial discriminator (Fu et al., 2020) can select high-quality pairs for augmentation. Environment Augmentation Generating more environment data not only helps generate more trajectories, but also alleviates the problem of overfitting in seen environments. Randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or simply splitting the house scenes and remixing them (Liu et al., 2021) could create new environments, which could further be used to generate more trajectory-instruction pairs (Fried et al., 2018). Training data may also be augmented by replacing some visual features with counterfactual ones (Parvaneh et al., 2020).", "filtered_refids": [["b11", null, "b1", "b23"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1016, "num_references": 4}
{"corpusid_sectionid": "247627890-s23", "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions", "date": "2022-03-22", "section_title": "Prior Exploration", "section": "Good performance in seen environments often cannot generalize to unseen environments (Hu et al., 2019;Parvaneh et al., 2020;Tan et al., 2019). Prior exploration methods allow the agent to observe and adapt to unseen environments, 3 bridging the performance gap between seen and unseen environments. Wang et al. (2019) introduce a self-supervised imitation learning to learn from the agent's own past, good behaviors. The best navigation path determined to align the instruction the best by a matching critic will be used to update the agent. Tan et al. (2019) leverage the testing environments to sample and augment paths for adaptation. Fu et al. (2020) propose environment-based prior exploration, where the agent can only explore a particular environment where it is deployed. When utilizing graph, prior exploration may construct a map or overview about the unseen environment to provide explicit guidance for navigation (Chen et al., 2021a;Zhou et al., 2021).", "filtered_refids": [["b11", null, "b1", "b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 964, "num_references": 4}
{"corpusid_sectionid": "247627890-s28", "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions", "date": "2022-03-22", "section_title": "B Simulator", "section": "The virtual features of the dataset are deeply connected with the simulator in which datasets are built. Here we summarize simulators frequently used during the VLN dataset creation process. House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.\n\nMatterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017). Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches. An agent can navigate between viewpoints along a pre-defined graph. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.\n\nHabitat ( where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED (Shridhar et al., 2020).\n\nGibson (Xia et al., 2018) is a real-world perception interactive environment with complex semantics. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.\n\nHouse3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects).\n\nLANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks. An agent needs to navigate between landmarks following the natural language instruction. Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.\n\nCurrently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity. Google Street View 4 , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN (Chen et al., 2019).", "filtered_refids": [["b16", null], [null], [null], [null, "b17"], ["b16"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2396, "num_references": 8}
{"corpusid_sectionid": "248426721-s2", "title": "What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification", "date": "2022-04-28", "section_title": "The Relation Extraction Task", "section": "Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.\n\nOne way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill\u00e9 et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.\n\nPipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taill\u00e9 et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.", "filtered_refids": [[], ["b32", null, "b7", "b3", "b36"], ["b43", "b54", "b44", "b56", null, "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 36, "num_chars": 5215, "num_references": 11}
{"corpusid_sectionid": "235422524-s1", "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP", "date": "2021-06-14", "section_title": "Data Augmentation for NLP", "section": "Data augmentation increases both the amount (the number of data points) and the diversity (the variety of data) of a given dataset (Cubuk et al., 2019). Limited labeled data often leads to overfitting on the training set and data augmentation works to alleviate this issue by manipulating data either automatically or manually to create additional augmented data.Such techniques have been widely explored in the computer vision field, with methods like geometric/color space transformations (Simard et al., 2003;Krizhevsky et al., 2012;Taylor and Nitschke, 2018), mixup (Zhang et al., 2018), and random erasing (Zhong et al., 2020;DeVries and Taylor, 2017). Although the discrete nature of textual data and its complex syntactic and semantic structures make finding labelpreserving transformation more difficult, there nevertheless exists a wide range of methods for augmenting text data that in practice preserve labels. In the following subsections, we describe four broad classes of data augmentation methods:", "filtered_refids": [["b50", null, "b22", "b42", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1012, "num_references": 5}
{"corpusid_sectionid": "235422524-s2", "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP", "date": "2021-06-14", "section_title": "Token-Level Augmentation", "section": "Token-level augmentations manipulate words and phrases in a sentence to generate augmented text while ideally retaining the semantic meaning and labels of the original text.\n\nDesigned Replacement. Intuitively, the semantic meaning of a sentence remains unchanged if some of its tokens are replaced with other tokens that have the same meaning. A simple approach is to fetch synonyms as words for substitutions (Kolomiyets et al., 2011;Yang, 2015;Zhang et al., 2015a;Wei and Zou, 2019;Miao et al., 2020). The synonyms are discovered based on pre-defined dictionaries such as WordNet (Kolomiyets et al., 2011), or similarities in word embedding space (Yang, 2015). However, improvements from this technique are usually minimal (Kolomiyets et al., 2011) and in some cases, performance may even degrade (Zhang et al., 2015a). A major drawback stems from the lack of contextual information when fetching synonyms-especially for words with multiple meanings and few synonyms. To resolve this, language models (LMs) have been used to replace the sampled words given their context (Kolomiyets et al., 2011;Fadaee et al., 2017;Kobayashi, 2018;Kumar et al., 2020). Other work preserves the labels of the text by conditioning on the label when generating the LMs' predictions (Kobayashi, 2018;Wu et al., 2019a). In addition, different sampling strategies for word replacement have been explored. For example, instead of sampling one specific word from candidates by LMs, Gao et al. (2019) propose to compute a weighted average over embeddings of possible words predicted by LMs as the replaced input since the averaged representations could augment text with richer information.\n\nRandom Insertion, Replacement, Deletion and Swapping. While well-designed local modifications can preserve the syntax and semantic meaning of a sentence (Niu and Bansal, 2018), random local modifications such as deleting certain tokens (Iyyer et al., 2015;Wei and Zou, 2019;Miao et al., 2020), inserting random tokens (Wei and Zou, 2019;Miao et al., 2020), replacing non-important tokens with random tokens (Xie et al., 2017(Xie et al., , 2020Niu and Bansal, 2018) or randomly swapping tokens in one sentence (Artetxe et al., 2018;Lample et al., 2018;Wei and Zou, 2019;Miao et al., 2020) can preserve the meaning in practice. Different  (2017)  kinds of operations can be further combined (Wei and Zou, 2019), where each example is randomly augmented with one of insertion, deletion, and swapping. These noise-injection methods can efficiently be applied to training, and show improvements when they augment simple models trained on small training sets. However, the improvements might be unstable due to the possibility that random perturbations change the meanings of sentences (Niu and Bansal, 2018). Also, finetuning large pre-trained models on specific tasks might attenuate improvements due to preexisting generalization abilities of the model (Shleifer, 2019).\n\nCompositional Augmentation. To increase the compositional generalization abilities of models, recent efforts have also focused on compositional augmentations (Jia and Liang, 2016; Andreas, 2020) where different fragments from different sentences are re-combined to create augmented examples. Compared to random swapping, compositional augmentation often requires more carefully-designed rules such as lexical overlap (Andreas, 2020), neural-symbolic stack machines (Chen et al., 2020e), and neural program synthesis (Nye et al., 2020). With the potential to greatly improve the generalization abilities to out-of-distribution data, compositional augmentation has been utilized in sequence labeling (Guo et al., 2020), semantic parsing (Andreas, 2020; Nye et al., 2020;Furrer et al., 2020), language modeling (Andreas, 2020; Shaw et al., 2020), and text generation (Feng et al., 2020).", "filtered_refids": [[], ["b28", "b47", "b29", "b35", null], ["b28", "b31", "b32", null, "b16"], ["b14", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3822, "num_references": 12}
{"corpusid_sectionid": "235422524-s3", "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP", "date": "2021-06-14", "section_title": "Sentence-Level Augmentation", "section": "Instead of modifying tokens, sentence-level augmentation modifies the entire sentence at once.\n\nParaphrasing. Paraphrasing has been widely adopted as a data augmentation technique in various NLP tasks (Yu et al., 2018;Xie et al., 2020;Kumar et al., 2019;He et al., 2020;Chen et al., 2020b,c;Cai et al., 2020), as it generally provides more diverse augmented text with different word choices and sentence structures while preserving the meaning of the original text. The most popular is round-trip translation (Sennrich et al., 2015;Edunov et al., 2018) -Tavor et al., 2020;Zhang and Bansal, 2019;Kumar et al., 2020;). An extra filtering process is often used to ensure high-quality augmented data. For example, in text classification, Anaby-Tavor et al. (2020) first fine-tune GPT-2 (Radford et al., 2019) with the original examples prepended with their labels, and then generate augmented examples by feeding the finetuned model certain labels. Only confident examples as judged by a baseline classifier trained on the original data are kept. Similarly, new answers are generated on the basis of given questions in question answering and are filtered by customized metrics like question answering probability (Zhang and Bansal, 2019) and n-gram diversity . Generative models used in this setting have been based on conditional VAE (Bowman et al., 2016;Hu et al., 2017;Guu et al., 2017;Malandrakis et al., 2019), GAN (Iyyer et al., 2018;Xu et al., 2018) or pre-trained language models like GPT-2 (Anaby- Tavor et al., 2020;Kumar et al., 2020). Overall, these conditional generation methods can create novel and diverse data that might be unseen in the original dataset, but require significant training effort.  (2020) proposes two simple yet effective adversarial transformations that reverse the position of subject and object or the position of premise and hypothesis.", "filtered_refids": [[], ["b13", "b31", "b45", null, "b37", "b33", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1871, "num_references": 7}
{"corpusid_sectionid": "235422524-s6", "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP", "date": "2021-06-14", "section_title": "Consistency Training with DA", "section": "While data augmentation (DA) can be applied in the supervised setting to produce better results when only a small labeled training dataset is available, data augmentation is also commonly used in semi-supervised learning (SSL). SSL is an alternative approach for learning from limited data that provides a framework for taking advantage of unlabeled data. Specifically, SSL assumes that our training set comprises labeled examples in addition to unlabeled examples drawn from the same distribution. Currently, one of the most common methods for performing SSL with deep neural networks is \"consistency regularization\" (Bachman et al., 2014;Tarvainen and Valpola, 2017). Consistency regularization-based SSL (or \"consistency training\" for short) regularizes a model by enforcing that its output doesn't change significantly when the input is perturbed. In practice, the input is perturbed by applying data augmentation, and consistency is enforced through a loss term that measures the difference between the model's predictions on a clean input and a corresponding perturbed version of the same input.\n\nFormally, let f \u03b8 be a model with parameters \u03b8, f\u03b8 be a fixed copy of the model where no gradients  are allowed to flow, x l be a labeled datapoint with label y, x u be an unlabeled datapoint, and \u03b1(x) be a data augmentation method. Then, a typical loss function for consistency training is\n\nwhere CE is the cross entropy loss and \u03bb u is a tunable hyperparameter that determines the weight of the consistency regularization term. In practice, various other measures have been used to minimize the difference between f\u03b8(x u ) and f \u03b8 (\u03b1(x u )), such as the KL divergence (Miyato et al., 2018;Xie et al., 2020) and the mean-squared error (Tarvainen and Valpola, 2017;Laine and Aila, 2017;Berthelot et al., 2019). Because gradients are not allowed to flow through the model when it was fed the clean unlabeled input x u , this objective can be viewed as using the clean unlabeled datapoint to generate a synthetic target distribution for the augmented unlabeled datapoint. Xie et al. (2020) showed that consistency training can be effectively applied to semi-supervised learning for NLP. To achieve stronger results, they introduce several other tricks including confidence thresholding, training signal annealing, and entropy minimization. Confidence thresholding applies the unsupervised loss only when the model assigns a class probability above a pre-defined threshold. Training signal annealing prevents the model from overfitting on easy examples by applying the supervised loss only when the model is less confident about predictions. Entropy minimization trains the model to output low-entropy (highly-confident) predictions when fed unlabeled data. We refer the reader to (Xie et al., 2020) for more details on these tricks.", "filtered_refids": [["b21", null], [], ["b21", null, "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2833, "num_references": 5}
{"corpusid_sectionid": "235422524-s8", "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP", "date": "2021-06-14", "section_title": "Datasets and Experiment Setup", "section": "To provide a quantitative comparison of the DA methods we have surveyed, we experiment with 10 of the most commonly used and model-agnostic augmentation techniques from different levels in Table 1   , Cutoff (Shen et al., 2020), and Mixup in the embedding space (Zhang et al., 2018). Most aforementioned techniques are not label-dependent (except mixup), thus can be applied directly to unlabeled data. We test them on different types of benchmark datasets including: (i) news classification tasks including AG News (Zhang et al., 2015b) and 20 Newsgroup (Joachims, 1997); (ii) topic classification tasks including Yahoo Answers (Chang et al., 2008) and PubMed news classification ( (Zhang et al., 2015b) (iii) inference tasks including MNLI, QNLI and RTE ; (iv) similarity and paraphrase tasks including QQP and MRPC ; and (v) single-sentence tasks including SST-2 and CoLA .\n\nFor all datasets, we experiment with 10 labeled data points per class 2 in a supervised setup, and an additional 5000 unlabeled data points per class in the semi-supervised setup. We use BERT base (Devlin et al., 2019) as the base language model and use the same hyper-parameters across all datasets/methods. We utilize accuracy as the evaluation metric for all datasets except for CoLA (which uses Matthews correlation) and PubMed (which uses accuracy and Macro-F1 score). Because the performance can be heavily dependent on the specific datapoints chosen (Sohn et al., 2020), for each dataset, we sample labeled data from the original dataset with 3 different seeds to form different training sets, and report the average result. For every setup, we fine-tune the model with the same seed as the dataset seed (in contrast to many works which report the max across different seeds). The detailed experimental setup is described in the Appendix.", "filtered_refids": [["b15", "b42", "b48", null], ["b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1823, "num_references": 5}
{"corpusid_sectionid": "235422524-s10", "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP", "date": "2021-06-14", "section_title": "Other Limited Data Learning Methods", "section": "This work mainly focuses on data augmentation and semi-supervised learning (consistency regularization) in NLP; however, there are other orthogonal directions for tackling the problem of learning with limited data. For completeness, we summarize this related work below.\n\nLow-Resourced Languages. Most languages lack large monolingual or parallel corpora, or sufficient manually-crafted linguistic resources for building statistical NLP applications (Garrette and Baldridge, 2013). Researchers have therefore developed a variety of methods for improving performance on low-resource languages, including cross-lingual transfer learning which transfers models from resource-rich to resource-poor languages ( Other Methods for Semi-Supervised Learning. Semi-supervised learning methods further reduce the dependency on labeled data and enhance the models when there is only limited labeled data available. These methods use large amounts of unlabeled data in the training process, as unlabeled data is usually cheap and easy to obtain compared to labeled data. In this paper, we focus on consistency regularization, while there are also other widely-used methods for NLP including self-training (Yarowsky, 1995;Zhang and Zong, 2016;He et al., 2020;Lin et al., 2020), generative methods (Xu et al., 2017;Yang et al., 2017;Kingma et al., 2014;Cheng et al., 2016), and cotraining (Blum and Mitchell, 1998;Clark et al., 2018;Cai and Lapata, 2019).\n\nFew-shot Learning. Few-shot learning is a broad technique for dealing with tasks with less labeled data based on prior knowledge. Compared to semi-supervised learning which utilizes unlabeled data as additional information, few-shot learning leverages various kinds of prior knowledge such as pre-trained models or supervised data from other domains and modalities . While most work on few-shot focuses on computer vision, few-shot learning has recently seen increasing adoption in NLP (Han et al., 2018;Rios and Kavuluru, 2018;Hu et al., 2018;Herbelot and Baroni, 2017). To better leverage pre-trained models, PET (Schick and Sch\u00fctze, 2021a,b) converts the text and label in an example into a fluent sentence, and then uses the probability of generating the label text as the class logit, outperforming GPT3 for few shot learning (Brown et al., 2020). How to better model and incorporate prior knowledge to handle few-shot learning for NLP remains an open challenge and has the potential to significantly improve model performance with less labeled data.", "filtered_refids": [[], ["b43", "b34", null, "b38", "b2", "b37"], [null, "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 2497, "num_references": 8}
{"corpusid_sectionid": "253736389-s1", "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications", "date": "2023-03-01", "section_title": "Terminology and Overview", "section": "We focus on tabular data that come in the form of a where all cells in a column share the same atomic type, for example, the relational table in Table 1.\n\nAn entity table has the same properties, but with a horizontal structure with only one entity whose properties are organized as rows. Spreadsheets, or matrix tables, are the most general kind with information that can be organized both horizontally and vertically, possibly with hierarchies in the header and formatting metadata, such as in financial tables. Tables can have rich metadata, such as attribute types (e.g., DATE), domain constraints, functional dependencies across columns and integrity constraints such as primary keys. In the table sample in Figure 1, column Country is a primary key. Most systems focus on a single table, with or without metadata. However, a few systems, such as GTR (Wang et al., 2021a), GRAPPA , and DTR (Herzig et al., 2021), consume databases, which are collections of relational tables, possibly under referential constraints. We identify as input the table(s) and its context. The context is a text associated to the table. Depending on the dataset and task at hand, it varies from table metadata, the text surrounding the tables or their captions up to questions, expressed in natural language, that can be answered with the tabular data (Badaro and Papotti, 2022).\n\nThe main advantage of the transformer architecture is its ability to generate a LM, a large neural network, with self-supervised pre-training. This pre-trained LM is then usually followed by supervised fine-tuning to adapt it to the target task with a small amount of training data. While transformers have proven to be effective in modeling textual content, tables have a rich structure that comes with its own relationships, such as those across values in the rows and attributes. New solutions are therefore needed to jointly model the characteristics of the table, its text content, and the text in the table context. As shown in Figure 1, we distinguish two main phases to spell out these contributions. First, we focus on the development of tabular LMs by using transformer-based deep neural networks (1). Given a table and its context, the goal is to learn a pre-trained representation of the structured data (cell values, rows, attributes) in a continuous vector space. We then discuss the use of those representations in the downstream tasks (2). Figure 1 shows the reference pipeline and the aspects that we propose to model existing systems.\n\n\u2022 Training Datasets (Sec. 3): the datasets used for pre-training and fine-tuning the models toward specific tasks; datasets for the latter case usually come with annotations and/or labels.\n\n\u2022 Input Processing (Sec. 4): the steps to prepare the data for the model processing, such as the transformation from the two dimensional tabular space to one dimensional input.\n\n\u2022 Transformer-based Encoder (Sec. 5): the pre-training objectives and customization of the typical transformer-based deep learning architecture.\n\n\u2022 Downstream Task Model (Sec. 6): the models consuming the representations or fine-tuning them to tackle downstream tasks.\n\n\u2022 Tabular Language Model (Sec. 7): the output representations, including at the token, row, column, table level, and their usage.\n\nFor the tasks consuming the models, we report on ", "filtered_refids": [[], ["b48", "b99", "b101"], [], [], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 3335, "num_references": 3}
{"corpusid_sectionid": "253736389-s2", "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications", "date": "2023-03-01", "section_title": "Training Datasets", "section": "We present both the datasets used for pre-training and for fine-tuning in the downstream tasks. Pre-training tables are not annotated, in some cases scraped from the web, while data used for fine-tuning have task-dependent annotation labels. The datasets consist of tables and their context, such as table metadata, surrounding texts, claims or questions. To construct large pre-training datasets and in an attempt to reduce bias, multiple sources can be used, independently of the target task at hand. For instance, unlike TAPAS (Herzig et al., 2020), which only uses Wikipedia Tables for QA, and TABULARNET , which only uses Spreadsheets for TMP, TABERT  uses Wikipedia Tables and WDC  for SP; GRAPPA uses Wikipedia Tables, Spider, and WikiSQL for SP; MMR (Kosti\u0107 et al., 2021) uses NQ, OTT-QA, and WikiSQL for TR; MATE  uses Wikipedia Tables and HybridQA for QA; and TUTA  uses Wikipedia Tables, WDC, and spreadsheets for TMP. In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations. For instance, Wikipedia tables has a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets include also entity tables and spreadsheets with complex structure. Table 2 summarizes the main characteristics for the most common datasets. We mark the tasks for which the dataset has been used by \u2714 under the column ''Task''. We note that the top four datasets are mostly used for pre-training, while the others can be used for fine-tuning as well since they include annotations for the target task, for example, questions/answers for QA. The column ''Large Tables'' is a binary indicator, where \u2714 and \u2718 indicate whether or not the tabular corpus include large tables and hence whether or not some pre-processing is needed to reduce table content to meet the limits of the transformer architecture (512 input tokens in most cases). Some works, such as TABERT, TAPEX (Liu et al., 2021a), and CLTR (Pan et al., 2021), apply filtering in any case to reduce noisy input. Finally, the ''Context'' column describes additional text that come with the tables. This can be text describing the table, such as caption or title of the document containing the table; table metadata, such as table orientation, header row, and keys; or questions and claims that can be addressed with the table.", "filtered_refids": [["b11", "b49", "b62", "b74", "b68"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2415, "num_references": 5}
{"corpusid_sectionid": "253736389-s10", "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications", "date": "2023-03-01", "section_title": "Vanilla Transformer", "section": "The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Residual connections and layer-normalization modules are also used. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.\n\nThe transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only , or decoder-only (Radford et al., 2019;Brown et al., 2020) model. The choice of the architecture depends on the final task. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).", "filtered_refids": [["b90", "b97"], ["b78", "b77", "b97", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1335, "num_references": 6}
{"corpusid_sectionid": "253736389-s16", "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications", "date": "2023-03-01", "section_title": "Downstream Tasks", "section": "Using neural representations for tabular data show, improvements in performance in several downstream tasks. In this section, we describe the tasks and define their input and output. While they all consume tables, settings can be quite heterogeneous, with systems exploiting different information, even for the same task. A summary of the covered tasks along their input, output, and some representative systems addressing them is shown in Table 4. We detail next the mandatory input elements and the different contexts.  (Dagan et al., 2013;Korman et al., 2018), checking facts with tables consists of verifying if a textual input claim is true or false against a trusted database (TAPEX, DECO, TABFACT), also provided as input. Some fact-checking systems, such as FEVEROUS, also output the cells used for verification as evidence (Nakov et al., 2021;Karagiannis et al., 2020).\n\nQuestion Answering (QA): In the free text setting, QA aims at retrieving passages that include the answer to a given question. In the tabular data setting, it consists of returning as output the cells that answer an input consisting of a question and a table. One can distinguish two levels of complexity. Simple QA involves lookup queries on tables (DTR, CLTR), while a more complex QA  Most of the systems in this survey aim at improving accuracy in QA with respect to hand-crafted embeddings.\n\nSemantic Parsing (SP): In the tabular data setting, given a question and a table as input, SP generates a declarative query in SQL over the table's schema to retrieve the answer to the question. While in QA the interest is in directly getting the answer, SP produces the (interpretable) query to obtain it (TABERT, GRAPPA, TAPEX).  table that can be used to answer the question. TR is helpful when trying to reduce the search space for a QA task (GTR, DTR, MMR). It is a challenging task given the limited input size of transformers, that is, their constraint to sequences of 512 tokens.  We observe that most tasks can be seen as traditional NLP problems where structured data replace free text, such as the case of QA where answers are located in tabular data instead of documents (Gupta and Gupta, 2012). TFC involves retrieving cells that entail or refute a given statement, whereas on free text the corresponding objective is to select sentences as evidence (Thorne et al., 2018). SP is the task of converting natural language utterance into a logical form (Berant and Liang, 2014), which in this setting is expressed as a declarative query over a relational table. TR on tabular data corresponds to passage retrieval on free text (Kaszkiel and Zobel, 1997). TCP is analogous to predicting missing words or values in a sentence . Finally, TMP can be related to syntactic parsing in NLP (Van Gompel and Pickering, 2007), where relationships between different tokens are depicted.\n\nWe conclude this section with an analysis of the performance of the systems over the different downstream tasks. For every task, we selected datasets for which at least two systems have reported results. All datasets reported in Table 5 (Ghasemi-Gol and Szekely, 2018), which contains web tables annotated with their type (relational, entity, matrix, list, and non-data), and EntiTab (Zhang and Balog, 2017), which contains web tables annotated with possible header labels for the column population task. Table 5 also contains the size, expressed as number of parameters, of the largest model used by every system. As some systems are not comparable on any shared datasets, we report here their size: TABFACT (110M), MMR (87M), TURL (314M), RPT (139M), and CLTR (235M). Larger models do not correlate with better performance across different systems for the same task, but a larger model always brings higher scores for the same system, as expected. Execution times for training and testing depend on the size of the model and the computing architecture.\n\nThe results in the table show that some tasks, such as TFC and TMP, can already be handled successfully by the systems, while some tasks, such as TCP, TR, and SP, are harder. QA is the task supported by most systems and the quality of the results vary depending on the dataset at hand. Differences in performance can be explained with different improvements across the systems. For example, MATE has better performance with respect to TAPAS in two tasks because of its mechanism to deal with larger input tables. Similarly, TUTA improves over TABERT because it handles tabular data beyond relational tables. Finally, TABERT and TAPAS are the systems that show most coverage in terms of tasks, with multiple papers using them as baselines in their experiments. For the systems that are not reported in Table 5, we notice that TURL obtains similar F1 results for column type prediction, but on a dataset different from VizNet; TURL, TABBIE, and TABERT also report comparable MAP results for the row population task (not in Table 5) over different datasets.", "filtered_refids": [["b30", "b72", "b61", "b54"], [], ["b10", "b47", "b55", "b93", "b96"], ["b111", "b42"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 35, "num_chars": 4972, "num_references": 11}
{"corpusid_sectionid": "251402499-s8", "title": "Abstractive Meeting Summarization: A Survey", "date": "2022-08-08", "section_title": "Evaluation methods", "section": "As noted in Section 2, the subjectivity of meeting summaries, stemming in part from the preference for abstractive summarization, complicates the evaluation task. Evaluation requires both verifying that the content in a system-based summary follows from the original transcript and measuring the overlap with a gold-standard summary. However, given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used.\n\nUnfortunately, the ROUGE metric (Lin, 2004), which remains the standard for both meeting and general text summarization, scores systemproduced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries. If we take the abstractive summary from Figure 1 (\"Some part of the casing will be made of a spongy material\") as a gold example, ROUGE would assign a higher score to a system that produces \"Some part of the casing will be made of broccoli\" than one that output \"A portion of the outer layer will be constructed from a sponge-like material,\" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false).\n\nA reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap. Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text. Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.\n\nDespite their rapid development, recent studies on meta-evaluation of these metrics show mixed results. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate. Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.\n\nClearly, evaluation is itself a very challenging task. And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency. When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight. Otherwise, the resulting summaries are not reliable for an end user. While we do not know of current work that focuses on evaluation of factuality explicitly for the meeting domain, the study of factual consistency in summarization more generally is a budding research area that we discuss in Section 6.\n\nIn the absence of a clear winner for summary evaluation metrics, none of the alternatives has yet to be widely adopted. In our comparison of different summarization systems in Section 5, we therefore stick with ROUGE, which offers the additional advantage of conceptual simplicity and lower computational cost compared to metrics based on contextual embeddings and pretrained language models.", "filtered_refids": [[], ["b17"], ["b52", "b55", "b27", "b49", null], ["b6", "b7"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3903, "num_references": 8}
{"corpusid_sectionid": "251402499-s10", "title": "Abstractive Meeting Summarization: A Survey", "date": "2022-08-08", "section_title": "Interpretation", "section": "Consider the following (fabricated) exchange that we might imagine taking place in the AMI meeting from Figure 1:\n\n(1) a. PM: So what color should the removable cover be? b. ID: I think we should offer a few options. What about raspberry, lime and blueberry and then black for those who don't like color? c. UI: Sounds good to me. d. ME: Me too. e. PM: OK. Let's go with that.\n\nThere is a very clear decision that has been made in (1), but what information allows us to recognize this decision so easily? The acknowledgement in (1-e) explicitly confirms the decision and thus plays a crucial role in inferring that a decision has been made, but it does not tell us what decision has been made.\n\nIn fact, none of the utterances (1-a)-(1-e) alone allow us to infer the decision. We must rather understand that (1-b) provides an answer to the question asked in (1-a) and that (1-e) is an acknowledgement of the suggestion in (1-b) and of the positive answers in (1-c) and (1-d). (If instead of agreeing in (1-c) and (1-d), the UI and ME had presented and defended an alternative proposal, we might have understood (1-e) as an acknowledgement of their suggestion rather than of (1-b).)\n\nBecause how an utterance contributes to the larger conversational context is often crucial for understanding the conversation as a whole, some summarization approaches, which we review in Section 5.1.1, enrich meeting transcripts with ex-plicit representations of those contributions. Other summarization accounts exploit other types of information relevant to discourse interpretation. HMNet (Zhu et al., 2020) and DDAMS (Feng et al., 2020), for instance, use information about speakers and turns (\"who said what\"), drawing on the fact that speaker information can help to convert (frequent) occurrences of first and second person pronouns into third person pronouns, as is necessary for summarization (Luo et al., 2009). Further interpretation-focused methods are studied in Section 5.1.2, in which we take a look at accounts that have opted to augment transcripts with non-linguistic information of multi-modal nature, such as information about the eye-gaze of conversational participants.  (1). Node b 1 represents the first sentence of (1-b), b 2 , the second, and similarly for e 1 and e 2 . The dashed box indicates that the PM's \"OK\" in (1-e) acknowledges and accepts the entire exchange from (1-b) to (1-d).", "filtered_refids": [[], [], [], [], ["b20", null, "b60"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2399, "num_references": 3}
{"corpusid_sectionid": "251402499-s11", "title": "Abstractive Meeting Summarization: A Survey", "date": "2022-08-08", "section_title": "Discursive information", "section": "While most work on abstractive meeting summarization implicitly assumes that conversation is merely a linear sequence of utterances without semantic relations between them, example (1) underscores the importance of semantic relations for conversational understanding. Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization. Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse. Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations. Figure 2 il-  , 2021a). The * indicates summary-level (with sentence split) ROUGE-L score (Lin, 2004).\n\nlustrates a possible SDRT graph for example (1).\n\nTo the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries. They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora. Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices. Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary. A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.\n\nAn alternative approach to discourse interpretation that developed largely independently of RST and SDRT is dialogue act classification. Detailing the differences between the approaches is out of the scope of this paper, but in a nutshell, dialogue acts provide a shallower notion of discourse structure (Jurafsky et al., 1997) in that they do not entail a full graph structure over a conversation. On the other hand, systems for dialogue act labeling, such as DAMSL (Allen and Core, 1997; Core and Allen, 1997) or DiAML (Bunt et al., 2010(Bunt et al., , 2012, place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting or informing.\n\nBoth the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.\n\nDialogue acts have also been used to good effect for summarizing decisions. Fern\u00e1ndez et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution. Then, key fragments of the decision related utterances are retained to form an extractive decision summary. Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007.", "filtered_refids": [["b21", "b1", null, "b17"], [], [null, "b2", "b31"], ["b11", null, "b10", "b0"], [], [null, "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3926, "num_references": 13}
{"corpusid_sectionid": "251402499-s12", "title": "Abstractive Meeting Summarization: A Survey", "date": "2022-08-08", "section_title": "Multimodality", "section": "Meetings tend to take place in shared visual contexts, allowing important information to be conveyed through gestures, facial expressions, head poses, eye gaze, and so on. It is thus reasonable to think that harnessing this information could lead to improved performance over a summarization system that draws on speech transcripts alone. Indeed, Li et al. (2019) has shown that the Visual Focus Of Attention (VFOA), which is estimated based on a participant's head orientation and eye gaze, can help a multi-modal summarization system determine salient utterances, thereby improving the quality of abstractive meeting summaries. In this work, VFOA is integrated with attention mechanisms in a PGN text decoder and topic segmenter trained in a multi-task learning setting.\n\nIn the context of extractive meeting summarization, Erol et al. (2003) exploits multimodality to select segments of meeting video recordings in order to facilitate browsing of the videos for important information. Along with TF-IDF text analysis of the speech transcript, their system attends to certain visual motion activities (e.g., someone entering the meeting room or standing up to make a presentation) and to acoustic features (e.g., speech volume and degree of interaction) to enhance detection of summary-worthy events. Similar ideas can be found in Xie and Liu (2010);Nihei et al. (2016Nihei et al. ( , 2018; Nihei and Nakano (2019).\n\nOf course, enhancing meeting transcripts with information to improve interpretation comes at a cost: most annotations are performed manually and require linguistic expertise to do well, and annotations produced automatically, as in Shi and Huang (2019) or Shang et al. (2020b), can add uncertainties to the data. It is perhaps thus unsurprising that the last couple of years have seen a shift away from Interpretation-focused methods presented in this section towards Generation methods, as shown in Table 1. Still, certain approaches showed promising results, especially Feng et al.\n\n(2020) and Li et al. (2019), and arguably, Interpretation approaches offer other advantages that should encourage us to explore them further. Discourse graphs, for example, can provide input to graph-based networks, helping to bypass limits on input document length imposed by some Generation approaches (see Section 5.3). And a rich source representation can arguably make a single transcript more valuable for training, partially offsetting data scarcity described in Section 3.", "filtered_refids": [["b16"], [null, "b46"], ["b30", "b31"], ["b16"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2484, "num_references": 6}
{"corpusid_sectionid": "251402499-s19", "title": "Abstractive Meeting Summarization: A Survey", "date": "2022-08-08", "section_title": "Long input processing", "section": "A straightforward solution to the length problem is to segment a long document into smaller segments to be processed. Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary. Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework. Within each stage, it first splits the source input into sufficiently short segments. Coarse summaries are then generated for each segment and then concatenated as the input to the next stage. This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.\n\nWhile such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem. Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022). Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions. Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems. Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors. That sequence is then processed by the turn-level encoder. The transformer decoder makes use of both levels of representation via cross-attention layers. Rohde et al. (2021) propose Hierachical Attention Transformer (HAT). Utterances are first prepended with a special BOS token. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations. Finally, the decoder leverages the outputs at both levels to produce a final summary. Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.", "filtered_refids": [[null, "b54"], ["b60", "b4", "b26", "b56", "b48", null, "b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2783, "num_references": 9}
{"corpusid_sectionid": "256662721-s2", "title": "Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems", "date": "2023-02-07", "section_title": "Training Strategies of LMRS", "section": "Given the significant impact that PLMs have had on NLP tasks in the pre-train and fine-tune paradigm, there has been a surge recently in adapting such paradigms to multiple recommendation tasks. As illustrated in Figure 1, there are mainly two classes regarding different training paradigms: pre-train, fine-tune paradigm and prompt learning paradigm. Each class is further classified into subclasses regarding different training efforts on different parts of the recommendation model. This section will go through various training strategies w.r.t. specific recommendation purposes. Figure  2(a) presents the statistics of recent publications of LMRSs grouped by different training strategies and the total number of published research works each year. Figure 2(b) shows the taxonomy and some corresponding representative LMRSs.\n\n4.1 Pre-train, fine-tune paradigm for RS\n\nThe \"pre-train, fine-tune\" paradigm attracts increasing attention from researchers in the recommendation field due to several advantages: 1) Pre-training provides a better model initialization, which usually leads to better generalization on different downstream recommendation tasks, improves recommendation performance from various perspectives, and speeds up convergence on the fine-tuning stage; 2) Pre-training on huge source corpus can learn universal knowledge which can be beneficial for the downstream recommenders; 3) Pre-training can be regarded as a kind of regularization to avoid overfitting on lowresource, and small datasets (Erhan et al., 2010).\n\nPre-train This training strategy can be seen as traditional end-to-end training with domain input. Differently, we only focus on research works adapting LM-based learning objectives into the training phase. Many typical LM-based RSs fall into this category, such as BERT4Rec , which models sequential user behaviour with a bidirectional self-attention network through Cloze task, and Transformers4Rec (de Souza Pereira Moreira et al., 2021) which adopts a haggingface transformer-based architecture as the base model for next-item prediction and explores four different LM tasks, namely Causal LM, MLM, Permutation LM, and Replacement Token Detection during training. These two models laid the foundation for LM-based recommender systems and have become popular baselines for their successors.\n\nPre-train, fine-tune holistic model Under this category, the model is pre-trained and fine-tuned with different data sources, and the fine-tuning process will go through adjusting the whole model parameters. The learning objectives can also vary between the pre-training and fine-tuning stages. Pre-training and fine-tuning with different domains of data sources, also called cross-domain recommendation, can refer to the works of Kang et al. (2021) and Qiu et al. (2021). Kang et al. (2021) pre-trained a GPT model using segmented source API code and fine-tuned it with API code snippets from another library for cross-library recommendation. Wang et al. (2022a) fine-tuned the pre-trained DialoGPT model on domain-specific datasets for conversational recommendation together with an R-GCN model to inject knowledge from DBpedia to enhance recommendation perfor- mance. Xiao et al. (2022) fine-tuned the PTM to learn news embedding together with a user embedding part in an auto-regressive manner for news recommendation. They also explored different fine-tuning strategies like tuning part of the PTM and tuning the last layer of the PTM but empirically found fine-tuning the whole model resulted in better performance, which gives us an insight into balancing the recommendation accuracy and training efficiency.\n\nPre-train, fine-tune partial model Since finetuning the whole model is usually time-consuming and less flexible, many LMRSs choose to fine-tune partial parameters of the model to achieve a balance between training overhead and recommendation performance (Hou et al., 2022;Yu et al., 2022;Wu et al., 2022a). For instance, to deal with the domain bias problem that BERT induces a non-smooth anisotropic semantic space for general texts resulting in a large language gap for texts from different domains of items, Hou et al. (2022) applied a linear transformation layer to transform BERT representations of items from different domains followed by an adaptive combination strategy to derive a universal item representation. Meanwhile, considering the seesaw phenomenon that learning from multiple domainspecific behavioural patterns can be a conflict, they proposed sequence-item and sequence-sequence contrastive tasks for multi-task learning during the pre-training stage. They found only fine-tuning a small proportion of model parameters could quickly adapt the model to unseen domains with cold-start or new items. Pre-train, fine-tune extra part of the model With the increase in the depth of PTMs, the representation captured by them makes the downstream recommendation easier. Apart from the aforementioned two fine-tuning strategies, some works leverage a task-specific layer on top of the PTMs for recommendation tasks. Fine-tuning only goes through such extra parts of the PTMs by optimizing the parameters of the task-specific layer. Shang et al. (2019) pre-trained a GPT and a BERT model to learn patient visit embeddings, which were then used as input to fine-tune the extra prediction layer for medication recommendation. Another approach is to use the PTM to initialize a new model with a similar architecture in the finetuning stage, and the fine-tuned model is used for recommendations. In , a bidirectional Transformer-based model was first pretrained on four different self-supervised learning objectives (associated attribute prediction, masked item prediction, masked attribute prediction and segment prediction) to learn item embeddings. Then, the learned model parameters were adopted to initialize a unidirectional Transformer-based model for fine-tuning with pairwise rank loss for recommendation. In (McKee et al., 2023), the authors leveraged the pre-trained BLOOM-176B to generate natural languages descriptions of music given a set of music tags. Subsequently, two distinct pre-trained models, namely CLIP and the D2T pipeline, were employed to initialize textual, video, and audio representations of the provided music content. Following this, a transformerbased architecture model was fine-tuned for multimodal music recommendation.", "filtered_refids": [[], [], ["b5"], [], ["b45", "b53", "b36", "b17"], ["b15", "b39", "b50", "b32", "b61"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 33, "num_chars": 6410, "num_references": 10}
{"corpusid_sectionid": "256662721-s3", "title": "Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems", "date": "2023-02-07", "section_title": "Prompting paradigm for RSs", "section": "Instead of adapting PLMs to different downstream recommendation tasks by designing specific objective functions, a rising trend in recent years is to use the \"pre-train, prompt, and inference\" paradigm to reformulate downstream recommendations through hard/soft prompts. In this paradigm, fine-tuning can be avoided, and the pretrained model itself can be directly employed to predict item ratings, generate top-k item ranking lists, make conversations, recommend similar libraries for programmers while coding, or even output subtasks related to recommendation targets such as explanations (Li et al., 2023b). Prompt learning breaks through the problem of data constraints and bridges the gap of objective forms between pre-training and fine-tuning. Fixed-PTM prompt tuning Prompt-tuning only requires tuning a small set of parameters for the prompts and labels, which is especially efficient for few-shot recommendation tasks. Despite the promising results achieved through constructing prompt information without significantly changing the structure and parameters of PTMs, it also calls for the necessity of choosing the most appropriate prompt template and verbalizer, which can greatly impact recommendation performance. Prompt tuning can be both in the form of discrete textual templates (Penha and Hauff, 2020), which are more human-readable, and soft continuous vectors (Wang et al., 2022c;Wu et al., 2022b). For instance, Penha and Hauff (2020) manually designed several prompt templates to test the performance of movie/book recommendations on a pre-trained BERT model with a similarity measure. Wu et al. (2022b) proposed a personalized prompt generator tuned to generate a soft prompt as a prefix before the user behaviour sequence for sequential recommendation. Fixed-prompt PTM tuning Fixed-prompt PTM tuning tunes the parameters of PTMs similarly to the \"pre-train, fine-tune\" strategy but additionally uses prompts with fixed parameters to steer the recommendation task. One natural way is to use artificially designed discrete prompt to specify recommendation items. For instance, Zhang et al.\n\n(2021b) designed a prompt \"A user watched item A, item B, and item C. Now the user may want to watch () \" to reformulate the recommendation as a multi-token cloze task during fine-tuning of the LM-based PTM. The prompts can also be one or several tokens/words to seamlessly shift/lead the conversations from various tasks. Deng et al. (2023)  token as a prompt to indicate the start of the recommendation process and to summarize the dialogue context for the conversational recommendation.\n\nTuning-free prompting This training strategy can be referred to as zero-shot recommendations, which directly generate recommendations or/and related subtasks without changing the parameters of the PTMs but based only on the input prompts. Zero-shot recommendation has been shown to be effective in dealing with new users/items in one domain or cross-domain settings (Sileo et al., 2022;Geng et al., 2022c), compared to state-ofthe-art baselines. Specifically, Geng et al. (2022c) learned multiple tasks, such as sequential recommendation, rating prediction, explanation generation, review summarization and direct recommendation, in a unified way with the same Negative Log-likelihood (NLL) training objectives during pre-training. At the inference stage, a series of carefully designed discrete textual template prompts were taken as input, including prompts for recommending items in the new domain (not appearing in the pre-training phase), and the trained model outputs the preferable results without a fine-tuning stage. The reason for the effectiveness of zero-shot recommendation is that the training data and pre-training tasks are able to distil rich knowledge of semantics and correlations from diverse modalities into user and item tokens, which can comprehend user preference behaviours w.r.t. item characteristics (Geng et al., 2022c). Building upon this research, Geng et al. (2023) extended their efforts to train an adapter for diverse multimodal assignments, including sequential recommendations, direct recommendations, and the generation of explanations. In particular, they utilized the pre-trained CLIP component to convert images into image tokens. These tokens were added to the textual tokens of an item to create a personalized multimodal soft prompt. This com-bined prompt was then used as input to fine-tune the adapter in an autoregressive manner. Prompt+PTM tuning In this setting, the parameters include two parts: prompt-relevant parameters and model parameters. The tuning phase involves optimizing all parameters for specific recommendation tasks. Prompt+PTM tuning differs from the \"pre-train, fine-tune the holistic model\" strategy by providing additional prompts that can provide additional bootstrapping at the start of model training. For example, Li et al. (2023b) proposed a continuous prompt learning approach by first fixing the PTM, tuning the prompt to bridge the gap between the continuous prompts and the loaded PTM, and then fine-tuning both the prompt and PTM, resulting in a higher BLUE score in empirical results. They combined both discrete prompts (three user/item feature keywords, such as gym, breakfast, and Wi-Fi) and soft prompts (user/item embeddings) to generate recommendation explanations. Case studies showed improvements in the readability and fluency of generated explanations using the proposed prompts. Note that the Prompt+PTM tuning stage does not necessarily mean the fine-tuning stage but can be any possible stage for tuning parameters from both sides for specific data input. Xin et al. (2022) adapted a reinforcement learning framework as a Prompt+PTM tuning strategy by learning rewardstate pairs as soft prompt encodings w.r.t. observed actions during training. At the inference stage, the trained prompt generator can directly generate soft prompt embeddings for the recommendation model to generate actions (items).", "filtered_refids": [["b48", "b33", "b20", null], ["b3"], ["b15", "b9", "b20", "b41", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 5996, "num_references": 10}
{"corpusid_sectionid": "256662721-s5", "title": "Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems", "date": "2023-02-07", "section_title": "Language modelling objectives to recommendation", "section": "The expensive manual efforts required for annotated datasets have led many language learning objectives to adopt self-supervised labels, converting them to classic probabilistic density estimation problems. Among language modelling objectives, autoregressive, reconstruction, and auxiliary are three categories commonly used (Liu et al., 2023b). Here, we only introduce several language modelling objectives used for RSs.\n\nPartial/ Auto-regressive Modelling (P/AM) Given a text sequence X 1:T = [x 1 , x 2 , \u00b7 \u00b7 \u00b7 x T ], the training objective of AM can be summarized as a joint negative log-likelihood of each variable given all previous variables:\n\nModern LMRS typically utilize popular pretrained left-to-right LMs such as GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al., 2022a,c) as the backbone for explainable and conversational recommendations, respectively, to avoid the laborious task of pre-training from scratch. While auto-regressive objectives can effectively model context dependency, the modelling context can only be accessed from one direction, primarily left-to-right. To address this limitation, PAM is introduced, which extends AM by enabling the factorization step to be a span. For each input X, one factorization order M is sampled. One popular PTM that includes PAM as an objective is UniLMv2 (Bao et al., 2020). The pretrained UniLMv2 model can be utilized to initialize the news embedding model for news recommendation (Yu et al., 2022). Besides directly leveraging PTMs trained on textual inputs, some researchers apply this objective to train inputs with sequential patterns, such as graphs (Geng et al., 2022b) and user-item interactions . These patterns serve as either scoring functions to select suitable paths from the start node/user to the end node/item or detectors to explore novel user-item pairs. Masked Language Modelling (MLM) Taking a sequence of textual sentences as input, MLM first masks a token or multi-tokens with a special token such as [M ASK]. Then the model is trained to predict the masked tokens taking the rest of the tokens as context. The objective is as follows:\n\nwhere M (X) and X M (X) represent the masked tokens in the input sequence X and the rest of the tokens in X respectively. A typical example of MLM training strategy can be found on BERT, which is leveraged as backbone in (Zhang et al., 2021a) to capture user-news matching signals for news recommendation. Concurrently, some research works propose multiple enhanced versions of MLM. RoBERTa  \n\nwhere x and y represent two segments from the input corpus, and c = 1 if x and y are consecutive, otherwise c = 0. The NSP objective involves reasoning about the relationships between pairs of sentences and can be utilized for better representation learning of textual items such as news articles, item descriptions, and conversational data for recommendation purposes. Moreover, it can be employed to model the intimate relationships between two components. Malkiel et al. (2020) used the NSP to capture the relationship between the title and description of an item for next-item prediction. Furthermore, models pre-trained with NSP (such as BERT) can be leveraged for probing the learned knowledge with prompts, which are then infused in the fine-tuning stage to improve model training on adversarial data for conversational recommendation (Penha and Hauff, 2020). Sentence Order Prediction (SOP) as a variation of the NSP takes two consecutive segments from the same document as positive examples, which are then swapped in order as negative examples. SOP has been used to learn the inner coherence of title, description, and code for tag recommendation on StackOverflow (He et al., 2022).\n\nNevertheless, some researchers have questioned the necessity and effectiveness of the NSP and SOP for downstream tasks (He et al., 2022), which highlights the need for further investigation in recommendation scenarios. Replaced Token Detection(RTD) It is used to predict whether a token is replaced given its surrounding context:\n\nMovieLens Link", "filtered_refids": [["b23"], [], [null, "b8", "b61", "b0"], ["b65"], ["b33", "b13", "b31"], ["b13"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4064, "num_references": 10}
{"corpusid_sectionid": "256662721-s11", "title": "Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems", "date": "2023-02-07", "section_title": "Adaptive objectives to recommendation", "section": "Numerous pre-training or fine-tuning objectives draw inspiration from LM objectives and have been effectively applied to specific downstream tasks based on the input data types and recommendation goals. In sequential recommendations, there is a common interest in modelling an ordered input sequence in an auto-regressive manner from left to right.\n\nAnalogous to text sentences,  and Xiao et al. (2022) treated the user's clicked news history as input text and proposed to model user behavior in an auto-regressive manner for next-click prediction. However, as the sequential dependency may not always hold strictly in terms of user preference for recommendations (Yuan et al., 2020a), MLM objectives can be modified accordingly. Yuan et al. (2020b) randomly masked a certain percentage of historical user records and predicted the masked items during training. Autoregressive learning tasks can also be adapted to other types of data. Geng et al. (2022b) modeled a series of paths sampled from a knowledge graph in an auto-regressive manner for recommendation by generating the end node from the pretrained model. Zhao (2022) proposed pre-training the Rearrange Sequence Prediction task to learn the sequence-level information of the user's entire interaction history by predicting whether the user interaction history had been rearranged, which is similar to Permuted Language Modelling (PerLM) (Yang et al., 2019).\n\nMLM, also known as Cloze Prediction, can be adapted to learn graph representations for different recommendation purposes.  proposed pre-training a transformer model on a reconstructed subgraph from a user-item-attribute heterogeneous graph, using Masked Node Prediction (MNP), Masked Edge Prediction (MEP), and meta-path type prediction as objectives. Specifically, MNP was performed by randomly masking a proportion of nodes in a heterogeneous subgraph and then predicting the masked nodes based on the remaining contexts by maximizing the distance between the masked node and the irrelevant node. Similarly, MEP was used to recover the masked edge of two adjacent nodes based on the surrounding context. Apart from that, MLM can also be adapted to multi-modal data called Masked Multimodal Modelling (MMM) (Wu et al., 2022a). MMM was performed by predicting the semantics of masked news and news image regions given the unmasked inputs and indicating whether a news image and news content segment correspond to each other for news recommendation purposes.\n\nThe NSP/SOP can be adapted for CTR prediction as Next K Behaviors Prediction (NBP). NBP was proposed to learn user representations in the pre-training stage by inferring whether a candidate behavior is the next i-th behavior of the target user based on their past N behaviors. NBP can also capture the relatedness between past and multiple future behaviors.", "filtered_refids": [[], ["b63", null, "b62", "b70", "b53", "b8"], ["b50"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2835, "num_references": 7}
{"corpusid_sectionid": "256662721-s14", "title": "Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems", "date": "2023-02-07", "section_title": "Evaluation metrics", "section": "As an essential aspect of recommendation design, evaluation can provide insights on recommendation quality from multiple dimensions. Apart from well-known metrics such as RMSE, MAP, AUC, MAE, Recall, Precision, MRR, NDCG, F1-score and HitRate in offline mode, some works define Group AUC  or User Group AUC  to evaluate the utility of group recommendations. JIANG et al. (2022) and  conducted A/B testing to evaluate performance with online users using Conversion rate or CTR.\n\nThe integration of generative modules such as GPT and T5 into existing recommender systems offers additional possibilities for recommender systems, such as generating free-form textual explanations for recommendation results or simulating more realistic real-life dialogue scenarios during conversational recommendations to enhance users' experience. In such cases, BLEU and ROUGE are commonly adopted to automatically evaluate the relevance of generated text based on lexicon overlap. Besides, Perplexity (PPL), Distinct-n, and Unique Sentence Ratio (USR) are also widely used metrics to measure fluency, diversity, and informativeness of generated texts. Other evaluation metrics are leveraged with respect to special requests in LMRSs. For instance, Xie et al. (2023) adopted Entailment Ratio and MAUVE to measure if the generated explanations are factually correct and how close the generated contents are to the ground truth corpus, respectively. Geng et al. (2022a) adopted Feature Diversity (DIV) and CLIPScore (CS) to measure the generated explanations and text-image alignment. Besides, to assess the system's capability to provide item recommendations during conversations, Wang et al. (2022a) computed the Item Ratio within the final generated responses. They evaluated the recommendation performance in an end-to-end manner to prevent the inappropriate insertion of recommended items into dialogues.\n\nHuman evaluation complements objective evaluation, as automatic metrics may not match sub-jective feedback from users. Liu et al. (2023a) pointed out that human subjective and automatic objective evaluation measurements may yield opposite results, which underscores the limitations of existing automatic metrics for evaluating generated explanations and dialogues in LMRSs. Figure  3 displays usage frequency statistics for different evaluation metrics in their respective tasks.", "filtered_refids": [[], ["b45", "b54", "b7"], ["b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2370, "num_references": 4}
{"corpusid_sectionid": "258426970-s5", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Format", "section": "An important decision to make when we want to improve language generation systems through human Example input and output for three tasks (machine translation, summarization, and instruction following) and possible different (example) feedback that can be given.\n\nfeedback is in what format to collect this feedback in. The choice of format has implications on the expressivity of the feedback, the ease of its collection, and how we can use it to improve systems. In particular, the complexity of the feedback format is an important factor: simpler formats are often easier to collect and use as part of the training/decoding process, but contain less information than more \"complex\" formats, and might not be able to capture important information for improving the system. The choice of format also has implications in the difficulty for humans to give feedback, its consistency/agreement, and the level of rationality of said feedback (Ghosal et al., 2023). Types of feedback are summarized in Table 1 with examples.\n\nNumerical Numerical feedback, which takes an input and output and returns a single score (X \u00d7 Y \u2192 N \u2286 R), is one of the simplest feedback formats to collect and use. Kreutzer et al. (2018) studied using categorical feedback, in the form of 5 possible \"stars\" that can be assigned to a translation, which are then averaged to produce a score (N = [1, 5]) and used to improve the model.  and Shi et al. (2021) used even simpler feedback, by asking humans to choose if a given response is good or not (N = {0, 1}). Numerical feedback has also been extensively used for evaluation, albeit not with the explicit goal of improving generation. For example, direct assessments (Graham et al., 2013) in machine translation ask humans to rate translations on a continuous scale, and some works have attempted to use this feedback data to train feedback models (Sellam et al., 2020;Rei et al., 2020a) and improve generation (Freitag et al., 2022a;Fernandes et al., 2022).\n\nAlthough easy to leverage, numerical feedback suffers from some limitations: depending on the complexity of the generation task, reducing feedback to a single score might generally be a hard and ill-defined task for humans, leading to a costly collection process and problems of subjectivity and variance (see \u00a76.2.1). Furthermore, such feedback might not be suited to distinguish between outputs of similar quality.\n\nRanking-based An alternative to asking humans to assign a single score to a given input-output pair is asking them to rank multiple possible alternative outputs h : X \u00d7 Y 1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Y n \u2192 S n where S n represents the set of all permutations/rankings of n elements (optionally allowing ties). This has been used extensively in evaluation (Chaganty et al., 2018). Compared to numerical feedback, this format tends to be easier to collect, and, potentially, for this reason, ranking-based feedback tends to be collected to improve model behavior rather than just for evaluation (since the former tends to require more feedback data). Ziegler et al. (2019) and Stiennon et al. (2020) asked humans to rank alternative summaries of the system they are trying to improve. Similarly, Ouyang et al. (2022) collected rankings of alternative responses to an instruction given to the model. They utilized these rankings to enhance the model's instruction-following capabilities. Subsequent research has also employed ranking-based feedback for the same task (Askell et al., 2021;Bai et al., 2022a,b). Natural Language Both numerical and rankingbased feedback lack the ability to capture detailed information about problems with the output, which can be crucial for improving generation systems. Instead of asking humans to rank or score outputs, we can instead ask for natural language feedback. In such cases, the feedback typically provides more detailed information, either highlighting the shortcomings of the current output or suggesting specific actions for improvement. For example, Li et al. (2017) asked humans to give natural language feedback to a dialogue question answering model, including positive or negative feedback, but also possibly providing the correct answer to the model or hinting about it.  and  gather natural language feedback on errors present in model-generated graphs and the model's interpretation of a given instruction. Scheurer et al. (2022Scheurer et al. ( , 2023 improve summarization capabilities of language models by asking humans to provide natural language feedback of summaries of the model. Li et al. (2022) collect natural language feedback (in addition to numerical feedback) for responses from a Question Answering (QA) system.\n\nOthers Besides these feedback types, other (potentially domain-specific) types of feedback can be used to improve model behavior. Commonly humans are asked to provide multi-aspect feedback (X \u00d7 Y \u2192 R d or F d more generally), scoring an output or ranking multiple outputs with respect to multiple dimensions (B\u00f6hm et al., 2019;Glaese et al., 2022;Madaan et al., 2023;Nguyen et al., 2022). Post-editions ask humans to provide corrections to the output in the form of small edits (e.g., replace X by Y), and post-edition data has been used to directly improve models (Denkowski et al., 2014) or train automatic post edition systems that correct model mistakes (Pal et al., 2016;Mehta and Goldwasser, 2019;Madaan et al., 2021;Talmor et al., 2020;Elgohary et al., 2021). There are also other feedback types that haven't been fully leveraged to improve generation: e.g., Multidimensional Quality Metrics (MQM) (Lommel et al., 2014b), the standard for evaluating translation quality, asks professional translators to identify errors spans in a translation, alongside severity and type of error.", "filtered_refids": [[], ["b3"], ["b60", "b9", "b80", null, "b24", "b76"], [], ["b87", "b29", "b71", "b30", null, "b72"], ["b90", "b40", "b35", null, "b5", "b42", "b38", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 5753, "num_references": 21}
{"corpusid_sectionid": "258426970-s6", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Objective", "section": "The purpose of collecting feedback is to align the model's behavior with some (often ill-defined) goal behavior: we might want our summarization model to generate summaries that contain all core information, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate businesscritical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014;Amodei et al., 2016;Bommasani et al., 2021). In addition, Kenton et al. (2021b) discuss some behavioral issues in language agents (natural language generation models) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective.\n\nBai et al. (2022a) explicitly divided the problem of \"aligning\" a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness factors (such as not producing toxic text or providing information that could lead to harm). 2\n\nHelpfulness Most often, feedback is collected with some helpfulness objective in mind: a necessary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018;Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream applications. Similarly, in summarization, most works leverage feedback related to aspects such as relevance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instructions (Ouyang et al., 2022): the task of instructionfollowing can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021).\n\nHarmlessness Another important alignment objective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (besides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their system, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could increase harmlessness without reducing helpfulness.", "filtered_refids": [[null, "b28"], [], ["b24", null, "b69"], ["b69", null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 3311, "num_references": 8}
{"corpusid_sectionid": "258426970-s8", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Optimizing for Human Feedback", "section": "Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \"optimizable\", i.e., possibly formulated as an optimization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem:\n\n( 2) Where D is the distribution of possible inputs. Various techniques have been suggested to optimize the model parameters, \u03b8, using the collected human feedback. These can be divided into three main categories based on the training mechanisms, which we will call feedback-based imitation learning, joint-feedback modeling, and reinforcement learning (RL).\n\nThe feedback-based imitation learning approach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled generations together with the corresponding inputs, D + . This can be achieved by minimizing the loss:\n\nAn instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model's answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine translation model on a set of positively-labeled translations, and Glaese et al. (2022) performed supervised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), according to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human utterances as targets to fine-tune the model. Scheurer et al. (2022Scheurer et al. ( , 2023 leverage the fact that LLMs can follow instructions and start by collecting natural language human feedback about the model generations, which often describes what an improved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corresponding feedback. The highest similarity refinements for each generation are then used to finetune the LLM. OpenAI's text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disregard the generations which do not receive positive feedback, which may contain useful information to optimize the model. On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural language). Having D as the dataset of inputs x, generations y, and human feedback f collected, this can be achieved by minimizing the following loss of the form\n\nOver all examples in D. These equation can be factorized as\n\n. Some works simply train the model to predict the feedback given to each generation (Weston, 2016, forward prediction), disregarding the second term of the factorization. One example of this approach is the work of Li et al. (2017), in which the authors asked humans to give natural language feedback (e.g., positive/negative feedback, providing the correct answer to the model, or giving a hint about the correct answer) to a dialogue question answering model. Then, after having collected the feedback, the model is trained to predict it. Hancock et al. (2019) proposed having an auxiliary model predicting the satisfaction of the human speaking with the model. Then, if the satisfaction score is lower than a pre-defined threshold, the model will ask the human for feedback. The model then leverages the natural language feedback humans give by learning to predict it. Yuan et al. (2023); Rafailov et al. (2023) showed that having summarization models predict the rankings of different summaries helps the model generate better summaries, and might even outperform more complicated approaches using feedback models ( \u00a75).\n\nOther works train the model to predict the generations and the corresponding human feedback. Xu et al. (2022) proposed using the DIRECTOR model introduced by Arora et al. (2022) to leverage human feedback. As this model has a unified decoderclassifier architecture, Xu et al. (2022) proposed using positively-labeled examples to train its language modeling head (similarly to feedback-based imitation learning) and using both the positive and negatively-labeled examples to train a classifier head that directs the model away from generating undesirable sequences. Thoppilan et al. (2022a) follow this approach to enforce the model's quality and safety. First, they collect dialogues between crowd-workers and the proposed language model LaMDA, which are annotated with feedback provided by the crowd-workers. This feedback states each response's quality (sensible, specific, and interesting) or safety. Then, LaMDA is fine-tuned to predict the high-quality responses and the rewards given to every response regarding its quality attributes and safety. At inference time, LaMDA is also used to filter out candidate responses for which its safety prediction is below a threshold.\n\nFinally, this can also be achieved by training the model to predict generation and conditioning on the feedback. This corresponds to minimizing the following loss: Liu et al. (2023) proposed prompt-based finetuning, where they create prompts containing previous generations rated by humans, in the order of preference. They also suggest inserting languagebased feedback (e.g., \"... is a worse answer than ...\") to the prompt, between the generations. Then, the model is fine-tuned to maximize the likelihood of generating the most preferred answer.\n\nFinally, reinforcement learning (RL) offers a more versatile approach, allowing for direct optimization of a model's parameters based on human feedback, regardless of the feedback's differentiability. A common RL algorithm used in this context is the REINFORCE algorithm (Williams, 1992), which updates the policy parameters using the following gradient:\n\n(7) Here, D represents the set of inputs x, and p \u03b8 is the policy. This flexibility enables RL to handle various types of feedback and better align the generated output with human preferences. For instance, Kreutzer et al. (2018) proposed using task-based implicit feedback from user queries as a reward signal to train a machine translation model using a word-level variant of minimum risk training (Shen et al., 2016), while Jaques et al. (2019) used implicit human reactions in chat to improve open-domain dialog systems through off-policy Q-learning (Watkins and Dayan, 1992). Given that collecting human feedback can be expensive and time-consuming, learning is done offline from logged data, which is typically more favorable than on-policy settings that need feedback on the fly. Later in \u00a75.2.1, we discuss several works that attempt to optimize feedback models using RL instead of directly optimizing human feedback. In conjuction, these aproaches are commonly known as Reinforcement Learning from Human Feedback (RLHF).", "filtered_refids": [[], [], [], ["b29", "b71", "b24", "b72", "b5"], [], ["b59", null, "b10", "b29"], ["b107", "b94"], ["b32"], ["b104"], ["b24", "b77", "b99", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 42, "num_chars": 7251, "num_references": 17}
{"corpusid_sectionid": "258426970-s9", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Decoding with Human Feedback", "section": "While directly optimizing model parameters provides greater control, modifying them may not always be feasible, particularly in the case of LLMs. Additionally, feedback might be unavailable during model training, limiting the scope for parameter adjustments. In such cases, leveraging human feedback during decoding plays a critical role in enhancing LLMs's performance. This type of feedback, derived from interactions between LLMs and users in practical scenarios, enables models to learn from their errors and offers opportunities for ongoing refinement without altering model parameters. In addition, the feedback functions as a guiding mechanism, allowing the model to generate more desirable outputs by leveraging its existing capabilities.\n\nThere are two broad categories in which human feedback is used in this setup: 1. Feedback Memory: Feedback Memory Utilization involves maintaining a repository of feedback from prior sessions. Then, when processing new inputs, the system uses relevant feedback from similar inputs in its memory to guide the model toward generating more desirable outputs based on past experiences and user preferences. While a classical concept (Riesbeck, 1981;Schank, 1983), recent work has shown the promise of such a memoryaugmented approach in both finetuning (Weston et al., 2014;Wu et al., 2018; and few-shot setups .\n\n2. Iterative Output Refinement: This method employs human feedback to refine the model's output iteratively. Users can provide feedback on intermediate responses, enabling the model to adjust its output until it meets the user's satisfaction. This process allows the model to better understand user preferences and produce more suitable outcomes (Reid and Neubig, 2022;Saunders et al., 2022;Schick et al., 2022;Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs.\n\nThese two techniques are not mutually exclusive and can be combined to achieve even better performance, creating a more adaptive and responsive system that caters to user expectations.", "filtered_refids": [[], ["b105", "b70", "b63", null], ["b69", null, "b62", "b49"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2117, "num_references": 8}
{"corpusid_sectionid": "258426970-s11", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Learning Models of Human Feedback", "section": "An alternative approach to obtaining human feedback is to develop models that can predict or ap-proximate it. Although these models may not be perfect, they offer the advantage of providing feedback at a low cost after training, thereby enabling the scaling of feedback-dependent techniques. More formally, given a feedback function h : X \u00d7 Y 1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Y n \u2192 F, we want to learn a parametric (numerical) feedback model\u0125 \u03d5 : X \u00d7 Y \u2192 R (with parameters \u03d5) that \"agrees\" with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss:\n\nL(\u03d5) = loss \u0125 \u03d5 (x, y 1 ), \u00b7 \u00b7 \u00b7 , h(x, y 1:n )\n\nFor example, if the feedback function we are trying to model is also numerical (h : X \u00d7 Y \u2192 R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(\u03d5) = \u0125 \u03d5 (x, y) \u2212 h(x, y) 2 . Importantly, while the feedback model is (generally) numerical, the human feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3\u0125 \u03d5 (x, y n ) on ranking-based feedback, using a loss of the form L(\u03d5) = log \u03c3 \u0125 \u03d5 (x, y +1 ) \u2212\u0125 \u03d5 (x, y \u22121 )\n\n(10) such that sample y +1 was preferred to y \u22121 for the same input x: h(x, y \u22121 , y +1 ) = (y \u22121 < y +1 ). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022;Askell et al., 2021;Qin et al., 2022;Yuan et al., 2023).\n\nThe problem of feedback modeling has been studied extensively in the context of metric learning for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to compute similarity scores between the generated text or code snippets and their references. In MT, Sellam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged annotated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary-level metric from a set of human judgements included in older summarization datasets (e.g., TAC-2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these reward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in \u00a75.2.\n\nRecently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (B\u00f6hm et al. Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally occurring implicit feedback, such as from user interactions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feedback, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminishing returns when the number of explicit-collected feedback increases. Nguyen et al. (2022) train a preference model based on rankings on three human-designed objectives: whether the summary has an appropriate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correcting the output of an MT system, can also be seen as feedback models (albeit non-numerical).", "filtered_refids": [[], [], ["b87"], ["b69", null, "b58"], ["b115", "b113", "b60", "b55", null], ["b42", null, "b83"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3890, "num_references": 12}
{"corpusid_sectionid": "258426970-s13", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Optimizing for Feedback Models", "section": "Similarly to optimizing for human feedback, one possible way to use the feedback model is to optimize model parameters with respect to the feedback it gives. If the feedback model outputs numerical feedback (\u0125 \u03d5 : X \u00d7 Y \u2192 R) we can define an optimization problem similar to Equation 2. However, due to the limitations of feedback models as imperfect proxies, typically a regularization term R is introduced to avoid \"overfitting\" to the feedback model (Ziegler et al., 2019) (more on this at the end of this section):\n\n(11) Due to the similarities between both optimization problems, approaches to tackle Equation 11 can be divided into two of the three categories in \u00a74.2: joint-feedback modeling and reinforcement learning. Recall that while in \u00a74.2 we discuss approaches for directly optimizing for human feedback, while this section is focused on cases where a model of human feedback is used instead.\n\nUnlike when using human feedback directly, most works attempt to optimize for feedback models using reinforcement learning. Gao et al. (2018);B\u00f6hm et al. (2019) use the (numerical) feedback collected in other works to train reward and preference models, and use reinforcement learning to optimize against these models, showing that humans preferred their summarization model to other supervised and RL-trained baselines. Ziegler et al. (2019) proposed a similar approach, but trained preference models using feedback collected on the model being improved, and introduced a KL regularization term\n\nto avoid the optimized model deviating too much from the original (supervised) model with parameters \u03b8 SL 4 . Stiennon et al. (2020) extended this work, by scaling both the summarization and preference models, showing that their model was highly preferred by humans, and generalized better than supervised baselines. Ouyang et al. (2022) also used reinforcement learning with preference models to improve the ability of LLMs to follow instructions, but combined the RL objective with the original pretraining objective to avoid performance regressions in public NLP benchmarks. Other works have also used reinforcement learning with preference models in a similar manner (Askell et al., 2021;Bai et al., 2022a;Wu et al., 2021;Nguyen et al., 2022). Underlying all these methods is that generally the model is first trained with imitation-learning on human demonstrations, which improves performance compared to using reinforcement learning directly on the pretrained policy. Glaese et al. (2022) compared doing feedbackbased imitation learning with human feedback ( \u00a74.1) with doing reinforcement learning with a feedback model, finding that the latter led to a better preference rate and lower rule violation rate.\n\nThe joint-feedback modeling with feedback models was explored by , who study pre-training an LLMs with a loss similar to Equation 6, based on feedback from a preference model trained on ranking-based feedback for toxicity. They showed that this leads to models producing less toxic generations, when compared to pretraining a model with vanilla MLE.\n\nIn an approach outside these main categories, Peyrard and Gurevych (2018) use a scoring function learned from human judgments as a fitness function for a genetic algorithm to generate summaries of input texts.", "filtered_refids": [[], [], [null], ["b87", null, "b69", "b42", "b5"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3280, "num_references": 6}
{"corpusid_sectionid": "258426970-s14", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Decoding with Feedback Models", "section": "As mentioned, feedback models have the advantage that they can be queried cheaply for feedback once trained. Perhaps for this reason, most approaches that leverage feedback models by sampling a large number of candidate generations, and reranking them according to the feedback model:\n\nwhere\u0125 \u03d5 is a trained (numerical) feedback model and C is a set of S candidate generations given specific regularization terms, such as the KL terms in PPO (Schulman et al., 2017). by the model (for example, by sampling from its distribution multiple times).\n\nIn machine translation, Fernandes et al. (2022) andFreitag et al. (2022a) build upon recent advances in automatic quality estimation and evaluation via feedback model training to improve generation. Their framework comprises a candidate generation stage followed by a ranking stage, in which the candidates are scored using quality metrics trained to regress on human assessments (reward models) (Rei et al., 2020a,b) via N -best list reranking or minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2002). The highest-scoring candidate is then chosen as the final translation. Li et al. (2022) collected a dataset of both numerical and natural language feedback for responses from a QA system, and finetuned a pretrained model to predict both kinds of feedback, using the predicted scores from this feedback model to re-rank the predictions from the model. Gao et al. (2022) also used this approach to study the scaling properties of feedback models and the problem of \"overoptimization\" (see below).\n\nAdditionally, there are several works combining MT and APE systems at decoding time, in which the output of an MT system is further improved by an APE system (Bhattacharyya et al., 2022).\n\nFeedback Model Overoptimization One problem that arises when optimizing a system with a feedback model is that this model is only an imperfect proxy for the ground truth human feedback, therefore, \"overoptimizing\" for them can lead to systems that receive good feedback from the model, but not humans. This problem is known as the overoptimization problem, and is the main reason for the regularization term in Equation 11 Gao et al. (2022) studies the overoptimization problem in preference models, by both optimizing against it with reinforcement learning (training) and reranking outputs with it (decoding). They found that both using preference models during training or decoding led to similar levels of overoptimization, and that the scale of the generation model helps little with this problem.", "filtered_refids": [[], ["b75"], ["b30", null, "b25"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2539, "num_references": 5}
{"corpusid_sectionid": "258426970-s15", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Collecting and Using Human Feedback", "section": "Collecting human feedback can be rather expensive and may present i ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethical considerations in the use and collection of human feedback.\n\nIn future, richer types of feedback may be collected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007;Amershi et al., 2014a;Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022;Gehrmann et al., 2022b).", "filtered_refids": [[], ["b88", "b1", "b2", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 891, "num_references": 4}
{"corpusid_sectionid": "258426970-s16", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Considerations in Data Collection", "section": "There are multiple facets to consider when collecting human feedback data for a generation task; a non-exhaustive list of axes along which data collection can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008;Sheng et al., 2008;Clark et al., 2021;Gillick and Liu, 2010;Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models. 2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020;Bai et al., 2022a;Freitag et al., 2021). 3. Collection method: Data can be gathered explicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018;Freitag et al., 2021). 4. Collection platform: Common platforms include Amazon Mechanical Turk, Upwork, and Scale AI. 5. Annotator demographics: Different groups may have varying opinions on quality generations; demographics may be collected during data collection. There is generally a trade-off between the effort needed to create the datasets and the reliability of judgments collected. For higher-stakes applications in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment.", "filtered_refids": [["b4", "b87", "b79", null, "b84", "b24", "b38"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1537, "num_references": 7}
{"corpusid_sectionid": "258426970-s18", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Subjectivity and variance in judgment", "section": "Considering K annotators with feedback functions h i K i=1 , judgments are given on data D = d 1 , ..., d N . Inter-rater reliability metrics, such as Cohen's Kappa, Fleiss' Kappa, or Krippendorff's alpha, can assess annotator agreement (Hayes and Krippendorff, 2007;Fleiss, 1971;Cohen, 1960). Low reliability may result from unclear tasks or evaluation criteria (Gehrmann et al., 2022b;Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022;Nie et al., 2020;Gordon et al., 2022).\n\nMitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021;Zerva et al., 2022). Clear annotation guidelines and including rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019).", "filtered_refids": [["b43", "b12", null, "b93", "b1", "b8"], ["b112", "b108", "b6", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 964, "num_references": 10}
{"corpusid_sectionid": "258426970-s19", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Bias in judgment", "section": "Even if all K annotators agree on a particular judgment for a certain data point, they may all be mistaken. There are well-known biases in human reasoning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if annotators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of   Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information.\n\nsystematic bias away from the originally intended task (Parmar et al., 2023). Anchoring/Confirmation bias: When annotators are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021). When asked to generate text, anchoring bias can cause people to write in a different manner than usual (Jakesch et al., 2023;Lehmann et al., 2022), which may influence what types of suggestions or corrections they give. Mitigation strategies include asking people to rank several diverse outputs and being explicit about the dimensions people are asked to evaluate.\n\nPositivity bias: When giving feedback to learners in traditional RL environments, users tend to give much more positive feedback than negative feedback, which may lead the agent to avoid the goal they are actually trying to reach in these scenarios (Amershi et al., 2014b;Knox and Stone, 2013;Thomaz and Breazeal, 2008).", "filtered_refids": [[], [null, "b27"], [null, "b22", "b92"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1772, "num_references": 5}
{"corpusid_sectionid": "258426970-s20", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Ethical considerations", "section": "Some subjectivity in annotator judgment can arise from differences across cultural or social groups. Santurkar et al. (2023) measure opinions in language model generations, demonstrating varying degrees of representation of demographic groups. Several works observe that tuning with human feedback increases the alignment of generated outputs with US liberal views on controversial topics (Perez et al. (2022b), Hartmann et al. (2023). Annotators with different demographic or political backgrounds may disagree on what qualifies as toxic content (Sap et al. (2022), Ding et al. (2022). This is particularly pronounced when annotators are asked to make ethical judgments, which may vary with cultural context and personal sensibilities (Jiang et al. (2022), Talat et al. (2022)). Steiger et al. (2021) survey moderators of toxic content, identifying harms ranging from slight discomfort to lasting psychological harm from the prolonged performance of content moderation tasks; however, the severity and frequency of toxic content examined in content moderation likely exceeds that in other types of human feedback annotation. Shmueli et al. (2021) identify toxicity classification and generation from open-ended inputs as two NLP annotation tasks that may trigger harmful responses in annotators. They further argue that this moves beyond the \"minimal risk\" requirement for Institutional Review Board exemption in the United States and encourage academic researchers using crowdworker annotation to file for this ethical review of their work.\n\nMedia attention has also focused on fair pay for annotators, with one TIME article 6 describing annotators paid $2 USD or less per hour to review toxic content and provide harmfulness annotations for model training. Research on crowdsourcing (Shmueli et al. (2021); Rothschild et al. (2022); Soratana et al. (2022);Toxtli et al. (2021); Hornuf and Vrankar (2022)) cautions that inadequate pay, especially for workers in lower-resourced regions, can be a form of worker exploitation.", "filtered_refids": [["b11", "b86", "b67", "b89", "b82", null, "b68"], ["b85", "b14", "b82", null, "b65"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2026, "num_references": 12}
{"corpusid_sectionid": "258426970-s21", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "AI Feedback", "section": "Feedback models have been crucial in advancing generation techniques by effectively leveraging feedback. However, they are heavily reliant on human input: for example, Gao et al. (2022) found that across various preference model sizes, utilizing fewer than 1,000 comparisons resulted in only minor improvements, with outcomes approximating chance. Moreover, employing static feedback can create consistency and accuracy challenges, as the integration of feedback leads to changes in the model's output distribution. AI-generated feedback, an emerging research area, focuses on harnessing the large language model's own abilities to evaluate and improve its output, enhancing the model without constant human intervention. Two primary approaches have emerged in this domain:\n\nSelf AI Feedback The first approach involves using the same model to provide feedback and improve its output. In this scenario, the model engages in a continuous self-improvement process, learning from its evaluations and refining its capabilities accordingly. Examples of this approach include prompting models to generate harmful responses and revising them for harmlessness (Bai et al., 2022b), or employing rule-based reward models for RLHF fine-tuning (OpenAI, 2023a). Techniques such as iterative output revision through few-shot prompting (Peng et al., 2023;Shinn et al., 2023;Paul et al., 2023;Madaan et al., 2023; have been explored using LLMs like GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023a). Notably, these techniques demonstrate potential when applied to LLMs trained to adhere to human instructions and align outputs with human preferences. This suggests that incorporating human feedback during training equips AI models to comprehend task requirements better, align outputs with directives, and function as dependable feedback mechanisms, thereby minimizing human intervention. Intriguingly, the capacity to offer valuable AI feedback may depend on the model being trained with human feedback.\n\nExternal AI Feedback: The second approach employs a separate model to provide feedback on the model's outputs which is being improved. In this setting, the task model is often paired with a separately trained feedback model (Yasunaga and Liang, 2020;Madaan et al., 2021;Welleck et al., 2022;Bai et al., 2022b;Aky\u00fcrek et al., 2023). An advantage of this approach is that the feedback model does not need to be a large, general-purpose model like GPT-4. Thus, training smaller feedback models becomes an attractive alternative when a large amount of feedback is available.", "filtered_refids": [[null], ["b52", "b50", null, "b81", "b37"], ["b100", "b38", "b110", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2567, "num_references": 10}
{"corpusid_sectionid": "132053537-s2", "title": "A Short Survey on Sense-Annotated Corpora", "date": "2018-02-13", "section_title": "WordNet", "section": "WordNet (Fellbaum, 1998) has been one of the most widely used knowledge resource in lexical semantics. In fact, it is the de-facto standard sense inventory for Word Sense Disambiguation since many years. The core unit in WordNet is the synset. A synset represents a concept or a meaning which is represented by its various lexicalizations (i.e. senses). For example, the synset defined as motor vehicle with four wheels can be expressed by its synonym senses auto, automobile, machine and motorcar. In what follows we list the main WordNet sense-annotated corpora, using WordNet 3.0 as reference sense inventory. SemCor. The first and most prominent example of senseannotated corpora is SemCor (Miller et al., 1993b). Sem-Cor was manually annotated and consists of 352 documents from the Brown Corpus (Kucera and Francis, 1979) and 226,040 sense annotations. SemCor is the largest manually-annotated corpus and the most used in the literature to train WSD supervised systems Figure 1: Overview of sense inventories with their corresponding sense-annotated corpora. Zhong and Ng, 2010;Raganato et al., 2017b;Luo et al., 2018;Loureiro and Jorge, 2019;Huang et al., 2019). SemEval. SemEval datasets provide reliable benchmarks for testing WSD systems. The main datasets from Senseval and SemEval competitions have been compiled and unified by Raganato et al. (2017a). In particular, the datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 task 1 (Snyder and Palmer, 2004), SemEval-2007 task 17 (Pradhan et al., 2007), SemEval-2013 task 12 , and SemEval-2015 task 13 (Moro and Navigli, 2015). These datasets, which have been mainly used as evaluation benchmarks for WSD systems, contain a total of 7,253 sense annotations. MASC-WSA. The MASC Word Sense Annotation (MASC-WSA) corpus (Ide et al., 2010)is an excerpt of the Manually Annotated Sub-Corpus of American English (Ide et al., 2008, MASC) and the Open American National Corpus (Ide et al., 2002, ANC) containing annotations for 45 distinct lexemes, i.e., lemma-pos pairs, for a total of 441 distinct WordNet word senses. 2 Each word occurrence has been manually annotated on Amazon Mechanichal Turk by roughly 25 persons for a total of 1M annotations. Princeton WordNet Gloss. The Princeton WordNet Gloss Corpus 3 is a sense-annotated corpus of textual definitions (glosses) from WordNet synsets. The corpus was tagged semi-automatically: 330,499 instances were annotated manually while the remaining annotations (i.e. 118,856) were obtained automatically. This corpus of disambiguated glosses has already proved to be useful in shtml tasks such as semantic similarity (Pilehvar et al., 2013), domain labeling (Gonz\u00e1lez et al., 2012) and Word Sense Disambiguation (Baldwin et al., 2008;Agirre and Soroa, 2009;Camacho-Collados et al., 2015).\n\nOntoNotes. OntoNotes (Weischedel et al., 2013) is a corpus from the Linguistic Data Consortium which comprises different kinds of explicitly-tagged syntactic and semantic information, including annotations at the sense level. The OntoNotes corpus consists of documents from diverse genres such as news, weblogs and telephone conversation. Its 5.0 released version contains 264,622 sense annotations.\n\nOMSTI. The task of gathering sense annotations has proved expensive and not easily scalable. That is the reason why more recent approaches have attempted to exploit semi-automatic or automatic techniques. OMSTI 4 (Taghipour and Ng, 2015a, One Million Sense-Tagged Instances), which is a semi-automatically constructed corpus annotated with WordNet senses, is a prominent example. It was built by exploiting the alignment-based WSD approach of Chan and Ng (2005) on a large English-Chinese parallel corpus (Eisele and Chen, 2010, MultiUN corpus). OM-STI, coupled with SemCor, has already been successfully leveraged as training data for training supervised systems (Taghipour and Ng, 2015a;Iacobacci et al., 2016;Raganato et al., 2017a).", "filtered_refids": [["b52", "b63", "b6", "b19", "b51", "b36", "b49", "b30", "b21", "b16", "b37", "b56", "b4", "b31", "b32", "b27", null, "b48", "b0"], ["b61"], ["b57", "b9", null, "b24", "b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 3944, "num_references": 25}
{"corpusid_sectionid": "132053537-s4", "title": "A Short Survey on Sense-Annotated Corpora", "date": "2018-02-13", "section_title": "BabelNet", "section": "BabelNet (Navigli and Ponzetto, 2012) is a wide-coverage multilingual semantic network obtained from the integration of various encyclopedias and dictionaries (WordNet and Wikipedia, inter alia). Being a superset of all these resources, BabelNet brings together lexicographic and encyclopedic knowledge, thus containing both named entities and concepts, and, unlike Wikipedia covering only noun instances, instances have diverse Part-Of-Speech (PoS) tags: nouns, verbs, adjectives and adverbs. Given its multilingual nature (i.e. BabelNet covers over 250 languages), Babel-Net has been used as a sense inventory for annotating text in languages other than English.\n\nSenseDefs. SenseDefs 8 (Camacho-Collados et al., 2019) extends the effort from the Princeton WordNet Gloss Corpus project (see Section 2.1) by automatically disambiguating textual definitions from various heterogeneous sources in 263 languages. The underlying idea lies on leveraging the cross-complementarities of definitions of identical concepts from different languages and resources. The approach couples a graph-based disambiguation method (Moro et al., 2014) with a refinement based on distributional similarity (Camacho-Collados et al., 2016). The proposed method was evaluated on four European languages (English, Spanish, French and Italian) with an estimated precision of over 80%.\n\nEuroSense. The construction of EuroSense 9 (Delli Bovi et al., 2017) follows a similar approach to SenseDefs. In this case, parallel corpora is exploited for a single multilingual disambiguation. The output is a sense-annotated corpus for 21 languages for the Europarl parallel corpus (Koehn, 2005). The estimated precision for four languages isover 80% on average, with a peak of almost 90% for German.", "filtered_refids": [["b39"], ["b38", "b7"], ["b29"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1763, "num_references": 4}
{"corpusid_sectionid": "132053537-s5", "title": "A Short Survey on Sense-Annotated Corpora", "date": "2018-02-13", "section_title": "Train-o-Matic.", "section": "Similarly to the previous approach, Train-o-Matic 10 (Pasini and Navigli, 2017, T-o-M) aims at automatically annotating words from a raw corpus with senses. The main difference with respect to EuroSense and OMSTI lies in the fact that T-o-M does not need parallel data in order to annotate the input corpus. While being language independent and fully automatic, it proved to lead supervised systems to high performance, close or even better than those achieved when a manually annotated corpus (e.g. SemCor) is used for training. Moreover, it has also proved effective in languages other than English : Italian, Spanish, French, German and Chinese.\n\nOneSeC. OneSeC 11 (Scarlini et al., 2019) is the most recent work among those aiming at automatically producing semantically-annotated data. Instead of the well-known \"one sense per discourse\" assumption made by Gale et al. (1992), this work makes a more relaxed hypotesis, i.e., \"one sense per Wikipedia Category\". That is, a noun is used always with the same meaning within a Wikipedia Category. By leveraging this conjecture, OneSeC exploits the texts contained within the pages of a given Wikipedia Category to annotate each noun occurrence therein with its most suitable meaning. The corpora for English showed to be of high-quality, leading a supervised English WSD model, i.e., It Makes Sense (Zhong and Ng, 2010, IMS), to achieve results that are higher than those attained by IMS trained on other automatically generated corpora. Furthermore, OneSeC has been used to generate annotated data for four other European languages, namely: Italian, Spanish, German and French.", "filtered_refids": [[null], ["b53", "b18", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1629, "num_references": 4}
{"corpusid_sectionid": "53296658-s3", "title": "A Survey on Natural Language Processing for Fake News Detection", "date": "2018-11-02", "section_title": "Short Claims", "section": "A recent benchmark dataset for fake news detection is LIAR (Wang, 2017). This dataset includes 12,836 real-world short statements collected from PolitiFact, where editors handpicked the claims from a variety of occasions such as debate, campaign, Facebook, Twitter, interviews, ads, etc. Each statement is labeled with six-grade truthfulness. The information about the subjects, party, context, and speakers are also included in this dataset. Vlachos and Riedel (2014) and Ferreira and Vlachos (2016) are the first to study PolitiFact data, but LIAR is orders of magnitude larger and more comprehensive. However, note that the original LIAR paper does not include the editor's justification or evidence due to copyright concerns, and users will need to retrieve the justification/evidence separately using an API. Also, even though both the claims and the evidence are from real-world occasions, they are highly un-structured. Fact-checking remains relatively challenging for this dataset. Fever ) is a dataset providing related evidences for fake news detection. Fever contains 185,445 claims generated from Wikipedia data. Each statement is labeled as Supported, Refuted, or Not Enough Info. They also marked which sentences from Wikipedia they use as evidence. Fever makes it possible to develop a system which can predict the truthfulness of a claim together with the evidence, even though the type of facts and evidence from Wikipedia may still exhibit some major stylistic differences from those in real-world political campaigns.\n\nPOLITIFACT, CHANNEL4.COM 2 , and SNOPES 3 are three sources for manually labeled short claims in news, which is collected and labeled manually. Many datasets, such as Wang (2017) and Rashkin et al. (2017), are created based on these websites.", "filtered_refids": [["b6", "b34", "b36"], ["b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 1780, "num_references": 4}
{"corpusid_sectionid": "53296658-s4", "title": "A Survey on Natural Language Processing for Fake News Detection", "date": "2018-11-02", "section_title": "Posts On Social Networking Services", "section": "In addition to the websites mentioned above, posts on Social Networking Services (SNS), such as Twitter and Facebook, can also be a source of short news statements. There are some datasets for fake news detection focusing on SNS, but they tend to have a limited set of topics and can be less related to news. BUZZFEEDNEWS 4 collects 2,282 posts from 9 news agencies on Facebook. Each post is factchecked by 5 BuzzFeed journalists. The advantages of this dataset are that the articles are collected from both sides of left-leaning and rightleaning organizations, and they are enriched in   Potthast et al. (2017) by adding data such as the linked articles. BUZZFACE (Santia and Williams, 2018) extends the BuzzFeed dataset with the comments related to news articles on Facebook. It contains 2,263 news articles and 1.6 million comments. SOME-LIKE-IT-HOAX 5 (Tacchini et al., 2017) consists of 15,500 posts from 32 Facebook pages, that is, the public profile of organizations (14 conspiracy and 18 scientific organizations). This dataset is labeled based on the identity of the publisher instead of post-level annotations so that it may have imposed a strong assumption. A potential major pitfall for such dataset is that such kind of labeling strategy can result in machine learning models learning characteristics of each publisher, rather than that of the fake news. PHEME (Zubiaga et al., 2016) and CRED-BANK (Mitra and Gilbert, 2015) are two Twitter datasets. PHEME contains 330 twitter threads (a series of connected Tweets from one person) of nine newsworthy events, labeled as true or false according to thread structures and follow-follower relationships. CREDBANK contains 60 million tweets covering 96 days, grouped into 1,049 events with a 30-dimensional vector of truthfulness labels. Each event was rated on a 5-point Likert scale of truthfulness by 30 human annotators. They simply concatenate 30 ratings as a vector because they find it difficult to reduce it to a one-dimensional score.\n\nAs mentioned above, these datasets were created for verifying the truthfulness of tweets. Thus they are limited to a few numbers of topics and can include tweets with no relationship to news. Hence both datasets are not so much ideal for fake news detection so that they are more frequently 5 https://github.com/gabll/some-like-it-hoax used for rumor detection.", "filtered_refids": [["b21", "b39", "b31", "b17"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2364, "num_references": 4}
{"corpusid_sectionid": "53296658-s5", "title": "A Survey on Natural Language Processing for Fake News Detection", "date": "2018-11-02", "section_title": "Entire-Article Datasets", "section": "There are several datasets for fake news detection focusing on fake news detection based on the entire article. For example, FAKENEWSNET (Shu et al., 2017a(Shu et al., ,b, 2018 is an ongoing data collection project for fake news research. It consists of headlines and body texts of fake news articles from BuzzFeed and PolitiFact. It also collects information about social engagements of these articles from Twitter. BS DETECTOR 6 is collected from a browser extension named BS Detector, which indicates its labels are the outputs of BS Detector, not human annotators. BS Detector searches all links on a web page at issue for references to unreliable sources by checking against a manually compiled list of unreliable domains. Note that the major issue with using this dataset is that the machine learning models trained on this dataset are learning the parameters of the BS Detector.\n\nWebsites such as BLUFF THE LISTENER and THE ONION create sarcastic and humorous (Rubin et al., 2015a) fake news intentionally. Note that the types of fake news from these sources are limited. Moreover, it is relatively easy to classify them against traditional new media articles. A dataset consists of articles from various publishers can be better (Rashkin et al., 2017), though individual claims must be checked. We should also note that one must avoid using aggregate labels simply based on website source, as it adds more confounding variables and it is more of a website classification task.", "filtered_refids": [[null, "b29"], ["b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1484, "num_references": 3}
{"corpusid_sectionid": "53296658-s13", "title": "A Survey on Natural Language Processing for Fake News Detection", "date": "2018-11-02", "section_title": "Preprocessing", "section": "Preprocessing usually includes tokenization, stemming, and generalization or weighting words.\n\nTo convert tokenized texts into features, Term Frequency-Inverse Document Frequency (TF-IDF) and Linguistic Inquiry and Word Count (LIWC) are frequently used.\n\nFor word sequences, pre-learned word embedding vectors such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) are commonly used.\n\nAppropriate preprocessing is necessary for a better understanding of fake news. Mihalcea and Strapparava (2009) use LIWC and find there is a difference in word usage between deceptive language and non-deceptive ones, so using word classification may have significant meaning on detection.\n\nWhen using entire articles as inputs, an additional preprocessing step is to identify the central claims from raw texts.  rank the sentences using TF-IDF and DrQA system (Chen et al., 2017). Solutions to the text summarization task can also be applied.", "filtered_refids": [[], [], ["b16", "b19"], ["b15"], ["b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 948, "num_references": 4}
{"corpusid_sectionid": "53296658-s18", "title": "A Survey on Natural Language Processing for Fake News Detection", "date": "2018-11-02", "section_title": "Neural Network Models", "section": "Many types of neural network models such as multi-layer perceptrons work for fake news detection, and many combinations of models are shown.\n\nRecurrent Neural Network (RNN) is very popular in Natural Language Processing, especially Long Short-Term Memory(LSTM), which solves the vanishing gradient problem. LSTMs can capture longer-term dependencies. For example, Rashkin et al. (2017) set up two types of LSTM model, one put simple word embeddings initialized with GloVe into LSTM, and the other con-catenate LSTM output with LIWC feature vectors before undergoing the activation layer. In both cases, they were more accurate than NBC and Maximum Entropy(MaxEnt) models, even though slightly. Ruchansky et al. (2017) extract representations of both users and articles as low-dimensional vectors, and for representation of articles, they use LSTM for each article. Textual information of each social engagement for an article is processed by doc2vec and put in LSTM, and are integrated with the score of the user in the last layer to classify.\n\nConvolutional neural networks (CNN) are also widely used since they succeed in many text classification tasks. Wang (2017) uses a model based on Kim's CNN (Kim, 2014). They concatenate the max-pooled text representations with the metadata representation from the bi-directional LSTM. CNN also used for analyzation using a variety of meta-data. For example, Deligiannis et al. give graph-like data of relationships between news and publishers to CNN and assess news from them. Karimi et al. (2018) proposed Multi-source Multi-class Fake news Detection framework (MMFD), in which CNN analyzes local patterns of each text in a claim and LSTM analyze temporal dependencies in the entire text. This model takes advantage of the characteristics of both models because LSTM works better for long sentences.\n\nAttention mechanisms are often incorporated into neural networks to achieve better performance. Long et al. (2017) use attention model that incorporates the speakers name and the statements topic to attend to features first, then weighted vectors are fed into an LSTM. Doing this increases accuracy by about 3 % (shown in Table 3, id 3,4). Kirilin and Strube (2018) use a very similar attention mechanism.\n\nMemory networks, which is a kind of attentionbased neural network, also shares the idea of attention mechanism. Pham (2018) uses Single Layer Memory network to learn a different representation of words by memorizing the set of words in the memory. When judging, input sentences weight the words in memory by attention mechanism. Thus the model can extract related words from its memory.\n\nWe compare empirical results on classification datasets via various machine learning models in this section. Table 3 summaries the results on four datasets: LIAR, FAKENEWSNET, FEVER, and PHEME.\n\nIn Table 3, we collect and compare the existing results of fake news classification research. For comparison, we use accuracy, which is a commonly used metric. Other evaluation metrics (Shu et al., 2017a) such as Precision, Recall, Fscores and ROC-AUC are also discussed. Research on fake news detection has been progressing, and the situation has changed since these requirements were defined in 2015. As the performances on fake news detection are improved, the more reality-based and detailed detection becomes more realistic so that new datasets should be useful to develop models realizing such detection. Thus, we add three new recommendations for a new dataset based on cases found in previous research. Concerning developing more realitybased datasets, requirement 10 and 12 should be fulfilled, and concerning more detailed datasets, requirement 11 should be fulfilled. 10: Easy to create from raw data: Pragmatic fake news detection should be performed on emerging news, so models learned from datasets should not require much hand-crafted information. In order to imitate this and set a challenging task, datasets must not include too much information tagged by human except for true-or-false labels. For example, Karimi et al. (2018) supplement LIAR by adding the verdict reports written by label generators. When they do so, the attention score for that reports tends to be high as shown in Table 3 in this paper and raise accuracy by 4%. This could be the problem because verdict reports are highly related to answering and not generated in emerging news. 11: Fine-grained truthfulness: News or claims might be a mixture of true and false statements, so it is not practical to categorize them totally into true or false. When creating human annotators engaged in labeling news tend to believe what they read, shown in Buntain and Golbeck (2017). Besides, the binary classification has already achieved high accuracy around 90% even if inputs are restricted to textual sources Bhattacharjee et al. (2017) achieve over 96% accuracy (Table 3, id16) using only textual data of the claims themselves from LIAR, while 6-class classification is still a challenging task (id 1-14), Della Vedova et al. (2018) achieve almost 90% accuracy even when there is little social engagement data. In order to define a more challenging and practical task, the datasets should include more detailed truthfulness information. 12: Quote claims or articles from various speakers or publisher: When creating a new dataset, data should not be extracted from only one specific publisher, because a model will learn not fake news features but that of publishers. Moreover, when we choose which websites we use, we should be careful to what types of fake news it indicates (Hoaxes, Propaganda or Satire (Rubin et al., 2015a) ). It is easier to use data from fact-checking sites such as PolitiFact, but the labels will rely on editors decision. In this way, we can avoid having confounding variables in the analysis that creates bias and complicates the study. For example, we strongly discourage anyone to use the BS Detector dataset, due to the lack of annotation and strong assumptions: This task is more like a classification of website types vs. fake news.", "filtered_refids": [[], ["b26"], ["b11", "b10"], ["b14"], [], [], ["b2", "b10", "b29", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 45, "num_chars": 6082, "num_references": 8}
{"corpusid_sectionid": "53296658-s19", "title": "A Survey on Natural Language Processing for Fake News Detection", "date": "2018-11-02", "section_title": "Models", "section": "First, we compare how each model process textual content based on NLP. Most models we shared in Table 3 used word embeddings, especially word2vec, for taking the meanings of each text.\n\nThe key to applying machine learning to fake news detection is choosing efficient features from just text with redundant information because features differ among fake news and real news, not among news topics or publishers of the news, should be extracted.  There are some essential features to extract particularly in fake news detection. First, the psycholinguistic categories of words used in the fake news have been proven to be different in some researches since Mihalcea and Strapparava (2009) find characteristics of the word used in deceptive languages. Shu et al. (2017b) achieve 64% accuracy on FAKENEWSNET by only analyzing word usage in LIWC. Thus it is clear analyzation on word usage contributes much to detecting fake news. Second, the rhetorical features may differ in fake news. Rubin and Vashchilko (2012) show that there should be some differences in the structure of sentences in deceptive languages. In Table 3, RST (4.3) is the only framework to learn such features, and achieve 61% accuracy on FAK-ENEWSNET.\n\nHowever, those hand-crafted features extraction may be replaced by neural networks. Rashkin et al. (2017) shows that adding LIWC did not improve the performance of the LSTM model but even harm it while Naive Bayes and MaxEnt models are improved. It may be because some neural network models like LSTM can learn lexical information in LIWC by themselves. There is no such a study on rhetorical features so we cannot conclude, but neural network models may also lean them, considering the RST model(id 17,23) achieve only low accuracies compared to other methods.\n\nHence it may be better to use automated learning methods. For Natural Language Processing, LSTM and attention based method such as attention attachments or memory network is often used. It is because they can analyze longterm and content-transitional information so that they can use the abundant word data of sentences and detect context. Actually, many research in Table 3 use attention methods (id 7,9-14,19,20,25,26,29,30,33,34) or 13,14,33,34,42,46) to learn textual models. A popular application of attention mechanism is to generate attention weights for hidden layers based on meta-data.\n\nSecond, considering additional information other than text in claims or articles, such as speaker credibility or social engagements data is the other efficient and practical method; thus most recent studies mainly focus on this method. Most studies on LIAR improve accuracy by changing the way to introduce not texts but speakers' in-formation because it is difficult to detect a lie from short sentences. Kirilin and Strube (2018) improve accuracy by 21% through replacing the credibility history in LIAR's with a larger credibility source they launched named speak2credit 7 (id 13-14) They show that their attention model relies on speaker's credibility by 43%, much higher than 17% on a statement of claim, by case study.\n\nHowever, the tendency to rely their judgments on speakers or publishers may cause some problem. Vlachos said that the most dangerous misinformation comes from the sources we trust , and upgrading or downgrading specific sources cause silencing minorities' voice (Graves, 2018).\n\nThus he developed new datasets FEVER including evidence so that it can be used for claim verification not only for classification. Such contentbased approaches should be developed more in the future. For claim verification on FEVER, Yin and Roth (2018) improves precision rate to 45% from 10% (the benchmark score). The point is that considering the recall rate does not change that dramatically (from 46% to 50%), this model has less chance of verifying fake claim incorrectly. Research on FEVER is fewer than that on others because this dataset was published very recently and the accuracy, recall and precision rate are relatively low in most studies. There are very latest results in Table 3 (id 35-40), but their performances do not make much difference.\n\nSocial engagements data also shows to be effective. For example, in Shu et al. (2017b) the model using only social engagements data (id 19,25) defeated the model using only textual data (id 17,18,23,24). The same as using speakers credibility, we should think about the proper use of additional data as Della Vedova et al. (2018)(id 21,28) developed model which uses the contentbased method when there are not enough socialengagements-based information and otherwise use mainly social-based one. 7 Related Problems 7.1 Fact Checking Fact checking is the task of assessing the truthfulness of claims made by public figures such as politicians, pundits, etc (Vlachos and Riedel, 2014). Many researchers do not distinguish fake news detection and fact checking since both of them are to assess the truthfulness of claims. However, fake news detection usually only focuses on news events while fact checking is broader.  provides a comprehensive review on this topic.", "filtered_refids": [[], ["b15", "b30", "b25"], ["b22"], [null], ["b12"], ["b7"], ["b37"], ["b30", null, "b34", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 5108, "num_references": 12}
{"corpusid_sectionid": "252819431-s1", "title": "Overview of MSLR2022: A Shared Task on Multi-document Summarization for Literature Reviews", "date": 2022, "section_title": "Task description", "section": "We give a brief description of the datasets, task, evaluation metrics, and submission protocol for the shared task.\n\nDatasets We provided two datasets for model iteration and evaluation. The MS\u02c62 dataset consists of 20k reviews (comprising 470K studies) from the literature to study the task of generating review summaries (DeYoung et al., 2021). Reviews and studies for MS\u02c62 were collected from PubMed. Input studies were filtered from cited articles using keyword heuristics and a SciBERT-based suitability classifier trained on human annotations, and the target summary was extracted from the review abstract using a SciBERT-based sequential sentence classifier trained on manually-labeled sentences from over 200 abstracts (see DeYoung et al. (2021) for details). Target summaries in the test set were manually reviewed and corrected. In addition to the abstracts of input studies and summaries, MS\u02c62 extracts a background section from each review as context for the research question.\n\nThe Cochrane dataset consists of 4.6K reviews from the Cochrane Library (Wallace et al., 2020). 2 The target summaries are the Authors' Conclusions sections of the review abstracts. The Cochrane dataset is smaller and more consistent than the MS\u02c62 dataset since all Cochrane reviews follow a similar process. For more information on dataset construction, please refer to the original dataset papers (DeYoung et al., 2021;Wallace et al., 2020).\n\nTask Given the abstracts of input studies pertaining to a research question (and in the case of MS\u02c62, a background section describing that research question), the task is to produce a summary that synthesizes the information from the input studies. The synthesis of information typically results in an evidence \"direction,\" e.g., the evidence overall suggests that the intervention studied increases/decreases/does not change the outcome measure for the studied population (DeYoung et al., 2020). The direction of the evidence indicated in a good generated summary should agree with that in the reference (gold) summary.\n\nEvaluation We perform automated evaluation using ROUGE (Lin, 2004), BERTScore (Zhang et al., 2020), and the evidence inference (Lehman et al., 2019) divergence metric defined in Wallace et al. (2020) and modified by DeYoung et al. (2021). For ROUGE, we report ROUGE-1, ROUGE-2, and ROUGE-L. For the evidence inference-based metric, we report the average divergence (\u2206EI Avg) and the Macro-F1 (\u2206EI F1) computed using a model trained on the dataset provided by DeYoung et al. (2020).\n\nFor human evaluation, we developed and iterated on an annotation protocol based on the analysis conducted by Otmakhova et al. (2022b). For each annotation task, annotators are shown a gold summary and a generated summary and asked to assess the latter for (i) fluency and (ii) agreement with the gold summary in terms of the \"PICO\" element alignment, 3 evidence inference directional agreement, and alignment regarding the strength of the claims made in summaries. We will provide further details on human annotation results following the shared task meeting.\n\nSubmissions Leaderboards for submissions are provided for the two subtasks: MS\u02c62 4 and Cochrane. 5 Submissions to the leaderboard are judged against the gold summaries in the test splits using the automated metrics described previously.", "filtered_refids": [[], ["b3"], [null, "b18", "b3"], [null], ["b10", "b18", null, "b22", "b8", "b3"], ["b13"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3338, "num_references": 12}
{"corpusid_sectionid": "252819431-s3", "title": "Overview of MSLR2022: A Shared Task on Multi-document Summarization for Literature Reviews", "date": 2022, "section_title": "Participating systems", "section": "We provide brief descriptions of all participating systems. System performance as assessed using automated evaluation metrics are given in Table 1.\n\nITTC (Otmakhova et al., 2022a) The team adapted PRIMERA (Xiao et al., 2022), a model based on Longformer Encoder-Decoder (Beltagy et al., 2020) that has been designed for multidocument summarization, resulting in strong performance on the MSLR Cochrane subtask. In addition to fine-tuning on the entire training sets of the MSLR shared task, the team also experimented with zero-and few-shot learning scenarios. The authors found that ROUGE did not adequately capture the performance drops observed in the zeroand 10-shot settings, where factuality of the generated summaries was poor. The team also experiment with using global attention to highlight PICO elements in the input and target texts. Though ROUGE did not vary significantly between these two settings, the authors found that when PICO spans are given global attention, the resulting summaries tended to be more abstractive.\n\nLongT5-Pubmed (Yu, 2022) The author attempted to finetune a LongT5 model (Guo et  PuneICT (Tangsali et al., 2022) The team experimented with finetuning BART-large, DistillBART, and T5-base for both the MS\u02c62 and Cochrane subtasks. On the MS\u02c62 subtask, finetuned BART-large had the highest performance of the three models based on ROUGE score; on the Cochrane subtask, DistillBART performed best.\n\nSciSpace (Shinde et al., 2022) The team combined a BERT-based extractive method with a Big-Bird PEGASUS-based abstractive summarization model (Zaheer et al., 2020), leading to strong performance on the MSLR Cochrane subtask. For the extractive step, the authors use a Lecture Summarizer model to identify the most important sentences from the input documents; this method encodes input sentences using BERT, then clusters the contextual representations and selects the sentences closest to the cluster centroids. The resulting sentences are used as input into a BigBird PEGA-SUS model pretrained on Pubmed, which is finetuned on the MSLR training data. In analysis, the authors observed that a common error is duplication of statements in the generated summary. The model submitted by the team to the Cochrane subtask leaderboard performs best among submissions based on ROUGE-L, though the authors report that the same training strategy does not lead to good performance on the MS\u02c62 subtask due to the much longer input sequences in MS\u02c62.\n\nLED-base-16k (Giorgi et al., 2022) The team fine-tuned Longformer Encoder-Decoder following a similar protocol to PRIMERA (Xiao et al., 2022), improving performance over baselines in both subtasks. Their input sequence included the titles and abstracts of up to 25 studies, separated by special tokens. No system description was submitted.", "filtered_refids": [[], ["b12", "b19", "b0"], [null, "b20", "b17"], ["b15", "b21"], ["b19", "b6"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2813, "num_references": 10}
{"corpusid_sectionid": "252818977-s5", "title": "Abstractive Approaches To Multidocument Summarization Of Medical Literature Reviews", "date": 2022, "section_title": "Training Details", "section": "For training the models we used the Simple Transformers 2 library, an API used for transformer mod-   els (Vaswani et al., 2017), which provides built-in support for various natural language processing tasks including text summarization. We trained our models on the Nvidia K80 GPU which has a GPU RAM of 15 gigabytes. CUDA was utilized for effective computing, and making the training and evaluation processes faster. All the models were trained on 10 epochs, with training and validation losses measured over time for each epoch.\n\nWe trained the BART-large and the DistilBART-CNN models on the datasets, by instantiating Seq2Seq models (Sutskever et al., 2014) and arguments provided by Simple Transformers. We later modified some of the arguments by making the maximum length for each sequence equal to 140. Due to limited RAM available on the CUDA used, we faced memory errors. Hence, after each epoch, the weights directory was overwritten for memory availability. Maximum sequence length for the tokenized sequences of each input document was set to 512. For T5 (Text-To-Text Transfer Transformer), we used the t5-base models (Raffel et al., 2020b), after providing t5-base tokenization, and trained them with the same aforementioned hyperparameters.\n\nAll the above mentioned hyperparameters were giving the best possible results, and hence we proceeded with the use of the same. We finetuned the basic configurations specified in the Fairseq documentation. 3", "filtered_refids": [["b20"], ["b19", "b17"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1465, "num_references": 3}
{"corpusid_sectionid": "210936331-s2", "title": "A Survey of the Perceived Text Adaptation Needs of Adults with Autism", "date": "2019-10-22", "section_title": "Autism Spectrum Disorder and Reading", "section": "Autism Spectrum Disorder (ASD) is a developmental disorder with neural origin characterised by impairment in communication and social interaction (American Psychiatric Association, 2013) and is known to affect about 1 in 100 people in the UK (Brugha et al., 2011). Language comprehension difficulties in autism cover phenomena such as difficulties in syntax processing of long sentences (Whyte et al., 2014), resolving ambiguity in meaning (Happe, F., and Frith, U, 2006), and identifying pronoun referents (O'Connor and Klein, 2004), as well as having difficulties in figurative language comprehension and making pragmatic inferences (MacKay and Shaw, 2004). These difficulties, together with the specific cognitive profile of individuals with autism (e.g., differences in the Theory of Mind (Baron-Cohen, 2000)) may lead to secondary issues such as challenges with identifying author intent and subtler nuances of meaning. In addition, web users with autism have been consistently shown to have different information searching strategies when processing web pages (Eraslan et al., 2017;Yaneva et al., 2018;Eraslan et al., 2019;, which relate to differences in visual attention. As a result of these difficulties, information contained in online user feedback can be less accessible for people with autism.", "filtered_refids": [["b11", "b6", "b4", "b13", "b20", null, "b3", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1307, "num_references": 8}
{"corpusid_sectionid": "210936331-s3", "title": "A Survey of the Perceived Text Adaptation Needs of Adults with Autism", "date": "2019-10-22", "section_title": "Automatic Text Adaptation for Adults with Autism", "section": "In terms of systems aimed at making text more accessible for autistic individuals who are fairly able, the OpenBook tool 1 is the most comprehensive existing system to date. The tool provides semi-automatic conversion of text documents by reducing syntactic complexity and disambiguating meaning by resolving pronominal reference, performing word sense disambiguation and detecting conventional metaphors Or\u0203san et al., 2018), with some initial efforts towards concept substitutions for images (Barbu et al., 2015). As part of the research project, the tool was evaluated together with end-users with ASD who were shown to find the adapted texts more accessible than the originals. Nevertheless, a major impediment for the automatic evaluation of such systems is the limited amount of userevaluated data. To the best of our knowledge, the only available resources containing a limited amount of such data are the ASD corpus (Yaneva et al., 2016a;Yaneva, 2016), followed by a corpus of easy-to-read documents that were specifically developed for people with cognitive disabilities (Yaneva et al., 2016b) 2 . Constrained by these limitations, some approaches propose to automatically evaluate text simplification systems for people with autism in terms the change in readability of the generated sentences Stajner and Saggion, 2013), the incorporation of user-evaluated data into larger corpora , or the use of corpora containing texts for children and language-learners (\u0160tajner et al., 2014). Therefore, very little is known about the perceptions of adults with high-functioning autism on the usefulness of specific simplification strategies.\n\nIn the following sections we present a survey on the perceptions of adults with high-functioning autism on the accessibility of user reviews.", "filtered_refids": [["b18", "b14", null, "b22", "b23", "b1"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1785, "num_references": 6}
{"corpusid_sectionid": "14225710-s1", "title": "Survey on the Use of Typological Information in Natural Language Processing", "date": "2016-10-11", "section_title": "Overview of Linguistic Typology", "section": "Languages may share universal features on a deep, abstract level, but the structures found in real-world, surface-level natural language vary significantly. This variation is conventionally characterised into 'languages' (e.g. French, Hindi, Korean) 2 , and linguistic typology describes how these languages resemble or differ from one another. The field comprises three pursuits: the definition of language features and their capacity for variance, the measurement and analysis of feature variance across empirical data, and the explanation of patterns observed in this data analysis. Bickel (2007) terms these three pursuits qualitative, quantitative and theoretical typology, respectively. Typological classifications of languages have strict empirical foundations. These classifications do often support theories of causation, such as historical, areal or phylogenetic relations, but importantly, these hypotheses come second to quantitative data (Bickel, 2007). Indeed, patterns of variance may even run contrary to established theories of relations between languages based on geographical or historical proximity. For instance, Turkish and Korean are typically considered to be highly divergent in lexical features, yet their shared syntactic features make the two languages structurally quite similar. Such indications of similarity are of value for NLP which primarily seeks to model (rather than explain) cross-linguistic variation.\n\nTypologists define and measure features according to the task at hand. Early studies, focused on word order, simply classified languages as SVO (Subject, Verb, Object), VSO, SOV, and so forth (Greenberg, 1963). There are now more various and fine-grained studies based on a wide range of features, including phonological, semantic, lexical and morphosyntactic properties (see (Bickel, 2007;Daniel, 2011) for an overview and further references). While a lot of valuable information is contained in these linguistic studies, this information is often not readily usable by NLP due to factors such as information overlap and differing definitions across studies. However, there is also a current trend towards systematically collecting typological information from individual studies in publicly-accessible databases, which are suitable for direct application in NLP (e.g., for defining features and their values). Table 1 presents a selection of current major databases, including the Syntactic Structures of the World's Languages (SSWL) (Collins and Kayne, 2009), the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013), the Phonetics Information Base and Lexicon (PHOIBLE) (Moran et al., 2014), the URIEL Typological Compendium (Littel et al., 2016), the Atlas of Pidgin and Creole Language Structures (APiCS) (Michaelis et al., 2013), and the Lyon-Albuquerque Phonological Systems Database (LAPSyD) (Maddieson et al., 2013). The table provides some basic information about these databases, including type, coverage, and additional notes. From these databases, WALS is currently by far the most commonly-used typological resource in NLP due to its broad coverage of features and languages.\n\nWe next discuss the potential of typological information to guide multilingual NLP and the means by which this can be done.", "filtered_refids": [["b8"], ["b18", "b14", "b28", null, "b42", "b38", "b8", "b37"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3276, "num_references": 9}
{"corpusid_sectionid": "14225710-s2", "title": "Survey on the Use of Typological Information in Natural Language Processing", "date": "2016-10-11", "section_title": "Multilingual NLP and the Role of Typologies", "section": "The recent explosion of language diversity in electronic texts has made it possible for NLP to move increasingly towards multilingualism. The biggest challenge in this pursuit has been resource scarcity. In order to achieve high quality performance, NLP algorithms have relied heavily on manually crafted resources such as large linguistically-annotated datasets (treebanks, parallel corpora, etc.) and rich lexical databases (terminologies, dictionaries, etc.). While such resources are available for key NLP tasks (POS tagging, parsing, etc.) in well-researched languages (e.g. English, German, and Chinese), for the majority of other languages they are lacking altogether. Since resource creation is expensive and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem.\n\nOne avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009;Reichart and Rappoport, 2010;Snyder, 2010;Spitkovsky et al., 2011;Goldwasser et al., 2011;Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T\u00e4ckstr\u00f6m et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010;Zhang et al., 2012;T\u00e4ckstr\u00f6m et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance.\n\nLanguage Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005;McDonald et al., 2011;Petrov et al., 2012;Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008;McDonald et al., 2011;S\u00f8gaard, 2011;Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language.\n\nWhile such a transfer approach outperforms unsupervised learning, it does not achieve optimal performance. One potential reason for this is that the tagset used by a POS tagger may not fit a target language which exhibits significantly different morphological features to a source language for which the tagset was initially developed (Petrov et al., 2012). Although parallel data can be used to give additional guidance which improves transfer (McDonald et al., 2011), such data are only available for some language pairs and cannot be used in truly resource-poor situations.\n\nAn alternative direction that has recently emerged uses typological information as a form of non-parallel guidance in transfer. This direction capitalises on the fact that languages do exhibit systematic crosslingual connections at various levels of linguistic description (e.g. similarities in language structure), despite their great diversity. Captured in typological classifications at the level of generalisation useful for NLP, such information can be used to support multilingual NLP in a variety of ways (Bender, 2011). For example, it can be used to define the similarity between two languages with respect to the linguistic information one hopes to transfer; it can also help to define the optimal degree, level and method of transfer. For example, direct transfer of POS tagging is more likely to succeed when languages are sufficiently similar in terms of morphology in particular (Hana et al., 2004;Wisniewski et al., 2014).\n\nTypological information has been used to guide language transfer mostly in the areas of part-of-speech tagging and parsing, e.g. (Cohen and Smith, 2009;McDonald et al., 2011;Berg-Kirkpatrick and Klein, 2010;Naseem et al., 2012;T\u00e4ckstr\u00f6m et al., 2013). Section 4 surveys such works in more detail.\n\nMultilingual Joint Learning Another approach involves learning information for multiple languages simultaneously, with the idea that the languages will be able to support each other (Snyder, 2010;Navigli and Ponzetto, 2012). This can help in the challenging but common scenario where none of the languages involved has adequate resources. This applies even with English, where annotations needed for training basic tools are primarily available only for newspaper texts and a handful of other domains. In some areas of NLP, e.g. word sense disambiguation (Navigli and Ponzetto, 2012), multilingual learning has outperformed independent learning even for resource-rich languages, with larger gains achieved by increasing the number of languages.\n\nSuccess has also been achieved on morphosyntactic tasks. For example, Snyder (2010) observes that cross-lingual variations in linguistic structure correspond to systematic variations in ambiguity, so that what one language leaves implicit, another one will not. For instance, a given word may be tagged as either a verb or a noun, yet its equivalent in other languages may not present such ambiguity. Together with his colleagues, Snyder exploited this variation to improve morphological segmentation, POS tagging, and syntactic parsing for multiple languages. Naseem et al. (2012) introduced a selective sharing approach to improve multilingual dependency parsing where the model first chooses syntactic dependents from all the training languages and then selects their language-specific ordering to tie model parameters across related languages. Because the ordering decisions are influenced by languages with similar properties, this cross-lingual sharing is modelled using typological features. In such works, typological information has been used to facilitate the matching of structural features across languages, as well as in the selection of languages between which linguistic information should be shared.", "filtered_refids": [[], ["b11", "b75", "b55", "b27", "b66", "b45", null, "b64", "b61"], ["b52", "b57", "b41", "b32", "b62", "b74", "b73"], ["b52", "b41"], ["b30", "b72", "b4"], ["b11", "b46", "b41", "b66", "b5"], ["b47", "b61"], ["b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 6680, "num_references": 29}
{"corpusid_sectionid": "14225710-s3", "title": "Survey on the Use of Typological Information in Natural Language Processing", "date": "2016-10-11", "section_title": "Development of Universal Models", "section": "A long-standing goal that has gained renewed interest recently is the development of language-independent (i.e. universal) models for NLP (Bender, 2011;Petrov et al., 2012). Much of the recent interest has been driven by the Universal Dependencies (UD) initiative. It aims to develop cross-linguistically consistent treebank annotation for many languages for the purposes of facilitating multilingual parser development and cross-lingual learning (Nivre et al., 2016). The annotation scheme is largely based on universal Stanford dependencies (de Marneffe et al., 2014) and universal POS tags (Petrov et al., 2012). UD treebanks have been developed for 40 languages to date. Whilst still biased towards contemporary Indo-European languages, the collection developed by this initiative is gradually expanding to include additional language families.\n\nThe development of a truly universal resource will require taking into account typological variation for optimal coverage. For example, while the current UD scheme allows for language-specific tailoring, in the future, language type-specific tailoring may offer a useful alternative, aligned with the idea of universal modeling (Bender, 2011).", "filtered_refids": [["b52", "b48", "b4", "b21"], ["b4"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1193, "num_references": 5}
{"corpusid_sectionid": "14225710-s5", "title": "Survey on the Use of Typological Information in Natural Language Processing", "date": "2016-10-11", "section_title": "Development of Typological Information for NLP", "section": "Typological information has been obtained using two main approaches: i) extraction from manually constructed linguistic resources, such as the databases reviewed in \u00a72; and ii) automatic learning. The two methods have been used independently and in combination, and both are based on the assumption (be it explicit or implicit) that typological relations may be fruitfully used in NLP.\n\nManual Extraction from Linguistic Resources Manually crafted linguistic resources -in particular the WALS database -have been the most commonly used sources of typological information in NLP. To date, syntactic parsing (Naseem et al., 2012;T\u00e4ckstr\u00f6m et al., 2013;Zhang and Barzilay, 2015;Ammar et al., 2016) and POS tagging (Zhang et al., 2012;Zhang et al., 2016) were the predominant areas for integration of structural information from such databases. In the context of these tasks, the most frequently used features related to word ordering according to coarse syntactic categories. Additional areas with emerging research which leverages externally-extracted typological features are phonological modeling (Tsvetkov et al., 2016;Deri and Knight, 2016) and language learning (Berzak et al., 2015).\n\nWhile information obtained from typological databases has been successfully integrated in several NLP tasks, a number of challenges remain. Perhaps the most crucial challenge is the partial nature of the documentation available in manually-constructed resources. For example, WALS currently covers about 17% of its possible feature values (Dryer and Haspelmath, 2013) (see Table 1 for feature coverage of other typological databases). The integration of information from different databases is challenging due to differences in feature taxonomies as well as information overlap across repositories. Furthermore, available typological classifications contain different feature types, including nominal, ordinal and interval variables, and features that mix several types of values. This property hinders systematic and efficient encoding of such features in NLP models -a problem which thus far has only received a partial solution in the form of feature binarisation (Georgi et al., 2010). Further, typological databases are constructed manually using limited resources, and do not contain information on the distribution of feature values within a given language. This results in incomplete feature characterisations, as well as inaccurate generalisations. For example, WALS encodes only the dominant noun-adjective ordering for French, although in some cases this language also permits the adjective-noun ordering.\n\nOther aspects of typological databases may require feature pruning and preprocessing prior to use. For example, some features in WALS, such as feature 81B \"Languages with two Dominant Orders of Subject, Object, and Verb\" are applicable only to a subset of the world's languages. Currently, no explicit specification for feature applicability is present in WALS or other typological resources. Furthermore, distinct features may encode overlapping information, as in the case of WALS features 81A \"Order of Subject Verb and Object\" and 83A \"Order of Verb and Object\", where the latter can be deduced from the former. Although many of these issues have been noted in previous research (\u00d6stling, 2015), there are currently no standard procedures for preprocessing typological databases for NLP use.\n\nDespite the caveats presented above, typological resources do offer an abundance of valuable structural information which can be integrated in many NLP tasks. This information is currently substantially underutilised. Out of 192 available features in WALS, only a handful of word order features are typically used to enhance multilingual NLP. Meanwhile, the complementary information on additional languages and feature types offered by other repositories has, to our knowledge, rarely been exploited in NLP. This readily-available information could be used more extensively in tasks such as POS tagging and syntactic parsing, which have already gained from typological knowledge, and it could also be used to support additional areas of NLP.\n\nAutomatic Learning of Typological Information The partial coverage of existing typological resources, stemming from the difficulty of obtaining such information manually, have sparked a line of work on automatic acquisition of typological information. Here too, WALS has been the most common reference for defining the features to be learned.\n\nSeveral approaches were introduced for automatic induction of typological information through multilingual word alignments in parallel texts. Mayer and Cysouw (2012) use alignments to induce language similarities, and use this approach to support learning of fine-grained features, such as the typology of person interrogatives (e.g., English \"who\"). In\u00d6stling (2015) multilingual word alignments are used to project POS tags and syntactic trees for translations of the New Testament, and subsequently learn typological information relating to word order. The predicted typological features, when evaluated against WALS, achieve high accuracy. This method not only extends WALS word order documentation to hundreds of new languages, but also quantifies the frequency of different word orders across languagesinformation that is not available in manually crafted typological repositories.\n\nTypological information can also be extracted from Interlinear Glossed Text (IGT). Such resources contain morphological segmentation, glosses and English translations of example sentences collected by field linguists. Lewis and Xia (2008) and Bender et al. (2013) demonstrate that IGT can be used to extract typological information relating to word order, case systems and determiners for a variety of languages.\n\nAnother line of work seeks to increase the coverage of typological information using existing information in typological databases. Daum\u00e9 III and Campbell (2007) and Bakker (2008) use existing WALS features to learn typological implications of the kind pioneered by Greenberg (1963). Such rules can then be used to predict unknown feature values for new languages. Georgi et al. (2010) use documented WALS features to cluster languages, and subsequently predict new feature values using nearest-neighbour projection. A classifier-based approach for predicting new feature values from documented WALS information is presented in (Takamura et al., 2016). Coke et al. (2016) predict word order typological features by combining documented typological and genealogical features with the multilingual alignment approach discussed above.\n\nAn alternative approach for learning typological information uses English as a Second Language (ESL) texts (Berzak et al., 2014). This work demonstrates that morphosyntactic typological similarities between languages are largely preserved in second language structural usage. It leverages this observation to approximate typological similarities between languages directly from ESL usage patterns and further utilise these similarities for nearest neighbor prediction of typological features. The method evaluates competitively compared to baselines in the spirit of (Georgi et al., 2010) which rely on existing typological documentation of the target language for determining its nearest neighbors.\n\nIn addition, a number of studies learned typological information tailored to the particular task and data at hand (i.e. task-based development). For example, Song and Xia (2014) process Ancient Chinese using Modern Chinese parsing resources. They manually identify and address statistical patterns in variation between monolingual corpora in each language, and ultimately optimise the model performance by selectively using only the Modern Chinese features which correspond to Ancient Chinese features.\n\nAlthough automatically-learned typological classifications have not been used frequently to date, they hold great promise for extending the use of typological information in NLP. Furthermore, such work offers an additional axis of interaction between linguistic typology and NLP, namely using computational modeling in general and NLP in particular to assist linguistic documentation and analysis of typological information. We discuss the future prospects of these research directions in \u00a7 6.", "filtered_refids": [[], ["b75", "b46", "b66", "b22", "b7", "b69", "b74", "b76", "b0"], ["b25"], [null], [], [], ["b40"], ["b3", "b36"], ["b67", "b28", "b13", "b25", "b20", "b2"], ["b6", "b25"], ["b63"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 51, "num_chars": 8326, "num_references": 23}
{"corpusid_sectionid": "14225710-s6", "title": "Survey on the Use of Typological Information in Natural Language Processing", "date": "2016-10-11", "section_title": "Uses of Typological Information in NLP", "section": "Multilingual Syntactic Parsing As mentioned in \u00a7 4.1, the main area of NLP in which information from structural typology has been exploited thus far is multilingual dependency parsing. In this task, a priori information about the predominant orderings of syntactic categories across languages are used to guide models when parsing a resource-poor language and using training data from other languages. This information is available in typological resources (e.g., WALS) which, among a variety of other syntactic features, list the dominant word orderings for many languages (see Table 1).\n\nA seminal work that integrates typological word order information in multilingual dependency parsing (Naseem et al., 2012) presents the idea of \"selective sharing\" between source and target languages. In brief, while the identity of possible dependents for a given syntactic category is (hypothesised to be) language-universal, their ordering is language-specific. The work then presents a generative multilingual parsing model in which dependent ordering parameters are conditioned on word order typology, obtained from WALS. Specifically, the paper utilises the following word order features (henceforth WALS Basic word Order, WBO): 81A (Subject Verb and Object), 85A (Adposition and Noun), 86A (Genitive and Noun), 87A (Adjective and Noun), 88A (Demonstrative and Noun) and 89A (Numeral and Noun). This information enables the model to take into account dependent orderings only when the source language has a similar word order typology to the target language. In a similar vain, T\u00e4ckstr\u00f6m et al. (2013) present an instance of the typologically guided selective sharing idea within a discriminative parsing framework. They group the model features into features that encode arc directionality and word order, and those that do not. The former group is then coupled with the same WBO features used by Naseem et al. (2012) via feature templates that match the WALS properties with their corresponding POS tags. Additional features that group languages according to combinations of WALS features as well as coarse language groups (Indo-European versus Altaic), result in further improvements in parsing performance. Zhang and Barzilay (2015) extended the selective sharing approach for discriminative parsing to tensor-based models using the same WBO features as in (Naseem et al., 2012) and (T\u00e4ckstr\u00f6m et al., 2013). While traditional tensor-based parsers represent and assign non-zero weights to all possible combinations of atomic features, this work presents a hierarchical architecture that enables discarding chosen feature combinations. This allows the model to integrate prior typological knowledge, while ignoring uninformative combinations of typological and dependency features. At the same time, it capitalises on the automatisation of feature construction inherent to tensor models to generate combinations of informative typology-based features, further enhancing the added value of typological priors.\n\nAnother successful integration of externally-defined typological information in parsing is the work of Ammar et al. (2016). They present a multilingual parser trained on a concatenation of syntactic treebanks of multiple languages. To reduce the adverse impact of contradicting syntactic information in treebanks of typologically distinct languages, while still maintaining the benefits of additional training data for cross-linguistically consistent syntactic patterns, the parser encodes a language-specific bias for each given input language. This bias is based on the identity of the language and its WBO features as used in (Naseem et al., 2012;T\u00e4ckstr\u00f6m et al., 2013;Zhang and Barzilay, 2015). Differently from prior work, their parsing model also encodes all other features in the WALS profile of the relevant language. Overall, this strategy leads to improved parsing performance compared to monolingually trained baseline parsers.\n\nWhile the papers surveyed above use prior information about word order typology extracted from WALS, word order information for guiding multilingual parsing can also be extracted in a bottom-up, data-driven fashion, without explicit reference to typological taxonomies. For example, in S\u00f8gaard (2011), training sentences in a source language are selected based on the perplexity of their coarse POS tag sequence under a target language POS language model. This approach essentially chooses sentences that exhibit similar word orderings in both source and target languages, thus realizing a bottom-up variant of the typology-based selective sharing methods discussed above.\n\nThere are also several methods which have made use of less explicit typological information. For instance, Berg-Kirkpatrick and Klein (2010) selectively combine languages in their method for cross-lingual dependency grammar induction using a phylogeny tree, which has been constructed from external (unspecified) knowledge of language families. Zeman and Resnik (2008) demonstrate improved performance of cross-lingually transferred dependency parsers within sets of typologically similar languages (e.g. Swedish-Danish, Hindi-Urdu); they do not explain how languages may be determined as \"closely-related\", though presumably this decision was based on the intuition of the researchers or on widely-acknowledged generalisations.\n\nPOS Tagging, Phonological Modeling and Language Learning Besides dependency parsing, several other areas have started integrating typological information in various forms. A number of such works revolve around the task of POS tagging. For example, in Zhang et al. (2012), the previously discussed WBO features were used to inform mappings from language-specific to a universal POS tagset. In (Zhang et al., 2016), WBO feature values are used to evaluate the quality of a multilingual POS tagger.\n\nAnother application area which benefited from integration of typological knowledge are phonological models of text. In (Tsvetkov et al., 2016) a multilingual neural phoneme-based language model is trained on several languages using a shared phonological inventory. The model is conditioned on the identity of the language at hand, as well as its phonological features obtained from a concatenation of phonological features from WALS, PHOIBLE and Ethnologue, extracted from URIEL. The resulting model subsumes and outperforms monolingually trained models for phone sequence prediction. Deri and Knight (2016) use URIEL to obtain phone and language similarity metrics, which are used for adjusting Grapheme to Phoneme (G2P) models from resource rich to resource poor languages. Berzak et al. (2015) use typological classifications to study language learning. Formalizing the theory of \"Contrastive Analysis\" which aims to analyse learning difficulties in a foreign language by comparing native and foreign language structures, they build a regression model that predicts language-specific grammatical error distributions by comparing typological features in the native and foreign languages.\n\n5 Typological Information and NLP: What's Next? \u00a7 4.2 surveyed the current uses of typological information in NLP. Here we discuss several future research avenues that might benefit from tighter integration of linguistic typologies and multilingual NLP.\n\nEncoding Typological Information in Traditional Machine Learning-based NLP One of the major open challenges for typologically-driven NLP is the construction of principled mechanisms for the integration of typological knowledge in machine learning-based algorithms. Here, we briefly discuss a few traditional machine learning frameworks which support encoding of expert information, and as such hold promise for integrating typological information in NLP.\n\nEncoding typological knowledge into machine learning requires mechanisms that can bias learning (parameter estimation) and inference (prediction) of the model towards predefined knowledge. Algorithms such as the structured perceptron (Collins, 2002) and structured SVM (Taskar et al., 2004) iterate between an inference step and a parameter update step with respect to gold training labels. The inference step is a natural place for encoding external knowledge through constraints. It biases the prediction of the model to agree with external knowledge, which, in turn, affects both the training process and the final model prediction. As typological information often reflects tendencies rather than strict rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010;Rush et al., 2012), information extraction (Riedel and McCallum, 2011;, and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in \u00a74.2, these type of frameworks could expedite principled integration of typological information in NLP.\n\nTypologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations -i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010;Collobert et al., 2011;Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012;Hermann and Blunsom, 2014;Coulmance et al., 2015;Vuli\u0107 and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually improves the alignment between the induced WEs and the meaning of the participating words in each of the involved languages. Naturally, as these models become more established and better understood, the challenge of external knowledge encoding becomes more prominent. Recent work has examined the ability to map from word embeddings to interpretable typological representations (Qian et al., 2016). Furthermore, a number of works (Faruqui et al., 2015;Rothe and Sch\u00fctze, 2015;Osborne et al., 2016;Mrk\u0161i\u0107 et al., 2016) proposed means through which external knowledge from structured knowledge bases and specialised linguistic resources can be encoded in these models. The success of these works suggests that more extensive integration of external linguistic knowledge in general, and typological knowledge in particular, is likely to play a key role in the future development of WE representations.\n\nCan NLP Support Typology Construction? As discussed in \u00a74, typological resources are commonly constructed manually by linguists. Despite the progress made in recent years in the digitisation and collection of typological knowledge in centralised repositories, their coverage remains limited. Following the work surveyed in \u00a74.1 on automatic learning of typological information, we believe that NLP could play a much larger role in the study of linguistic typology and the expansion of such resources. Future work in these directions will not only assist in the global efforts for language documentation, but also substentially extend the usability of such resources for NLP purposes.", "filtered_refids": [[], ["b66", "b74", "b46"], ["b66", "b74", "b46", "b0"], ["b62"], ["b73", "b5"], ["b75", "b76"], ["b69", "b22", "b7"], [], [], ["b15", "b39", "b26", "b60", "b56", "b9", "b29", "b59", "b24", "b68"], ["b10", "b34", "b58", "b44", "b53", "b35", "b71", "b49", null, "b23", "b70", "b16", "b17"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 69, "num_chars": 12181, "num_references": 38}
{"corpusid_sectionid": "250390996-s4", "title": "The Why and The How: A Survey on Natural Language Interaction in Visualization", "date": 2022, "section_title": "Discover Task", "section": "Using natural language to discover information is one of the most common visualization tasks targeted by V-NLI. Brehmer and Munzner (2013) differentiate between different levels of task granularity such as discover -search -query (see Figure 2). The discovery of concepts, objects, and relationships in a visualization depends on the role that the user and the interface take in the visualizationoriented dialogue, as well as on the concreteness of the user's intent. Intents are formulated in oral or written form. Less concrete user intents lead to a more exploratory character of the search. Concrete intents formalized in a query lead to a specific system response. Vague and fuzzy intents are much more difficult to formalize in a single query and must be inferred by the V-NLI through the application of intelligent recommendations or user guidance. interaction to a multi-turn interactive visual exploration also referred to as analytical conversation. Analytical conversation is the support of visual analysis processes by V-NLI with the aim of inspecting visual features through a visualization-oriented human-machine dialogue, as studied by Turkay and Henkin (2018); Aurisano et al. (2015). In contrast to visualization creation (see section 3.4), where visualizations are generated based on natural language text, the manipulation or composition of a visualization in the query dialog is used in the sense of a speech act. The produced or manipulated visualization can be seen here as a dynamically generated visual response to a user query with the goal of providing information in the dialog. Setlur and Tory (2017); Hoque et al.  , 2015;Yang et al., 2016;Anderson et al., 2018) with the goal to answer questions related to the visual content of images. In VIS, the aim is to answer complex questions related to visual models such as charts or scientific illustrations as in Singh and Shekhar (2020); Chaudhry et al. (2020). Infographics as sophisticated arrangements of visual elements and text are supported by VQA in Mathew et al. (2021). Meeting the high informative standards of response generation required to harness the explanatory purposes of visualizations presents itself as a challenging task.\n\nBrowsing. Browsing supports users with a vague or fuzzy data-related intent in discovering visualizations. The idea is to narrow down the user intent through language interaction using text input, multi-step questions, or dialogue and suggest appropriate next steps in the interaction with the visualization. Luo et al. (2018) use keyword input to execute personalized visualization recommendations. Other approaches leverage auto-completion in text input Dhamdhere et al., 2017) ", "filtered_refids": [[null, "b2", "b4", "b3"], ["b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2700, "num_references": 5}
{"corpusid_sectionid": "250390996-s5", "title": "The Why and The How: A Survey on Natural Language Interaction in Visualization", "date": 2022, "section_title": "Produce Task", "section": "Brehmer and Munzner (2013) refer to produce as a 'reference to tasks in which the intent is to generate new artifacts'. Artifacts generated through natural language interaction are, e.g., annotations of objects in a visualization, scene descriptions, or task reports as used, e.g., in medical visual analysis.\n\nAnnotation. Annotating areas of interest, comparing them among each other, and sharing them with colleagues is a common language interaction while working with visualizations ( Vanhulst et al. (2021), who propose a classification framework that enables a structured capture and ordering of annotations.\n\nDocumentation. Visualization systems are used by experts, e.g., in the medical domain (Meuschke et al., 2021) to plan and discuss a surgery. Reporting, summarizing, and sharing this visualizationrelated work is an important task that is an additional burden to the surgeon and therefore should be executed by a machine. Nafari and Weaver (2013,2015) generate natural language questions from queries executed on a visualization resulting in a natural language translation of the interaction. This leaves a step-by-step report of the interaction finding usage as a report of done work.\n\nVisualization Creation. Visualization creation considers the production of a visual model from a natural language description -also referred to as text2viz. Rashid et al. (2021) (2016) design an interactive production process for generating timelines from unstructured text input. Language-based 3D scene generation, also referred to as text2scene, which allows users to describe 3D scenes using text without having to learn software tools, is investigated in Coyne and Sproat (2001) ", "filtered_refids": [[], [null], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1684, "num_references": 3}
{"corpusid_sectionid": "250390996-s7", "title": "The Why and The How: A Survey on Natural Language Interaction in Visualization", "date": 2022, "section_title": "NLP Methods", "section": "For each paper in the collection, both the NLP methods used, if any, and if named the specific NLP toolkits used for implementation are elaborated. For the sake of clarity, the methods are roughly divided into two areas: Natural Language Understanding (NLU) and Natural Language Generation (NLG). The majority of the systems apply standard NLP methods like tokenization, stemming or stopword removal to pre-process text inputs, which is why these are not recorded separately. For a detailed inspection, we refer to Appendix C. Figure 3 shows the distribution of applied NLU methods over all papers. Semantic Parsing, which relies on rule-based mapping procedures from recognized input tokens to semantic predicates, is predominantly used. Often, POS-Tagging, Word Embeddings, and Named Entity Recognition (NER) are additionally applied to increase the accuracy of the mapping. For Word Sense Disambiguation WordNet, VerbNet or ConceptNet are leveraged. Speech-to-Text APIs are a common method used in many systems to enable auditory input. A small number of pioneering  Fluid interaction between user and system in real time is a crucial factor for the success of a visualization application. Adopting state-of-the-art deep learning models to real-time interactions in visualization, e.g., by using Knowledge Distillation (Hinton et al., 2015) or Quantization (Jacob et al., 2018) leaves space for future work. Figure 4 shows the distribution of applied NLG methods over all papers. Template-based language generation is used by the majority of the systems followed by a significantly smaller number of deep learning-based Seq2Seq Modeling approaches. Multi-turn systems are predominantly based on rule-based or probabilistic Dialogue Management. Only a few systems use the Text-to-Speech functionality, as most of the generated responses consist of visual elements. In order to advance the adoption of deep learningbased methods in visualization-related text generation, extensive training data sets are required, as pointed out by Kumar et al. (2020a). There is a limited number of data sets for Visualization Description Generation (Obeid and Hoque, 2020), Visual Question Answering (Mathew et al., 2021; and Natural Language Querying (Fu et al., 2020;Srinivasan et al., 2021b;Luo et al., 2021). In particular, the compilation of data sets for emerging dialogue scenarios in Analytical Conversation, Hypothesis Verification or collaborative authoring in Visualization Creation would motivate the use of deep learning based NLP methods in these tasks. Therefore, generating high-quality data sets for the aforementioned visualization tasks leaves room for future work.", "filtered_refids": [["b4", "b9", null, "b2", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2670, "num_references": 5}
{"corpusid_sectionid": "245144787-s1", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Definitions of Robustness in NLP", "section": "Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) \u223c D and its prediction over x as f (x); now given test data (x , y ) \u223c D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )\u223cD [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).\n\nThe above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.", "filtered_refids": [["b40"], ["b18", null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1283, "num_references": 4}
{"corpusid_sectionid": "245144787-s2", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Robustness against Adversarial Attacks", "section": "In one line of research, D is constructed by perturbations around input x to form x (x typically being defined within some proximity of x). This topic has been widely explored in computer vision under the concept of adversarial robustness, which measures models' performances against carefully crafted noises generated deliberately to deceive the model to predict wrongly, pioneered by (Szegedy et al., 2013;Goodfellow et al., 2015), and later extended to NLP, such as (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022) and multilingual adversaries . The generation of adversarial examples primarily builds upon the observation that we can generate samples that are meaningful to humans (e.g., by perturbing the samples with changes that are imperceptible to humans) while altering the prediction of the models for this sample. In this regard, human's remarkable ability in understanding a large set of synonyms (Li et al., 2020) or interesting characteristics in ignoring the exact order of letters  are often opportunities to create adversarial examples. A related line of work such as data-poisoning (Wallace et al., 2021) and weight-poisoning (Kurita et al., 2020) exposes NLP models' vulnerability against attacks during the training process. One can refer to more comprehensive reviews and broader discussions on this topic in Zhang et al. (2020c) and Morris et al. (2020b). Assumptions around Label-preserving and Semantic-preserving Most existing work in vision makes a relatively simplified assumption that the gold label of x remains unchanged under a bounded perturbation over x, i.e., y = y, and a model's robust behaviour should be f (x ) = y (Szegedy et al., 2013;Goodfellow et al., 2015). A similar line of work in NLP follows the same label-preserving assumption with small text perturbations like token and character swapping (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019;Ebrahimi et al., 2018), paraphrasing (Iyyer et al., 2018;Gan and Ng, 2019), semantically equivalent adversarial rules (Ribeiro et al., 2018), and adding distractors (Jia and Liang, 2017). However, this label-preserving assumption might not always hold, e.g., Wang et al. (2021b) studied several existing text perturbation techniques and found that a significant portion of perturbed examples are not label-preserving (despite their label-preserving assumptions), or the resulting labels have a high disagreement among human raters (i.e., can even fool humans). Morris et al. (2020a) also call for more attention to the validity of perturbed examples for a more accurate robustness evaluation.\n\nAnother line of work aims to perturb the input x to x in small but meaningful ways that explicitly change the gold label, i.e., y = y, under which case the robust behaviour of a model should be f (x ) = y and f (x ) = y (Gardner et al., 2020;Kaushik et al., 2019;Schlegel et al., 2021). We believe these two lines of work are complementary to each other, and both should be explored in future research to measure models' robustness more comprehensively.\n\nOne alternative notion is whether the perturbation from x to x is \"semantic-preseving\" (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019) or \"semantic-modifying\" (Shi and Huang, 2020;Jia and Liang, 2017). Note this is slightly different from the above label-preserving assumptions, as it is defined over the perturbations on (x, x ) rather than making an assumption on (y, y ), e.g., semantic-modifying perturbations can be either label-preserving (Jia and Liang, 2017;Huang, 2020) or label-changing (Gardner et al., 2020;Kaushik et al., 2019).", "filtered_refids": [["b52", "b63", "b6", "b50", null, "b80", "b23", "b7", "b24", "b74"], [null, "b1"], [null, "b1"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 3831, "num_references": 14}
{"corpusid_sectionid": "245144787-s3", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Robustness under Distribution Shift", "section": "Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss\u00e0, 2019).", "filtered_refids": [["b67", "b26", "b21", null, "b83", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1535, "num_references": 6}
{"corpusid_sectionid": "245144787-s4", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Connections and A Common Theme", "section": "The above two categories of robustness can be unified under the same framework, i.e., whether D represents a synthetic distribution shift (via adversarial attacks) or a natural distribution shift. Existing work has shown a model's performance might degrade substantially in both cases, but the transferability of the two categories is relatively underexplored. In the vision domain, Taori et al. (2020) investigate models' robustness to natural distribution shift, and show that robustness to synthetic distribution shift might offer little to no robustness improvement under natural distribution shift. Some studies show NLP models might not generalize to unseen adversarial patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), but more work is needed to systematically bridge the gap between NLP models' robustness under natural and synthetic distribution shifts.\n\nTo better understand why models exhibit a lack of robustness, some existing work attributed this to the fact that models sometimes utilize spurious correlations between input features and labels, rather than the genuine ones, where spurious features are commonly defined as features that do not causally affect a task's label (Srivastava et al., 2020;Wang and Culotta, 2020b): they correlate with task labels but fail to transfer to more challenging test conditions or out-of-distribution data (Geirhos et al., 2020). Some other work defined it as \"prediction rules that work for the majority examples but do not hold in general\" (Tu et al., 2020). Such spurious correlations are sometimes referred as dataset bias (Clark et al., 2019;He et al., 2019), annotation artifacts (Gururangan et al., 2018), or group shift (Oren et al., 2019) in the literature. Further, evidence showed that controlling model's learning in spurious features will improve model's performances in distribution shifts (Wang et al., 2019a,b); also, discussions on the connections between adversarial robustness and learning of spurious features have been raised (Ilyas et al., 2019;. Theoretical discussions connecting these fields have also been offered by crediting a reason of model's lack of robustness in either distribution shift or adversarial attack to model's learning of spurious features (Wang et al., 2021c).\n\nFurther, in certain applications, model \"robustness\" can also be connected with models' instability (Milani Fard et al., 2016), or models having poorly-calibrated uncertainty estimation (Guo et al., 2017), where Bayesian methods (Graves, 2011;Blundell et al., 2015), dropout-based (Gal and Ghahramani, 2016;Kingma et al., 2015) and ensemble-based approaches (Lakshminarayanan et al., 2017) have been proposed to improve models' uncertainty estimation. Recently, Ovadia et al. (2019) have shown models' uncertainty estimation can degrade significantly under distributional shift, and call for more work to ensure a model \"knows when it doesn't know\" by giving lower uncertainty estimates over out-of-distribution data. This is another example where models can be less robust under distributional shifts, and again emphasizes the need of building more unified benchmarks to measure a model's performance (e.g., robust accuracy, calibration, stability) under distribution shifts, in addition to in-distribution accuracy.", "filtered_refids": [[null, "b39"], ["b32", "b55", "b30", null, "b65"], [null, "b9", "b20", "b4"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 3293, "num_references": 11}
{"corpusid_sectionid": "245144787-s6", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Continuous vs. Discrete in Search Space", "section": "The most obvious characteristic is probably the discrete nature of the space of text. This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP (Lei et al., 2019;Zhang et al., 2020c), in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space, and multiple novel attack methods are proposed to fill the gap, as we will discuss in later sections.\n\nPerceptible to Human vs. Not On a related topic, one of the most impressive property of ad-versarial attack in vision is that small perturbation of the image data imperceptible to human are sufficient to deceive the model (Szegedy et al., 2013), while this can hardly be true for NLP attacks. Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible). On the other hand, there are ways to generate samples where the changes, although being perceptible, are often ignored by human brain due to some psychological prior on how a human processes the text (Anastasopoulos et al., 2019;. Support vs. Density Difference of the Data Distributions Another difference is more likely seen in the discussion of the domain adaptation of vision and NLP study. In vision study, although the images from training distribution and test distribution can be sufficiently different, the train and test distributions mostly share the same support (the pixels are always sampled from a 0-255 integer space), although the density of these distributions can be very different (e.g., photos vs. sketches). On the other hand, domain adaptation of NLP sometimes studies the regime where the supports of the data differ, e.g., the vocabularies can be significantly different in cross-lingual studies (Abad et al., 2020;Zhang et al., 2020a).\n\nA Common Theme Despite the disparities between vision and NLP, the common theme of pushing the model to generalize from D to D preserves. The practical difference between D and D is more than often defined by the human's understanding of the data, and can differ in vision and NLP as humans perceive and process images and texts in subtly different ways, which creates both opportunities for learning and barriers for direct transfer. Certain lines of research try to bridge the learning in the vision domain to the embedding space in the NLP domain, while other lines of research create more interpretable attacks in the discrete text space (see Table 1 for these two lines of work). How those two lines of research transfer to each other, or complement each other, is not fully explored and calls for additional research.", "filtered_refids": [["b11", "b80"], [null, "b76", "b0"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2755, "num_references": 5}
{"corpusid_sectionid": "245144787-s8", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Human Prior and Error Analyses Driven", "section": "An increasing body of work has been conducted on understanding and measuring robustness in NLP models (Tu et al., 2020;Sagawa et al., 2020b;Geirhos et al., 2020) across various NLP tasks, largely relying on human priors and error analyses.\n\nNatural Language Inference Naik et al. (2018) sampled misclassified examples and analyzed their potential sources of errors, which are then grouped into a typology of common reasons for error. Such error types then served as the bases to construct the stress test set, to further evaluate whether NLI models have the ability to make real inferential decisions, or simply rely on sophisticated pattern matching. Gururangan et al. (2018) found that current NLI models are likely to identify the label by relying only on the hypothesis, and Poliak et al. (2018)     showed another approach by limiting the input space of the characters so that the models will be likely to perceive data typos and misspellings.\n\nSyntactic and Semantic Parsing Robust parsing has been studied in several existing works (Lee et al., 1995;A\u00eft-Mokhtar et al., 2002). More recent work showed that neural semantic parsers are still not robust against lexical and stylistic variations, or meaning-preserving perturbations (Marzinotto et al., 2019;Huang et al., 2021), and proposed ways to improve their robustness through data augmentation (Huang et al., 2021) and adversarial learning (Marzinotto et al., 2019).\n\nText Generation Existing work found that text generation models also suffer from robustness issues, e.g., text summarization models suffer from positional bias (Jung et al., 2019), layout bias (Kryscinski et al., 2019), and a lack of faithfulness and factuality (Kryscinski et al., 2019;Maynez et al., 2020;Chen et al., 2021b); data-to-text models sometimes hallucinate texts that are not supported by the data (Parikh et al., 2020;Wang et al., 2020d). In addition, Sellam et al. (2020);  pointed out the deficiency of existing automatic evaluation metrics and proposed new metrics to better align the generation quality with human judgements.  (2019) show that commonly used crowdsourced datasets for training NLI models might make certain syntactic heuristics more easily adopted by statistical learners. Further, Bras et al. (2020) propose to use a lightweight adversarial filtering approach to filter dataset biases, which is approximated using each instance's predictability score.", "filtered_refids": [[null], [null, "b27"], [null, "b10", "b17"], ["b18", null, "b66", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 2414, "num_references": 10}
{"corpusid_sectionid": "245144787-s10", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Model-based Identification", "section": "In addition to the human-prior and error-analysis driven approaches which are usually specific to each task, other lines of work identify robustness failures that are task-agnostic like white-box text attack methods (Ebrahimi et al., 2018;Alzantot et al., 2018;Jin et al., 2020), and even input-agnostic like universal adversarial triggers (Wallace et al., 2019a) and natural attack triggers (Song et al., 2021). Another line of work proposes to learn an additional model to capture biases, e.g., in visual question answering, Clark et al. (2019) 2020a) propose to learn a biased model that only uses dataset-bias related features. This framework has also been used to capture unknown biases assuming that the lower capacity model learns to capture relatively shallow correlations during training (Clark et al., 2020). In addition, Wang and Culotta (2020a) identify model shortcuts by training classifiers to better distinguish \"spurious\" correlations from \"genuine\" ones based on human annotated examples.", "filtered_refids": [[null, "b47", "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 1006, "num_references": 3}
{"corpusid_sectionid": "245144787-s11", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Model-in-the-loop vs.", "section": "Human-in-the-loop Some work adopts human-in-the-loop to generate challenging examples, e.g., Counterfacutal-NLI (Kaushik et al., 2019) and Natural-Perturbed-QA (Khashabi et al., 2020). Other work applies modelin-the-loop to increase the likelihood that the perturbed examples are challenging for state-of-the-art models, but it might also introduce biases towards the particular model used. For example, SWAG (Zellers et al., 2018) was introduced that fooled most models at the time of publishing but was soon \"solved\" after BERT (Devlin et al., 2019) was introduced. As a result, Yuan et al. (2021) present a study over the transferability of adversarial examples, and Contrast Sets (Gardner et al., 2020) intentionally avoid using model-in-the-loop. Further, more recent work adopts adversarial humanand-model-in-the-loop to create more difficult examples for benchmarking, e.g., Adv-QA (Bartolo et al., 2020), Adv-Quizbowl (Wallace et al., 2019b), ANLI (Nie et al., 2020), and Dynabench (Kiela et al., 2021).", "filtered_refids": [["b29", "b48", "b75", "b1", "b2", "b73", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1011, "num_references": 7}
{"corpusid_sectionid": "245144787-s13", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Data-driven Approaches", "section": "Data augmentation recently gained a lot of interest, in improving performance in low-resourced language settings, few-shot learning, mitigating biases, and improving robustness in NLP models (Feng et al., 2021;Dhole et al., 2021). Techniques like Mixup (Zhang et al., 2018), MixText (Chen et al., 2020), CutOut (DeVries and Taylor, 2017), Aug-Mix (Hendrycks et al., 2020b), HiddenCut (Chen et al., 2021a), have been shown to substantially improve the robustness and the generalization of models. Such mitigation strategies are operated at the data level, and often hard to be interpreted in terms of how and why mitigation works.\n\nOther lines of work deal with spans or regions associated within data points to prevent models from heavily relying on spurious patterns. To make NLP models more robust on sentiment analysis and NLI tasks, Kaushik et al. (2019) proposed curating counterfactually augmented data via a human-inthe-loop process, and showed that models trained on the combination of this augmented data and original data are less sensitive to spurious patterns. Differently, Wang et al. (2021d) performed strategic data augmentation to perturb the set of \"shortcuts\" that are automatically identified, and found that mitigating these leads to more robust models in multiple NLP tasks. This line of mitigation strategies closely relates to how spurious correlations can be measured and identified, as many of the challenging or adversarial examples (Table 1) can sometimes be used to augment the original model to improve its robustness, either in the discrete input space as additional training examples (Liu et al., 2019;Kaushik et al., 2019;Anastasopoulos et al., 2019;Vaibhav et al., 2019;Khashabi et al., 2020), or in the embedding space (Zhu et al., 2020;Zhao et al., 2018b;Miyato et al., 2017;.", "filtered_refids": [["b78", null, "b50"], ["b86", "b44", "b14", null, "b22", "b62", "b84", "b1", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1811, "num_references": 12}
{"corpusid_sectionid": "245144787-s14", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Model and Training-based Approaches", "section": "Pre-training Recent work has demonstrated pretraining as an effective way to improve NLP models' out-of-distribution robustness (Hendrycks et al., 2020a;Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations. Tu et al. (2020) showed a few other factors can also contribute to robust accuracy, including larger model size, more fine-tuning data, and longer fine-tuning. A similar observation is made by Taori et al. (2020) in the vision domain, where the authors found training with larger and more diverse datasets offer better robustness consistently in multiple cases, compared to various robustness interventions proposed in the existing literature. In general, the training strategy with an emphasis on a subset of samples that are particularly hard for the model to learn is sometimes also referred to as group DRO (Sagawa et al., 2020a), as an extension of vanilla distributional robust optimization (DRO) (Ben-Tal et al., 2013;Duchi et al., 2021). Extensions of DRO are mostly discussing the strategies on how to identify the samples considered as minority: Nam et al. (2020) trained two models in parallel, where the \"debiased\" model focuses on examples not learned by the \"biased\" model; Lahoti et al. (2020) used an adversary model to identify samples that are challenging to the main model; Liu et al. (2021) proposed to train the model a second time via up-weighting examples that have high training losses during the first time.\n\nWhen to Use Data-driven or Model-based Approaches? In many cases both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness. One interesting phenomenon observed by (Liu et al., 2019) is to attribute models' robustness failures to blind spots in the training data, or the intrinsic learning ability of the model. The authors found that both patterns are possible: in some cases models can be inoculated via being exposed to a small amount of challenging data, similar to the data augmentation approaches mentioned in Section 5.1; on the other hand, some challenging patterns remain difficult which connects to the larger question around generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), which is relatively under-explored but deserves much attention.", "filtered_refids": [["b13", null, "b28", "b39"], ["b14", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 2559, "num_references": 6}
{"corpusid_sectionid": "245144787-s15", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Inductive-prior-based Approaches", "section": "Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features. This is closely connected to the human-prior-based identification approaches in Section 4.1 as those human-priors can often be used to re-formulate the training objective with additional regularizers. To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component. The construction of this side component usually relies on prior knowledge of what the misaligned features are. Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020). Similarly, Clark et al. (2019Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this \"bias-only\" model such that the main model is discouraged from using biases. More recent work (Xiong et al., 2021) shows the ensemble-based approaches can be further improved via better calibrating the bias-only model. Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).\n\nIn a broader scope, given that one of the main challenges of domain adaptation is to counter the model's tendency in learning domain-specific spurious features (Ganin et al., 2016), some methods contributing to domain adaption may have also progressed along the line of our interest, e.g., domain adversarial neural network (Ganin et al., 2016). This line of work also inspires a family of methods forcing the model to learn auxiliary-annotationinvariant representations with a side component (Ghifary et al., 2016;Wang et al., 2017;Rozantsev et al., 2018;Motiian et al., 2017;Li et al., 2018;Vernikos et al., 2020).\n\nDespite the diverse concrete ideas introduced, the above is mainly training for small empirical loss across different domains or distributions in addition to forcing the model to be invariant to domain-specific spurious features. As an extension along this direction, invariant risk minimization (IRM) (Arjovsky et al., 2019) introduces the idea of invariant predictors across multiple environments, which was later followed and discussed by a variety of extensions (Choe et al., 2020;Ahmed et al., 2020;Rosenfeld et al., 2021). More recently, Dranker et al. (2021) applied IRM in natural language inference and found that a more naturalistic characterization of the problem setup is needed.", "filtered_refids": [["b28", null, "b69", "b16", "b51"], [null, "b25", "b56"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2830, "num_references": 9}
{"corpusid_sectionid": "173188095-s1", "title": "A Survey on Biomedical Image Captioning", "date": "2019-05-26", "section_title": "Datasets", "section": "Datasets for biomedical image captioning comprise medical images and associated texts. Publicly available datasets contain X-rays (IU X-RAY in Table 1), clinical photographs (PEIR GROSS in Table 1), or a mixture of X-rays and photographs (ICLEF-CAPTION in Table 1). The associated texts may be single sentences describing the images, or longer medical reports based on the images (e.g., as in Figure 1b). Current publicly available datasets are rather small (IU X-RAY, PEIR GROSS) or noisy (e.g., IMAGE-CLEF, which is the largest dataset, was created by automatic means that introduced a lot of noise). We do not include in Table 1 datasets like the one of Wang et al. (2017), because their medical reports are not publicly available. 2 Furthermore, we observe that all three publicly available biomedical image captioning datasets suffer from two main shortcomings:\n\n\u2022 There is a great class imbalance, with most images having no reported findings.\n\n\u2022 The wide range of diseases leads to very scarce occurrences of disease-related terms, making it difficult for models to generalize. -Fushman et al. (2015) presented an approach for developing a collection of radiology examinations, including images and narrative reports by radiologists. The authors suggested an accurate anonymization approach for textual radiology reports and provided public access to their dataset through the Open Access Biomedical Image Search Engine (OpenI). 3 The images are 7,470 frontal and lateral chest X-rays, and each radiology report consists of four sections. The 'comparison' section contains previous information about the patient (e.g., preceding medical exams); the 'indication' section contains symptoms (e.g., hypoxia) or reasons of examination (e.g., age); 'findings' lists the radiology observations; and 'impression' outlines the final diagnosis. A system would ideally generate the 'findings' and 'impression' sections, possibly concatenated (Jing et al., 2018). The 'impression' and 'findings' sections of the dataset of Demner-Fushman et al. (2015) were used to manually associate each report with a number of tags (called manual encoding), which were Medical Subject Heading (MESH) 4 and RadLex 5 terms assigned by two trained coders. Additionally, each report was associated with automatically extracted tags, produced by Medical Text Indexer 6 (called MTI encoding). These tags allow systems to learn to initially generate terms describing the image and then use the image along with the generated terms to produce the caption. Hence, this dataset, which is the only one in the field with manually annotated tags, has an added value. From our processing, we found that 104 reports contained no image, 489 were missing 'findings', 6 were missing 'impression', and 25 were missing both 'findings' and 'impression'; the 40 image-caption-tags triplets corresponding to the latter 25 reports were discarded in our later experiments. We shuffled the instances of the dataset (image-text-tags triplets) and used 6,674 of them as the training set (images from the 90% of the reports), with average caption length 38 words and vocabulary size 2,091. Only 2,745 training captions were unique, because 59% of them were the same in more than one image (e.g., similar images with the same condition). Table 1 provides more information about the datasets and their splits.", "filtered_refids": [["b57"], [], ["b21", null, "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3359, "num_references": 4}
{"corpusid_sectionid": "173188095-s5", "title": "A Survey on Biomedical Image Captioning", "date": "2019-05-26", "section_title": "ICLEF-CAPTION", "section": "This dataset was released in 2017 for the Image Concept Detection and Caption Prediction (ICLEF-CAPTION) task (Eickhoff et al., 2017) of IMAGE-CLEF (de Herrera et al., 2018). The dataset consists of 184,614 biomedical images and their captions, extracted from biomedical articles on PubMed Central (PMC). 11 The organizers used an automatic method, based on a biomedical image 7 http://peir.path.uab.edu/library/ 8 PEIR pathology contains 23 sub-categories, but only 22 contain a gross sub-collection (7,443 images in total). We observe that one image was not included by Jing et al. (2018). 9 Our code is publicly available at https://github. com/nlpaueb/bio_image_caption. 10 We used 10% of the dataset for testing, as the 1k images used by Jing et al. for validation and testing were not released. 11 https://www.ncbi.nlm.nih.gov/pmc/ type hierarchy (M\u00fcller et al., 2012), to classify the 5.8M extracted images as clinical or not and also discard compound ones (e.g., images consisting of multiple X-rays), but their estimation was that the overall noise in the dataset would be as high as 10% or 20% (Eickhoff et al., 2017).\n\nIn 2018, the ICLEF-CAPTION organizers employed a Convolutional Neural Network (CNN), to classify the same 5.8M images based on their type and to extract the non-compound clinical ones, leading to 232,305 images along with their respective captions (de Herrera et al., 2018). Although they reported that compound images were reduced, they noted that noise still exists, with nonclinical images present (e.g., images of maps). Additionally, a wide diversity between the types of the images has been reported (Liang et al., 2017). The length of the captions varies from 1 to 816 words (Su et al., 2018;Liang et al., 2017). Only 1.4% of the captions are duplicates (associated with more than one image), probably due to the wide image type diversity. The average caption length is 21 words and the vocabulary size is 157,256. A further 10k instances were used for testing in 2018, but they are not publicly available. Hence, in our experiments we split the 235,305 instances into training and test subsets ( Table 1).\n\nFor tag annotation, the organizers used QUICK-UMLS (Soldaini and Goharian, 2016) to identify concepts of the Unified Medical Language System (UMLS) in the caption text, extracting 111,155 unique concepts from the 222,305 captions. Each image is linked to 30 UMLS concepts, on average, while fewer than 6k have one or two asso-ciated concepts and there are images associated with even thousands of concepts. The organizers observe the existence of noise and note that irrelevant concepts have been extracted, mainly due to the fully automatic extraction process. Varges et al. (2012) employed Natural Language Generation to assist medical professionals turn cardiological findings (e.g., from diagnostic imaging procedures) into fluent and readable textual descriptions. From a different perspective, Schlegl et al. (2015) used both the image and the textual report as input to a CNN, trained to classify images with the help of automatically extracted semantic concepts from the textual report. Kisilev et al. (2015a,b) employed a radiologist to mark an image lesion, and a semi-automatic segmentation approach to define the boundaries of that lesion. Then, they used structured Support Vector Machines (Tsochantaridis et al., 2004) to generate semantic tags, originating from a radiology lexicon, for each lesion. In subsequent work they used a CNN to rank suspicious regions of diagnostic images and, then, generate tags for the top ranked regions, which can be embedded in diagnostic sentence templates (Kisilev et al., 2016). Shin et al. (2016) were the first to apply a CNN-RNN encoder-decoder approach to generate captions from medical images. They used the IU X-RAY dataset and a Network in Network (Lin et al., 2013) or GoogLeNet (Szegedy et al., 2015) as the encoder of the images, obtaining better results with GoogLeNet. The encoder was pretrained to predict (from the images) 17 classes, corresponding to MESH terms that were frequent in the reports and did not co-occur frequently with other MESH terms. An LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) was used as the RNN decoder to generate image descriptions from the image encodings. In a second training phase, the mean of the RNNs state vectors (obtained while describing each image) was used as an improved representation of each training image. The original 17 classes that had been used to pretrain the CNN were replaced by 57 finer classes, by applying k-means clustering to the improved vector representations of the training images. The CNN was then retrained to predict the 57 new classes and this led to improved BLEU (Papineni et al., 2002) scores for the overall CNN-RNN system. The gen-erated descriptions, however, were not evaluated by humans. Furthermore, the generated descriptions were up to 5 words long and looked more like bags of terms (e.g., 'aorta thoracic, tortuous, mild'), rather than fluent coherent reports. Zhang et al. (2017b) were the first to employ an attention mechanism in biomedical image to text generation, with their MDNET. 12 MDNET used RESNET (He et al., 2016) for image encoding, but extending its skip connections to address vanishing gradients. The image representation acts as the starting hidden state of a decoder LSTM, enhanced with an attention mechanism over the image. (During training, this attention mechanism is also employed to detect diagnostic labels.) The decoder is cloned to generate a fixed number of sentences, as many as the symptom descriptions. This model performed slightly better than a state of the art generic image captioning model (Karpathy and Fei-Fei, 2015) in most evaluation measures. Jing et al. (2018) segment each image to equally sized patches and use   (Simonyan and Zisserman, 2014) to separately encode each patch as a 'visual' feature vector. A Multi-Layer Perceptron (MLP) is then fed with the visual feature vectors of each image (representing its patches) and predicts terms from a pre-determined term vocabulary. The word embeddings of the predicted terms of each image are treated as 'semantic' feature vectors representing the image. The decoder, which produces the image description, is a hierarchical RNN, consisting of a sentencelevel LSTM and a word-level LSTM. The sentencelevel LSTM produces a sequence of embeddings, each specifying the information to be expressed by a sentence of the image description (acting as a topic). For each sentence embedding, the wordlevel LSTM then produces the words of the corresponding sentence, word by word. More precisely, at each one of its time-steps, the sentencelevel LSTM of Jing et al. examines both the visual and the semantic feature vectors of the image. Following previous work on image captioning, that added attention to encoder-decoder approaches (Xu et al., 2015;You et al., 2016;Zhang et al., 2017b), an attention mechanism (an MLP fed with the current state of the sentence-level LSTM and each one of the visual feature vectors of the image) assigns attention scores to the visual feature vectors, and the weighted sum of the visual feature vectors (weighted by their attention scores) becomes a visual 'context' vector, specifying which patches of the image to express by the next sentence. Another attention mechanism (another MLP) assigns attention scores to the semantic feature vectors (that represent the terms of the image), and the weighted sum of the semantic feature vectors (weighted by attention) becomes the semantic context vector, specifying which terms of the image to express by the next sentence. At each time-step, the sentence-level LSTM considers the visual and semantic context vectors, produces a sentence embedding and updates its state, until a stop control instructs it to stop. Given the sentence embedding, the word-level LSTM produces the words of the corresponding sentence, again until a special 'stop' token is generated. Jing et al. showed that their model outperforms models created for general image captioning with visual attention (Vinyals et al., 2015;Donahue et al., 2015;Xu et al., 2015;You et al., 2016).  adopted an approach similar to that of Jing et al. (2018), using a RESNET-based CNN to encode the images and an LSTM decoder to produce image descriptions, but their LSTM is flat, as opposed to the hierarchical LSTM Wang et al. 13 ICLEF-CAPTION run successfully for two consecutive years (Eickhoff et al., 2017;de Herrera et al., 2018) and stopped in 2019. Participating systems (see Table 3) used image similarity to retrieve images similar to the one to be described, then aggregating the captions of the retrieved images; or they employed an encoder-decoder architecture; or they simply classified each image based on UMLS concepts and then aggregated the respective UMLS 'semantic groups' 14 to form a caption. Liang et al. (2017) used a pre-trained VG-GNET CNN encoder and an LSTM decoder, similarly to Karpathy and Fei-Fei (2015). They trained three such models on different caption lengths and used an SVM classifier to choose the most suitable decoder for the given image. Furthermore, they used a 1-Nearest Neighbor method to retrieve the caption of the most similar image and aggregated it with the generated caption. Zhang et al. (2018), who achieved the best results in 2018, used the Lucene Image Retrieval software (LIRE) to retrieve images from the training set and then simply concatenated the captions of the top three retrieved images to obtain the new caption. Abacha et al. (2017) used GoogLeNet to detect UMLS concepts and returned the aggregation of their respective UMLS semantic groups as a caption. Su et al. (2018) and Rahman (2018) also employed different encoder-decoder architectures. Gale et al. (2018) argued that existing biomedical image captioning systems fail to produce a satisfactory medical diagnostic report from an image, and to provide evidence for a medical decision. They focused on classifying hip fractures in pelvic X-rays, and argued that the diagnostic report of such narrow medical tasks could be simplified to two sentence templates; one for positive cases, including 5 placeholders to be filled by descriptive terms, and a fixed negative one. They used DENSENET (Huang et al., 2017) to get image embeddings and a two-layer LSTM, with attention over the image, to generate the constrained textual report. Their results, shown in Table 2, are very high, but this is expected due to the extremely simplified and standardized ground truth reports. (Gale et al. report an improvement of more than 50 BLEU points when employing this assumption.) The reader is also warned that the results of Table 2 are not directly comparable, since they are obtained from very different datasets.  Table 2: Evaluation of biomedical image captioning methods with BLEU-1/-2/-3/-4 (B1, B2, B3, B4), METEOR (MET), ROUGE-L (ROU), and CIDER (CID) percentage scores. Zhang et al. (2017a) and Han et al. (2018) also performed biomedical captioning, but did not provide any evaluation results. Datasets with \u2020 are not publicly available; BDIDR consists of 1,000 pathological bladder cancer images, each with 5 reports; FRONTAL PELVIC X-RAYS comprises 50,363 images, each supplemented with a radiology report, but simplified to a standard template; CHEST X-RAY 14 is publicly available, but without its medical reports.  Table 3: Top-5 participating systems at the ICLEF-CAPTION competition, ranked based on average BLEU (%), the official evaluation measure. Systems used an encoder-decoder (ED), image retrieval (IR), or classified UMLS concepts (CLS). We exclude 2017 systems employing external resources, which may have seen test data during training (Eickhoff et al., 2017). 2018 models were limited to use only pre-trained CNNs.", "filtered_refids": [["b21", null, "b13", "b37"], ["b49", "b29", "b18"], ["b52", "b63", "b10", "b40", "b13", "b50", "b20", "b59", "b19", "b17", "b46", "b18", "b49", "b21", "b7", "b16", "b55", "b45", "b22", "b24", "b53", "b61", "b15", "b47", "b31", "b29", "b48", null, "b62", "b64", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 68, "num_chars": 11881, "num_references": 38}
{"corpusid_sectionid": "173188095-s7", "title": "A Survey on Biomedical Image Captioning", "date": "2019-05-26", "section_title": "Evaluation", "section": "The most common evaluation measures in biomedical image captioning are BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005), which originate from machine translation and summarization. The more recent CIDER measure , which was designed for general image captioning (Kilickaya et al., 2016), has been used in only two biomedical image captioning works (Zhang et al., 2017b;Jing et al., 2018). SPICE (Anderson et al., 2016), which was also designed for general image captioning (Kilickaya et al., 2016), has not been used in any biomedical image captioning work we are aware of. Below, we describe each measure separately and discuss its advantages and limitations with respect to biomedical image captioning. BLEU is the most common measure (Papineni et al., 2002). It measures word n-gram overlap between the generated and the ground truth caption.\n\nA brevity penalty is added to penalize short generated captions. BLEU-1 considers unigrams (i.e., words), while BLEU-2, -3, -4 consider bigrams, trigrams, and 4-grams respectively. The average of the four variants was used as the official measure in ICLEF-CAPTION.\n\nMETEOR (Banerjee and Lavie, 2005) extended BLEU-1 by employing the harmonic mean of precision and recall (F-score), biased towards recall, and by also employing stemming (Porter stemmer) and synonymy (WordNet). To take into account longer subsequences, it includes a penalty of up to 50% when no common n-grams exist between the machine-generated description and the reference. ROUGE-L (Lin et al., 2013) is the ratio of the length of the longest common subsequence between the machine-generated description and the reference human description, to the size of the reference (ROUGE-L recall); or to the generated description (ROUGE-L precision); or a combination of the two (ROUGE-L F-measure). We note that several ROUGE variants exist, based on different ngram lengths, stemming, stopword removal, etc., but ROUGE-L is the most commonly used variant in biomedical image captioning so far. CIDER  measures the cosine similarity between n-gram TF-IDF representations of the two captions (words are also stemmed). This is calculated for unigrams to 4grams and their average is returned as the final evaluation score. The intuition behind using TF-IDF is to reward frequent caption terms while penalizing common ones (e.g., stopwords). However, biomedical image captioning datasets contain many scientific terms (e.g., disease names) that are common across captions (or more gener-ally document collections), which may be mistakenly penalized. We also noticed that the scores returned by the provided CIDER implementation may exceed 100%. 15 We exclude CIDER results, since these issues need to be investigated further. SPICE (Anderson et al., 2016) extracts tuples from the two captions (machine-generated, reference), containing objects, attributes and/or relations; e.g., (patient), (has, pain), (male, patient). Precision and recall are computed using WordNet synonym matching between the two sets of tuples, and the F1 score is returned. The creators of SPICE report improved results over both METEOR and CIDER, but it has been noted that results depend on parsing quality (Kilickaya et al., 2016). When experimenting with the provided implementation 16 of this measure, we noticed that it failed to parse long texts to evaluate them. Similarly to CIDER, we exclude SPICE from further analysis below.\n\nWord Mover's Distance (WMD) (Kusner et al., 2015) computes the minimum cumulative cost required to move all word embeddings of one caption to aligned word embeddings of the other caption. 17 It completely ignores, however, word order, and thus readability, which is one of the main assessment dimensions in the biomedical field (Tsatsaronis et al., 2015). Other previously discussed n-gram based measures also largely ignore word order, but at least consider local order (inside n-grams). WMD scores are included in Table 4 as similarity values WMS = (1 + WMD) \u22121 .", "filtered_refids": [["b40", "b30", "b21", "b23", "b64", "b1", "b3"], [], ["b31", null, "b23", "b1", "b3"], ["b51", "b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4018, "num_references": 14}
{"corpusid_sectionid": "208541377-s2", "title": "The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations Conference or Workshop Item The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations", "date": 2019, "section_title": "Rating and Likert scales", "section": "In this section we use illustrative examples to underline the differences between rating and Likert scales. We use the term scale with the following two meanings:\n\n\u2022 Given a statement, the term scale is the group of points making up the options offered to respondents. We refer to the combination of the statement and the scale as an item.\n\n\u2022 In the case of an aggregate scale 2 , such as the Likert scale, we use the term scale to indicate a collection of items.\n\nRating scales: Rating scales are items used in surveys to estimate feeling, opinions or attitudes of responders. The data collected through a rating scale can be interpreted both as ordinal and interval. A rating scale is composed of an n-point scale. Scales with 3, 5, 7, 10 or 11 points are used most often. Rating scales can be both numerical and verbal.\n\nIn a numerical rating scale, a number is associated with each point. A variation of a numerical scale uses label words at the extreme values and leaves the intermediate values with a numerical label, as for example shown in Figure 1. A rat- ing scale that uses words as labels for the points is named a graphic rating scale 3 . An example of this kind of rating scale is pictured in Figure 2. Some- times the points of a graphic rating scale can also be labelled with numbers. Another sort of rating scale is the comparative rating scale. This kind of scale is used to ask respondents to answer a question in terms of a comparison. An example of a comparative rating scale is given in Figure 3.\n\nLikert scale: A Likert scale is an aggregate scale. The items that make a Likert scale are In other words, it is a composite of items which are summed or averaged all together to get an overall positive or negative orientation towards the object under examination in the survey.\n\n3 Sometimes a graphic rating scale is called Likert item or Likert-style scale. However, Likert items and Likert-style scale are particular cases of graphic rating scales. graphic rating scales. In this context, each graphic rating scale is called a Likert item. Likert scales are usually expressed in terms of agreement and disagreement. An example of a Likert scale is shown in Figure 4. The items that make a Lik- ert scales are designed to collectively capture the phenomenon under analysis. Accordingly, they shouldn't be considered in isolation and they should be summed or averaged to produce a total score. However, individual items by themselves are often considered as a single scale. Because of this ambivalent use of the Likert scale and its items, the nature of the Likert scale is highly controversial. Researchers are split between who consider it an interval scale and those who consider it an ordinal scale; see for example Jamieson (2004), Pell (2005), Norman (2010).\n\nThe confusion generated by the ambivalent use of the Likert scale and its items is well illustrated and explained in Joshi et al. (2015), where an image similar to Figure 5 is introduced. Likert scales are built in such a way that respondents express their level of agreement or disagreement with the sentences expressed by the Likert items. Because all the items are presented all together and with the same point labels, it is assumed that each respondent gives the same interpretation to the answer points -that is, as suggested by Likert, the distances between the points in the scale can be considered equal. 4 This assumption licenses use of the scale as an interval scale. Consequently, adding or averaging the items annotated by the same respondent is justified. This raises the interval interpretation, depicted by the vertical arrow of Figure 5.\n\nOtherwise, an item-by-item analysis -that is a separate analysis of a single item extracted from an aggregate scale -cannot justify the assumption that the difference between adjacent points is equal. Indeed, we cannot assume that different respondents perceive the difference between adjacent label points as being of equal distance. The difference between \"agree\" and \"strongly agree\" can be perceived differently from one respondent to another. Consequently, the addition or the average of the items extracted from an aggregate scale is not justified. In such cases, the median or mode can be used as the measure of central tendency. This follows the ordinal interpretation, depicted by the horizontal arrow of Figure 5.\n\nUnfortunately, in many cases there is not a clear understanding of the difference between the horizontal and the vertical direction of the aggregate scale. It is common to see item-by-item analysis (that is the horizontal direction) that makes use of parametric statistics without a justification of this choice. Indeed, as shown in Section 4, the interpretation of Likert items as interval scales has become a common practice. This particularly applies to the use of the mean for measuring the central tendency for the analysis of Likert items. 4 Some authors, for example Jamieson (2004), do not accept such an assumption and do not consider the points as equally distant. In this case the Likert scales themselves, and not only the Likert items, are considered ordinal.\n\nIn this section we use illustrative examples to underline the differences between rating and Likert scales. We use the term scale with the following two meanings:\n\n\u2022 Given a statement, the term scale is the group of points making up the options offered to respondents. We refer to the combination of the statement and the scale as an item.\n\n\u2022 In the case of an aggregate scale 2 , such as the Likert scale, we use the term scale to indicate a collection of items.\n\nRating scales: Rating scales are items used in surveys to estimate feeling, opinions or attitudes of responders. The data collected through a rating scale can be interpreted both as ordinal and interval. A rating scale is composed of an n-point scale. Scales with 3, 5, 7, 10 or 11 points are used most often. Rating scales can be both numerical and verbal.\n\nIn a numerical rating scale, a number is associated with each point. A variation of a numerical scale uses label words at the extreme values and leaves the intermediate values with a numerical label, as for example shown in Figure 1. A rat- ing scale that uses words as labels for the points is named a graphic rating scale 3 . An example of this kind of rating scale is pictured in Figure 2. Some- times the points of a graphic rating scale can also be labelled with numbers. Another sort of rating scale is the comparative rating scale. This kind of scale is used to ask respondents to answer a question in terms of a comparison. An example of a comparative rating scale is given in Figure 3.\n\nLikert scale: A Likert scale is an aggregate scale. The items that make a Likert scale are In other words, it is a composite of items which are summed or averaged all together to get an overall positive or negative orientation towards the object under examination in the survey.\n\n3 Sometimes a graphic rating scale is called Likert item or Likert-style scale. However, Likert items and Likert-style scale are particular cases of graphic rating scales. graphic rating scales. In this context, each graphic rating scale is called a Likert item. Likert scales are usually expressed in terms of agreement and disagreement. An example of a Likert scale is shown in Figure 4. The items that make a Lik- ert scales are designed to collectively capture the phenomenon under analysis. Accordingly, they shouldn't be considered in isolation and they should be summed or averaged to produce a total score. However, individual items by themselves are often considered as a single scale. Because of this ambivalent use of the Likert scale and its items, the nature of the Likert scale is highly controversial. Researchers are split between who consider it an interval scale and those who consider it an ordinal scale; see for example Jamieson (2004), Pell (2005), Norman (2010).\n\nThe confusion generated by the ambivalent use of the Likert scale and its items is well illustrated and explained in Joshi et al. (2015), where an image similar to Figure 5 is introduced. Likert scales are built in such a way that respondents express their level of agreement or disagreement with the sentences expressed by the Likert items. Because all the items are presented all together and with the same point labels, it is assumed that each respondent gives the same interpretation to the answer points -that is, as suggested by Likert, the distances between the points in the scale can be considered equal. 4 This assumption licenses use of the scale as an interval scale. Consequently, adding or averaging the items annotated by the same respondent is justified. This raises the interval interpretation, depicted by the vertical arrow of Figure 5.\n\nOtherwise, an item-by-item analysis -that is a separate analysis of a single item extracted from an aggregate scale -cannot justify the assumption that the difference between adjacent points is equal. Indeed, we cannot assume that different respondents perceive the difference between adjacent label points as being of equal distance. The difference between \"agree\" and \"strongly agree\" can be perceived differently from one respondent to another. Consequently, the addition or the average of the items extracted from an aggregate scale is not justified. In such cases, the median or mode can be used as the measure of central tendency. This follows the ordinal interpretation, depicted by the horizontal arrow of Figure 5.\n\nUnfortunately, in many cases there is not a clear understanding of the difference between the horizontal and the vertical direction of the aggregate scale. It is common to see item-by-item analysis (that is the horizontal direction) that makes use of parametric statistics without a justification of this choice. Indeed, as shown in Section 4, the interpretation of Likert items as interval scales has become a common practice. This particularly applies to the use of the mean for measuring the central tendency for the analysis of Likert items. 4 Some authors, for example Jamieson (2004), do not accept such an assumption and do not consider the points as equally distant. In this case the Likert scales themselves, and not only the Likert items, are considered ordinal.", "filtered_refids": [[], [], [], [], [], [], ["b14", "b5"], ["b7"], [], ["b5"], [], [], [], [], [], [], ["b14", "b5"], ["b7"], [], ["b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 92, "num_chars": 10284, "num_references": 8}
{"corpusid_sectionid": "208541377-s15", "title": "The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations Conference or Workshop Item The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations", "date": 2019, "section_title": "Rating and Likert scales", "section": "In this section we use illustrative examples to underline the differences between rating and Likert scales. We use the term scale with the following two meanings:\n\n\u2022 Given a statement, the term scale is the group of points making up the options offered to respondents. We refer to the combination of the statement and the scale as an item.\n\n\u2022 In the case of an aggregate scale 2 , such as the Likert scale, we use the term scale to indicate a collection of items.\n\nRating scales: Rating scales are items used in surveys to estimate feeling, opinions or attitudes of responders. The data collected through a rating scale can be interpreted both as ordinal and interval. A rating scale is composed of an n-point scale. Scales with 3, 5, 7, 10 or 11 points are used most often. Rating scales can be both numerical and verbal.\n\nIn a numerical rating scale, a number is associated with each point. A variation of a numerical scale uses label words at the extreme values and leaves the intermediate values with a numerical label, as for example shown in Figure 1. A rat- ing scale that uses words as labels for the points is named a graphic rating scale 3 . An example of this kind of rating scale is pictured in Figure 2. Some- times the points of a graphic rating scale can also be labelled with numbers. Another sort of rating scale is the comparative rating scale. This kind of scale is used to ask respondents to answer a question in terms of a comparison. An example of a comparative rating scale is given in Figure 3.\n\nLikert scale: A Likert scale is an aggregate scale. The items that make a Likert scale are In other words, it is a composite of items which are summed or averaged all together to get an overall positive or negative orientation towards the object under examination in the survey.\n\n3 Sometimes a graphic rating scale is called Likert item or Likert-style scale. However, Likert items and Likert-style scale are particular cases of graphic rating scales. graphic rating scales. In this context, each graphic rating scale is called a Likert item. Likert scales are usually expressed in terms of agreement and disagreement. An example of a Likert scale is shown in Figure 4. The items that make a Lik- ert scales are designed to collectively capture the phenomenon under analysis. Accordingly, they shouldn't be considered in isolation and they should be summed or averaged to produce a total score. However, individual items by themselves are often considered as a single scale. Because of this ambivalent use of the Likert scale and its items, the nature of the Likert scale is highly controversial. Researchers are split between who consider it an interval scale and those who consider it an ordinal scale; see for example Jamieson (2004), Pell (2005), Norman (2010).\n\nThe confusion generated by the ambivalent use of the Likert scale and its items is well illustrated and explained in Joshi et al. (2015), where an image similar to Figure 5 is introduced. Likert scales are built in such a way that respondents express their level of agreement or disagreement with the sentences expressed by the Likert items. Because all the items are presented all together and with the same point labels, it is assumed that each respondent gives the same interpretation to the answer points -that is, as suggested by Likert, the distances between the points in the scale can be considered equal. 4 This assumption licenses use of the scale as an interval scale. Consequently, adding or averaging the items annotated by the same respondent is justified. This raises the interval interpretation, depicted by the vertical arrow of Figure 5.\n\nOtherwise, an item-by-item analysis -that is a separate analysis of a single item extracted from an aggregate scale -cannot justify the assumption that the difference between adjacent points is equal. Indeed, we cannot assume that different respondents perceive the difference between adjacent label points as being of equal distance. The difference between \"agree\" and \"strongly agree\" can be perceived differently from one respondent to another. Consequently, the addition or the average of the items extracted from an aggregate scale is not justified. In such cases, the median or mode can be used as the measure of central tendency. This follows the ordinal interpretation, depicted by the horizontal arrow of Figure 5.\n\nUnfortunately, in many cases there is not a clear understanding of the difference between the horizontal and the vertical direction of the aggregate scale. It is common to see item-by-item analysis (that is the horizontal direction) that makes use of parametric statistics without a justification of this choice. Indeed, as shown in Section 4, the interpretation of Likert items as interval scales has become a common practice. This particularly applies to the use of the mean for measuring the central tendency for the analysis of Likert items. 4 Some authors, for example Jamieson (2004), do not accept such an assumption and do not consider the points as equally distant. In this case the Likert scales themselves, and not only the Likert items, are considered ordinal.\n\nIn this section we use illustrative examples to underline the differences between rating and Likert scales. We use the term scale with the following two meanings:\n\n\u2022 Given a statement, the term scale is the group of points making up the options offered to respondents. We refer to the combination of the statement and the scale as an item.\n\n\u2022 In the case of an aggregate scale 2 , such as the Likert scale, we use the term scale to indicate a collection of items.\n\nRating scales: Rating scales are items used in surveys to estimate feeling, opinions or attitudes of responders. The data collected through a rating scale can be interpreted both as ordinal and interval. A rating scale is composed of an n-point scale. Scales with 3, 5, 7, 10 or 11 points are used most often. Rating scales can be both numerical and verbal.\n\nIn a numerical rating scale, a number is associated with each point. A variation of a numerical scale uses label words at the extreme values and leaves the intermediate values with a numerical label, as for example shown in Figure 1. A rat- ing scale that uses words as labels for the points is named a graphic rating scale 3 . An example of this kind of rating scale is pictured in Figure 2. Some- times the points of a graphic rating scale can also be labelled with numbers. Another sort of rating scale is the comparative rating scale. This kind of scale is used to ask respondents to answer a question in terms of a comparison. An example of a comparative rating scale is given in Figure 3.\n\nLikert scale: A Likert scale is an aggregate scale. The items that make a Likert scale are In other words, it is a composite of items which are summed or averaged all together to get an overall positive or negative orientation towards the object under examination in the survey.\n\n3 Sometimes a graphic rating scale is called Likert item or Likert-style scale. However, Likert items and Likert-style scale are particular cases of graphic rating scales. graphic rating scales. In this context, each graphic rating scale is called a Likert item. Likert scales are usually expressed in terms of agreement and disagreement. An example of a Likert scale is shown in Figure 4. The items that make a Lik- ert scales are designed to collectively capture the phenomenon under analysis. Accordingly, they shouldn't be considered in isolation and they should be summed or averaged to produce a total score. However, individual items by themselves are often considered as a single scale. Because of this ambivalent use of the Likert scale and its items, the nature of the Likert scale is highly controversial. Researchers are split between who consider it an interval scale and those who consider it an ordinal scale; see for example Jamieson (2004), Pell (2005), Norman (2010).\n\nThe confusion generated by the ambivalent use of the Likert scale and its items is well illustrated and explained in Joshi et al. (2015), where an image similar to Figure 5 is introduced. Likert scales are built in such a way that respondents express their level of agreement or disagreement with the sentences expressed by the Likert items. Because all the items are presented all together and with the same point labels, it is assumed that each respondent gives the same interpretation to the answer points -that is, as suggested by Likert, the distances between the points in the scale can be considered equal. 4 This assumption licenses use of the scale as an interval scale. Consequently, adding or averaging the items annotated by the same respondent is justified. This raises the interval interpretation, depicted by the vertical arrow of Figure 5.\n\nOtherwise, an item-by-item analysis -that is a separate analysis of a single item extracted from an aggregate scale -cannot justify the assumption that the difference between adjacent points is equal. Indeed, we cannot assume that different respondents perceive the difference between adjacent label points as being of equal distance. The difference between \"agree\" and \"strongly agree\" can be perceived differently from one respondent to another. Consequently, the addition or the average of the items extracted from an aggregate scale is not justified. In such cases, the median or mode can be used as the measure of central tendency. This follows the ordinal interpretation, depicted by the horizontal arrow of Figure 5.\n\nUnfortunately, in many cases there is not a clear understanding of the difference between the horizontal and the vertical direction of the aggregate scale. It is common to see item-by-item analysis (that is the horizontal direction) that makes use of parametric statistics without a justification of this choice. Indeed, as shown in Section 4, the interpretation of Likert items as interval scales has become a common practice. This particularly applies to the use of the mean for measuring the central tendency for the analysis of Likert items. 4 Some authors, for example Jamieson (2004), do not accept such an assumption and do not consider the points as equally distant. In this case the Likert scales themselves, and not only the Likert items, are considered ordinal.", "filtered_refids": [[], [], [], [], [], [], ["b14", "b5"], ["b7"], [], ["b5"], [], [], [], [], [], [], ["b14", "b5"], ["b7"], [], ["b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 92, "num_chars": 10284, "num_references": 8}
{"corpusid_sectionid": "219946512-s2", "title": "Automatic summarization of medical conversations, a review", "date": 2019, "section_title": "Summarization criteria", "section": "In the following lines we will describe two important criteria to summarize text. The first criteria is based on the frequency of the words. The second criteria are focused on the text features to detect the importance of the sentences.\n\nFrequency criteria Over the years, several criteria have been developed to generate extractive summaries. One of the most cited in the literature is TF-IDF. TF (Term-Frequency) was proposed by Luhn (1958) and is the frequency of a word in the document. IDF (Inverse Document Frequency) was proposed by Sparck Jones (1972) and attenuates the weight of words that appear in a lot of the documents of the collection and increases the weight of words that occur in a few of them. The first works in summarization were based on T F \u2212 IDF . For instance, Wang & Cardie (2011) used unsupervised methods like TF-IDF, LDA, topic modeling and supervised clustering to produce a concise abstract of the decisions taken in spoken meetings.\n\nSurface features criteria An alternative to detect the relevance of a sentence is through features of different kind. Yogan et al. (2016)  Machine Learning Approaches In recent years, researchers have proposed methods based-on machine learning to summarize text. Naive Bayes was used by Kupiec et al. (1995) to chose if a sentence belongs or not to a summary. Recently Ramanujam & Kaliappan (2016) extended the application of the Naive Bayes algorithm to automatic summarization in multi-documents. Aside from Nayve Bayes, clustering algorithms have been used by Aliguliyev (2009) and KM & Soumya (2015) to get extractive summaries. Aliguliyev (2009) proposed a method based on sentence clustering, while KM & Soumya (2015) prepared the cluster center using a n-dimensional vector space and the documents similarity is measure by cosine similarity to generate the documents clusters.\n\nBesides, researchers have used Support Vector Machine (SVM). For example, (Schilder & Kondadadi, 2008) work on query-based multi-document summarization and they use SVM to rank all sentences in the topic cluster for summarization. Another algorithm that researchers have been used is genetic algorithms. For instance, Chatterjee et al. (2012) worked on extractive summaries representing the document as a Direct Acyclic grapth (DAC). They used genetic algorithm to maximize the fitness function and get the summary. After few years, Bossard & Rodrigues (2017) used objective function from generic algorithms to explore summaries space and compute the probability distribution of tokens.\n\nOne of the most widely used family of machine learning methods for automatic summarization is Neural Networks (NN). In NLP, the currently most relevant recurrent neural networks are : LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit). LSTM has the ability to add or remove information through structures called gates : forget gate, input gate, candidate gate and output gate layer. Meanwhile, GRU is a variant of LSTM and it combines the forget and input gates into a single update gate. LSTM and GRU present some advantages, such as the ability to store long-term dependencies, they avoid the problem of vanishing gradient and they consider the order of the words.\n\nIn abstractive text summarization, taking into account the order of words is one of the greatest advances because the summaries present more coherence.\n\nKaikhah (2004)  Automatic summarization methods have been applied on various kinds of documents, such as text (news, articles, etc), dialogues, in medical and other domains. In the literature, we can find automatic summarization of dialogues in meetings, recorded conversations in call centers and other events that happen everyday. The majority of works on medical summarization have focused on research articles. However in our case we are interested to get automatic summaries from medical conversations. In sections 3 and 4, we describe independently NLP works on dialogue analysis and the medical domain, with a focus on summarization. ", "filtered_refids": [[], ["b55", "b35"], ["b64", "b25", "b27"], ["b51"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 4012, "num_references": 6}
{"corpusid_sectionid": "219946512-s5", "title": "Automatic summarization of medical conversations, a review", "date": 2019, "section_title": "Medical Conversational Systems", "section": "There are many conversational applications that have been developed to help the health field, such as intelligence agents and health dialog systems. Allen et al. (2006) developed Chester, a prototype of medical advisor. Chester provides information, advises based on prescribed medications and reports back to medical support team, it can answer questions asked by users. In the meantime, de Rosis et al.\n\n(2006) developed a conversational agent to talk to patients to influence them to change their dietary behavior. Migneault et al. (2006) describe the approach of how to write dialogues for TLC (Telephone-Linked Care) telephone systems, whose objective is to offer health-related services. At the same time, (Beveridge & Fox, 2006) studied the automatic generation of dialogue combining knowledge of the structure of tasks and ontological knowledge, the objective is to decide if a patient must be referred or not with a cancer specialist.", "filtered_refids": [["b1"], ["b4", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 943, "num_references": 3}
{"corpusid_sectionid": "51623319-s1", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "The Phenomenon of Non-NA Anaphora", "section": "Example (1) demonstrated a simple case of anaphora, in which the antecedent Maya is a simple noun phrase. When the antecedent is a non-nominal constituent, as in Example (2), we refer to the relation between an anaphor and its non-NA as non-NA anaphora. Semantically, such non-NAs typically denote complex entities, such as propositions, facts, events, or situations. These entities are complex because they can contain a number of other entities and events, as well as the relationships between them.\n\nThere are different terminologies used for the antecedent (i.e., the surface linguistic constituent) and its interpretation. For instance, Byron (2004) uses Luperfoy's (1991) terminology, referring to the antecedent as the sponsor, and Byron (2003) uses the term linguistic anchor to indicate that these are not ordinary nominal antecedents. The actual meaning of an antecedent is referred to as its referent or interpretation. In this article, we will use the term antecedent to refer to the expression that most closely represents the interpretation of the anaphor, so far as it is overtly realized, and referent to refer to the interpretation itself.\n\nA few simple examples of non-NA anaphora are shown in Example (4). Here, the anaphors are: this, this fact, and it; the antecedents are: the sentence Women are a rarity in mathematics and engineering in Example (4a), the sentence They will not win many races in Example (4b), and the verb phrase made her butternut squash recipe in Example (4c); and the referents are: the proposition that women are a rarity in mathematics and engineering in (4a), the fact that those who run with large lateral motion will not win many races in (4b), and the action of making the butternut squash recipe in (4c).\n\n(4) a. Women are a rarity in mathematics and engineering. As a female engineering student, I see this every day. (NYT 4 ) b. Those who run with large lateral motion are not running well; they may be good runners who are very tired, or simply poor runners. They will not win many races, but it is far too simplistic to attribute this fact to the \"extra distance\" that must be covered. (NYT) c. Anna finally made her butternut squash recipe this morning. It took her twenty minutes.\n\nThese examples are relatively simple examples, where the antecedents precede the anaphor, are given explicitly in the text, and are in the close vicinity of the anaphor. None of these circumstances are necessarily the case. In Example (5a), the antecedent follows the anaphor this, creating an instance of cataphora. In Example (5b), there is no clear syntactic constituent representing the antecedent of the anaphor this reason-instead, the antecedent is distributed throughout the preceding text. And in Example (5c), the syntactic constituent representing the antecedent is three sentences away from the anaphor and the actual issue here, that is, the referent is whether to allow some form of audiovisual coverage of court proceedings or not, which does not occur explicitly in the text.\n\n(5) a. This, I now realize, was a very bad idea-suggesting we do whatever Terry Crews wants for the day. 5 b. Because all of us carry some baggage from our past, I seldom arrive in Paris,\n\nwhere work takes me four or five times a year, without some feeling of being an ugly duckling or, at any rate, a small-town person. No doubt it is for this reason-I can think of no other-that I stay in the same hotel, in the same room, and consider the area around the Place Vend\u00f4me my neighborhood. (NYT) c. New York is one of only three states that do not allow some form of audio-visual coverage of court proceedings. Some lawmakers worry that cameras might compromise the rights of the litigants. But a 10-year experiment with courtroom cameras showed that televised access enhanced public understanding of the judicial system without harming the legal process. New York's backwardness on this issue hurts public confidence in the judiciary . . . (NYT) Linguistic accounts of anaphora usually assume that in processing text or utterances, speakers and hearers build a discourse model (Kamp 1979;Webber 1979), a mental model of the current discourse state, which is dynamically updated as new utterances are processed. A discourse model contains representations of entities that have been referred to in the discourse up to now, attributes of these entities, and the relationships between them. The entities are called discourse entities or discourse referents (Karttunen 1976). To determine the referent of a nominal anaphoric expression, a suitable antecedent with the appropriate features, for example, matching gender and number, has to be found, whose discourse referent then serves as the referent of the anaphor. With non-NA anaphoric expressions, on the other hand, there is not necessarily a discourse referent available in the discourse model that the anaphor could refer to. Interpreting non-NA anaphora is therefore often said to involve additional steps of interpretation (Webber 1991) (cf. Section 3.2.1). 6", "filtered_refids": [[], ["b14", "b13", "b71"], [], [null], [], [], ["b52", "b124", "b122", null, "b53"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 5045, "num_references": 9}
{"corpusid_sectionid": "51623319-s2", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Other Terminologies Describing Similar Phenomena", "section": "The phenomenon we discuss in this article has been a subject of interest for linguists, philosophers, and computational linguists for decades. Consequently, it has been addressed in various contexts from a variety of perspectives, as discussed in the following sections.\n\n2.1.1 Abstract Anaphora. Asher (1993), Navarretta (2007), and Dipper et al. (2011) use the terms abstract anaphora or abstract object anaphora, as in this phenomenon the anaphor refers to an abstract object, such as a fact, an event, a proposition, or a situation, in Ashers's typology of saturated abstract objects (Asher 1993, page 57). contrast to a concrete object, such as a person or a location. Asher formalized the notion of an abstract object by extending Vendler's (1967) approach of using linguistic tests to differentiate various types of abstract objects. The resulting typology ( Figure 1) makes a broad distinction between eventualities (i.e., events and states, which have spatial, temporal, and causal properties and can be observed by the senses) and purely abstract objects (i.e., facts and propositions, which do not have a spatiotemporal location and are not perceivable by the senses but are only mentally conceivable; e.g., Asher 1993, page 57). According to Asher (1993, page 86), eventualities are similar to concrete objects in that they can be directly introduced into the discourse model by some syntactic construction. Whereas concrete objects are introduced by noun phrases (or, more precisely, by their determiners), eventualities are introduced by finite clauses (or, more precisely, by their inflectional marking). In contrast, facts or propositions are introduced by the semantic constraints imposed by specific nouns, such as fact, or verbs, such as believe, which require their arguments (e.g., a that clause) to be of a certain type (Asher 1993, pages 116 and 175). Asher collectively calls the events, states, processes, propositions, facts, and similar entities that populate these two categories saturated abstract objects. They are \"saturated\" in the Fregean sense that they are themselves either true or false, whereas properties or concepts, although abstract, are only true or false as applied to their arguments (Asher 1993, page 15). It is primarily this category of objects-saturated abstract objects-to which non-NA anaphors refer.\n\n2.1.2 Discourse Deixis. Another popular term is discourse deixis (e.g., Webber 1988Webber , 1991Eckert and Strube 2000;Byron 2004;Recasens 2008). 7 Webber (1988Webber ( , 1991, attributing the term to Lakoff (1974), 8 calls non-NA anaphors discourse-deictic because the anaphor deictically points to some part of the discourse model from which it gets its reference. 7 The term deixis refers to the linguistic phenomenon in which an expression's reference is determined in relation to its extra-linguistic context, e.g., the time (now), place (here), or participants (I, you) of the utterance. Such expressions are called deictic (Huddleston andPullum 2002, page 1451). 8 Lakoff (1974) uses the term in a broader sense, including both NA and non-NA anaphora. Webber (1991) states that it makes sense to call the phenomenon discourse deixis because such relations are usually signaled by deictic expressions, that is, demonstratives this and that, compared with it. Cornish (2007) contrasts deixis with anaphora, describing them as the poles of a scale: Whereas anaphora involves the retrieval of an existing discourse entity from the current model, deixis shifts the focus to a new discourse entity or a new aspect of an existing entity.\n\nThe term discourse deixis has also been used in the literature with a different meaning: According to Levinson (1983), discourse deixis occurs when reference is made to the linguistic form of an utterance rather than its referent or when demonstrative expressions refer meta-linguistically to the preceding or following discourse segments (e.g., this section, this chapter). One can argue that the antecedents in such cases (i.e., this chapter and this section) are big chunks of text and therefore non-nominal. However, though these are certainly interesting cases, we do not focus on them in this article.\n\n2.1.3 Impure Textual Deixis. Lyons (1977) distinguishes between three different types of entities: First-order entities are physical objects. Second-order entities are events, states of affairs, and processes (Asher's eventualities), which are located in time and involve first-order entities and interactions between them. Third-order entities are propositions and facts (Asher's purely abstract objects), which have no spatiotemporal location, and involve first-and second-order entities and the interactions between them.\n\nFor anaphoric relations, Lyons introduced the term textual deixis, which describes the deictic relation obtained between a referring expression such as a pronoun and a piece of text. He distinguishes between pure textual deixis, where the referring expression refers to a textual unit as such (similar to Levinson's [1983] notion of discourse deixis), and impure textual deixis, where the expression is related to the third-order entity denoted by a textual unit, such as a fact or a proposition. If the relation involves a second-order entity (e.g., an event), it is not clear whether Lyons considers this relation an instance of impure textual deixis or simply ordinary anaphora.\n\n2.1.4 Situational Reference. Fraurud (1992) uses the term situational reference. She defines situations as entities representing eventualities (e.g., events, processes, and states) and factualities (e.g., facts and propositions). She uses the term antecedent for the clause or sentence that provides the anaphor's referent, but often the anaphor refers to a \"larger situation\"-for example, a whole sequence of events.\n\n2.1.5 Non-nominal Direct and Indirect Anaphora. Gundel, Hedberg, and Zacharski (2004) and Hedberg, Gundel, and Zacharski (2007) use the terms non-nominal direct and indirect anaphora. They operationalize this terminology as follows. An anaphoric relation is direct if the anaphor's referent is the same as the antecedent's referent, and it is indirect if the interpretation of the anaphor depends on that of the antecedent but they are not coreferential because the interpretation involves an additional step. Example (6), from Hedberg, Gundel, and Zacharski (2007), is an example of a direct anaphoric relation because both the anaphor and the antecedent refer to the event of the stock doubling on its first day of trading. In contrast, in Example (7), from Hedberg, Gundel, and Zacharski (2007), the clausal antecedent introduces the state of Giuliani being sleepy and the marked anaphor refers to the fact that he was sleepy, so the anaphor is not coreferential with the antecedent here, and it would be classified as an instance of indirect anaphora.\n\n(6) The winner was Internet Capital Group, a company that invests in other Internet companies. It more than doubled its first day of trading, Aug. 5., and that was just the beginning.", "filtered_refids": [[], [null, "b22", "b5", "b117"], ["b14", "b107", "b124", "b25", "b20", "b66", "b123", null, "b48"], ["b70"], ["b72"], ["b70"], ["b32"], ["b44", "b40"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 7069, "num_references": 19}
{"corpusid_sectionid": "51623319-s3", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "554", "section": "(7) Mayor Rudolph Giuliani, who gave himself the job of ubiquitous master of ceremonies of the city's New Year celebration, said he began his last day of the 1900s at 5:30 a.m. having trouble getting his lights on. \"I was convinced that it was Y2K,\" the mayor said, but \"actually I was sleepy.\" This perhaps explains an interesting mishap. . . Botley (2006) provides the following characteristic properties for indirect anaphora: (a) The antecedent is not nominal and is difficult to define directly, (b) the link between anaphor and antecedent is not one of coreference, and (c) the hearer may have to carry out a complex process of inference to arrive at the antecedent. Botley considers three main types of indirect anaphora: textual deixis 9 (Lyons 1977), situational reference (Fraurud 1992), and labeling (Francis 1986). We discussed textual deixis and situational reference in the previous subsections, and we will discuss labeling (i.e., shell nouns) in Section 3.1.\n\n2.1.6 Complex Anaphora. Consten, Knees, and Schwarz-Friesel (2007) coin the term complex anaphora, where anaphors are nominal expressions referring to propositionally structured referents, such as propositions, states, facts, and events. They define two criteria for complex anaphora: First, the antecedent has to be a syntactically complex entity-it must consist of at least one clause; and second, the antecedent must denote a conceptually complex item. 10 Consten, Knees, and Schwarz-Friesel define a conceptually complex item as a second-or third-order entity, according to Lyons' (1977) hierarchy (see Section 2.1.3).\n\n2.1.7 Extended Reference and Text Reference. Halliday and Hasan (1976, pages 52-53, 66-70) distinguish between two kinds of references of demonstrative pronouns and the pronoun it: extended reference and text reference. 11 An example from Halliday and Hasan (1976, page 52) is given in Example (8). 12 The first instance of it in the example refers to curtseying while you're thinking what to say, which they call extended reference, as the reference is no longer to a person or object but to a whole process or complex phenomenon, and the referent is expressed by a clause or string of clauses instead of a simple noun phrase. In contrast, the second instance of it is a case of text reference because it requires its referent to be transmuted into the fact that curtseying while you're thinking what to say saves time. Alice wondered a little at this, but she was too much in awe of the Queen to disbelieve it. 9 Botley calls this type text/discourse deixis. Discourse deixis is here understood in the sense of Levinson (1983), which is very close to Lyons' pure textual deixis. 10 Both conditions are necessary to distinguish non-NA anaphora from bridging relations (see footnote 6):\n\nExample (ia) is a case of a non-NA anaphor with this incident referring to the biting event reported in the previous sentence. Example (ib) is a case of a bridging relation: The expression the scars does not refer to an event but to a concrete entity, which is inferred from an entity involved in the biting event. ( 555 Table 1 Overview of terminology used for non-NA anaphora.", "filtered_refids": [["b30", "b72", "b10", "b32"], ["b72", "b18"], ["b70", null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3166, "num_references": 9}
{"corpusid_sectionid": "51623319-s5", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "II. Approaches that do not make a distinction between eventualities and factualities", "section": "Reference to abstract objects Asher (1993) and others abstract anaphora Webber (1988) and others discourse deixis Fraurud (1992) situational reference Consten, Knees, and Schwarz-Friesel (2007) complex anaphora 2.1.8 Comparison of Terminology. The overview of the terminology in the previous sections showed that the approaches have a lot in common. In particular, many approaches distinguish between two subclasses of abstract entities:\n\n1. Events and states: These are called eventualities by Asher and Fraurud, and secondorder entities by Lyons.\n\n2. Facts and propositions: These are called purely abstract objects by Asher, factualities by Fraurud, and third-order entities by Lyons.\n\nThe two subclasses make up a superclass, called saturated abstract objects, by Asher and situations by Fraurud. In this article, we will use the terms eventualities and factualities for the two subclasses, and abstract objects or abstract entities for the superclass (contrasting these with concrete objects/entities). The different kinds of terminology introduced here can roughly be divided in two classes, depending on whether they make a (terminological) distinction between anaphoric relations involving eventualities and relations involving factualities or not, as shown in Table 1. 13 Another distinction is whether non-NA anaphora is seen as a type of anaphora or deixis. If it is considered a type of anaphora, finding the referent involves first determining the antecedent. The anaphor's referent is then either the same as the antecedent's referent (i.e., they are coreferent), or it is derived from it. If non-NA anaphora is viewed as an instance of deixis, no antecedent is involved; rather, the anaphor's referent is determined by pointing to a region of the discourse or discourse model. As already stated, in this article we use the term antecedent to refer to the expression that most closely represents the interpretation of the anaphor, so far as it is overtly realized. We also occasionally say that an anaphor refers to a non-NA, meaning that the anaphor refers to some abstract referent that is represented in the text by the non-NA. Table 2 Typology of anaphora on syntactic and semantic scales. Non-NA anaphora as discussed in this article falls in Cell D.", "filtered_refids": [["b18", "b123", "b5", "b32"], [], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2269, "num_references": 5}
{"corpusid_sectionid": "51623319-s8", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Typology of Anaphora on Syntactic and Semantic Scales", "section": "The overview of different terminologies showed that most of them highlight semantic aspects of the phenomenon. In the context of building computational approaches, we argue for the use of the syntactic notion of non-NA anaphora. Table 2 describes the phenomenon of anaphora in general from syntactic and semantic aspects. On the syntactic scale, we show two ends of the scale: non-nominal and nominal. 14 On the semantic scale, we have abstract objects on the one hand, which include events, states, and processes along with purely abstract objects, such as propositions and facts; and on the other hand we have concrete objects. Cell A represents concrete nominal objects (cf. Example (1)). Most of the coreference and anaphoric relations annotated in corpora such as MUC-7 15 and OntoNotes (Weischedel et al. 2013) fall in this category. OntoNotes also includes annotated instances of certain types of relations involving eventualities, so that OntoNotes also covers a subset of Cell D relations (but see Section 4.2.5).\n\nCell B is representative of examples such as Example (9), where the surface linguistic form of the antecedent same-sex marriage is nominal but it represents an abstract concept. Some approaches we discuss in this article do include such examples in their annotation and resolution (e.g., Kolhatkar 2015). Note that one can argue that the actual issue here is not just the concept of same-sex marriage but rather whether to allow same-sex marriage or not, which is only implicitly stated in the text.\n\n(9) Same-sex marriage is currently one of the most divisive political issues in our nation. Analyzing this issue will help us understand what is happening in our country, and where we might go from here. 16\n\nCell C represents examples having non-NAs but that do not represent abstract objects.\n\nMetalinguistic anaphoric expressions such as this section or this statement, which refer to the spatiotemporal coordinates of the text or act of utterance, are examples of this category. The antecedents of such expressions are clearly non-nominal but they do not usually represent abstract objects according to our definition. Finally, Cell D is 14 Note that the table only shows two ends of the syntactic spectrum and the boundary between them is fuzzy. For instance, gerunds denoting events, such as the phrase the mayor's throwing of the pizza in the guest of honor's face (Asher 1993, page 16), fall somewhere in between the two. 15 https://catalog.ldc.upenn.edu/LDC2001T02. 16 Adapted from Martha Nussbaum. A right to marry? Same-sex marriage and constitutional law.\n\nDissent, Summer 2009. https://www.dissentmagazine.org/article/ a-right-to-marry-same-sex-marriage-and-constitutional-law.\n\nrepresentative of the Examples (4) through (8). We consider members of this category instances of non-NA anaphora. 17", "filtered_refids": [["b126"], ["b55"], [], [], [null], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 2833, "num_references": 3}
{"corpusid_sectionid": "51623319-s11", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Realization of Non-NA Anaphors", "section": "3.1.1 Pronouns. A very common way to signal non-NA anaphora is with the singular demonstrative pronouns this and that. Example (10) shows this and that referring to non-NAs. In Example (10a), this refers to the fact that my daughter is all grown up and going away to college; and in Example (10b), that refers to the proposition that this policy will help the poor. 18 (10) a. My daughter is all grown up and going away to college. This is so bittersweet.\n\nb. He says that this policy will help the poor. But we all know that's not true.\n\nThe personal pronoun it may also be used to refer to non-NAs. In Example (11), the pronoun it refers to the fact that John is a gifted player.\n\n(11) John is a gifted player, and he knows it.\n\nAlthough the pronouns this, that, and it have the capacity to refer to non-NAs, there is strong evidence showing that demonstrative pronouns are more likely. Two kinds of preferences with regard to pronominal anaphors can be discerned.\n\nFirst, we can examine whether, given a certain type of antecedent, a certain anaphor is preferred. There are clear correlations between the syntactic type of the antecedent and the choice of pronominal anaphors: Nominal antecedents are associated with it and non-NAs with demonstrative pronouns. In a corpus of career-counseling interviews, Schiffman (1985, Section 5.5.2) found that from a total of 298 non-NAs, that is used in 78.2%, and it in 21.8% (there were only very few instances of this in the corpus); if the antecedent is a sentence or paragraph, that is used in 88.9% of these cases. With nominal antecedents, her results do not show a clear preference: From a total of 227 nominal antecedents, 51.5% are referred to by it and 48.5% by that. 19 In a corpus of six texts from different domains, Webber (1991) observed that 98% of 81 nominal antecedents occurred with it and only 2% with this/that. There were 96 cases of clausal antecedents, and it was used in 16%, this in 65%, and that in 20%.\n\nThe second preference is in the opposite direction: Given a certain type of anaphor, we ask which type of antecedent is more likely. There are also correlations between the type of the pronoun and the syntactic type of the antecedent it refers to. For example, the majority of instances of it refer to nominal, non-abstract objects, as in Example (12).\n\n(12) There was a black kitten in the backyard. It was chasing a squirrel.\n\nThe figures reported by Webber (1991) show that 84% of (referential) it instances refer to nominal antecedents and 16% to clausal ones. In contrast, 98% of this/that instances refer to non-NAs. In a corpus of dialogues, Byron (2004, page 10) finds that from a total of 260 pronouns, 74% of the personal pronouns (including it) refer to a nominal antecedent but only 23% of the demonstrative pronouns do, whereas only 7% of the personal pronouns refer to a non-NA, in contrast to 32% of the demonstrative pronouns. Halliday and Hasan (1976, page 66), who considered demonstratives in general (both pronouns and determiners), counted 51 demonstratives in total in the last two chapters of Alice's Adventures in Wonderland: 43% this, 47% that, 6% these, and 4% those. Of these instances, 61% were used with non-NAs. In their analysis of instances of it, this, and that in the Santa Barbara corpus of spoken American English Part-I (Du Bois et al. 2000Bois et al. -2005, 20 Gundel, Hedberg, and Zacharski (2004) observed that among 56 examples of demonstrative pronouns, 24 (42.9%) were classified as non-NA anaphors and 21 (37.5%) were classified as NA anaphors. In contrast, among 2,046 instances of third person pronouns, only 110 instances (5.38%) had non-nominal antecedents.\n\n3.1.2 Shell Nouns. Another common way to signal non-NA anaphora is with shell nouns (Schmid 2000;Kolhatkar 2015). These are abstract nouns, such as fact, issue, or 19 These figures are not directly specified in Schiffman (1985). We consider only Schiffman's \"true NPs\" to be nominal antecedents and her \"sentence-like NPs\" to be non-nominal antecedents (see also Section 4.2.1). Details on how these figures (and others in this survey) have been calculated can be found at https://github.com/kvarada/non-NA_Resources. 20 http://www.linguistics.ucsb.edu/research/santa-barbara-corpus. Table 3 Lexico-grammatical patterns of shell nouns from Schmid (2002, adapted from page 24). The patterns are marked in italics. Shell noun phrases are underlined, and the antecedents or structurally related phrases providing the shell content are marked in bold, as usual. ' * ' marks additional patterns discussed by Schmid. All examples are from the New York Times Corpus (Sandhaus 2008).", "filtered_refids": [[], [], [], [], [], [null, "b124"], [], [], [null, "b124", "b40"], [null, "b108", "b109", "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 4658, "num_references": 9}
{"corpusid_sectionid": "51623319-s14", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Structurally-determined relations", "section": "3 N-be-to Our plan is to hire and retain the best managers we can. 4\n\nN-be-that The major reason is that doctors are uncomfortable with uncertainty. 5 N-be-wh Of course, the central, and probably insoluble, issue is whether animal testing is cruel. 6\n\nSub-be-N * If the money is available, however, cutting the sales tax is a good idea. 7\n\nN-to The decision to disconnect the ventilator came after doctors found no brain activity. decision, that have the capability to encapsulate and refer to propositional content. These nouns are known by a great variety of names in the literature, including container nouns (Vendler 1968), type-3 vocabulary (Winter 1977), anaphoric nouns (Francis 1986), label nouns (Francis 1994), and carrier nouns (Ivani\u010d 1991). They are likewise included in Halliday and Hasan's (1976) concepts of extended reference and text reference. In this article, we use Schmid's (2000) term shell nouns, which derives from these nouns' tendency to function as conceptual shells for propositional content.\n\nTo be a shell noun is not an inherent property of nouns themselves; rather, it is a property of particular instances of these nouns, which can be characterized individually as shell noun usages. In the context of shell nouns, the term shell content is often used to refer to the text that provides the interpretation of the shell noun phrase. 21 Schmid (2000) observed a number of lexico-grammatical patterns in which shell nouns tend to occur. Table 3 shows these patterns. Shell nouns may refer anaphorically to their shell content, as shown on lines 1 and 2, 22 or they can be structurally related to their shell content, as shown on lines 3 to 10. These relations include copula structures (lines 3-6) 23 and postnominal complement and modifier clauses (lines 7-10). As the pattern labels 21 The concept of shell content is similar to the concept of an antecedent except that in some shell noun constructions, the term antecedent is not quite appropriate. That said, to be consistent, we use the term antecedent for shell content except when it is absolutely necessary to use the term shell content. 22 Schmid (2000) clarifies that \"[i]n a way, the pattern th-be-N is a blend of the copular type N-be-cl and the anaphoric type th-N.\" (page 25). 23 The pattern Sub-be-N is not part of Schmid's (2000) original list but he discusses it in his example (3.5') on page 26.", "filtered_refids": [[], [], [], ["b43", "b31", "b49", "b127", "b30", "b117", "b109"], ["b109"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2394, "num_references": 8}
{"corpusid_sectionid": "51623319-s15", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "560", "section": "N vs. th-N suggest, in structurally determined shell noun constructions, definite noun phrases and even indefinite uses are common, whereas in anaphoric constructions, the noun is most frequently used with a demonstrative determiner. Note that although plural pronouns are only very rarely used in non-NA anaphora (if at all), plural demonstrative noun phrases as in th-N (Example (13)) and th-be-N (Example (14)) are possible.\n\n(13) Before applying for an appropriate job, you need to think about which city you want to live in, whether you want an academic or an industry position, whether you are willing to live away from your family. The application process is smooth if these decisions are made in advance.\n\n(14) Today we hear heated rhetoric between nuclear powers; we see a deeply divided United States; we watch as populist nationalisms take hold around the world. These are some of the major global obstacles that will trouble any engaged citizen of our planet.\n\nShell nouns occur frequently in many varieties of text, but they are especially frequent in argumentative texts, such as political debates, news articles, and academic discourse (Schmid 2000;Botley 2006;Kolhatkar 2015). Schmid (2000) provides a list of 670 nouns that can be used as shell nouns and occur in the patterns provided in Table 3. 24 He observed that shell nouns such as fact, idea, point, and problem are among the one hundred most frequently occurring nouns in a corpus of 225 million running words of British English. 25 Because of the detailed existing documentation on shell nouns, their lexico-grammatical patterns, and the relatively stable word order of English, it is possible to gather shell noun instances largely automatically, which makes them, from a computational linguistics perspective, a great source of information for studying the phenomenon of non-NA anaphora.\n\n3.1.3 Reflexives, Pro-verbs, Pro-actions, Pro-adjectives, Adverbs, etc. Apart from the main constructions described earlier, there are a number of other constructions that may also be used in non-NA anaphora.\n\nThe first one is reflexive pronouns. Non-NA anaphora can be realized with the reflexive pronoun itself, as shown in Example (15).\n\n(15) That we are hungry shows itself in our crankiness.\n\nThe second one is the pro-verb construction where the anaphor is a form of do (cf. Halliday and Hasan 1976, pages 125-126;Hirst 1981, page 19;Miller 2011). 26 The following example from Hirst (1981, page 19) contains such a construction. Here, the pro-verb does refers to the verb phrase orders sweet and sour fried short soup as its antecedent.\n\n(16) When Ross orders sweet and sour fried short soup, Nadia does too.\n\nThen there are pro-action constructions in which do is used in conjunction with so, it, or demonstrative pronouns (Hirst 1981;Miller 2011) and whose antecedents usually denote actions. Example (17), from Hirst (1981, page 20), demonstrates such a usage. Here, the anaphor does it refers not to the previous event but to the action-Sue cooks Ross's dinner and not her own.\n\n24 Note that Schmid's lists of shell nouns and their patterns are not exhaustive. 25 Schmid used the Bank of English corpus, which is jointly owned by HarperCollins Publishers and the University of Birmingham http://www.titania.bham.ac.uk/docs/. 26 Note that these pro-action constructions may also be considered cases of verb phrase ellipsis.\n\n(17) Ross makes his dinner on weekdays, but when she stays the weekend Sue does it for him. Cornish (1992) points out non-NA anaphora realized with the adverb so. In Example (18), the antecedent is an adjective phrase, and in Example (19), the antecedent is a prepositional phrase (both examples are from Cornish 1992).\n\n(18) I'm extremely busy at the moment, and expect to be so for the next two hours at least.\n\n(19) The entire factory is on strike, and is forecast to stay so for some considerable time.\n\nThese constructions have not yet received much attention in computational approaches, and we do not discuss them in the rest of the article. 27\n\n3.1.4 Summary of Expressions Used as Anaphors Referring to Non-Nominal Antecedents. As we saw in the previous subsections, non-NA anaphors can be realized with a variety of expressions. We will use the following terms throughout this article. 28 1. Personal pronoun: it 2. Demonstrative pronouns: this, that 3. Shell nouns: Any of a number of nouns that may occur in one of the patterns listed in Table 3 4. This NP: The subset of shell nouns that occur with a demonstrative determiner, e.g., this issue, these facts, that situation 5. Demonstratives: an umbrella term for demonstrative pronouns and this NPs\n\nNote that although the expressions in this list have the capacity to refer to non-NAs, not all instances of these expressions refer to such antecedents. For instance, we have already seen examples of the pronoun it referring to nominal antecedents, as in Example (12). Other instances of it are pleonastic, namely, they do not refer to any specific entity but serve as syntactic placeholders, as in Example (20).\n\n(20) a. It is gorgeous out with blue skies and big fluffy white clouds and a light breeze. b. It is snowing.\n\nFrom a computational perspective, automatically identifying whether a given instance of a shell noun or this, that, or it refers to a non-NA or not is not possible with regular expression-based queries alone. At a minimum, we need part-of-speech 27 Anand and Hardt (2016) present a machine learning approach to sluicing, an elliptical phenomenon closely related to the verb phrase ellipsis in Example (16). 28 Note that the terms pronoun and demonstrative are sometimes used differently in the literature. We reserve the term pronoun for cases where the pronoun substitutes an entire NP, e.g., as in Example (ia). If it is used like an article, we call it a determiner; see Example (ib). Other work would call the expression this a demonstrative pronoun in both uses (e.g., Artstein and Poesio 2006). The term demonstrative is sometimes used to refer to demonstrative pronouns only (as in Example (ia)), excluding full NPs with a demonstrative determiner (as in Example (ib)), e.g., by Eckert and Strube (2000).", "filtered_refids": [[], [], [], [null, "b10", "b109", "b55"], [], [], [], [null, "b76"], [], [null, "b76", "b46"], [], ["b19"], [], [], [], [], [], [], [null, "b25", "b4"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 47, "num_chars": 6200, "num_references": 13}
{"corpusid_sectionid": "51623319-s18", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Lexical and Semantic Preferences.", "section": "Non-NA anaphora is in many crucial respects a semantic phenomenon and a number of semantic preferences and properties are associated with it. These properties can be broadly categorized into three types: those imposed by the context of the anaphor, by the anaphor or the antecedent itself, or by the context of the antecedent.\n\nPreferences imposed by the anaphor context. It is often assumed that a non-NA does not introduce various types of semantic objects into the discourse model but, instead, that one semantic object-for example, an event-type discourse referent-is introduced, which can be transformed into other types when it is referred to anaphorically. Webber (1988), Eckert and Strube (2000), and Byron (2004) refer to this property as referent coercion and Webber (1991) calls it ostension. An example of referent coercion adapted from Eckert and Strube (2000) is shown in Example (22). c. This shows how careless he is. (fact)\n\nHere, the antecedent John crashed the car denotes an event, whereas the pronouns refer to a variety of semantic types, depending upon the context. For instance, in Example (22b), this refers to the event of crashing, and, as in Example (22c), this refers to the fact that John crashed the car. The predication context of an anaphoric expression plays an important role in identifying the semantic type of the referent. For our purposes, semantic type is an abstract object, as defined by Asher (1993) (e.g., fact, proposition, or event). For instance, the verb believe can only be applied to an object argument that represents a proposition (e.g., see Example (22a)), and happen can only be used with subjects denoting some sort of event (Asher 1993, pages 22, 192) (e.g., see Example (22b)).\n\nThe anaphor's semantic type preferences can constrain the syntactic type of antecedents. In Example (23), the anaphor refers to a concept and, accordingly, the antecedent is realized by a verb phrase (VP) instead of a sentence.\n\n(23) John crashed the car. Jane did that too. (concept) Eckert and Strube (2000) introduce the concepts of A(bstract)-incompatibility and I(ndividual)-incompatibility. A particular instance of an anaphor is described as Iincompatible if, given its context, it cannot refer to an individual, concrete entity. Typical contexts for I-incompatible anaphors include those in which adjectives are used that can only be applied to abstract entities, such as in x is true or x is correct, and those in which the anaphor is equated with abstract entities, such as facts or reasons: x is why he's late implies that x is a reason, an abstract entity. A-incompatibility refers to the opposite situation, in which the context precludes the anaphor from referring to abstract objects, as in x is loud or eat/drink/smell x.\n\nPreferences imposed by the anaphor or the antecedent. According to Hegarty, Gundel, and Borthen (2001), the degree of world immanence of an entity and the degree of its individuation contribute in deciding whether it can be referred to by demonstratives or the pronoun it. As Asher (1993) points out, eventualities have causal, spatial, and temporal properties, and thus they have high world immanence. The personal pronoun it preferably refers to such entities. In contrast, factualities such as facts, situations, or propositions have very low world immanence, and demonstratives are highly preferred to refer to them; see Example (24) from Gundel, Hedberg, and Zacharski (2005). Example (24a) shows that it (and that) can refer to the event of insulting. If one wants to refer to the situation, demonstrative pronouns are clearly preferred over the personal pronoun it; compare Examples (24b) and (24c).\n\n(24) John insulted the ambassador.\n\na. It/that happened at noon. (event) b. That/this was intolerable to the embassy. (situation) c. ?? It was intolerable to the embassy. (situation)\n\nAnother preference imposed by an antecedent is with respect to tense. Schmid (2000, pages 104-105) notes that there is a strong present or past tense preference (as opposed to future tense or modal forms) in antecedents whose semantic type is fact, which he relates to the semantics of facts: Future facts are not knowable and therefore speakers will prefer other shell nouns for these types of content.\n\nPreferences imposed by the antecedent context. Hegarty, Gundel, and Borthen (2001) describe how the complements of bridge verbs, such as think, believe, and say, are typically accessible to reference with demonstratives, but not with the personal pronoun it, as shown in Example (25), from their paper.\n\n(25) A: Alex believes that the company destroyed the file.\n\na. B: That's false; the file has been submitted to the district judge. b. B: # It's false; the file has been submitted to the district judge.\n\nOn the other hand, entities introduced with complements of factive verbs, such as verify or know, are equally accessible to demonstratives and the pronoun it, as shown in Example (26), from their paper. According to Hegarty, Gundel, and Borthen (2001), factive verbs mark the entity expressed by the complement clause as already familiar, so that successive reference by it is possible.\n\n(26) A: Alex verified that the company destroyed the file. a. B: That's false; the file has been submitted to the district judge. b. B': It's false; the file has been submitted to the district judge.", "filtered_refids": [[], ["b14", "b123", "b25"], [null], [], ["b25"], ["b45", "b41"], [], [], [null], ["b45"], [], [], ["b45"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 41, "num_chars": 5366, "num_references": 10}
{"corpusid_sectionid": "51623319-s19", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Syntactic Preferences.", "section": "According to Asher (1993, page 226), the range of syntactic constructs of abstract antecedents is quite broad. Some examples of the different linguistic constructions that may function as abstract antecedents are:\n\n1. That clauses; e.g., John believed that Mary was sick 2. Infinitival phrases; e.g., Fred wanted to go to the movies 3. Naked infinitive complements; e.g., John saw Mary arrive 4. Noun phrases that appear to denote proposition-like entities; e.g., The claim that Susan got a C on the test was surprising\n\nMoreover, it seems evident that the semantic type of the antecedent also suggests corresponding syntactic realizations. Schmid (2000, page 381) provides the frequencies of 670 shell nouns from the 225 million-word corpus of the British section of the Bank of English corpus. For each shell noun, he provides the frequency distribution of that shell noun across the different lexico-grammatical patterns from Table 3. Among other tendencies, it is evident from these frequencies that purposes and decisions are more likely to be represented by to-clauses; explanations and facts by that-clauses; and questions and issues by wh-question clauses. Passonneau (1989) analyzed local contexts of it and that in four career counseling interviews. She observed that grammatical functions (e.g., subject) play an important role: If both the anaphor and its antecedent are subjects, it is far more likely than that; if either one or both are not subjects, that is more likely. Moreover, the syntactic type of the antecedent is also relevant: If the antecedent is pronominal, it is more likely than that. If the antecedent is non-nominal or a gerund, that is more likely. With canonical NP antecedents, both are equally likely.", "filtered_refids": [[null], [], [null, "b90"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1736, "num_references": 3}
{"corpusid_sectionid": "51623319-s20", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Discourse-Level Preferences.", "section": "In the literature, discourse-level properties of non-NA anaphora are discussed in terms of three notions: salience, focus, and topic. These notions are usually grounded in Centering Theory (Grosz and Sidner 1986) and theories explaining the cognitive status of these expressions.\n\nCentering Theory models coherence and salience. It assumes that each utterance introduces new discourse entities into the discourse, which are organized in a focus space. Focus spaces, which constitute the global focus, are ordered in a stack so that only entities of the most recent space are in the local focus and, e.g., accessible for subsequent reference by pronouns. The discourse entities introduced by an utterance are ranked and the most highly ranked entity is referred to as the preferred center (CP). The backward-looking center (CB) of an utterance is defined as the highest ranked element of the previous utterance that is realized in the current utterance. The theory itself keeps many notions, such as utterance or ranking, deliberately open, and researchers define these notions differently, depending on their theory and the language under investigation. An utterance is generally defined as a sentence or a clause. Ranking is generally based on the grammatical function (e.g., subject is ranked higher than object) or on information status (e.g., hearer-old entities are ranked higher than hearer-new entities) (cf. Poesio et al. 2004).\n\nThe CP of an utterance is considered the most salient entity. The notion of CB is the closest concept to the traditional notion of topic (e.g., Taboada and Wiesemann 2010). Another relevant term is activation, which can be defined in the framework of Centering Theory in different ways. A discourse referent can be considered activated if it is in the local focus, or in the global focus and sufficiently salient (Poesio and Modjeska 2005).\n\nGivenness Hierarchy from Gundel, Hedberg, and Zacharski (1993, page 275) The general idea behind the cognitive status of an anaphoric expression is as follows. In language, we use different expressions to refer to the same thing. For instance, a particular fact can be referred to as a fact, the fact, this fact, that fact, this, that, or it. The question is, when do writers use a particular anaphoric expression, and what enables readers to interpret this expression appropriately? Many researchers have made claims about the cognitive status of the antecedents of different expressions, and they support their claims with appropriately annotated data. Gundel, Hedberg, and Zacharski (1993) propose a Givenness Hierarchy of six cognitive statuses of discourse referents, which reflect a speaker's assumptions about the addressee's knowledge and current state of attention. These statuses determine the necessary and sufficient conditions on the use of each referring form in discourse (cf. Figure 2). For instance, by using it as an anaphor, the speaker signals that the expression refers to an entity in focus, whereas with a form such as this N, the speaker refers to an activated entity. Gundel, Hedberg, and Zacharski note that pronominal anaphors, as a universal tendency, prefer their referent to at least be activated, which makes sense because the pronouns only have minimal descriptive content, so in order to facilitate identification of their referents they have to be at least activated.\n\nBased on these notions, the following observations have been made in the literature. 29\n\nReferring to the entities in focus. It has been demonstrated by different researchers in different domains that the pronoun it requires its referent to be in an addressee's focus of attention, whereas demonstratives only require them to be activated, namely, present in working memory (the global focus) but not necessarily in focus (i.e., in the local focus). Hegarty, Gundel, and Borthen (2001) start from the hypothesis that the Givenness Hierarchy can explain the findings of Schiffman (1985) and Webber (1991) that nominal anaphora are preferably realized with it and non-NA anaphora with demonstratives (see Section 3.1.1). These findings can be explained if entities introduced by nominals are more easily brought into focus than their non-nominal counterparts. They link an entity's property of being in focus with its degree of world immanence (compare Section 2.1). Accordingly, concrete objects are often brought into focus, eventualities less so, and factualities only rarely. Eventualities are more accessible than factualities because they can be directly introduced by clauses, whereas factualities have to be derived from them (see Section 2.1). The findings from their corpus support this: Gundel, Hedberg, and Zacharski (2002) observed that of 2,046 instances of third-person personal pronouns (including she, they, etc.) from the Santa Barbara Corpus of Spoken American English, 83.34% had nominal antecedents and 5.38% were instances of the anaphor it with non-NAs. Among these instances, only 14.54% involved facts or propositions, 57.27% involved situations, and 27.27% eventualities. Gundel, Hedberg, and Zacharski classify situations as less abstract than facts and propositions and note that the distinction between eventualities and situations is not always clear. Hedberg, Gundel, and Zacharski (2007) analyze 321 instances of pronominal this (44%) and that (56%) in a corpus composed of two issues of the New York Times. They define an entity as activated if it is in the local focus but not salient (i.e., it has been introduced in the previous sentence but not in a syntactically prominent position). It is not clear from their paper how many instances are non-NA anaphora but the majority probably are. They observe that of 256 instances for which the annotators were in agreement, 96% of the demonstrative pronouns refer to activated entities that are not in focus. They also find that the majority of these cases are indirect anaphora, requiring coercion. An annotated example from Hedberg, Gundel, and Zacharski (page 35) is shown in Example (27). (27) <P num=\"405\"> With the exception of Japanese equities, <1> Americans have been selling more foreign stocks than they have been buying in recent months. </1> </P> <P num=\"406\"> But <that ACTIVATED INDIRECT \"the situation that Americans have been selling foreign stocks more than buying them\" 405.1 A3 num=\"06\"> that </that> could change. </P>\n\nIn the example, the antecedent is located in the sentence identified as 405, and is marked by the SGML tags <1>. . . </1>. The tag containing the anaphor that carries all annotated features, for example, ACTIVATED for the cognitive status, INDIRECT for the type of anaphoric relation, a description of the anaphor's referent (\"the situation that . . . \"), and a pointer to the antecedent (405.1).\n\nReferring to salient (highly ranked) entities. If being in (local) focus is defined via the focus space, all entities introduced in the same utterance share the same degree of being in focus. Such entities, however, can be differentiated by another property, salience. An entity is made salient if it is introduced in a syntactically prominent position (e.g., as the subject or object) or if it is mentioned repeatedly (Gundel, Hedberg, and Zacharski 1993;Gundel, Hegarty, and Borthen 2003). Gundel and colleagues have shown that (unstressed) personal pronouns in general are used to refer to highly ranked, salient entities in discourse, operationalized as the preferred center in Centering Theory. In contrast, demonstratives are associated with focus shift (Gundel, Hedberg, and Zacharski 1988), when the anaphor does not refer to the preferred center, contrary to the unmarked case. Hegarty, Gundel, and Borthen (2001) assume that clausal propositions, facts, or situations are more accessible to reference with it if they have already been mentally represented by the addressee. If a speaker refers to that entity, it causes the addressee to reprocess that entity, which renders it more salient. Two examples from Hegarty, Gundel, and Borthen illustrate this. Example (28a) shows that it cannot immediately be used to refer to the situation. Instead, it can only refer to the snake here. In contrast, in Example (28b), reference to the situation by that is possible, and due to that prior mention, subsequent reference by it is also possible.\n\n(28) There was a snake on my desk.\n\na. # It scared me.\n\nb. That scared me. It scared my office-mate too.\n\nThe second example illustrates that it vs. that can be used to indicate prior beliefs. In Example (29), the alternative replies (29a) and (29b) imply different background knowledge. After the statement by speaker A, the fact that linguists earn less than psychologists is at least activated. In Example (29a), speaker B uses that, thereby signalling the activated cognitive status of the abstract entity. However, in Example (29b), speaker B' uses it, thereby implying that she already knew about this fact (such that it must already have been activated before the statement by speaker A), and due to being mentioned by speaker A, it has become salient and an entity in focus.\n\n(29) A: I just read that linguists earn less than psychologists. Poesio and Modjeska (2005) analyzed 112 instances of this and these (used as pronouns and determiners). Forty-nine percent referred to nominal antecedents and 17% to non-nominal ones. They observed that 75-80% of the instances referred to entities other than highest-ranked entity (CP).\n\nReferring to the topic of the utterance. The pronoun it tends to refer to the topic of the conversation, whereas demonstratives tend to refer to more peripheral antecedents. Poesio and Modjeska (2005) in their study of 112 instances of this and these found evidence for the hypothesis that these anaphors are used to refer to entities other than the CB of the current utterance (this was supported by 61-65% of the instances) or the CB of the previous utterance (supported by 90-93%). 30 3.2.4 Distance Between Anaphor and Antecedent. In anaphora resolution, an important factor that affects the accessibility of antecedents is the distance between the anaphor and antecedent. Recency plays an important role in anaphora resolution systems (Mitkov 2002;Poesio, Ponzetto, and Versley 2010). A short distance between the anaphor and the antecedent implies a smaller search space and a smaller number of competing antecedent candidates, whereas a long distance implies a larger search space with many competing antecedent candidates. The distance can be measured in terms of tokens, sentences, spatiotemporal proximity, or the number of edges between nodes in some discourse structure, such as those posited by Rhetorical Structure Theory (RST) (Mann and Thompson 1988).\n\nThe distance preferences vary according to the anaphoric expressions used. We now list some tentative suggestions about these preferences from the literature.\n\nLinear distance: demonstrative pronouns vs. personal pronouns. Comparing pronominal instances of NA and non-NA anaphora, Byron (2003, pages 34-35) observed that the more semantic information a pronoun has, the larger the average distance to its antecedent. The average distance of all pronouns in one of her data sets was 8.81 words. The largest 30 Poesio and Modjeska (2005, Section 3.1) call the CB (discourse) focus as well as topic.\n\n568 average distance occurred with gender-marked personal pronouns (she, hers, her: 9.84; he, his, him: 9.2), followed by neuter personal pronouns (it, its, they, them, their: 8.72). Demonstrative pronouns occur closest to their antecedents (this, that, these, those: 8.18). The differences between the types do not seem large, though. Instances of non-NA anaphora would fall into one of the last two classes.\n\nLinear distance: pronominal demonstratives vs. this NPs. The pronominal demonstratives this and that and the personal pronoun it are typically closer to their antecedents than this NPs (Schmid 2000;Kolhatkar 2015). In particular, demonstrative pronouns on their own are not particularly informative, and so the distance between the anaphor and the antecedent is fairly small and the textual coherence fairly strong (i.e., there are fewer competing candidates). In contrast, this NPs are informative because they are headed by a content noun. They license long-distance as well as short-distance antecedents, as shown in the following examples.\n\n(30) Once an international poverty line is set, it must be converted to local currencies. This is trickier than it sounds. Currency exchange rates are inappropriate because most of the items that the poor consume are not traded on world markets. Living expenses are much lower in rural India than in New York, but this fact is not fully captured if prices are converted with currency exchange rates. (NYT)\n\nHere, the distance between the anaphor and the antecedent is small: The antecedent of this fact occurs in the preceding clause. In contrast, in Example (31), the antecedent of this question occurs four sentences away from the anaphor sentence. 31\n\n(31) Among Roman Catholics, the differences were even more striking. Only 28 percent of Catholics who said religion was very or extremely important to them favored keeping abortion legal, but 72 percent of Catholics for whom religion was less important favored the legal status quo.\n\nThe sense of a public struggling with a morally difficult issue was dramatically conveyed when the survey asked: \"Would you approve or disapprove of someone you know having an abortion?\"\n\nThirty-nine percent said they would approve and 32 percent said they would disapprove. But 25 percent more volunteered a response not included in the question: They said their view would depend on the circumstances involved. An additional 5 percent did not know. The lack of a clear majority for either of the unequivocal responses to this question may be the best indicator of where public opinion really stands on abortion. (NYT)\n\nTemporal and spatial distance: this vs. that. Among demonstrative pronouns, this tends to be associated with the entities that are spatially, temporally, or textually close to the speaker, whereas that tends to be associated with the entities that are not close to the speaker (Halliday and Hasan 1976). Textual proximity can be defined in terms of the relation between participants in a dialogue, that is, something said by the speaker versus something said by an interlocutor (Halliday and Hasan 1976). For instance, imagine a dialogue between two speakers, as shown in Example (32). Note how speaker A uses the pronoun this to refer to their own statement (i.e., it's time we take action), whereas speaker B uses that to refer to the same statement.\n\n(32) A. It's time we take action. I know I have said this before, but now I really mean it. B. What do you mean by that?\n\nExample (33), from Halliday and Hasan (1976), demonstrates temporal proximity preferences: That tends to be associated with a past-time referent, whereas this for one in the present or immediate future.\n\n(33) a. We went to the opera last night. That was our first outing for months.\n\nb. We're going to the opera tonight. This'll be our first outing for months.\n\nDistance in a discourse structure. There are suggestions in the literature regarding the accessibility of antecedents in a discourse representation. This constraint is typically referred to as the right-frontier constraint (Polanyi 1985;Webber 1991) or the principle of availability (Asher 1993, page 313). The formal definition of the constraint varies according to the discourse representation theory and structure under consideration. That said, the general idea is that only those discourse segments can yield referents for anaphors that correspond to nodes on the right frontier of a formal discourse tree (Polanyi 1985;Webber 1991;Asher 1993, page 270;Asher 2008). The constraint is a visual representation of which salient nodes in a given discourse are accessible for later reference. The intuition is that given a discourse structure, represented as a tree, a referring expression cannot attach to a constituent to the left of the current constituent. An example from Webber (1991) is given in Example (34).\n\n(34) a. There's two houses you might be interested in. b. House A is in Palo Alto. It's got 3 bedrooms and 2 baths, and was built in 1950. It's on a quarter acre, with a lovely garden, and the owner is asking $425K. But that's all I know about it. c. House B is in Portola Valley. It's got 3 bedrooms, 4 baths and a kidney-shaped pool, and was also built in 1950. It's on 4 acres of steep wooded slope, with a view of the mountains. The owner is asking $600K. I heard all this from a real-estate friend of mine. d. Is that enough information for you to decide which to look at? e. # But that's all I know about House A.\n\nHere, parts (b) and (c) are central parts of the text. According to Webber, the continuation (e) is ill-formed, because, at this point, the information about House A is closed off and no longer accessible. The only accessible antecedents are the ones on the right frontier: (1) the information on both houses, that is, the information spanned by the root node and (2) the information on House B. Figure 3, from Webber (1991), illustrates the structure of the discourse tree at that stage. Only the nodes on the right side of the tree can serve as attachment points for (e). Asher (1993Asher ( , 2008 and Afantenos and Asher (2010) present a version of rightfrontier constraint in the Segmented Discourse Representation Theory (SDRT) framework. They even demonstrate that SDRT's version of this constraint is respected about 95% of the time in their corpus of texts in French from different genres.\n\nFrom a computational linguistics perspective, there are two problems with this constraint. First, researchers have demonstrated violations of the constraint (e.g., Poesio, 570 !\"#", "filtered_refids": [["b34"], ["b101"], ["b97"], [null, "b38"], [], ["b39", "b44", "b124", "b45", null], [], ["b45", "b42", "b38", "b37"], [], [], [], [], ["b97"], ["b73", "b77", "b97", "b99"], [], [null], [], ["b109", "b55"], [], [], [], [], [], ["b43"], [], ["b43"], [], [], [null, "b6", "b124", "b104"], [], ["b6", "b124", "b5", "b0"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 141, "num_chars": 17924, "num_references": 32}
{"corpusid_sectionid": "51623319-s24", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Challenges Associated with Annotating Non-NA Anaphora", "section": "Annotating non-NA anaphora is a challenging problem and researchers have discussed associated difficulties, low inter-annotator agreement, and how they worked around the difficulties.\n\nClassifying an anaphor as an instance of non-NA anaphora. Halliday and Hasan (1976, pages 66-68) informally analyzed 51 demonstratives (determiners and pronouns) in the last two chapters of Alice's Adventures in Wonderland as part of their investigations of text cohesion. They mention that it is not always easy to distinguish between NA and non-NA uses.\n\nIdentifying suitable non-nominal antecedents. Botley and McEnery (2001) and Botley (2006) make the point that non-NA anaphora poses difficulties for corpus-based linguistics in that 29% of the cases (186 instances) were hard to analyze. Botley (2006) points out two main reasons for this difficulty: the lack of clear surface linguistic boundaries and the complex or unclear inference process for retrieving antecedents. Poesio andModjeska (2002, 2005) analyzed 112 this NPs. They were interested in the cognitive status of this NPs in the given discourse. Because of the difficulties associated with identifying the precise antecedents of this NPs, they developed an annotation scheme where the annotators do not have to mark the actual antecedents. Rather, the scheme instructs the annotators to classify this NPs into different categories such as visual deixis, discourse deixis, and anaphoric, and, based on these categories, they assign a cognitive status to each this-NPs instance. The annotators achieved agreement of \u03ba 0.82 in this classification task. 32 Artstein and Poesio (2006) report two experiments where 20 untrained annotators were asked to mark antecedents of NA and non-NA anaphora in a TRAINS91 dialog (Allen and Heeman 1995). 33 In the first experiment, they asked naive annotators to mark unconstrained regions of text as antecedents for NPs in general, including non-NA anaphors. In the second experiment, four annotators with prior experience annotated another dialog. This time, only (sets of) entire utterances could be marked as the antecedent. In the first experiment, in only 42% of the cases did annotators agree with the most popular choice for the beginning of the antecedent, and in 64% they agreed with the most popular choice for the end. The second experiment showed a similar tendency for annotators to agree more on the ends of the segments than on their beginnings.\n\nDetermining semantic and cognitive features. Gundel, Hedberg, and Zacharski (2004) analyzed 99 instances of demonstrative pronouns. Initially, they planned to annotate the semantic types of both the anaphor and its antecedent (which might differ because of coercion). They assumed that there are clear correlations between the syntactic form and the semantic type of an antecedent (e.g., VPs denote either activities or states). In contrast, the semantic type of the anaphor is hard to determine and can only in certain cases be deduced easily from the predicate's semantic restrictions. They therefore abandoned annotating exact semantic types and instead marked the relation as direct when the referent of the anaphor was the same as the referent of the antecedent and indirect otherwise. They classified the pronouns into six categories: nominal direct, nominal indirect, non-nominal direct, non-nominal indirect, pleonastic, and other. All three coders agreed on the classification of only 56 out of 99 instances. Hedberg, Gundel, and Zacharski (2007) analyzed 321 instances of demonstrative pronouns and report \u03ba 0.46 (moderate agreement) for identifying the cognitive status of the antecedent (activated or in focus), and \u03ba 0.70 (substantial agreement) for identifying the type of the antecedent (direct or indirect). 34 They do not report agreement in identifying the actual antecedents.", "filtered_refids": [[], [null], ["b10", "b4", null, "b93", "b2"], ["b44", "b40"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 29, "num_chars": 3842, "num_references": 8}
{"corpusid_sectionid": "51623319-s25", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Resources", "section": "Synthesizing annotation approaches related to non-NA anaphora is not trivial, as these approaches (a) do not always share the same goals, (b) consider different anaphoric expressions in different corpora and domains, or (c) focus on different properties. A survey of annotation efforts for non-NA anaphora in different languages has been written by Dipper and Zinsmeister (2012), and we do not want to repeat that information in this article. Instead, in this section, we will focus on corpora that provide useful information for building computational systems. In particular, we will focus on the following questions.\n\n1. What is the goal of the annotation? 2. What is covered by the annotation? This question concerns three aspects:\n\nFirst, which anaphoric expressions are considered for annotation? Second, are antecedents marked and, if so, how does the annotation approach the challenge of annotating text segments with imprecise boundaries? Third, how are other NPs annotated, for example, non-referring NPs or NPs that do not co-refer with some other expression (singletons). The last aspect is relevant to all systems that deal with naturally occurring data, where the task is first to classify an NP as referring or not, and second, to classify a referring NP as an instance of a NA or non-NA anaphor.\n\n5. What documentation is available? How do they measure the inter-annotator agreement?\n\n6. Which corpora are available for researchers and what information from the annotations can be incorporated into computational systems? To what extent can the corpora be used as training data for machine learning systems?\n\nFor coreference in general, there are quite a few annotated corpora available. Most of them, however, focus on what Uryupina et al. (2018) call \"relatively easy cases\" of anaphoric reference, that is, anaphora with nominal antecedents. As described in the previous section, annotating non-NA anaphora represents a major challenge and is often done by trained expert annotators. Hence, corpora annotated with non-NA relations are scarce and often small. In this section, the most important resources of non-NA anaphora in terms of size and richness of annotation are described in detail. 35 (1985); Passonneau (1989). Passonneau (n\u00e9e Schiffman) investigated the use of pronominal it and that. The assumption underlying her work is that the two pronouns serve complementary discourse functions: In certain contexts, the anaphor it will be the unmarked (i.e., most frequently chosen) option and that the marked one, and vice versa. Schiffman (1985) and Passonneau (1989) cover both NA and non-NA anaphora.", "filtered_refids": [["b23"], [], [], [], [], ["b115", "b90", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 2626, "num_references": 4}
{"corpusid_sectionid": "51623319-s26", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Schiffman", "section": "In her thesis, Schiffman assumed that conversational data (in particular, conversations involving problem-solving, information-gathering, or counseling situations) would contain a high frequency of anaphoric it and that. She decided to examine conversations involving speakers from similar backgrounds to increase the degree of uniformity across different individuals, and the participants should not know one another so that their conversations would be easy to follow for observers. This resulted in a corpus of career-counseling interviews between a professional counselor and graduate students at the university.\n\nFrom eight interviews, four were randomly selected for the current study, annotated by a trained student research assistant, and checked several times by the author of the study. In total, the corpus contained 31,780 tokens, among them 983 instances of pronominal it and that, of which 298 were instances of non-NA anaphora.\n\nIn her study, Schiffman (1985) distinguished between three kinds of anaphoric relations, depending on the form of the antecedent: NPs, VPs, and sentential constituents (clauses, sentences, and paragraphs). 36 In her work, the term NP is defined very broadly and includes nominalizations in Vendler's (1968) sense, such as gerunds or that clauses. This makes it rather difficult to extract results from her thesis that concern just the instances of non-NA anaphora according to our definition. At some point in her analysis, however, she distinguishes between different types of NP antecedents, which can be ordered according to their \"nouniness,\" from canonical, \"true NPs\" with a lexical head to derived nominals and gerunds to sentence-like NPs in the form of infinitives and clauses introduced by that, whether, or if. The categories that are the least noun-like (infinitives and clauses) would be instances of non-NA anaphora by our definition.\n\nSchiffman (1985) considered a range of syntactic and pragmatic factors, with a focus on directly observable, theory-independent aspects. Syntactic features include 35 At https://github.com/kvarada/non-NA_Resources we maintain a repository that collects all kinds of resources related to non-NA anaphora, including (links to) corpora and annotation guidelines. 36 Schiffman uses the term nominal anaphora to refer to anaphora with NP antecedents, and non-nominal anaphora for anaphora with VP antecedents. the clause level (if it occurs in a main or subordinate clause) of the anaphor and its antecedent, if present, and its grammatical function (subject, direct object, copula predicate, 37 other). Another feature concerned the form of the antecedent, if present, with the possible values NP, VP, S, none (mainly for non-referential uses), discontinuous, and multiple (for ambiguous cases). Further features described the relation between the anaphor and the antecedent: sentence distance (same, adjacent, remote sentence) and adjacency, recording whether some other NP intervenes between the anaphor and the antecedent that could serve as an antecedent. Possible values of adjacency were: adjacent (no intervening NP), nearest semantic match (no inanimate, singular NP intervenes), not adjacent (at least one inanimate, singular NP intervenes). Schiffman (1985) compares the use of it and that with a statistical analysis of the annotated features. The results show certain preferences for one of the pronouns, depending on the antecedent's form: If the antecedent is non-nominal, that is preferred; Otherwise, if the antecedent is a pronoun, it is preferred. If the antecedent is a (true) NP, it and that are equally likely (Schiffman 1985, Section 5.5.2). In such cases, the grammatical function of the antecedent plays a role: If it is a subject, it is preferred; otherwise, that is more likely. (2000). These researchers annotated data from spoken language with the goal of evaluating the performance of their anaphora resolution algorithm. They chose spoken rather than written data because they expected spoken data to contain more pronominal anaphors and a more diverse range of anaphor types, including non-NA anaphors and vague anaphors, which lack a clearly defined linguistic antecedent. The corpus they analyze is the Switchboard corpus (Godfrey and Holliman 1993), 38 which contains transcribed telephone conversations between two people who were not acquainted with each other and were asked to talk about given topics such as childcare. The authors argue that this sort of data is easier to follow than unconstrained conversations, and, at the same time, it is more diverse than task-oriented dialogues like TRAINS (Allen and Heeman 1995), 39 which often contain a lot of imperative-like constructions and mostly refer to concrete objects. Eckert and Strube (2000) randomly selected five dialogues from the Switchboard corpus. Two were used for training the annotators and the other three for calculating inter-annotator agreement and evaluating their resolution algorithm.", "filtered_refids": [[], [], ["b108", null, "b117"], ["b108", "b33", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4984, "num_references": 6}
{"corpusid_sectionid": "51623319-s27", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Eckert and Strube", "section": "The corpus consists of 8,421 dialogue act units, which roughly correspond to main clauses plus any subordinate clauses. The annotators analyzed personal pronouns (527 instances) and demonstrative pronouns (151 instances). They did not mark first and second-person pronouns, or non-referring pronouns, such as expletives (pleonastic it). Among the annotated pronouns, only 45.1% referred to nominal antecedents and 22.6% to non-NAs. In addition to these two classes, they define two further classes: vague anaphors and inferrable-evoked pronouns. Vague anaphors are pronouns without a clearly defined linguistic antecedent, which, for example, refer to the general discourse topic (13.2% of the pronouns). Inferrable-evoked pronouns are particular uses of plural they without explicit antecedents and whose referent has to be inferred; these are never instances of non-NA anaphors (19.1%). For the classification task (nominal, 37 Schiffman (1985)  576 non-nominal, vague, inferrable-evoked), Eckert and Strube report an inter-annotator agreement of Fleiss' (1971) \u03ba 0.81 for personal pronouns, and \u03ba 0.80 for demonstrative pronouns.\n\nThe subset of anaphors that were classified in the same way by both annotators was further annotated in that both annotators marked the antecedents and then agreed upon a reconciled version of the data. Non-nominal antecedents consisted of only VPs and clausal antecedents. Annotator accuracy for antecedent marking was measured against the reconciled version. For non-NA anaphora, accuracy (percent agreement) of the two annotators was 85.7% (60 correct of 70 cases) and 94.3% (66 correct), respectively (as compared with NA anaphora with accuracies of 96.1% and 98.4%, respectively).\n\nThe annotation procedure is documented in Eckert and Strube (2000), but the annotated corpus is not available. (2001); Botley (2006). The goal of Botley and McEnery (2001) and Botley (2006) was to investigate the use of demonstratives from a corpus linguistics perspective and, in particular, to investigate the different usages of such anaphora across different genres. They examined three corpus samples, each with 100,000 words: the Associated Press corpus (AP) of American newswire texts, the Hansard corpus with proceedings from the Canadian House of Commons, and the American Printing House for the Blind corpus (APHB), a collection of literary works and motivational narrative. 40 They expected that anaphors would function differently in the three corpora. For instance, the Hansard corpus contains spoken data, in particular, exchanges between parliamentarians who refer to each other's arguments as well as their own using non-NA anaphora. Whereas the Hansard sample is a continuous record of one parliamentary session, the AP and APHB samples contain a range of texts dealing with a variety of topics.", "filtered_refids": [[null], [], [null, "b10", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2833, "num_references": 4}
{"corpusid_sectionid": "51623319-s28", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Botley and McEnery", "section": "They considered both demonstrative pronouns and this NPs in their study. In addition to non-NA anaphora, they also investigated instances of pure textual deixis (see Sections 2.1.3 and 2.1.5 for more on their notion of indirect anaphora).\n\nThe three corpus samples contained 403 non-NA demonstratives, of which 57% occurred in the Hansard sample, 29% in the APHB sample, and 14% in the AP sample, providing evidence that spoken data exhibits more instances of non-NA anaphora than written data.\n\nThe corpus was annotated by the authors without any reliability measurement, and the data itself is no longer available (S. Botley, personal communication). The annotations included the type of anaphora (non-NA anaphora, NA anaphora, exophoric), direction of reference (anaphoric, cataphoric), the type of the anaphor (this NP, pronoun, not applicable), 41 and the type of the antecedent (nominal, clausal, propositional/factual, 42 adjectival, none). Instances of non-NA anaphora fall into the classes of clausal and propositional antecedents. Botley and McEnery (2001) perform a series of statistical tests, combining the individual forms (this, that, these, those) with each of the annotated features. Figures on non-NA anaphora cannot easily be extracted from most of the tables, though, 40 http://ucrel.lancs.ac.uk/corpora.html. 41 In Botley and McEnery's (2001) terms: noun modifier (for determiner, within a shell noun phrase) and noun head (for pronoun). 42 These are \"surface statements or utterances made by speakers or writers\" (Botley 2006, page 11), i.e., probably direct speech.\n\nbecause they include instances of nominal indirect anaphora, also called bridging (see footnote 6). (2003). Byron's (2003) primary goal was to compare and contrast the use of personal vs. demonstrative pronouns, and her secondary goal was to compare and contrast their use in two different genres. These investigations were intended to aid in the development of an automated system that could deal with and interpret anaphora in natural language, including non-NA anaphora. She performed two extensive annotation studies, both based on spoken data, which contain a larger number of non-NA instances than written data. The first study was based on the TRAINS93 corpus (Allen and Heeman 1995), 43 which consists of task-oriented spoken dialogues. The dialogues involve two participants, one person who has a certain task to accomplish, involving the routing and scheduling of freight trains, and one person who helps with the planning. The second study used the Boston University Radio Speech corpus (Ostendorf, Price, and Shattuck-Hufnagel 1996) 44 (BUR), comprising transcripts of news stories read over the radio. For the second study, the annotation scheme was modified in various places, wherever the annotators had difficulties in the first annotation task; the annotators were two students without prior knowledge of the topic and the author of the study. For instance, the initial guidelines used the term linguistic antecedent, which turned out difficult to work with. The antecedent was then called linguistic trigger and finally linguistic anchor. The new genre (prepared spoken monologues rather than spontaneous dialogues) also required some adaptation of the guidelines. For instance, the scheme of TRAINS93 only provided two values for grammatical function (subject and nonsubject). The BUR corpus contained more elaborate structures than TRAINS93, so the grammatical function attribute was subdivided into more fine-grained values (subject, direct object, indirect object, object of a preposition, possessive, and other). Similarly, though the initial scheme distinguished between four different values for the syntactic category of the antecedent (NP, pronoun, non-NP, and none), the final scheme had eight values (NP, pronoun, non-NP, none, name, title, noun modifier, and possessive).", "filtered_refids": [[], [], [null], [null, "b13", "b85"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3891, "num_references": 4}
{"corpusid_sectionid": "51623319-s29", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Byron", "section": "Byron focused on referring singular and plural neuter personal and demonstrative pronouns (it, its, itself, them, themselves, they, their, this, that, these, and those). The BUR annotation additionally included he, his, him, himself, she, her, and herself (none of these forms occurred in TRAINS93). For TRAINS93, the annotators were trained on one dialogue, with subsequent corrections of the annotation guidelines. They then analyzed pre-marked referring pronouns in 19 randomly selected dialogues, with a total of 10,420 tokens, among them 347 relevant pronouns. For BUR, they used two monologues for training, and analyzed 35 monologues. The monologues were selected such that each monologue contained at least one demonstrative pronoun, hence, the BUR figures are not representative of the corpus. BUR has a total of 13,415 tokens, among them 380 relevant pronouns.\n\nByron annotated 190 instances of it or its (122 from TRAINS93 corpus and 68 from BUR corpus), and 227 instances of demonstrative pronouns (177 from TRAINS93 and 50 from BUR corpus). In TRAINS93, 11 (7%) of the personal and 29 (32%) of the demonstrative pronouns are instances of non-NA anaphors; in BUR, 5 (2%) of the personal and 23 (46%) of the demonstrative pronouns. Byron's (2003) annotation scheme is based on Schiffman's (1985) annotation scheme (see Section 4.2.1). In particular, she used a variety of syntactic features of Schiffman's scheme: the clause level of the anaphor and the antecedent, their grammatical function, the distance between the anaphor and its antecedent, and the antecedent's syntactic category. In TRAINS93, possible antecedents had to be contained within the same utterance, to prevent the annotators from marking huge blocks of discourse as the antecedent, for example, if reference was made to the plan under discussion. In BUR, in contrast, marking antecedents in a previous paragraph was allowed.\n\nIn addition to Schiffman's features, Byron's (2003) scheme also aims at specifying properties of the anaphor's referent, which is called its semantic antecedent. According to Byron (2003, page 12) \"the semantic antecedent is your gut feeling about what the pronoun is standing in for.\" She suggests, as a test, substituting (an appropriate expression realizing) the semantic antecedent for the pronoun to see if the meaning remains constant. An example from Byron (2003, page 13) involving non-NA anaphora is provided in Example (37). The anaphor and the (linguistic) antecedent are marked as usual, the semantic antecedent of that is the fact or the proposition or the idea that an engine cannot arrive at Bath in time. The (linguistic) antecedent is then a previous mention of the semantic antecedent, possibly using different words. For the (linguistic) antecedent, only those cases were considered where both annotators agreed on the semantic antecedent. The \u03ba value was 0.77 (TRAINS93) and 0.95 (BUR) for personal pronouns, and 0.37 (TRAINS93) and 0.62 (BUR) for demonstratives. Personal pronouns were easier to annotate than demonstratives with respect to these fields because the majority of these instances are usually NA anaphora. (Linguistic) antecedents with demonstratives showed very low agreement in TRAINS93. This is partly because one category dominates ('no antecedent' in 49%), and expected agreement is high, resulting in a low \u03ba value. The corpora are freely available. 46 Detailed documentation of the annotation is provided in Byron (2003).", "filtered_refids": [[], ["b13"], [null, "b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3469, "num_references": 3}
{"corpusid_sectionid": "51623319-s30", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Pradhan et al. (2007):", "section": "OntoNotes. Starting in 2006, the OntoNotes project, with members from BBN Technologies, the University of Colorado, the University of Pennsylvania, and the University of Southern California's Information Sciences Institute, has worked over a period of several years to create a large, multilingual resource called OntoNotes. The corpus comprises various genres (telephone conversations, newswire, newsgroups, broadcast news, broadcast conversation, weblogs) in three languages (English, Chinese, and Arabic). The final release, OntoNotes Release 5.0, was published in 2013 and contains about 1.5 million tokens. The corpus builds on other resources, including the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) 47 and the Proposition Bank (Palmer, Kingsbury, and Gildea 2005), 48 and is richly 45 Byron (2003) uses Scott's \u03c0 (Scott 1955 annotated with syntax and predicate-argument structure, word senses, and coreference. Because to its size and rich annotations, OntoNotes is very popular and has been used in several CoNLL shared tasks.\n\nThe focus of OntoNotes is on NA anaphora. In addition, events are also considered, if they are mentioned again either by a pronominal anaphor or instances of certain event-denoting NPs (e.g., nominals derived from verbs such as growth or contribution). In OntoNotes, only the verbal head of the event-denoting expression that serves as the antecedent is annotated; see Example (38), from Pradhan et al. (2007).\n\n(38) Sales of passenger cars grew 22%. The strong growth followed year-to-year increases.\n\nAccording to Chen, Su, and Tan (2010), Release 2.0 of OntoNotes contained 1,235 anaphora referring to an event, which were 19.97% of all anaphora annotated in the corpus. Among them, 59.35% were instances of event-NP anaphors (like growth), and 40.65% were instances of the pronominal anaphors this (16.9%), that (54.1%), and it (29.0%).\n\nThe OntoNote creators state that they strove to ensure that each annotation layer has a least 90% inter-annotator agreement (Weischedel et al. 2013, page 4). OntoNotes is available via LDC, along with documents describing the annotation guidelines. 49 A similar enterprise is the task of event coreference annotation, which requires identification of co-referring event verbs, as shown in Example (39), taken from Lee et al. (2012).\n\n(39) The New Orleans Saints placed Reggie Bush on the injured list on Wednesday.\n\nSaints put Bush on I.R. (38) and (39) clearly differ from non-NA anaphora as considered in this survey, in that the anaphor is either a verb or the anaphoric noun does not allow for the usual lexico-grammatical patterns of shell nouns (see Section 3.1.2).", "filtered_refids": [["b75", null, "b87", "b110"], ["b105"], [], [], ["b69", null], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2665, "num_references": 7}
{"corpusid_sectionid": "51623319-s32", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "M\u00fcller (2008).", "section": "M\u00fcller deals with the automatic resolution of the pronouns it, this, and that in unrestricted multi-party dialogue as part of a summarization system producing extracts as summaries, where unresolved pronouns often constitute a problem. He aims to create a system that is usable in a real-world setting, so no manual preprocessing (such as filtering non-referring pronouns) is involved. However, the system's input are manually created transcripts with correct spelling. M\u00fcller (2008) used the ICSI Meeting Corpus (Dhillon et al. 2004), 50 which was produced as part of a project called Meeting Recorder. It is a collection of transcripts of group discussions during project meetings involving 3 to 10 speakers, dealing with rather technical topics. For the study, M\u00fcller randomly selected five dialogues dealing with different topics (natural language understanding, the Meeting Recorder project itself, the Internet and networking, signal processing, and robustness for speech recognition). The study had three goals: (a) Examining the distribution of different usages of this, that, and it in this multi-party spoken dialogue domain; (b) automatically detecting non-referential it; and (c) automatically identifying antecedents of referential instances of this, that, and it. With regard to non-NA anaphora, M\u00fcller (2008)  580 It is unclear how many tokens the annotated corpus consists of. It contains at most 150 instances of a subset of non-NA anaphora (depending on the setting; see the following description).\n\nM\u00fcller carried out two annotation experiments. The first focused on classifying instances of it, this, and that into six classes. The main class (normal) contains NA and non-NA anaphors. The other classes cover vague pronouns (those without an identifiable antecedent, referring, e.g., to the general topic), discarded pronouns (if an utterance is abandoned), the pronoun it in specific syntactic constructions (two classes: with extraposition and as a syntactic filler), and one class for other uses. The first two classes, normal and vague, are considered subtypes of referential it, and the other four are considered subtypes of non-referential it.\n\nThe first task was performed by two naive annotators, non-native speakers of English, who annotated all five dialogues. 51 M\u00fcller (2008) only reports agreement results for 1,040 instances of the pronoun it. The annotators achieve a value of \u03ba 0.64 for the main class (normal), containing NA and non-NA it. 52 After the annotation, the annotators created a reconciled gold version.\n\nThe second experiment focused on identifying antecedents of referential instances of this, that, and it from the first experiment, in the form of pronouns, (full) NPs, or VPs. M\u00fcller chose to work with naive annotators, which precluded the use of sophisticated annotation schemes. For NP antecedents, the annotators were instructed to mark the head of the NP plus any premodifiers, but excluding any postmodification. For VP antecedents, they were instructed to mark only the head of the VP. He chose this approach to improve inter-annotator agreement and to make it easier to develop an automated system for this task. The second task involved the two annotators from the first experiment plus two new annotators, who were native speakers of English.\n\nEvaluating agreement concerned two aspects: identifying some text span as an antecedent and linking anaphors to their antecedents. On average, all four annotators agreed on some text span as an antecedent only 27.77% of the time. Antecedents could be the pronouns it, this, or that (occurring in a chain of anaphors), NPs, or verbal heads. In particular, M\u00fcller observed that annotators encountered the most difficulty in identifying NP and verbal-head antecedents: On average, all four annotators agreed on such antecedents only 6.06% of the time. With regard to anaphoric linking, M\u00fcller reports chance-corrected agreement in terms of a variant of Krippendorff's (2004) \u03b1 described in Passonneau (2004). This metric requires all annotations to contain the same set of antecedents. So he computed Krippendorff's \u03b1 on the intersection of antecedents found by all annotators-that is, a rather small set in the case of non-pronominal antecedents. For linking these non-pronominal antecedents, the inter-annotator agreement was in the range of \u03b1 0.70-0.88.\n\nInstead of having the annotators create a reconciled version, M\u00fcller (2008) automatically generates a core data set, based on the four annotations. He creates three different sets, one with all anaphoric links that at least two annotators agreed on (n 2), one with all links from three annotators (n 3), and one with the links from all four annotators (n 4). In all settings, VP antecedents are infrequent: 16.84% of the links (150 instances) with n 2, 12.24% for n 3, and 6.38% for n 4. The drop in the proportion with 51 M\u00fcller analyzed the initial annotations and corrected problems in the annotation scheme. The same annotators annotated the same dialogues again, according to the modified scheme. Inter-annotator agreement refers to the second annotation. 52 M\u00fcller (2008) uses Fleiss' measure (Fleiss 1971), which amounts to Scott's \u03c0 in the case of two annotators. 581 increasing n can be taken to indicate that these cases are more difficult to annotate than other instances of anaphora.\n\nThe annotated corpus is not available. M\u00fcller (2008) contains detailed documentation of the annotation. (2012): This-issue corpus. Kolhatkar and Hirst annotate instances of non-NA anaphora realized by the shell noun phrase this issue. Their goal was to build training and test data that can be used by a machine learning system to resolve instances of this issue. They chose to work with this issue instances from MEDLINE abstracts 53 because (a) the antecedents of this issue are relatively well defined in this domain, (b) the limited context of abstracts restricts the antecedent search space, and (c) issues in MEDLINE abstracts are generally associated with clinical problems in the medical domain, and the extraction of this information would therefore be useful in any biomedical information retrieval system.", "filtered_refids": [["b21", "b80"], [], ["b80"], [], ["b91", "b62"], ["b80", "b28", null], ["b80"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 46, "num_chars": 6174, "num_references": 9}
{"corpusid_sectionid": "51623319-s33", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Kolhatkar and Hirst", "section": "Their corpus contains 183 instances of this modifier* issue (i.e., this followed by optional adjectives plus the noun issue) along with the surrounding context from MEDLINE abstracts. Of these instances, 132 instances were independently annotated by two annotators, a domain expert and a non-expert, and the remaining 51 instances were annotated only by the domain expert. The annotation task was to identify and mark automatically parsed constituents as antecedents, without concern for their syntactic types. The majority of antecedents were non-nominal: clauses (37.9%) or sentences (26.5%) or a sequence of adjacent constituents (18.2%).\n\nExample (40) shows an annotated example from their corpus. The antecedent that avoidance of nitrous oxide . . . is marked for the anaphor with ID=\"2\". The REFERENT TYPE of this antecedent is \"CLAUSE\" and the DIST attribute has the value \"ADJA\" as it lies in the adjacent sentence. The annotator included an EXTRA attribute of type PARAPHRASE in the annotation because the actual referent (which would be whether avoidance of nitrous oxide . . . ) is not explicitly stated in the text.\n\n(40) From this preliminary study with a low statistical power, it appears <ANTECEDENT ID=\"2\">that avoidance of nitrous oxide in one's practice may not affect the outcome in the neurosurgical patients</ANTECEDENT>. Further large systemic trials are needed to address <ANAPHOR ID=\"2\" DET=\"this\" NOUN=\"issue\" REFERENT TYPE=\"CLAUSE\" DIST=\"ADJA\" EXTRA=\"PARAPHRASE:whether avoidance of nitrous oxide in one's practice affects the outcome in the neurosurgical patients\">this issue</ANAPHOR>.\n\nBecause the boundaries of such antecedents are fuzzy, Kolhatkar and Hirst argue that such annotations need an inter-annotator agreement coefficient that goes beyond the match/mismatch binary and incorporates distance between strings more elegantly than Krippendorff's \u03b1 with conventional distance metrics. They use Krippendorff's unitizing \u03b1 (\u03b1 u ; Krippendorff 1995Krippendorff , 2004Krippendorff , 2013 and report an inter-annotator agreement of \u03b1 u 0.86, which is considered to be a strong indicator for reliably annotated data. The corpus and the annotation guidelines are available for non-commercial use. 54 4.2.8 Kolhatkar and colleagues: The ASN and CSN corpora. Hirst (2013a, 2013b), Kolhatkar and Hirst (2014), and Kolhatkar (2015) annotated two relevant 53 MEDLINE is a database of references and abstracts on life sciences and biomedical topics.\n\nhttps://www.nlm.nih.gov/bsd/pmresources.html. 54 https://github.com/kvarada/non-NA_Resources/tree/master/ Kolhatkar-Hirst_2012. 582 corpora: the Anaphoric Shell Nouns (ASN) corpus and the Cataphora-like Shell Nouns (CSN) corpus.\n\nThe ASN corpus. Kolhatkar, Zinsmeister, and Hirst extended the annotation of this issue to other shell nouns in news domain. They created the ASN corpus, which comprises 1,810 anaphoric instances of six shell nouns: fact, reason, issue, decision, question, and possibility from the New York Times corpus (Sandhaus 2008), 55 and their corresponding antecedents. Similar to Kolhatkar and Hirst (2012), they chose shell noun instances following the pattern this modifier* shell noun (this followed by optional adjectives followed by a shell noun). With this corpus, the authors hoped (a) to learn to what extent non-experts can identify non-NAs, and (b) to create a corpus for the evaluation of their computational system that resolves anaphoric shell noun phrases.\n\nFor (a), they chose CrowdFlower 56 as their crowdsourcing platform and annotated 2,323 anaphoric shell noun instances. They divide the annotation task of marking non-NAs into two relatively simple sequential annotation tasks: identifying the sentence containing the antecedent and, given this sentence, identifying the antecedent segment in that sentence. 57 Each instance was annotated by eight annotators. Their final curated corpus contains 1,810 instances of anaphoric shell noun phrases and their antecedents.\n\nThey report a Krippendorff's unitizing \u03b1 of 0.54 and a Krippendorff's \u03b1 of 0.51 and 0.61, using the Jaccard and Dice distance metrics, respectively, indicating only moderate inter-annotator agreement. That said, considering that the crowdsourcing platforms are not designed for comprehensive annotation guidelines and the annotators are nonexperts in the linguistic phenomenon, the results are encouraging, as they suggest that people do have more or less similar intuitions about non-NAs.\n\nThey observed two primary challenges in their annotation experiments. First, the question of \"what to annotate\" as mentioned by Fort, Nazarenko, and Rosset (2012) was not straightforward for ASN antecedents, as identifying the boundaries of the antecedents is more complicated in that case than with ordinary NA anaphora. And second, the notion of a right answer was not well defined for non-NAs, because the boundaries of antecedents are not always clearly delimited.\n\nThey also assess the quality of the crowd annotation on a sample of 300 instances with the help of experts. They asked two judges to rate the acceptability of the crowdsourced answers based on the extent to which they provided the correct interpretation of the corresponding anaphor. They observed that 84.6% of the total instances were acceptable according to both judges.\n\nThe CSN corpus. The CSN corpus consists of about 114,700 cataphora-like instances of the same six shell nouns from the New York Times corpus ( fact, reason, issue, decision, question, and possibility) and their shell content. Kolhatkar, Zinsmeister, and Hirst (2013b) aimed to use supervised machine learning methods to resolve anaphoric shell noun phrases. In order to do that, they started by automatically creating training data using the cataphora-like shell noun constructions from Table 3 in Section 3.1.2 and extracting the shell content using syntactic information. In particular, they extracted instances following seven cataphora-like patterns: N-be-to, N-be-that, N-be-wh, N-to, N-that, N-wh, and N-of, and one anaphora-like pattern, Sub-be-N. Later, Kolhatkar and Hirst (2014) observed that simply using lexico-syntactic patterns creates noisy data. So they used Schmid's (2000) semantic frames to extract the semantic preferences of different shell nouns and incorporated this information when creating automatically labeled training data. To evaluate the quality of their automatically labeled data, they annotated about 100 instances of each of 12 shell nouns (idea, issue, concept, decision, plan, policy, problem, trouble, difficulty, reason, fact, and phenomenon) using crowdsourcing. They observed that three out of five annotators agreed on 1,152 instances. 58 4.2.9 Uryupina et al. (2018): The ARRAU corpus. Uryupina et al. present the second release of the ARRAU corpus. ARRAU's goal is to serve as a corpus for \"the next generation of coreference/anaphora resolution systems.\" The corpus encodes a large variety of linguistic information, so that advanced systems that combine such information with world knowledge can profit from the data.\n\nIn the ARRAU corpus, all NPs are annotated, including non-referring NPs and non-NA anaphors. 59 For both NA and non-NA anaphora, antecedents are also marked. ARRAU builds on other corpora, so that existing annotations can be reused. In particular, ARRAU integrates: r A subcorpus of news texts called RST (around 230,000 tokens), which consists of the subset of the Penn Treebank that was annotated for the RST Discourse Treebank 60 (Carlson, Marcu, and Okurowski 2001). This subcorpus is already annotated with syntax (through the Penn Treebank), rhetorical structure (through the RST Discourse Treebank), and argument structure (through the PropBank). r A subcorpus of spoken dialogues called TRAINS (85,000 tokens), which includes task-oriented dialogues from the TRAINS91 and TRAINS93 corpora. r A subcorpus called GNOME (20,000 tokens), which consists of a subset of the GNOME corpus (Poesio 2000) 61 and includes descriptions of museum objects and brief texts about the artists that produced them, and a selection of pharmaceutical leaflets providing patients with mandatory information about their medications. The corpus is already annotated with discourse units and NA anaphors and their antecedents. r A subcorpus of spoken narratives called PEAR (15,000 tokens), which includes all narratives in English from the Pear Stories project 62 (Chafe 1980). These are narratives by subjects who watched a film involving pears and then recounted its contents. 58 More details on these corpora can be found in Kolhatkar (2015, pages 86, 105) and Kolhatkar, Zinsmeister, and Hirst (2013b). Annotation guidelines are available at https://github.com/kvarada/non-NA_Resources. The corpora are available on request. 59 Especially in dialogues, such expressions can be discontinuous. These cases are also annotated in ARRAU. 60 https://www.isi.edu/~marcu/discourse/Corpora.html. 61 http://cswww.essex.ac.uk/Research/nle/corpora/GNOME/. 62 http://www.linguistics.ucsb.edu/faculty/chafe/pearfilm.htm.", "filtered_refids": [[], [], [], ["b63", "b57", "b60", "b55", null, "b62"], [null], ["b108", "b56", null], [], [], ["b29"], [], ["b115", "b57", "b59", null, "b109"], ["b15", "b59", null, "b93", "b16"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 60, "num_chars": 9079, "num_references": 21}
{"corpusid_sectionid": "51623319-s34", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "584", "section": "ARRAU contains about 350,000 tokens in total. There are 1,633 instances of non-NA anaphora: 631 from RST, 862 from TRAINS, 73 from GNOME, and 67 from PEAR. Even though RST is by far the largest subcorpus, TRAINS contains considerably more instances of non-NA anaphora, showing that non-NA anaphora is especially frequent in spoken data.\n\nThey identify and annotate these cases as follows. When a coder specifies that a referring expression is discourse-old, they ask the coder whether its antecedent was introduced using a phrase or a discourse segment. If the coder selects segment as the type of antecedent, they have to mark a sequence of (predefined) clauses as the antecedent. For the subset of non-NA annotations, Uryupina et al. do not report interannotator agreement.\n\nAll NPs have been manually annotated for a variety of features, including agreement features (gender, number, person), grammatical function (e.g., subj, obj, adjunct, np-mod) and semantic type (person, animate, concrete, organization, space, time, plan, numerical, abstract, unknown) and genericity.\n\nThe annotation of ARRAU is thoroughly documented in Uryupina et al. (2018) and the GNOME annotation guidelines. 63 The ARRAU corpus is publicly available from the LDC. 64 The ARRAU corpus has recently been used for the shared task Resolution of discourse deixis 65 , organized by the NAACL Workshop on Computational Models of Reference, Anaphora, and Coreference (CRAC). (2018): The ParCorFull corpus. Lapshinova-Koltunski, Hardmeier, and Krielke present ParCorFull, a large corpus of parallel texts in English and German, annotated with coreference information. The corpus is intended both as a resource for NLP applications and as a basis for contrastive linguistic research in translation studies. It combines data from three sources: the test sets of two shared tasks (IWSLT 2013 andDiscoMT 2015), composed of TED talks, and news texts selected from the test set of another shared task (WMT 2017). The annotations are partly based on the ParCor corpus (Guillou et al. 2014). 66 The English part of the corpus contains 82,379 tokens, the German part 78,350. In the English subset, there are 468 instances of non-NA anaphora, and in the German subset, there are 444.", "filtered_refids": [[], [], [null], ["b115", null, "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2246, "num_references": 4}
{"corpusid_sectionid": "51623319-s35", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Lapshinova-Koltunski, Hardmeier, and Krielke", "section": "The annotations cover NPs and different kinds of pronouns (personal, demonstrative, relative, and reflexive). In addition, pronominal adverbs are also included in the German subset. Such adverbs are fusions of locative adverbs da, hier, wo ('there, here, where') and certain prepositions, as in damit (literally 'therewith'). They occur rather often in German but are rarely considered in annotation projects. The annotation scheme covers a vast array of anaphora and coreference-related phenomena, including one anaphora, ellipsis, and comparative reference (repeated mentions as in, e.g., the same students). The annotation scheme is based on other coreference guidelines designed for multilingual data. The annotation covers non-NA anaphora, and allows for non-NAs in the form of VPs or (sets of) clauses. With regard to non-NA anaphora, the guidelines (Lapshinova-Koltunski and Hardmeier 2018) are not very detailed, however. The corpus has been annotated by well-trained expert annotators. Lapshinova-Koltunski, Hardmeier, and Krielke (2018) compute inter-annotator agreement on two TED talks, using the mention overlap and entity-based CEAF scores (Luo 2005), and treating one of the annotators as the system output and the other as the reference (\"truth\"). This results for English in F 1 -scores of 80.71% (mentions) and 74.13% (CEAF e ), and, for German, 76.54% (mentions) and 65.88% (CEAF e ), so that English appears to be easier to annotate than German.\n\nParCorFull is freely available, 67 and the repository includes the guidelines.\n\n4.2.11 Further Corpora. We conclude this section by briefly describing further corpora with non-NA annotations, for languages other than English. Ku\u010dov\u00e1 and Haji\u010dov\u00e1 (2004), Nedoluzhko and M\u00edrovsk\u00fd (2012): Prague Dependency Treebank. Nedoluzhko and M\u00edrovsk\u00fd describe the coreference annotation of the Czech Prague Dependency Treebank, 68 which extends the original annotation of Ku\u010dov\u00e1 and Haji\u010dov\u00e1 (2004). The Prague Dependency Treebank consists of 49,431 annotated sentences and about 1.8 million words in total. Coreference is annotated on the tectogrammatical layer, a kind of dependency structure of content words in which the meaning of functional words such as determiners or auxiliary verbs has been integrated into the content nodes. The coreference annotation includes all forms of anaphors (NPs, personal and demonstrative pronouns, etc.). Non-NA and NA anaphora are both marked as coref text if their antecedent cannot be specified by grammatical means in the same sentence. The antecedent (NP, VP, clause, or sentence) is identified in terms of a (sub)tree, whose ID is used to annotate the anaphor. Multi-sentence antecedents are not explicitly marked. In this case the anaphor is just assigned the feature segm. The annotation uses a principle of maximal size of an anaphoric expression, which means that it always includes the whole subtree of an anaphor and antecedent, respectively. This approach avoids many consistency issues that would arise otherwise during annotation. It is difficult to extract statistics of non-NA anaphora from Ku\u010dov\u00e1 and Haji\u010dov\u00e1 (2004) and Nedoluzhko and M\u00edrovsk\u00fd (2012) because they do not differentiate between non-NA anaphora and NA anaphora. A related resource is the parallel Prague Czech-English Dependency Treebank, 69 which consists of the one-million token English Wall Street Journal corpus of the Penn Treebank and its translation into Czech. Parts of the English coreference annotation was derived from the BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein 2005) 70 whereas the Czech annotation was done from scratch (Nedoluzhko et al. 2016). (2010): AnCora. The corpus consists of two data sets, one in Catalan (AnCora-CA) and one in Spanish (AnCora-ES), each with about 500,000 words. 71 The corpus is richly annotated: Besides lemma, part of speech, and syntactic information, the annotation includes various kinds of semantic information-for example, argument structures, thematic roles, semantic verb classes, and WordNet nominal senses. For coreference, all NPs are considered and marked as referential or non-referential. ", "filtered_refids": [[null, "b67"], [], ["b64", null, "b82", "b125"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 4156, "num_references": 6}
{"corpusid_sectionid": "51623319-s37", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "586", "section": "Coreference links cover a range of phenomena, including split antecedents and non-NA anaphora (which they call discourse deixis). They distinguish between three types of non-NA relations: anaphoric reference to the same event-token, to the same eventtype, and to the proposition. 72 Navarretta and Olsen (2008). Navarretta and Olsen studied the equivalents of this, that, and it in Danish (455 instances) and Italian (114 instances) written and spoken data. 73 Their goal was to understand the use of these pronouns as a basis for their automatic processing. They annotated the following properties for each instance: the type of the pronoun, the antecedent, the semantic and syntactic type of the antecedent, and the distance between the anaphor and its antecedent in terms of clauses. Dipper and Zinsmeister (2012). The authors annotated 225 instances of non-NA anaphora in German from the Europarl corpus (Koehn 2005) 74 , concentrating on personal and demonstrative pronoun anaphors. 75 Their idea was to design an annotation procedure that makes the way speakers conceptualize the entities denoted by non-NAs explicit. For that, they suggested linguistic tests that help the annotators to identify the antecedent and determine the semantic types of the anaphors and antecedents. Simonjetz and Roussel (2016). A related project examined the crosslingual behavior of shell nouns, involving the annotation of about 2,140 shell noun instances from the Europarl corpus in English and German. To increase annotation consistency, they concentrated on annotating candidates from a set of 50 predefined shell noun lemmas. In the subcorpus used to calculate inter-annotator agreement, the annotators agreed on the shell noun status of 1,140 of 1,329 instances, and for approximately 65% of the instances that were marked as shell nouns, identical antecedent spans were annotated (the antecedent spans overlap 96% of the time). Besides single non-NAs, they also considered multiple antecedents, nominalized antecedents, and instances of plural shell nouns.", "filtered_refids": [["b54", "b82", "b112", null, "b23"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2050, "num_references": 5}
{"corpusid_sectionid": "51623319-s39", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "587", "section": "Another distinction among these corpora is with respect to the domain, which affects the difficulty level of the annotation. If we assume a spectrum with closed-domain corpora at one end and open-domain corpora at the other, Byron's (2003) corpus and parts of Uryupina et al.'s (2018) ARRAU corpus would be close to the closed-domain end, as TRAINS93 dialogues and the GNOME corpus are closed domain corpora, where the topics or objects of discussion are fixed. On the other end of the spectrum, we have Eckert and Strube's (2000) annotation of a subset of the Switchboard corpus and M\u00fcller's (2008) annotation of unrestricted multi-party dialogues from the ICSI Meeting corpus. Kolhatkar and Hirst's (2012) corpus of MEDLINE abstracts is more varied than the TRAINS93 dialogue and the GNOME corpus, but it is still on the closed-domain side. The corpora of Kolhatkar, Zinsmeister, and Hirst (2013a,b) fall somewhere in the middle of the spectrum.\n\nAnother difference concerns the coverage of the annotation. We see three kinds of approaches to annotate non-NAs: annotating semantic type, annotating representative verbal head antecedents (which act as proxies for clausal linguistic antecedents), and annotating linguistic antecedents.\n\nAnnotating semantic type. Some approaches annotate semantic types, like event or fact. As we discussed in Section 3.2.1, the semantic type of the anaphor is determined by the anaphor's context. However, the semantic type of the antecedent, as determined by the meaning of the antecedent, also plays a role. The two types can differ, a phenomenon referred to as referent coercion.\n\nIn a first attempt, Gundel, Hedberg, and Zacharski (2004) annotated the semantic types of antecedents. They assumed that clauses denote eventualities (either events or states of affairs/situations, depending on whether the predicate is eventive or stative), and VPs denote either activities or states, so determining the antecedent's type was relatively straightforward in these cases. For the anaphor, semantic constraints of the predicate had to be used as cues. This proved difficult for multiple reasons: Either the predicates of the pronouns did not force a single interpretation, there was no suitable term to label the type, or the referent was too vague. They switched to annotating the kind of relation as direct or indirect instead. Byron (2003) also annotated the anaphor's semantic type. In the TRAINS corpus, semantic types are strongly domain-dependent (e.g., a plan or the time taken by an action). The BUR corpus shows a greater variety of types, with types like events or processes. She also tried to label the relation between the antecedent and the anaphor, but the student annotators had too many problems with this task, and so it was abandoned.\n\nAnnotating verbal head antecedents. M\u00fcller's (2008) scheme (as well as Weischedel et al. [2013] and the Prague Dependency Treebank [Nedoluzhko et al. 2016]) marks representative verbal heads for non-NAs, assuming that they act as proxies for clausal or sentential antecedents. This scheme thus provides a degree of flexibility and is able to avoid some problems associated with annotating precise antecedents. However, there are two problems with this scheme. First, the verb gives only partial information about the antecedent and its type. Only marking a verb as the antecedent would not tell us whether we are talking about an event, a concept, or a fact. Moreover, if it is an event, for instance, it is not clear which arguments of the verb should be included in the antecedent. Second, antecedents with multiple verbs or with discontinuous antecedents cannot be expressed effectively with this annotation scheme.\n\nAnnotating linguistic antecedents. Eckert and Strube (2000), Byron (2003), Artstein and Poesio (2006), Kolhatkar and Hirst (2012) and Kolhatkar, Zinsmeister, and Hirst (2013a) mark clausal, sentential, and verbal syntactic constituents. The main issue is the underspecification of such antecedents-all references do not need to be fully specified for successful communication. Recasens (2008) suggests that computational approaches should bear this in mind and that annotation efforts must not insist on setting fixed boundaries in every case.\n\nSeveral researchers point out the difficulties associated with annotating different aspects of this phenomenon, in particular with respect to identifying the precise boundaries of non-NAs. There is no standard way to report inter-annotator agreement for this kind of annotation. Some studies use Krippendorff's \u03b1 with distance metrics such as Dice and Jaccard; others use Krippendorff's unitizing alpha. The agreement numbers in either case are rather low, especially for open domains such as newswire. Some studies report only observed percentage agreement with results in the range of about 0.40-0.55 Vieira et al. (2002); Dipper et al. (2011). 76 Table 4 summarizes prominent annotation efforts in non-NA anaphora. The primary focus of annotation has been on the demonstratives this and that and the personal pronoun it. Most of the studies were carried out as preliminary investigations, and very few corpora are available for reuse. Also, the size of most of the corpora is relatively small for training a machine learning system. In Table 4, we mark publicly available corpora with an asterisk (*).\n\nThe data format and the tool used in the annotation process often have an impact on the design decisions of the annotation schemes or the workflow. The most commonly used annotation tools in non-NA anaphora annotation are: MMAX2 (Strube and M\u00fcller 2003), the AnCoraPipe annotation suite (Bertran et al. 2008), TrEd (\"TreeEditor\") (Pajas and\u0160t\u011bp\u00e1nek 2008), 77 and PALinkA (Or\u0203san 2003). See  for a review of the currently available corpora for anaphora and tools to create such corpora. 78 Although a few projects have attempted to annotate non-NA anaphora in a way that can be useful for the development of computational systems (e.g., the ASN Corpus by Kolhatkar, Zinsmeister, and Hirst and the ARRAU corpus by Uryupina et al.), if we want to see real progress in computational methods for this phenomenon, we will need larger, systematically annotated corpora for benchmarking computational systems. We have created a GitHub repository 79 for documenting all the relevant resources for non-NA anaphora.", "filtered_refids": [["b56", "b13", "b25", "b80", null], [], [], ["b13", "b40"], ["b80", "b126", "b82"], ["b58", "b4", "b56", "b107", "b13", "b25"], ["b22", "b119"], [null, "b114", "b84"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 47, "num_chars": 6362, "num_references": 21}
{"corpusid_sectionid": "51623319-s42", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Work Corpus data Anaphora", "section": "Schiffman ( Botley (2006) and American Printing that: 244, these: 9, those: 1) House for the Blind Gundel, Hedberg, and Santa Barbara Corpus of ( \u2021) 110 personal pronouns ( syntactically represented with a non-NA (albeit not referring to an abstract entity in our sense).\n\n(41) A number is multiplied by 6. This product is increased by 44. The result is 68. Find the number.\n\nWinograd's (1972) SHRDLU system resolves it and that by remembering the last possible event. Furthermore, Winograd's heuristic stipulates that it refers to the event mentioned by the speaker, whereas that can refer to the last event mentioned by anyone.\n\nThough they provide interesting clues, these heuristics do not cover the entire range of the phenomenon and they are unlikely to be particularly effective outside of the considerably restricted domains for which they were conceived. Since then, more sophisticated approaches have been developed that attempt to meet the challenges particular to this task.\n\nWhat makes the resolution of non-NA anaphora particularly challenging with respect to conventional anaphora resolution or coreference resolution is that precisely those features that make those tasks tractable problems are the ones that are missing in this domain. Thus, systems that attempt the resolution of non-NA anaphora must attempt to access semantic or discourse-related information, which is not always easily accessible, in order to make resolution decisions.\n\nWhereas NP coreference algorithms can easily and efficiently select an appropriate set of candidate antecedents, namely, by considering NPs only, this is not possible for non-NA anaphora. The antecedents can have a great variety of syntactic shapes and can be difficult to distinguish from one another, as in Example (2), repeated here. Here, whether may or may not be included in the antecedent and it is unclear to what degree these are different antecedents. Furthermore, the same constituent may represent various semantic entities, partly depending on which anaphor is used to refer to it and on the anaphor's context. It is unclear which of these potential candidates ought to be offered to an algorithm and when, since it would be inefficient to consider all their possible variations proactively.\n\n(2) The municipal council had to decide whether to balance the budget by raising revenue or cutting spending. The council had to come to a resolution by the end of the month. This issue was dividing communities across the country.\n\nSecond, agreement features, such as number and gender, critical to the resolution of nominal anaphora, are generally absent for non-NA anaphora. Rather, the features that are useful for determining the compatibility of anaphors and their non-NAs tend to refer to levels of annotation, such as semantic and discourse structures, that are not easily accessible or generally available.\n\nFinally, existing NP coreference algorithms can also generally depend on there being multiple references to a single entity, in which case each mention offers additional information about the entity being described, which in turn can be useful to resolution algorithms. Though some resources, such as ARRAU (Poesio et al. 2013), 80 include such referential chains, many others do not. However, even where these chains are available, their usefulness for non-NA anaphora is limited by the lack of agreement features and the ability of anaphors to adjust the types of their antecedents (cf. referent coercion in Section 3.2.1, Example (22)). As a result, resolution algorithms for non-NA anaphora generally must consider each instance in isolation.\n\nIn this section, we will present some attempts that have been made to address these particular aspects of non-NA anaphora resolution and discuss their effectiveness as well as the implications this has for future work in this area.", "filtered_refids": [[null, "b10"], [], [], [], [], [], [], [], ["b94"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 3860, "num_references": 3}
{"corpusid_sectionid": "51623319-s43", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Rule-Based Methods", "section": "Whereas the methods used by the historical systems previously mentioned relied on simple heuristics and tightly controlled domains to resolve non-NA anaphora, the systems and approaches described in the following involve more sophisticated algorithms. They incorporate linguistic knowledge about discourse structure and anaphoric reference and offer more detailed evaluations of their performance on naturally occurring data in a variety of domains.\n\n5.1.1 Resolution as Linear Search. The approach of Eckert and Strube (2000) is intended to cover both NA anaphora (which they call individual anaphora), and non-NA anaphora (which they call discourse deixis), in the Switchboard corpus (Godfrey and Holliman 1993). 81 To this end they use a simple discourse model consisting of two lists: individuals (i.e., referents of NA anaphors) are recorded in a list, the S(alience)-list, and ordered according to salience, whereas abstract discourse entities that have been referred to, for instance by non-NA demonstratives, are recorded in the A(bstract)-list. These two lists are incrementally updated as a text is processed. Vendler (1967) and Asher (1993) have previously observed that the context of an anaphoric expression provides valuable information as to the nature of its referent, and by extension the range of possible antecedents. Eckert and Strube (2000) use the similar concept of A(bstract)or I(ndividual)-incompatibility to determine which anaphoric expressions may or may not refer to non-NAs. As mentioned in Section 3.2.1, Eckert and Strube describe a particular anaphor instance as I-incompatible if its context does not allow for reference to an individual or concrete entity. I-incompatible contexts include sentences such as x is true or x is correct. A-incompatibility refers to contexts in which the anaphor may not refer to abstract objects.\n\nIn order to determine the non-nominal antecedent, Eckert and Strube (2000) suggest a context ranking algorithm, which is inspired by Webber's (1991) right-frontier constraint (see Section 3.2.4). The authors work with two units: Initiation and Acknowledgment. Initiations are the dialogue acts that convey semantic content, whereas Acknowledgments do not convey semantic content but have the pragmatic function of signaling that the other participant's utterance has been heard or understood. A single Initiation and the Acknowledgment that follows (if present) jointly constitute a synchronizing unit. The context ranking algorithm effectively carries out a linear search. It first searches for an appropriate antecedent in the A-list and, if there is none, it considers clauses in the same and then in previous Initiations. If an antecedent is found, the algorithm stops and adds the antecedent to the A-list. The algorithm determines a clause or sentence as the antecedent if it is:\n\n(i) in the A-list (with all abstract objects previously referred to anaphorically in the same synchronizing unit-usually empty),\n\n(ii) the clause to the left of the clause containing the anaphor in the same Initiation, ", "filtered_refids": [[], ["b25", null, "b117", "b33", "b5"], ["b124", "b25"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3069, "num_references": 7}
{"corpusid_sectionid": "51623319-s45", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Initiation", "section": "A.52 and that wouldn't do us any good. (SW3241) Eckert and Strube (2000) evaluate their algorithm using three selected Switchboard dialogues, which were annotated by two annotators, as described in Section 4.2.2. Their algorithm had a precision of 63.6% and a recall of 70.0% for the non-NA anaphors in the data. According to the authors, the results show that the algorithm primarily has trouble with the classification of anaphors: If an anaphor was resolved incorrectly, it was usually also classified incorrectly, for example, a non-NA anaphor was incorrectly classified as a NA (individual) anaphor.\n\n5.1.2 PHORA. Byron (2002Byron ( , 2004 describes the design and evaluation of the PHORA algorithm for the resolution of pronominal reference to both NA and non-NA anaphora. Her algorithm constructs a discourse model, as a text is interpreted phrase by phrase. The discourse model involves two lists, one of mentioned entities and one of activated entities.\n\nAll referential NPs (i.e., excluding expletive it) result in discourse entities in the list of mentioned entities. One of these is considered the focus. Though Byron's algorithm leaves the exact method of calculating the focus open, in the implementation that was evaluated, the left-most NP in each clause was considered the focus. Constituents that can act as the antecedents of non-NA anaphors (in PHORA: infinitives, gerunds, entire sentences, subordinate clauses) result in discourse entity (DE) proxies in the list of activated entities. Whereas mentioned discourse entities remain in the model for the duration of the discourse, activated entities only remain in the model for the duration of the following clause. This behavior reflects Webber's (1988) right-frontier constraint. However, once an abstract entity has been referred to anaphorically, it is treated as an ordinary mentioned entity, thus enabling multiple references to a single abstract entity.\n\nSimilar to the approach of Eckert and Strube (2000), the compatibility of a pronoun with a candidate antecedent is determined according to their semantic types, which are inferred from their context. For example, verbs impose semantic restrictions on their arguments: In the sentence Load them into the boxcar, the semantics of load requires its theme argument to be of the semantic type cargo. Certain copular adjectives have a similar function. In the statement that's correct, the entity which is described as being correct must be a proposition.\n\nThe algorithm also provides for a set of referring functions that govern the conversion of discourse entities between types and from proxies to abstract entities. Depending on the entity's syntactic, semantic, and speech act-relevant properties, the entity may be coerced to a certain semantic type as required (cf. the process of referent coercion in Section 3.2.1). For instance, the referring function Proposition() can be applied to a sentential proxy d in an assertion and coerces the proxy to its propositional content Table 5 Referring functions adapted from Byron (2002, page 84). A referring function takes a proxy d as its input and coerces it into an appropriate referent.", "filtered_refids": [["b25"], ["b14", "b12"], ["b123"], ["b25"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3166, "num_references": 6}
{"corpusid_sectionid": "51623319-s48", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Constituent Number Semantic class Specificity Referent Salience", "section": "Engine characterized as a proposition. Table 5 contains a number of such referring functions for different semantic types. When a demonstrative pronoun, such as that, is encountered, the algorithm first attempts to resolve it to one of the activated entities from the previous sentence. This can only be successful if the type of the pronoun (as determined by its predication context) is compatible with that of a potential antecedent. For example, the discourse model after reading the sentence of Example (43a) is shown in Table 6 (from Byron 2002). At that point, the discourse model contains several concrete entities (ENG1, AVON, ORANGES1) as well as two abstract entities, whose semantics is not spelled out; instead, the constituent itself serves as a proxy of its semantics (to get [the] oranges, Engine 1 goes to Avon to get [the] oranges).\n\n(43) a. Engine 1 goes to Avon to get the oranges.\n\nb. That takes two hours.\n\nHere the demonstrative that is resolved as follows. First the predicate complements are examined and checked in the list of predicates. The semantics of take (time) requires an argument of the type event. Following the search order for demonstratives, the algorithm first looks for the activated DEs, using the referring function Event(d). The function will be successful on the proxy DE Engine 1 goes to Avon to get the oranges in (43a), as the verb goes is a tensed eventive verb. Thus the referring function states that that refers to the event of Engine 1 getting to Avon, which takes two hours. In contrast to Eckert and Strube (2000), who present a system design that was not implemented, Byron (2002Byron ( , 2004) presents an implemented system. She evaluates her algorithm using 180 pronoun instances from the TRAINS93 corpus (Allen and Heeman 1995), 82 a subset of the dialogues annotated in the Byron (2003) study 82 http://www.cs.rochester.edu/research/cisd/resources/trains.html. 594 described in Section 4.2.4. The algorithm is compared against a baseline that does not distinguish between personal and demonstrative pronouns and uses salience alone to select antecedents. This baseline resolved 37% of pronouns in the evaluation set correctly. With all system components active, the PHORA system resolved 72% of the pronouns correctly. Byron notes that many of the remaining errors are the result of the linear discourse structure used in the system, which could potentially be improved by detecting embedded dialogues and structural shifts.", "filtered_refids": [["b12"], [], [], ["b14", "b12", "b2", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2484, "num_references": 5}
{"corpusid_sectionid": "51623319-s49", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "A Hybrid Centering", "section": "Theory-Based Discourse Model. Pappuswamy, Jordan, and VanLehn (2005) describe an algorithm that is intended to recognize and resolve non-NA anaphors in computer-mediated tutorial dialogues on physics. The algorithm implements a discourse model that combines elements of Centering Theory (Grosz, Weinstein, and Joshi 1995) of Grosz and Sidner's (1986) theory of discourse structure, and focus stacks (Pfleger, Alexandersson, and Becker 2003) (see Section 3.2.3).\n\nThe main idea of this discourse model is that a given discourse has a focus, which remains the same for a few sentences before shifting to a new entity. The entity that is the focus is normally pronominalized at that point in the discourse; therefore, keeping track of the focus should be useful in the interpretation of pronominal expressions. Pappuswamy, Jordan, and VanLehn (2005) make use of two types of focus structures in their algorithm, a global focus and a local focus. The global focus stack keeps track of topics as they relate a discourse as a whole, and each member of this stack has its own local focus stack, which keeps track of discourse objects (i.e., potential antecedents) pertaining to this topic.\n\nOnce the algorithm has established that a particular mention of this, that, or it requires a non-NA, it will first attempt to replace the pronoun with the previous sentence. If the substitution is \"complete and coherent,\" then the previous sentence is accepted as the antecedent; otherwise, in the case of that and it, the list of utterances in the same discourse segment is searched for a similarly compatible substitution. In the case of this, the algorithm will also check if the discourse center is compatible with the global or local focus, in which case the global or local focus is taken to be the antecedent. Pappuswamy, Jordan, and VanLehn (2005) do not specify how completeness of the substitutions is determined. Coherence is determined based on constraints from Centering Theory and from domain knowledge about the topic. Pappuswamy, Jordan, and VanLehn (2005) did not implement the algorithm. They tested it by hand on 40 referring expressions in their corpus of physics tutorial dialogues. They report that their algorithm resolved 91% of the discourse-deictic anaphors (20 out of 22 cases) successfully. Interestingly, whereas all of the that and it instances were resolved successfully, only 75% of the this instances (for which a slightly different algorithm was used, involving the discourse center and global/local focus) were resolved successfully, suggesting that this strategy is less effective than the one used for that and it.", "filtered_refids": [["b92", "b88", "b34", "b35"], ["b88"], ["b88"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2635, "num_references": 6}
{"corpusid_sectionid": "51623319-s50", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Machine Learning Approaches", "section": "5.2.1 Resolving this, that, and it. The earliest attempt to resolve pronominal anaphors to non-NAs using machine learning methods was that of Strube and M\u00fcller (2003). This approach aimed to resolve anaphoric reference to both NA and non-NA antecedents. Every NP that was not an indefinite NP was considered a potential anaphor, and every NP that preceded a potentially anaphoric NP was considered a potential antecedent, insofar as it was compatible in terms of agreement features. All such pairs were used to train the system to resolve references to nominal antecedents. When the potential anaphor was an instance of it or that, however, candidate antecedents were generated by selecting S and VP constituents from the two sentences preceding the anaphor. In order to approximate the right-frontier constraint (Webber 1991), constituents that were not the last constituent in a given sentence were not considered candidates, because they were considered \"inaccessible\" by the algorithm. 83 The feature set used in Strube and M\u00fcller (2003) is split between agreement-based features that are only intended for NP-coreference and those that are useful for non-NA anaphora. (Table 7 later in this article provides a summary of the features used by Strube and M\u00fcller (2003) as well as those used by the other systems described in this section.) This includes features that record a verb's preference for nominal or nonnominal arguments, which were inspired by Eckert and Strube's (2000) and Byron's (2002) observations regarding the preferences established by a pronoun's predicative context. These preferences are implemented here as a list of verbs together with the frequencies with which they were used with arguments of the NP, VP, or S types. The authors also use a feature intended to capture the importance of a particular antecedent candidate with respect to the dialogue as a whole. This feature is implemented as TF*IDF, comparing word frequencies in specific documents with their frequencies in the complete set of Switchboard dialogues (Godfrey and Holliman 1993). For non-NAs, an average TF*IDF value was calculated based on all of the words in the antecedent. Strube and M\u00fcller (2003) use a decision tree classifier to decide between the potential antecedents for each annotated anaphoric instance. When tested with all pronouns, both those with nominal and those with non-nominal antecedents, the system receives an F 1 -score of 47.42, 56.74 precision, and 40.72 recall. Resolution performance for thirdperson neuter pronouns, the only pronouns with non-NAs, was F 1 19.26, P 40.41, and R 12.64. M\u00fcller ( , 2008 describes an algorithm for the resolution of instances of this, that, and it in a corpus of spoken dialogues (described in Section 4.2.6). Instances of these pronouns are resolved either to NPs or VPs; in the case of VPs, only the verbal head is annotated, such that the verbal head substitutes equally for VP or S antecedents. Candidate antecedents are those that occur within a given temporal distance: 9 seconds for NPs and 7 seconds for VPs. However, VP candidate antecedents are only generated for instances of that or objects of the verb do. M\u00fcller uses these data to train a logistic regression classifier, which decides whether or not a particular anaphor-antecedent pair constitutes a case of anaphoric reference. This approach uses an interesting means of estimating the I-incompatibility (cf. Section 5.1.1) of lemmas using corpus frequency counts. The likelihood that an adjective can be used to modify an abstract entity is calculated as the conditional probability of the adjective to occur with a to-infinitive complement, as in Equation (1). A similar formula estimates the probability of an adjective to occur with a that-sentence complement, which similarly is indicative of abstract entities.\n\nSimilar features encode the likelihood of verbs to appear with sentence complements and the semantic compatibility of anaphors and NP antecedents. The system's best performing configuration receives only an F 1 -score of 12.59 (with P 13.43 and R 11.84) for non-NAs. The author notes, however, that the contribution of the corpusbased probability estimates did not significantly improve the system's overall performance. (Nevertheless, similar features do appear in later systems.) Despite the system's low performance, it is the first to attempt the fully-automatic resolution of non-NA anaphora: Previous systems relied either on the anaphors being pre-selected for resolution or on costly domain-specific knowledge and manual annotations. Chen, Su, and Tan (2010) resolve instances of it, this, and that in the OntoNotes Release 2.0 data set (Hovy et al. 2006) to their verbal antecedents using a ranking support vector machine (SVM) model with a composite kernel. The composite kernel combines a series of positional, lexical, and syntactic features with the output of a convolution tree kernel, which encodes the similarity between two syntactic structures and allows the system to better distinguish antecedent candidates.\n\nTheir approach considers the preceding verbs in the anaphor's sentence, together with the verbs from the previous two sentences as the candidate antecedent set. All of the antecedents in this set are compared pairwise using the SVM model, and the antecedent that wins the greatest number of such comparisons is selected as the anaphor's antecedent, provided that this antecedent's score exceeds a certain threshold. If no antecedent meets this criterion, then the anaphor instance is taken to refer to a nominal antecedent and left unresolved.\n\nThe authors collect two types of negative instances: those antecedents that belong to the candidate set of non-NA anaphors and antecedents from the candidate set of NA anaphors. The intuition is that both types of negative antecedent provide different types of information about the form of non-NAs. However, because this results in an even greater imbalance between positive and negative instances than would ordinarily be present, the authors also use random desampling to balance the training data. Chen, Su, and Tan (2010) evaluate their system using 10-fold cross validation, and their results show that using additional negative instances and balancing the training data both improve performance (primarily by increasing recall). In its best performing configuration, their system has an F 1 -score of 57.9 (P 62.6, R 54.0). Jauhar et al. (2015) build on the approach of , separating the task into two discrete stages: classification and resolution. The classification stage involves a decision as to whether or not a particular instance of this, that, or it refers to a non-NA and is thus a candidate for resolution. If the instance is classified as a positive instance, then in the resolution stage, potential antecedent candidates are considered and a second classifier decides for each antecedent whether or not an anaphoric link is to be established. For both stages, maximum entropy classifiers are used. As in , features that estimate the likelihood of a verb to have a clausal or verbal argument or which approximate I-incompatibility using corpus frequency counts are included (here, from the in-house Google News corpus 84 ), as well as similar features measuring the association strength between a pronoun's parent verb-as determined by dependency parsing-and an antecedent's verbal head, among others. The system's overall performance is 22.2 F 1 -score (P 22.6, R 21.8), as compared with a baseline of F 1 16.5 (P 15.3, R 17.9). The system performs best for the pronoun that (F 1 28.0, compared with F 1 17.1 for this), and the pronoun it proves especially hard to resolve (F 1 2.4). 84 The data set is not freely available.", "filtered_refids": [["b12", "b124", "b25", null, "b80", "b114", "b33"], ["b47", "b17"], [], [null, "b50"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 48, "num_chars": 7758, "num_references": 11}
{"corpusid_sectionid": "51623319-s51", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Resolving Shell", "section": "Nouns. The first study to examine shell nouns (Section 3.1.2) in a computational context, Kolhatkar and Hirst (2012), addressed anaphoric shell noun instances of this issue in the MEDLINE abstracts corpus (the annotations are described in Section 4.2.7). In particular, they built a candidate ranking model to rank all eligible antecedent candidates of this issue in the corpus. The eligible candidates are extracted from the same sentence or the two previous sentences whose type (according to the Penn Treebank tag set) is contained in the set {S, SBAR, NP, SQ, SBARQ}. They also considered infinitive phrases as eligible candidates, which are typically analyzed by automatic parsers such as (S (VP to go to school)). The set of candidate constituents is then expanded (in part to attenuate the effects of parser errors) by also adding \"mixedtype constituents,\" which are created by concatenating NP and VP sister constituents, as shown in Example (44). Here, the correct antecedent is of mixed type; it consists of a NP constituent and its sister VP constituent while the PP part of the parent S node is not part of the antecedent.\n\n(44) (S (PP Given these data) (, ,) (NP decreasing HTD to < or = 5 years) (VP may have a detrimental effect on patients with locally advanced prostate cancer) (. .)) Only a randomized trial will conclusively clarify this issue.\n\nThe candidates are encoded using a series of automatically extracted features (see Table 7 later in this article) and then ranked using SVMs. The system's top-ranked candidate matched the manually annotated candidate in 60.78% of the cases in the system's best-performing configuration, which corresponds to an F 1 -score of 77.92 in terms of token overlaps in system and gold-standard antecedents.\n\nWhereas Kolhatkar and Hirst (2012) used relatively small amounts of manually annotated training data, Kolhatkar, Zinsmeister, and Hirst (2013b) attempted to circumvent this training data bottleneck by extracting training data from instances conforming to structurally determined relations of shell noun phrases and their shell content (see Table 3). 85 An illustration is shown in Example (45), where the shell content (whether animal testing is cruel) given in a copula structure can be easily extracted automatically.\n\n(45) Of course, the central, and probably insoluble, issue is whether animal testing is cruel. (NYT)\n\nKolhatkar, Zinsmeister, and Hirst hypothesize that the shell content of these structurally determined relations bear some degree of similarity to the antecedents of anaphoric shell noun instances. For example, specific syntactic patterns are associated with specific shell nouns (e.g., whether clauses are typically used to express questions and issues). They exploit this insight and use the relatively easy-to-gather information to automatically resolve the harder anaphoric cases. They gather automatically labeled training data using typical shell noun patterns from Table 3 and train an SVM ranker with this training data. They apply these trained models to predict antecedents of harder anaphoric cases.\n\nThe authors evaluated the effectiveness of this approach using crowdsourced annotations: Participants were asked to select the best antecedent candidate for a given anaphoric shell noun instance from among the top ten randomly ordered alternatives 85 See Section 4.2.8. 598 provided by the system. 86 The annotators agreed with the system's top-ranked candidate in between 35% (in the case of decision) and 72% (reason) of the tested instances. Kolhatkar and Hirst (2014) focus, in contrast to the previous two studies, primarily on improving the resolution of cataphora-like shell noun instances. Though the relative reliability of lexico-syntactic patterns was one of the motivations for the approach in Kolhatkar, Zinsmeister, and Hirst (2013b), there remain a number of unclear cases, which the Kolhatkar and Hirst (2014) study attempted to address. The reason for these unclear cases is that shell nouns take different types of one or more semantic arguments, and the problem is identifying the appropriate semantic argument that is the shell content of the shell noun phrase in the given context. For instance, in Examples (46) and (47), the shell nouns are resolved to the postnominal that clause and the copula that clause, respectively. 87 Resolving the shell noun phrase the usual reason in Example (47) involves identifying (a) that reason generally expects two semantic arguments: cause and effect, (b) that the cause argument (and not the effect argument) represents the shell content, and (c) that a particular constituent in the given context is the cause argument. The system implemented in this study integrates a number of preferences, as described in Schmid's (2000) study on the linguistic properties of shell nouns. In order to evaluate their system, the authors compiled a gold standard annotation using crowdsourcing and compared it with a baseline configuration that uses lexico-syntactic patterns (as in Table 3) alone to make resolution decisions. Where the baseline averages 57% accuracy, the system manages either 64% or 69%, depending on whether or not the linguistic clues described by Schmid are used or not, respectively. These clues, such as that reason disprefers the N-clause pattern, are a significant help for nouns such as reason (+24%) and fact (+13%), because these nouns tend to occur in clearly defined syntactic environments. For others, such as difficulty or problem, the clues do not improve the system's performance.", "filtered_refids": [["b56"], [], [], ["b59", null, "b56"], [], [], ["b59", null, "b57", "b109"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 29, "num_chars": 5561, "num_references": 8}
{"corpusid_sectionid": "51623319-s52", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "A Deep Learning", "section": "Approach to Non-NA Anaphora Resolution. Marasovi\u0107 et al. (2017) describe an attempt to resolve both pronouns and shell nouns that refer to non-NAs using an LSTM-based (Long Short-Term Memory) mention-ranking model and an innovative method to generate training data. Because of the requirement of neural methods for large amounts of training data, the authors generate training data by automatically pairing the context of anaphor instances with antecedent constituents.\n\nFor instance, whenever a verb (v) occurs with an embedded sentential constituent S', as in Example (48), the S' node represents an artificial antecedent, and is replaced in the original sentence by an appropriate (yet randomly selected) anaphor, either this, that, or it. The extracted artificial antecedent (49a) is then paired with the resulting sentence with the anaphor (49b) as a training instance (Examples (48) and (49)  The approach posits that there exists some as yet unclear semantic relation between the sentences in Example (49a) and (49b), which the neural network encodes. In order to encode this relation, the model uses a 'Siamese' bidirectional LSTM neural network architecture, whose name refers to the twinned LSTM components working in parallel:\n\nOne of these components is applied to the anaphoric sentence and the other to the antecedent. Each of the representations thus derived are then combined into a single, joint representation of the anaphor-antecedent pair. The system's ultimate resolution decisions are then derived from this joint representation. The authors test the performance of their model on the resolution of both shell nouns and pronominal non-NA anaphora in two settings. In the first setting, they consider shell nouns only and use the anaphoric and cataphoric-like shell noun data sets (ASN and CSN) from Kolhatkar, Zinsmeister, and Hirst (2013a,b) and Kolhatkar and Hirst (2014), described in Sections 4.2.8 and 5.2.2. CSN is used for training, ASN for evaluation. On the ASN data set from Kolhatkar, Zinsmeister, and Hirst (2013b), the authors report significantly improved results, ranking the correct antecedent first in 76.09% (for decision) to 93.14% (for possibility) of the instances (this measure can be thought of as roughly similar to accuracy).\n\nIn the second setting, they use the artificially generated data for training and evaluate their system using data from the ARRAU corpus (Poesio et al. 2013), described in Section 4.2.9, which contains both shell nouns and pronominal non-NA anaphors. In its best configuration, their model ranked the correct antecedent first in 29.06% of the pronominal instances and in 51.89% of the shell noun instances. Table 7 contains an overview of the features used in machine learning algorithms for the resolution of non-NA anaphora. Note that, as the table is intended to give a general picture of the features used in various systems, certain simplifications were necessary: Features that are highly domain-specific or do not appear to be sufficiently useful for the task (e.g., the features removed during feature selection by Jauhar et al. [2015]) are omitted here. The features included in the table have been sorted according to whether they primarily involve information about the anaphor, a candidate antecedent, or the relation between the two. Though the studies do not tend to implement many of these features in precisely the same way, we tried nevertheless to summarize them in a way that allows for useful comparisons across systems. Features relating to \"parents\" imply the use of dependency-parsed data and Table 7 Comparison of features used for the resolution of non-NA anaphora. SM03 Strube and M\u00fcller (2003); M07 M\u00fcller (2007M07 M\u00fcller ( , 2008; CST10 Chen, Su, and Tan (2010); KH12 Kolhatkar and Hirst (2012); JGGR15 Jauhar et al. (2015). ", "filtered_refids": [["b74"], [], ["b59", null, "b57"], ["b94", "b114", "b50", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3826, "num_references": 8}
{"corpusid_sectionid": "15084403-s2", "title": "A Survey of Text Mining Architectures and the UIMA Standard", "date": "2012-05-01", "section_title": "Parallelization and distribution:", "section": "With the increasing amount of unstructured data, it becomes more and more important to provide an architecture that enables parallelization and distribution. It is analyzed, if the architectures provide methods for parallelization and distribution and in which way they support processing units in using these possibilities. 6. Annotation model: A typical Information Retrieval toolchain enriches unstructured data with relevant informations like part of speech tags. It is analyzed in which way the architectures model and store the information, and if efficient and convenient access structures exist. Typical questions are if annotations are stored inline or as stand-off markup, if they are typed to provide formal verification and declaration and if annotation types can be inherited. Furthermore the architecture should allow annotations as fields of other annotations (e.g. for parse trees) as well as indexing mechanisms, iterators and subiterators for given types.\n\n2.1. TIPSTER TIPSTER describes a common architecture developed to provide means for efficient information retrieval and information extraction to government agencies and general research ( (Grishman, 1996), chapter 1.0). TIPSTER was 5 corpus and document based processing iteratively applied not only designed for multilingual applications in a wide range of software and hardware environments, but also introduced the thought of interchangeable modules from different sources. While being defined in an abstract (yet object-oriented) way, the TIPSTER architecture is implemented in a number of programming languages like C, Tcl and Common Lisp. The TIPSTER architecture can be seen as document centric. Each document may be contained in one or more collections and is the atomic unit of information retrieval which is considered as the repository of any extracted data. It is possible to derive documents from other documents, thereby forming logical documents. Any information about the text is given by stand-off annotations. Each annotation can be defined by the system engineer using arbitrary annotation names with arbitrary attributes. Each annotation name has to be defined in a type declaration, which is merely used for documentation, but intended to serve as a base for formal verifications. It is possible to assign each annotation to one or more spans of text in the document. Attributes allow primitive data types as values as well as references to other annotations or documents, thereby allowing even hierarchical structures such as parse trees. Some annotation types and general annotation attributes are predefined according to the Corpus Encoding Standard (CES, (Ide, 1998)) to facilitate the interchangeability of modules and the usage of the architecture.\n\nAnnotations are managed and indexed to ensure efficient access for different use case scenarios. TIPSTER's strength is the sophisticated typed annotation model -adopted by as well UIMA (Ferrucci and Lally, 2004), GATE (Cunningham et al., 2002) and Ellogon (Petasis et al., 2002) -and the integration of existing standards like CES. The main shortcoming is the sparse specification of processing resources. Besides being able to work with the Tipster document model no further characteristics are defined. There is no parameter or resource management by the architecture, and no sophisticated workflow management. This makes interchangeability, a standardized parallelization and distribution of processing and language resources impossible.", "filtered_refids": [[], ["b11", "b12"], [null, "b10", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 3493, "num_references": 5}
{"corpusid_sectionid": "15084403-s4", "title": "A Survey of Text Mining Architectures and the UIMA Standard", "date": "2012-05-01", "section_title": "GATE", "section": "The General Architecture for Text Engineering (GATE, (Cunningham, 2000)) was developed to provide an infrastructure for a wide range of language engineering activities that also considers the prior infrastructural findings of the scientific community. It was originally released 1996 and is today available in version 5. The current version is implemented completely in Java, uses Unicode as default encoding and is also capable of processing audio-visual content. Besides comfortable GUI editing tools, it comprises two central elements (cp. (Cunningham, 2000), chapter 7):\n\n1. The GATE Document Manager (GDM) is implemented according to the TIPSTER specifications about document management. Therefore the core of the manager is given by a collection of documents containing text and annotations, which -similar to Ellogon -can be modified online. With the GDM being the common interface to all processing resources it is also the central data repository. All processing resources obtain the annotated documents from the GDM and return them for later processing steps. Annotations on documents are organized in so-called annotation graphs (Bird et al., 2000). Except the information about start and end node, every annotation defines an identifier, a type declaration and additional attributes. Annotation schemes similar to TIPSTER define common annotations with their attributes (cp. (Bontcheva et al., 2004)). Although one annotation is determined to refer to only one span of text, the architecture offers the possibility to create multiple annotation graphs per document.", "filtered_refids": [["b9"], ["b2", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1577, "num_references": 3}
{"corpusid_sectionid": "15084403-s5", "title": "A Survey of Text Mining Architectures and the UIMA Standard", "date": "2012-05-01", "section_title": "A Collection of REusable", "section": "Objects for Language Engineering (CREOLE) which can be seen as a library of processing resources, language resources and data structures for general usage (cp. (Cunningham, 2000), chapter 7.2). Users can extend the CREOLE objects by own implementations using the CREOLE API and initialize and run them on documents using the GGI or programmatic access. All necessary information for the processing resource (PR) is provided by the GDM in the form of documents with text and annotations and results are written back respectively.\n\nEvery CREOLE component must specify its configuration to facilitate workflow creation, accessibility by the Gate Graphical Interface (GGI) and interchangeability. This metadata comprises parameters as well as pre-and postconditions (in the form of annotations and attributes). It is expressed in XML or by using Java Annotations (Cunningham et al., 2010), which simplifies inheritance of processing resources significantly.\n\nBesides the infrastructural capabilities GATE offers an exhaustive library of GUI tools, data access structures, language resources and import filters for common document formats. The workflow management offers possibilities for conditional processing and collection level processing. Although language resources may be distributed and applications may run on different machines, there is still no sophisticated workflow management allowing iterative, nested or parallel processing (cp. (Bontcheva et al., 2004), (Cunningham et al., 2010)). An impressive feature is the possibility for finite state processing over annotations based on regular expressions. This Java Annotation Pattern Engine (JAPE) operates on given pattern/action rules which define a pattern of annotations and their features in the input document, and a corresponding action to perform if the pattern is matched. Corresponding actions may also include the creation of new annotations or the modification of the matched ones. GATE can be seen as quite exhaustive. Resources are separated and described using metadata that can be composed in workflows. Inheritance of modules is facilitated using Java Annotations, collection level processing is possible and the document model with typed annotations is as well comprehensive as well defined. Shortcomings of GATE are the lack of a sophisticated workflow management (especially with respect to parallelization) and that formal declarations of resources are not analysis aware -neither pre-or postconditions nor parameters can be defined with respect to annotations and attributes. Although many different kinds of resources can be accessed via predefined structures, there is no formal specification for individual resource management. Type inheritance is not possible.", "filtered_refids": [["b9"], ["b8"], ["b8", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2743, "num_references": 4}
{"corpusid_sectionid": "15084403-s6", "title": "A Survey of Text Mining Architectures and the UIMA Standard", "date": "2012-05-01", "section_title": "Heart of Gold", "section": "Heart of Gold (Sch\u00e4fer, 2006) has been developed within several research projects 6 funded by the EU and the German ministry for education and research BMBF and is described as a lightweight and XML-based middleware architecture for shallow and deep processing workflows of NLP components (Sch\u00e4fer, 2008). The main architectural design principle behind Heart of Gold is the use of open XML standoff markup to represent the input and output of all components as it is easily exchangeable and transformable using for example XSLT 7 . Unicode handling is directly given by the XML standard. The core of the architecture is the Module Communication Manager which serves as an interface to the application by getting requests and returning results (cp. (Callmeier et al., 2004)). Internally the manager organizes the workflow of processing resources, the persistence layer and the data exchange between components. Processing resources can be implemented in Java or may be called using XML-RPC -even on remote machines. Workflows are specified using the System Description Language SDL (Krieger, 2003) which covers sequential, parallel and iterative execution. By defining so-called sub-architectures consisting of other modules, SDL also allows nested and cascaded workflows. Analysis results are represented as stand-off annotations in an RMRS-XML format (Copestake et al., 2006). Every input document can be enriched by a collection of annotations, which may also refer to other annotations and collections by the use of unique identifiers. If modules create output in different formats (or two cooperating modules use different annotation schemes), the Heart of Gold architecture supports XSLT transformation to support module communication. XSLT is also utilized to combine and query annotations.\n\nHeart of Gold offers no capabilities for the definition of pre-and postconditions and there is no parameter or resource management. Furthermore conditional workflows are not supported by the architecture. The usage of standard XML formats and transformation techniques makes the architecture however very flexible, although requiring expensive transformation algorithms.", "filtered_refids": [["b6", "b18", "b13", "b5", "b17"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2168, "num_references": 5}
{"corpusid_sectionid": "15084403-s7", "title": "A Survey of Text Mining Architectures and the UIMA Standard", "date": "2012-05-01", "section_title": "Other NLP software", "section": "A widespread and common toolbox is OpenNLP 8 , which considers itself to be \"an umbrella for various open source NLP projects to work with greater awareness and (potentially) greater interoperability\". With respect to this work OpenNLP is of minor importance, as it does not define any infrastructural base, but is just a collection of perhaps even completely different NLP tools.\n\nThe Advanced Language Engineering Platform (ALEP) is neglected here, because its restrictions with respect to operating systems (only Unix) methods and resources prevented it from being used in a large scale. Another toolbox widely used is LingPipe 9 , which sees itself as a \"suite of Java libraries for the linguistic analysis of human language\". The Java based software includes a wide range of machine learning algorithms for classification and clustering like k-means, SVM or Na\u00efve Bayes, but unfortunately does not provide a very sophisticated infrastructure and is only available under a very restrictive license. Other toolboxes and libraries which are freely available for research purpose but does not provide sophisticated infrastructure capabilities beyond simple pipelines are FreeLing (Atserias et al., 2006), MALLET (McCallum, 2002) and NLTK (Loper and Bird, 2002). Another toolkit which provides more complex workflows and stand-off annotations is LinguaStream (Bilhaut and Widl\u00f6cher, 2006).", "filtered_refids": [[], ["b15", "b16", "b1", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1389, "num_references": 4}
{"corpusid_sectionid": "5853439-s0", "title": "Ontologies of Linguistic Annotation: Survey and Perspectives", "date": "2012-05-01", "section_title": "Background", "section": "The heterogeneity of linguistic annotations has been recognized as a key problem limiting the interoperability and reusability of NLP tools and linguistic data collections. In Natural Language Processing, standard architectures such as UiMA (Egner et al., 2007) and GATE (Cunningham, 2002) address the interoperability of linguistic data structures by providing wrappers around existing NLP components that make use of formalisms that represent input and output of these modules in a tool-independent way. While this approach indeed yields interoperable data structures, and thereby establishes structural interoperability, it is limited insofar as the annotations itself, their content and their meaning, are not standardized in the same way. This problem, the establishment of conceptual interoperability between different linguistic annotations, is addressed here. This problem has long been recognized and numerous initiatives have addressed the problem to represent linguistic annotations in an interoperable way. By now, it is generally agreed upon that repositories of linguistic annotation terminology represent a key element in the establishment of conceptual interoperability. With a terminological reference repository, it is possible to abstract from the heterogeneity of annotation schemes: Reference definitions provide an interlingua that allows to map linguistic annotations from annotation scheme A to annotations in accordance with scheme B. Several repositories of linguistic annotation terminology have been developed by the NLP/computational linguistics community (Aguado de Cea et al., 2004) as well as in the field of language documentation/typology (Saulwick et al., 2005), and their continuous application is expected to enhance the consistency of linguistic metadata and annotations. The General Ontology of Linguistic Description (Farrar and Langendoen, 2010, GOLD) and the ISO TC37/SC4 Data Category Registry (Kemps-Snijders et al., 2009, ISOcat) address both communities. At the moment, however, two problems for the practical application of any of these terminology repositories persist:\n\n\u2022 Different communities develop and maintain independent terminology repositories (e.g., GOLD and ISOcat), and these repositories are not always compatible with respect to the definitions they provide, with re-spect to the technologies employed, or with respect to the underlying philosophy. These problems are actively addressed by the GOLD and ISOcat communities, e.g., in the context of the RELISH project (Kemps-Snijders, 2010). The possible integration between GOLD and ISOcat is, however, expected to by a longer process.\n\n\u2022 There is no commonly agreed formalism to link linguistic annotations to terminology repositories. For GOLD, concrete annotations are linked to reference concepts by means of hand-crafted mapping scripts (Simons et al., 2004). For ISOcat, RDF has been suggested as a means of addressing data categories only recently (Windhouwer and Wright, 2012).\n\nThe Ontologies of Linguistic Annotation (OLiA) have been developed to address both problems in order to facilitate the development of applications that take benefit of a welldefined terminological backbone even before the GOLD and ISOcat repositories have converged into a generally accepted reference terminology. The OLiA ontologies introduce an intermediate level of representation between ISOcat, GOLD and other repositories of linguistic reference terminology and are interconnected with these resources, and they provide not only means to formalize reference categories, but also annotation schemes, and the way that these are linked with reference categories.", "filtered_refids": [["b40", "b18", null, "b0", "b17"], [null, "b31"], ["b45", "b52"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 3664, "num_references": 9}
{"corpusid_sectionid": "5853439-s2", "title": "Ontologies of Linguistic Annotation: Survey and Perspectives", "date": "2012-05-01", "section_title": "Linking Annotations with Reference Categories", "section": "The classic approach to link annotations with reference concepts is to specify rules that define a direct mapping (Teufel, 1995). It is, however, not always possible to find a 1:1 mapping. One problem is conceptual overlap: A common noun may occur as a part of a proper name, e.g., German Palais 'baroque-style palace' in Neues Palais lit. 'new palace', a Prussian royal palace in Potsdam/Germany. Palais is thus both a proper noun (in its function), and a common noun (in its form). Such conceptual overlap is sometimes represented with a specialized tag, e.g., in the TIGER scheme (Brants et al., 2004). ISOcat does currently not provide the corresponding hybrid category, so that Palais is to be linked to both properNoun/DC-1371 and commonNoun/DC-1256 if the information carried by the original annotation is to be preserved. Cliticization and fusion are similar in that multiple word classes can be assigned to (different parts of) one token as represented, for example, by the English gonna, that can be annotated in the PennTreebank tagset as both VBG (gerund, going) and TO (to) (Santorini, 1990). A somewhat different problem is the representation of ambiguity: The SUSANNE (Sampson, 1995) tag ICSt applies to English after both as a preposition and as a subordinating conjunction. The corresponding ISOcat category is thus either preposition/DC-1366 or subordinatingConjunction/DC-1393. Without additional disambiguation, ICSt is to be linked to both data categories.\n\nTechnically, such problems can be solved with a 1:n mapping between annotations and reference concepts. Yet, overlap/contraction and ambiguity differ in their underlying meaning: While overlapping/contracted categories are in the intersection ( ) of reference categories, ambiguous categories are are in their join ( ). This difference is relevant for subsequent processing, e.g., to decide whether disambiguation is necessary. A standard mapping approach, however, fails to distinguish or . Being based on a decidable fragment of first-order predicate logic, OWL/DL represents a formalism that supports the necessary operators and flexibility: With reference concepts and annotation concepts are formalized as OWL classes, the linking between them can be represented by rdfs:subClassOf ( ). OWL/DL provides operators such as owl:intersectionOf ( ), owl:unionOf ( ) and owl:complementOf (\u00ac), and it allows to define properties and restrictions on the respective concepts. An OWL/DL-based formalization has the additional advantage that it can employ existing terminological repositories, e.g., GOLD (native OWL/DL) and ISOcat (with an OWL/DL conversion as described by (Chiarcos, 2010a)). GOLD and ISOcat are, however, under development. The efforts to maintain the linking between annotations and the terminological repository can be reduced if another ontology is introduced that mediates between terminological repositories and annotation schemes: If a major revision of the repository occurs, only the linking between the intermediate ontology and the repository is to be revised, but the linking with not every single tagset. Moreover, this intermediate ontology allows linking annotations to multiple terminological repositories at the same time. The OLiA ontologies implement the idea of an architecture of modular OWL/DL ontologies with an ontology mediating between terminological repositories and annotation schemes.", "filtered_refids": [["b1", "b38", "b39", "b50"], ["b12"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 3405, "num_references": 5}
{"corpusid_sectionid": "5853439-s3", "title": "Ontologies of Linguistic Annotation: Survey and Perspectives", "date": "2012-05-01", "section_title": "A Modular Architecture of OWL/DL Ontologies", "section": "The Ontologies of Linguistic Annotations (Chiarcos, 2008) represent a modular architecture of OWL/DL ontologies that formalize several intermediate steps of the mapping between annotations, a 'Reference Model' and existing terminology repositories ('External Reference Models').\n\nThe OLiA ontologies were developed as part of an infrastructure for the sustainable maintenance of linguis-tic resources (Schmidt et al., 2006), and their primary fields of application include the formalization of annotation schemes and concept-based querying over heterogeneously annotated corpora (Rehm et al., 2007;Chiarcos et al., 2008). In the OLiA architecture, four different types of ontologies are distinguished:\n\n\u2022 The OLIA REFERENCE MODEL specifies the common terminology that different annotation schemes can refer to. It is derived from existing repositories of annotation terminology and extended in accordance with the annotation schemes that it was applied to.\n\n\u2022 Multiple OLIA ANNOTATION MODELs formalize annotation schemes and tagsets. Annotation Models are based on the original documentation, so that they provide an interpretation-independent representation of the annotation scheme.\n\n\u2022 For every Annotation Model, a LINKING MODEL defines relationships between concepts/properties in the respective Annotation Model and the Reference Model. Linking Models are interpretations of Annotation Model concepts and properties in terms of the Reference Model.\n\n\u2022 Existing terminology repositories can be integrated as EXTERNAL REFERENCE MODELs, if they are represented in OWL/DL. Then, Linking Models specify relationships between Reference Model concepts and External Reference Model concepts.\n\nThe OLiA Reference Model specifies classes for linguistic categories (e.g., olia:Determiner) and grammatical features (e.g., olia:Accusative), as well as properties that define relations between these (e.g., olia:hasCase). Far from being yet another annotation terminology ontology, the OLiA Reference Model does not introduce its own view on the linguistic world, but rather, it is a derivative of EAGLES (Leech and Wilson, 1996), MULTEXT/East (Erjavec, 2004), and GOLD (Farrar and Langendoen, 2010) that was introduced as a technical means to interpret linguistic annotations with respect to these terminological repositories, and further enriched with information drawn from the annotation schemes it was applied to. Conceptually, Annotation Models differ from the Reference Model in that they include not only concepts and properties, but also individuals: Individuals represent concrete tags, while classes represent abstract concepts similar to those of the Reference Model.", "filtered_refids": [["b11"], ["b43", "b11", "b36"], [], [], [], [], [null, "b20", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2670, "num_references": 7}
{"corpusid_sectionid": "5853439-s6", "title": "Ontologies of Linguistic Annotation: Survey and Perspectives", "date": "2012-05-01", "section_title": "Corpus Linguistics", "section": "Initially, the OLiA ontologies have been intended to serve a documentation function, i.e., as a formal means to specify the semantics of annotation schemes (Schmidt et al., 2006). From the ontologies, dynamic HTML can be generated, 1 and tags in the annotation can be represented as hyperlinks pointing to the corresponding definition . Figure 1 shows a screenshot of the HTML version of 1 http://code.google.com/p/ co-ode-owl-plugins/wiki/OWLDoc the OLiA Annotation Models of the MULTEXT/East morphosyntactic specifications (Chiarcos and Erjavec, 2011).\n\nOLiA has been integrated in corpus query systems, e.g., ANNIS (Chiarcos and G\u00f6tze, 2007), and SPLICR (Rehm et al., 2007), so that corpus queries could be formulated on the basis of Reference Model concepts. It is thus possible to explore corpora with unfamiliar tagsets, e.g., to reduce the initial bias to evaluate their appropriateness for a given problem. Moreover, ontology-based corpus queries allow to abstract from individual annotation schemes and make it possible to query across heterogeneously annotated corpora. Technically, this was implemented as a query preprocessor: Instead of querying for cat=\"NX\" to retrieve noun phrases from the T\u00fcBa-D/Z corpus (Telljohann et al., 2003) or cat=\"NP\" on the NEGRA corpus (Skut et al., 1998, both are corpora of German newspaper text), we can just query for cat in {olia:NounPhrase} and this is then expanded into a disjunction of possible tags (Chiarcos and G\u00f6tze, 2007). Due to the possibility to create extremely large disjunctions (by just querying for top-level concepts), this approach was relatively inefficient and hence not incorporated in the release versions of the systems mentioned above. One alternative would be linking OLiA Annotation Models and annotations in a corpus directly. For reasons of interoperability, this approach should respect existing standards to implement such a linking. Despite on-going efforts of the linguistics and NLP communities to develop such formalisms (Nancy Ide, p.c.), I am not aware of any existing recommendation to interlink corpora in traditional NLP formats directly with terminology repository. However, such a possibility is provided by Semantic Web formalisms, namely RDF/OWL. In recent years, several researchers have developed schemes to convert specific corpora (Burchardt et al., 2008), specific types of annotation (Hellmann, 2010;Rubiera et al., accepted), or generic corpus representation formalisms (Cassidy, 2010;Chiarcos, this vol) to RDF/OWL. With both corpora and terminology repositories represented in RDF, linking them is reduced to an application of the Linked Data Paradigm (Berners-Lee, 2006), i.e., the main function of RDF in the Semantic Web context, and hence, well-supported by RDF-based technologies.\n\nAs a first example application,  developed the TIGER Corpus Navigator, a tool to explore the RDF representation of the TIGER corpus (Brants et al., 2004). Given a user's input, a query is automatically generated and run against the repository. Query generation was performed using ontology-based learning techniques (Lehmann et al., 2011); to represent linguistic annotations, the OLiA ontologies were employed. The TIGER Corpus Navigator was an experimental pilot study focusing on one particular corpus. More recently, OLiA was combined with POWLA (Chiarcos, 2012), a generic formalism to represent corpora in RDF. So far, two corpora have been converted to POWLA, the MASC corpus (Ide et al., 2008), a genre-balanced multi-layer corpus for American English, and the NEGRA corpus (Skut et al., 1998), a German newspaper corpus, 2 and their morphosyntactic and syntactic annotations have been converted to links to the corresponding OLiA Annotation Models. It is thus possible to query these corpora and the OLiA ontologies using the RDF query language SPARQL.", "filtered_refids": [["b43", "b7"], ["b36", "b46", "b27", "b49", null, "b8", "b5", "b3"], ["b46", "b14", "b29", "b1", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3850, "num_references": 15}
{"corpusid_sectionid": "5853439-s7", "title": "Ontologies of Linguistic Annotation: Survey and Perspectives", "date": "2012-05-01", "section_title": "Interoperability", "section": "Linking annotations to terminology repositories is also essential in terms of interoperability of NLP resources. Interoperability involves two core aspects (Ide and Pustejovsky, 2010), structural ('syntactic') interoperability (different resources make use of the same representation formalism), and conceptual ('semantic') interoperability (resources make use of a shared, well-defined vocabulary). OLiA can be used to establish conceptual interoperability between linguistic corpora, in that the same queries can be applied to corpora with different annotation schemes. In a similar vein, Buyko et al. (2008) suggested to employ OLiA in UiMA (Ferrucci and Lally, 2004), a pipeline architecture for NLP. However, similar problems with the integration between different formalisms persist as observed above for corpus queries, e.g., directly integrating ontologies led to a drop in performance because optimizations based on the use of annotation type system cannot be applied any more (but cf. Verspoor et al., 2009). More recently, it has been suggested to develop NLP pipeline systems using Semantic Web technologies, with the objective to integrate the output of NLP tools in ontology-based machine learning algorithms (Hellmann, 2010). Along with other ontologies, that specify units of analysis (NLP Interchange Format, NIF), 3 the OLiA ontologies represent one fundamental part of this framework: Using RDF as a representation format, and RDF-based wrappers around existing NLP components, linguistic annotations are formalized as direct links to the corresponding OLiA Annotation Models. Subsequent processing modules can make use of this information, and also chose the appropriate level of abstraction by working either directly with the Annotation Models (that precisely reflect the information found in the original annotation and its documentation), with the OLiA Reference Model (that involves an interpretation of the original descriptions as defined in the 2 For details and links, see http://purl.org/powla.  5 The word diese 'this' from the example can thus be described in terms of the OLiA Reference Model as olia:DemonstrativeDeterminer, etc. These ontology-based descriptions are comparable across different corpora and/or NLP tools, across different languages, and even across different types of language resources: McCrae et al. (2011) describe the application of the OLiA ontologies to represent grammatical specifications of machine-readable dictionaries, that are thus interoperable with OLiA-linked corpora. Moreover, through the linking with External Reference Models like GOLD and ISOcat, OLiA-linked resources are also conceptually interoperable with resources directly grounded in either GOLD or ISOcat.", "filtered_refids": [["b6", "b34", "b27", "b21", "b24", "b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 2735, "num_references": 6}
{"corpusid_sectionid": "5853439-s8", "title": "Ontologies of Linguistic Annotation: Survey and Perspectives", "date": "2012-05-01", "section_title": "Ontology-Based NLP", "section": "Using Semantic Web formalisms to represent corpora and annotations also provides us with the possibility to develop novel NLP algorithms. RDF/OWL as common representation formalism allows, for example, to integrate analyses of different levels of description, say, syntax and semantics, in order to disambiguate each other (Cimiano and Reyle, 2003). But even within traditional fields of NLP, possible applications can be found, e.g., in ensemble combination architectures: Ensemble combination means that different NLP modules (say, part-of-speech taggers) are applied in parallel, that they produce annotations for one particular phenomenon, and that these annotations are then integrated, e.g., by choosing one possible analysis based on the evaluation of the agreement between the different modules.\n\nIf modules are combined that use different approaches, e.g., different machine learning paradigms, it has been frequently observed that the combination of tools yields an increase in accuracy and robustness (Brill and Wu, 1998;Halteren et al., 2001). So far, however, these approaches were limited to combine tools trained on the same tagset. If tools with different tagsets are combined, we should not only expect an increase in accuracy and robustness, but also an increase in the level of detail: If the ensemble combination has decided to adopt one particular analysis, then, information from another module with more detailed analyses can be merged with this candidate if it is compatible with the favored analysis. Chiarcos (2010b) described such an architecture and tested it with 7 tools for the morphosyntactic analysis of German that used 4 different annotation schemes. Figure 2 shows how the original morphosyntactic annotations were translated into ontological descriptions for the word diese, and Fig. 3 for the corresponding annotations created by the Connexor dependency parser (Tapanainen and J\u00e4rvinen, 1997) and the RFTagger (Schmid and Laws, 2008). Both analyses in Fig. 3 vary with respect to case and with respect to the part-of-speech (olia:Pronoun or olia:Determiner). These conjunctions are transformed into multisets of conjuncts, and conjunct frequencies establish a simple confidence ranking among these. To this ranking, a pruning routine was applied that filtered out every conjunct that was inconsistent with a higher-ranked conjunct. Consistency can be defined within the ontology. The resulting set of conjuncts was then compared against manual annotations. For evaluation, data from three German newspaper corpora was considered, the NEGRA corpus (Skut et al., 1998), the TIGER corpus (Brants et al., 2004) and the Potsdam Commentary Corpus (Stede, 2004): It could be shown that recall 6 monotonically increased with the number of tools combined. Combining part-of-speech annotations from all (7) tools outperformed the best-performing individual tool. 7 This experiment serves as a proof-of-concept implementation for ontology-based ensemble combination approaches; with more elaborate voting techniques, more significant improvements can be expected. Comparable results have been reported for the analysis of an ambiguous particle in Spanish (Pareja-Lora and Aguado de Cea, 2010).\n\nTaken together, both studies support our observation that the ontology-based integration of morphosyntactic analyses enhances both the robustness and the level of detail of morphosyntactic and morphological analyses. 6 An evaluation in terms of precision would be inappropriate, because some NLP annotations are more fine-grained than the original manual annotation. 7 The only exception was RFTagger on the NEGRA corpus, albeit only due to the fact that it was trained on NEGRA. ", "filtered_refids": [["b16"], ["b46", "b1", "b47", "b25", "b48", "b42", "b2"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3702, "num_references": 9}
{"corpusid_sectionid": "215238860-s3", "title": "More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction", "date": "2020-04-07", "section_title": "Statistical Relation Extraction Models", "section": "As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied.\n\nOne typical SRE approach is feature-based methods (Kambhatla, 2004;Zhou et al., 2005;Jiang and Zhai, 2007;Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers.\n\nDue to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004;Bunescu and Mooney, 2005;Zhao and Grishman, 2005;Mooney and Bunescu, 2006;Zhang et al., 2006b,a;Wang, 2008).\n\nThere are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods Yih, 2002, 2004;Sarawagi and Cohen, 2005;Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations.\n\nInspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings Riedel et al., 2013;Gormley et al., 2015). Furthermore, , Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE.\n\nAlthough SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003;Bach and Badaskar, 2007;Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models.", "filtered_refids": [[], ["b132", "b38", "b40", "b1"], ["b60", "b130", "b13", null, "b7", "b99"], [null, "b116", "b79"], ["b46", "b25", "b101", null, "b75"], ["b69", "b1", "b117"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2082, "num_references": 21}
{"corpusid_sectionid": "215238860-s4", "title": "More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction", "date": "2020-04-07", "section_title": "Neural Relation Extraction Models", "section": "Neural relation extraction (NRE) models introduce neural networks to automatically extract semantic features from text. Compared with SRE models, NRE methods can effectively capture textual information and generalize to wider range of data.\n\nStudies in NRE mainly focus on designing and utilizing various network architectures to capture the relational semantics within text, such as recursive neural networks (Socher et al., 2012;Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; (Zhang and Wang, 2015;Nguyen and Grishman, 2015a;Vu et al., 2016; that can better handle long sequential data, graph neural networks (GNNs) Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016;Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010;Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there are also other works integrating syntactic information into NRE models. Xu et al. (2015a) and Xu et al. (2015b) adopt CNNs and RNNs over shortest dependency paths respectively.  propose a recursive neural network based on augmented dependency paths.  and Cai et al. (2016) utilize deep RNNs to make further use of dependency paths. Besides, there are some efforts combining NRE with universal schemas Riedel et al., 2013). Recently, Transformers (Vaswani et al., 2017) and pre-trained language models (Devlin et al., 2019) have also been explored for NRE (Du et al., 2018;Verga et al., 2018; By concisely reviewing the above techniques, we are able to track the development of RE from pattern and statistical methods to neural models. Comparing the performance of state-of-the-art RE models in years (Figure 2), we can see the vast increase since the emergence of NRE, which demonstrates the power of neural methods.", "filtered_refids": [[], ["b107", "b59", "b83", "b17", "b94", "b97", "b91", "b109", "b89", "b134", "b55", "b119", "b75", "b8", "b15", "b111", "b133", "b122", "b48", "b64"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2371, "num_references": 20}
{"corpusid_sectionid": "215238860-s6", "title": "More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction", "date": "2020-04-07", "section_title": "Utilizing More Data", "section": "Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009;Nguyen and Moschitti, 2011;Min et al., 2013). As shown in Figure 3, for any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme.  Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label \"Bill Gates retired from Microsoft\" with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs.\n\nThe existing methods to alleviate the noise problem can be divided into three major approaches:\n\n(1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017;Han et al., 2018b;Zhang et al., 2019a;Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity Wang et al., 2018).\n\n(3) Many methods tend to utilize sophisticated mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks.  incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018;Zeng et al., 2018) and adversarial training (Wu et al., 2017;Wang et al., 2018;Han et al., 2018a) have also been adopted in DS.\n\nThe researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring:\n\n(1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schemes are still similar to the original one in (Mintz et al., 2009), which just covers the case that the entity pairs are mentioned in the same sentences. To achieve better coverage and less noise, exploring better DS schemes for autolabeling data is also valuable.\n\n(2) Inspired by recent work in adopting pretrained language models Wu and He, 2019;Baldini Soares et al., 2019) and active learning (Zheng et al., 2019) for RE, to perform unsupervised or semi-supervised learning for utilizing large-scale unlabeled data as well as using knowledge from KGs and introducing human experts in the loop is also promising.\n\nBesides addressing existing approaches and future directions, we also propose a new DS dataset to advance this field, which will be released once the paper is published. The most used benchmark for DS, NYT-10 (Riedel et al., 2010), suffers from small amount of relations, limited relation domains and extreme long-tail relation performance. To alleviate these drawbacks, we utilize Wikipedia and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) to construct Wiki-Distant in the same way as Riedel et al. (2010). As demonstrated in Table 1, Wiki-Distant covers more relations and possesses more instances, with a more reasonable N/A proportion. Comparison results of state-of-the-art models on these two datasets 2 are shown in Table 2, indicating that Wiki-Distant is more challenging and there is a long way to resolve distantly supervised RE.", "filtered_refids": [["b66", "b57", "b56"], [], ["b100", "b29", "b71", "b125", "b37"], ["b100", "b97", "b4", "b121", "b28", "b20", "b106"], [], ["b57"], ["b105", "b131", "b2"], ["b74", "b96"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3910, "num_references": 21}
{"corpusid_sectionid": "215238860-s7", "title": "More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction", "date": "2020-04-07", "section_title": "Performing More Efficient Learning", "section": "Real-world relation distributions are long-tail: Only the common relations obtain sufficient training instances and most relations have very limited 2 Due to the large size, we do not use any denoise mechanism for BERT, which still achieves the best results.  relational facts and corresponding sentences. We can see the long-tail relation distributions on two DS datasets from Figure 4, where many relations even have less than 10 training instances. This phenomenon calls for models that can learn longtail relations more efficiently. Few-shot learning, which focuses on grasping tasks with only a few training examples, is a good fit for this need.\n\nTo advance this field, Han et al. (2018d) first built a large-scale few-shot relation extraction dataset (FewRel). This benchmark takes the Nway K-shot setting, where models are given N random-sampled new relations, along with K training examples for each relation. With limited information, RE models are required to classify query instances into given relations ( Figure 5).\n\nThe general idea of few-shot models is to train good representations of instances or learn ways of fast adaptation from existing large-scale data, and then transfer to new tasks. There are mainly two ways for handling few-shot learning: (1) Metric learning learns a semantic metric on existing data and classifies queries by comparing them with training examples (Koch et al., 2015;Vinyals et al., 2016;Snell et al., 2017;Baldini Soares et al., 2019).  (2) Meta-learning, also known as \"learning to learn\", aims at grasping the way of parameter initialization and optimization through the experience gained on the meta-train data (Ravi and Larochelle, 2017;Finn et al., 2017;Mishra et al., 2018).\n\nResearchers have made great progress in fewshot RE. However, there remain many challenges that are important for its applications and have not yet been discussed.  propose two problems worth further investigation:\n\n(1) Few-shot domain adaptation studies how few-shot models can transfer across domains . It is argued that in the real-world application, the test domains are typically lacking annotations and could differ vastly from the training domains. Thus, it is crucial to evaluate the transferabilities of fewshot models across domains.\n\n(2) Few-shot none-of-the-above detection is about detecting query instances that do not belong to any of the sampled N relations. In the N -way K-shot setting, it is assumed that all queries express one of the given relations. However, the real case is that most sentences are not related to the relations of our interest. Conventional few-shot models cannot well handle this problem due to the difficulty to form a good representation for the none-of-the-above (NOTA) relation. Therefore, it is crucial to study how to identify NOTA instances.\n\n(3) Besides the above challenges, it is also important to see that, the existing evaluation protocol may over-estimate the progress we made on fewshot RE. Unlike conventional RE tasks, few-shot RE randomly samples N relations for each evaluation episode; in this setting, the number of relations is usually very small (5 or 10) and it is very likely to sample N distinct relations and thus reduce to a very easy classification task. We carry out two simple experiments to show the problems ( Figure 6): (A) We evaluate few-shot models with increasing N and the performance drops drastically with larger relation numbers. Considering that the real-world case contains much more relations, it shows that existing models are still far from being applied. (B) Instead of randomly sampling N relations, we hand-pick 5 relations similar in semantics and evaluate few-shot RE models on them. It is no surprise to observe a sharp decrease in the results, which suggests that existing few-shot models may overfit simple textual cues between relations instead of really understanding the semantics of the context. More details about the experiments are in Appendix A.", "filtered_refids": [[], ["b31"], ["b58", "b82", "b21", "b95", "b42", "b2", "b73"], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 29, "num_chars": 3976, "num_references": 8}
{"corpusid_sectionid": "215238860-s8", "title": "More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction", "date": "2020-04-07", "section_title": "Handling More Complicated Context", "section": "As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences , which should not be neglected.\n\nThere are already some works proposed to extract relations across multiple sentences:\n\n(1) Syntactic methods (Wick et al., 2006;Gerber and Chai, 2010;Swampillai and Stevenson, 2011;Yoshikawa et al., 2011;Quirk and Poon, Jeff Bezos, an American entrepreneur, graduated from Princeton in 1986. Although there are some efforts investing into extracting relations from complicated context (e.g., documents), the current RE models for this challenge are still crude and straightforward. Followings are some directions worth further investigation:\n\n(1) Extracting relations from complicated context is a challenging task requiring reading, memorizing and reasoning for discovering relational facts across multiple sentences. Most of current RE models are still very weak in these abilities.\n\n(2) Besides documents, more forms of context is also worth exploring, such as extracting relational facts across documents, or understanding relational information based on heterogeneous data.\n\n(3) Inspired by Narasimhan et al. (2016), which utilizes search engines for acquiring external information, automatically searching and analysing context for RE may help RE models identify relational facts with more coverage and become practical for daily scenarios.  ", "filtered_refids": [[], [], ["b115", "b103", "b88", null, "b24"], [], [], ["b62"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1672, "num_references": 6}
{"corpusid_sectionid": "215238860-s9", "title": "More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction", "date": "2020-04-07", "section_title": "Orienting More Open Domains", "section": "Most RE systems work within pre-specified relation sets designed by human experts. However, our world undergoes open-ended growth of relations and it is not possible to handle all these emerging relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios.\n\nThere are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007;Fader et al., 2011;Mausam et al., 2012;Del Corro and Gemulla, 2013;Angeli et al., 2015;Stanovsky and Dagan, 2016;Mausam, 2016;Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts.\n\n(2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011);Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006);Elsahar et al. (2017);  cast relation discovery as a clustering task.\n\nThough relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered:\n\n(1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely benefit the applications of Open IE. There are already some preliminary works in this area (Gal\u00e1rraga et al., 2014;Vashishth et al., 2018) and more efforts are needed.\n\n(2) The not applicable (N/A) relation has been  Table 3: Results of state-of-the-arts models on the normal setting, masked-entity (ME) setting and only-entity (OE) setting. We report accuracies of BERT on Wiki80, F-1 scores of BERT on TACRED and AUC of PCNN-ATT on NYT-10 and Wiki-Distant. All models are from the OpenNRE package .\n\nhardly addressed in relation discovery. In previous work, it is usually assumed that the sentence always expresses a relation between the two entities (Marcheggiani and Titov, 2016). However, in the real-world scenario, a large proportion of entity pairs appearing in a sentence do not have a relation, and ignoring them or using simple heuristics to get rid of them may lead to poor results. Thus, it would be of interest to study how to handle these N/A instances in relation discovery.", "filtered_refids": [[], ["b52", "b86", "b12", "b14", "b19", "b53", "b0", "b3"], ["b112", "b81", "b51", "b18"], [], ["b90", "b67", "b22"], [], ["b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2673, "num_references": 16}
{"corpusid_sectionid": "215238860-s13", "title": "More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction", "date": "2020-04-07", "section_title": "RE Datasets towards Special Interests", "section": "There are already many datasets that benefit RE research: For supervised RE, there are MUC (Grishman and Sundheim, 1996), ACE-2005(Ntroduction, 2005, SemEval-2010 Task 8 (Hendrickx et al., 2009), KBP37 (Zhang and Wang, 2015) and TACRED (Zhang et al., 2017); and we have NYT-10 (Riedel et al., 2010), FewRel (Han et al., 2018d) and DocRED  for distant supervision, few-shot and document-level RE respectively.\n\nHowever, there are barely datasets targeting special problems of interest. For example, RE across sentences (e.g., two entities are mentioned in two different sentences) is an important problem, yet there is no specific datasets that can help researchers study it. Though existing document-level RE datasets contain instances of this case, it is hard to analyze the exact performance gain towards this specific aspect. Usually, researchers (1) use handcrafted sub-sets of general datasets or (2) carry out case studies to show the effectiveness of their models in specific problems, which is lacking of convincing and quantitative analysis. Therefore, to further study these problems of great importance in the development of RE, it is necessary for the community to construct well-recognized, well-designed and fine-grained datasets towards special interests.", "filtered_refids": [["b26", "b122", "b32", "b31", null, "b74", "b68", "b128"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1270, "num_references": 8}
{"corpusid_sectionid": "56657817-s2", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Methods", "section": "The most common approach for associating neural network components with linguistic properties is to predict such properties from activations of the neural network. Typically, in this approach a neural network model is trained on some task (say, MT) and its weights are frozen. Then, the trained model is used for generating feature representations for another task by running it on a corpus with linguistic annotations and recording the representations (say, hidden state activations). Another classifier is then used for predicting the property of interest (say, part-of-speech [POS] tags). The performance of this classifier is used for evaluating the quality of the generated representations, and by proxy that of the original model. This kind of approach has been used in numerous papers in recent years; see Table SM1 for references. 5 It is referred to by various names, including ''auxiliary prediction tasks'' (Adi et al., 2017b), ''diagnostic classifiers'' (Veldhoen et al., 2016), and ''probing tasks' ' (Conneau et al., 2018).\n\nAs an example of this approach, let us walk through an application to analyzing syntax in neural machine translation (NMT) by Shi et al. (2016b). In this work, two NMT models were trained on standard parallel data-English\u2192 French and English\u2192German. The trained models (specifically, the encoders) were run on an annotated corpus and their hidden states were used for training a logistic regression classifier that predicts different syntactic properties. The authors concluded that the NMT encoders learn significant syntactic information at both word level and sentence level. They also compared representations at different encoding layers and found that ''local features are somehow preserved in the lower layer whereas more global, abstract information tends to be stored in the upper layer.'' These results demonstrate the kind of insights that the classification analysis may lead to, especially when comparing different models or model components.\n\nOther methods for finding correspondences between parts of the neural network and certain properties include counting how often attention weights agree with a linguistic property like anaphora resolution (Voita et al., 2018) or directly computing correlations between neural network activations and some property; for example, correlating RNN state activations with depth in a syntactic tree (Qian et al., 2016a) or with Melfrequency cepstral coefficient (MFCC) acoustic features (Wu and King, 2016). Such correspondence may also be computed indirectly. For instance, Alishahi et al. (2017) defined an ABX discrimination task to evaluate how a neural model of speech (grounded in vision) encoded phonology. Given phoneme representations from different layers in their model, and three phonemes, A, B, and X, they compared whether the model representation for X is closer to A or B. This discrimination task enabled them to draw conclusions about which layers encoder phonology better, observing that lower layers generally encode more phonological information.", "filtered_refids": [["b64", null], ["b51"], ["b73", null, "b65"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3056, "num_references": 6}
{"corpusid_sectionid": "56657817-s3", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Linguistic Phenomena", "section": "Different kinds of linguistic information have been analyzed, ranging from basic properties like sentence length, word position, word presence, or simple word order, to morphological, syntactic, and semantic information. Phonetic/phonemic information, speaker information, and style and accent information have been studied in neural network models for speech, or in joint audio-visual models. See Table SM1 for references.\n\nWhile it is difficult to synthesize a holistic picture from this diverse body of work, it appears that neural networks are able to learn a substantial amount of information on various linguistic phenomena. These models are especially successful at capturing frequent properties, while some rare properties are more difficult to learn. Linzen et al. (2016), for instance, found that long short-term memory (LSTM) language models are able to capture subject-verb agreement in many common cases, while direct supervision is required for solving harder cases.\n\nAnother theme that emerges in several studies is the hierarchical nature of the learned representations. We have already mentioned such findings regarding NMT (Shi et al., 2016b) and a visually grounded speech model (Alishahi et al., 2017). Hierarchical representations of syntax were also reported to emerge in other RNN models (Blevins et al., 2018).\n\nFinally, a couple of papers discovered that models trained with latent trees perform better on natural language inference (NLI) (Williams et al., 2018;Maillard and Clark, 2018) than ones trained with linguistically annotated trees. Moreover, the trees in these models do not resemble syntactic trees corresponding to known linguistic theories, which casts doubts on the importance of syntax-learning in the underlying neural network. 6", "filtered_refids": [[], ["b36"], [null, "b51"], ["b72", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1771, "num_references": 5}
{"corpusid_sectionid": "56657817-s6", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Visualization", "section": "Visualization is a valuable tool for analyzing neural networks in the language domain and beyond. Early work visualized hidden unit activations in RNNs trained on an artificial language modeling task, and observed how they correspond to certain grammatical relations such as agreement (Elman, 1991). Much recent work has focused on visualizing activations on specific examples in modern neural networks for language (Karpathy et al., 2015;K\u00e1d\u00e1r et al., 2017;Qian et al., 2016a;Liu et al., 2018) and speech (Wu and King, 2016;Nagamine et al., 2015;Wang et al., 2017b). Figure 1 shows an example visualization of a neuron that captures position of words in a sentence. The heatmap uses blue and red colors for negative and positive activation values, respectively, enabling the user to quickly grasp the function of this neuron.\n\nThe attention mechanism that originated in work on NMT (Bahdanau et al., 2014) also lends itself to a natural visualization. The alignments obtained via different attention mechanisms have produced visualizations ranging from tasks like NLI (Rockt\u00e4schel et al., 2016;Yin et al., 2016), summarization (Rush et al., 2015), MT post-editing (Jauregi Unanue et al., 2018), and morphological inflection (Aharoni and Goldberg, 2017) to matching users on social media (Tay et al., 2018). Figure 2 reproduces a visualization of attention alignments from the original work by Bahdanau et al. Here grayscale values correspond to the weight of the attention between words in an English source sentence (columns) and its French translation (rows). As Bahdanau et al. explain, this visualization demonstrates that the NMT model learned a soft alignment between source and target words. Some aspects of word order may also be  Godin et al., 2018). Saliency can also be computed with respect to intermediate values, rather than input features (Ghaeini et al., 2018). 7 An instructive visualization technique is to cluster neural network activations and compare them to some linguistic property. Early work clustered RNN activations, showing that they organize in lexical categories (Elman, 1989(Elman, , 1990. Similar techniques have been followed by others. Recent examples include clustering of sentence embeddings in an RNN encoder trained in a multitask learning scenario (Brunner et al., 2017), and phoneme clusters in a joint audio-visual RNN model (Alishahi et al., 2017).\n\nA few online tools for visualizing neural networks have recently become available. LSTMVis (Strobelt et al., 2018b) visualizes RNN activations, focusing on tracing hidden state dynamics. 8 Seq2Seq-Vis (Strobelt et al., 2018a) visualizes different modules in attention-based seq2seq models, with the goal of examining model decisions and testing alternative decisions. Another tool focused on comparing attention alignments was proposed by Rikters (2018). It also provides translation confidence scores based on the distribution of attention weights. NeuroX (Dalvi et al., 2019b) is a tool for finding and analyzing individual neurons, focusing on machine translation.\n\nEvaluation As in much work on interpretability, evaluating visualization quality is difficult and often limited to qualitative examples. A few notable exceptions report human evaluations of visualization quality. Singh et al. (2018) showed human raters hierarchical clusterings of input words generated by two interpretation methods, and asked them to evaluate which method is more accurate, or in which method they trust more. Others reported human evaluations for attention visualization in conversation modeling (Freeman et al., 2018) and medical code prediction tasks (Mullenbach et al., 2018).\n\nThe availability of open-source tools of the sort described above will hopefully encourage users to utilize visualization in their regular research and development cycle. However, it remains to be seen how useful visualizations turn out to be.", "filtered_refids": [["b70", null, "b38", "b73"], ["b15", "b12", null, "b75", "b42", "b61", "b2"], ["b54", "b55"], ["b53", null, "b3"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 3905, "num_references": 16}
{"corpusid_sectionid": "56657817-s7", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Challenge Sets", "section": "The majority of benchmark datasets in NLP are drawn from text corpora, reflecting a natural frequency distribution of language phenomena. While useful in practice for evaluating system performance in the average case, such datasets may fail to capture a wide range of phenomena. An alternative evaluation framework consists of challenge sets, also known as test suites, which have been used in NLP for a long time (Lehmann et al., 1996), especially for evaluating MT systems (King and Falkedal, 1990;Isahara, 1995;Koh et al., 2001). Lehmann et al. (1996) noted several key properties of test suites: systematicity, control over data, inclusion of negative data, and exhaustivity. They contrasted such datasets with test corpora, ''whose main advantage is that they reflect naturally occurring data.'' This idea underlines much of the work on challenge sets and is echoed in more recent work . For instance, Cooper et al. (1996) constructed a semantic test suite that targets phenomena as diverse as quantifiers, plurals, anaphora, ellipsis, adjectival properties, and so on.\n\nAfter a hiatus of a couple of decades, 9 challenge sets have recently gained renewed popularity in the NLP community. In this section, we include datasets used for evaluating neural network models that diverge from the common averagecase evaluation. Many of them share some of the properties noted by Lehmann et al. (1996), although negative examples (ill-formed data) are typically less utilized. The challenge datasets can be categorized along the following criteria: the task they seek to evaluate, the linguistic phenomena they aim to study, the language(s) they target, their size, their method of construction, and how performance is evaluated. 10 Table SM2 (in the supplementary materials) categorizes many recent challenge sets along these criteria. Below we discuss common trends along these lines.", "filtered_refids": [["b30", null, "b26"], ["b30", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1883, "num_references": 5}
{"corpusid_sectionid": "56657817-s8", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Task", "section": "By far, the most targeted tasks in challenge sets are NLI and MT. This can partly be explained by the popularity of these tasks and the prevalence of neural models proposed for solving them. Perhaps more importantly, tasks like NLI and MT arguably require inferences at various linguistic levels, making the challenge set evaluation especially attractive. Still, other high-level tasks like reading comprehension or question answering have not received as much attention, and may also benefit from the careful construction of challenge sets.\n\nA significant body of work aims to evaluate the quality of embedding models by correlating the similarity they induce on word or sentence pairs with human similarity judgments. Datasets containing such similarity scores are often used 9 One could speculate that their decrease in popularity can be attributed to the rise of large-scale quantitative evaluation of statistical NLP systems. 10 Another typology of evaluation protocols was put forth by Burlot and Yvon (2017). Their criteria are partially overlapping with ours, although they did not provide a comprehensive categorization like the one compiled here. to evaluate word embeddings (Finkelstein et al., 2002;Bruni et al., 2012;Hill et al., 2015, inter alia) or sentence embeddings; see the many shared tasks on semantic textual similarity in SemEval (Cer et al., 2017, and previous editions). Many of these datasets evaluate similarity at a coarse-grained level, but some provide a more fine-grained evaluation of similarity or relatedness. For example, some datasets are dedicated for specific word classes such as verbs (Gerz et al., 2016) or rare words (Luong et al., 2013), or for evaluating compositional knowledge in sentence embeddings (Marelli et al., 2014). Multilingual and cross-lingual versions have also been collected (Leviant and Reichart, 2015;Cer et al., 2017). Although these datasets are widely used, this kind of evaluation has been criticized for its subjectivity and questionable correlation with downstream performance (Faruqui et al., 2016).", "filtered_refids": [[], [null, "b10", "b40", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2068, "num_references": 4}
{"corpusid_sectionid": "56657817-s9", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Linguistic Phenomena", "section": "One of the primary goals of challenge sets is to evaluate models on their ability to handle specific linguistic phenomena. While earlier studies emphasized exhaustivity (Cooper et al., 1996;Lehmann et al., 1996), recent ones tend to focus on a few properties of interest. For example, Sennrich (2017) introduced a challenge set for MT evaluation focusing on five properties: subject-verb agreement, noun phrase agreement, verb-particle constructions, polarity, and transliteration. Slightly more elaborated is an MT challenge set for morphology, including 14 morphological properties (Burlot and Yvon, 2017). See Table SM2 for references to datasets targeting other phenomena.\n\nOther challenge sets cover a more diverse range of linguistic properties, in the spirit of some of the earlier work. For instance, extending the categories in Cooper et al. (1996), the GLUE analysis set for NLI covers more than 30 phenomena in four coarse categories (lexical semantics, predicate-argument structure, logic, and knowledge). In MT evaluation, Burchardt et al. (2017) reported results using a large test suite covering 120 phenomena, partly based on Lehmann et al. (1996). 11 Isabelle et al. (2017) and Isabelle and Kuhn (2018) prepared challenge sets for MT evaluation covering fine-grained phenomena at morpho-syntactic, syntactic, and lexical levels.\n\nGenerally, datasets that are constructed programmatically tend to cover less fine-grained linguistic properties, while manually constructed datasets represent more diverse phenomena.", "filtered_refids": [["b30", null], ["b30", null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1529, "num_references": 4}
{"corpusid_sectionid": "56657817-s12", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Construction Method", "section": "Challenge sets are usually created either programmatically or manually, by handcrafting specific examples. Often, semi-automatic methods are used to compile an initial list of examples that is manually verified by annotators. The specific method also affects the kind of language use and how natural or artificial/synthetic the examples are. We describe here some trends in dataset construction methods in the hope that they may be useful for researchers contemplating new datasets. Several datasets were constructed by modifying or extracting examples from existing datasets. For instance, Sanchez et al. (2018) and Glockner et al. (2018) extracted examples from SNLI (Bowman et al., 2015) and replaced specific words such as hypernyms, synonyms, and antonyms, followed by manual verification. Linzen et al. (2016), on the other hand, extracted examples of subject-verb agreement from raw texts using heuristics, resulting in a large-scale dataset. Gulordava et al. (2018) extended this to other agreement phenomena, but they relied on syntactic information available in treebanks, resulting in a smaller dataset.\n\nSeveral challenge sets utilize existing test suites, either as a direct source of examples (Burchardt et al., 2017) or for searching similar naturally occurring examples . 12 Sennrich (2017) introduced a method for evaluating NMT systems via contrastive translation pairs, where the system is asked to estimate the probability of two candidate translations that are designed to reflect specific linguistic properties. Sennrich generated such pairs programmatically by applying simple heuristics, such as changing gender and number to induce agreement errors, resulting in a large-scale challenge set of close to 100 thousand examples. This framework was extended to evaluate other properties, but often requiring more sophisticated generation methods like using morphological analyzers/ generators (Burlot and Yvon, 2017) or more manual involvement in generation (Bawden et al., 2018) or verification (Rios Gonzales et al., 2017).\n\nFinally, a few studies define templates that capture certain linguistic properties and instantiate them with word lists (Dasgupta et al., 2018;Rudinger et al., 2018;Zhao et al., 2018a). Template-based generation has the advantage of providing more control, for example for obtaining a specific vocabulary distribution, but this comes at the expense of how natural the examples are.", "filtered_refids": [["b14", "b20", "b45", null, "b36"], [null], ["b80", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2429, "num_references": 8}
{"corpusid_sectionid": "56657817-s13", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Evaluation", "section": "Systems are typically evaluated by their performance on the challenge set examples, either with the same metric used for evaluating the system in the first place, or via a proxy, as in the contrastive pairs evaluation of Sennrich (2017). Automatic evaluation metrics are cheap to obtain and can be calculated on a large scale. However, they may miss certain aspects. Thus a few studies report human evaluation on their challenge sets, such as in MT (Isabelle et al., 2017;Burchardt et al., 2017).\n\nWe note here also that judging the quality of a model by its performance on a challenge set can be tricky. Some authors emphasize their wish to test systems on extreme or difficult cases, ''beyond normal operational capacity'' (Naik et al., 2018). However, whether one should expect systems to perform well on specially chosen cases (as opposed to the average case) may depend on one's goals. To put results in perspective, one may compare model performance to human performance on the same task (Gulordava et al., 2018).", "filtered_refids": [["b48", null], [null, "b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1019, "num_references": 4}
{"corpusid_sectionid": "56657817-s15", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Adversary's Knowledge", "section": "Adversarial examples can be generated using access to model parameters, also known as white-box attacks, or without such access, with black-box attacks (Papernot et al., 2016a(Papernot et al., , 2017Narodytska and Kasiviswanathan, 2017;Liu et al., 2017).\n\nWhite-box attacks are difficult to adapt to the text world as they typically require computing gradients with respect to the input, which would be discrete in the text case. One option is to compute gradients with respect to the input word embeddings, and perturb the embeddings. Since this may result in a vector that does not correspond to any word, one could search for the closest word embedding in a given dictionary (Papernot et al., 2016b); Cheng et al. (2018) extended this idea to seq2seq models. Others computed gradients with respect to input word embeddings to identify and rank words to be modified (Samanta and Mehta, 2017;Liang et al., 2018). Ebrahimi et al. (2018b) developed an alternative method by representing text edit operations in vector space (e.g., a binary vector specifying which characters in a word would be changed) and approximating the change in loss with the derivative along this vector.\n\nGiven the difficulty in generating white-box adversarial examples for text, much research has been devoted to black-box examples. Often, the adversarial examples are inspired by text edits that are thought to be natural or commonly generated by humans, such as typos, misspellings, and so 14 These criteria are partly taken from Yuan et al. (2017), where a more elaborate taxonomy is laid out. At present, though, the work on adversarial examples in NLP is more limited than in computer vision, so our criteria will suffice. on (Sakaguchi et al., 2017;Heigold et al., 2018;Belinkov and Bisk, 2018). Gao et al. (2018) defined scoring functions to identify tokens to modify. Their functions do not require access to model internals, but they do require the model prediction score. After identifying the important tokens, they modify characters with common edit operations. Zhao et al. (2018c) used generative adversarial networks (GANs)  to minimize the distance between latent representations of input and adversarial examples, and performed perturbations in latent space. Since the latent representations do not need to come from the attacked model, this is a black-box attack.\n\nFinally, Alzantot et al. (2018) developed an interesting population-based genetic algorithm for crafting adversarial examples for text classification by maintaining a population of modifications of the original sentence and evaluating fitness of modifications at each generation. They do not require access to model parameters, but do use prediction scores. A similar idea was proposed by Kuleshov et al. (2018).", "filtered_refids": [[null, "b39"], [null, "b44", "b35"], ["b43", "b82", null, "b7", "b76"], ["b28"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2770, "num_references": 11}
{"corpusid_sectionid": "56657817-s16", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Attack Specificity", "section": "Adversarial attacks can be classified to targeted vs. non-targeted attacks (Yuan et al., 2017). A targeted attack specifies a specific false class, l , while a nontargeted attack cares only that the predicted class is wrong, l = l. Targeted attacks are more difficult to generate, as they typically require knowledge of model parameters; that is, they are white-box attacks. This might explain why the majority of adversarial examples in NLP are nontargeted (see Table SM3). A few targeted attacks include Liang et al. (2018), which specified a desired class to fool a text classifier, and Chen et al. (2018a), which specified words or captions to generate in an image captioning model. Others targeted specific words to omit, replace, or include when attacking seq2seq models (Cheng et al., 2018;Ebrahimi et al., 2018a).\n\nMethods for generating targeted attacks in NLP could possibly take more inspiration from adversarial attacks in other fields. For instance, in attacking malware detection systems, several studies developed targeted attacks in a blackbox scenario (Yuan et al., 2017). A black-box targeted attack for MT was proposed by Zhao et al. (2018c), who used GANs to search for attacks on Google's MT system after mapping sentences into continuous space with adversarially regularized autoencoders (Zhao et al., 2018b).", "filtered_refids": [[null, "b76", "b35"], ["b81", "b76", "b82"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1331, "num_references": 6}
{"corpusid_sectionid": "56657817-s19", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Coherence and Perturbation Measurement", "section": "In adversarial image examples, it is fairly straightforward to measure the perturbation, either by measuring distance in pixel space, say ||x \u2212 x || under some norm, or with alternative measures that are better correlated with human perception (Rozsa et al., 2016). It is also visually compelling to present an adversarial image with imperceptible difference from its source image.\n\nIn the text domain, measuring distance is not as straightforward, and even small changes to the text may be perceptible by humans. Thus, evaluation of attacks is fairly tricky. Some studies imposed constraints on adversarial examples to have a small number of edit operations (Gao et al., 2018). Others ensured syntactic or semantic coherence in different ways, such as filtering replacements by word similarity or sentence similarity (Alzantot et al., 2018;Kuleshov et al., 2018), or by using synonyms and other word lists (Samanta and Mehta, 2017;Yang et al., 2018). Some reported whether a human can classify the adversarial example correctly (Yang et al., 2018), but this does not indicate how perceptible the changes are. More informative human studies evaluate grammaticality or similarity of the adversarial examples to the original ones (Zhao et al., 2018c;Alzantot et al., 2018). Given the inherent difficulty in generating imperceptible changes in text, more such evaluations are needed.", "filtered_refids": [[null], ["b44", "b28", "b82", null, "b7", "b74"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1380, "num_references": 7}
{"corpusid_sectionid": "56657817-s20", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Explaining Predictions", "section": "Explaining specific predictions is recognized as a desideratum in intereptability work (Lipton, 2016), argued to increase the accountability of machine learning systems (Doshi-Velez et al., 2017). However, explaining why a deep, highly non-linear neural network makes a certain prediction is not trivial. One solution is to ask the model to generate explanations along with its primary prediction (Zaidan et al., 2007;Zhang et al., 2016), 15 but this approach requires manual annotations of explanations, which may be hard to collect.\n\nAn alternative approach is to use parts of the input as explanations. For example, Lei et al. (2016) defined a generator that learns a distribution over text fragments as candidate rationales for justifying predictions, evaluated on sentiment analysis. Alvarez-Melis and Jaakkola (2017) discovered input-output associations in a sequence-to-sequence learning scenario, by perturbing the input and finding the most relevant associations. Gupta and Sch\u00fctze (2018) inspected how information is accumulated in RNNs towards a prediction, and associated peaks in prediction scores with important input segments. As these methods use input segments to explain predictions, they do not shed much light on the internal computations that take place in the network.\n\nAt present, despite the recognized importance for interpretability, our ability to explain predictions of neural networks in NLP is still limited.", "filtered_refids": [["b79", "b77", null, "b37"], ["b22", "b31"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1438, "num_references": 6}
{"corpusid_sectionid": "56657817-s21", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Other Methods", "section": "We briefly mention here several analysis methods that do not fall neatly into the previous sections.\n\nA number of studies evaluated the effect of erasing or masking certain neural network components, such as word embedding dimensions, hidden units, or even full words (Li et al., 2016b;Feng et al., 2018;Khandelwal et al., 2018;Bau et al., 2018). For example, Li et al. (2016b) erased specific dimensions in word embeddings or hidden states and computed the change in probability assigned to different labels. Their experiments revealed interesting differences between word embedding models, where in some models information is more focused in individual dimensions. They also found that information is more distributed in hidden layers than in the input layer, and erased entire words to find important words in a sentiment analysis task.\n\nSeveral studies conducted behavioral experiments to interpret word embeddings by defining intrusion tasks, where humans need to identify an intruder word, chosen based on difference in word embedding dimensions (Murphy et al., 2012;Fyshe et al., 2015;Faruqui et al., 2015). 16 In this kind of work, a word embedding model may be deemed more interpretable if humans are better able to identify the intruding words. Since the evaluation is costly for high-dimensional representations, alternative automatic metrics were considered (Park et al., 2017;Senel et al., 2018).\n\nA long tradition in work on neural networks is to evaluate and analyze their ability to learn different formal languages (Das et al., 1992;Casey, 1996;Gers and Schmidhuber, 2001;Bod\u00e9n and Wiles, 2002;Chalup and Blair, 2003). This trend continues today, with research into modern architectures and what formal languages they can learn (Weiss et al., 2018;Bernardy, 2018;Suzgun et al., 2019), or the formal properties they possess (Chen et al., 2018b).", "filtered_refids": [[], [null, "b34"], [null, "b47", "b4"], [null, "b9", "b58", "b71"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1861, "num_references": 9}
{"corpusid_sectionid": "49564714-s1", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Overview of Linguistic Typology", "section": "There is no consensus on the precise number of languages in the world. For example, Glottolog provides the estimate of 7748 (Hammarstr\u00f6m et al. 2016), while Ethnologue (Lewis, Simons, and Fennig 2016) refers to 7097. 1 This is due to the fact that defining what constitutes a 'language' is in part arbitrary. Mutual intelligibility, which is used as the main criterion for including different language variants under the same label, is gradient in nature. Moreover, social and political factors play a role in the definition of language.\n\nLinguistic Typology is the discipline that studies the variation among the world's languages through their systematic comparison (Comrie 1989;Croft 2003). The comparison is challenging because linguistic categories cannot be pre-defined (Haspelmath 2007). Rather, cross-linguistically significant categories emerge inductively from the comparison of known languages, and are progressively refined with the discovery of new languages. Crucially, the comparison needs to be based on functional criteria, rather than formal criteria. Typologists distinguish between constructions, abstract and universal functions, and strategies, the type of expressions adopted by each language to codify a specific construction (Croft et al. 2017). For instance, the passive voice is considered a strategy that emphasizes the semantic role of patient: languages like Qawasqar (isolate) lack this strategy and use others strategies to express the construction.\n\nThe classification of the strategies in each language is grounded in typological documentation (Bickel 2007, p. 248). Documentation is empirical in nature and involves collecting texts or speech excerpts, and assessing the features of a language based on their analysis. The resulting information is stored in large databases (see \u00a7 4.1) of attributevalues (this pair is henceforth referred to as typological feature), where usually each attribute corresponds to a construction and each value to the most widespread strategy in a specific language.\n\nAnalysis of cross-lingual patterns reveals that cross-lingual variation is bounded and far from random (Greenberg 1966b). Indeed, typological features can be interdependent: the presence of one feature may implicate another (in one direction or both). This sort of generalizations (called statistical universals) are tendencies rather than actual rules (Corbett 2010). For example, if a language (such as Hmong Njua, Hmong-Mien family) has prepositions, then genitive-like modifiers follow their head. If, instead, a language (such as Slavey, Na-Den\u00e9 family) has postpositions, the order of heads and genitive-like modifiers is swapped. However, there are known exceptions: Norwegian (Indo-European) has prepositions but genitives precede nouns. 2 Moreover, some typological features are rare while others are highly frequent. Interestingly, this also means that some languages are intuitively more plausible than others. Implications and frequencies of features are important as they unravel the deeper explanatory factors underlying the patterns of cross-linguistic variation (Dryer 1998).\n\nCross-lingual variation can be found at all levels of linguistic structure. The seminal works on Linguistic Typology were concerned with morpho-syntax, mainly morphological systems (Sapir 2014(Sapir [1921, p. 128) and word order (Greenberg 1966b). This level of analysis deals with the form of meaningful elements (morphemes and words) and their combination, hence it is called structural typology. As an example, consider the alignment of the nominal case system (Dixon 1994): some languages like Nenets (Uralic) use the same case for subjects of both transitive and intransitive verbs, and a different one for objects (nominative-accusative alignment). Other languages like Lezgian (Northeast Caucasian) group together intransitive subjects and objects, and treat transitive subjects differently (ergative-absolutive alignment).\n\nOn the other hand, semantic typology studies languages at the semantic and pragmatic levels. This area was pioneered by anthropologists interested in kinship (d 'Andrade 1995) and colors (Berlin and Kay 1969), and was expanded by studies on lexical classes (Dixon 1977). The main focus of semantic typology has been to categorize languages in terms of concepts (Evans 2011) in the lexicon, in particular with respect to the 1) granularity, 2) division (boundary location), and 3) membership criteria (grouping and dissection). For instance, consider the event expressed by to open (something). It lacks a precise equivalent in languages such as Korean, where similar verbs overlap in meaning only in part (Bowerman and Choi 2001). Moreover, the English verb encodes the resulting state of the event, whereas an equivalent verb in another language such as Spanish (abrir) rather expresses the manner of the event (Talmy 1991). Although variation in the categories is pervasive due to their partly arbitrary nature, it is constrained crosslingually via shared cognitive constraints (Majid et al. 2007).\n\nSimilarities between languages do not always arise from language-internal dynamics but also from external factors. In particular, similarities can be inherited from a common ancestor (genealogical bias) or borrowed by contact with a neighbor (areal bias) (Bakker 2010). Owing to genealogical inheritance, there are features that are widespread within a family but extremely rare elsewhere (e.g. the presence of click phonemes in the Khoisan languages). As an example of geographic percolation, most languages in the Balkan area (Albanian, Bulgarian, Macedonian, Romanian, Torlakian) have developed, even without a common ancestor, a definite article that is put after its noun simply because of their close proximity.\n\nResearch in linguistic typology has sought to disentangle such factors and to integrate them into a single framework aimed at answering the question \"what's where why?\" (Nichols 1992). Language can be viewed as a hybrid biological and cultural system. The two components co-evolved in a twin track, developing partly independently and partly via mutual interaction (Durham 1991). The causes of cross-lingual variation can therefore be studied from two complementary perspectives -from the perspective of functional theories or event-based theories (Bickel 2015). The former theories involve cognitive and communicative principles (internal factors) and account for the origin of variation, while the latter ones emphasize the imitation of patterns found in other languages (external factors) and account for the propagation (or extinction) of typological features (Croft 1995(Croft , 2000.\n\nExamples of functional principles include factors associated with language use, such as the frequency or processing complexity of a pattern (Cristofaro and Ramat 1999). Patterns that are easy or widespread get integrated into the grammar (Haspelmath 1999, inter alia). On the other hand, functional principles allow the speakers to draw similar inferences from similar contexts, leading to locally motivated pathways of diachronic change through the process known as grammaticalization (Greenberg 1966a(Greenberg , 1978Bybee 1988). For instance, in the world's languages (including English) the future tense marker almost always originates from verbs expressing direction, duty, will, or attempt because they imply a future situation.\n\nThe diachronic and gradual origin of the changes in language patterns and the statistical nature of the universals explain why languages do not behave monolithically. Each language can adopt several strategies for a given construction and partly inconsistent semantic categories. In other words, typological patterns tend to be gradient. For instance, the semantics of grammatical and lexical categories can be represented on continuous multi-dimensional maps (Croft and Poole 2008). Bybee and McClelland (2005) have noted how this gradience resembles the patterns learned by connectionist networks (and statistical machine learning algorithms in general). In particular, they argue that such architectures are sensitive to both local (contextual) information and general patterns, as well as to their frequency of use, similarly to natural languages.\n\nTypological documentation is limited by the fact that the evidence available for each language is highly unbalanced and many languages are not even recorded in a written form. 3 However, large typological databases such as WALS (Dryer and Haspelmath 2013) nevertheless have an impressive coverage (syntactic features for up to 1519 languages). Where such information can be usefully integrated in machine learning, it can provide an alternative form of guidance to manual construction of resources that are now largely lacking for low resource languages. We will discuss the existing typological databases and the integration of their features into NLP models in sections 4 and 5.", "filtered_refids": [[null, "b98"], [null, "b34", "b44", "b83"], [null], ["b54", "b38", "b76"], ["b76", "b53", "b139"], ["b52", "b105", "b153", null, "b24", "b61", "b17"], ["b10"], ["b43", "b59", "b21", null, "b117"], ["b26", null, "b75", "b42", "b73"], ["b45", "b25"], ["b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 58, "num_chars": 8939, "num_references": 34}
{"corpusid_sectionid": "49564714-s2", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Overview of Multilingual NLP", "section": "The scarcity of data and resources in many languages represents a major challenge for multilingual NLP. Many state-of-the-art methods rely on supervised learning, hence their performance depends on the availability of manually crafted datasets annotated with linguistic information (e.g., treebanks, parallel corpora) and/or lexical databases (e.g., terminology databases, dictionaries). Although similar resources are available for key tasks in a few well-researched languages, the majority of the world's languages lack them almost entirely. This gap cannot be easily bridged: the creation of linguistic resources is a time-consuming process and requires skilled labor. Furthermore, the immense range of possible tasks and languages makes the aim of a complete coverage unrealistic.\n\nOne solution to this problem explored by the research community abandons the use of annotated resources altogether and instead focuses on unsupervised learning. This class of methods infers probabilistic models of the observations given some latent variables. In other words, it unravels the hidden structures within unlabeled text data. Although these methods have been employed extensively for multilingual applications (Snyder and Barzilay 2008;Vuli\u0107, De Smet, and Moens 2011;Titov and Klementiev 2012, inter alia), their performance tends to lag behind the more linguistically informed supervised learning approaches (T\u00e4ckstr\u00f6m, McDonald, and Nivre 2013). Moreover, they have been rarely combined with typological knowledge. For these reasons, we will not review them in this chapter.\n\nOther promising ways to overcome data scarcity include transferring models or data from resource-rich to resource-poor languages ( \u00a7 3.1) or learning joint models from annotated examples in multiple languages ( \u00a7 3.2) in order to leverage language inter-dependencies. Early approaches of this kind have relied on universal, high-level delexicalized features, such as PoS tags and dependency relations. More recently, however, the incompatibility of (language-specific) lexica has been countered by mapping equivalent words into the same multilingual semantic space through representation learning ( \u00a7 3.3). This has enriched language transfer and multilingual joint modelling with lexicalized features. In this chapter, we provide an overview of these methods, as they constitute the backbone of the typology-savvy algorithms surveyed in \u00a7 5.", "filtered_refids": [[], ["b160", null, "b149", "b145"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2418, "num_references": 4}
{"corpusid_sectionid": "49564714-s3", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Language Transfer", "section": "Linguistic information can be transferred from resource-rich languages to resource-poor languages: these are commonly referred to as source languages and target languages, respectively. Language transfer is challenging as it requires us to match word sequences with different lexica and word orders, or syntactic trees with different (anisomorphic) structures (Ponti et al. 2018a). As a consequence, the information obtained from the source languages typically needs to be adapted, by tailoring it to the properties of the target languages. The methods developed for language transfer include annotation projection, (de)lexicalized model transfer, and translation (Agi\u0107 et al. 2014). We will illustrate them below using dependency parsing as an example.\n\nAnnotation projection was introduced in the seminal work of Yarowsky, Ngai, and Wicentowski (2001) and Hwa et al. (2005). In its original formulation, as illustrated in Figure 1a, a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g. PoS tags and dependency trees) is then projected directly and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Pad\u00f3 and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014;Agi\u0107 et al. 2016) or sets of most likely labels (Khapra et al. 2011;Wisniewski et al. 2014) can be projected instead of single categorical labels. These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or support 'ambiguous learning' on the target language, respectively.\n\nModel transfer instead involves training a model (e.g. a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure  1b. Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (T\u00e4ckstr\u00f6m, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see \u00a7 3.3).\n\nMachine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1c, a source sentence is machine translated into a target language, (Banea et al. 2008) or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translated documents can also be employed to generate multilingual sentence representations, which facilitate language transfer (Zhou, Wan, and Xiao 2016).\n\nSome of these methods are hampered by their resource requirements. In fact, annotation projection and translation need parallel texts to align words and train translation systems, respectively (Agi\u0107, Hovy, and S\u00f8gaard 2015). Moreover, comparisons of state-of-the-art algorithms revealed that model transfer is competitive with machine translation in terms of performance (Conneau et al. 2018). Partly owing to these reasons, typological knowledge has been mostly harnessed in connection with model transfer, as we will discuss in \u00a7 5.2. Moreover, typological features can guide the selection of the best source language to match to a target language for language transfer (Agi\u0107 et al. 2016, inter alia), which benefits all the above-mentioned methods (see \u00a7 5.3).", "filtered_refids": [["b129", "b4"], ["b92", "b88", "b48", "b167", "b172", "b125", "b169", "b3"], ["b150", "b177", "b119", "b173"], ["b11", "b179", "b60"], [null, "b2", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3651, "num_references": 20}
{"corpusid_sectionid": "49564714-s4", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Multilingual Joint Supervised Learning", "section": "NLP models can be learned jointly from the data in multiple languages. In addition to facilitating intrinsically multilingual applications, such as Neural Machine Translation and Information Extraction, this approach often surpasses language-specific monolingual models as it can leverage more (although noisier) data (Ammar et al. 2016, inter alia). This is particularly true in scenarios where either a target or all languages are resource-lean (Khapra et al. 2011) or in code-switching scenarios (Adel, Vu, and Schultz 2013). In fact, Figure 2: In multilingual joint learning, representations can be private or shared across languages. Tied parameters are shown as neurons with identical color. Image adapted from Fang and Cohn (2017). multilingual joint learning improves over pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017). 4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b) or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Malagasy on the left and Polish on the right). Parameter sharing, however, does not necessarily imply parameter identity: it can be enforced by minimizing the distance between parameters (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011;Zhou et al. 2015) in separate language-specific models.\n\nAnother common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016;\u00d6stling and Tiedemann 2017) or neural machine translation tasks (Johnson et al. 2017;Ha, Niehues, and Waibel 2016). Ammar et al. (2016) instead used language vectors as a prior for language identity or typological features. In \u00a7 5.2, we will discuss ways in which typological knowledge is used to balance private and shared neural network components and provide informative input language vectors. In \u00a7 6.3, we will argue that language vectors do not need to be limited to features extracted from typological databases, but also include automatically induced typological information (Malaviya, Neubig, and Littell 2017, see \u00a7 4.3).", "filtered_refids": [["b63", "b90", "b92", "b57", "b56", "b178", "b118", null, "b77", "b171", "b126", "b0"], ["b90", "b79", "b158", "b77", "b123", null, "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2907, "num_references": 19}
{"corpusid_sectionid": "49564714-s5", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Multilingual Representation Learning", "section": "The multilingual algorithms reviewed in \u00a7 3.1 and \u00a7 3.2 are facilitated by dense realvalued vector representations of words, known as multilingual word embeddings (MWEs). These can be learned from corpora and provide pivotal lexical features to several downstream NLP applications. In MWEs, similar words (regardless of the actual language) obtain similar representations. Various methods to generate MWEs have been developed. We follow the classification proposed by Ruder (2018), whereas we refer the reader to Upadhyay et al. (2016) for an empirical comparison.\n\nMonolingual mapping generates independent monolingual representations and subsequently learns a map between a source language and a target language based on a bilingual lexicon (Mikolov, Le, and Sutskever 2013) or in an unsupervised fashion through adversarial networks (Conneau et al. 2017). Alternatively, both spaces can be cast into a new, lower-dimensional space through canonical correlation analysis (CCA) based on dictionaries (Ammar et al. 2016) or word alignments (Guo et al. 2015). Figure 3 shows how equivalent words in the separate semantic spaces of different languages X and Y can be re-orientated through a learned transformation W.\n\nPseudo-cross-lingual approaches merge words with contexts of other languages and generate representations based on this mixed corpus. Substitutions are based on Wiktionary (Xiao and Guo 2014) or machine translation (Gouws and S\u00f8gaard 2015;Duong et al. 2016). Moreover, the mixed corpus can be produced by randomly shuffling words between aligned documents in two languages (Vuli\u0107 and Moens 2015).\n\nCross-lingual training approaches jointly learn embeddings from parallel corpora and enforce cross-lingual constraints. This involves minimizing the distance of the hidden sentence representations of the two languages (Hermann and Blunsom 2014) or decoding one from the other (Lauly, Boulanger, and Larochelle 2013), possibly adding a correlation term to the loss (Chandar et al. 2014).\n\nJoint optimization typically involves learning distinct monolingual embeddings, while enforcing cross-lingual constraints. These can be based on alignment-based translations (Klementiev, Titov, and Bhattarai 2012), cross-lingual word contexts (Luong, Pham, and Manning 2015), the average representations of parallel sentences (Gouws, Bengio, and Corrado 2015), or images (Rotman, Vuli\u0107, and Reichart 2018).\n\nIn this section, we have briefly outlined the most widely used methods in multilingual NLP. Although they offer a solution to data scarcity, cross-lingual variation remains a challenge for transferring knowledge across languages or learning from several languages simultaneously. Typological information offers promising ways to address this problem. In particular, we have noted that it can support model transfer, parameter sharing, and input biasing through language vectors. In the next two sections, we will elaborate on these solutions. In particular, we will review the development of typological information and the specific features which are selected for various NLP tasks ( \u00a7 4). Afterwards, we discuss ways in which these features are integrated in NLP algorithms, for which applications they have been harnessed, and whether they truly benefit system performance ( \u00a7 5).", "filtered_refids": [["b159", "b137"], ["b78", "b110", "b7", "b35"], ["b161", "b72", "b170", "b58"], ["b96", "b27", "b85"], ["b93", "b135", "b71", "b103"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3293, "num_references": 17}
{"corpusid_sectionid": "49564714-s7", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Hand-Crafted Documentation in Typological Databases", "section": "Other databases only cover features at a specific level of linguistic description. For example, both Syntactic Structures of the World's Languages (SSWL) (Collins and Kayne 2009) and AUTOTYP (Bickel et al. 2017) focus on syntax. SSWL features are manually crafted, whereas AUTOTYP features are derived automatically from primary linguistic data using scripts. The Valency Patterns Leipzig (ValPaL) (Hartmann, Haspelmath, and Taylor 2013) provides verbs as attributes and predicate-argument structures as their values. For example, in both Mandinka and Sliammon, the verb to laugh has a valency of 1; in other words, it requires only one mandatory argument, the subject. However, Mandinka requires the subject to precede the verb, whereas Sliammon requires the verb to morphology agree with the subject.\n\nFor phonology, the Phonetics Information Base and Lexicon (PHOIBLE) (Moran, McCloy, and Wright 2014) collates information on segments (binary phonetic features). In the Lyon-Albuquerque Phonological Systems Database (LAPSyD) (Maddieson et al. 2013), attributes are articulatory traits, syllabic structures or tonal systems. Finally, StressTyp2 (Goedemans, Heinz, and der Hulst 2014) deals with stress and accent patterns. For instance, in Koromf\u00e9 each word's first syllable has to be stressed, but not in Cubeo.\n\nOther databases document various aspects of semantics. The World Loanword Database (WOLD) (Haspelmath and Tadmor 2009) documents loanwords by identifying the donor languages and the source words. The Automated Similarity Judgment Program (ASJP) (Wichmann, Holman, and Brown 2016) and the Intercontinental Dictionary Series (IDS) (Key and Comrie 2015) indicate how a meaning is lexicalized across languages: e.g. the concept of WORLD is expressed as mir in Russian, and as\u0101rki\u015bos . i in Tocharian A.\n\nAlthough typological databases store abundant information on many languages, they suffer from shortcomings that limit their usefulness. Perhaps the most significant shortcoming of such resources is their limited coverage. In fact, feature values are missing for most languages in most databases. Other shortcomings are related to feature granularity. In particular, most databases fail to account for feature value variation within each language: they report only majority value rather than the full range of possible values and their corresponding frequencies. For example, although the dominant Adjective Noun word order in Italian is Adjective before Noun, the opposite order is attested as well. This information is often missing from typological databases.\n\nFurther challenges are posed by restricted feature applicability and feature hierarchies. Firstly, some features apply, by definition, only to subsets of languages that share another feature value. For instance, WALS feature 113A documents \"Symmetric and Asymmetric Standard Negation\", whereas WALS feature 114A \"Subtypes of Asymmetric Standard Negation\". Although a special NA value is assigned for symmetric-negation languages in the latter, there are cases where languages without the prerequisite feature are simply omitted from the sample. Secondly, features can be partially redundant, and subsume other features. For instance, WALS feature 81A \"Order of Subject, Object and Verb\" encodes the same information as WALS feature 82A \"Order of Subject and Verb\" and 83A \"Order of Object and Verb\", with the addition of the order of Subject and Object.", "filtered_refids": [[null, "b22", "b32"], ["b104"], ["b168", "b84"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 3433, "num_references": 6}
{"corpusid_sectionid": "49564714-s8", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Feature Selection from Typological Databases", "section": "The databases presented above can serve as a rich source of typological information for NLP. In this section, we survey the feature sets that have been extracted from these databases in typologically informed NLP studies. In \u00a7 5.4, we review in which ways and to which degree of success these features have been integrated in machine learning algorithms.\n\nMost NLP work incorporated a subset of word order features from WALS (Dryer and Haspelmath 2013), mostly for the task of syntactic dependency parsing, where word order provides crucial guidance. The feature subsets used in different studies are shown in Figure 5. As depicted in the figure, these studies employed quite different word order features. The feature set first established by Naseem, Barzilay, and Globerson (2012) served as inspiration for subsequent works. However, it was also adjusted according to the desired experimental setup. For instance, features with the same value for all the languages in the dataset employed in the experiment were discarded.\n\nAnother group of studies used more comprehensive feature sets. The feature set of Daiber, Stanojevi\u0107, and Sima'an (2016) included not only WALS word order features but also nominal categories (e.g. 'Conjunctions and Universal Quantifiers') and nominal syntax (e.g. 'Possessive Classification'). Berzak, Reichart, and Katz (2015) considered all features from WALS associated with morphosyntax and pruned out the redundant ones, resulting in a total of 119 features. S\u00f8gaard and Wulff (2012) (Littel, Mortensen, and Levin 2016). These features encoded the presence of single segments, classes of segments, minimal contrasts in a language inventory, and the number of segments in a class. For instance, they record whether a language allows two sounds to differ only in voicing, such as /t/ and /d/.\n\nFinally, a small number of experiments adopted the entire feature inventory of typological databases, without any sort of pre-selection. In particular Agi\u0107 (2017) and Ammar et al. (2016) extracted all the features in WALS, while (Deri and Knight 2016) all the features in URIEL. Schone and Jurafsky (2001) did not resort to basic typological features, but rather to \"several hundred [implicational universals] applicable to syntax\" drawn from the Universal Archive (Plank and Filiminova 1996).\n\nTypological attributes that are extracted from typological databases are typically represented as feature vectors in which each dimension encodes a feature value. This feature representation is often binarized (Georgi, Xia, and Lewis 2010): for each possible value v of each database attribute a, a new feature is created with value 1 if it corresponds to the actual value for a specific language and 0 otherwise. Note that this increases the number of features by a factor of 1 ||a|| ||a|| i=1 ||v a i ||. Although binarization helps harmonizing different features and different databases, it overshadows the different natures of typological variables.\n\nTo what extent do the limitations of typological databases mentioned in \u00a7 4.1 affect the feature sets surveyed in this section? The coverage is generally broad for the languages used in these experiments, as they tend to be well-documented. For instance, on average 79.8% of the feature values are populated for the 14 languages appearing in Berzak, Reichart, and Katz (2015), as opposed to 17 percent for all the languages in WALS.\n\nThe quality of the information contained in the feature sets is hard to assess at a large scale. However, as a proxy, we can examine how informative they are. For instance, are they redundant in comparison to other language properties, such as genealogy? In Figure 6, we show three feature sets appearing in Ammar et al. (2016), each depicted as a  heatmap. Each row represents a language in the data, each cell is colored according to the feature value, ranging from 0 to 1. In particular, the feature set of Figure 6a is simply a one-hot language encoding; Figure 6b is the subset of word order features listed in Figure 5; and Figure 6c is a large set of WALS features where values are averaged by language genus to fill in missing values. In order to compare the similarities of the typological feature vectors among languages, we clustered languages hierarchically based on such vectors. 5 Intuitively, the more this hierarchy resembles their actual family tree, the more redundant typological information is. This is the case for 6c. On the other hand, 6b and 6a pass the test, as their clusters differ from language geneology: for instance, English and Czech are merged although they belong to different genera (Germanic and Slavic). However, these two feature sets have other drawbacks. The first fails to account for fine-grained differences among related languages: for instance, French, Spanish, and Italian receive the same encoding. 6 The second, one-hot language encoding, does not provide any information on the respective languages.\n\nTo sum up, this section's survey on typological feature sets reveals that most experiments have taken into account a small number of databases and features therein. However, several studies did utilize a larger set of coherent features or full databases. Although well-documented languages do not suffer much from coverage issues, we showed how difficult it is to select typological features that are non-redundant with genealogy, fully discriminative, and informative. The next section addresses these problems proposing automatic prediction as a solution.", "filtered_refids": [[], ["b115"], ["b147", "b100", "b19", "b46"], ["b127", "b51", "b7", "b141"], ["b67"], ["b19"], [null, "b7"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 40, "num_chars": 5515, "num_references": 13}
{"corpusid_sectionid": "49564714-s10", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Heuristics based on morphosyntactic annotation.", "section": "Morphosyntactic feature values can be extracted via heuristics from morphologically and syntactically annotated texts.\n\nFor example, word order features can be calculated by counting the average direction of dependency relations or constituency hierarchies (Liu 2010). Consider the tree of a sentence in Welsh from Bender et al. (2013)  Morphosyntactic annotation is often unavailable for resource-lean languages. In such cases, it can be projected from a source directly to several target languages through language transfer. For instance, \u00d6stling (2015) project source morpho-syntactic annotation directly to several languages through a multilingual alignment. After the alignment and projection, word order features are calculated by the average direction of dependency relations. Similarly, Zhang et al. (2016) transfer POS annotation with a model transfer technique relying on multilingual embeddings, created through monolingual mapping. After the projection, they predict feature values with a multiclass Support Vector Machine (SVM) using POS tag n-gram features.\n\nFinally, typological information can be extracted from Interlinear Glossed Texts (IGT). Such collections of example sentences are collated by linguists and contain grammatical glosses with morphological information. These can guide alignment between the example sentence and its English translation. Lewis and Xia (2008) and Bender et al. (2013) project chunking information from English and train Context Free Grammars on target languages. After collapsing identical rules, they arrange them by frequency and infer word order features.", "filtered_refids": [[], ["b16", "b176", "b101", null], ["b16", "b99"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1609, "num_references": 6}
{"corpusid_sectionid": "49564714-s11", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Unsupervised propagation.", "section": "Another line of research seeks to increase the coverage of typological databases borrowing missing values from the known values in other languages. One approach is clustering languages according to some criterion and propagating the majority value within each cluster. Hierarchical clusters can be created either according to typological features (e.g. Teh, Daum\u00e9 III, and Roy (2007)) or based on language genus (Coke, King, and Radev 2016). Through extensive evaluation, Georgi, Xia, and Lewis (2010) demonstrate that typology based clustering outperforms genealogical clustering for unsupervised propagation of typological features. Among the clustering techniques examined, k-means appears to be the most reliable as compared to k-medoids, the Unweighted Pair Group Method with Arithmetic mean (UPGMA), repeated bisection, and hierarchical methods with partitional clusters.\n\nLanguage similarity measures can also rely on a distributed representation of each language. These language vectors are trained end-to-end as part of neural models for downstream tasks such as many-to-one Neural Machine Translation. In particular, language vectors can be obtained from artificial trainable tokens concatenated to every input sentence, similarly to Johnson et al. (2017), or from the aggregated values of the hidden state of a neural encoder. Using these language representations, typological feature values are propagated using K Nearest Neighbors (Bjerva and Augenstein 2018) or predicted with logistic regression (Malaviya, Neubig, and Littell 2017).\n\nLanguage vectors can be conceived as data-driven, continuous typological representations of a language, and as such provide an alternative to manually crafted typological representations. Similarly to the analysis carried out in \u00a7 4.2, we can investigate how informative language vectors are. Figure 8 compares continuous representations based on artificial tokens ( Figure 8a) and encoder hidden states (Figure 8b) with vectors of discrete WALS features from URIEL ( Figure 8c). All the representations are reduced to 2 dimensions with t-SNE, and color-coded based on their language family.\n\nAs the plots demonstrate, the information encoded in WALS vectors is akin to genealogical information, partly because of biases introduced by family-based propagation of missing values (Littel, Mortensen, and Levin 2016, see \u00a7 4.3.2). On the other hand, artificial tokens and encoder hidden states cannot be reduced to genealogical clusters. Yet, their ability to predict missing values is not inferior to WALS features (see \u00a7 4.3.5). This implies that closeness in the space of such representations is genuinely based on typological properties, rather than being biased by non typological factors. Overall, discrete and continuous representations appear to capture different aspects of the cross-lingual variation. For this reason, they are possibly complementary and could be combined in the future.", "filtered_refids": [["b155", "b67", "b31"], ["b90", "b23", "b106"], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2944, "num_references": 7}
{"corpusid_sectionid": "49564714-s12", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Supervised learning.", "section": "As an alternative to unsupervised propagation, one can learn an explicit model for predicting feature values through supervised classification. For instance, Takamura, Nagata, and Kawasaki (2016) use logistic regression with WALS features and evaluate this model in a cross-validation setting where one language is held out in each fold. Wang and Eisner (2017) provide supervision to a feed-forward neural network with windows of PoS tags from natural and synthetic corpora.\n\nSupervised learning of typology can also be guided by non typological information (see \u00a7 2). Within the Bayesian framework, Murawaki (2017) exploits not only typological but also genealogical and areal dependencies among languages to represent each language as a binary latent parameter vector through a series of autologistic models. Eisner (2017, 2018) develop a point-process generative model of vowel inventories (represented as either IPA symbols or acoustic formants) based on some universal cognitive principles: dispersion (phonemes are as spread out as possible in the acoustic space) and focalization (some positions in the acoustic space are preferred due to the similarity of the main formants).\n\nAn alternative approach to supervised prediction of typology is based on learning implicational universals of the kind pioneered by Greenberg (1963) with probabilistic models from existing typological databases. Using such universals, features can be deduced by modus ponens. For instance, once it has been established that the presence of 'High consonant/vowel ratio' and 'No front-rounded vowels' implies 'No tones', the latter feature value can be deduced from the premises if those are known. Daum\u00e9 III and Campbell (2007) proposes a Bayesian model for learning typological universals that predicts features based on the intuition that their likelihood does not equal their prior probability, but rather is constrained by other features. Lu (2013) cast this problem as knowledge discovery, where language features are encoded in a Directed Acyclic Graph. The strength of implication universals is represented as weights associated with the edges of this graph.", "filtered_refids": [["b165"], [null], ["b49", "b74", "b102"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2149, "num_references": 5}
{"corpusid_sectionid": "49564714-s14", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Comparison of the strategies.", "section": "Establishing which of the above-mentioned strategies is optimal in terms of prediction accuracy is not straightforward. In Figure 10, we collect the scores reported by several of the surveyed papers, provided that they concern specific features or the whole WALS dataset (as opposed to subsets) and are numerical (as opposed to graphical plots). However, these results are not strictly comparable, since language samples and/or the split of data partitions may differ. The lack of standardization in this respect allows us to draw conclusions only about the difficulty of predicting each feature relative to a specific strategy: for instance, the correct value of passive voice is harder to predict than word order according to Bender et al. (2013).\n\nHowever, some papers carry out comparisons of the different strategies within the same experimental setting. According to Coke, King, and Radev (2016), propagation from the genus majority value outperforms logistic regression among word-order typological features. On the other hand, Georgi, Xia, and Lewis (2010) argue that typology-based clusters are to be preferred in general. This apparent contradiction stems from the nature of the target features: genealogy excels in word order features due to their diachronic stability. As they tend to be preserved over time, they are often shared by all members of a family. In turn, majority value propagation is surpassed by supervised classification when evaluated on the entire WALS feature set (Takamura, Nagata, and Kawasaki 2016).\n\nIn general, there appears to be no 'one-size-fits-all' algorithm. For instance, Coke, King, and Radev (2016) outperform Wang and Eisner (2017) for object-verb order (83A) but are inferior to it for adposition-noun (85A). In fact, each strategy is suited for different features, and requires different resources. The extraction of information from morphosyntactic annotation is well suited for word order features, whereas distributional heuristics from multi-parallel texts are more informative about lexicalization patterns. On the other hand, unsupervised propagation and supervised learning are general-purpose strategies. Moreover, the first two presuppose some annotated and/or parallel texts, whereas the second two need a pre-existing database documentation. Strategies may be preferred according to which resources are available for a specific language.\n\nMany strategies have a common weakness, however, as they postulate incorrectly that language samples are independent and identically distributed (Lu 2013;Cotterell and Eisner 2017). This is not the case due to the interactions of family, area, and implicational universals. The solutions adopted to mitigate this weakness vary: Wang and Eisner (2017) balance the data distribution with synthetic examples, whereas Takamura, Nagata, and Kawasaki (2016) model family and area interactions explicitly. However, according to Murawaki (2017), these interactions have different degrees of impact on typological features. In particular, inter-feature dependencies are more influential than inter-language dependencies, and horizontal diffusibility (borrowing from neighbours) is more prominent than vertical stability (inheriting from ancestors).\n\nFinally, a potential direction for future investigation emerges from this section's survey. In addition to missing value completion, automatic prediction often also accounts for the variation internal to each language. However, some strategies go even further, and \"open the way for a typology where generalizations can be made without there being any need to reduce the attested diversity of categorization patterns to discrete types\" (W\u00e4lchli and Cysouw 2012). In fact, language vectors (Malaviya, Neubig, and Littell 2017; Bjerva and Augenstein 2018) and distributional information from multi-parallel texts (Asgari and Sch\u00fctze 2017) are promising insofar they capture latent properties of languages in a bottom-up fashion, preserving their gradient nature. This offers an alternative to hand-crafted database features: in \u00a7 6.3 we make a case for integrating continuous, data-driven typological representations into NLP algorithms.", "filtered_refids": [["b16"], ["b67", "b152", "b31"], ["b31", "b165"], ["b102", "b39", "b114", "b165"], ["b163"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4174, "num_references": 11}
{"corpusid_sectionid": "49564714-s18", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Selective sharing.", "section": "This framework was introduced by Naseem, Barzilay, and  and was subsequently adopted by T\u00e4ckstr\u00f6m, McDonald, and Nivre (2013) and Zhang and Barzilay (2015). It aims at parsing sentences in a language transfer setting (see \u00a7 3.1) where there are multiple source languages and a single unobserved target language. It assumes that head-modifier relations between part of speech pairs are universal, but the order of parts of speech within a sentence is language-specific. For instance, adjectives always modify nouns, but in Igbo (Niger-Congo) they linearly precede nouns, while in Nihali (isolate) they follow nouns. Leveraging this intuition, selective sharing models learn dependency relations from all source languages, while ordering is learned from typologically related languages only.\n\nSelective sharing was originally implemented in a generative framework, factorizing the recursive generation of dependency tree fragments into two steps (Naseem, Barzilay, and Globerson 2012). The first one is universal: the algorithm selects an unordered (possibly empty) set of modifiers {M } given a head h with probability P ({M }|h), where both the head and the modifiers are characterized by their PoS tags. The second step is language-specific: each dependent m is assigned a direction d (left or right) with respect to h based on the language l, with probability P (d|m, h, l). Dependents in the same direction are eventually ordered with a probability drawn from a uniform distribution over their possible unique permutations. The total probability is then defined as follows:\n\nIn Equation 1, the first step is expressed as two factors: the estimation of the number n of modifiers, parametrized by \u03b8 1 , and the actual selection of modifiers, parametrized by \u03b8 2 , with the softmax function \u03c3 converting the n values into probabilities. The second step, overseeing the assignment of a direction to the dependencies, is parametrized by w that multiplies a feature function g(), whose arguments include a typology feature vector f l . The values of all the parameters are estimated by maximizing the likelihood of the observations. T\u00e4ckstr\u00f6m, McDonald, and Nivre (2013) proposed a discriminative version of the model, in order to amend the alleged limitations of the original generative variant. In particular, they dispose of the strong independence assumptions (e.g. between choice and ordering of modifiers) and invalid feature combinations. For instance, the WALS feature 'Order of subject, verb, and object' (81A) should be taken into account only when the head under consideration is a verb and the dependent is a noun, but in the generative model this feature was fed to g() regardless of the head-dependency pair. The method of T\u00e4ckstr\u00f6m, McDonald, and Nivre (2013) is a delexicalized first-order graph-based parser based on a carefully selected feature set. From the set proposed by McDonald, Crammer, and Pereira (2005), they keep only (universal) features describing selectional preferences and dependency length. Moreover, they introduce (language-specific) features for the directionality of dependents, based on combinations of the PoS tags of the head and modifiers with corresponding WALS values.\n\nThis approach was further extended to tensor-based models by Zhang and Barzilay (2015), in order to avoid the shortcomings of manual feature selection. They induce a compact hidden representation of features and languages by factorizing a tensor constructed from their combination. The prior knowledge from the typological database enables the model to forbid the invalid interactions, by generating intermediate feature embeddings in a hierarchical structure. In particular, given n words and l dependency relations, each arc h \u2192 m is encoded as the tensor product of three feature vectors for heads \u03a6 h \u2208 R n , modifiers \u03a6 m \u2208 R n and the arcs \u03a6 h\u2192m \u2208 R l . A score is obtained through the inner product of these and the corresponding r rank-1 dense parameter matrices for heads H \u2208 R n\u00d7r , dependents M \u2208 R n\u00d7r , and arcs M \u2208 R l\u00d7r . The resulting embedding is subsequently constrained through a summation with the typological features T u \u03c6 t u :\n\n(2) Equation 2 shows how the overall score of a labeled dependency is enriched (by elementwise product) with (1) the features and parameters for arc labels L\u03c6 l constrained by the typological vector T l \u03c6 t l ; and (2) features and parameters for head contexts H c \u03c6 h c and dependent contexts M c \u03c6 m c . This loss is optimized within a maximum soft-margin objective through on-line passive-aggressive updates.\n\nThe different approaches to selective sharing presented here explicitly deal with cases where the typological features do not match any of the source languages, which may lead learning astray. Naseem, Barzilay, and Globerson (2012) propose a variant of their algorithm where the typological features are not observed (in WALS), treating them as latent variables, and learning the model parameters in an unsupervised fashion with the Expectation Maximization algorithm (Dempster, Laird, and Rubin 1977). T\u00e4ckstr\u00f6m, McDonald, and Nivre (2013) tackle the same problem from the side of ambiguous learning. The discriminative model on the target language is trained on sets of automatically predicted ambiguous labels y. Finally, Zhang and Barzilay (2015) employ semi-supervised techniques, where only a handful of annotated examples from the target language is available.", "filtered_refids": [["b149", "b175"], ["b115"], ["b108", "b149"], ["b175"], [], ["b115", "b149", "b175", "b50"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 34, "num_chars": 5444, "num_references": 10}
{"corpusid_sectionid": "49564714-s19", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Multi-lingual Biasing.", "section": "Some papers leverage typological features to gear the shared parameters of a joint multilingual model toward the properties of a specific language. Daiber, Stanojevi\u0107, and Sima'an (2016) develop a reordering algorithm that estimates the permutation probabilities of aligned word pairs in multi-lingual parallel texts. The best sequence of permutations is inferred via k-best graph search in a finite state automaton, producing a lattice. This algorithm, which receives lexical, morphological, and syntactic features of the source word pairs and typological features of the target language as input, has shown to benefit a downstream machine translation task.\n\nThe joint multilingual parser of Ammar et al. (2016) shares hidden-layer parameters across languages, and combines both language-invariant and language-specific features in its copious lexicalized input feature set. This transition-based parser selects the next action z (e.g. SHIFT) from a pool of possible actions given its current state p t , as defined in Equation 3:\n\nP (z|p t ) is defined in terms of a set of iteratively manipulated, densely represented data structures: a buffer b t , a stack s t , and an action history a t . The hidden representation of these modules are the output of stack-LSTMs, that are in turn fed with input word feature representations (stack and buffer) and action representations (history). The shared parameters are biased toward a particular language through language embeddings l it . The language embeddings consist of (a non-linear transformation of) either a mere one-hot identity vector or a vector of typological properties taken from WALS. In particular, they are added to both input feature and action vectors, to affect the three above-mentioned modules individually, and concatenated to the hidden module representations, to affect the entire parser state. The resulting state representation is propagated through an action-specific layer parametrized by g t and q t , and activated by a softmax function \u03c3 over actions.\n\nSimilarly, typological features have been employed to bias input and hidden states of language models. For example, Tsvetkov et al. (2016) proposed a multilingual phonemelevel language model where an input phoneme x and a language vector at time t are linearly mapped to a local context representation and then passed to a global LSTM. This hidden representation G t is factored by a non-linear transformation of typological features t , as shown in Equation 4:\n\nAs described in Equation 5, G t is then vectorized and mapped to a probability distribution of possible next phonemes \u03c6 t . The phoneme vectors, learned by the language model in an end-to-end manner, were demonstrated to benefit two downstream applications: lexical borrowing identification and speech synthesis. Moreover, typological features (in the form of implicational universals) can guide the design of Bayesian networks. Schone and Jurafsky (2001) assign part-of-speech labels to word clusters acquired in an unsupervised fashion. The underlying network is acyclic and directed, and is converted to a join-tree network to handle multiple parents (Jensen 1996). For instance, the sub-graph for the ordering of numerals and nouns is intertwined also with properties of adjectives and adpositions. The final objective maximizes the probability of a tag T i and a feature set \u03a6 i given the implicational universals U as", "filtered_refids": [["b46"], ["b7"], [], ["b158"], ["b89", "b141"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3416, "num_references": 5}
{"corpusid_sectionid": "49564714-s20", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Data Selection, Synthesis, and Preprocessing", "section": "Another way in which typological features are used in NLP is to guide data selection. This procedure is crucial for 1) language transfer methods, as it guides the choice of the most suitable source languages and examples; and 2) multilingual joint models, in order to weigh the contribution of each language and example. The selection is typically carried out through general language similarity metrics. For instance, Deri and Knight (2016) base their selection on the URIEL language typology database, considering information about genealogical, geographic, syntactic, and phonetic properties. This facilitates language transfer of grapheme-to-phoneme models, by guiding the choice of source languages and aligning phoneme inventories. Metrics for source selection can also be extracted in a data-driven fashion, without explicit reference to structured taxonomies. For instance, Rosa and Zabokrtsky (2015) estimate the Kullback-Leibler divergence between part-of-speech trigram distributions for delexicalized parser transfer. In order to approximate the divergence in syntactic structures between languages, Ponti et al. (2018a) employ the Jaccard distance between morphological feature sets and the tree edit distance of delexicalized dependency parses of similar sentences.\n\nA-priori and bottom-up approaches can also be combined. For delexicalized parser transfer, Agi\u0107 (2017) relies on a weighted sum of distances based on 1) the PoS divergence defined by Rosa and Zabokrtsky (2015); 2) the character-based identity prediction of the target language; and 3) the Hamming distance from the target language typological vector. In fact, they have different weaknesses: language identity (and consequently typology) fail to abstract away from language scripts. On the other hand, the accuracy of PoS-based metrics deteriorates easily in scenarios with poor amounts of data.\n\nSource language selection is a special case of source language weighting where weights are one-hot vectors. However, weights can also be gradient and consist of real numbers. S\u00f8gaard and Wulff (2012) adapt delexicalized parsers by weighting every training instance based on the inverse of the Hamming distance between typological (or genealogical) features in source and target languages. An equivalent bottom-up approach is developed by S\u00f8gaard (2011) who weighs source language sentences based on the perplexity between their coarse PoS tags and the predictions of a sequential model trained on the target language.\n\nAlternatively, the lack of target annotated data can be alleviated by synthesizing new examples, thus boosting the variety and amount of the source data. For instance, the Galactic Dependency Treebanks stem from real trees whose nodes have been permuted probabilistically according to the word order rules for nouns and verbs in other languages (Wang and Eisner 2016). Synthetic trees improve the performance of model transfer for parsing when the source is chosen in a supervised way (performance on target development data) and in an unsupervised way (coverage of target PoS sequences).\n\nRather than generating new synthetic data, Ponti et al. (2018a) leverage typological features to pre-process treebanks in order to reduce their variation in language transfer tasks. In particular, they adapt source trees to the typology of a target language with respect to several constructions in a rule-based fashion. For instance, relative clauses in Arabic (Afro-Asiatic) with an indefinite antecedent drop the relative pronoun, which is mandatory in Portuguese (Indo-European). Hence, the pronoun has to be added, or deleted in the other direction. Feeding pre-processed syntactic trees to lexicalized syntaxbased neural models, such as feature-based recurrent encoders (Sennrich and Haddow 2016) or TreeLSTMs (Tai, Socher, and Manning 2015), achieves state-of-the-art results in Neural Machine Translation and cross-lingual sentence similarity classification.", "filtered_refids": [["b133", "b51", "b129"], ["b133"], ["b147", "b146"], ["b164"], ["b142", "b129", "b151"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 3953, "num_references": 10}
{"corpusid_sectionid": "49564714-s21", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Comparison", "section": "In light of the performance of the above methods, to what extent can typological features benefit downstream NLP tasks and applications? To answer this key question, consider the performance scores of each model reported in Figure 11. Each model has been evaluated in the original paper in one (or more) of the three main settings: with gold database features (Typology), with latently inferred typological features (Data-driven), or without both (Baseline), and with otherwise identical architecture and hyper-parameters.\n\nIt is evident that typology-enriched models consistently outperform baselines across several NLP tasks. Indeed, the scores are higher for metrics that increase (Unlabeled Attachment Score, F1 Score and BLEU) and lower for metrics that decrease (Word Error Rate, Mean Average Error and Perplexity) with better predictions. Nevertheless, improvements tend to be moderate, and only a small number of experiments support them with statistical significance tests. In general, it appears that they fall short of the potential usefulness of typology: in \u00a7 6 we analyse the possible reasons for this. Some of the experiments we have surveyed investigate the effect of substituting typological features with features related to Genealogy and Language Identity (e.g. one-hot encoding of languages). Based on the results in Figure 11, it is unclear whether typology should be preferred, as it is sometimes rivaled by other types of features. In particular, it is typology that excels according to Tsvetkov et al. (2016), genealogy according to S\u00f8gaard and Wulff (2012) and T\u00e4ckstr\u00f6m, McDonald, and Nivre (2013), and language identity according to Ammar et al. (2016). However, drawing conclusions from the last experiment seems incautious: in \u00a7 4.2, we argued that their selection of features (presented in Figure 6) is debatable due to low diversification or noise. Moreover, it should be emphasized that one-hot language encoding is limited to the joint multilingual learning setting: since it does not convey any information, it is of no avail in language transfer.\n\nFinally, let us consider the effectiveness of the above methods with respect to incorporating typological features in NLP models. In case of selective sharing, the tensorbased discriminative model (Zhang and Barzilay 2015) outperforms the graph-based discriminative model (T\u00e4ckstr\u00f6m, McDonald, and Nivre 2013), which in turn surpasses the generative model (Naseem, Barzilay, and Globerson 2012). With regard to biasing multilingual models, there is a clear tendency toward letting typological features interact not merely with the input representation, but also with deeper levels of abstraction such as hidden layers.\n\nOverall, this comparison supports the claim that typology can potentially aid in designing the architecture of algorithms, engineering their features, and selecting and pre-processing their data. Nonetheless, this discussion also revealed that many challenges lie ahead for each of these goals to be accomplished fully. We discuss them in the next section.", "filtered_refids": [[], ["b158", "b147", "b7", "b149"], ["b115", "b149", "b175"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3059, "num_references": 7}
{"corpusid_sectionid": "49564714-s23", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Extending the Usage to New Tasks and Features", "section": "The trends observed in \u00a7 5 reveal that typology is integrated into NLP models mostly in the context of morphosyntactic tasks, and particularly syntactic parsing. Some exceptions include other levels of linguistic structure, such as phonology (Tsvetkov et al. 2016;Deri and Knight 2016) and semantics (Bender 2016;Ponti et al. 2018a). As a consequence, the set of selected typological features is mostly limited to a handful of word-order features from a single database, WALS. Nonetheless, the array of tasks that pertain to polyglot NLP is broad, and other typological datasets that have thus far been neglected may be relevant for them.\n\nFor example, typological frame semantics might benefit semantic role labeling, as it specifies the valency patterns of predicates across languages, including the number of arguments, their morphological markers, and their order. This information can be cast in the form of priors for unsupervised syntax-based Bayesian models , guidance for alignments in annotation projection (Pad\u00f3 and Lapata 2009;Van der Plas, Merlo, and Henderson 2011), or regularizers for model transfer in order to tailor the source model to the grammar of the target language (Kozhevnikov and Titov 2013). Cross-lingual information about frame semantics can be extracted, for example, from the Valency Patterns Leipzig database (ValPaL).\n\nTypological information regarding lexical semantics patterns can further assist various NLP tasks, by providing information about translationally equivalent words across languages. Such information is provided in databases such as the World Loanword Database (WOLD), the Intercontinental Dictionary Series (IDS), and the Automated Similarity Judgment Program (ASJP). One example task is word sense disambiguation, as senses can be propagated from multilingual word graphs (Silberer and Ponzetto 2010) by bootstrapping from a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WE-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016).\n\nAnother task where lexical semantics is crucial is sentiment analysis, for similar reasons: bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fern\u00e1ndez, Esuli, and Sebastiani 2015;Ziser and Reichart 2018). Moreover, sentiment analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli\u0107, and Korhonen 2017).\n\nFinally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al. 2018) setups. In fact, the degree of fusion between roots and inflectional/derivative morphemes impacts the type/token ratio of texts, and consequently their rate of infrequent words. Moreover, the ambiguity of mapping between form and meaning of morphemes determines the usefulness of injecting character-level information (Gerz et al. 2018). This variation has to be taken into account in both language transfer and multilingual joint learning.\n\nAs a final note, we stress that the addition of new features does not concern just future work, but also the existing typology-savvy methods, which can widen their scope. For instance, the parsing experiments grounded on selective sharing ( \u00a7 5.2) could also take into consideration WALS features about Nominal Categories, Nominal Syntax, Verbal Categories, Simple Clauses, and Complex Sentences, as well as features from other databases such as SSWL, APiCS, and AUTOTYP. Likewise, models for phonological tasks (Tsvetkov et al. 2016;Deri and Knight 2016) could also extract features from typological databases such as LAPSyD and StressTyp2.", "filtered_refids": [["b15", "b158", "b51", "b129"], ["b95", "b125", "b128"], ["b174", "b97", "b92", "b143"], ["b131", "b180", "b65", "b5"], ["b90", "b8", "b68"], ["b158", "b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3983, "num_references": 20}
{"corpusid_sectionid": "49564714-s24", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Injecting Typological Information into Machine Learning Algorithms", "section": "In \u00a7 5, we discussed the potential of typological information to provide guidance to NLP methods, and surveyed approaches such as network design in Bayesian models (Schone and Jurafsky 2001), selective sharing (Naseem, Barzilay, and Globerson 2012, inter alia), and biasing of multilingual joint models (Ammar et al. 2016, inter alia). However, many other frameworks (including those already mentioned in \u00a7 3) have been developed independently in order to allow the integration of expert and domain knowledge into traditional feature-based machine learning algorithms and neural networks. In this section we survey these frameworks and discuss their applicability to the integration of typological information into NLP models.\n\nEncoding cross-language variations and preferences into a machine learning model requires a mechanism that can bias the learning (i.e. training and parameter estimation) and inference (prediction) of the model towards some pre-defined knowledge. In practice, learning algorithms, both linear (e.g. structured perceptron (Collins 2002), MIRA (Crammer and Singer 2003) and structured SVM (Taskar, Guestrin, and Koller 2004)) and non-linear (deep neural models) iterate between an inference step and a step of parameter update with respect to a gold standard. The inference step is the natural place where external knowledge could be encoded through constraints. This step biases the prediction of the model to agree with the external knowledge which, in turn, affects both the training process and the final prediction of the model at test time.\n\nInformation about cross-lingual variation, particularly when extracted empirically (see \u00a7 4), reflects tendencies rather than strict rules. As a consequence, soft, rather than hard constraints are a natural vehicle for their encoding. We next survey a number of existing approaches that can efficiently encode such constraints.\n\nThe goal of an inference algorithm is to predict the best output label according to the current state of the model parameters. 7 For this purpose, the algorithm searches the space of possible output labels in order to find the best one. Efficiency hence plays a key role in these algorithms. Introducing soft constraints into an inference algorithm, therefore, posits an algorithmic challenge: how can the output of the model be biased to agree with the constraints while the efficiency of the search procedure is kept? In this paper we do not answer this question directly but rather survey a number of approaches that succeeded in dealing with it.\n\nSince linear models have been prominent in NLP research for a much longer time, it is not surprising that frameworks for the integration of soft constraints into these models are much more developed. The approaches proposed for this purpose include posterior regularization (PR) (Ganchev et al. 2010), generalized expectation (GE) (Mann and McCallum 2008), constraint-driven learning (CODL) (Chang, Ratinov, and Roth 2007), dual decomposition (DD) (Globerson and Jaakkola 2007;Komodakis, Paragios, and Tziritas 2011) and Bayesian modeling (Cohen 2016). These techniques employ different types of knowledge encoding, e.g. PR uses expectation constraints on the posterior parameter distribution, GE prefers parameter settings where the model's distribution on unsupervised data matches a predefined target distribution, CODL enriches existing statistical models with Integer Linear Programming (ILP) constraints, while in Bayesian modeling a prior distribution is defined on the model parameters.\n\nPR has already been used for incorporating universal linguistic knowledge into an unsupervised parsing model (Naseem et al. 2010). In the future, it could be extended to typological knowledge, which is a good fit for soft constraints. As another option, Bayesian modeling sets prior probability distributions according to the relationships encoded in typological features (Schone and Jurafsky 2001). Finally, DD has been applied to multi-task learning, which paves the way for typological knowledge encoding through a multi-task architecture in which one of the tasks is the actual NLP application and the other is the data-driven prediction of typological features. In fact a modification of this archiecture has already been applied to minimally-supervised learning and domain adaptation with soft (non-typological) constraints (Rush et al. 2012;Reichart and Barzilay 2012).\n\nThe same ideas could be exploited in deep learning algorithms. We have seen in \u00a7 3.2 that multilingual joint models combine both shared and language-dependent parameters, in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, similarly to what Chen et al. (2016) implemented by predicting language identity.\n\nRecently, Hu et al. (2016a,b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similarly to CoDL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models.\n\nA particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representation learning ( \u00a7 3.3). In this domain a number of works (Faruqui et al. 2015;Rothe and Sch\u00fctze 2015;Osborne, Narayan, and Cohen 2016;Mrk\u0161i\u0107 et al. 2016) have proposed means through which external knowledge sourced from linguistic resources (such as WordNet, BabelNet, or lists of morphemes) can be encoded in word embeddings. Among the state-of-the-art specialization methods ATTRACT-REPEL Vuli\u0107 et al. 2017) pushes together or pulls apart vector pairs according to relational constraints, while preserving the relationship between words in the original space and possibly propagating the specialization knowledge to unseen words or transferring it to other languages (Ponti et al. 2018b). The success of these works suggests that a more extensive integration of external linguistic knowledge in general, and typological knowledge in particular, is likely to play a key role in the future development of word representations.", "filtered_refids": [[null, "b141"], ["b154", "b33", "b41"], [], [], ["b94", "b107", "b28", "b66", "b30", "b69"], ["b132", "b116", "b138", "b141"], ["b29"], ["b86", null, "b87", "b166"], ["b113", "b130", "b162", "b121", "b134", "b64"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 34, "num_chars": 6562, "num_references": 26}
{"corpusid_sectionid": "49564714-s25", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "A New Typology: Gradience and Context-Sensitivity", "section": "As shown in \u00a7 4.2, most of the typology-savvy algorithms thus far exploited features extracted from manually-crafted databases. However, this approach is riddled by several shortcomings, which are reflected in the small performance improvements observed in \u00a7 5.4. Luckily, these shortcomings may potentially be averted through the use of methods that allow typological information to emerge from the data in a bottom-up fashion, rather than being predetermined. In what follows we advocate for such a data-driven approach, based on several considerations. Firstly, typological databases provide incomplete documentation of the cross-lingual variation, in terms of features and languages. Raw textual data, which is easily accessible and cost-effective, may provide a valid alternative that can facilitate automatic learning of more complete knowledge. Secondly, database information is approximate, as it is restricted to the majority strategy within a language. However, in theory each language allows for multiple strategies in different contexts and with different frequencies, hence databases risk to hinder models from learning less likely but plausible patterns (Sproat 2016). Inferring typological information from text would enable a system to discover patterns within individual examples, including both the frequent and the infrequent ones. Thirdly, typological features in databases are discrete, employing predefined categories devised to make high-level generalizations across languages. However, several categories in natural language are gradient (see for instance the discussion on semantic categorization in \u00a7 2), hence they are better captured by continuous features. In addition to being psychologically motivated, this sort of representations is also more compatible with machine learning algorithms and particularly with deep neural models that naturally operate with real-valued multi-dimensional word embeddings and hidden states.\n\nTo sum up, the automatic development of typological information and its possible integration into machine learning algorithms have the potential to solve an important bottleneck in polyglot NLP. Current manually curated databases consist of incomplete, approximate, and discrete features, that are intended to reflect contextual and gradient information implicitly present in text. These features are fed to continuous, probabilistic, and contextual machine learning models -which do not form a natural fit for the typological features. Instead, we believe that modeling cross-lingual variation directly from textual data can yield typological information that is more suitable for machine learning.\n\nSeveral techniques surveyed in \u00a7 4.3 are suited to serve this purpose. In particular, the extraction from morphosyntactic annotation (Liu 2010, inter alia) and alignments from multi-parallel texts (Asgari and Sch\u00fctze 2017, inter alia) provide information about typological constructions at the level of individual examples. Moreover, language vectors (Malaviya, Neubig, and Littell 2017;Bjerva and Augenstein 2018) and alignments from multi-parallel texts preserve the gradient nature of typology through continuous representations.\n\nThe successful integration of these components would affect the way multilingual feature engineering is performed. As opposed to using binary vectors of typological features, the information about language-internal variation could be encoded as realvalued vectors where each dimension is a possible strategy for a given construction and its relative frequency within a language. As an alternative, selective sharing and multilingual biasing could be performed at the level of individual examples rather than languages as a whole. In particular, model parameters could be transferred among similar examples and input/hidden representations could be conditioned on contextual typological patterns. Finally, focusing on the various instantiations of a particular type rather than considering languages as indissoluble blocks would enhance data selection, similarly to what S\u00f8gaard (2011) achieved using PoS n-grams for similarity measurement. The selection of similar sentences rather than similar languages as source data in language transfer is likely to yield large improvements, as demonstrated by Agi\u0107 (2017) for parsing in an oracle setting.\n\nFinally, the bottom-up development of typological features may address also radically resource-less languages that lack even raw textual data in a digital format. For this group, which still constitutes a large portion of the world's languages, there are often available reference grammars written by field linguists, which are the ultimate source for typological databases. These grammars could be queried automatically, and fine-grained typological information could be harvested through information extraction techniques.", "filtered_refids": [["b148"], [], ["b23", "b101", "b106"], ["b1"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 4860, "num_references": 5}
{"corpusid_sectionid": "9088773-s1", "title": "SURVEY OF THE MESSAGE UNDERSTANDING CONFERENCES", "date": "1993-03-21", "section_title": "REVIEW OF PAST MUCs", "section": "The first Message Understanding Conference (MUC) was held in 1987, used ten narrative paragraphs from naval messages as a training corpus and two others as test data, and had no defined evaluation task or metrics. Researchers from six organizations ran their systems on the test data during the conference, then demonstrated and explained how the systems analyzed the texts. Two years later, the second MUC was held [10]. It made use of a training corpus of 105 naval message narratives of four different types, a dry-run test set of 20 narratives, and a final test set of five. An information extraction task was defined that consisted of identifying ten different pieces of information and representing them as slot fillers in a template resembling a semantic frame. This task emulates an information management application requiring the culling of facts from a large body of free text as a means to generate updates to a formatted database.\n\nA rudimentary set of scoring standards was developed, and the templates produced by the eight systems (including four of the six systems represented at the 1987 evaluation) were scored by hand by comparison with a hand-generated answer key. The nature of the corpus used for the second MUC was difficult enough that grammar coverage and parsing efficiency were serious issues. The domain was complex enough that the knowledge engineering job was greatly facilitated by the availability of documentation presenting much of the essential, declarative domain knowledge in a structured format.\n\nAfter another two-year interval, MUC-3 was held in May, 1991, followed by MUC-4 in June, 1992. There are published proceedings for the third and fourth conferences [8,9], including descriptions and test results of the participating systems (15 for MUC-3, 17 for MUC-4).\n\nA new corpus of 1,400 texts on the subject of Latin American terrorism was used that includes 16 text types (transcribed speeches, newspaper articles, editorial reports, etc.). The template developed for MUC-3 contained slots for 17 pieces of information; the number of informationbearing slots increased to 22 for MUC-4. The scoring metrics were refined and implemented for MUC-3 and MUC-4 in a semiautomated scoring program.\n\nFor MUC-3, a study was carried out to measure the complexity of the MUC-3 terrorism task vis-a-vis the naval task, and the scores obtained in the 1989 evaluation were recomputed using the MUC-3 method of scoring [5]. Although these scores were lower, the conclusion was that significant progress had been made, because the increase in difficulty in the task more than offset the decrease in scores.\n\nIt was possible to conduct a more refined study of the progress from MUC-3 to  that showed that higher levels of performance by nearly all veteran systems were achieved despite the relative difficulty of the MUC-4 test set that was used in the comparison and despite increased strictness of the scoring with respect to spurious data generation. The results of MUC-4 show that higher recall is usually correlated with higher precision 1, which is consistent with the results of previous evaluations and suggests that there is still a variety of techniques with potential for attaining even higher levels of performance in the future. In absolute terms, however, recall and precision scores were still only moderate.\n\nAccording to an analysis of the effectiveness of techniques used by MUC-3 systems [4], pattern-matching techniques (with hand-crafted or automatically acquired patterns) and probabilistic text categorzafion techniques proved successful only when combined with linguistic techniques. The use of robust processing including robust parsing was shown to correlate with the success of the system. In a comparison of MUC-3 and MUC-4 systems, minimal improvement from MUC-3 to MUC-4 was demonstrated by the two systems that did not use linguistically-based processing [12]. Several linguistically-based MUC-3 systems improved considerably via extensions made for MUC-4, as did one MUC-3 system that was converted from a generic text understanding system to an information extraction system that maintains its basis in linguistics but is streamlined for speed and geared specifically to the demands of information extraction. However, other systems which underwent a complete overhaul for MUC-4 showed only slight progress or even a degradation in performance.\n\nError analyses point to the critical need for further research in areas such as discourse reference resolution and inferencing. For example, the inability to reliably determine whether a description found in one part of the text refers or does not refer to something previously described inhibits both recall and precision because it could result in the system either missing information or generating spurious information; the inability to pick up subtle relevance indications (e.g., that persons described as being \"in\" a place that was attacked could be targets of the attack) and not-so-subtle ones (e.g., that a vehicle whose roof collapsed as a result of a bomb explosion was damaged by the explosion) places a limitation on recall because it results in missed information. The ability to take advantage of sophisticated approaches to discourse that have already received computational treatment is limited by a dependence on error-free outputs from earlier stages of processing. Thus, there is a need for renewed attention to robust processing at the sentence level.", "filtered_refids": [[null], [], [null], [], [null], [], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 5478, "num_references": 4}
{"corpusid_sectionid": "44130961-s1", "title": "TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation", "date": "2018-05-11", "section_title": "Prerequisite Chains", "section": "Prerequisite chains refer to edges in a graph describing which topics are dependent on the knowledge of another topic. Prerequisite chains play an important role in curriculum planning and reading list generation. Liu et al. (2016) propose \"Concept Graph Learning\" in order to induce a graph from which they can predict prerequisite relations among university courses. Their framework consists of two graphs: (1) a higher-level graph which consists of university courses and (2) a lowerlevel graph which consists of induced concepts and pair-wise sequential preferences in learning or teaching the concept. Liang et al. (2017) experiment with prerequisite chains on education data but focus on the recovery of a concept graph rather than on predicting unseen course relations as in Liu et al. (2016). They introduce both a synthetic dataset as well as one scraped from 11 universities which includes course prerequisites as well as conceptprerequisite labels. Concept graphs are also used in (Gordon et al., 2016) to address the problem of developing reading lists for students. The concept graph in this case is a labeled graph where nodes represent both documents and concepts (determined using Latent Dirichlet Allocation (LDA) (Blei et al., 2003)), and edges represent dependencies. They propose methods based on cross entropy and information flow for determining edges in the graph. Finally, finding prerequisite relationships has also been used in other contexts such as Massive Open Online Courses (MOOCs) (Pan et al., 2017a,b).", "filtered_refids": [["b12", "b13", null, "b1", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1535, "num_references": 5}
{"corpusid_sectionid": "44130961-s18", "title": "TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation", "date": "2018-05-11", "section_title": "Dataset and Annotation Statistics", "section": "We created reading lists for 182 of the 200 topics we identify in Section 4.2. Resources were not found for 18 topics due to the granularity of the topic (e.g., Radial Basis Function Networks) as well as our intended restriction of the chosen resources to PowerPoint presentations and HTML pages. The average number of resources per reading list for the 182 topics is 3.94. As an extension to the reading lists we collected Wikipedia pages for 184 of the topics and present these urls as part of the dataset. We annotated prerequisite relations for the 200 topics described above. We present a subset of our annotations in Figure 1, which shows the network of topic relations (nodes without incoming edges were not annotated for their prerequisites as part of this shown inter-annotation round). Our network consists of 794 unidirectional edges and 33 bidirectional edges. The presence of bidirectional edges stems from our definition of a prerequisite, which does not preclude bidirectionality (one topic can help explain another and viceversa) as well as the similarity of the topics. The set of bidirectional edges consists of topic pairs (BLEU -ROUGE; Word Embedding -Distributional Semantics; Backpropagation -Gradient descent) which could be collapsed into one topic to create a directed acyclic graph in the future.\n\nFor survey extraction, we automatically split 313 resources into content cards which we annotated for usefulness in survey extraction. These resources are a subset of the reading lists limited in number due to constraints in downloading urls and parsing to our annotation interface. The total number of cards which were not marked as repeats/mis-parsed totals 17,088, with 54.59 per resource. 6,099 cards were labeled as somewhat relevant or relevant for the target topic. The resources marked as non-relevant may be poorly  presented or may not pertain fully to the topic of that survey. These numbers confirm the appropriateness of this survey corpus as a non-trivial information retrieval task.\n\nTo better understand the difficulty of our annotation tasks, we performed inter-annotator agreement experiments for each of our annotations. We randomly sampled twenty-five resources and had annotators label for pedagogical function. Additionally, we sampled twenty-five topics for prerequisite annotations and five topics with reading list lengths of five for survey annotation. We used Fleiss's Kappa (Fleiss et al., 2004), a variant of Cohen's Kappa (Cohen, 1960) designed to measure annotator agreement for more than two annotators. The results are shown in Table 5. Using the scale as defined in Landis and Koch (1977), pedagogical function annotation exhibits substantial agreement while prerequisite annotation and survey extraction annotation show fair agreement. The Kappa score for pedagogical function is comparable to that of Sheng et al. (2017) (0.68) while the prerequisite annotation is slightly lower than the agreement metric used in Gordon et al. (2016) (0.36) although they measure agreement through Pearson correlation. We believe that the sparsity of the labels plays a role in these scores.", "filtered_refids": [[], [], ["b10", null, "b2", "b5", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 3135, "num_references": 5}
{"corpusid_sectionid": "44130961-s19", "title": "TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation", "date": "2018-05-11", "section_title": "Comparison to Similar Datasets", "section": "Our corpus distinguishes itself in its magnitude, manual collection and focus on annotation for educational purposes in addition to research tasks. We use similar categories for classifying pedagogical function as , but our corpus is hand-picked and over four-times larger, while exhibiting similar annotation agreement. Gordon et al. (2016) present a corpus for prerequisite relations among topics, but this corpus differs in coverage. They used LDA topic modeling to generate a list of 300 topics, while we manually create a list of 200 topics based on criteria described above. Although their topics are generated from the ACL Anthology and related to NLP, we find less than a 40% overlap in topics. Additionally, they only annotate a subset of the topics for prerequisite annotations while we focus on broad coverage, annotating two orders of magnitude larger in terms of prerequisite edges while exhibiting fair inter-annotator agreement.\n\nPrevious work and datasets on generating surveys for scientific topics have focused on scientific articles (Jha et al., 2013(Jha et al., , 2015Jaidka et al., 2016) and Wikipedia pages (Sauper and Barzilay, 2009;Liu et al., 2018) as a summarization task. We, on the other hand, view this problem as an information retrieval task and focus on extracting content from manually-collected PowerPoint slides and online tutorials. Sauper and Barzilay (2009) differ in their domain coverage, and while the surveys of Jha et al. (2013Jha et al. ( , 2015 focus on NLP, we collect resources for an order of magnitude larger set of topics. Finally, our focus here in creating surveys, as well as the other annotations, is first and foremost to create a useful tool for students and researchers. Websites such as the ACL Anthology 3 and arXiv 4 provide an abundance of resources, but do not focus on the pedagogical aspect of their content. Meanwhile, websites such as Wikipedia which aim to create a survey of a topic may not reflect the latest trends in rapidly changing fields.", "filtered_refids": [["b5"], ["b6", "b14", "b9", "b21", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2012, "num_references": 6}
{"corpusid_sectionid": "254854669-s1", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "John F. Kennedy", "section": "Negotiation is one of the crucial abilities in human communication that involves two or more individuals discussing goals and tactics to resolve conflicts, achieve mutual benefit, or find mutually acceptable solutions (Fershtman, 1990;Bazerman and Neale, 1993;Lewicki et al., 2011). It is a common aspect of human interaction, occurring whenever people communicate in order to manage conflict or reach a compromise. Scientifically, one of the long-term goals of dialogue research is to empower intelligent agents with such ability. Agent effectively negotiating with a human in natural language could have significant benefits in many scenarios, from bargaining prices in everyday trade-in (He et al., 2018) to high-stakes political or legal situations (Basave and He, 2016). Negotiation dialogue systems (Lewandowska, 1982;Lambert and Carberry, 1992;Chawla et al., 2021c) is an emerging research field that aims to build intelligent conversational agents that can automatically negotiate with a human in natural languages, e.g., CICERO 1 from Meta AI. Agents negotiate with human through multi-turn interaction using logically reasoning (Sycara and Dai, 2010) over goals (Zhang et al., 2020), strategies (Zhou et al., 2020) and psychology factors (Yang et al., 2021). As illustrated in Figure 1, negotiation dialogue agents interact with the human through multiturn cycles. A successful negotiation process involves efficient information exchange, strategic discussion toward their goals, and a closing section.\n\nDespite the significant amount of research that has been conducted on the task, there is a lack of a systematic review of the topic. In this work, we aim to fill this gap by reviewing contemporary work in the emerging field of negotiation dialogue systems, covering aspects such as benchmarks, evaluation, methodology, and future directions. In recent years, various benchmarks have been proposed for negotiation dialogue systems, ranging from bargaining (Lewis et al., 2017) and game scenarios (Asher et al., 2016) to job interviews  and items exchanging (Chawla et al., 2021c). Our survey will provide an overview of these benchmarks and discuss how they have been used to evaluate the performance of negotiation dialogue systems.\n\nModeling the negotiation process for conversational agents also imposes challenges. Firstly, these agents must be able to reason about and employ various strategies in different situations. In addition to strategy modeling, it is also necessary to model the personalities (e.g., mind, emotion, and behaviors) of the negotiators. Thirdly, an effective policy learning method is essential for the successful use of language. To address these challenges, we can categorize existing solutions into three areas: (1) Personality modeling helps us understand negotiator's preferences, (2) Strategy modeling enables agents to make reasonable decisions based on gathered information, and (3) Policy learning methods utilize information effectively to maximize results.\n\nIn summary, our contributions are three-fold: (1) To the best of our knowledge, we systematically categorize current negotiation dialogue benchmarks from the perspective of distributive and integrative, with each category based on different goal types of negotiation dialogue tasks. (2) We categorize typical evaluation methods and current solutions into an appropriate taxonomy. (3) We pointed out the current limitation and promising research directions in the future.", "filtered_refids": [["b10", "b56", "b4", "b47", "b29", "b32", "b55", "b59", "b24", "b33", "b2", "b17"], ["b1", "b10", "b34"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3479, "num_references": 15}
{"corpusid_sectionid": "254854669-s3", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Negotiation in Human", "section": "Humans negotiate everyday in their daily routines. Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011;Lewicki et al., 2011). Research on negotiation has been conducted for almost 60 years in the field of psychology, political science, and communication. It has evolved over the past decades from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000). Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010). Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.\n\nNegotiation is a process by which two or more parties attempt to resolve their opposing interests. Strategy of negotiation can be distributive, such as bargaining (Fershtman, 1990) and integrative, such as maximizing unilateral interests (Bazerman and Neale, 1993), both of which are used in various social situations such as informal, peer to peer, organizational, and diplomatic country to country settings. The implications for enhancing outcomes are thus large and important to understand. Research from psychology demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011).", "filtered_refids": [["b4", "b18", "b47", "b33", "b51", "b3"], ["b43", "b42", "b4", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1734, "num_references": 10}
{"corpusid_sectionid": "254854669-s6", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Integrative Negotiation Datasets", "section": "In integrative negotiations, there is normally more than one issue available to be negotiated. To achieve optimal negotiation goals, the involved players should make trade-offs for multiple issues.\n\nMulti-player Strategy Games The strategy video games provide ideal platforms for people to verbally communicate with other players to accomplish their missions and goals. Asher et al. (2016) propose the STAC benchmark, which is the player dialogue in the game of Catan. In this game, players need to gather resources, including wood, wheat, sheep, and more, with each other to purchase settlements, roads and cities. As each player only has access to their own resources, they have to communicate with each other. To investigate the linguistic strategies used in this situation, STAC also includes an SDRT-styled discourse structure. Boritchev and Amblard (2022) also collect a DinG dataset from French-speaking players in this game. The participants are instructed to focus on the game, rather than talk about themselves. As a result, the collected dialogues can better reflect the negotiation strategy used in the game process.\n\nNegotiation for Item Assignment The item assignment scenarios involve a fixed set of items as well as a predefined priority for each player in the dialogue. As the players only have access to their own priority, they need to negotiate with each other to exchange the items they prefer. Nouri and Traum (2014) propose InitiativeTalking, occurring between the owners of two restaurants. They discuss how to distribute the fruits (i.e., apples, bananas, and strawberries) and try to reach an agreement. Lewis et al. (2017) propose DealorNoDeal, a similar two-party negotiation dialogue benchmark where both participants are only shown their own sets of items with a value for each and both of them are asked to maximize their total score after negotiation. Chawla et al. (2021c) propose CaSiNo, a dataset on campsite scenarios involving campsite neighbors negotiating for additional food, water, and firewood packages. Both parties have different priorities over different items.\n\nNegotiation for Job Interview Another commonly encountered negotiation scenario is job offer negotiation with recruiters. Yamaguchi et al. (2021a) fill this gap and propose the JobInterview dataset. JobInterview includes recruiter-applicant interactions over salary, day off, position, company, and workplace. The participants are shown negotiators' preferences and the corresponding issues and options and are given feedback in the middle of the negotiation.", "filtered_refids": [[], ["b1", "b6"], ["b10", "b34", "b41"], ["b53"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2567, "num_references": 6}
{"corpusid_sectionid": "254854669-s7", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Distributive Negotiation Datasets", "section": "Distributive negotiation is about the discussion over a fixed amount of value (i.e., slicing up the pie). In such negotiation, the involved people normally talk about a single issue (e.g., item price) and therefore, there are hardly trade-offs between multiple issues in such negotiation.\n\nPersuasion For Donation Persuasion, convincing others to take specific actions, is a necessary required skill for negotiation dialogue (Sycara, 1990;Sierra et al., 1997). Wang et al. (2019) focus on persuasion and propose PersuasionforGood, a twoparty persuasion conversations about charity donations. In the data annotation process, the persuaders are provided some persuasion tips and example sentences, while the persuaders are only told that this conversation is about charity. The annotators are required to complete at least ten utterances in a dialogue and are encouraged to reach an agreement at the end of the conversations. Dutt et al. (2020) further extend PersuasionforGood by adding the utterance-level annotations that change the positive and/or the negative face of the participants in a conversation. A face act can either raise or attack the positive face or negative face of either the speaker or the listener in the conversation.\n\nNegotiation For Product Price Negotiations over product prices can be observed on a daily basis. He et al. (2018) propose CraigslistBargain, a negotiation benchmark based on a realistic item price bargaining scenario. In CraigslistBargain, two agents, a buyer and a seller, are required to negotiate the price of a given item. The listing price is available to both sides, but the buyer has a private price as the target. Then two agents chat freely to decide the final price. The conversation is completed when both agents agree with the price or one of the agents quits.  propose Ne-goCoach benchmark on similar scenarios, but with an additional negotiation coach who monitors messages between the two annotators and recommends tactics in real-time to the seller to get a better deal.\n\nUser Privacy Protection Privacy protection of negotiators has become more and more vital. Participant (e.g., attackers and defenders) goals are also conflicting. Li et al. (2020b) propose Anti-Scam benchmark which focuses on online customer service. In Anti-Scam, users try to defend themselves by identifying whether their components are attackers who try to steal sensitive personal information.\n\nAnti-Scam provides an opportunity to study human elicitation strategies in this scenario.", "filtered_refids": [[], ["b52", "b48", "b13", "b46"], ["b24"], ["b37"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2516, "num_references": 6}
{"corpusid_sectionid": "254854669-s9", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Goal-based Metrics", "section": "Goal-oriented metrics mainly consider the agent's proximity to the goal from the perspective of strategy modeling, task fulfillment and sentence realization. Success Rate (SR) is the most widely used, which measures how frequently an agent completes the task within their goals. A similar metric Prediction Accuracy (PA) is to evaluate the agent's strategy predictions or the outcome of negotiations, such as macro or average F1 score (Wang et al., 2019;Dutt et al., 2020;Chawla et al., 2021c). For those scenario-related tasks, Yamaguchi et al.\n\n(2021a) present a task where the model is required to label the human-human negotiation outcomes as either a success or a breakdown, including area under the curve (ROC-AUC), confusion matrix (CM), and average precision (AP). Kornilova et al. (2022) propose a model-based evaluation based on Item Response Theory to analyze the effectiveness of persuasion on the audience. In terms of language realization for negotiation dialogue, Hiraoka et al. (2015) employ a predefined naturalness metric (a bigram overlap between the system responses and the ground-truth responses) as part of the reward to evaluate policies in cooperative persuasive dialogues. Other classical metrics for evaluating the quality of response are also used, i.e., perplexity (PPL), BLEU-2, ROUGE-L, and BOW Embedding-based Extrema matching score (Lewis et al., 2017).", "filtered_refids": [["b52", "b10", "b13"], ["b28", "b34", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1386, "num_references": 6}
{"corpusid_sectionid": "254854669-s10", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Game-based Metrics", "section": "Different from goal-oriented metrics, which focus on evaluating the accuracy of strategies or actions, game-based evaluation provides a user-centric perspective through multi-turn interactions. Keizer et al. (2017) measure the bots' negotiation strategies within the online game \"Settlers of Catan\" by proposing the metrics WinRate (the percentage of games won by the humans when playing with the bot opponents) and AvgVPs (the average number of victory points gained by the human players). He et al. (2018) present a task that two agents bargain to get the best deal using natural language. They use task-specific scores to test the performance of the agents, including utility (a score that is higher when the final price is closer to one agent's expected price), fairness (the difference between two agents' utilities), and length (the number of the sentences exchanged between the two agents).  design a task where a seller and a buyer try to achieve a mutually acceptable price through a natural language negotiation. They adopt different metrics to evaluate the dialogue agent, i.e., average sale-to-list ratio and the task completion rate. Besides, Cheng et al. (2019) propose an adversarial attacking evaluation approach to test the robustness of negotiation systems.", "filtered_refids": [["b24", "b11", "b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1275, "num_references": 3}
{"corpusid_sectionid": "254854669-s11", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Human Evaluation", "section": "To evaluate the users' satisfaction with the dialogue systems, human judgment is employed as a subjective evaluation of the generated output. Hiraoka et al. (2015) use a dialogue system as the salesperson to bargain with the human customers and have the users annotate subjective customer satisfaction (a five-level score), the final decision of making a purchase (a binary number indicating whether persuasion is successful), and the correct response rate in the dialogues. Lewis et al. (2017) employ crowd-sourcing workers to highlight that essential information when bargaining with dialogue systems, covering the percentage of dialogues where both interlocutors finally achieve an agreement, and Pareto optimality, i.e., the percentage of the Pareto optimal solutions in all the agreed deals. He et al. (2018) propose human likeness as a metric in evaluating how well the dialogue system is doing in a bargain. They ask workers to manually score the dialogue agent using a Likert metric to judge whether the agent acts like a real human or not.", "filtered_refids": [["b24", "b34", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1048, "num_references": 3}
{"corpusid_sectionid": "254854669-s14", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Integrative Strategy", "section": "Integrative strategy (known as win-win) modeling aims to achieve mutual gain among participants. For instance, Zhao et al. (2019) proposes to model the discourse-level strategy using a latent action reinforcement learning (LaRL) framework. LaRL can model strategy transition within a latent space. However, due to the lack of explicit strategy labels, LaRL can only analysis strategies in implicit space.\n\nTo resolve the problem, Chawla et al. (2021c) define a series of explicit strategies such as Elicit-Preference, Coordination and Empathy. While Elicit-Preference is a strategy attempting to discover the preference of the opponent, Coordination promotes mutual benefits by explicit offer or implicit suggestion. In order to capture user's preference, Chawla et al. (2022) utilize those strategies using a hierarchical neural model. Besides, Yamaguchi et al. (2021b) present another collaborative strategy set to negotiate workload and salaries during the interview, which goal is to reach an agreement between employer and employee. It assists humans in becoming better negotiators during this process, e.g., communicating politely, addressing concerns, and providing side offers.", "filtered_refids": [["b57"], ["b54", "b10", "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1185, "num_references": 4}
{"corpusid_sectionid": "254854669-s15", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Distributive Strategy", "section": "Distributive strategy (known as win-loss) modeling focuses on achieving one's own goals and maximizing unilateral interests more than mutual benefits. Distributive strategy can be used when you insist on your position or resist the opponent's deal . For example, Dutt et al. (2021a) investigate four resisting categories, namely contesting, empowerment, biased processing, and avoidance (Fransen et al., 2015). Each individual category contains fine-grain strategic behaviors. For example, contesting refers to attacking the message source, and empowerment implies reinforcing personal preference to contradict a claim (Attitude Bolstering) or attempting to arouse guilt in the opponent (Self Pity). Besides, Wang et al. (2019) design a set of persuasion strategies to persuade others to donate to charity. It contains 10 different strategies containing logical appeal, emotional appeal, source-related inquiry and etc. Li et al. (2020a) explore the role structure to enhance the strategy modeling. Dutt et al. (2020) further enhances the role modeling with facing act, which helps utilize strategy between asymmetric roles.", "filtered_refids": [["b52", "b14", "b13", "b35", "b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1124, "num_references": 5}
{"corpusid_sectionid": "254854669-s18", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Mind Modeling", "section": "Mind modeling in negotiation dialogue systems encompasses several tasks, such as mind preference estimation and opponent response prediction. Mind preference estimation helps the agent infer the intention of the opponents and guess how their own utterances would affect the opponent's mental preference. Nazari et al. (2015) propose a heuristic frequency-based method to estimate the negotiator's preference. Langlet and Clavel (2018) consider a rule-based system incorporating linguistic features to identify user's preference. A critical challenge for mind modeling in negotiation is that it usually requires complete dialogues, so it is difficult to predict those preferences precisely for partial dialogue. To make it applicable for those partial dialogues, which is widespread in real-world applications, Chawla et al. (2022) formulated mind preference estimation as a ranking task and proposed a transformer-based model that can be trained directly on partial dialogue. In terms of opponent response prediction, He et al. (2018) firstly propose to decouple the modeling of the strategy of generation containing a parser to map utterances with dialogue acts and a dialogue manager to predict the skeleton of dialogue acts. Yang et al. (2021) further improve the negotiation system with a first-order model based on the theory of Mind (Frith and Frith, 2005) , which allows the agents to compute an expected value for each mental state. They provided two variance variants of ToM-based dialogue agents: explicit and implicit, which can fit both pipeline and end-to-end systems.", "filtered_refids": [["b40", "b9", "b20", "b55", "b30", "b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1581, "num_references": 6}
{"corpusid_sectionid": "254854669-s22", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Reinforcement Learning", "section": "Reinforcement learning (RL) is one of the most common frameworks chosen for policy learning. English and Heeman (2005) are the first to use RL techniques for negotiation dialogue systems. They employed a single-agent pattern to learn the policy of two opponents individually. But singleagent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment. Therefore, Georgila et al. (2014) further advances the framework with concurrent progress using multi-agent RL techniques, which simultaneously model two parties and provide a way to deal with multi-issues scenarios. Besides, Keizer et al. (2017) propose to learn the action of the target agents with a Qlearning reward function. They further propose a method based on hand-crafted rules and a method using Random Forest trained on a large human negotiation corpus from (Afantenos et al., 2012).\n\nMost recent works try to equip RL with deep learning techniques. For instance, Zhang et al. (2020) propose OPPA, which lets the target agent behave given the system actions. The system actions are predicted and conditioned on the target agent's actions. The reward of the actions for the target agent is obtained by predicting a structured output given the whole dialogue. Besides, Shi et al. (2021) use a modular framework containing a language model to generate responses, a response detector would automatically annotate the response with a negotiation strategy, and an RL-based reward function to assign a score to the strategy. Instead of the modular framework which separates policy learning and response generation, Gao et al. (2021) propose an integrated framework with deep Q-learning, which includes multiple channel negotiation skills. It allows agents to leverage parameterized DQN to learn a comprehensive negotiation strategy that integrates linguistic communication skills and bidding strategies.", "filtered_refids": [["b16", "b22", "b0", "b27"], ["b21", "b44", "b56"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1931, "num_references": 7}
{"corpusid_sectionid": "254854669-s23", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Supervised Learning", "section": "Supervised learning (SL) is another popular paradigm for policy learning. (Lewis et al., 2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data. However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as i) the utility function of the final price for the buyer and seller ii) the difference between two agents' utilities iii) the number of utterances in the dialogue. Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy. In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm. Recently, Dutt et al. (2021b) propose a generalised framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019).", "filtered_refids": [["b12", "b34", "b26", "b59", null, "b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1213, "num_references": 6}
{"corpusid_sectionid": "232320269-s1", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "date": "2021-03-16", "section_title": "Factuality", "section": "Veracity of information has been studied at different levels: (i) claim-level (e.g., fact-checking), (ii) article-level (e.g., \"fake news\" detection), (iii) user-level (e.g., hunting for trolls), and (iv) medium-level (e.g., source reliability estimation). Our primary interest here is in the latter.\n\nAt the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim [Castillo et al., 2011;Kochkina et al., 2018]. A set of web pages and snippets from search engines have also been used as a source of information [Karadzhov et al., 2017]. In either case, the most important information for the claimlevel tasks are stance (does a tweet or a news article agree or disagree with the claim?) and source reliability (do we trust the user who posted the tweet or the medium that published the news article?).\n\nThe problem of source reliability remains largely underexplored.\n\nIn the case of social media and community fora, it concerns modeling the user. In particular, there has been research on finding opinion manipulation trolls, paid [Mihaylov et al., 2015a] or just perceived [Mihaylov et al., 2015b], sockpuppets [Maity et al., 2017], Internet water army [Chen et al., 2013], and seminar users [Darwish et al., 2017]. In the case of the Web, it is about the trustworthiness of the source (the URL domain, the medium). The latter is our focus here.\n\nIn early work, the source reliability of news media has often been estimated automatically based on the general stance of the target medium with respect to known true/false claims, without access to gold labels about the overall medium-level factuality of reporting [Mukherjee and Weikum, 2015].\n\nMore recent work has addressed the task as one on its own right. [Baly et al., 2018] used gold labels from Media Bias/Fact Check, 1 and a variety of information sources: articles published by the medium, what is said about it on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, [Baly et al., 2019] used the same representation to jointly predict media factuality and bias on an ordinal scale, using a multi-task ordinal regression setup. Finally, [Baly et al., 2020b] extended the information sources to include Facebook followers and speech signals from the news medium's channel on YouTube (if any). Finally, [Hounsel et al., 2020] proposed to use domain, certificate, and hosting information of the website infrastructure.", "filtered_refids": [[], ["b19", "b10"], [], ["b14", "b12", "b22"], ["b22"], ["b18", "b2", "b5", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2548, "num_references": 10}
{"corpusid_sectionid": "232320269-s3", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "date": "2021-03-16", "section_title": "A Variety of Dimensions in Media Bias", "section": "Compared to factuality, which is decided by whether a piece of information is true or not, media bias has more complex dimensions. For the last few decades, many scholars have conceptualized media bias in different ways. For instance, a bias can be defined as \"imbalance or inequality of coverage rather than as a departure from truth\" [Stevenson et al., 1973]. They 1 http://mediabiasfactcheck.com particularly note that a departure from truth, as a bias, can be measured only when the accurate record of the event is available (e.g., trial transcript).\n\nA different definition, \"Any systematic slant favoring one candidate or ideology over another\" [Waldman and Devitt, 1998], is proposed to capture various other dimensions rather than coverage imbalance, such as favorability conveyed in visual representations (i.e., news photos). For example, smiling, speaking at the podium, cheering crowd, and eye-level shots are preferred over frowning, sitting, being alone, and shots from above, respectively.\n\nD'Alessio and Allen reviewed 59 quantitative studies about partisan media bias in presidential elections [D'Alessio and Allen, 2000], and based on this analysis, they proposed to categorize media bias into the following three types: (i) gatekeeping bias, where editors and journalists 'select' the stories to report, (ii) coverage bias, where the amount of news coverage (e.g., the length of newspapers articles, or the time given on television) each party receives is systematically biased to one party at the expense of the other one, and (iii) statement bias, where news media interject their attitudes or opinions in the news reporting. Groeling proposed a more relaxed concept of media bias, which is \"a portrayal of reality that is significantly and systematically (not randomly) distorted,\" to take a variety of media bias dimensions into account [Groeling, 2013]. In particular, he focused on two main forms of media bias-selection bias (i.e., what to cover) and presentation bias (i.e., how to cover it)-driven by the choices of newsmakers.\n\nSelection bias or gatekeeping bias, has been studied in various ways, including qualitative interviews or surveys of journalists and editors about the decision making process they use to select the news stories in their newsroom [Tandoc Jr, 2014]. Data-driven research on selection bias follows the common steps: (i) collect news articles (for newspapers or online news) or transcripts (for television news) for a target period, (ii) conduct content analysis to find the news coverage of politicians, parties, events, etc. Sometimes the tone of the news articles can be studied (i.e., negative news stories are more frequently reported or selected by the editors compared to positive news) [Soroka, 2012], and (iii) identify systematic biases by comparing their news coverage. An exhaustive database of news stories is, thus, essential for selection bias research. While commercial databases, such as Lexis Nexis, have been widely used [Soroka, 2012], publicly available datasets, such as GDELT or Google News, start to get attention [Kwak et al., 2014;Boudemagh and Moise, 2017;Kwak et al., 2018] and are getting validated by comparing multiple sources [Weaver and Bimber, 2008;Kwak et al., 2016].", "filtered_refids": [[null], ["b25"], ["b16", "b13"], ["b25", null, "b19", "b23", "b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 3255, "num_references": 9}
{"corpusid_sectionid": "232320269-s5", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "date": "2021-03-16", "section_title": "Framing Bias", "section": "Framing refers to a process that highlights a certain aspect of an event or an issue more than the others [Entman, 1993]. Emphasizing an issue's particular aspect can deliver a distorted view toward the issue even without the use of biased expressions.\n\nFraming biases have been typically studied at issue level. Researchers collect news articles about a particular issue or event, conduct manual content analysis on them, and build a frame detection model [Baumer et al., 2015]. Although this approach successfully characterizes diverse frames, it is not trivial to compare media's framing across different issues.\n\nThe Media Frames Corpus (MFC) was proposed to address this limitation. It contains articles annotated with 15 generic frames (including others) across three policy issues [Card et al., 2015]. Several studies have demonstrated reasonable prediction performance of the general media frames with different datasets [Field et al., 2018;. These 15 general frames were also used for identifying frames in political discourse on social media [Johnson et al., 2017]. General media frames are often customized to a specific issue by adding issue-specific frames [Liu et al., 2019], even though doing so somewhat contradicts the original motivation of using general media frames, namely to be able to compare frames across various issues.", "filtered_refids": [["b14"], ["b8"], [null, "b19", "b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1345, "num_references": 5}
{"corpusid_sectionid": "232320269-s6", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "date": "2021-03-16", "section_title": "News Slant", "section": "As a related concept to framing, news slant was proposed to characterize how the framing in news reports favors one side over the other [Entman, 2007]. The media-level slant thus could be different across issues [Ganguly et al., 2020].\n\nA variety of methods have been proposed to quantify the extent of news slant in traditional news media by (i) linking media outlets to politicians with known political positions, (ii) directly analyzing news content, and (iii) using shared audience among media outlets. For example, Groseclose and Milyo assigned an ADA (Americans for Democratic Action) score for each media outlet by investigating cocitations of think-tanks by members of Congress and media outlets [Groseclose and Milyo, 2005]. Genzkow and Shapiro proposed an ideological slant index of news media in a seminal study [Gentzkow and Shapiro, 2010]. The news slant is measured by the extent of phrases in news coverage that are more frequently used by one political party (i.e., Democratic or Republican) congress members than by another one in the 2005 Congress Record. Their frequency-based approach successfully finds politically charged phrases such as death tax or war on terror Republicans and associated media and estate tax or war in Iraq by Democrats and associated media, and they further computed media a slant index for 433 newspapers. The choice of words by political party members and news media can be considered framing as well because they purposely highlight some aspect of the issue over other ones. An et al. proposed a method to compute media slant scores by measuring distances between media sources by their mutual followers on Twitter and mapping them to a two dimensional space [An et al., 2011;. [Stefanov et al., 2020] identified the political leanings of media outlets and influential people on Twitter based on their stance on controversial topics. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using a procedure proposed in [Darwish et al., 2020]. ", "filtered_refids": [["b14"], ["b14", "b16", "b1", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2043, "num_references": 5}
{"corpusid_sectionid": "232320269-s8", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "date": "2021-03-16", "section_title": "Textual Content Representation", "section": "The most natural representation for a source is as a sample of articles it has published, which in turn can be represented using linguistic features or as continuous representations. Linguistic Features: These features focus on language use, and they have been shown to be useful for detecting fake articles, as well as for predicting the political bias and the factuality of reporting of news media [Horne et al., 2018;Baly et al., 2018]. For example, [Horne and Adali, 2017] showed that \"fake news\" pack a lot of information in the title (as many people do not read beyond the title, e.g., in social media), and use shorter, simpler, and repetitive content in the body (as writing fake information takes a lot of effort). Such features can be generated based on the Linguistic Inquiry and Word Count (LIWC) lexicon and used to distinguish articles from trusted sources vs. hoaxes vs. satire vs. propaganda [Pennebaker et al., 2001]. They can be also modeled using linguistic markers [Mihaylova et al., 2018] such as factives from [Hooper, 1975], assertives from [Hooper, 1975] , 2003], and sentiment cues from [Liu et al., 2005]; see Table 1 for examples. There are 141 such features implemented in the NELA toolkit [Horne et al., 2018], grouped in the following categories:\n\n\u2022 Style: part-of-speech tags, use of specific words (function words, pronouns, etc.), and features for clickbait title classification;\n\n\u2022 Complexity: type-token ratio, readability, number of cognitive process words (identifying discrepancy, insight, certainty, etc.); \u2022 Bias: features modeling bias using lexicons and subjectivity as calculated using pre-trained classifiers; \u2022 Affect: sentiment scores from lexicons and full systems; \u2022 Morality: features based on the Moral Foundation Theory [Graham et al., 2009] and lexicons; \u2022 Event: features modeling time and location.\n\nEmbedding representations: An alternative way to represent an article is to use embedding representations, typically based on BERT [Devlin et al., 2019]. This can be done without fine-tuning, e.g., by encoding an article (possibly truncated, e.g., BERT can take up to 512 tokens as an input) and then averaging the word representations extracted from the second-to-last layer. 2 Alternatively, one can use pre-trained sentence encoders such as Sentence BERT [Reimers and Gurevych, 2019] or the Universal Sentence Encoder (USE) [Cer et al., 2018]. Finally, one can obtain representations that are relevant to the target task, e.g., by fine-tuning BERT to predict the label (bias or factuality) of the medium that an article comes from, in the form of distant supervision [Baly et al., 2020b]. One issue with distant supervision is that the model can end up learning to detect the source of the target news article instead of predicting its factuality/bias, which can be fixed using adversarial media adaptation and a specially adapted triplet loss [Baly et al., 2020a].", "filtered_refids": [["b18", null, "b22", "b2", "b17"], [], ["b15"], ["b11", "b4", "b14", null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2922, "num_references": 11}
{"corpusid_sectionid": "232320269-s10", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "date": "2021-03-16", "section_title": "Multimedia Content", "section": "We have come to understand events in a far more visual way than we have ever before. As a result, multimedia content is now heavily relied upon as a source of news and opinion and has been an important element of almost all news websites. This dependence, however, also makes multimedia a very effective means for dispensing an intended, and even manipulated, message. The increasing availability of automated and AI-powered multimedia editing and synthesis tools, combined with massive computational power, makes such capabilities accessible to everyone.\n\nGiven that multimedia editors of a news site typically follow a defined workflow when creating, acquiring, editing, and curating content for their pages, this pattern thus adds a crucial dimension to profiling the factuality and the bias of a news source. In fact, questions around the origin and the veracity of photographic images and videos have long been the subject of multimedia forensics research [Sencar et al, 2013].\n\nWith , and discrimination of synthesized (i.e., GAN generated) media [Agarwal et al., 2020;Verdoliva, 2020]. Currently, these capabilities have only been sparsely explored in the context of predicting factuality and bias.\n\nExisting work mainly considered characterization of images appearing at trustworthy sources and such obtained from low-factuality news sources. These methods have proposed to use visual characteristics of images [Jin et al., 2016] ", "filtered_refids": [[], [null], ["b25", "b0"], ["b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1438, "num_references": 4}
{"corpusid_sectionid": "232320269-s11", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "date": "2021-03-16", "section_title": "Audience Homophily", "section": "The well-known homophily principle, \"birds of a feather flock together,\" crucially asserts that similar individuals interact with each other at a higher rate than dissimilar ones. Therefore, audience representation could be another approach to describe a news media outlet whereby an overall, descriptive characteristic of followers of the outlet is obtained. Then, by evaluating the similarity of audience-centric representations with previously categorized news media, the factuality and the bias of the medium in question can be inferred.\n\n[ Ribeiro et al., 2018] used Facebook's targeted advertising tool to infer the ideological leaning of online media based on the political leaning of the users who interacted with these media, according to Facebook. [An et al., 2012] relied on follow relationships on Twitter to ascertain the ideological leaning of news media and users. [Wong et al., 2013] studied retweet behavior to infer the ideological leanings of online media sources and popular Twitter accounts. [Barber\u00e1, 2015] proposed a statistical model based on the follower relationships to media sources and Twitter personalities to estimate their ideological leaning.\n\n[ Stefanov et al., 2020] predicted the political leaning of media with respect to a topic by observing the users of which side of the debate on a polarizing topic were sharing content from which media in support of their position in the context of that debate. In particular, they constructed a user-media graph and then used label propagation and graph neural networks to derive representations for media, which they used for classification. They further aggregated the leanings across several polarizing topics to come up with a left-center-right polarization prediction.\n\nFollowing a similar approach, [Baly et al., 2020b] considered three social media platforms for audience characterization. On Twitter, they proposed to use self-descriptions in publicly accessible profiles of users following the account of a medium. For each medium, a representation is obtained by encoding the biographic descriptions of Twitter followers and averaging the resulting textual representations. The second characterization involves how the audience of the medium's YouTube channel responds to each video in terms of number of comments, views, likes and dislikes. By averaging these statistics over all videos, a medium-level representation is obtained. The last audience representation is obtained using Facebook's advertising platform, which is used to obtain demographic information for the audience interested in each medium. This data is used to obtain the audience distribution over the political spectrum. The distribution is then divided into five categories to label each medium accordingly: very conservative, conservative, moderate, liberal, and very liberal.", "filtered_refids": [[], ["b6", "b1", "b22", "b25"], [null], ["b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2835, "num_references": 6}
{"corpusid_sectionid": "232320269-s12", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "date": "2021-03-16", "section_title": "Infrastructure Characteristics", "section": "Beyond textual, visual, and audience features, news sites also exhibit distinct characteristics that relate to the underlying infrastructure and technological components deployed to serve their content online. In this regard, the prediction problem is analogous to a well-studied one in the cybersecurity domain where the goal has been to identify infrastructure characteristics of malicious domains [Anderson et al., 2007;Invernizzi et al., 2014]. Since establishing the infrastructure of a news medium involves several decisions with respect to technological aspects, it is plausible to expect that news media with varying IT practices and different levels of access to IT human resources will differ in their characteristics.\n\nSo far, only a few works exploited this dimension with a focus on network, web design, and data elements of a news website to essentially discriminate new sites based on factuality and bias. At the network level, [Hounsel et al., 2020] considered a comprehensive set of features that relate to a website's domain, certificate, and hosting properties. In a classification setting with three classes (i.e., disinformation websites, authentic websites, and sites not related to news or politics), their results showed that features related to a website's domain name, registration, and DNS configuration serve as best predictors for classification. Concerning the web design aspect, [Castello et al., 2019] introduced a web page classifier based on several features that govern the structure and style of a page in addition to three categories of linguistic features. Their binary classification results (real or fake news) obtained on several datasets showed that the web-markup features consistently perform well and are complementary to linguistic features. Lastly, at the data level, [Fairbanks et al., 2018] examined the source of web pages to identify shared data objects, such as mutually linked sites, scripts, and images, across web sites. This information is then used to create a shared data object graph. By comparing the content level features with the structural properties of the graph, they found that the use of mutually shared objects yields better performance in predicting both factuality and bias of a site with a significant difference in the former task. Overall, a major advantage of utilizing infrastructure features comes from their content-and audienceagnostic nature. Because of this, they allow making reliable predictions when only limited textual and visual content is available and without an established audience interest in a news medium.", "filtered_refids": [["b2", "b18"], ["b14", "b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2599, "num_references": 4}
{"corpusid_sectionid": "232320269-s14", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "date": "2021-03-16", "section_title": "Major Challenges", "section": "Ordinal scales: While the ideological bias (news slant) is typically modeled as left-center-right, there exists a spectrum within each bias based on bias intensity. A hyperpartisan (an extreme partisan) bias prediction task has been tested to differentiate far-right from right and far-left from left, but it does not model the political bias using an ordinal scale. Difficulties in labeling the bias (i.e., creating ground-truth datasets) by experts or crowdsourcing is a major hurdle for modeling ideological bias as an ordinal variable.\n\nJoint modeling: There is a well-known connection between factuality and bias. 3 For example, hyper-partisanship is often linked to low trustworthiness, e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and trustworthy [Baly et al., 2019]. Thus, it makes sense to model factuality and bias jointly.\n\nMultimodality: In news reporting, a news photo typically gets high attention from readers. The fact that readers sometimes can understand news stories from news photos onlyeven without reading text-indicates that news text and photos are strongly coupled together and deliver relevant information about news stories to readers. Thus, there should be a benefit from modeling news text and photos together to understand their bias and factuality.\n\nEvaluation granularity: The label of a news medium is essentially inferred from a sample of observations. This potentially introduces a measurement bias when a news medium does not exhibit the same reporting behavior with all news items it publishes. This is especially the case for media that have a particular stance in only certain issues [Ganguly et al., 2020]. Thus, reliable estimation of factuality and bias labels require analyzing a relatively large amount of content covering a range of issues.\n\nVariability in factuality & bias ratings: These ratings are inherently not static and may change over time when a news medium takes corrective action to address issues raised by fact-checkers. In other words, the ground truth needed for building a learning approach varies, triggering the need for reevaluating the performance of proposed approaches. Therefore, there is also a need to take into account the sensitivity of a learning approach to such small but nevertheless inevitable variations.\n\nDataset size: The existing datasets for media-level factuality and bias are relatively small in size, typically of a few hundred examples, sometimes a few thousand. These are derived from sites, such as Media Bias/Fact Check and All-Sides, where domain experts perform a careful manual analysis based on clear guidelines.\n\nAnnotation vs. modeling: One problem is that human annotators judge the factuality of reporting and the bias of media based on criteria that are not easy to automate or based on information that may not be accessible to automatic systems. For example, if a news outlet is judged to be of mixed factuality based on it having failed just 2-3 fact-checks, for an automatic system to arrive at the same conclusion using the same idea, it would have to select for analysis the exact same articles where the false claims were made. 3 http://www.poynter.org/fact-checking/media-literacy/2021/should-you-trust-media-bias-charts/ Data availability: Primarily due to copyright issues, there are only a few publicly available datasets of the full text of news for research purposes. Instead, indexed data (e.g., GDELT dataset 4 ) by mentioned actors, events, locations, sources, or tones are available and have been analyzed in many studies. A set of news headlines collected from news websites or aggregated websites (e.g., AllSides 5 ) are also shared more actively for research purposes. Considering the importance of social media channels in news dissemination, researchers collect and analyze social media posts of official accounts of news media. As social media posts are relatively more informal than news articles to fit for social media audience [Park et al., 2021], more studies are required for understanding their biases and factuality correctly.", "filtered_refids": [[], ["b3"], [], ["b14"], [], [], ["b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4129, "num_references": 3}
{"corpusid_sectionid": "264439179-s3", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Metrics", "section": "Previous studies (Mitchell et al., 2023;Sadasivan et al., 2023) predominantly used the Area Under the Receiver Operating Characteristic (AUROC) score to gauge the effectiveness of detection algorithms.As a binary classification problem, AUROC shows the results under different thresholds, and the F1 score is also helpful.Krishna et al. (2023); Yang et al. (2023b) suggest that AUROC may not consistently provide a precise evaluation, particularly as the AUROC score nears the optimal limit of 1.0 since two detectors with identical AUROC score of 0.99 could exhibit substantial variations in detection quality from a user's perspective.From a practical point of view, ensuring a high True Positive Rate (TPR) is imperative while keeping the False Positive Rate (FPR) to a minimum.As such, current research (Krishna et al., 2023;Yang et al., 2023b) both report TPR scores at a fixed 1% FPR, along with the AUROC.Other work (Sadasivan et al., 2023) also refer to Type I and Type II errors following the binary hypothesis test and even report TPR at 10 \u22126 FPR (Fernandez et al., 2023).", "filtered_refids": [["b57", "b92", "b78", "b30", "b127"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1083, "num_references": 5}
{"corpusid_sectionid": "264439179-s4", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Datasets", "section": "In this section, we discuss the common datasets used for this task.The corpus is usually adopted from previous NLP tasks, and reconstructed by prompting LLMs to generate new outputs as candidate machine-generated text.Usually, there are two prompting methods: 1).prompting LLMs with the questions in some question-answering datasets.2).prompting LLMs with the first 20 to 30 tokens to continue writing in datasets without specific questions.Specifically, several datasets have been compiled and utilized in the field.Some noteworthy datasets include TURINGBENCH (Uchendu et al., 2021), HC3 (Guo et al., 2023), CHEAT (Yu et al., 2023a), Ghostbuster (Verma et al., 2023), OpenGPTText (Chen et al., 2023), M4 (Wang et al., 2023d), MGTBench (He et al., 2023), and MULTI-TuDE (Macko et al., 2023) and some other datasets not explicitly built for detection have also been used, such as C4 (Raffel et al., 2019), shareGPT2 , and alpaca (Taori et al., 2023), as summarized in Table 1.For text detection, we only list datasets explicitly built for detection, while some general datasets like C4 (Raffel et al., 2019) or alpaca (Taori et al., 2023) can also be used.For code detection, we only list datasets that have been used in previous code detection work (Lee et al., 2023;Yang et al., 2023e).And other codegeneration corpora can also be adopted.The detailed description is included in Appendix A.3.\n\nData Contamination.Despite those released standard datasets, we argue that static evaluation benchmarks might not be desirable for this problem with the rapid progress of LLMs trained, tuned, or aligned on large amounts of data across the whole internet.On the one hand, Aaronson (2022) mentioned that some text from Shakespeare or the Bible is often classified as AI-generated because such classic text is frequently used in the training datasets for generative language models.On the other hand, many detectors did not fully disclose their training data, especially commercial tools like GPTZero (Tian, 2023).It is natural to worry that those standard evaluation benchmarks would face a serious test data contamination problem, considering the commercial detectors would consistently improve their products for profits.So, with the rapid evolution of LLMs and detectors, the traditional paradigm of providing standard benchmarks might no longer be suitable for AI-generated text detection.We provide a unique solution to this:\n\nUtilize the most latest human-written content to reduce data contamination problem by collecting such content from the most updated open-source websites, which themselves explicitly forbid posting AI-written posts.", "filtered_refids": [["b115", "b86", "b63", "b103", "b130", "b41", "b134", "b88", "b129", "b74", "b38", "b110"], ["b104", "b0"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2640, "num_references": 14}
{"corpusid_sectionid": "264439179-s15", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Zero-Shot", "section": "In the zero-shot setting, we do not require extensive training data to train a discriminator.Instead, we can leverage the inherent distinctions between machine-generated and human-written text, making the detector training-free.The key advantage of training-free detection is its adaptability to new data distributions without the need for additional data collection and model tuning.It's worth noting that while watermarking methods can also be considered zero-shot, we treat them as an independent track.Previous work utilizes entropy (Lavergne et al., 2008), average log-probability score (Solaiman et al., 2019), perplexity (Beresneva, 2016), uncommon n-gram frequencies (Grechnikov et al., 2009;Badaskar et al., 2008) obtained from a language model as the judge for determining its origin.However, those simple features fail as LLMs are becoming diverse and high-quality text generators.\n\nSimilarly, there are also black-and white-box detection, as summarized below.", "filtered_refids": [["b98", "b9", "b62", "b5", "b36"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 971, "num_references": 5}
{"corpusid_sectionid": "264439179-s16", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Black-Box", "section": "When the source of the black-box model is known, DNA-GPT (Yang et al., 2023b) achieves superior performance by utilizing N-Gram divergence between the continuation distribution of re-prompted text and the original text.Besides, DetectGPT (Mitchell et al., 2023) also investigates using another surrogate model to replace the source model but achieves unsatisfactory results.In contrast, Mireshghallah et al. (2023) proves that a smaller surrogate model like OPT-125M (Zhang et al., 2022) can serve as a universal black-box text de-tector, achieving close or even better detection performance than using the source model.Additionally, Krishna et al. (2023) suggests building a database of generated text and detecting the target text by comparing its semantic similarity with all the text stored in the database.Finally, Detect-GPT4Code (Yang et al., 2023e) also investigates detecting codes generated by ChatGPT through a proxy small code generation models by conditional probability divergence and achieves significant improvements on code detection tasks.\n\nWhen the source of the model is unknown, PHD (Tulchinskii et al., 2023) observes that real text exhibits a statistically higher intrinsic dimensionality compared to machine-generated texts across various reliable generators by employing the Persistent Homology Dimension Estimator (PHD) as a means to measure this intrinsic dimensionality, combined with an additional encoder like Roberta to facilitate the estimation process.", "filtered_refids": [["b57", "b78", "b130", "b127", "b77", "b139"], ["b108"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1485, "num_references": 7}
{"corpusid_sectionid": "264439179-s17", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "White-Box", "section": "When the partial access to the model is given, traditional methods use the features such as entropy (Lavergne et al., 2008), average log-probability score (Solaiman et al., 2019) for detection.However, these approaches struggle to detect text from the most recent LLMs.Then, the pioneer work De-tectGPT (Mitchell et al., 2023) observes that LLMgenerated text tends to occupy negative curvature regions of the model's log probability function and leverages the curvature-based criterion based on random perturbations of the passage.DNA-GPT (Yang et al., 2023b) utilizes the probability difference between the continuous distribution among re-prompted text and original text and achieves state-of-the-art performance.Later, Deng et al. (2023) improves the efficiency of DetectGPT with a Bayesian surrogate model by selecting typical samples based on Bayesian uncertainty and interpolating scores from typical samples to other ones.Furthermore, similar to DNA-GPT (Yang et al., 2023b) on using the conditional probability for discrimination, Fast-DetectGPT (Bao et al., 2023) builds an efficient zero-shot detector by replacing the probability in DetectGPT with conditional probability curvature and witnesses significant efficiency improvements.Additionally, GPT-who (Venkatraman et al., 2023) utilizes Uniform Information Density (UID) based features to model the unique statistical signature of each LLM and human author for accurate authorship attribution.\n\nWhen the full access to the model is given, Su et al. (2023a) leverages the log-rank information for zero-shot detection through one fast and efficient DetectLLM-LRR (Log-Likelihood Log-Rank ratio) method, and another more accurate DetectLLM-NPR (Normalized perturbed log rank) method, although slower due to the need for perturbations.", "filtered_refids": [["b98", "b78", "b127", "b23", "b62", "b7", "b114"], ["b99"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1795, "num_references": 8}
{"corpusid_sectionid": "264439179-s18", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Watermarking", "section": "Text watermarking injects algorithmically detectable patterns into the generated text while ideally preserving the quality and diversity of language model outputs.Although the concept of watermarking is well-established in vision, its application to digital text poses unique challenges due to the text's discrete and semantic-sensitive nature (Kutter et al., 2000).Early works are edit-based methods that modify a pre-existing text.The earliest work can be dated back to Atallah et al. ( 2001), which designs a scheme for watermarking natural language text by embedding small portions of the watermark bit string in the syntactic structure of the text, followed by paraphrasing (Atallah et al., 2003), syntax tree manipulations (Topkara et al., 2005;Meral et al., 2009) and synonym substitution (Topkara et al., 2006).Besides, text watermarking has also been used for steganography and secret communication (Fang et al., 2017;Ziegler et al., 2019;Abdelnabi and Fritz, 2021), and intellectual property protection (He et al., 2022a,b;Zhao et al., 2022Zhao et al., , 2023b)), but this is out the scope of this work.In light of growing ethical considerations, text watermarking has been increasingly used to ascertain the origin of textual content and detect AI-generated content (Grinbaum and Adomaitis, 2022).The primary focus of this paper is on the use of text watermarking to detect AI-generated text.\n\nIn general, watermarking for text detection can also be classified into white-box and black-box watermarking.Watermarking is designed to determine whether the text is coming from a specific language model rather than universally detecting text generated by any potential model.As such, knowledge of the model source is always required in text watermarking for detection.", "filtered_refids": [["b4", "b141", "b105", "b29", null, "b145", "b142", "b61", "b1", "b76", "b37", "b106"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1775, "num_references": 12}
{"corpusid_sectionid": "264439179-s19", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Black-Box Watermarking", "section": "In black-box setting, such as API-based applications, the proprietary nature of the language models used by LLM providers precludes downstream users from accessing the sampling process for commercial reasons.Alternatively, a user may wish to watermark human-authored text via postprocessing.In such cases, black-box watermarking aims to automatically manipulate generated text to embed watermarks readable by third parties.Traditional works designed complex linguistic rules such as paraphrasing (Atallah et al., 2003), syntax tree manipulations (Topkara et al., 2005;Meral et al., 2009) and synonym substitution (Topkara et al., 2006), but lack scalability.Later work turns to pretrained language models for efficient watermarking.For example, Yang et al. (2022) proposes a natural language watermarking scheme based on contextaware lexical substitution (LS).Specifically, they employ BERT (Devlin et al., 2019) to suggest LS candidates by inferring the semantic relatedness between the candidates and the original sentence.Yang et al. (2023a) first defines a binary encoding function to compute a random binary encoding corresponding to a word.The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 is approximately 0.5.To inject a watermark, they alter the distribution by selectively replacing words representing bit-0 with contextbased synonyms that represent bit-1.A statistical test is then used to identify the watermark.", "filtered_refids": [["b4", "b105", null, "b126", "b76", "b125", "b106"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1512, "num_references": 7}
{"corpusid_sectionid": "264439179-s20", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "White-Box Watermarking", "section": "The most popular 1 training-free watermark directly manipulates the decoding process when the model is deployed.In the efforts of watermarking GPT outputs, Aaronson (2022) works with Ope-nAI to first develop a technique for watermarking language models using exponential minimum sampling to sample text from the model, where the inputs to the sampling mechanism are a hash of the previous k consecutive tokens through a pseudorandom number generator.By Gumbel Softmax (Jang et al., 2016) rule, their method is proven to ensure guaranteed quality.Besides, Christ et al. (2023) provides the formal definition and construction of undetectable watermarks.Their cryptographically inspired watermark design proposes watermarking blocks of text from a language model by hashing each block to seed a sampler for the next block.However, there are only theoretical concepts for this method without experimental results.Another pioneering work of training-free watermark (Kirchenbauer et al., 2023a) embeds invisible watermarks in the decoding process by dividing the vocabulary into a \"green list\" and a \"red list\" based on the hash of prefix token and subtly increases the probability of choosing from the green list.Then, a third party, equipped with knowledge of the hash function and random number generator, can reproduce the green list for each token and monitor the violation of the green list rule.Subsequently, Zhao et al. (2023a) simplifies the scheme by consistently using a fixed green-red list split, showing that the new watermark persists in guaranteed generation quality and is more robust against text editing.Kuditipudi et al. ( 2023) create watermarks that are distortion-free by utilizing randomized watermark keys to sample from token probability distribution by inverse transform sampling and exponential minimum sampling.Hou et al. (2023) propose a sentence-level semantic watermark based on locality-sensitive hashing (LSH), which partitions the semantic space of sentences.The advantage of this design is its enhanced robustness against paraphrasing attacks.DiPmark (Wu et al., 2023b) is an unbiased distribution-preserving watermark that preserves the original token distribution during watermarking and is robust to moderate changes of tokens by incorporating a novel reweight strategy, combined with a hash function that assigns unique i.i.d.ciphers based on the context.Drawn on the drawbacks of random green-red list splitting, Fu et al. (2023) uses input sequence to get semantically related tokens for watermarking to improve certain conditional generation tasks.\n\nDespite training-free watermarking, text watermarks can also be injected through pre-inference training or post-inference training: 2 trainingbased watermark.One example of pre-inference training is REMARK-LLM (Zhang et al., 2023), which injects the watermark by a message encoding module to generate a dense token distribution, following a message decoding module to extract messages from the watermarked textual and reparameterization is used as a bridge to connect the dense distribution with tokens' one-hot encoding.The drawback is that training is required on source data and might not generalize well to unseen text data.On the contrary, post-inference training involves adding a trained module to assist in injecting watermarks during inference.For instance, Liu et al. (2023a) proposes a semantic invariant robust watermark for LLMs, by utilizing another embedding LLM to generate semantic embeddings for all preceding tokens.However, it is not training-free since these semantic embeddings are transformed into the watermark logits through their trained watermark model.\n\nDespite from 0-bit watermark, there is also 3 multi-bit watermarking.For example, Yoo et al. (2023a) designs a multi-bit watermarking following a well-known proposition from image watermarking that identifies natural language features invariant to minor corruption and proposes a corruptionresistant infill model.COLOR (Yoo et al., 2023b) subsequently designs another multi-bit watermark by embedding traceable multi-bit information during language model generation while allowing zero-bit detection simultaneously.Fernandez et al. (2023) also consolidates watermarks for LLMs through more robust statistical tests and multi-bit watermarking.", "filtered_refids": [["b140", "b54", "b124", "b20", "b31", null, "b51", "b0"], ["b69", null], ["b133", "b132", "b30"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4311, "num_references": 13}
{"corpusid_sectionid": "264439179-s23", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Paraphrasing Attack", "section": "Paraphrasing could be performed by human writers or other LLMs, and even by the same source model.Paraphrasing can also undergo several rounds, influenced by a mixture of different models.Current research mostly focuses on the simple paraphrase case where another model rewrites a machine-generated text for one round.For instance, Krishna et al. (2023) trains a T5-11b model for paraphrasing text and discovers that all detectors experience a significant drop in quality when faced with paraphrased text.Additionally, simple paraphrasing attacks involve word substitutions 25000 words at $10.99/MonthNo Table 2: A summary of popular commercial tools to detect AI-generated text.(Shi et al., 2023).Moreover, paraphrasing can also be achieved through translation attacks.However, conducting more in-depth analysis and research on complex paraphrasing techniques in the future is crucial.Becker et al. (2023) systemically examines different classifiers encompassing both classical approaches and Transformer techniques for detecting machine (like T5) or human paraphrased text.", "filtered_refids": [["b57", "b8", "b95"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1075, "num_references": 3}
{"corpusid_sectionid": "264439179-s25", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Prompt Attack", "section": "Current LLMs are vulnerable to prompts (Zhu et al., 2023), thus, users can utilize smartly designed prompts to evade established detectors.For example, Shi et al. (2023) examines instructional prompt attacks by perturbing the input prompt to encourage LLMs to generate texts that are difficult to detect.Lu et al. (2023) also show that LLMs can be guided to evade AI-generated text detection by a novel substitution-based In-Context example Optimization method (SICO) to automatically generate carefully crafted prompts, enabling ChatGPT to evade six existing detectors by a significant 0.54 AUC drop on average.Nevertheless, limited attention has been devoted to this topic, indicating a notable research gap that merits significant scholarly exploration in the immediate future.Notably, a recent work (Chakraborty et al., 2023a) introduces the Counter Turing Test (CT2), a benchmark consisting of techniques aiming to evaluate the robustness of existing six detection techniques comprehensively.Their empirical findings unequivocally highlight the fragility of almost all the proposed detection methods under scrutiny.Despite the hard prompt attack, Kumarage et al. (2023) first creates an evasive soft prompt tailored to a specific PLM through prompt tuning; and then, they leverage the transferability of soft prompts to transfer the learned evasive soft prompt from one PLM to another and find the universal efficacy of the evasion attack.", "filtered_refids": [["b60", "b14", "b144", "b95", "b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1444, "num_references": 5}
{"corpusid_sectionid": "264439179-s27", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Theorical Analysis", "section": "Inspired by the binary hypothesis test in (Polyanskiy and Wu, 2022), (Sadasivan et al., 2023) claims that machine-generated text will become indistinguishable as the total variance between the distributions of human and machine approaches zero.In contrast, Chakraborty et al. (2023b) demonstrates that it is always possible to distinguish them by curating more data to make the detection of AUROC increase exponentially with the number of training instances.Additionally, DNA-GPT (Yang et al., 2023b) demonstrates the difficulty of obtaining a high TPR while maintaining a low FPR.Nevertheless, a dearth of theoretical examination persists regarding the disparities in intrinsic characteristics between human-written language and LLMs.Scholars could leverage the working mechanisms of GPT models to establish a robust theoretical analysis, shedding light on detectability and fostering the development of additional detection algorithms.", "filtered_refids": [["b127", null, "b92"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 937, "num_references": 3}
{"corpusid_sectionid": "264439179-s28", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "LLM-Generated Code Detection", "section": "Previous detectors usually only focus on the text, but LLMs-generated codes also show increasing quality (see a recent survey (Zan et al., 2022)).Among the first, Lee et al. (2023) found that previous watermarking (Kirchenbauer et al., 2023a) for text does not work well in terms of both detectability and generated code quality.It is evidenced that low entropy persists in generated code (Lee et al., 2023), thus, the decoding process is more deterministic.They thus adapt the text watermarks to code generation by only injecting watermarks to tokens with higher entropy than a given threshold and achieve more satisfactory results.Code detection is generally believed to be even harder than text detection due to its shorter length, low entropy, and non-natural language properties.DetectGPT4Code (Yang et al., 2023e) detects codes generated by ChatGPT by using a proxy code model to approximate the logits on the conditional probability curve and achieves the best results over previous detectors.", "filtered_refids": [["b130", "b63", "b54", "b136"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1000, "num_references": 4}
{"corpusid_sectionid": "264439179-s29", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Model Sourcing", "section": "Model sourcing (Yang et al., 2023b), is also known as origin tracking (Li et al., 2023a) or authorship attribution (Uchendu et al., 2020).Unlike the traditional distinction between human and machinegenerated texts, it focuses on identifying the specific source model from a pool of models, treating humans as a distinct model category.With the fast advancement of LLMs from different organizations, it is vital to tell which model or organization potentially generates a certain text.This has practical applications, particularly for copyright protection.Consequently, we believe that in the future, it may become the responsibility of organizations releasing powerful LLMs to determine whether a given text is a product of their system.Previous work either (Li et al., 2023a) trains a classifier or utilizes the intrinsic genetic properties (Yang et al., 2023b) to perform model sourcing, but still can not handle more complicated scenarios.GPT-who (Venkatraman et al., 2023) utilizes Uniform Information Density (UID) based features to model the unique statistical signature of each LLM and human author for accurate authorship attribution.", "filtered_refids": [["b127", null, "b140", "b109"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1142, "num_references": 4}
{"corpusid_sectionid": "264439179-s31", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Generalization", "section": "Currently, the most advanced LLMs, like Chat-GPT, are getting actively updated, and OpenAI will make a large update every three months.How to effectively adapt existing detectors to the updated LLMs is of great importance.For example, Tu et al. (2023) records the ChatLog of ChatGPT's response to long-form generation every day in one month, observes performance degradation of the Robertabased detector, and also finds some stable features to improve the robustness of detection.As LLMs continuously benefit from interacting with different datasets and human feedback, exploring ways to effectively and efficiently detect their generations remains an ongoing research area.Additionally, Kirchenbauer et al. (2023b) investigates the reliability of watermarks for large language models and claims that watermarking is a reliable solution under human paraphrasing and various attacks at the context length of around 1000.Pu et al. (2023) examines the zero-shot generalization of machinegenerated text detectors and finds that none of the detectors can generalize to all generators.All those findings reveal the difficulty of reliable generalization to unseen models or data sources of detection.", "filtered_refids": [["b107", null, "b85"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1193, "num_references": 3}
{"corpusid_sectionid": "232075945-s2", "title": "A Survey on Stance Detection for Mis-and Disinformation Identification", "date": "2021-02-27", "section_title": "Source(s) Target", "section": "Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) \u01cc Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) \u01cc Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  \u0240  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, \u01cc News, \u0240ikipedia, Reddit. Evidence: Single, Multiple, Thread.\n\n2 What is Stance?\n\nIn order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as \"a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field\", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by K\u00fc\u00e7\u00fck and Can (2020): \"for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text\" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).\n\nFinally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.", "filtered_refids": [["b12", null, "b57", "b17"], [], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2863, "num_references": 5}
{"corpusid_sectionid": "232075945-s5", "title": "A Survey on Stance Detection for Mis-and Disinformation Identification", "date": "2021-02-27", "section_title": "Stance as a (Mis-/Dis-)information Detection Component", "section": "Fully automated systems can assist in gauging the extent and studying the spread of false information online. This is in contrast to the previously discussed applications of stance detection -as a stand-alone system for detecting mis-and disinformation. Here, we review its potency to serve as a component in an automated pipeline. Figure 1b illustrates the setup, which can also include steps such as modelling the user or profiling the media outlet among others. We discuss in more detail media profiling and misconceptions in Appendix B.\n\nRumors Stance detection can be used for rumour detection and debunking, where the stance of the crowd, media, or other sources towards a claim are used to determine the veracity of a currently circulating story or report of uncertain or doubtful factuality. More formally, for a textual input and a rumour expressed as text, stance detection here is to determine the position of the text towards the rumour as a category label from the set {Support, Deny, Query, Comment}. Zubiaga et al. (2016b) define these categories as whether the author: supports (Support) or denies (Deny) the veracity of the rumour they are responding to, \"asks for additional evidence in relation to the veracity of the rumour\" (Query) or \"makes their own comment without a clear contribution to assessing the veracity of the rumour\" (Comment). This setup was widely explored for microblogs and social media. Qazvinian et al. (2011) started with five rumours and classified the user's stance as endorse, deny, unrelated, question, or neutral. While they were among the first to demonstrate the feasibility of this task formulation, the limited size of their study and the focus on assessing the stance of individual posts limited its real-world applicability. Zubiaga et al. (2016b) analysed how people spread rumours on social media based on conversational threads. They included rumour threads associated with nine newsworthy events, and users' stance before and after the rumours were confirmed or denied. Ferreira and Vlachos (2016) collected claims and news articles from rumour sites with annotations for stance and veracity by journalists as part of the Emergent project. The goal was to use the stance of a news article, summarised into a single sentence, towards a claim as one of the components to determine its veracity. A downside is the need to summarise, in contrast to FNC-1 (Pomerleau and Rao, 2017), where entire news articles were used.  Zubiaga et al. (2016b). Bo\u0161njak and Karan (2019) studied stance detection and claim verification of comments for Croatian news articles.", "filtered_refids": [[], ["b12", "b57", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2609, "num_references": 3}
{"corpusid_sectionid": "232075945-s6", "title": "A Survey on Stance Detection for Mis-and Disinformation Identification", "date": "2021-02-27", "section_title": "Multiple languages", "section": "In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.\n\nFact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.\n\nMohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.\n\nMore recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.\n\nGuderlei and A\u00dfenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).\n\nSome formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.\n\nAnother notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using \"born in/on\". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as \"Sarawak is a ...\", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).\n\nError analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., \"Andrea Pirlo is an American professional footballer.\" vs. \"Andrea Pirlo is an Italian professional footballer who plays for an American club.\", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., \"Terry Crews played on the Los Angeles Chargers.\" (NotE-noughInfo) is classified as refuted, given the sentence \"In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ...\", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, \"The heart beats at a resting rate close to 22 bpm.\" is not classified as refuted based on the evidence sentence \"The heart beats at a resting rate close to 72 bpm.\", and similarly for months.\n\nThreaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.\n\nThese approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).\n\nA major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).\n\nAnother factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.\n\nMulti-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.\n\nEarlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Sch\u00fctze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.", "filtered_refids": [[], [null, "b19"], [], ["b31"], [null], ["b43", "b52", "b47", "b50", "b30", null, "b51", "b8"], [null], [], [null, "b57"], ["b49", null, "b55", "b36"], ["b49", null, "b57"], [null, "b38"], [], ["b24", null, "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 65, "num_chars": 11884, "num_references": 27}
{"corpusid_sectionid": "232075945-s9", "title": "A Survey on Stance Detection for Mis-and Disinformation Identification", "date": "2021-02-27", "section_title": "Shades of Truth", "section": "The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).\n\nLabel Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.", "filtered_refids": [["b9", "b20", "b18", "b3"], [null, "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1431, "num_references": 6}
{"corpusid_sectionid": "232075945-s10", "title": "A Survey on Stance Detection for Mis-and Disinformation Identification", "date": "2021-02-27", "section_title": "Explainability", "section": "The ability for a model to be able to explain its decisions is getting increasingly important, especially for mis-and disinformation detection, as one could argue that it is a crucial step towards adopting fully automated fact-checking. The FEVER 2.0 task formulation (Thorne et al., 2019) can be viewed as a step towards obtaining such explanations, e.g., there have been efforts to identify adversarial triggers that offer explanations for the vulnerabilities at the model level (Atanasova et al., 2020b). However, FEVER is artificially created and is limited to Wikipedia, which may not reflect real-world settings. To mitigate this, explanation by professional journalists can be found on fact-checking websites, and can be further combined with stance detection in an automated system. In a step in this direction, Atanasova et al. (2020a) generated natural language explanations for claims from PolitiFact 4 given gold evidence document summaries by journalists.\n\nMoreover, partial explanations can be obtained automatically from the underlying models, e.g., from memory networks (Mohtarami et al., 2018), attention weights (Zhou et al., 2019;Liu et al., 2020b), or topic relations (Si et al., 2021). However, such approaches are limited as they can require gold snippets justifying the document's stance, attention weights can be misleading (Jain and Wallace, 2019), and topics might be noisy due to their unsupervised nature. Other existing systems (Popat et al., 2017(Popat et al., , 2018Nadeem et al., 2019) offer explanations to a more limited extent, highlighting span overlaps between the target text and the evidence documents. Overall, there is a need for holistic and realistic explanations of how a factchecking model arrived at its prediction.\n\nIntegration People question false information more and tend to confirm true information (Mendoza et al., 2010). Thus, stance can play a vital role in verifying dubious content. In Appendix C, we discuss existing systems and real-world applications of stance for mis-and disinformation identification in more detail. However, we argue that a tighter integration between stance and factchecking is needed. Stance can be expressed in different forms, e.g., tweets, news articles, user posts, sentences in Wikipedia, and Wiki tables, among others and can have different formulations as part of the fact-checking pipeline (see Section 3). All these can guide human fact-checkers through the process of fact-checking, and can point them to relevant evidence. Moreover, the wisdom of the crowd can be a powerful instrument in the fight against mis-and disinformation (Pennycook and Rand, 2019), but we should note that vocal minorities can derail public discourse (Scannell et al., 2021). Nevertheless, these risks can be mitigated by taking into account the credibility of the user or of the information source, which can be done automatically or with the help of human fact-checkers.", "filtered_refids": [[null], ["b52", "b14", "b13", "b30", null, "b1"], [null, "b10", "b23"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2941, "num_references": 10}
{"corpusid_sectionid": "232075945-s14", "title": "A Survey on Stance Detection for Mis-and Disinformation Identification", "date": "2021-02-27", "section_title": "B Additional Formulations of Stance as a Component for Fact-Checking", "section": "Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline. Below, we describe some work that follows these formulations.\n\nMisconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020). This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.\n\nMedia Profiling Stance detection has also been used for media profiling. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020). This is an important step towards understanding media biases. Tweet: Wow, that is fascinating! I hope you never mock our proud Scandi heritage again.\n\n(b) Examples from Qazvinian et al. (2011) andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. \u0240iki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992. \u0240iki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.\n\n(c) Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show \u01cc Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun \u01cc Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how do you know its an ISIS flag? Can you actually confirm that? \u0273 u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit \u0273  Table 3: Illustrative examples for different stance detection scenarios included in our survey. We annotate the expressed stance with (support, for), (deny, against), \u0273 (query), and (comment).\n\nThe reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.\n\nMore recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.\n\nThere is a well-known connection between factuality and bias. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.\n\nUser Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).", "filtered_refids": [[], [null], ["b32"], [null, "b17"], [], ["b14", null, "b13", "b0"], ["b16"], ["b15"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 29, "num_chars": 4816, "num_references": 11}
{"corpusid_sectionid": "254043519-s5", "title": "Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources", "date": "2022-11-28", "section_title": "Input and Label Collection", "section": "Input Source: Collection Process for Input Texts (x). We classify input text source into three high-level categories (i.e., generated by human, collected from websites, and extracted from curated sources) and break them down into nine finegrained categories. Table 4 shows the categories and annotation results. While we cover 222 languages, only 40 of them (18%) have input text specifically written by humans for the task. * The news is the most common source, used by 40 of 156 datasets, often in summarization or classification tasks, followed by web corpora * and Wikipedia. Many languages have datasets derived only from Wikipedia text. Figure 6 in appendix shows pertask input source distributions. Overall, we observe that high-resource languages entertain a variety of input sources, while low-resource ones rely on fewer resources such as Wikipedia and news.\n\nLabel Source: Collection Process for Labels (y). Table 5 presents the statistics on how the output labels were collected, split into five categories: annotated by authors or linguists, crowdsourced, automatically induced, derived from linguistic resources, and not mentioned. Label collection methods affects dataset quality. While manually annotated datasets can exhibit artifacts (Gururangan et al., 2018;Poliak et al., 2018), they are often val-* Due to overlap of language covered among datasets, there are 40 unique languages instead of 53 (from Table 4). * The \"web\" fine-grained category refers to the collection of sentences scraped or sampled at large-scale from the web. idated via inter-annotator agreement. In contrast, automatically induced datasets are often introduced without such kind of validation phases and tend to be noisy. For example, bullet points from news article are often considered to be the summary of the article, which can contain missing background information in the rest of the article (Kang and Hashimoto, 2020;Goyal et al., 2022). Fifty-three datasets had automatically induced labels, most commonly seen for summarization and classification tasks, and 95 used manual annotation. This further breakdowns to 27 datasets solely annotated by domain experts and 56 datasets solely annotated by crowdworkers. We investigated annotator pools for non-English languages in Section 5.\n\nLabel Source and Task Types. Figure 3 presents label collection methods per task type. QA with retrieval (e.g., XQA; Liu et al. 2019) and generation tasks show a high proportion of automatically induced datasets. In contrast, structured prediction datasets were rarely automatically induced; they were more often annotated by authors or linguists. Crowdsourcing is commonly used to construct reading comprehension and classification datasets.\n\nLabel Source and Language Diversity. Figure 4 shows the distributions of the label data collection methods for the top 10 languages and for 20 sampled languages. In high-resource languages, a large number of datasets are labeled manually, where in low-resource languages, the percentage of automatically induced datasets increases, with 135 languages have only automatically induced datasets. On (macro-)average, the 10 highest resource languages show 43.4% of their datatsets with only automatically induced labels; however, for all languages, 84.9% of the datasets use only automatically induced labels. Prior work often uses the total number of datasets in a target language as a proxy for resource availability of the language, which our analysis suggests is limited.", "filtered_refids": [[], [null, "b7"], ["b2"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 3499, "num_references": 3}
{"corpusid_sectionid": "254043519-s10", "title": "Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources", "date": "2022-11-28", "section_title": "Pilot Study on Crowdsourcing Multilingual Data on MTurk", "section": "How easy is it to collect multilingual dataset on popular annotation platform (MTurk) at the moment? Crowdsourcing enables large-scale, cost efficient data collection; however, for many languages, the number of language-proficient crowdworkers is limited (Garcia et al., 2021). We quantify the availability of MTurk workers with proficiency in non-English languages. We formulated a four-way sentiment analysis task using the Multilingual Amazon Review Corpus (Keung et al., 2020) and analyzed the annotation quality, cost, and time to finish tasks in English, Spanish, German, French, Japanese and Chinese of crowdworkers.\n\nIn all settings, we asked annotators to translate the same English sentence to assess their actual (rather than professed) language proficiency. We found that many production-level MT systems fail to translate his sentence due to its compositionality. Further, we investigated the newly introduced \"language qualification\" in MTurk, which available for only four aforementioned languages and Brazilian Portuguese as of 2022. For our sentiment analysis task, without the language qualification, the accuracy of human binary classification performance in all non-English languages (55.2%) was significantly worse than that of English (77%). Past recommendations, such as constraining location and HIT acceptance rate, are insufficient (as of 2022) to collect high quality data even for languages considered \"easier\" to crowdsource in prior work (Pavlick et al., 2014). Language qualification improved performance by up to 40% and reduced the prevalence of cheating across all languages. However, with the language qualification, the data collection process usually took more time and cost ($1 per assignment). More pilot study details are in Appendix C.\n\nQuality Control Using Translation Task. We investigate whether crowdworkers relied on automatic machine translation, despite our instruction saying not to use them. We ask native speakers to compare the crowdsourced translation with the translation results from three major translation platforms: Google Translate, * Microsoft Bing Translator, * and DeepL Translator. * Without language qualification, we identified 33% of crowdworkers copy-and-pasted automatic translation outputs (with qualification, 7%). This is significantly higher than what Pavlick et al. (2014) report (10%), suggesting more crowdworkers have started to use MT services.\n\nWe found that we could potentially use the translation task to identify good submissions: if we take only the submissions whose translation (1) do not match translation from MT and (2) valid translated judged by native speakers (labeled as either correct or partially correct), * binary task accuracy rises to 81.0% from 63.5%, matching the binary accuracy of English.\n\nOur pilot study suggests that translation quality can reflect the target task performance if workers who copy from MT systems are filtered, and can be a good proxy for the languages without aforementioned language qualifications.", "filtered_refids": [[null], ["b6"], ["b6"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3023, "num_references": 3}
{"corpusid_sectionid": "259833865-s3", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "MRG Systems and Applications", "section": "Since the seminal work of Kukich (1983), there have been several kinds of medical report systems and applications, such as the generation of psychiatric case notes (Kazi and Kahanda, 2019), the generation of consultation notes from transcripts (Papadopoulos Korfiatis et al., 2022), the generation of radiology reports (Chen et al., 2021), nurse-patient summaries (Liu et al., 2019), counseling (conversation) summarization , discharge summaries or clinical notes (Krishna et al., 2021), and even data augmentation for other medical tasks (Kocabiyikoglu et al., 2021). Joshi et al. (2020) provided a general definition of a medical report in the case of medical dialogue summarization: \"the medical report captures and summarizes the important parts of the medical conversation necessary for clinical decision-making and subsequent follow-ups.\"\n\nDespite the diversity of their tasks, structures and audiences, the main characteristics of MRG remain similar, namely the use of the documentation and the subsequent use of the diagnosis, which can also be used for administration and by institutions, subsequently referenced by clinicians and retained by patients. The main objectives of such systems in clinical practice are to reduce the time spent by clinicians on manual writing and facilitate medical decision-making.", "filtered_refids": [["b12", "b6", "b18", "b4", null, "b7", "b8", "b2"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 1319, "num_references": 8}
{"corpusid_sectionid": "259833865-s5", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "Text-to-text", "section": "There are three main tasks in the Text-to-Text category: 1) summarizing medical dia-logues/conversations, including spoken conversations and online medical conversations; 2) summarizing hospital stays/hospitalizations; and 3) summarizing medical reports, where the original reports may come from different domains, such as radiology reports or general clinical reports.\n\nThe most common work in text-based MRG is that of summarizing medical conversations, where the input source can be either transcripts of clinician-patient spoken conversations (Kazi and Kahanda, 2019;Enarvi et al., 2020;Liu et al., 2019;Krishna et al., 2021;Molenaar et al., 2020;Lacson et al., 2006;Moramarco et al., 2022;Yim and Yetisgen, 2021); or online medical conversations (Chintagunta et al., 2021;Nair et al., 2021;Joshi et al., 2020;Song et al., 2020;Chen et al., 2022).\n\nRegarding the summarization of hospital stays, some work (Di Eugenio et al., 2014;Acharya et al., 2016) used both physician discharge notes (free text) and the structured nursing documentation (such as nursing plans of care) to generate a unique summary. Others generated summaries from longform hospital admissions (Adams et al., 2023).\n\nMoreover, work has also been carried out on summarizing medical reports. For example, Moramarco et al. (2021) used the MTSamples dataset to fill automatically the 'description field' of a medical report based on the information present in the overall report. In addition, radiology report summarization Karn et al., 2022) is intended to produce a concise and easily comprehensible 'IMPRESSIONS' section from the rest of the radiology report. The 'IM-PRESSIONS' section of a radiology report is considered a summary of the radiologist's reasoning and conclusions, which helps the referring physician confirm or exclude certain diagnoses (Karn et al., 2022).", "filtered_refids": [[], ["b12", "b4", "b18", "b9", null, "b33", "b7", "b38", "b2"], [null], [null, "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1848, "num_references": 12}
{"corpusid_sectionid": "259833865-s6", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "Data-to-text", "section": "As presented in Table 1, there are different tasks in the Data-to-Text category: 1) generation of reports from medical images, such as radiology images, brain image data.; 2) generation of text summaries from intensive care data; and 3) generation of medical reports from multimodal inputs; 4) other applications such as the generation of tailored smoking cessation letters based on responses to a smoking questionnaire (Reiter et al., 2003).\n\nIn order to meet the growing demand of imagebased diagnosis from patients using artificial in- Other applications Generation of tailored smoking cessation letters Reiter et al. (2003)  telligence and applying image captioning to the medical field, radiology report generation is the subject of continuous work and growing interest from researchers, which aims to describe radiology images with professional quality reports (Chen et al., 2020(Chen et al., , 2021Lovelace and Mortazavi, 2020;Nooralahzadeh et al., 2021;Yan et al., 2021;Qin and Song, 2022;Miura et al., 2021;Liu et al., 2021). Such research has also been applied to generation of clinician reports from brain imaging data (Jordan et al., 2014). Besides images, the summarization of physiological data as also been the subject of research. In the BabyTalk project, Portet et al. (2009) presented a prototype that generates textual summaries of about 45 minutes of continuous physiological signals and discrete events. Their evaluation with physicians showed that text summaries could be an effective decision-support aids for clinicians.\n\nTo cope with the high workload due to the time required for proper documentation, Maas et al. (2021) presented a real-time automated report of the interaction between care provider and patient, taking multimodal inputs that include audio, video, and sensor data from medical consultations, and in-troducing knowledge graphs -the Patient Medical Graph. They used speech and action recognition technology to first transform multimodal inputs into text before formally representing them and generating reports.", "filtered_refids": [["b26"], ["b11", "b26", "b13", "b20", null, "b22", "b1", "b37"], ["b15"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 2053, "num_references": 10}
{"corpusid_sectionid": "259833865-s8", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "Automatic Evaluation", "section": "Automatic evaluation is popular because it is cheap and fast and it is widely used in benchmarking activity and for system development.\n\nThere is a wide range of automatic evaluation metrics used in NLG (Sai et al., 2022) and we will restrict to the two most popular: 1) the corpusbased metrics and 2) the trainable metrics. The corpus-based metrics rely on a set of reference texts (i.e. gold standard outputs) to which system outputs are compared. For instance, it can be based on n-grams, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004); or on edit distance: WER (Woodard and Nelson, 1982), TER (Snover et al., 2006), etc.\n\nMost automatic metrics require gold references, but these are not always available. Reference-less metrics, where neural models are trained to predict human ratings from texts (e.g. regression models trained on ratings data), are getting more and more attention recently. For example, BLEURT (Sellam et al., 2020) is a learned evaluation metric for English to predict human judgments. It relies on the BERT model using unsupervised techniques with millions of synthetic examples. There are also metrics based on question-answer pairs on a given source document Rebuffel et al., 2021), for example QuestEval  uses pre-trained models to assess if two different inputs contain the same information. Note that QuestEval can also be used with references.\n\nTo summarize, ROUGE scores assess the similarity between candidates and references based on the overlap of unigrams, bigrams, and the longest common sequence, likewise for BLEU; while BLEU focuses on precision, ROUGE focuses on recall. BERTScore evaluates the similarity between candidates and references at token level, using contextual embeddings from BERT, while QuestEval assesses whether a summary contains all the relevant information from its source document and BLEURT attempts to model human judgments.\n\nHowever, automatic evaluation metrics have their limitations and do not sufficiently reflect human judgments of system performance (Novikova et al., 2017).", "filtered_refids": [[], ["b19", "b10", "b31", "b36"], ["b23"], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2057, "num_references": 6}
{"corpusid_sectionid": "259833865-s9", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "Human Evaluation", "section": "Human evaluation is considered the most informative form of evaluation of NLG systems, but it can be expensive and time-consuming since qualified human evaluators have to be recruited. Hence, human evaluation is difficult to scale up unless using crowd sourcing approaches but these are difficult to apply in medicine for expertise and privacy reasons.\n\nThere are several commonly used methods for human evaluation, including the Likert scale scoring and pairwise comparison for general text generation, as well as Pyramid and binary factuality evaluation specifically designed for summarization (Gao et al., 2023). Some other methods consist in evaluating how much information can be extracted back from the text in a formal form (A. Baez Miranda et al., 2015).\n\nIt has been argued that human evaluation approaches are difficult to compare (van der Lee et al., Belz et al., 2020) since different tasks and criteria are used (with different names). Furthermore, only a small number of papers provide full details of human evaluation experiments (Belz et al., 2020). Howcroft et al. (2020) concluded that due to a pervasive lack of clarity in reports and extreme diversity in approaches, human evaluation in NLG presents as extremely confused in 2020, and that the field is in urgent need of standard methods and terminology.\n\nIn addition, van der Lee et al. (2019) provided an overview of best practices in human evaluation of automatically generated text based on papers published at INLG (N=51) and ACL (N=38) in 2018, and released a list of best practices on 7 different topics: general, criteria, sampling, annotation, measurement, design and statistics.", "filtered_refids": [[], [null, "b0"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1658, "num_references": 3}
{"corpusid_sectionid": "259833865-s12", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "Text Quality", "section": "For automatic text quality assessment, there are word-overlap-based metrics like ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), embedding-based metrics such as BERTScore ; learned evaluation metrics like BLEURT (Sellam et al., 2020); and evaluation metrics which rely on question answering like QuestEval . ROUGE (Lin, 2004) has been widely used in MRG tasks: some papers reported only ROUGE-L F1 score (Joshi et al., 2020;Enarvi et al., 2020;Nair et al., 2021), while some others reported ROUGE-1, ROUGE-2, and ROUGE-L scores (Song et al., 2020;Yim and Yetisgen, 2021). Yim and Yetisgen (2021) reported BLEU (Papineni et al., 2002) in addition to ROUGE performances across different note sections.  Commonly used automated metrics, such as ROUGE and BLEU, have their limitations and are known to correlate poorly with human evaluations (van der Lee et al., 2019). Therefore, other measures such as QuestEval  and BLEURT (Sellam et al., 2020) which can correlate better with human judgements are used,  used these two scores in addition to ROUGE. In addition to ROUGE and BLEURT, Ben Abacha et al. (2023) also reported BERTScore.", "filtered_refids": [["b10", "b35", "b30", null, "b19", "b38", "b33", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1134, "num_references": 8}
{"corpusid_sectionid": "259833865-s13", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "Medical Correctness: Report-based", "section": "In the study by Joshi et al. (2020) two measures are defined: Medical Concept Coverage and Negation Correctness. The former captures the coverage of medical terms in the predicted summaries to the gold standard reference, while the latter identifies the negated status of medical concepts. In the healthcare domain, it is crucial to ensure highquality results in terms of accurate usage of medical terms and capturing negation.\n\nThe evaluation of Concept involves using specific and in-house extractors and Named Entity Recognition (NER) models. They refer to domainspecific knowledge and compare the match of extracted concepts to standardized health and biomedical vocabularies, such as the Unified Medical Language Systems (UMLS). Several studies have utilized concept correctness measures, such as F1score, precision, recall, and false positives, at various levels of granularity, including the report level and section level.\n\nFor instance, Joshi et al. (2020) used an inhouse medical entity extractor to match concepts in the summary to UMLS, and they used Negex (Harkema et al., 2009) to determine negated concepts. Medical concepts in the predicted summary that were not present in the original conversation would be false positives, and vice versa for false negatives. Among the concepts present in the predicted summary, they assessed precision and recall to see whether the predicted negation was accurate for the decoded concepts and computed a Negation F1. The set of automatic metrics proposed was then used in several works (Chintagunta et al., 2021;Nair et al., 2021).\n\nIf in-house entity extractor to match concepts in the summary to UMLS have been frequent (Soldaini and Goharian, 2016;Joshi et al., 2020;, entity extraction using machine learning has appeared recently, which is even more specific to the task. For instance, Enarvi et al. (2020) employed a machine learning-based clinical fact extractor to measure factual correctness by extracting medical facts from both the predicted reports and the ground-truth reports, such as conditions and medications, as well as their attributes such as body part, severity, or dosage, then calculat-ing the F1 score from these two sets. To compute Concept-F1, Chen et al. (2022) used the medical entity extractor -BERT-CRF (Devlin et al., 2019) trained on their NER task to match entities in the predicted summary to the reference summary.\n\nSimilarly, Ben Abacha et al. (2023) used \"factbased metrics (Fact Scores)\", which is a machine learning-based medical fact extraction system. The Fact Score metric measures the F1-score of medically relevant facts extraction, is used to assess the factual consistency of the generated summaries. The Fact-based metrics consist of two variants: Fact-Core, which relies on the extraction of seven core fact attributes, and Fact-Full, which combines these core facts and five additional attributes.\n\nIn addition, there is also work combining the two approaches to extract concepts:  extracted medical relevant concepts via one of two systems: their in-house rule-based system and quickUMLS (Soldaini and Goharian, 2016) -a Python implementation of UMLS. Their rulebased system was found to be effective in capturing symptom-related findings in clinical reports, and quickUMLS is capable of extracting a wide scope of medical findings such as symptoms, diseases, medication and procedures.\n\nMoreover, Molenaar et al. (2020) measured the quality of the dialogue summarization pipeline for healthcare reporting by establishing the number of items included in the generated and gold standards, using precision, recall and false positives (FPs) as metrics. They followed the SOEP/SOAP format -Subjective (S), Objective (O), Evaluation (E) / Assessment (A) and Plan (P) -commonly used by general practitioners in the Netherlands. It appears that they manually calculated the number of items included in each section of the SOEP format for the eight reports generated.\n\nHowever, concept-based evaluation can have its own limitations, particularly with regard to false positives errors,  employed filtering rules to attempt to mitigate this issue.\n\nAdditionally, Chen et al. (2022) reported Regexbased Diagnostic Accuracy (RD-Acc), which measures the model's ability to diagnose the disease. Their reference reports written by annotators contain six parts, RD-Acc is calculated using the regexbased approach based on the diagnosis part. They calculated for what percentage of the generated reports, the content of their diagnosis part contains the actual disease text or key concepts.", "filtered_refids": [["b2"], [], [null, "b2"], [null, "b2", "b32"], [], ["b32"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 4577, "num_references": 7}
{"corpusid_sectionid": "259833865-s14", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "Medical Correctness: Auxiliary or Intermediate Tasks", "section": "Another subcategory of automatic measures of concept correctness is those that evaluate auxiliary or intermediate tasks, there are two types: classification of electronic health record (EHR) categories and utterances classification.\n\nTo generate case notes from digital transcripts of doctor-patient conversations, Kazi and Kahanda (2019) divided the task into two subtasks: (1) predict semantic topics for segments of the transcripts (EHR categories) and then (2) generate a more formal version of the text that goes into the corresponding section of the EHR form. They used the AUROC (Area Under the ROC Curve) scale (Bewick et al., 2005) to assess their first task of predicting EHR categories, which could be any of the following: Client Details, Chief Complaint, Family History, Social History, Medical History and Others. Correct prediction of EHR categories could be useful for subsequent formal text generation.\n\nFor utterances classification, there are different types of classification such as classifying noteworthy utterance (Krishna et al., 2021), label prediction for medical conversation utterances (Song et al., 2020), and dialogue turn classification (Lacson et al., 2006).\n\nIn detail, Krishna et al. (2021) evaluated the multi-label classification of noteworthy utterances that are relevant to each summary section before clustering related utterances and generating one summary sentence per cluster. Their modular summarization technique outperforms its purely abstractive counterpart, producing much more factual and coherent sentences. Besides, Song et al. (2020) first identified two types of utterances (problem statements and treatment recommendations) and then generated summaries, they showed that for the particular dataset used, high-quality summaries can be generated by extracting these two types of utterances. Thus, in addition to reporting ROUGE scores, they also reported the precision, recall, and F scores of the predicted labels for utterances of medical conversations, compared to the standard labels. In addition, Lacson et al. (2006) also measured precision, recall, and F measure of dialogue turns classification.", "filtered_refids": [[], [null], ["b33", "b7", "b9"], ["b33", "b7", "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2154, "num_references": 7}
{"corpusid_sectionid": "259833865-s16", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "Intrinsic Approaches", "section": "The intrinsic human evaluation of generated reports comprises two categories as for automated metrics: text quality and medical correctness.\n\nText quality is important in MRG as in general NLG output. For text quality, a wide variety of properties can be considered and various linguistic parameters can be used, e.g. relevance, consistency, fluency, coherence, missing, hallucination, repetition and contraction. As an example,  used four standard linguistic parameters: relevance (selection of relevant content), consistency (factual alignment between the summary and the source), fluency (linguistic quality of each sentence), and coherence (structure and organization of summary). In addition to these commonly used and well-studied criteria, the evaluation of MRG also concludes other medical correctness criteria, such as factually correct and medically relevant information (Joshi et al., 2020;Chintagunta et al., 2021), which are specific to MRG tasks. As another example, Ben Abacha et al. (2023) performed expert-based manual evaluation using NLG criteria such as Fluency and Non-redundancy, and medical criteria such as Critical Omissions, Hallucinations, Correct Facts, Incorrect Facts based on fact extraction. Furthermore, depending on whether evaluators assess the output directly or by comparing different texts, intrinsic human evaluation can be classified into direct and relative evaluation. As for the articles involving human evaluation, they all used at least direct evaluation, i.e. the evaluators judged the generated texts directly on a defined scale. Some authors also performed relative evaluation in addition to direct evaluation: Joshi et al. (2020) and Chintagunta et al. (2021) performed a comparison task in which, given two summaries generated by different models and the associated dialogue, annotators had to choose which summary was better, they could also choose \"both\" and \"none\". Yim and Yetisgen (2021) ranked the four systems against each other, with 1 being the best, in addition to evaluating each system independently with a score from 1 to 5 for the categories relevancy, factual accuracy, writing-style, completeness, and overall.\n\nIn general, MRG outputs are evaluated at the report level, however, depending on the design of the model, some are additionally evaluated at the sentence/section/part level. For example, Krishna et al. (2021) divided SOAP notes into several subsections: Family Medical History, Past Surgical History, Chief Complaint, etc. Therefore, they evaluated the generated SOAP notes in two ways: 1) SOAP note sentence level and 2) SOAP note level.", "filtered_refids": [[], [null, "b38", "b2"], ["b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2616, "num_references": 4}
{"corpusid_sectionid": "259833865-s17", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "Extrinsic Approaches", "section": "As for extrinsic human evaluation: to evaluate generated summaries, a team of mental health experts used clinical acceptability framework (Sekhon et al., 2017), which includes six parameters: affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness . In addition, to perform a task-based evaluation and measure the usefulness of summaries for preserving important information in the medical setting, Lacson et al. (2006) asked physicians and nurses to create a list of key questions based on topics that commonly arise between hemodialysis patients and caregivers, and then asked five physicians to answer each of the six \"yes/no\" questions using each of 40 dialogues. Furthermore, in a study evaluating the correlation between human evaluation and automatic metrics in consultation note generation, Moramarco et al. (2022) asked 5 clinicians to post-edit generated notes and extract all errors.", "filtered_refids": [["b9", "b29", "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 3, "num_chars": 936, "num_references": 3}
{"corpusid_sectionid": "259833865-s18", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "Presence of Domain Experts", "section": "Most of the articles we reviewed that included a human evaluation involved domain experts, such as doctors serving patients on their telehealth platform (Chintagunta et al., 2021;Joshi et al., 2020), five licensed physicians (Lacson et al., 2006), three general practice physicians (Moramarco et al., 2021), an annotator with a medical degree (Yim and Yetisgen, 2021), etc. Sometimes, the expertise of the annotators is not specified, e.g. \"trained human annotators\" (Krishna et al., 2021).\n\nWe also note that of the 9 articles including human evaluation, 5 of them reported Inter-Evaluator Agreement: three of the medical dialogue summarization articles Moramarco et al., 2022;Ben Abacha et al., 2023), and two medical (report) summarization articles (Moramarco et al., 2021;Karn et al., 2022). It would be prefer-able to indicate Inter-Evaluator Agreement in the presence of several annotators.", "filtered_refids": [["b9", null, "b7", "b38", "b2"], [null, "b18", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 896, "num_references": 8}
{"corpusid_sectionid": "3176028-s2", "title": "Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review", "date": "1981-04-01", "section_title": "Kantor's thesis", "section": "Kantor's main exhibit is the following text:\n\n(2-1) A good share of the amazing revival of commerce must be credited to the ease and security of communications within the empire. 'The Imperial fleet kept the Mediterranean Sea cleared of pirates. In each province, the Roman emperor repaired or constructed a number of skillfully designed roads. They were built for the army but served the merchant class as well. Over them, messengers of the Imperial service, equipped with relays of horses, could average fifty miles a day.\n\nHe claims that the they in the penultimate sentence is hard to comprehend, and that most informants need to reread the previous text to find its referent. Yet the sentence is neither semantically anomalous nor ambiguous --the roads is the only plural NP available as a referent, and it occurs immediately before the pronoun with only a full-stop intervening. To explain this paradox is the task Kantor set himself.\n\nKantor's explanation is based on discourse topic and the listener's expectations. In (2-1), the discourse topic of the first three sentences is ease and security of communication in the Roman empire. In the fourth sentence, there is an improper shift to the roads as the topic: improper, because it is unexpected, and there is no discourse cue to signal it. Had the demonstrative these roads been used, the shift would have been okay.\n\n3 Underlining is used in this and subsequent examples to indicate the anaphor(s) of interest. It does not indicate stress.\n\n(Note that a definite NP such as the roads is not enough.) Alternatively, the writer could have clarified the text by combining the last three sentences with semicolons, indicating that the last two main clauses were to be construed as relating only to the preceding one rather than to the discourse as a whole.\n\nKantor identifies a continuum of factors affecting the comprehension of pronouns. At one end is unrestricted expectation and at the other negative expectation. What this says in effect is that a pronoun is easy to understand if its referent is expected, and difficult if it is unexpected. This is not as vacuous as it at first sounds; Kantor provides an analysis of some subtle factors which affect expectation.\n\nThe most expected pronominalizations are those whose referent is the discourse topic, or something associated with it (though note the qualifications to this below). Consider:\n\n(2-2) The final years of Henry's reign, as recorded by the admiring Hall, were given over to sport and gaiety, though there was little of the licentiousness that characterized the French court. The athletic contests were serious but very popular. Masques, jousts and spectacles followed one another in endless pageantry. He brought to Greenwich a tremendously vital court life, a central importance in the country's affairs, and above all, a great naval connection. 4\n\nIn the last sentence, he is quite comprehensible, despite the distance back to its referent, because the discourse topic in all the sentences is Henry's reign.\n\nAn example of the converse --an unexpected pronoun which is difficult despite recency --can be seen in (2-1) above. Between these two extremes are other cases involving references to aspects of the local topic, changes in topic, syntactic parallelism, and, in topicless instances, recency (though the effect of recency decays very fast). I will not describe these here; the interested reader is referred to Section 2.6.5 of Kantor's dissertation (1977).\n\nKantor then defines the notion of the activatedness of a concept. This provides a continuum of Concept givenness, which contrasts with the simple binary given-new distinction usually accepted in linguistics (for example, Chafe 1970). Kantor also distinguishes activatedness from the similar \"communicative dynamism\" of the Prague school (Firbas 1964). Activated-4 From: Hamilton, Olive and Hamilton, Nigel. Royal Greenwich. Greenwich: The Greenwich Bookshop, 1969. Quoted by Halliday and Hasan (1976:14), quoted by Kantor (1977). ness is defined in terms of the comprehensibility phenomena described above: the more activated a concept is, the easier it is to understand an anaphoric reference to it. Thus activatedness depends upon discourse topic, context, and so forth.", "filtered_refids": [[], [], [], [], [], [], [], [], [], [], [null], ["b20", null, "b7", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 37, "num_chars": 4262, "num_references": 5}
{"corpusid_sectionid": "3176028-s3", "title": "Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review", "date": "1981-04-01", "section_title": "The implications of Kantor's work", "section": "What are the ramifications of Kantor's thesis for focus? Clearly, the notions of activatedness and focus are very similar, though the latter has not generally been thought of as a continuum. It follows that the factors Kantor finds relevant for activatedness and comprehensibility of pronouns are also important for those of us who would maintain focus in computerbased natural language understanding (NLU) systems; we will have to discover discourse topic and topic shifts, generate pronominalization expectations, and so forth.\n\nIn other words, if we could dynamically compute (and maintain) the activatedness of each concept floating around, we would have a measure for the ordering of the focus set by preferability as referent; the referent for any given anaphor would be the most highly activated element which passes basic tests for number, gender and semantic reasonableness. And to find the activatedness of the concepts, we follow Kantor's pointers (which he himself concedes are very tenuous and difficult) to extract and identify the relevant factors from the text.\n\nIt may be objected that by applying Kantor's insights all we have done is produce a mere notational variant of our original problem. This is partly true. One should not gainsay the power of a good notation, however, and what we can buy here even with mere notational variance is the power of Kantor's investigations. And there is more. Previously, it has been suggested that items either are in focus or they aren't, and that at each separate anaphor we need to compute a preference ranking of the focus elements for that anaphor. What Kantor tells us is that such a ranking exists independently of the actual use of anaphors in the text, and that we can find the ranking by looking at things like discourse topic. Some miscellaneous comments on Kantor's work:\n\n1. It can be seen as a generalization albeit a weakening of Grosz's (1977aGrosz's ( , 1977bGrosz's ( , 1978) findings on focus in task-oriented dialogues (where each sub-task becomes the new discourse topic, opening up a new set of possible referents), which are discussed below in Section 3. (Kantor and Grosz were apparently unaware of each other's work; neither cites the other.) 2. It provides an explanation for focus problems that have previously baffled us. For example, in Hirst (1977a) I contemplated the problem of the illformedness of this text:\n\n(2-3) *John left the window and drank the wine on the table. It was brown and round.\n\nI had previously thought this to be due to a syntactic factor --that cross-sentence pronominal reference to an NP in a relative clause or adjectival phrase qualifying an NP was not possible. However, it can also be explained as a grossly inconsiderate pronoun which does not refer to the topic properly --the table occurs only as a descriptor for the wine, and not as a concept \"in its own right\". This would be a major restriction on possible reference to sub-aspects of topics.\n\n3. Like too many other researchers, Kantor makes many claims about comprehensibility and the degree of well-formedness of sentences which others (as he concedes) may not agree with. He uses only himself (and his friends, sometimes) as an informant, and then only at an intuitive level. 5 Claims as strong and subtle as Kantor's cry out for empirical testing.6 Barbara Grosz (1977a, 1977b studied the maintenance of the focus of attention in task-oriented dialogues and its effect on the resolution of definite reference, as part of SRI's speech understanding system project (Walker 1978). By a task-oriented dialogue is meant one which has some single major welldefined task as its goal. For example, Grosz collected and studied dialogues in which an expert guides an apprentice in the assembly of an air compressor. She found that the structure of such dialogues parallels the structure of the task. That is, just as the major task is divided into several well-defined sub-tasks, and these perhaps into sub-sub-tasks and so on, the dialogue is likewise divided into sub-dialogues, sub-sub-dialogues, etc, 7 each corresponding to a task component, much as a well-structured Algol program is composed of blocks within blocks within blocks. As the dialogue progresses, each sub-dialogue in turn is performed in a strict depth-first order corresponding to the order of subtask performance in the task goal (though note that some sub-tasks may not be ordered with respect to 5 For a discussion of the problem of idiosyncratic wellformedness judgments, and a suggested solution, see Sections 4.2 and 7.3 of Hirst (1981). 6 Kantor tells me that he hopes to test some of his assertions by observing the eye movements of readers of considerate and inconsiderate texts, to find out if inconsiderate texts actually make readers physically search back for a referent.", "filtered_refids": [[], [], [], ["b14", "b9", "b8", "b10"], [], [], [null, "b28", "b8", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4822, "num_references": 8}
{"corpusid_sectionid": "3176028-s7", "title": "Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review", "date": "1981-04-01", "section_title": "Focus in the PAL system", "section": "The PAL personal assistant program (Bullwinkle 1977a) is a system designed to accept natural language requests for scheduling activities. A typical request (from Bullwinkle 1977b:44) is:\n\n(4-1) I want to schedule a meeting with Ira. It should be at 3 pm tomorrow. We can meet in Bruce's office.\n\nThe section of PAL that deals with discourse pragmatics and reference was developed by Candace Sidner [Bullwinkle] (Bullwinkle 1977b;Sidner 1978a). Like Grosz's system, PAL attempts to find a focus of attention in its knowledge structures to use as a focus for reference resolution.\n\nSidner sees the focus as equivalent to the discourse topic; in fact in Bullwinkle (1977b) the word topic is used instead of focus.\n\nThere are three major differences from Grosz's system:\n\n1. PAL does not rely heavily on discourse structures.\n\n2. Knowledge is represented in frames.\n\n3. Focus selection and shifting are handled at a more superficial level.\n\nI will discuss each difference in turn.", "filtered_refids": [["b1"], [], [null, "b2", "b37"], [], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 976, "num_references": 4}
{"corpusid_sectionid": "3176028-s9", "title": "Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review", "date": "1981-04-01", "section_title": "The frame as focus", "section": "The representation of knowledge in PAL is based on frames, and its implementation uses the FRL frame representation language (actually a dialect of LISP) developed by Goldstein (1977a, 1977b).\n\nIn PAL, the frame corresponds to Grosz's focus space. Following Rosenberg's (1976Rosenberg's ( , 1977 work on discourse structure and frames, the antecedent for a definite NP is first assumed to be either the frame itself, or one of its slots. So, for example, in (4-2):\n\n(4-2) I want to have a meeting with Ross (1). It should be at three pm. The location will be the department lounge. Please tell Ross (2). it refers to the MEETING frame (not to the text a meeting) which provides the context for the whole discourse; the location refers to the LOCATION slot that the MEETING frame presumably has (thus the CLOSELY ASSOCIATED WITH relation (Hirst 1981) is handled), and Ross (e) to the contents 9 of the CO-MEETER slot, previously given as Ross.\n\nIf the antecedent cannot be found in the frame, it is assumed to be either outside the discourse or inferred. In (4-2), PAL would search its database to find referents for Ross (1) and the department lounge. Personal names are resolved with a special module that knows about the semantics of names (Bullwinkle 1977b:48).\n\nPAL carries out database searches for references like the department lounge apparently by searching a hierarchy of frames, looking at the frames in the slots of the current focus, and then in the slots of these frames, and so on (Sidner 1978a:211), though it is not apparent why this should usefully constrain the search in the above example. 10 9 Sidner only speaks of reference to slots (1978a:211), without saying whether she means the slot itself or its contents; it seems reasonable to assume, as I have done here, that she actually means both.\n\n10 In fact there is no need in this particular example for a referent at all. The personal assistant need only treat the department lounge as a piece of text, presumably meaningful to both the speaker and Ross, denoting the meeting location. A human might do this when passing on a message he or she didn't understand: (i) Ross asked me to tell you to meet him in the arboretum, whatever the beck that is. On the other hand, an explicit antecedent would be needed if PAL had been asked, say, to deliver coffee to the meeting in the department lounge. Knowing when to be satisfied with ignorance is a difficult problem which Sidner does not consider, preferring the safe course of always requiring an antecedent.", "filtered_refids": [[null], ["b33", "b32"], ["b17"], [null], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2528, "num_references": 6}
{"corpusid_sectionid": "3176028-s10", "title": "Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review", "date": "1981-04-01", "section_title": "Focus selection", "section": "In PAL, the initial focus is the first NP following the main verb of the first sentence of the discourse -usually, the object of the sentence --or, if there is no such NP, then the subject of that sentence. This is a short-cut method, which seems to be sufficient for requests to PAL, but which Sidner readily admits is inadequate for the general case (Sidner 1978a:209). I will briefly review some of the problems. Charniak (1978) has shown that the frameselection problem (which is here identical to the initial focus selection problem, since the focus is just the frame representing the theme of the discourse) is in fact extremely difficult, and is not in the most general case amenable to solution by either strictly top-down or bottom-up methods.\n\nSidner's assumption that the relevant frame is given by an explicitly mentioned NP is also a source of trouble, even in the examples she quotes, such as these two (Sidner 1978b:92):\n\n(4-3) I was driving along th__ S freeway the other day. Suddenly the engine began to make a funny noise.\n\n(4-4) I went to a new restaurant with Sam. The waitress was nasty. The food was great.\n\n(Underlining indicates what Sidner claims is the focus.) In (4-3), Sidner posits a chain of inferences to get from the engine to the focus, the FREEWAY frame.\n\nThis is more complex than is necessary; if the frame/focus were DRIVING (with its LOCATION slot containing the FREEWAY frame), then the path from the frame to the engine is shorter and the whole arrangement seems more natural. Thus we see that focus need not be based on an NP at all.\n\nIn (4-4), our problem is what to do with Sam, who could be referenced in a subsequent sentence.\n\nIt is necessary to integrate Sam into the RESTAURANT frame/focus, since clearly he should not be considered external to the discourse and sought in the database. While the RESTAURANT frame may indeed contain a COMPANION slot for Sam to sit in, it is clear that the first sentence could have been I went <anywhere at all> with Sam, requiring that any frame referring to something occupying a location must have a COMPANION slot. This is clearly undesirable.\n\nBut the RESTAURANT frame is involved in (4-4); otherwise the waitress and the food would be external to the discourse. A natural solution is that the frame/focus of (4-4) is actually the GOING-SOMEWHERE frame (with Sam in its COMPANION slot), containing the RESTAURANT frame in its PLACE slot, with both frames together taken as the focus. Sidner does not consider mechanisms for a multi-frame focus.\n\nIt is, of course, not always true that the frame/focus is explicit. Charniak (1978) points out that (4-5) is somehow sufficient to invoke the MAGICIAN frame:\n\n(4-5) The woman waved as the man on stage sawed her in half.\n\n(See also Charniak (1981) for more on frame invocation problems.)\n\nFocus shifting in PAL is restricted: the only shifts permitted are to and from sub-aspects of the present focus (Sidner 1978a:209). Old topics are stacked for possible later return. This is very similar to Grosz's open-focus hierarchy. It is unclear whether there is a predictive aspect to PAL's focus-shift mechanism, 11 but the basic idea seems to be that any new phrase in a sentence is picked as a potential new focus. If in a subsequent sentence an anaphoric reference is a semantically acceptable coreferent for that potential focus, then a shift to that focus is ipso facto indicated (Sidner 1978a:209).\n\nPresumably this check is done after a check of focus has failed, but before any database search. A potential focus has a limited life span, and is dropped if not shifted to by the end of the second sentence following the one in which it occurred.\n\nAn example (Sidner 1978a:209):\n\n(4-6) I want to schedule a meeting with George, Jim, Steve and Mike. We can meet in my office. It's kind of small, but the meeting won't last long anyway.\n\n(4-7) I want to schedule a meeting with George, Jim, Steve and Mike. We can meet in my office. It won't take more than 20 minutes.\n\nIn the second sentence my office is identified as a potential focus, and it, in the first reading of the third sentence, as an acceptable coreferent to my office confirms the shift. In the second reading, it couldn't be my office, so no shift occurs. The acceptability decision is based on selectional and case-like restrictions.\n\nWhile perhaps adequate for PAL, this mechanism is, of course, not sufficient for the general case, where a true shift, as opposed to an expansion upon a previll On page 209 of Sidner (1978a) we are told: \"Focus shifts cannot be predicted; they are detectable only after they occur\". Yet on the following page, Sidner says: \"Sentences appearing in mid-discourse are assumed to be about the focus until the coreference module predicts a focus shift ....\n\nOnce an implicit focus relation is established, the module can go onto [sic] predictions of focus shift\". My interpretation of these remarks is that one cannot be certain that the next sentence will shift focus, but one can note when a shift might happen, requiring later checking to confirm or disconfirm the shift. ously mentioned point, may occur. This is exemplified by many of the shifts in Grosz's task-oriented dialogues.\n\nAnother problem arising from this shift mechanism is that two different focus shifts may be indicated at the same time, but the mechanism has no way to choose between them. For example:\n\n(4-8) Schedule a meeting of t..h_e Experimental Theology Research Group, and tell Ross Andrews about it too. I'd like him to hear about the deocommunication work that they're doing.\n\nEach of the two underlined NPs in the first sentence would be picked as a potential focus. Since each is pronominally referenced in the second sentence, the mechanism would be confused as to where to shift the focus. (Presumably Ross Andrews would be the correct choice here.)", "filtered_refids": [[null, "b4"], [null], [], [], [], [], [], [], [], ["b4"], [], ["b5"], [null], [], [null], [], [], [], ["b37"], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 50, "num_chars": 5862, "num_references": 8}
{"corpusid_sectionid": "3176028-s12", "title": "Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review", "date": "1981-04-01", "section_title": "Webber's formalism", "section": "In the preceding sections of this paper, we saw approaches to anaphor resolution that were mainly top-down in that they relied on a notion of theme and/or focus of attention to guide the selection of focus (although theme determination may have been bottom-up).\n\nAn alternative approach has been suggested by Bonnie [Nash-]Webber (Nash- Webber and Reiter 1977;Webber 1978aWebber , 1978b, wherein a set of rules is applied to a logical-form representation of the text to derive the set of entities that that text makes available for subsequent reference. Webber's formalism attacks some problems caused by quantification that have not otherwise been considered by workers in NLU, 12 In her thesis (1979) [which was not available to me when this paper was first written], Sidner subsequently proposed the use of an association network instead of frames, and presented more sophisticated focus selection and shifting algorithms. I have emphasized her earlier work here, as it has received much wider circulation. I can only give the flavor of Webber's formalism here, and I shall have to assume some familiarity with logical forms. Readers who want more details should see her thesis (1978a); readers who find my exposition mystifying should not worry unduly --the fault is probably mine --but should turn to the thesis for illumination.\n\nIn Webber's formalism, it is assumed that an input sentence is first converted to a parse tree, and then, by some semantic interpretation process, to an extended restricted-quantification predicate calculus representation. It is during this second conversion that anaphor resolution takes place. When the final representation, which we shall simply call a logical form, is complete, certain rules are applied to it to generate the set of referable entities and descriptions that the sentence evokes. Webber considers three types of antecedents those for definite pronouns, those for one-anaphora, 13 and those for verb phrase ellipsis. Each type has its own set of rules; we will briefly look at the first.\n\n(The others are discussed in Sections 5.4.2 and 5.4.3 of Hirst 1981.) ", "filtered_refids": [[], ["b43", "b44", "b25"], [], ["b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2114, "num_references": 4}
{"corpusid_sectionid": "3176028-s16", "title": "Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review", "date": "1981-04-01", "section_title": "Discourse-cohesion approaches to anaphora resolution", "section": "Another approach to coreference resolution attempts to exploit local discourse cohesion, building a representation of the discourse with which references can be resolved. This approach has been taken by (inter alia) Klappholz and Lockman (1977;Lockman 1978). By using only cues to the discourse structure at the sentence level or lower, one avoids the need to search for referents in pre-determined dialogue models such as those of Grosz's task-oriented dialogues, or rigidly predefined knowledge structures such as scripts (Schank and Abelson 1977) and frames (Minsky 1975), which Klappholz and Lockman, for example, call overweight structures that inflexibly dominate processing of text. Klappholz and Lockman emphasize that the structure through which reference is resolved must be dynamically built up as the text is processed; frames or scripts could assist in this building, but cannot, however, be reliably used for refer-ence resolution, because deviations by the text from the pre-defined structure will cause errors.\n\nThe basis of this approach is that there is a strong interrelationship between coreference and the cohesive ties in a discourse that make it coherent. By determining what the cohesive ties in a discourse are, one can put each new sentence or clause, as it comes in, into the appropriate place in a growing structure that represents the discourse. This structure can then be used as a focus to search for coreference antecedents, since not only do coherently connected sentences tend to refer to the same things, but knowledge of the cohesion relation can provide additional reference resolution restraints.  in particular sees the problem of coreference resolution as being automatically solved in the process of discovering the coherence relations in a text. (An example of this will be given in Section 6.2.) Conversely, it is frequently helpful or necessary to resolve coreference relations in order to discover the coherence relations. This is not a vicious circle, claims Hobbs, but a spiral staircase.\n\nIn our discussion below, we will cover four issues:\n\n1. deciding on a set of possible coherence relations;\n\n2. detecting them when they occur in a text;\n\n3. using the coherence relations to build a focus structure; and 4. searching for referents in the structure.", "filtered_refids": [["b21", "b22", "b34", "b23"], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2300, "num_references": 4}
{"corpusid_sectionid": "3176028-s17", "title": "Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review", "date": "1981-04-01", "section_title": "Coherence relations", "section": "The first thing required by this approach is a complete and computable set of the coherence relations that may obtain between sentences and/or clauses. Various sets have been suggested by many people, including Eisenstadt (1976), Phillips (1977), Pitkin (1977aPitkin ( , 1977b, Hirst (1977bHirst ( , 1978, Lockman (1978),  and Reichman (1978). 15 None of these sets fulfill all desiderata; and while Halliday and Hasan (1976) provide an extensive analysis of cohesion, it does not fit within our computational framework of coherence relations, and those, such as Hobbs, Lockman, Eisenstadt and Hirst, who emphasize computability, provide sets insufficient, I believe, to capture all the semantic subtleties of discourse cohesion. Nevertheless, the works cited above undoubtedly serve as a useful starting point for development of this area.\n\nTo illustrate what a very preliminary set of cohesion relations could look like, I will briefly present a set abstracted from the various sets of Eisenstadt, Hirst, Hobbs, Lockman and Phillips (but not faithful to any one of these).\n\nThe set contains two basic classes of coherence relations: expansion or elaboration on an entity, concept or event in the discourse, and temporal continuation or time flow. Expansion includes relations like EFFECT, CAUSE, SYLLOGISM, ELABORATION, CONTRAST, PARALLEL and EXEMPLIFICATION. In the following examples, \"u\" is used to indicate the point where the cohesive tie illustrated is acting: (One may disagree with my classification of some of the relations above; the boundaries between categories are yet ill-defined, and it is to be expected that some people's intuitions will differ from mine.)\n\nTemporal flow relations involve some continuation forwards or backwards over time:\n\n(6-8) VICTORIA --A suntanned Prince Charles arrived here Sunday afternoon, \u2022 and was greeted with a big kiss by a pretty English au pair girl. 17 (6-9) SAN JUAN, Puerto Rico --Travel officials tackled a major job here Sunday to find new accommodations for 650 passengers from the burned Italian cruise liner\n\nAngelina Lauro.\n\n\u2022 The vessel caught fire Friday while docked at Charlotte Amalie in the Virgin Islands, but most passengers were ashore at the time. 18\n\nTemporal flow may be treated as a single relation, as Phillips, for example, does, or it may be subdivided, as by Eisenstadt and Hirst, into categories like TIME STEP, FLASHBACK, FLASHFORWARD, TIME EDIT, and so on. Certainly, time flow in a text may be quite contorted, as in (6-10) (from Hirst 1978); \"m\" indicates a point where the direction of the time flow changes:\n\n(6-10) Slowly, hesitantly, Ross approached Nadia. \u2022 He had waited for this moment for many days. \u2022 Now he was going to say the words \u2022 which he had agonized over \u2022 and in the very room \u2022 he had often dreamed about. \u2022 He gazed lovingly at her soft green eyes.\n\nIt is not clear, however, to what extent an analysis of time flow is necessary for anaphor resolution. I suspect that relatively little is necessary --less than is required for other aspects of discourse understanding.\n\nI see relations like those exemplified above as primitives from which more complex relations could be built. For example, the relation between the two sentences of (6-3) above clearly involves FORWARD TIME STEP as well as EFFECT. I have hypothesized elsewhere (Hirst 1978) the possibility of constructing a small set of discourse relations (with cardinality about twenty or less) from which more complex relations may be built up by simple combination, and, one hopes, in such a way that the effects of relation Ri+R 2 would be the sum of the individual effects of relations R 1 and R 2. Rules for permitted combinations would be needed; for example, FORWARD TIME STEP could combine with EFFECT, but not with BACKWARD TIME STEP. What would the formal definition of a coherence relation be like? Here is Hobbs's (1979:73) definition of ELABORATION: Sentence S 1 is an ELABORATION of sentence S O if some proposition P follows from the assertions of both S O and $1, but S 1 contains a property of one of the elements of P that is not in S 0. The example in the next section will clarify this.", "filtered_refids": [["b15", "b11", "b6", "b26", "b28", "b29", "b27", "b22", "b16"], [], [], [], [], [], [], ["b16"], [], [], ["b16", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4166, "num_references": 12}
{"corpusid_sectionid": "234093015-s1", "title": "A Survey of Data Augmentation Approaches for NLP", "date": "2021-05-07", "section_title": "Background", "section": "What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hern\u00e1ndez-Garc\u00eda and K\u00f6nig, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.\n\nWhat are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.\n\nRule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.\n\nFurther, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.\n\nKashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical \"run-all-heuristics\" comparison, which can be very time and cost intensive.\n\nInterpretation of DA Dao et al. (2019) note that \"data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles\", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in \u00a76, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.", "filtered_refids": [[null], [null, "b23"], [null, "b1"], [], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3180, "num_references": 6}
{"corpusid_sectionid": "234093015-s3", "title": "A Survey of Data Augmentation Approaches for NLP", "date": "2021-05-07", "section_title": "Rule-Based Techniques", "section": "Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model's feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space \"analogy\" transformations between examples of known classes to augment for novel classes (see \u00a74.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally \"stretch\" an example along the class-manifold. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap. They show improved performance on many text classification tasks. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.\n\nFor paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges. They use balance theory and transitivity to infer augmented sentence pairs from this graph. Motivated by image cropping and rotation, \u015eahin and Steedman (2018) propose dependency tree morphing. For dependency-annotated sentences, children of the same parent are swapped (\u00e0 la rotation) or some deleted (\u00e0 la cropping), as seen in Figure 2. This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic).", "filtered_refids": [[null, "b1", "b7"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1538, "num_references": 4}
{"corpusid_sectionid": "234093015-s4", "title": "A Survey of Data Augmentation Approaches for NLP", "date": "2021-05-07", "section_title": "Example Interpolation Techniques", "section": "Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).\n\nAnother class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, \u015eahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see \"Multimodal challenges\" in \u00a76).\n\nA bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the \"hard\" version samples a binary mask (from a Bernoulli with a \u03b2(\u03b1, \u03b1) prior) and picks from one of two sequences at each token position, while the \"soft\" version softly interpolates between sequences based on a coefficient sampled from \u03b2(\u03b1, \u03b1). The \"soft\" version is found to outperform the \"hard\" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).", "filtered_refids": [[null, "b20"], [null, "b19"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2131, "num_references": 5}
{"corpusid_sectionid": "234093015-s10", "title": "A Survey of Data Augmentation Approaches for NLP", "date": "2021-05-07", "section_title": "Tasks", "section": "In this section, we discuss several DA works for common NLP tasks. 2 We focus on nonclassification tasks as classification is worked on by default, and well covered in earlier sections (e.g.\n\n\u00a73 and \u00a74 which substitutes a portion of the input text with its translation in another language, improving performance across multiple languages on NLI tasks including the SQuAD QA task. Asai and Hajishirzi (2020) use logical and linguistic knowledge to generate additional training data to improve the accuracy and consistency of QA responses by models. Yu et al. (2018) introduce a new QA architecture called QANet that shows improved performance on SQuAD when combined with augmented data generated using backtranslation. Dai and Adel (2020) modify DA techniques proposed for sentence-level tasks for named entity recognition (NER), including label-wise token and synonym replacement, and show improved performance using both recurrent and transformer models. Zhang et al. (2020) propose a DA method based on MIXUP called SEQMIX for active sequence labeling by augmenting queried samples, showing improvements on NER and Event Detection.", "filtered_refids": [[null], ["b21", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1133, "num_references": 3}
{"corpusid_sectionid": "235790370-s1", "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments", "date": "2021-07-08", "section_title": "Up a Tree", "section": "You are about 10 feet above the ground nestled among some large branches. On the branch is a small birds nest. In the bird's nest is a large egg encrusted with precious jewels, apparently scavenged somewhere by a childless songbird. >take egg\n\nTaken. >climb down tree   (Lebling et al., 1979), frequently used as a benchmark for agent performance. User-entered actions are italicized.\n\nemerged as a recent methodological focus that allow studying many embodied research questions while reducing some of the development costs associated with modeling complex and photorealistic 3D environments (e.g. C\u00f4t\u00e9 et al., 2018). More than simply reducing development costs, Text Worlds also offer paradigms to study developmental knowledge representation, embodied task learning, and transfer learning at a higher level than perceptuallygrounded studies, enabling different research questions that explore these topics in isolation of the open problems of perceptual input, object segmentation, and object classification regularly studied in the vision community (e.g. He et al., 2016c;Szegedy et al., 2017;Zhai et al., 2021).", "filtered_refids": [[], [null], [null, "b9", "b34", "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1116, "num_references": 5}
{"corpusid_sectionid": "235790370-s3", "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments", "date": "2021-07-08", "section_title": "Why use Text Worlds?", "section": "For many tasks, Text Worlds can offer advantages over other embodied environment modelling paradigms -typically in reduced development costs, the ability to model large action spaces, and the ability to study embodied reasoning at a higher level than raw perceptual information.\n\nEmbodied Reasoning: Embodied agents have been proposed as a solution to the symbol grounding problem (Harnad, 1990), or the problem of how concepts acquire real-world meaning. Humans likely resolve symbol grounding at least partially by assigning semantics to concepts through perceptually-grounded mental simulations (Barsalou et al., 1999). Using embodied agents that take in perceptual data and perform actions in real or virtual environments offers an avenue for studying semantics and symbol grounding empirically (Cangelosi et al., 2010;Bisk et al., 2020;Tamari et al., 2020a,b). Text Worlds abstract some of the challenges in perceptual modeling, allowing agents to focus on higher-level semantics, while hybrid worlds that simultaneously render both text and 3D views (e.g. Shridhar et al., 2020b) help control what kind of knowledge is acquired, and better operationalize the study of symbol grounding.\n\nEase of Development: Constructing embodied virtual environments typically has steep development costs, but Text Worlds are typically easier to construct for many tasks. Creating new objects does not require the expensive process of creating new 3D models, or performing visualpercept-to-object-name segmentation or classification (since the scene is rendered linguistically). Similarly, a rich action semantics is possible, and comparatively easy to implement -while 3D environments typically have one or a small number of action commands (e.g. Kolve et al., 2017;Shridhar et al., 2020a), Text Worlds typically implement dozens of action verbs, and thousands of valid Verb-NounPhrase action combinations (Hausknecht et al., 2020).\n\nCompositional Reasoning: Complex reasoning tasks typically require multi-step (or compositional) reasoning that integrates several pieces of knowledge in an action procedure that arrives at a solution. In the context of natural language, compositional reasoning is frequently studied through question answering tasks (e.g. Yang et al., 2018;Khot et al., 2020;Xie et al., 2020;Dalvi et al., 2021) or procedural knowledge prediction (e.g. Dalvi et al., 2018;Tandon et al., 2018;Dalvi et al., 2019). A contemporary challenge is that the number of valid compositional procedures is typically large compared to those that can be tractably annotated as gold, and as such automatically evaluating model performance becomes challenging (Jansen et al., 2021). In an embodied environment, an agent's actions have (generally) deterministic consequences for a given environment state, as actions are grounded in an underlying action language (e.g. McDermott et al., 1998) or linear logic (e.g. Martens, 2015. Embodied environments can offer a more formal semantics to study these reasoning tasks, where correctness of novel procedures could be evaluated directly.\n\nTransfer Learning: Training a text-only agent for embodied tasks allows the agent to learn those tasks in a distilled form, at a high-level. This performance can then be transferred to more realistic 3D environments, where agents pretrained on text versions of the same environment learn to ground their high-level knowledge in low-level perceptual information, and complete tasks faster than when trained jointly (Shridhar et al., 2020b). This offers the possibility of creating simplified text worlds to pretrain agents for challenging 3D tasks that are currently out of reach of embodied agents.", "filtered_refids": [[], [null, "b7"], [null, "b6", "b27"], ["b14", null, "b22", "b25"], ["b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 3676, "num_references": 10}
{"corpusid_sectionid": "235790370-s4", "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments", "date": "2021-07-08", "section_title": "Text World Simulators", "section": "Text World simulators render an agent's world view directly into textual descriptions of their environment, rather than into 2D or 3D graphical renderings. Similarly, actions the agent wishes to take are provided to the simulator as text (e.g. \"read the letter\" in Zork), requiring agent models to both parse input text from the environment, and generate output text to to interact with that environment.\n\nIn terms of simulators, the Z-machine (Infocom, 1989) is a low-level virtual machine originally designed by Infocom for creating portable interactive fiction novels (such as Zork). It was paired with a high-level LISP-like domain-specific language (ZIL) that included libraries for text parsing, and other tools for writing interactive fiction novels. The Z-machine standard was reverse-engineered by others (e.g. Nelson, 2014) in an effort to build their own high-level interactive fiction domain-specific languages, and has since become a standard compilation target due to the proliferation of existing tooling and legacy environments. 1 Inform7 (Nelson, 2006) is a popular high-level language designed for interactive fiction novels that allows environment rules to be directly specified in a simplified natural language, substantially lowering the barrier to entry for creating text worlds. The text generation engine allows substantial variation in the way the environments are described, from dry formulaic text to more natural, varied, conversational descriptions. Inform7 is compiled to Inform6, an earlier object-oriented scripting language with C-like syntax, which itself is compiled to Z-machine code.\n\nCeptre (Martens, 2015) is a linear-logic simulation engine developed with the goal of specifying more generic tooling for operational logics than Inform 7. TextWorld (C\u00f4t\u00e9 et al., 2018) adapt Ceptre's linear logic state transitions for environment descriptions, and add tooling for generative environments, visualization, and RL agent coupling, all of which is compiled into Inform7 source code. Parallel to this, the Jericho environment (Hausknecht et al., 2020) allows inferring relevant vocabulary and template-based object interactions for Z-machine-based interactive fiction games, easing action selection for agents. 1 A variety of text adventure tooling, including the Adventure Game Toolkit (AGT) and Text Adventure Development System (TADS), was developed starting in the late 1980s, but these simulators have generally not been adopted by the NLP ", "filtered_refids": [[], ["b2", "b3"], ["b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2479, "num_references": 3}
{"corpusid_sectionid": "235790370-s6", "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments", "date": "2021-07-08", "section_title": "Environment Modelling", "section": "Environments are typically modeled as an object tree that represents all the objects in an environment and their nested locations, as well as a set of action rules that implement changes to the objects in the environment based on an agent's actions.\n\nObjects: Because of the body of existing interactive fiction environments for Z-machine environments, and nearly all popular tooling (In-form7, TextWorlds, etc.) ultimately compiling to Z-machine code, object models typically use the Zmachine model (Nelson, 2014). Z-machine objects have names (e.g. \"mailbox\"), descriptions (e.g. \"a small wooden mailbox\"), binary flags called attributes (e.g. \"is_container_open\"), and generic properties stored as key-value pairs. Objects are stored in the object tree, which represents the locations of all objects in the environment through parent-child relationships, as shown in Figure 1.\n\nAction Rules: Action rules describe how objects change in response to a given world state, which is frequently a collection of preconditions followed by an action taken by an agent (e.g. \"eat the apple\"), but can also be due to environment states (e.g. a plant dying because it hasn't been watered for a time greater than some threshold). Ceptre (Martens, 2015) and to use mature action languages such as STRIPS (Fikes and Nilsson, 1971) or GDL (Genesereth et al., 2005;Thielscher, 2010Thielscher, , 2017 as the basis of a world model, though each of these languages have tradeoffs in features (such as object typing) and general expressivity (such as being primarily agent-action centered, rather than implementing environment-driven actions and processes) that make certain kinds of complex modeling more challenging. As a proof-of-concept, ALFWorld (Shridhar et al., 2020b) uses the Planning Domain Definition Language (PDDL, McDermott et al., 1998) to define the semantics for the variety of pick-and-place tasks in its text world rendering of the ALFRED benchmark.", "filtered_refids": [[], ["b3"], [null, "b7", "b18", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1950, "num_references": 5}
{"corpusid_sectionid": "235790370-s12", "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments", "date": "2021-07-08", "section_title": "Generative Environments", "section": "A difficulty with statically-initialized environments is that because their structure is identical each time the simulation is run, rather than learning general skills, agents quickly overfit to a particular task and environment, and rarely generalize to unseen environments (Chaudhury et al., 2020). Procedurally generated environments help address this need by generating variations of environments centered around specific goal conditions. The TextWorld simulator (C\u00f4t\u00e9 et al., 2018) allows specifying high-level parameters such as the number of rooms, objects, and winning conditions, then uses a random walk to procedurally generate environment maps in the Inform7 language meeting those specifications, using either forward or backward chaining during generation to verify tasks can be successfully completed in the random environment. As an example, the First TextWorld Problems shared task 2 used TextWorld to generate 5k variations of a cooking environment, divided into train, development, and test sets. Similarly, Murugesan et al. (2020a) introduce TextWorld Com-monSense (TWC), a simple generative environment for household cleaning tasks, modelled as a pick-and-place task where agents must pick up common objects from the floor, and place them in their common household locations (such as placing shoes in a shoe cabinet). Other related environments include Coin Collector (Yuan et al., 2018), a generative environment for a navigation task, and Yin et al.'s (2019b) procedurally generated environment for cooking tasks. Adhikari et al. (2020) generate a large set of recipe-based cooking games, where an agent must precisely follow a cooking recipe that requires collecting tools (e.g. a knife) and ingredients (e.g. carrots), and processing those ingredients correctly (e.g. dice carrots, cook carrots) in the correct order. Jain et al. (2020) propose a similar synthetic benchmark for multi-step compositional reasoning called SaladWorld. In the context of question answering,  procedurally generate a simple environment that requires an agent to search and investigate attributes of objects, such as verifying their existence, locations, or specific attributes (like edibility). On the balance, while tooling exists to generate simple procedural environments, when compared to classic interactive fiction games (such as Zork), the current state-of-the-art allows for generating only relatively simple environments with comparatively simple tasks and near-term goals than human-authored interactive fiction games.", "filtered_refids": [[null, "b31", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2531, "num_references": 3}
{"corpusid_sectionid": "235790370-s13", "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments", "date": "2021-07-08", "section_title": "Text World Agents", "section": "Recently a large number of agents have been proposed for Text World environments. This section briefly surveys common modeling methods, paradigms, and trends, with the performance of recent agents on common interactive fiction games (as categorized by the Jericho benchmark, Hausknecht et al., 2020) shown in Table 2.\n\nReinforcement Learning: While some agents rely on learning frameworks heavily coupled with heuristics (e.g., Kostka et al., 2017, Golovin), owing to the sampling benefits afforded by operating in a virtual environment, the predominant modeling paradigm for most contemporary text world agents is reinforcement learning. Narasimhan et al. (2015) demonstrate that \"Deep-Q Networks\"  To support these modelling paradigms, Zelinka et al. (2019) introduce TextWorld KG, a dataset for learning the subtask of updating knowledge graphs based on text world descriptions in a cooking domain, and show their best ensemble model is able to achieve 70 F1 at this subtask. Similarly, Annamabrolu et al. (2021a) introduce JerichoWorld, a similar dataset for world modeling using knowledge graphs but on a broader set of interactive fiction games, and subsequently introduce World-Former (Ammanabrolu and Riedl, 2021b), a multitask transformer model that performs well at both knowledge-graph prediction and next-action prediction tasks. Question Answering: Agents can reframe Text World tasks as question answering tasks to gain relevant knowledge for action selection, with these agents providing current state-of-the-art performance across a variety of benchmarks. ", "filtered_refids": [[], [null, "b1", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1572, "num_references": 3}
{"corpusid_sectionid": "235790370-s14", "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments", "date": "2021-07-08", "section_title": "Contemporary Focus Areas", "section": "World Generation: Generating detailed environments with complex tasks is labourious, while randomly generating environments currently provides limited task complexity and environment cohesiveness. World generation aims to support the generation of complex, coherent environments, either through better tooling for human authors (e.g. Temprado-Battad et al., 2019), or automated generation systems that may or may not have a humanin-the-loop. Fan et al. (2020) explore creating cohesive game worlds in the LIGHT environment using a variety of embedding models including Starspace (Wu et al., 2018a) and BERT (Devlin et al., 2019). Automatic evaluations show performance of between 36-47% in world building, defined as cohesively populating an environment with locations, objects, and characters. Similarly, hu-man evaluation shows that users prefer Starspacegenerated environments over those generated by a random baseline. In a more restricted domain, Ammanabrolu et al. (2019) show that two models, one Markov chain model, the other a generative language model (GPT-2), are capable of generating quests in a cooking environment, while there is a tradeoff between human ratings of quest creativity and coherence.\n\nAmmanabrolu et al. (2020a) propose a largescale end-to-end solution to world generation that automatically constructs interactive fiction environments based on a story (such as Sherlock Holmes) provided as input. Their system first builds a knowledge graph of the story by framing KG construction as a question answering task, using their model (AskBERT) to populate this graph. The system then uses either a rule-based baseline or a generative model (GPT-2) to generate textual descriptions of the world from this knowledge graph. User studies show that humans generally prefer these neural-generated worlds to the rule-generated worlds (measured in terms of interest, coherence, and genre-resemblance), but that neural-generated performance still substantially lags behind that of human-generated worlds.\n\nHybrid 3D-Text Environments: Hybrid simulators that can simultaneously render worlds both graphically (2D or 3D) as well as textually offer a mechanism to quickly learn high-level tasks without having to first solve grounding or perceptual learning challenges. The ALFWorld simulator (Shridhar et al., 2020b) combines the ALFRED 3D home environment (Shridhar et al., 2020a) with a simultaneous TextWorld interface to that same environment, and introduce the BUTLER agent, which shows increased task generalization on the 3D environment when first trained on the text world. Prior to ALFWorld, Jansen (2020) showed that a language model (GPT-2) was able to successfully generate detailed step-by-step textual descriptions of ALFRED task trajectories for up to 58% of unseen cases using task descriptions alone, without visual input. Building on this, Micheli (2021) confirmed GPT-2 also performs well on the text world rendering of ALFWorld, and is able to successfully complete goals in 95% of unseen cases. Taken together, these results show the promise of quickly learning complex tasks at a high-level in a text-only environment, then transferring this performance to agents grounded in more complex environments.", "filtered_refids": [[null, "b20"], [], ["b6", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 3238, "num_references": 4}
{"corpusid_sectionid": "235790370-s16", "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments", "date": "2021-07-08", "section_title": "A Extended List of Simulators", "section": "Simulators provide the infrastructure to implement the environments, objects, characters, and interactions of a virtual world, typically through a combination of a scripting engine to define the behavior of objects and agents, with a rendering engine that provides a view of the world for a given agent or user. Simulators for embodied agents exist on a fidelity spectrum, from photorealistic 3D environments to worlds described exclusively with language, where a trade-off typically exists between richer rendering and richer action spaces. This fidelity spectrum (paired with example simulators) is shown in Table 3, and described briefly below. Note that many of these higher-fidelity simulators are largely out-of-scope when discussing Text Worlds, except as a means of contrast to text-only worlds, and in the limited context that these simulators make use of text.\n\n3D Environment Simulators: 3D simulators provide the user with complex 3D environments, including near-photorealistic environments such as AI2- Thor (Kolve et al., 2017), and include physics engines that model forces, liquids, illumination, containment, and other object interactions. Because of their rendering fidelity, they offer the possibility of inexpensively training robotic models in virtual environments that can then be transferred to the real world (e.g. RoboThor, Deitke et al., 2020). Adding objects to 3D worlds can be expensive, as this requires 3D modelling expertise that teams may not have. Similarly, adding agent actions or object-object interactions through a scripting language can be expensive if those actions are outside what is easily implemented in the simulator (like creating gasses, or using a pencil or saw to modify  Table 3: Example embodied simulation environments broken down by environment rendering fidelity. D specifies that environments supply natural language directives to the agent, I specifies that environments are interacted with (at least in part) using natural language input and/or output, and no rating represents environments that do not have a significant text component.\n\nan object). Because of this, action spaces tend to be small, and limited to movement, and one (or a small number of) interaction commands. Some simulators and environments include text directives for an agent to perform, such as an agent being asked  to \"slice an apple then cool it\" in the ALFRED environment (Shridhar et al., 2020a). Other hybrid environments such as ALFWorld (Shridhar et al., 2020b) simultaneously render an environment both in 3D as well as in text, allowing agents to learn high-level task knowledge through text interactions, then ground these in environment-specific perceptual input though transfer learning.\n\nVoxel-based Simulators: Voxel-based simulators create worlds from (typically) large 3D blocks, lowering rendering fidelity while greatly reducing the time and skill required to add new objects. Similarly, creating new agent-object or object-object interactions can be easier because they can generally be implemented in a coarser manner -though some kinds of basic spatial actions (like rotating an object in increments smaller than 90 degrees) are generally not easily implemented. Malmo (Johnson et al., 2016) and MineRL (Guss et al., 2019) offer wrappers and training data to build agents in the popular Minecraft environment. While the agent's action space is limited in Minecraft (see Table 4), the crafting nature of the game (that allows collecting, creating, destroying, or combining objects using one or more voxels) affords exploring a variety of compositional reasoning tasks with a low barrier to entry, while still using a 3D environment. Text directives, like those in CraftAssist (Gray et al., 2019), allow agents to learn to perform compositional crafting actions in this 3D environment from natural language dialog.\n\nGridWorld Simulators: 2D gridworlds are comparatively easier to construct than 3D environments,", "filtered_refids": [[], [null], ["b6", "b7"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3962, "num_references": 4}
{"corpusid_sectionid": "236460206-s2", "title": "Towards Argument Mining for Social Good: A Survey", "date": 2021, "section_title": "Framework", "section": "Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including N\u00e4ive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).\n\nRelation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.\n\nTo simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.\n\nConsider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question \"Does public health demand vaccinations?\" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.\n\nA2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.\n\nHere, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.\n\nA1: Marvel Universe is better than DC Universe.\n\nA2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.\n\nA3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.\n\nA4: This is especially true due to his unfortunate passing.\n\nA5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.\n\nThe seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).\n\nClearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).", "filtered_refids": [["b26", null, "b28", "b29"], ["b49", "b14"], [null, "b44", "b35", "b31"], [], [], [], [], [], [], [], [], [], ["b44"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 36, "num_chars": 5945, "num_references": 11}
{"corpusid_sectionid": "236460206-s3", "title": "Towards Argument Mining for Social Good: A Survey", "date": 2021, "section_title": "Scaling Up Argument Mining", "section": "In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.\n\nSocial media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.\n\nRecent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.\n\nDespite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.\n\nThe linguistic, structural, and logistic complexity and \"openness\" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.\n\nMultilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.\n\nVarious recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.\n\nOther work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.", "filtered_refids": [[], ["b15"], [null], ["b38"], [], [null], ["b16", "b52", null], ["b62", null, "b38", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 5218, "num_references": 11}
{"corpusid_sectionid": "236460206-s4", "title": "Towards Argument Mining for Social Good: A Survey", "date": 2021, "section_title": "Argument Quality: An Integrated Definition", "section": "The second stage in the framework of AM is defined as relation assignment (c.f. Section 2.1); a complex task that aims to predict the relations holding between the arguments defined in the first stage. Being able to model the relations between arguments and components within the structure, for example in argument graphs (Besnard and Hunter, 2014; Craven and Toni, 2016), allows us to actually work with the argumentative text in an applicationbased setting, understand the stance and context of arguments, and develop a story for the consequential impact of arguments on the discourse, among other things. Generally speaking, we can use this task as an approach to analyze argument quality (AQ). However, within the AM community, an open question concerns the adequate definition and operationalization of the notion of AQ. Despite this, to move forward with the task of AQ analysis and to create large corpora with crowd-sourced annotations, some approaches rely on the relative assessment of quality: Given two arguments, which is more convincing? (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020) Thus the natural way of quantifying the success of an argument is in terms of its persuasiveness. Indeed, plenty of previous work has explored the many factors which contribute to the persuasiveness of a message: the linguistic features employed by the authors (Persing and Ng, 2017), the semantic type of claims and premises (Hidey et al., 2017), the different sources of evidence produced to support an argument (Addawood and Bashir, 2016), the effects of the personality traits and prior beliefs on persuasiveness (Lukin et al., 2017;Durmus and Cardie, 2018;Al Khatib et al., 2020), the interaction with other participants (Ji et al., 2018;Egawa et al., 2020), the use of argument invention when debating about unknown topics (Bilu et al., 2019), the structure of the arguments (Li et al., 2020), and the effect of the style of the text in achieving persuasion (El Baff et al., 2020).\n\nPersuasiveness is, however, not the only way to define whether an argument is good -at least not from a deliberation point of view. A good contribution to a debate is one which uncovers a previously unnoticed aspect of a problem, thus generating a perturbation in the discourse (controversies can be productive!). Or else, a good contribution is one that settles an issue, by stating the differences between opposing views and allowing the discourse to stabilize in a series of clusters (convergence on just one position is not necessarily a good outcome).\n\nMost recent research projects (Wachsmuth et al., 2017b) aim to address the challenge of redefining the notion of AQ, away from persuasiveness and towards a more \"situated\" definition which has to do with the needs of argumentation in a real-world scenario. This new definition has been the basis for the creation of new corpora from different domains , where feature-based (Wachsmuth and Werner, 2020) and neural models were tested for automatic prediction . Other aspects of AQ have become the subject of AM research such as the relevance and impact of arguments (Durmus et al., 2019), the verifiability (Park and Cardie, 2018), local acceptability (Yang et al., 2019) and the best \"deliberative move\" (Al-Khatib et al., 2018).\n\nWe argue that this shift is necessary for two reasons: (1) Working with real-world applications of AM naturally forces us into the more heterogeneous realm of data structures, such as social media, in which language, structure, and content are less uniform and confined to the classic notion of logical debate; and (2) In order to encourage deliberation from an open audience of citizens, we need to redefine our concept of AQ and productive discourse such that there is equal worth and participation granted to each contributor of the argument.\n\nDeliberative Quality We therefore propose adapting the definition of quality to integrate the abundant research on the topic from the field of Social Sciences. Here, the quality of a discourse has been investigated in the context of deliberation with the focus on inclusivity: how can the interplay of the different participants in the discourse lead to an optimal outcome for the collective? The focus here is not on the quality of the individual contributions. Instead, an overall quality of the discourse is determined by the fact that the individual quality dimensions are distributed among different contributions (e.g some participants do more rational reasoning, others share personal experiences). We would like to integrate those aspects that focus on inclusivity and cooperation.\n\nSimilar to Wachsmuth et al. (2017b), social scientists have developed a taxonomy, the discourse quality index (DQI), that describes the different desirable aspects of a discourse (Steenbergen et al., 2003). This taxonomy has been used to analyze the quality of deliberation in different contexts, ranging from more formal contexts, such as parliamentary debates (Steiner et al., 2005), to informal discussions in online forums (Tr\u00e9nel, 2004). Both implementations integrate logical coherence as one dimension, cogency in Wachsmuth et al. (2017b), justification in the DQI. Some aspects of inclusivity are also being touched upon in the rhetorical and dialectical dimension of Wachsmuth et al. (2017b), such as using appropriate language (Appropriateness) or whether an argument supports conflict resolution (global relevance). We concentrate on the following dimensions from the DQI, which particularly focus on the collaborative aspect of discourse.\n\n\u2022 Respect: this dimension includes respectful tone, respect for other social groups/backgrounds, and openness towards other opinions.\n\n\u2022 Equality / Participation: it is not desirable that some dominant participants make the bulk of contributions while many others remain passive. All participants should have equal opportunities to contribute and all topics, including those that DQI (Steenbergen et al., 2003)  \u2022 Interactivity: beyond simply sharing opinions, acknowledging other viewpoints and interacting with other participants through listening and responding lead to new perspectives arising -compromises can emerge.\n\n\u2022 Testimoniality / Report of personal accounts:\n\nsharing stories and personal narratives as an alternative form of communication can involve more people in the discourse, especially those who cannot identify themselves with rational argumentation. It can also make other participants aware of other perspectives as it generally increases empathy. Especially when traditional or universal norms need to be questioned, narratives are particularly well suited, as their ambiguity and vagueness creates room for interpretation. This is particularly important when new ideas or perspectives are introduced, since they cannot yet be rationally articulated. Table 1 establishes a direct comparison between discourse quality dimensions of the DQI (Steenbergen et al., 2003;Steiner et al., 2005) and argument quality dimensions as defined in Wachsmuth et al. (2017b). Apart from the potential theoretical insights, the existing guidelines can be applied to annotate new or enrich existing corpora for AM. Despite the small size, the data already annotated based on the DQI can be made usable and extended for NLP. In addition, some of the quality dimensions can be further quantified or approximated using statistical methods. For example, interactivity or equality can be assessed with frequency-based methods, such as frequency of posts by distinct participants and response rate.\n\nSumming up The overview of the definitions of AQ along with the discussion of the potential of the integration of Deliberative Quality features into an AM framework has one strong take-home message:\n\nThe need for the scope of the investigation to go beyond (a) the persuasiveness of a an argumentative text (speeches, forum posts, tweets), and (b) their relation to the immediate preceding discourse. Instead, we pointed out the need to also assess the potential of the impact of that argumentative text on the upcoming discourse: this dimension of quality, inherently related to the interpretation of argumentation as a cooperation challenge, is currently lacking in current approaches to AQ.", "filtered_refids": [["b6", "b47", "b9", "b27", null, "b23", "b36"], [], ["b56", "b55", null, "b33", "b61"], [], [], ["b45", "b46", "b50", "b55"], [], ["b45"], [], ["b45", "b46", "b55"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 41, "num_chars": 8280, "num_references": 20}
{"corpusid_sectionid": "236460206-s5", "title": "Towards Argument Mining for Social Good: A Survey", "date": 2021, "section_title": "Grounding AQ in deliberation: moderation as a real-world application", "section": "Grounding AQ in a discourse perspective which quantifies \"team-playing\" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies. We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Tr\u00e9nel, 2009). To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom. This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix). Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers). The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks? 5 The example involves two users who clearly differ in their argumentation style and position. User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text. User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair. This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones. A moderator then joins the discussion to (a) provide a clarification as to why the focus is on texting and a link to further information on the matter, and (b) ask User 2 to elaborate on the personal communication issue, and to propose alternatives. In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a \"discourse traffic director\", pointing out that the user should read and contribute to different threads in the discussion.\n\nThe guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (eRulemaking Initiative et al., 2017) which reflect the moderator actions mentioned in the examples. In the protocol the moderator roles were divided into two main classes. Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties. Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse. As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences).\n\nRegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants. However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling. Park et al. (2012) used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol. Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment. However this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as \"in need for moderation\".\n\nBesides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy. 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.\n\nDigital innovation supporting deliberation Argument maps (Walton, 2005) are widely employed to support online discussions, as an emerging optimization of the deliberation. Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post. Their contribution to the argument maps is often reviewed by a moderator. So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (the user) and above (the moderator).\n\nThanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (or many) of the available options, without having to read a large number of long posts. An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (Klein, 2011). Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (Yang et al., 2021;Ito, 2018). Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (2015) tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament. This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps. The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (fewer new ideas as compared to the traditional forum), it also reduced the rate of claims without further discussion.\n\nYet, the need for trained moderators tends to be a significant bottleneck (both in terms of time and of costs) in digital deliberation. Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation. For example, the experiment in Spada and Vreeland (2013) tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process.", "filtered_refids": [["b12", "b51"], [], ["b34"], [null], ["b57"], ["b41", "b8", "b13", "b60"], ["b42"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 36, "num_chars": 7502, "num_references": 10}
{"corpusid_sectionid": "236460206-s6", "title": "Towards Argument Mining for Social Good: A Survey", "date": 2021, "section_title": "NLP-Supported Moderation: desiderata and challenges", "section": "NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is \"flaming\", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014). Here, moderators could benefit from hate-speech and trolling detection methods in NLP. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017). Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).\n\nBeyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019). In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion. Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Aky\u00fcrek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.\n\nFurther NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).\n\nHow to represent discourse? Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input. One question remains open: How to represent the argumentative discourse within a contribution (e.g. a forum post) and across contributions (e.g. an entire online deliberation campaign)? We can approach also this question from an interdisciplinary perspective. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017). A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim. Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013). Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Pad\u00f3 et al., 2019;Haunss et al., 2020). Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).\n\nModeration can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying \"hot\" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.\n\nWho moderates the (NLP) moderators? The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Splieth\u00f6ver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups. This is a real risk when quality is equated to \"success\", in turn quantified in terms of likes, replies, retweets. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the \"cooperation challenge\" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited.", "filtered_refids": [["b24", null, "b19", "b17"], ["b11", null, "b2", "b37"], ["b15", "b39", "b58", "b59", null, "b53"], ["b4", "b31", "b32", "b21", null, "b22", "b1", "b3"], [], ["b43", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 5644, "num_references": 24}
{"corpusid_sectionid": "236460241-s1", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies", "date": "2023-01-05", "section_title": "Competing models of C-S", "section": "For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.\n\n2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).\n\n3. I love Horlicks maar hier\u015b niks 'I love Horlicks but there's nothing there '\n\nAlthough it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).\n\nFor many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.\n\n3 Why do speakers code-switch?\n\nIn addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Mysl\u00edn and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojal\u00e1 or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).\n\nAccording to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.", "filtered_refids": [[], ["b43", "b71", null, "b16", "b3"], [], [null, "b37", "b32"], ["b57", "b31", "b32", null, "b53", "b33"], [], ["b57", "b44", "b9", "b82", "b42"], ["b52", "b10", "b79", "b30", null, "b7", "b65", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 37, "num_chars": 5847, "num_references": 27}
{"corpusid_sectionid": "236460241-s2", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies", "date": "2023-01-05", "section_title": "Code-switching, Borrowing, Transfer, Loan Translation", "section": "While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogru\u00f6z and Backus, 2009). 4.\u0130lkokul-u\u0130stanbul-da yap-t\u0131-m.\n\nprimary.school-ACC\u0130stanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'\n\nIn transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher apr\u00e8s 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.", "filtered_refids": [["b58", null, "b64", "b2", "b37"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2206, "num_references": 5}
{"corpusid_sectionid": "236460241-s3", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies", "date": "2023-01-05", "section_title": "C-S across Languages: European Context", "section": "The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).\n\nWithin an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, J\u00f8rgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.\n\nC-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.\n\nC-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, O\u017ca\u0144ska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and \u00c7 etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogru\u00f6z (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.\n\nIn addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.\n\nWithin the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.\n\nSimilar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.", "filtered_refids": [["b11", "b54", "b39", "b56", "b40", "b29", null, "b17"], [], ["b69", "b75", "b28"], ["b51", "b12", "b52", "b46", "b80", null, "b23", "b24", "b1", "b38"], [], ["b83", "b35"], ["b81"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 33, "num_chars": 5583, "num_references": 24}
{"corpusid_sectionid": "236460241-s4", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies", "date": "2023-01-05", "section_title": "C-S across Languages: Indian Context", "section": "According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.\n\nIn addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.\n\nInstead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).\n\nC-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche \"renovation do\") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.\n\nFrom an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.\n\nIn their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.", "filtered_refids": [[null, "b19"], ["b73", "b27"], [null], ["b67", "b84"], ["b41", "b50", null, "b72", "b36"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 49, "num_chars": 8160, "num_references": 12}
{"corpusid_sectionid": "236460241-s6", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies", "date": "2023-01-05", "section_title": "Data", "section": "The use of Deep Neural Networks, which require large amounts of labeled and unlabeled training data have become the de facto standard for building speech and NLP systems. Since C-S languages tend to be low resourced, building Deep Learningbased models is challenging due to the lack of large C-S datasets. Massive multilingual Language Models (LMs) such as multilingual BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have shown promise in enabling the coverage of lowresource languages without any labeled data by using the zero-shot framework. These LMs are typically trained in two phases: a \"pre-training\" phase, in which unlabeled data from one or multiple languages may be used and a \"fine-tuning\" phase, in which task-specific labeled data is used to build a system capable of solving the task.\n\nSince multilingual LMs are trained on multiple languages at the same time, it has been suggested that these models may be capable of processing C-S text (Johnson et al., 2017), with promising results initially reported on POS tagging (Pires et al., 2019). Khanuja et al. (2020) found that multilingual BERT outperforms older task-specific models on C-S tasks, however, the performance on C-S is much worse than the performance on the same tasks in a monolingual setting. Further, these LMs are either trained primarily on monolingual datasets such as Wikipedia in the case of mBERT, or Com-mon Crawl 1 in the case of XLM-R. So, they are either not exposed to C-S data at all during training, or they miss out on several language pairs, types and functions of C-S that are encountered in daily life but not available on the web.\n\nSince massive multilingual LMs are now replacing traditional models across many NLP applications, it is crucial to consider how they can be trained on C-S data, or made to work for C-S by incorporating other sources of knowledge.", "filtered_refids": [[], ["b14", "b25", "b55"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1873, "num_references": 3}
{"corpusid_sectionid": "236460241-s8", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies", "date": "2023-01-05", "section_title": "User-facing applications", "section": "Although speech and NLP models for C-S have been built for various applications, a major limitation of the work done so far in computational processing of C-S is the lack of end-to-end userfacing applications that interact directly with users in multilingual communities. For example, there is no widely-used spoken dialogue system that can understand as well as produce code-switched speech, although some voice assistants may recognize and produce C-S in limited scenarios in some locales. Although computational implementations of grammatical models of C-S exist (Bhat et al., 2016), they do not necessarily generate natural C-S utterances that a bilingual speaker would produce (Pratapa et al., 2018). Most crucially, current computational approaches to C-S language technologies do not usually take into account the linguistic and social factors that influence why and when speakers/users choose to code-switch.\n\nBawa et al. (2020) conducted a Wizard-of-Oz study using a Hindi-English chatbot and found that not only did bilingual users prefer chatbots that could code-switch, they also showed a preference towards bots that mimicked their own C-S patterns. Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment. In a follow-up study, Agarwal et al. (2017) find that Hindi is the preferred language for swearing in Hindi-English C-S tweets, and swearing may be a motivating factor for users to switch to Hindi. The study also finds a gender difference, with women preferring to swear in English more often than Hindi. Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch.\n\nDue to the paucity of user-facing systems and standard benchmarks covering only a handful of simpler NLP tasks, it is likely that we overestimate how well computational models are able to handle C-S. In sum, language technologies for C-S seem to be constrained by the lack of availability of diverse C-S training data, evaluation benchmarks and the absence of user-facing applications. They need to go beyond pattern recognition and grammatical constraints of C-S in order to process and produce C-S the way humans do. Hence, it is important for the CL community to be aware of the vast litera-ture around C-S in linguistics, particularly as we proceed to solving more challenging tasks.", "filtered_refids": [["b59", null], ["b63", null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2511, "num_references": 4}
{"corpusid_sectionid": "11250379-s1", "title": "A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin", "date": "1996-08-01", "section_title": "Continuous Speech Recognition of Mandarin", "section": "There have been many attempts to find so called distinctive features of speech (e.g. [Fant 1973]) which are invariant to a number of factors. Certain distinctive (phonetic) features, such as nasality and voicing, can be used to represent the place and manner of articulation of speech sounds so that speech can be uniquely identified by detecting the acoustic-phonetic properties of the signal. By organizing such knowledge in a systematic manner, speech recognition can (in theory) be performed by first identifying and labeling the sequence of feature vectors and then identifying the corresponding sounds in the speech signal, followed by decoding the corresponding sequence of words using lexical access to a dictionary of words. This has been demonstrated in spectrogram reading by a human expert who can visually segment and identify some speech sounds based on knowledge of acoustic-phonetics of English. Although the collection of distinctive features, in theory, offers a set of invariant features for speech recognition, it is not generally used in most speech recognitions systems. This is due to the fact that the set of distinctive features are usually difficult to identify in spontaneous continuous speech and the recognition results are generally unreliable.\n\nA more successful approach to automatic speech recognition is to treat the speech signal as a stochastic pattern and to adopt a statistical pattern recognition approach. For this approach we assume a source-channel speech generation model (e.g. [Bahl et al. 1983]) shown in Figure  1, in which the source produces a sequence of words, W. Because of uncertainty and inaccuracy in converting from words to speech, we model the conversion from W to an observed speech waveform, S, as a noisy channel. Speech recognition is then formulated as a maximum a posteriori (MAP) decoding problem, as shown in Figure 1. Instead of working with the speech signal S directly, one way to simplify the problem is to assume that S is first parametrically represented as a sequence of acoustic vectors A. We then use the Bayes rule to reformulate the decoding problemas follows, arg max ( | ) arg max ( | ) ( )\n\nwhere \u0393 is the set of all possible sequences of words, P(A|W)is the conditional probability of the acoustic vector sequence, A, given a particular sequence of words W, and P (W) is the a priori probability of generating the sequence of words W. The first term, P (A|W), is often referred to as an acoustic model, and the second term, P (W), is known as a language model. The noisy channel in Figure 1 is a model jointly characterizing the speech production system, the speaker variability, the speaking environment, and the transmission medium. Since it is not feasible to have a complete knowledge about such a noisy channel, the statistical approach often assumes particular parametric forms for P\u03b8 (A|W) and P\u03c9 (W), i.e. according to specific models. All the parameters of the statistical models (i.e. \u03b8 and \u03c9 ) needed in evaluating the acoustic probability, P\u03b8 (W|A), and the language probability, P\u03c9(W), are usually estimated from a large collection (the so-called training set) of speech and text training data. This process is often referred to as model training or learning. We will discuss this important issue later in the paper.\n\nThere is some recent attempt trying to separate the speech production part from the source-channel model by incorporating knowledge about the human speech production mechanism. Knowledge about the transducers used for capturing speech and the channel used for transmitting speech can also be explicitly modeled. However, the effectiveness of such approaches is yet to be shown.", "filtered_refids": [[null], ["b2"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3688, "num_references": 3}
{"corpusid_sectionid": "11250379-s6", "title": "A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin", "date": "1996-08-01", "section_title": "Speech Analysis and Feature Extraction", "section": "The purpose of the feature analysis module is to parametrize the speech into a parsimonious sequence of feature vectors that contain the relevant (for recognition) information about the sounds within the utterance. Although there is no consensus as to what constitutes the optimal feature analysis, most systems extract spectral features with the following properties: having good discrimination to readily distinguish between similar speech sounds, being easy to model statistically without the need for an excessive amount of training data, and having statistical properties which are somewhat invariant across speakers and over a wide range of speaking environments. To our knowledge there is no single feature set that possesses all the above properties. The features used in speech recognition systems are largely derived from their utility in speech analysis, speech coding, and psycho-acoustics.\n\nFourier analysis is still the most widely used method for extracting spectral features for speech recognition. Implementations of feature extraction include:\n\n\u2022 Language Model features along with its first and second time derivatives (e.g. [Furui 1986]). \u2022 Frequency-Warped Spectral Features: Sometimes non-uniform frequency scales are used in spectral analysis to provide the so-called mel-frequency or bark-scale spectral feature sets (e.g. [Davis andMermelstein 1980;Junqua et al. 1993]). The motivation is to mimic the human auditory system which processes the spectral information on a non-uniform frequency scale.\n\nAlthough many research directions are being pursued to create new feature sets for ASR, most promising ones include: (1) extraction of segment features to allow variable-length speech analysis to incorporate discriminative information from neighboring frames; (2) extraction of articulatory and auditory features to make use of the human speech production and perception mechanism (e.g. [Deng and Sun 1994;Ghitza 1988;Seneff 1988]); (3) extraction of discriminative features which are functions of the data, the task, and the classifiers being used (e.g. [Ney et al. 1992;Biem et al. 1993;Rahim and Lee 1996]).", "filtered_refids": [[], [], ["b24", "b42", null], ["b63", "b10", "b29", "b19", "b84", "b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 2135, "num_references": 9}
{"corpusid_sectionid": "11250379-s8", "title": "A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin", "date": "1996-08-01", "section_title": "Continuous Speech Recognition of Mandarin", "section": "\u2022 Units Other than Phones: Units smaller than a phone, such as phone-labeled acoustic states, have been used to reduce the number of states needed to represent the set of speech units. Larger units, including diphones, demisyllables, syllables, whole-words and even phrases, have all been used to better characterize coarticulation between adjacent sounds. Acoustic segment units have also been investigated [Lee 1988].\n\n\u2022 Units with Linguistic Context Dependency: Different ways of incorporting linguistic context in a speech subword unit, such as double context dependent phones (often known as triphones) and generalized triphones, have been proposed (e.g. [Lee 1989]). It has been shown that the recognition accuracy of a task can be increased when linguistic context dependency is properly incorporated to reduce the acoustic variability of the speech units being modeled. In fluent continuous speech it has also been shown that incorporation of interword units takes into account cross-word coarticulation and therefore provides more accurate modeling of speech units than simply using intraword context-dependent units. Word-dependent units have also been used to model poorly articulated speech sounds such as function words like a, the, in, and, etc. (e.g. [Lee 1989]).\n\nFor a given task, high recognition accuracy can be achieved only when the subword unit set contains context-dependent phones which maximally covers the vocabulary and the task language and when these phone units are adequately modeled using a large training set [Hon 1992]. However, the collection of a large amount of task-specific training data for every individual application is not practical. Task and vocabulary independent acoustic training and task-specific vocabulary learning (e.g. [Hon 1992;Lee et al.1996]) are therefore important research topics. Task-independent modeling has also been applied to word spotting for training of acoustic models and rejection models [Rose and Hofstetter 1993;Sukkar and Lee 1996]. However, we do not yet know how to design a task-independent training database suitable for a wide range of vocabularies and applications.", "filtered_refids": [["b45"], ["b51"], ["b77", null, "b33", "b87"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2144, "num_references": 6}
{"corpusid_sectionid": "11250379-s9", "title": "A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin", "date": "1996-08-01", "section_title": "Acoustic Modeling of Speech Units", "section": "Training of subword unit models consists of estimating the model parameters from a training set of continuous speech utterances in which all of the relevant subword units are known to occur 'sufficiently' often. The way in which training is performed greatly affects the overall recognition system performance. A key issue in training is the size of the training set. Since infinite size training sets are impossible to obtain (and computationally unmanageable), we must use a finite size training set. This immediately implies that some subword units may not occur as often as others. Hence there is a tradeoff between using fewer subword units (where we get good coverage of individual units, but poor resolution of linguistic context), and more subword units (where we get poor coverage of the infrequently occurring units, but good resolution of linguistic context).\n\nAn alternative to using a large training set is to start with some initial set of subword unit models and adapt the models over time (with new training material, possibly derived from actual test utterances) to the task, the speaker and/or the environment. Such methods of adaptive training are usable for new speakers, tasks and environments, and provide an effective way of creating a good set of application-specific models from a more general set of models (which are speaker, environment, task, and context independent).\n\nSpeech patterns not only exhibit highly variable spectral properties but also show considerable temporal variation. There are not many modeling approaches that are both mathematically well-defined and computationally tractable, for modeling the speech signal. The most widely used and the most successful modeling approach to speech recognition is the use of hidden Markov models (HMMs). The reader is referred to a tutorial by Rabiner [1989] for an introduction to the HMM approach and its applications. Artificial neural network (ANN) approaches have also been used to provide an alternative modeling framework and a new computing paradigm [Bourlard and Wellekens 1992;Bourlard and Morgan 1994;Robinson 1994]. Almost all modern speech recognition systems use hidden Markov models and their extensions to model speech units. We will give a more detailed description of the HMM framework in the next section.", "filtered_refids": [[], [], ["b69", "b12", "b75", "b11"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2307, "num_references": 4}
{"corpusid_sectionid": "11250379-s12", "title": "A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin", "date": "1996-08-01", "section_title": "Language Modeling and Sentence Match", "section": "The sentence-level match module uses the constraints imposed by a grammar (or syntax) to determine the optimal sentence in the language. The grammar, consisting of a set of syntactic and semantic rules, is usually specified based on a set of task requirements. Although there have been proposed a number of different forms for the grammar (e.g. context-free grammar, N-gram word probabilities, word pair, etc.), the commonly used ones can all be represented as finite state networks (FSNs). In this manner it is relatively straightforward to integrate the grammar directly with the word-level match module.\n\nThe language models used in smaller, fixed-vocabulary tasks are usually specified manually in terms of deterministic finite state representations. For large vocabulary recognition tasks, stochastic N-grams such as bigram and trigram models have been extensively used (e.g. [Jelinek 1985]). Due to the sparse training data problem, smoothing of the N-gram probabilities is generally required for cases with N \u22652. Class-dependent bigrams and trigrams have also been proposed. To account for longer language constraints, tree language models have been proposed [Bahl et al. 1989]. The use of a context-free language in recognition [Ney 1991] is still limited mainly due to the increase in computation required to implement such grammars.\n\nAdvances in language modeling are needed to improve the efficiency and effectiveness of large vocabulary speech recognition tasks. Some of the advances will come from better stochastic language modeling. However the language models, obtained from a large body of domain-specific training data, often cannot be applied directly to a different task. Adaptive language modeling, which combines information in an existing language model and a small amount of application-specific text data, is an attractive approach to circumvent such difficulties.", "filtered_refids": [[], ["b38", "b62", "b4"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1889, "num_references": 3}
{"corpusid_sectionid": "11250379-s15", "title": "A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin", "date": "1996-08-01", "section_title": "Hidden Markov Modeling of Speech", "section": "The hidden Markov model is a statistical model that uses a finite number of states and the associated state transitions to jointly model the temporal and spectral variations of signals. It has been used extensively to model fundamental speech units in speech recognition because the HMM can adequately characterize both the temporal and spectral varying nature of the speech signal [Rabiner 1989;Rabiner and Juang 1993].\n\nAlthough many variants exist, perhaps the simplest subword model is a left-to-right HMM with only self and forward transitions. Within each state of the model there is an observation density function which specifies the probability of a spectral vector. This observation density can either be a discrete density (implying the use of one or more codebooks to discretize the input spectral vector, e.g. [Lee 1989]), or a continuous mixture Continuous Speech Recognition of Mandarin density (e.g. ]), or a so-called semi-continuous density (e.g. [Huang and Jack 1989]) or a tied-mixture density (e.g. [Bellegarda and Nahamoo 1990]) which is a set of common continuous densities whose weights are chosen according to the model state. Tying can also be done at the HMM state level or at the state distribution level (e.g. [Hwang and Huang 1993;Young, Odell and Woodland 1994]). Stochastic segment modeling (e.g. [Lee, Soong and Juang 1988;Ostendorf and Roukos 1989;Deng 1993]), dynamic system modeling (e.g. [Digalakis, Rohlicek and Ostendorf 1993]), and stochastic trajectory modeling [Gong and Haton 1994] and successive state splitting [Takami and Sagayama 1992] have also been proposed to extend the HMM to handle intra-state, inter-state, and inter-sample correlations in a more precise manner. Some of the most often used acoustic modeling approaches include:\n\n\u2022 Maximum Likelihood (ML) Estimation of HMM: Estimation of HMM parameters is usually accomplished in a batch mode using the ML approach based on the EM (estimation-maximization) algorithm (e.g. [Baum et al. 1970;Liporace 1982;Juang 1985]). Segmental ML approaches have also been extensively used (e.g. [Rabiner, Wilpon and Juang 1986]). Although ML estimation has good asymptotic properties, it often requires a large size training set to achieve reliable parameter estimation.\n\nSmoothing techniques, such as deleted interpolation [Jelinek and Mercer 1980] and Bayesian smoothing [Gauvain and Lee 1992], have been proposed to circumvent some of the problems associated with sparse training data. \u2022 Maximum Mutual Information (MMI) Estimation of HMM: Instead of maximizing the likelihood of observing both the given acoustic data and the transcription, the MMI estimation procedure maximizes the mutual information between the given acoustic data and the corresponding transcription [Bahl et al. 1986;Normandin and Morgera 1991]. As opposed to ML estimation, which uses only class-specific data to train the classifier for the particular class, MMI estimation takes into account information from data in other classes due to the necessary inclusion of all class priors and conditional probabilities in the definition of mutual information.\n\n\u2022 Maximum A Posteriori (MAP) Estimation of HMM: Perhaps the ultimate way to train subword units is to adapt them to the task, to the speaking environment, and to the speaker. One way to accomplish adaptive training is through Bayesian learning in which an initial set of seed models (e.g. speaker-independent or SI models) are combined with the adaptation data to adjust the model parameters so that the resulting set of subword models matches the acoustic properties of the adaptation data. This can be accomplished by maximum a posteriori estimation of HMM parameters [Lee, Lin and Junag 1991;Gauvain and Lee 1992;Gauvain and Lee 1994] and has been successfully applied to HMM-based speaker and context adaptation of whole-word and subword models. On-line adaptation, which continuously adapts HMM parameters and hyperparameters, has also been developed (e.g. [Huo and Lee 1996]).\n\n\u2022 Minimum Classification Error (MCE) Estimation of HMM and ANN: One new direction for speech recognition research is to design a recognizer that minimizes the error rate on task-specific training data. The problem here is that the error probability is not easily expressed in a close functional form because the true probability density function of the speech signal is not known. An alternative is to find a set of model parameters that minimizes the recognition error based on a given set of application-specific, training or cross-validation data [Juang and Katagiri 1992]. Each training utterance is first recognized and then used for both positive and negative learning by adjusting the model parameters of all competing classes in a systematic manner. For HMM-based recognizers, a family of generalized probabilistic descent (GPD) algorithms has been successfully applied to estimate model parameters based on the minimum classification error criterion (e.g. [Katagiri, Lee and Juang 1991;Chou, Juang and Lee 1992;Juang and Katagiri 1992;Su and Lee 1994]). The MCE/GPD approaches are also capable of maximizing the separation between models of speech units so that both discrimination and robustness of a recognizer can be simultaneously improved.", "filtered_refids": [["b69", "b70"], ["b94", "b34", "b18", "b20", "b45", "b30", "b88", "b51", "b8", "b65", "b36"], ["b58", "b68", "b39", "b7"], ["b64", "b3", "b37", "b27"], [null, "b28", "b35", "b27"], ["b43", "b14", "b86", "b41"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 5261, "num_references": 29}
{"corpusid_sectionid": "11250379-s19", "title": "A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin", "date": "1996-08-01", "section_title": "Continuous Speech Recognition of Mandarin", "section": "In the following we illustrate, by example, how the pattern recognition approach we discussed above can be used to put together an integrated continuous speech recognition system for Mandarin. The algorithm described here was first developed for recognition of American English . The same strategy was also applied to recognition of Spanish [Alvarez-Cercardillo et al. 1995] and Mandarin Chinese as shown here. Only the language-dependent part of the algorithm was modified to deal with the lexical and language constraints of different languages. Although the selection of an appropriate set of fundamental speech units for recognition is a crucial part, we assume only little knowledge about the phonology of a particular language in our studies. It is important to note that the examples shown here are meant to explore new research dimensions in Mandarin recognition. It is not our intention to get the best recognition results for any of the tasks, discussed here. Therefore no system parameter tuning was attempted in all of our experiments. Although the tonal property is a unique feature of Mandarin (as well as in some other Oriental languages), we do not address the issue in this paper. We only consider Mandarin base syllables, i.e. we do not distinguish syllables with only tone differences. The readers are referred to a number of studies for tone properties of Mandarin (e.g. [Tseng 1981]), tone recognition (e.g. [Wang and Lee 1994;Wang and Chen 1994]), and combined tone and syllable recognition (e.g. [Lin et al. 1993]).\n\nFor some historical and practical reasons, most of the existing Mandarin recognition systems developed in Taiwan and Mainland China are syllable based. Although modeling of sub-syllabic units has been attempted recently, most systems still perform syllable recognition in terms of a syllable lattice followed by lexical and language processing to determine the recognized sentence (e.g. Chiang, Lin and Su 1996]). Because real-time system implementation has always been a major design factor in developing Mandarin recognition systems, this decoupled approach was largely adopted to avoid the search complexity implied in integrated approaches. Although some success has been reported for speaker-trained recognition, large-scale, speaker-independent Mandarin recognition has not been widely studied. Recall in our discussion above that due to error propagation from one stage to the next, decoupled approaches often degrade in performance more drastically than that in integrated approaches when inadequate intermediate search theories are saved for further processing. This is of major concern especially when testing in adverse conditions.", "filtered_refids": [["b91", "b1", "b92", "b56"], ["b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2682, "num_references": 5}
{"corpusid_sectionid": "264833196-s11", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Transformer encoders", "section": "A transformer encoder is a length-preserving function T : \u03a3 * \u2192 (R  ) * parameterized by the weights of an input layer  and  transformer layers L 1 , . . ., L  .A transformer encoder with postnorm layers is:\n\nwhere each L  is a post-norm layer (1) and \u2022 denotes function composition.A transformer encoder with pre-norm layers is additionally parameterized by the weights of a layernorm N and is defined as:\n\nwhere each L  is a pre-norm layer (2).The encoder's output is a sequence of vectors in (R  ) * .To use it as a language recognizer, we add an output layer that converts T () to a probability\n\nwhere w \u2208 R  ,  \u2208 R, and  is a distinguished position.The encoder accepts if p \u2265 1 2 and rejects if p < 1 2 .Chiang and Cholak (2022) also consider a requirement that an encoder accepts/rejects strings with bounded cross-entropy.That is, we say that an encoder recognizes a language  with crossentropy at most  iff for all strings  \u2208 , we have \u2212 log p \u2264 , and for all strings  \u2209 , we have \u2212 log(1 \u2212 p) \u2264 .\n\nWe are aware of two choices for the distinguished position .Most papers use the last position ( =  \u2212 1) but some (Chiang and Cholak, 2022;Chiang et al., 2023), inspired by binary classifiers based on BERT (Devlin et al., 2019), prepend a special symbol CLS at position 0 and use  = 0.", "filtered_refids": [[], [], [], [], ["b12", "b13", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1291, "num_references": 3}
{"corpusid_sectionid": "264833196-s12", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Transformer decoders", "section": "A transformer decoder generates rather than recognizes strings.The input is the prefix of previouslygenerated symbols,\n\nwhere W \u2208 R |\u03a3| \u00d7 and b \u2208 R |\u03a3| .We assume  0 = BOS and every string ends with EOS, where BOS and EOS are special symbols that do not occur anywhere else.To sample a string, we first sample  1 from p( 1 | BOS), then, for each time step  > 1, sample   from p(  |  < ).The process stops when   = EOS.Because each sampled output symbol becomes part of the input at the next time step, this kind of model is called autoregressive.\n\nTwo different ways have been proposed for defining whether a transformer decoder generates a (weighted) language.\n\nFirst, Hahn (2020) considers a weighted language as a distribution over strings ().For any length , the KL divergence (relative entropy) of the model p() from the true distribution (), for predicting   conditioned on all previous words, is\n\n.\n\nAs Hahn's results are negative, he does not spell out a positive criterion, but he seems to implicitly require that this divergence vanish at infinity:\n\nSecond, let us say that a transformer decoder -generates  iff\n\nThen Yao et al. (2021), following Hewitt et al. (2020), say that a transformer decoder  generates a language  iff there exists an  > 0 such that  -generates .(This means that a transformer decoder may generate more than one language, depending on the  chosen.)They also show that any -generator can be converted into a recognizer.\n\nWhile not specifically focusing on transformers, Lin et al. (2021) demonstrate limitations of autoregressive models for generation; for example, they show that there is a language  \u2208 P that cannot be -generated in polynomial time for any  > 0 if P \u2260 NP.", "filtered_refids": [[], [], [], [], [], [], [], ["b74", "b26"], ["b34"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1708, "num_references": 3}
{"corpusid_sectionid": "264833196-s13", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Transformer encoder-decoders", "section": "A transformer encoder-decoder combines a transformer encoder and decoder, adding to each layer of the decoder an additional attention sublayer, known as cross attention, which attends to the output of the encoder.\n\nIn the literature surveyed here, only the construction of P\u00e9rez et al. (2021) and related constructions (Bhattamishra et al., 2020b;Wei et al., 2022) employ an encoder-decoder architecture.In these constructions, a string  is fed to the encoder, and the decoder is allowed to run for an arbitrary number of steps.Then  is accepted iff the decoder eventually outputs a vector belonging to a fixed set of accept vectors.\n\nAs we will see ( \u00a77.2.1), this setup vastly increases the model's power.It could be likened to a language model that is allowed to \"think step by step\" (Kojima et al., 2022) before generating a final accept decision.", "filtered_refids": [[], ["b54", "b9", "b71"], ["b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 851, "num_references": 4}
{"corpusid_sectionid": "264833196-s15", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Number representations", "section": "Transformers, like most neural networks, operate, in principle, on real numbers.While hard attention transformers could be defined using only rational numbers, even rational numbers can represent an arbitrary amount of information.In the area of expressivity of RNNs, the use of real or rational numbers has led to results that make RNNs appear more powerful in theory than in practice (Siegelmann andSontag, 1995, 1994;Weiss et al., 2018).Consequently, some studies limit numeric representations to have  (1) bits, as floating-point numbers do in practice (Chiang et al., 2023).\n\nBut the need to handle arbitrary lengths ( \u00a75.2) also makes it reasonable for precision to depend on .Merrill and Sabharwal (2023a) argue that in  (1) precision, an attention head cannot attend uniformly to a string of length , because the attention weights () would all round down to zero.So  (log ) bits of precision is a common choice (Yao et al., 2021;Merrill and Sabharwal, 2023b,a).Other choices are possible as well: Merrill and Sabharwal (2023b) use the set F of rational numbers whose denominator is unbounded but constrained to be a power of two.\n\nRestricting the intermediate activations to limited precision introduces numerous decisions about when and how rounding should take place, which can potentially affect the expressivity of the model.For example, when summing  numbers, one could round after each addition or only at the end of the summation.Better formalizing these decisions and their impact on expressivity is an area for future research.", "filtered_refids": [["b72", null, "b13"], ["b74", null, "b44"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1544, "num_references": 6}
{"corpusid_sectionid": "264833196-s18", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Automata and classes L, NL, P", "section": "We assume familiarity with finite automata and Turing machines; for definitions, please see the textbook by Sipser (2013).Background on counter machines is given by Fischer et al. (1968).The standard deterministic model of a multi-tape Turing machine is used to define the language classes L (languages recognizable in space  (log )) and P (languages recognizable in polynomial time).The definition of NL (languages recognizable in non-deterministic space  (log )) uses a nondeterministic Turing machine.We also consider a random-access model of a deterministic Turing machine, which allows meaningful computation in time  (log ) (Mix Barrington et al., 1990).Problem reductions computable with bounded resources (e.g., in L) are used to define problems complete or hard for various classes of languages.\n\nIt is known that\n\nbut none of these inclusions are known to be strict.\n\n6.2 Circuits and classes AC 0 , ACC 0 , TC 0 , NC 1\n\nCircuits are a model of parallel computation particularly relevant to transformers.For more details, please see the textbook by Arora and Barak (2009).Circuits operate on binary values.If we choose a fixed-length encoding of the symbols of \u03a3 as strings of  = \u2308log 2 |\u03a3|\u2309 bits, then a circuit can simulate input alphabet \u03a3 by encoding the value of the -th input symbol into positions  to + (\u22121).For the rest of this section, we assume \u03a3 = {0, 1}.\n\nCircuits A circuit  with input length  is a directed acyclic graph with  input vertices  1 , . . .,   and zero or more gate vertices, each labeled by NOT, AND, or OR.Input vertices have fan-in (in-degree) zero, NOT gates have fan-in one, and the fan-in of AND and OR gates can be either two or unbounded.One (input or gate) vertex  is designated the output of the circuit.\n\nGiven an input string  \u2208 {0, 1}  , each input vertex   is assigned the value   , and each gate vertex is assigned the value computed by applying its corresponding logical function to the values assigned to its in-neighbors.The circuit computes the boolean function  : {0, 1}  \u2192 {0, 1}, mapping each input string to the value assigned to .The depth of , denoted  (), is the length of the longest directed path from any   to .The size of , denoted | |, is the number of vertices in .Uniformity As defined, a circuit family contains a different circuit for each length , with no constraint on the relationship between the circuits.For example, let  be any unary language:  \u2286 {1} * .For  \u2208 N, if 1  \u2209 , define   to be a circuit for the constant 0 function (an OR gate with fan-in 0), and if 1  \u2208 , define   to be a circuit for the AND of all the inputs.Thus, every unary language, even an undecidable one, is recognized by a circuit family of size  () and depth  (1).", "filtered_refids": [[null, "b61", "b50"], [], [], [], ["b3"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2715, "num_references": 4}
{"corpusid_sectionid": "264833196-s20", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "First-order logic", "section": "A formal language can also be defined as a set of finite strings that satisfy a closed formula of a logic.For more details, refer to Thomas (1997) or Straubing (1994).\n\nFor example, in the first-order logic of strings, or FO, the formulas are the smallest set containing: \u2022 Variables , , and so on.\n\n\u2022 Atomic formulas   (),  = ,  < , where  \u2208 \u03a3 is a symbol and ,  are variables.\n\n 2 are formulas.\u2022 \u2200., \u2203.,where  is a variable and  is a formula.Under the intended interpretation, variables stand for positions of a finite string , and   () is true iff   = .For example, \u2200.\u2200. () \u2227   () \u2192  <  defines the regular language  *  * .The language defined by a closed formula  consists of those strings that satisfy .\n\nThe languages definable in FO are exactly the star-free languages (McNaughton and Papert, 1971).\n\nWe are also interested in variants that add other quantifiers:\n\n\u2022 FOC adds counting quantifiers \u2203 = ., which hold iff there are exactly  values of  that make  true (Mix Barrington et al., 1990).\u2022 FOM adds majority quantifiers M., which hold iff at least half of the values of  make  true (Mix Barrington et al., 1990).We are also interested in various sets of predicates: \u2022 Modular predicates MOD   (), which hold iff  \u2261  (mod ) (Mix Barrington et al., 1992).\n\n\u2022 BIT(, ), which holds iff the -th bit of  is 1.\n\n\u2022 ARB, the set of all possible predicates on one or more positions.A logic extended with predicates is conventionally written with the predicates in square brackets; for example, we write FO[BIT] for first-order logic with the BIT predicate.", "filtered_refids": [["b63", "b67"], [], [], [], ["b38"], [], ["b49", "b50"], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1560, "num_references": 6}
{"corpusid_sectionid": "264833196-s24", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Beyond TC 0", "section": "As we will see ( \u00a77.3.2),some transformer variants lie within uniform TC 0 .What problems are beyond uniform TC 0 ?\n\nPermutations and the language W( 5 ) A permutation of [] is a bijection  : [] \u2192 [], and   is the set of all permutations of [\ud835\udc58].We can write one as a list, for example, [0, 3, 1, 2] is the permutation that maps 0 to 0, 1 to 3, and so on.Composition of permutations is associative but not commutative.For example (applying the permutations from left to right):\n\nBy treating   as an alphabet and compositions of permutations as strings, we can define the language W(  ) of compositions of permutations of [] that equal the identity permutation.For example, W(\n\n. This is straightforward for finite automata to recognize-each state represents the current composed permutation-but difficult when given only fixed computation depth.In particular, the language W( 5 ) is known to be complete for NC 1 under DLOGTIME-uniform AC 0 reductions (Barrington, 1989).Thus it is not in DLOGTIMEuniform TC 0 assuming that TC 0 \u228a NC 1 (as is widely believed).This makes it a valuable example of a regular language that transformer encoders probably cannot recognize.\n\nThe languages W(  ) have some relevance to natural language: Paperno (2022) study expressions like the child of the enemy of Ann where the interpretation of the child of is (roughly) a permutation of possible referents.Additionally, W(  ) resembles permutation problems that have been used to benchmark transformers' state-tracking abilities (Kim and Schuster, 2023).\n\nOther languages Besides W( 5 ), other problems that are (widely believed to be) not in uniform TC 0 include: \u2022 Undirected graph connectivity, which is Lcomplete (Reingold, 2008), so is not in Luniform TC 0 unless L-uniform TC 0 = L.\n\n\u2022 Solving linear equalities, which is P-complete (Greenlaw et al., 1991), so is outside L-uniform TC 0 unless L-uniform TC 0 = P. \u2022 Matrix permanent, which is known to be outside of DLOGTIME-uniform TC 0 (Allender, 1999).", "filtered_refids": [[], [null], [], ["b6"], ["b31"], ["b58"], ["b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1992, "num_references": 5}
{"corpusid_sectionid": "264833196-s25", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Circuits and logics", "section": "Figure 1 indicates that DLOGTIME-uniform AC 0 and TC 0 are equivalent to FO[BIT] and FOM[BIT], respectively.There are many such equivalences between circuit classes and logics.As a general rule of thumb, adding unbounded fan-in gates to a circuit family correlates with adding quantifiers to the corresponding logic, and increasing the degree of non-uniformity of a circuit family correlates with adding numerical predicates to the corresponding logic (Mix Barrington and Immerman, 1994).For example, making AC 0 and TC 0 completely nonuniform corresponds to adding arbitrary numerical predicates (ARB) to FO and FOM, respectively (Immerman, 1997; Mix Barrington et al., 1990).As we will see below, circuits and logics each have their advantages and disadvantages for capturing the expressivity of transformers.An advantage of the circuit approach is that they have a more transparent resemblance to transformers.Transformers are computations with bounded depth, so it's not hard to see that they should be computable by circuit families with bounded depth (AC 0 or TC 0 ).On the other hand, an advantage of the logical approach is that if we seek an exact characterization of transformers, it can be easier in a logic to add or remove quantifiers or predicates, to limit quantifier depth or number of variables, to partition terms into different sorts, and so on, than to make adjustments to a circuit family.", "filtered_refids": [[null, "b50", "b48"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1410, "num_references": 3}
{"corpusid_sectionid": "264833196-s28", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "PARITY", "section": "As the classic example of a language in (uniform) TC 0 but not AC 0 (Ajtai, 1983;Furst et al., 1984), PARITY is a particularly interesting case-study.Hahn (2020) showed that leftmost-hard attention transformers cannot recognize PARITY, using a variant of Furst et al.'s random restriction method.He also showed that softmax attention transformers cannot generate PARITY under the following two conditions: \u2022 all position-wise functions are Lipschitzcontinuous, and\n\n\u2022 generation is defined using the KL divergence criterion in Eq. ( 5).\n\nOn the other hand, Chiang and Cholak (2022) showed that transformer encoders whose PE includes / do recognize PARITY.They give two constructions, corresponding to Hahn's two assumptions.The first has Lipschitz-continuous position-wise functions, but has high cross-entropy ( \u00a74.3.1); as a generator, it would not meet criterion (5).The second construction uses layernorm with  N = 0, which is not Lipschitz-continuous, but it has arbitrarily low cross-entropy.\n\nThe apparent contradiction is resolved by considering the different assumptions underlying each result.The fact that PARITY's recognizability is so sensitive to assumptions suggests that it is close to the borderline of what transformer encoders can recognize.Empirically, several authors (Bhattamishra et al., 2020a;Del\u00e9tang et al., 2023) have found that transformer encoders do not learn PAR-ITY.", "filtered_refids": [["b21", "b1", "b23"], [], [], ["b16", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1398, "num_references": 5}
{"corpusid_sectionid": "264833196-s29", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Dyck languages", "section": "Hahn (2020) investigates both DYCK-1 and DYCK-2.He shows that leftmost-hard attention transformer encoders cannot recognize either one.Furthermore, he shows that softmax attention transformer encoders, under the same restrictions as for PARITY ( \u00a77.1.1),cannot recognize DYCK-2.Bhattamishra et al. (2020a) prove that SHUFFLE-DYCK- (which is equal to DYCK-1 when  = 1) is recognizable by a soft-attention transformer encoder with future masking, no position encoding, no layernorm, and no residual connections.Yao et al. (2021) 1: Surveyed claims on encoder models and their assumptions.In some cases, assumptions are simplified or omitted to save space; please see main text for details.Ebrahimi et al. (2020) perform experiments to train transformers to recognize DYCK-2 and DYCK-4, finding that they are competitive with LSTMs, and that prepending a BOS symbol helps.", "filtered_refids": [["b74", "b8", "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 869, "num_references": 3}
{"corpusid_sectionid": "264833196-s30", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Other languages", "section": "The language MAJORITY, like PARITY, is not in AC 0 , but P\u00e9rez et al. (2019) prove that a transformer encoder (rather, a transformer encoder-decoder with a trivial decoder) without any position encoding recognizes MAJORITY; Merrill et al. (2022) prove the same for transformer encoders.Bhattamishra et al. (2020a) experiment with training transformer encoders on some counter languages: DYCK-1, SHUFFLE-DYCK- for  \u2208 {2, 4, 6}, -ary boolean expressions in prefix notation for  \u2208 {3, 5}, {a  b  }, {a  b  c  }, {a  b  c  d  }, observing nearly perfect learning and generalization to longer lengths with masking and no positional encoding.They also experiment with learning regular languages including the Tomita (1981) languages, PARITY, (aa) * , (abab) * , and DYCK-(1, ) for  \u2208 {2, 3, 4, 12}, where they find that only the star-free languages of dot-depth 1 are learned and generalized well.Del\u00e9tang et al. (2023) study experimentally how well transformer encoders (and other networks) learn tasks at various levels of the Chomsky hierarchy, including generalization to longer strings.Languages include a\u03a3 * a + b\u03a3 * b, PARITY and MAJORITY.Of these three languages, they find that transformers learn only MAJORITY.3) takes the negative absolute value of the dot-product, and Eq. ( 4) uses average-hard attention.\u2022 The FFNs use sigmoids instead of ReLUs.As described above ( \u00a74.3.3), the decoder is allowed to run for arbitrarily many time steps until an acceptance criterion is met.Under these assumptions, transformer encoder-decoders can recognize any recursively enumerable language. 2his result uses arbitrary precision, but as a corollary, P\u00e9rez et al. (2021) show that a  ()time-bounded Turing machine can be simulated in a transformer using  (log  ()) bits of precision.2023) study the ability of transformer decoders to simulate deterministic finite automata (DFAs), in the sense of computing not only the same acceptance decision but also the same state sequence.Although a transformer with depth  can simulate a DFA for  timesteps, Liu et al. show how to construct lower-depth shortcuts for subclasses roughly corresponding to classes of regular languages in Fig. 1.These constructions depend on , but in the context of this survey, a noteworthy finding is that any regular language in ACC 0 can be recognized up to length  by a transformer decoder whose FFNs use sine activations and whose number of parameters is independent of  (although the parameters themselves do depend on ).", "filtered_refids": [["b54", "b56", "b46", "b16", "b8", "b68"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2492, "num_references": 6}
{"corpusid_sectionid": "264833196-s32", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Circuits", "section": "The results in this section and the next all consider transformer encoders.For brevity, we write \"transformer\" to mean \"transformer encoder.\"2022) generalize the leftmost-hard attention results of Hahn (2020) to show that any language recognized by a transformer with leftmosthard attention is in AC 0 .The proof gives a normal form for transformers with leftmost-hard attention and uses it to construct an AC 0 circuit family.Like the leftmost-hard attention results of Hahn (2020), this result applies to a generalized model of transformers with few restrictions on the component functions.It uses the fact that only  (log ) bits of information are needed per position.2022), but for averagehard attention transformers.They show that a transformer with activations in F can be simulated in TC 0 .The key reason why soft attention extends the power to TC 0 is because it enables counting (cf.\u00a77.2.3), and counting can be used to solve problems like MAJORITY that are outside AC 0 .Furthermore, Merrill and Sabharwal (2023b) show that softmax attention,  (log )-precision transformers are in L-uniform TC 0 , and Strobl (2023) shows that average-hard attention transformers are in L-uniform TC 0 as well.This uniform TC 0 upper bound fits transformers into other known complexity hierarchies, revealing that transformers likely cannot solve problems complete for NC 1 , L, and other classes believed to be above TC 0 ( \u00a76.4).", "filtered_refids": [["b45", "b23", "b64"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1425, "num_references": 3}
{"corpusid_sectionid": "264833196-s34", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "First-order logic with majority", "section": "Merrill and Sabharwal (2023a) further tighten the L-uniform TC 0 upper bound of Merrill and Sabharwal (2023b) to DLOGTIME-uniform TC 0 , and therefore FOM [BIT].The proof constructs subroutines to answer queries about the types of nodes and connectivity of pairs of nodes in the computation graph of a transformer, and shows that these queries can be translated to queries for a TC 0 circuit family with  (log ) time overhead.which is first-order logic with counting quantifiers, using two sorts for positions and counts (Immerman, 1999, p. 185-187), where positions have the MOD predicate (but not < or =), and counts have <, +, and =, capturing the fact that transformers can add and compare activations, but not positions.They show that this logic is intermediate in expressivity between  (1)-precision and infinite-precision transformers.The lower-bound proof makes use of a normal form that eliminates quantifiers over counts and makes quantifiers over positions have depth 1; a perhaps surprising consequence is that  (1)-precision transformers are no more powerful than 2-layer uniform-attention transformers.Weiss et al. (2021) define a programming language called RASP (Restricted Access Sequence Programming Language) and show that it can be compiled to transformers with average-hard attention and two extensions: \u2022 Attention weights are directly computed from the previous layer without being confined to dotproducts of query and key vectors.\u2022 Position-wise FFNs compute arbitrary computable functions.Lindner et al. (2023) describe a RASP compiler that outputs standard transformers.It compiles RASP selectors to dot-product attention, with syntactic restrictions on selectors and a maximum string length.Element-wise operations are approximately compiled to ReLU FFNs.Friedman et al. (2023) define Transformer Programs, a restricted class of transformers that can be translated into RASP programs.", "filtered_refids": [["b20", "b45", null, "b73", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1911, "num_references": 5}
{"corpusid_sectionid": "265607947-s7", "title": "Teach Me How to Argue: A Survey on NLP Feedback Systems in Argumentation", "date": 2023, "section_title": "Pedagogy", "section": "Before discussing the four dimensions mentioned a priori, it is essential to know the pedagogy used to teach argumentation and adopted by computational models.This section presents some standard pedagogical methods used in teaching how to argue.\n\nToulmin model The Toulmin model (Toulmin, 1958), often seen as the foundation of teaching argumentation, is a popular framework for constructing, analyzing and evaluating arguments, and can contribute to the improvement of students' argumentative writing (Rex et al., 2010;Yeh, 1998) as well as critical thinking skills (Giri and Paily, 2020).This approach deconstructs an argument into six elements (Appendix, Figure 4), and students are taught to identify each element within an argument.\n\nBy identifying elements from the Toulmin model, models can provide users with rich feedback.\n\nRhetorical structure theory Based on Mann and Thompson (1988), the rhetorical structure theory was originally developed in the context of computer-based text generation in order to attribute a formal structure to a text (Hou et al., 2020).This theory employs graphical representations, such as mind maps or graphs, to illustrate the relationships between different components of the text's architecture.This visual approach can help students visualize the connections between different concepts and enhance their understanding of complex topics (Matsumura and Sakamoto, 2021).The advent of tools like Tiara (Putra et al., 2020) has given rise to the deployment of the rhetorical structure theory, i.e. the generation of visual feedback.\n\nCollaborative argumentation In collaborative argumentation-based learning, also described as CABLE by Baker et al. (2019), individuals work together to construct, refine, and evaluate arguments on a particular topic or issue.The main goal of collaborative argumentation is to foster constructive dialogue, critical thinking, and the exploration of different perspectives.Weinberger and Fischer (2006) differentiate four dimensions of CABLE:\n\n\u2022 Participation: Do learners participate at all? Do they participate on an equal basis?\n\n\u2022 Epistemic: Are learners engaging in activities to solve the task (on-task discourse) or rather concerned with off-task aspect?\n\n\u2022 Argumentative: Are learners following the structural composition of arguments and their sequences?\n\n\u2022 Social: To what extent do learners refer to the contributions of their learning partners?Are they gaining knowledge by asking questions?Veerman et al. (2002); Baker et al. (2019) show CABLE's positive effects on students' argumentation development.Nevertheless, they also highlight the challenges of this method, as not every dialogue can be predicted.By using CABLE, models can generate interactive feedback.", "filtered_refids": [[], ["b100", "b23", "b82", "b71"], [], ["b59", "b68", "b33", "b58"], ["b98", "b7"], [], [], [], ["b86", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2745, "num_references": 12}
{"corpusid_sectionid": "265607947-s9", "title": "Teach Me How to Argue: A Survey on NLP Feedback Systems in Argumentation", "date": 2023, "section_title": "Richness -What is an Error and Why?", "section": "To improve students' critical thinking skills, we first need to evaluate their argumentative texts, i.e., identify argumentative errors.In this section, we focus on models providing shallow explanations, i.e., models that identify what should be corrected in the arguments.We discuss relevant works that identify properties such as the structure of arguments which is helpful in this process.\n\nComponents Identifying argumentative components is one of the fundamental tasks in argumentation (Teufel, 1999;Stab and Gurevych, 2014;Jo et al., 2020).Such works primarily focus on identifying components such as claims and premises.More recently, the usefulness of identifying such components can be seen in tasks such as counterargument generation.For example, in Alshomary et al. (2021), weak premises are identified and ranked to generate counter-arguments.\n\nRelations After identifying the different components of an argumentative text, it is necessary to distinguish the multiple relations between them, ultimately to assert the arguments' quality.Indeed, supporting or refuting a claim is made of complex logical moves, such as promoting, contradicting, or acknowledging a fact.To identify the different relations patterns, Yuan et al. (2021) focus on finding interactive argument pairs, whereas Mim et al. ( 2022) enables annotating complex attack relations.\n\nSchemes In addition to components and relations, Walton et al. (2008) proposed a set of roughly 80 logical argumentation schemes to categorize the underlying logic.Each scheme has a set of critical questions which provide a template to assess the strength of the argument depending upon the associated scheme.Since the first work on automatically detecting argumentation schemes in argumentative texts (Feng and Hirst, 2011), the use of such schemes has been explored in tasks such as essay scoring (Song et al., 2014).\n\nFallacies Although a good structure with a claim and premises is necessary for a good argument, it is not sufficient.An argument has more complex properties, such as its logical, dialectical, and rhetorical aspects.A fallacy is a logical error or deceptive argument that undermines the validity of a conclusion or reasoning, which poses a substantial issue due to its propensity to generate miscommunication.Towards teaching students to avoid making errors in logical reasoning, logical fallacies have received attention (Habernal et al., 2017;Bonial et al., 2022;Zhivar et al., 2023;Nakpih and Santini, 2020).Motivated by the gamification method made by Habernal et al. (2017), Bonial et al. (2022) aimed to capture similar fallacy types for news articles, but the low distribution of fallacy types in the wild makes identification challenging.However, most natural texts do not have recurrent specific patterns, compared to current datasets, like the Logic and LogicClimate datasets (Jin et al., 2022).Moreover, given the large number of logical fallacies that exist (over 100 types), long arguments can be grouped into multiple fallacies, resulting in difficulties in classification (Goffredo et al., 2022).", "filtered_refids": [[], ["b80", "b81", "b41", "b5"], ["b101"], ["b21", "b89", "b79"], ["b40", "b13", "b104", "b27", "b62", "b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3093, "num_references": 14}
{"corpusid_sectionid": "265607947-s10", "title": "Teach Me How to Argue: A Survey on NLP Feedback Systems in Argumentation", "date": 2023, "section_title": "Debate patterns", "section": "In a case of a debate, an opponent is willing to give a counter-argument synchronously and interactively.Analyzing and evaluating a debate is a difficult task as we need to retrieve not only the argumentation structure of each opponent but also the relations between them.Bao et al. (2022) focuses on argument pair extraction (APE), which consists of finding two interactive arguments from two argumentative passages of a discussion.Although the APE task gives insights into relations between different argumentative texts, it does not indicate complex relations (i.e., how claims, supports, attacks and the intention of the speakers are interrelated).To palliate this issue, Hautli-Janisz et al. ( 2022) identified and analyzed the dialogical argumentative structure of debates using Inference Anchoring Theory (IAT) (Budsziyska et al., 2014).Following the same IAT theory, Kik-teva et al. (2022) showed that the type of questions (e.g., pure, assertive, and rhetorical questions) leads to different argumentative discourse.Focused more on the opponent's side, Naito et al. (2022) propose diagnostic comments for assessing the quality of counter-arguments by providing expressive, informative and unique templates.The feedback is then written by template selection and slot filling.\n\nIn-Depth Explanations Although identifying such argumentative structures (components, relations, and schemes) and properties (fallacies and debate patterns) is important, it has limitations in terms of effective feedback.Identifying a missing claim or a wrong premise is insufficient to understand how to improve the argumentation properly.Thus, we relate the identification of structure and properties to shallow explanations in the sense that users can still benefit from the output of the models.\n\nShallow explanations can be difficult to understand, especially for beginners, as they tend to be minimalist and lack guidance.To explain more effectively the errors in an argument, a model should go a step further, hence by providing in-depth explanations, which attempt to identify the argument's implicit components to explain why it is an error in a particular argument.In Figure 2, we implicitly know that hamburgers belong to the American cuisine, as same as the Cobb salad, a healthy garden salad from California.Therefore, if the model is able to reason out this implicit knowledge, it can better explain the invalid generalization in Figure 2.\n\nImplicit Knowledge and Reasoning in Arguments To provide in-depth explanations, we need to know how to refine the argument, i.e., how to identify implicit information.Recently, many works have focused their attention on this aim.The main goal of such studies is to make the structure and reasoning of arguments explicit to explain the arguments for humans better.Additionally, this focus can eventually help build robust argumentation machines that can be enriched with language understanding capacity.Following the pioneer works of Razuvayevskaya and Teufel (2017), the ExpLAIN project (Becker et al., 2021) and Jo et al. (2021) are one such example that focuses extensively on reconstructing implicit knowledge in arguments by relying on knowledge graphs among others.Taking a step further in this direction, Heinisch et al. (2022) and Saadat-Yazdi et al. (2023) proposed to utilize such implicit information to bridge the im-plicit reasoning gap in arguments to help students explain their arguments better.\n\nLarge annotated corpora are required to improve implicit reasoning detection for models.To address this need, various studies have proposed methods for annotating implicit knowledge, leading to the development of multiple datasets (Becker et al., 2020;Singh et al., 2021Singh et al., , 2022)).In Singh et al. (2021), semi-structured warrants, i.e. links between a claim and evidence (c.f.Appendix Figure 4), were annotated via crowdsourcing, whereas Becker et al. (2020) focus on reconstructing omitted information, semantic clause types, and commonsense knowledge relations through expert annotation.Corpora can be dedicated to a specific domain or sentence patterns.For example, (Singh et al., 2022) focused on domain-specific knowledge using six topics.However, implicit knowledge may take various forms, such as warrants, causal relations, facts, beliefs, or assumed-known arguments.Thus, revealing implicit knowledge in an unknown text through annotated datasets can be challenging.\n\nIn recent years, LLMs have made significant progress in exhibiting reasoning abilities.A comprehensive overview of the current state of reasoning abilities in LLMs is provided in the survey Huang and Chang (2023).The increasing interest in LLMs and implicit reasoning prompted the first ever workshop on natural language reasoning and structured explanations in 2023 (Dalvi Mishra et al., 2023).This workshop discussed that while LLMs have demonstrated good capabilities to find implicit components within an argument, they often cannot correctly explain the logical reasons behind their responses.To bridge this gap, a novel category of explanation techniques has arisen, playing a vital role in shaping the logical reasoning of models.One such example is the chain-of-thought prompting (Wei et al., 2022;Wang et al., 2023a), which employs explanations as a means for LLMs to emulate human reasoning procedures.While the references Huang and Chang (2023) and Dalvi Mishra et al. ( 2023) do not primarily focus on argumentative tasks, they can be a valuable source of inspiration in argumentation.", "filtered_refids": [["b14", null, "b8", "b61"], [], [], ["b10", "b31", null, "b72", "b42"], ["b78", "b77", "b9"], [null, "b97", "b94", "b35"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 29, "num_chars": 5538, "num_references": 16}
{"corpusid_sectionid": "265607947-s11", "title": "Teach Me How to Argue: A Survey on NLP Feedback Systems in Argumentation", "date": 2023, "section_title": "Visualization -How to Show the Error?", "section": "The effectiveness of any argument does not solely rely on its content but also on its presentation.This is where visualization of argumentative feedback emerges as a crucial factor.Visualizing feedback empowers individuals to perceive the intricacies of an argument in a more comprehensive manner.By using visual aids like graphs, feedback becomes more accessible and engaging, fostering constructive discussions.In this section, we discuss how visualization impacts argumentative feedback.\n\nHighlights A simple approach to visualization is highlighting, i.e., application of visual emphasis on a specific pattern with the intention of drawing the viewer's attention to this specific pattern.For example, Lauscher et al. (2018)  Highlighting serves as an essential key step in the cognitive input process, enabling viewers to quickly identify crucial argumentative structures.However, its use should be complemented with other visualization techniques to ensure a more profound exploration and comprehension of complex Based on the classroom-setting evaluation, students using such systems wrote texts with a better formal quality of argumentation compared to the ones using the traditional approach.\n\nNevertheless, the current accuracy of such systems' feedback still leaves a large improvement space in order for users to be motivated to use them.More recent work such as Zhang et al. (2023) incor-porate feedback generated by state-of-the-art LLMs in their graphical systems.Nonetheless, factual inaccuracies, as well as inconsistent or contradictory statements, are still generated, exposing the user to confusion and leaving room for improvement.\n\nDialogue Systems In the realm of visualization, a novel approach gaining attraction is the integration of dialogue systems to enhance the interaction between users and visual representations.Dialogue systems, commonly known as chatbots like ChatGPT, have been increasingly explored for their potential to facilitate information comprehension (Rach et al., 2020;Wambsgan\u00df et al., 2021).\n\nThis kind of representation is challenging in terms of user-friendliness.Particularly, in a pedagogical context, users may have difficulties visualizing their previous feedback and progress.Indeed, users may be lost in the discussion flow and struggle to keep track of the ongoing discussions, lessons, or feedback because the representation does not provide clear signposts or structure.Students may forget a specific lesson and want to verify some information, or they simply need to reread their lessons and exercises.However, finding specific information in a chat discussion may take much effort.Thus, it is important (i) to have a chat session per lesson, exercise or test and (ii) to keep structured notes of the issues users face and how these issues can be solved.Eventually, a personal dashboard showing a user's progress through time could be beneficial not only for students but also for teachers.Indeed, with a dashboard, teachers can see if a specific student needs more attention.Moreover, teachers sometimes need to compare students among them, specifically during a test.Therefore, we believe that to improve the user-friendliness of pedagogical dialogue systems, other visual elements should be used.\n\nDespite the growing popularity of both graphs and chatbots in data visualization, limited work has directly compared their effectiveness in improving critical thinking skills.Further research is needed to provide more nuanced insights on the comparison on one hand between both approaches and on the other between works among the same approach.\n\nThe importance of visualization lies in its ability to enhance the understanding of complex ideas.In this section, we highlighted the potential of the visualization of argumentative feedback and how it can improve students' learning process.\n\n6 Interactivity -Who Talks to the User?\n\nTeaching how to argue is a multifaceted task that demands more than the dissemination of theoretical knowledge; it requires fostering interactive learning environments that facilitate active engagement and practice.The traditional approach to teaching argumentation often centers on lecturing and one-way communication, where instructors impart information to students.While didactic methods have their place in education, a more interactive pedagogical approach, one that encourages learners to actively participate, can be used.In this section, we will see in which ways current argumentative computational models enable a form of interaction.\n\nInteraction between different users NLP systems mostly allow communication between a user and a conversational agent.Nonetheless, some works chose to apply the CABLE pedagogy ( \u00a73) by allowing a user to dialog with other users.Following the footsteps of Petasis ( 2014 The collaboration between multiple users within NLP systems is promising.Nevertheless, only a few works focus on the CABLE pedagogy.It is essential to acknowledge that some challenges and barriers have hindered its use in NLP, possibly due to the difficulty of designing and evaluating such tools, as human resources in a real-class setting (e.g., students, teachers) are required.\n\nInteraction with a conversational agent As seen in \u00a75, several research papers have showcased the feasibility of employing current conversational agents for educational purposes (Lee et al., 2022;Macina et al., 2023;Wang et al., 2023b).Often based on state-of-the-art language models, these agents have shown great capabilities in understanding and generating human-like responses.They can engage in dynamic and contextually relevant conversations, making them potentially valuable tools for educational purposes.\n\nThe use of conversational agents as dialog tutors has been explored outside of argumentation (Wambsgan\u00df et al., 2021;Mirzababaei and Pammer-Schindler, 2022;Aicher et al., 2022).For instance, in Mirzababaei and Pammer-Schindler (2022), an agent examines arguments to determine a claim, a warrant, and evidence, identifies any missing elements, and then assists in completing the argument accordingly.Wambsgan\u00df et al. (2021) create an interactive educational system that uses interactive dialogues to teach students about the argumentative structure of a text.The system not only provides feedback on the user's texts but also learning sessions with different exercises.\n\nResearch on chatbots in education is still preliminary due to the limited number of studies exploring the application of effective learning strategies using chatbots.This indicates a significant opportunity for further research to facilitate innovative teaching methods using conversational agents (Hwang and Chang, 2021).However, extraction and classification of useful data remain challenging, as the data collected are noisy and much effort still has to be made to make it trainable (Lin et al., 2023).Researchers must also continue to account for ethical considerations, including biased representations and data privacy safeguards, to ensure that their chatbots positively impact users (Kooli, 2023).\n\nOverall, integrating interaction in teaching how to argue is not merely a pedagogical choice but an essential requirement to cultivate adept arguers who can navigate the intricacies of argumentation.Therefore, we encourage researchers to consider this dimension in their future pedagogical systems.\n\n7 Personalization -To Whom is it For?\n\nEven if the feedback mentioned in \u00a74 are a step towards good guidance, they are static, which can be problematic.Beginners and professionals in argumentation do not need the same amount of feedback.A child and an adult have different levels of understanding and knowledge.Therefore, it is essential that a model knows to whom it should explain the errors and hence how to adapt its output by providing personalized explanations.\n\nLevels of explanations A first approach to personalization is to discretize different users' proficiency levels in argumentation into a small number of categories.For instance, with the system described in Wambsgan\u00df et al. (2020) and Wambsganss et al. (2022a), users can select their own level among the following categories: Novice, Advanced, Competent, Proficient, Expert.\n\nAlthough Wambsgan\u00df et al. (2020) and Wambsganss et al. (2022a) propose different granularity levels of explanations, their study is restrained to students from their university.Having end-users from different backgrounds may imply the need for new levels of explanations.Wachsmuth and Alshomary (2022) show that the explainee's age affects the way an explainer explains the topic at hand.Thus, we consider that information such as the learner's age should be considered in future interactive argumentative feedback systems, where terminology such as fallacy and their existence would require different explanation approaches for younger students (i.e., elementary) compared to older students.\n\nSelf-personalization For more personalized feedback, systems such as Hunter et al. (2019) and Putra et al. (2020) rely on user's inputs.They allow users to make their custom tags or to choose their preferences among a set of rubrics.Nevertheless, manually personalizing the system can be overwhelming and time-consuming for users.Hunter et al. (2019) argue that the next direction for personalized argumentative feedback would be to develop argumentation chatbots for persuasion and infer the user's stance based on the discussion.Chatbots' personalization capabilities enable them to tailor their responses to individual learners' needs and learning styles, potentially enhancing the effectiveness of the tutoring process (Lin et al., 2023).However, bridging the gap among personalized chatbots (Qian et al., 2021;Ma et al., 2021), personalized educational methods (Gonz\u00e1lez-Gonz\u00e1lez et al., 2023;Ismail et al., 2023;Liu et al., 2020) and argumentation has remained unexplored.Thus, we think researchers should focus in the future on providing more personalized explanations (i.e., precisely adjusted by considering the learner's background) to improve the users' critical thinking skills efficiently.", "filtered_refids": [[], ["b47"], ["b103"], ["b70", "b92"], [], [], [], [], [], [], [null, "b50", "b95"], [null, "b92", "b2"], ["b53", "b46", "b37"], [], [], [], ["b93", "b90"], ["b93", "b90", "b87"], ["b54", "b39", "b56", "b25", "b69", "b53", "b68", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 59, "num_chars": 10122, "num_references": 26}
