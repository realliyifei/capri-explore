{"corpusid_sectionid": "8729907-s1", "title": "The Language ENvironment Analysis (LENA) System: A Literature Review", "date": "2016-11-15", "section_title": "The LENA System", "section": "The LENA System's hardware includes a digital language processor (DLP) that can audio record for up to 16 hours. It measures 3-3/8\" x 2-3/16\" x 1/2\", weighs less than two ounces, and consists of a display screen, a USB port for uploading, and two buttons for powering and recording. The processor is held in a specially designed t-shirt or vest with a pocket on the front to secure the device. The audio quality is a 16-bit channel at a 16kHz sample rate (Ford, Baer, Xu, Yapanel, & Gray, 2008). Once the recording is complete it can be uploaded to the LENA software. Recordings are stored in the software by participant, allowing repeated recordings of one participant to be saved and compared over time. Once uploaded and recharged, the same participant or a new participant can use the DLP again without affecting the data stored in the software. The LENA System automatically segments the recordings into 12 categories including speakers, environmental sounds, and silence using Gaussian mixture models. A daylong audio file typically consists of 20,000 to 50,000 segments (VanDam et al., 2016). The software then estimates: adult word count (AWC), child vocalization count (CVC), and conversational turn count (CTC). The amount of background noise, electronic sounds, meaningful speech, and silence that were part of the child's listening environment are reported as percentages of the total sound present in the day and are displayed in user-friendly LENA generated graphs along with the AWC, CVC, and CTC. Additional details can be extracted using ADEX software provided by the LENA Foundation (Ford, et al., 2008;VanDam, Ambrose, & Moeller, 2012).\n\nIn addition to the raw data counts, Richards, Gilkerson, Paul, & Xu (2008) discuss the Automatic Vocalization Assessment (AVA) generated by the LENA System, which is correlated with traditional expressive language standard scores including those from the Preschool Language Scale -4 th Edition (PLS-4) (Zimmerman, Steiner, & Pond, 2002) and the Receptive-Expressive Emergent Language Test -3 rd Edition (REEL-3) (Bzoch, League, & Brown, 2003). To learn more about the LENA hardware and software, consult Ford et al. (2008) and .\n\nIn order to establish reliability, human transcribers coded 70 full day English recordings and their results were compared with those obtained by the automated software (Xu, Yapanel, Gray, & Baer, 2008). This data was collected as part of the Natural Language Study (NLS), the LENA Foundation's normative study . The LENA System correctly identified 82 and 76 percent of the segments humans coded as adult speech and child vocalizations respectively, indicating reasonable levels of agreement Warren et al., 2010;Xu et al., 2008;). Validity has also been shown in Spanish, French, Mandarin, Korean, and Vietnamese (Canault, Le Normand, Foudil, Loundon, & Thai- Van, 2015;Ganek & Eriks-Brophy, in revision;Gilkerson et al., 2015;Pae et al., 2016;Weisleder & Fernald, 2013). Although these studies show high fidelity, recording in a child's natural environment can produce a degraded auditory signal that may negatively impact validation. Possible causes of interference might include environmental factors such as background noise, overlapping speech, and reverberation, speaker variation like pitch or voice quality, and hardware variability. Although LENA clothing has been rigorously tested, fabric sound absorption rates may also impact accuracy ).", "filtered_refids": [[null, "b10", "b39", "b41"], ["b33", "b10", "b2", "b55"], ["b52", "b14", "b47", "b31", "b45", null, "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3438, "num_references": 15}
{"corpusid_sectionid": "8729907-s4", "title": "The Language ENvironment Analysis (LENA) System: A Literature Review", "date": "2016-11-15", "section_title": "Type of Study", "section": "Studies were divided into three types: comparative studies that examined LENA results between at least two cohorts, longitudinal studies that measured children's progress over time, and crosssectional studies that investigated children's ability at a specific point in time. Sixteen of the papers reviewed were comparative. They generally matched typically developing children to children with a communication disorder, though some compared language groups or treatment versus control groups. Eleven longitudinal studies evaluated child development over time. Both comparative and longitudinal studies measured the effects of treatment. Treatments including traditional speech therapy , formal established treatment programs such as Hanen's It Takes Two to Talk (Manolson, 1992;Weil & Middleton, 2011), and treatment associated specifically with provision of LENA feedback (Pae et al., 2016;Suskind et al., 2013). The remaining eleven cross-sectional studies often relied on a single day of recording.", "filtered_refids": [["b38", "b23", "b31", "b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1001, "num_references": 4}
{"corpusid_sectionid": "8729907-s15", "title": "The Language ENvironment Analysis (LENA) System: A Literature Review", "date": "2016-11-15", "section_title": "Socio-Economic Status (SES)", "section": "Socio-economic status (SES) is a measure of a person's social position based on income, education, and occupation. Hart and Risley (1995) famously reported a correlation between SES, language stimulation, and language abilities. Their study, and those like it, inspired the creation of the LENA System. Even though the impact of SES on language outcomes is widely known, few of the studies reported here were able to control for it. Ten studies failed to report SES and another six reported that comparative groups were matched either to each other or to census data. Six represented a range of maternal educational levels. Nine of the studies reported that their samples skewed towards high SES participants while five others reported collecting only low SES participants. Two studies also reported an SES mismatch between comparative groups (Jackson & Callender, 2014;Wood, et al., 2016).", "filtered_refids": [["b49", "b16", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 890, "num_references": 3}
{"corpusid_sectionid": "263153015-s2", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Background", "section": "In recent years, with the continuous expansion of computing power, large-scale language models have sprung up (Brown et al., 2020;OpenAI, 2023;Touvron et al., 2023a;Scao et al., 2022;Touvron et al., 2023b;Zhao et al., 2023b), and as the model size continues to grow, many new capabilities have emerged, such as in-context learning and chain-ofthought reasoning (Brown et al., 2020;Wei et al., 2022b,a;Schaeffer et al., 2023).Brown et al. (2020) finds that large-scale language models have excellent in-context learning (ICL) ability.ICL incorporates input-output demonstrations into the prompt text.With ICL, off-the-shelf LLMs can be employed without additional fine-tuning while achieving comparable performance.Nevertheless, this end-to-end approach tends to underperform when faced with complex reasoning tasks.Wei et al. (2022b) finds that the reasoning ability of LLMs can be improved by adding step-by-step reasoning processes to the demonstration, which is known as chain-of-thought prompting.CoT prompting enables the model to gain a more precise understanding of both the question's intricacies and the reasoning process.Furthermore, the model generates a sequence of reasoning steps, which grants us a transparent view of the model's cognitive process, further enhancing interpretability.", "filtered_refids": [["b158", null, "b8", "b124"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1299, "num_references": 4}
{"corpusid_sectionid": "263153015-s4", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Mathematical Reasoning", "section": "Mathematical reasoning is often used to measure the reasoning power of a model.Early benchmarks contain simple arithmetic operations (Hosseini et al., 2014;Koncel-Kedziorski et al., 2015;Roy and Roth, 2015;Koncel-Kedziorski et al., 2016).Ling et al. (2017) labels the reasoning process in natural language form, and Amini et al. (2019) builds on AQUA by labeling the reasoning process in program form.Later benchmarks (Miao et al., 2020;Patel et al., 2021;Cobbe et al., 2021;Gao et al., 2023) contain more complex and diverse questions.(Zhu et al., 2021;Chen et al., 2021Chen et al., , 2022b) ) require reasoning based on the table content.\n\nThere are also general benchmarks (Hendrycks et al., 2021;Mishra et al., 2022a,b) and reading comprehension form benchmarks (Dua et al., 2019;Chen et al., 2023).Recently, (Yu et al., 2021a) endowed pre-trained model with the ability of mathematical reasoning by using hierarchical reasoning and knowledge.", "filtered_refids": [["b43", "b14", "b203", "b13", "b31", "b112", "b120", null, "b99", "b16", "b83", "b2", "b65"], ["b26", "b44", "b179", "b41", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 947, "num_references": 18}
{"corpusid_sectionid": "263153015-s5", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Commonsense Reasoning", "section": "Commonsense reasoning is the process of making inferences, judgments, and understandings based on knowledge that is generally known and commonly perceived in the everyday world.How to acquire and understand commonsense knowledge  (Ling et al., 2017) 100,000 Question Option Natural Language Math reasoning with NL rationale ASDiv (Miao et al., 2020) 2305 Question Number Equation Multi-step math reasoning SVAMP (Patel et al., 2021) 1,000 Question Number Equation Multi-step math reasoning GSM8K (Cobbe et al., 2021) 8,792 Question Number Natural Language Multi-step math reasoning GSM-Hard (Gao et al., 2023) 936 Question Number Natural Language GSM8K with larger number MathQA (Amini et al., 2019) 37,297 Question Number Operation Annotated based on AQUA DROP (Dua et al., 2019) 96,567 Question+Passage Number+Span Equation Reading comprehension form TheoremQA (Chen et al., 2023) 800 Question+Theorem Number \u2717 Answer based on theorems TAT-QA (Zhu et al., 2021) 16,552 Question+Table+Text Number+Span Operation Answer based on tables FinQA (Chen et al., 2021) 8  (Park et al., 2020) 1,465,704 Image+Event Action+Intent \u2717 Visual commonsense reasoning PMR (Dong et al., 2022) 15,360 Image+Background Option \u2717 Premise-based multi-modal reasoning ScienceQA (Lu et al., 2022) 21,208 Q+Image+Context Option Natural Language Multi-modal reasoning with NL rationales VLEP (Lei et al., 2020) 28,726 Premise+Video Option \u2717 Video event prediction CLEVRER (Yi et al., 2020) 305,280 Question+Video Option/Free-form Program Video temporal and causal reasoning STAR (Wu et al., 2021) 600,000 Question+Video Option \u2717 Video situated reasoning NEXT-QA (Xiao et al., 2021) 47 is a major impediment to models facing commonsense reasoning.Many benchmarks and tasks are proposed focusing on commonsense understanding (Talmor et al., 2019(Talmor et al., , 2021;;Bhakthavatsalam et al., 2021;Mihaylov et al., 2018;Geva et al., 2021;Huang et al., 2019;Bisk et al., 2020), event temporal commonsense reasoning (Rashkin et al., 2018;Zhou et al., 2019) , and commonsense verification (Wang et al., 2019).", "filtered_refids": [["b90", "b13", "b160", "b83", "b174", "b203", "b49", "b112", "b7", "b16", "b2", "b200", "b163", "b99", "b119", "b24", "b145", "b138", "b5", "b111", "b100", "b26", "b44", "b31", "b137", "b72", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 2078, "num_references": 27}
{"corpusid_sectionid": "1040974-s15", "title": "A SURVEY OF MACHINE TRANSLATION: ITS HISTORY, CURRENT STATUS, AND FUTURE PROSPECTS", "date": 1985, "section_title": "Jonathan Sloeum", "section": "A Survey of Machine Translation SYSTRAN SYSTRAN was one of the first MT systems to be marketed; the first installation replaced the IBM Mark II Russian-English system at the USAF FTD in 1970, and is still operational. NASA selected SYSTRAN in 1974 to translate materials relating to the Apollo-Soyuz collaboration, and EURATOM replaced GAT with SYSTRAN in 1976. Also by 1976, FTD was augmenting SYSTRAN with word-processing equipment to increase productivity (e.g., to eliminate the use of punched cards). The system has continued to evolve, for example by the shift toward a more modular design and by the allowance of topical glossaries (essentially, dictionaries specific to the subject area of the text). The system has been argued to be ad hoc -particularly in the assignment of semantic features (Pigott 1979). The USAF FTD dictionaries number over a million entries; Bostad (1982) reports that dictionary updating must be severely constrained, lest a change to one entry disrupt the activities of many others. (A study by Wilks (1978) reported an improvement/degradation ratio [after dictionary updates] of 7:3, but Bostad implies a much more stable situation after the introduction of stringent quality-control measures.)\n\nIn 1976 the Commission of the European Communities purchased an English-French version of SYSTRAN for evaluation and potential use. Unlike the FTD, NASA, and EURATOM installations, where the goal was information acquisition, the intended use by CEC was for information disseminationmeaning that the output was to be carefully edited before human consumption. Van Slype (1982) reports that \"the English-French standard vocabulary delivered by Prof. Toma to the Commission was found to be almost entirely useless for the Commission environment.\" Early evaluations were negative (e.g., Van Slype 1979), but the existing and projected overload on CEC human translators was such that investigation continued in the hope that dictionary additions would improve the system to the point of usability. Additional versions of SYSTRAN were purchased (French-English in 1978, andEnglish-Italian in 1979). The dream of acceptable quality for post-editing purposes was eventually realized: Pigott (1982) reports that \" . . . the enthusiasm demonstrated by [a few translators] seems to mark something of a turning point in [machine translation].\" Currently, about 20 CEC translators in Luxembourg are using SYSTRAN on a Siemens 7740 computer for routine translation; one factor accounting for success is that the English and French dictionaries now consist of well over 100,000 entries in the very few technical areas for which SYSTRAN is being employed.\n\nAlso in 1976, General Motors of Canada acquired SYSTRAN for translation of various manuals (for vehicle service, diesel locomotives, and highway transit coaches) from English into French on an IBM mainframe. GM's English-French dictionary had been expanded to over 130,000 terms by 1981 (Sereda 1982). Subseque~ly~ GM purchased an English-Spanish version of SYSTRAN, and began to build the necessary [very large] dictionary. Sereda (1982) reports a speed-up of 3-4 times in the productivity of his human translators (from about 1000 words per day); he also reveals that developing SYSTRAN dictionary entries costs the company approximately $4 per term (word-or idiom-pair).\n\nWhile other SYSTRAN users have applied the system to unrestricted texts (in selected subject areas), Xerox has developed a restricted input language (Multinational Customized English) after consultation with LATSEC. That is, Xerox requires its English technical writers to adhere to a specialized vocabulary and a strict manual of style. SYSTRAN is then employed to translate the resulting documents into French, Italian, Spanish, German, and Portuguese. Ruffino (1982) reports \"a five-to-one gain in translation time for most texts\" with the range of gains being 2-10 times. This approach is not necessarily feasible for all organizations, but Xerox is willing to employ it and claims it also enhances source-text clarity.\n\nCurrently, SYSTRAN is being used in the CEC for the routine translation, followed by human post-editing, of around 1,000 pages of text per month in the couples English-French, French-English, and English-Italian (Wheeler 1983). Given this relative success in the CEC environment, the Commission has recently ordered an English-German version as well as a French-German version. Judging by past experience, it will be quite some time before these are ready for production use, but when ready they will probably save the CEC translation bureau valuable time, if not real money as well.", "filtered_refids": [["b52", "b34", "b4"], ["b48", "b35", null], ["b39"], ["b36"], ["b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 4655, "num_references": 9}
{"corpusid_sectionid": "1040974-s16", "title": "A SURVEY OF MACHINE TRANSLATION: ITS HISTORY, CURRENT STATUS, AND FUTURE PROSPECTS", "date": 1985, "section_title": "LOGOS", "section": "Development of the LOGOS system was begun in 1964. The first installation, in 1971, was used by the U.S. Air Force to translate English maintenance manuals for military equipment into Vietnamese. Due to the termination of U.S. involvement in that war, its use was ended after two years. (A report by Sinaiko and Klare (1973) disparaged LOGOS's cost-effectiveness, but this claim was argued to be seriously flawed and was formally protested (Scott, personal communication).) The linguistic foundations of LOGOS are not well advertised, presumably for reasons involving trade secrecy. The system developer states that \"our linguistic approach ... has evolved in ways analogous to case grammar/valency theory . . . mapping natural language into a semantosyntactic abstraction language organized as a tree\" (Scott, personal communication).\n\nLOGOS continued to attract customers. In 1978, Siemens AG began funding the development of a LOGOS German-English system for telecommunications manuals. After three years LOGOS delivered a \"production\" system, but it was not found suitable for use (due in part to poor quality of the translations, and in part to the economic situation within Siemens which had resulted in ff much-reduced demand for translation, hence no imme-diate need for an MT system). Eventually LOGOS forged an agreement with the Wang computer company that allowed the :implementation of the German-English system (formerly restricted to large IBM mainframes) on Wang office computers.\n\nThis system reached the commercial market, and has been purchased by several multi-national organizations (e.g., Nixdorf, Triumph-Adler, Hewlett-Packard); development of other language pairs (e.g., English-French, English-German) is underway (Scott, personal communication). METEO TAUM-METEO is the world's only example of a truly fully-automatic MT system. Developed as a spin-off of the TAUM technology, as discussed earlier, it was fully integrated into the Canadian Meteorological Center's (CMC's) nation-wide weather communications network by 1977. METEO scans the network traffic for English weather reports, translates them \"directly\" into French, and sends the translations back out over the communications network automatically. Rather than relying on post-editors to discover and correct errors, METEO detects its own errors and passes the offending input to human editors; output deemed \"correct\" by METEO is dispatched without human intervention, or even overview.\n\nTAUM-METEO was probably also the first MT system where translators were involved in all phases of the design/development/refinement; indeed, a CMC translator instigated the entire project. Since the restrictions on input to METEO were already in place before the project started (i.e., METEO imposed no new restrictions on weather forecasters), METEO cannot quite be classed with the Xerox SYSTRAN system, which relies on restrictions geared to the characteristics of SYSTRAN. But METEO is not extensible -though similar systems could be built for equally restricted textual domains, if they exist.\n\nOne of the more remarkable side effects of the METEO installation is that the translator turnover rate within the CMC went from 6 months, prior to METEO, to several years, once the CMC translators began to trust METEO's operational decisions and not review its output (Brian Harris, personal communication). METEO's input constitutes over 24,000 words per day, or 8.5 million words per year. Of this, it now correctly translates 90-95%, shuttling the other (\"more interesting\") 5-10% to the human CMC translators.\n\nAlmost all of these \"analysis failures\" are attributable to communications noise (the CMC network garbles some traffic), misspellings (METEO does not attempt corrections), or words missing from the dictionary, though some failures are due to the inability of the system to handle certain linguistic constructions. METEO's computational requirements total about 15 CPU minutes per day on a CDC 7600 (Thouin 1982). By 1981, it appeared that the builtin limitations of METEO's theoretical basis had been reached, and further improvement was not likely to be cost-effective.", "filtered_refids": [[null, "b41"], [], [null], [], [], ["b45"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 4160, "num_references": 4}
{"corpusid_sectionid": "1040974-s18", "title": "A SURVEY OF MACHINE TRANSLATION: ITS HISTORY, CURRENT STATUS, AND FUTURE PROSPECTS", "date": 1985, "section_title": "SPANAM", "section": "Following a promising feasibility study, the Pan American Health Organization in Washington, D.C. decided in 1975 to undertake work on a machine translation system, utilizing some of the same techniques developed for GAT. Consultants were hired from nearby Georgetown University, the home of GAT. The official PAHO languages are English, French, Portuguese, and Spanish; Spanish-English was chosen as the initial language pair, due to the belief that \"This combination requires fewer parsing strategies in order to produce manageable output [and other reasons relating to expending effort on software rather than linguistic rules]\" (Vasconcellos 1983). Actual work started in 1976, and the first prototype was running in 1979, using punched card input on an IBM mainframe. With the subsequent integration of a word-processing system, production use could be seriously considered.\n\nAfter further upgrading, an in-house translation service based on SPANAM was created in 1980. Later that year, in its first major test, SPANAM reduced manpower requirements for a test translation effort by 45%, resulting in a monetary savings of 61% (Vasconcellos 1983). (Because these SPANAM translation and on-line post-editing figures appear to be contrasted against the purely manual, hardcopy translation tradition at PAHO, the gains from using SPANAM per se may be hopelessly confounded with the gains of working on-line; thus, it is difficult or impossible to say how much increase in productivity is accounted for by SPANAM alone.) Since 1980, SPANAM has been used to translate well over a million words of text, averaging about 4,000 words per day per post-editor. The post-editors have amassed \"a bag of tricks\" for speeding the revision work, and special string functions have also been built into the word processor for handling SPANAM's English output.\n\nConcerning the early status of SPANAM, sketchy details implied that the linguistic technology underlying it was essentially that of GAT; the grammar rules seemed to be built into the programs, in the GAT tradition. The software technology was updated in that the programs are modular. The system is not sophisticated: it adopts the direct translation strategy, and settles for local analysis of phrases and some clauses via a sequence of primitive, independent processing stages (e.g., homograph resolution) -again, in the Georgetown tradition. SPANAM is currently used by three PAHO translators in their routine work.\n\nA follow-on project to develop ENGSPAN (for English-Spanish), underway since 1981, has also delivered a production system -this one characterized by a more advanced design (e.g., an ATN parser), some features of which may find their way into SPANAM. (SPANAM is currently \"undergoing a major overhaul\" (Vasconcellos, personal communication).)\n\nFour PAHO translators already employ ENGSPAN in their daily work. Based on the successes of these two systems, development of ENGPORT (with Portuguese as the Target Language) has begun. In the future, \"all translators [in the Language Services bureau of PAHO will be] expected to use MT at least part of the time, and the corresponding duties are included in the post descriptions\". (Vasconcellos, personal communication).", "filtered_refids": [[null], [], [], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3233, "num_references": 3}
{"corpusid_sectionid": "245124108-s7", "title": "Surfer100: Generating Surveys From Web Resources, Wikipedia-style", "date": "2021-12-13", "section_title": "Content Selection", "section": "We first tested the quality of the content selection methods for generic retrieval of content relevant to a topic on our data. We choose the Semantic Search, WikiCite, and RoBERTa-Rank methods from Table 1 for analysis. For Semantic Search, we experiment with three types of sentence embeddings, the original sentence-transformer BERT embeddings (SS-BERT), embeddings fine-tuned with SciBERT (SS-SciBERT), and a version fine-tuned to differentiate whether two paragraphs belong to the same Wikipedia section (SS-Wiki). Surprisingly, we found such content was often returned during retrieval despite the poor grammaticality and relevance. We hypothesize that the tendency to return short sentences, often with odd punctuation may relate to the extension of these methods to paragraph levels while inherently being developed for sentencelevel tasks. We then remove sentences shorter than 6 tokenized words, as well as apply heuristics for removing sentences based on the number of parentheses, brackets, and other tokens such as equal signs. We required that each paragraph returned consist of at least two sentences and require that the topic word (or one word within the topic, for multi-word topics) appear in the paragraph. About 85 paragraphs per topic remain after this filtering. The comparison of results before and after preprocessing and filtering is found in Table 3. Notably, the WikiCite method performs much better than semantic search and close to RoBERTa. We believe this is because the method is trained for content selection based on a topic and not simply trained for returning content with high recall. A potential problem with current methods in this two-step approach is that content selection is trained and evaluated with recall in mind, to capture as large a range of the topic, which produces models without the precision necessary in a real-world application. This aligns with previous work in extractive summarization suggesting that optimizing for recall gives suboptimal results (Zopf et al., 2018). Section-Specific Content Selection: We investigated the ability of our content selection models to retrieve content specific for each chosen section, for example, querying \"History of BERT\"rather than \"BERT.\"We 2 https://github.com/IreneZihuiLi/Surfer100  observed large overlaps between the returned results, between 5 and 9 paragraph overlap between the top 10 results for each section. Among all methods, Wikicite has the least overlap. As an alternative method to select distinct content for each section, we investigate clustering methods, using out-of-the-box Agglomerative (M\u00fcllner, 2011) clustering provided by scikit-learn 3 . We cluster the embeddings obtained before the final output layer from the WikiCite and RoBERTa methods, and the Search-Wiki embeddings. We annotated the coherence of each cluster. Clusters obtained using embeddings from RoBERTa, Search-Wiki and Wi-kiCite had a corresponding average coherence of 3.07, 3.40, and 3.52 on a 1-5 scale, signaling slightly aboveaverage coherence for each clustering. Again, the poor performance of RoBERTa in clustering may be due to the more general topic training method. As suggested by Deutsch and Roth (2019), the WikiCite method may dilute topic information in the final layer despite topic attention in previous layers and thus benefit from using embeddings before the final layer as clustering.", "filtered_refids": [["b12", null, "b2", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3395, "num_references": 4}
{"corpusid_sectionid": "251307817-s5", "title": "Rhetorical Structure Approach for Online Deception Detection: A Survey", "date": 2022, "section_title": "Online Deception Detection", "section": "Over the past year, on the sharp growth of the web and social media, cyber-crimes such as identity blows, thief, fraud, and misinformation have become increasingly common. Theses deceptive activities often are characterized by the ease of deception and concealment of one's real identity (P\u00e9rez-Rosas et al., 2017). The research area responsible for investigating and providing methods to detect deceptive activities is known as deception detection. According to , automated deception detection, as a field within NLP and Information Science (IS), is responsible for the development of methods to distinguish truth from deception in textual data, identifying linguistic predictors of deception with text processing and machine learning techniques. Deception detection in textual information has became a relevant study area within NLP, mainly due to the sharing of fake news on the web and social media around the world. Online deceptive activities are addressed by literature on different tasks, which handle a wide range of aspects, such as credibility of users and sources, information veracity, information verification, and linguistic aspects of deceptive language (Atanasova et al., 2019). Unless otherwise stated, these tasks include the discovery of fake news (Lazer et al., 2018); rumor detection in social media (Vosoughi et al., 2018); information verification in question answering systems (Mihaylova et al., 2018); detection of information manipulation agents (Chen et al., 2013;Mubarak et al., 2020); assertive technologies for investigative journalism (Hassan et al., 2015); detection of fake reviews (Ott et al., 2011); detection of deceptive discussions (Larcker and Zakolyukina, 2012). A definition with relevance for the area rotates around the concept of \"deceptive language\". Deceptive language is defined by Communication, Linguistics and Psychology literature as a type of language deliberately used with aim of attempting to mislead others. For instance, falsehoods communicated by people who are mistaken or self-deceived are not lies, nevertheless, literal truths designed to mislead are lies as a deliberate attempt to mislead others. Besides that, most relevant literature on deception refers mainly to levels of deceit and typology of media (e.g., face-to-face, voice, text) (Zhou et al., 2003). DePaulo et al. (2003) claim that deceptive linguistic style may present weak employment of singular and third-person pronouns, negative polarity, and high employment of movement verbs. Nahari et al. (2019) suggest that a basic assumption related to deceptive language is that liars differ from truth-tellers in their verbal behavior, making it possible to classify them by inspecting their verbal accounts. Additionally, a set of linguistic behaviors may predict deception, as tones of words and kinds of preposition, conjunctions, and pronouns (Newman et al., 2003). Taking advantage of the discourse-level analysis, Galasi\u0144ki (2000) presents a pioneer study on fictional deceptive stories. According to the author, discourse analysis of deceptive texts deception is intrinsically tied with \"information manipulation\", which consists of presenting a reality that is misrepresented. The author argues that deception should be classified in three different levels: (i) falsification (i.e., attributing false statements to a debater), (ii) distortion (i.e., manipulating by understating or overstating what a debater states), and (iii) de-contextualization (i.e., taking the words a debater uses out of their context).", "filtered_refids": [["b12", "b44", "b4", "b26", "b18", "b47", "b25", "b32", "b27", "b29", "b30", "b19", "b5", "b8", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3541, "num_references": 15}
{"corpusid_sectionid": "251307817-s7", "title": "Rhetorical Structure Approach for Online Deception Detection: A Survey", "date": 2022, "section_title": "Corpora, Models and Methods", "section": "A very plausible assumption, when one opts for the discourse-aware approach applied to deception detection, would be that there are significant differences between structures of truthful and deceptive stories. Indeed, it has been proposed by various authors. While the research community currently lacks discourse annotated corpora for deception detection tasks, recent works have proposed discourse-tagged corpora for the English, Portuguese and Russian languages. Table 1 provides a summary of the discourse-tagged corpora proposed in literature. As it is shown in Table 1, the discourse-tagged corpora for the fake news and fake reviews detection tasks were proposed for the English, Russian, and Portuguese languages. As being particularly a human time-onerous task and a kind of challenging annotation process, the corpora present a small set of documents. Furthermore, both monolingual and multilingual corpora were proposed.\n\nMoving forward, as it is known from research proposals on fake news and fake reviews, a wide variety of models have been proposed to tackle online deception detection. Most of them rely on linguistic features such as n-grams, language complexity, part-of-speech tags, and syntactic and semantic features. On the other hand, discourse-level structure approach is usually framed as a supervised learning problem, which embodies in a model coherence relations followed by hierarchical nuclearity information to build automatic classifiers. In Table 2, we also summarize discourse-aware models and methods proposed in literature. Notice that models use bag-of-rst, dependency parsing, embeddings and BERT tokenizer as features, and both classical and neural machine learning have been applied. Finally, f1score performance is reported in column \"%\", except for Karimi and Tang (2019), whose authors reported values related to accuracy. \n\nBag-of-rst SVM English 63% Fake News 4.2. Fake News Detection Kuzmin et al. (2020) Fake news prediction is a global problem, and most of approaches have been developed for the English language (Kuzmin et al., 2020). Nevertheless, fake news is spread around the world, and it may be written originally in several languages. In this proposal, the authors trained and compared different models for fake news detection in Russian. They assess whether different language-based features including the vectorization of rhetorical structure obtained from both -a RST parsing and a rst manually annotated corpus -could be helpful for the fake news detection task. This proposal was implemented and evaluated using classical machine learning methods, as Support Vector Machine (SVM) and Logistic Regression (LR) over bag-of-ngrams and bag-of-rst representations. Besides that, sophisticated machine learning techniques, as BERT (Devlin et al., 2019) were also implemented. The authors used three different corpora of fake news in Russian. The first one was proposed by Pisarevskaya (2017) (see Table 1 -manually annotated). The second one was proposed by Zaynutdinova et al. (2019); it is composed of 1,366 fake news and 7,501 true news. Finally, Taiga Corpus 2 was also applied. Furthermore, three distinct representations were used (i) bag-of-ngrams with tfidf preprocessing, (ii) bag-of-rst, which consists of the vectorization of coherence relations and nuclearity, and (iii) pre-trained BERT-based model, more specifically, the RuBERT2 obtained using DeepPavlov 3 (Burtsev et al., 2018) with Transformers (Wolf et al., 2020). The authors reported that classical approaches using bagof-n-grams and bag-of-rst presented high results (90% of F1-score) overcoming the neural network approach, which uses the RuBERT ((88% of F1-score). Moreover, the authors suggest that satire is similar to fake news, and satire differs from real news. The authors also concluded that humans rarely perform better than chance at detecting deceptive activities. Therefore, humans performed worse than the best automated model.", "filtered_refids": [[], ["b15"], ["b6", "b46", "b45", "b2", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 31, "num_chars": 3967, "num_references": 6}
{"corpusid_sectionid": "251307817-s8", "title": "Rhetorical Structure Approach for Online Deception Detection: A Survey", "date": 2022, "section_title": "Karimi and Tang (2019)", "section": "Discourse-level structure analysis of deceptive and truthful news is a tremendous challenge, mainly due to existing methods for capturing discourse-level structure rely on annotated corpora, which are not available for fake news datasets (Karimi and Tang, 2019). In this proposal, the authors provide a new dependency parsing approach, titled \"Hierarchical Discourse-level Structure for Fake news detection\". The HDSF consists of an automated manner to learn a discourselevel structure for a given document through an approach based on the dependency parsing at the sentence level. It should be noted that in this approach, sentences are classified as elementary discourse units (EDU's). An example of discourse-level structure of a document (fake news) using the proposed dependency tree is shown in Figure 5. Note that a document is segmented into sentences (S1, S2, S3, S4 and S5), and hierarchically organized. Figure 5: Hierarchical discourse-level structure of a document using a dependency tree. This fake news was extracted from Politifact.\n\nThe HDSF framework build a hierarchical structure between sentences without relying on an annotated corpus, as may be seen in Figure 6. Note that the HDSF receives as input a corpus of fake/real news documents (i.e., D). A model M may automatically learn hierarchical and structurally rich representations for documents in D. Meanwhile, given binary labels Y, model M uses the hierarchical representations to automatically predict the labels of unseen news documents. In order to compare the HDSF approach with baseline and state-of-art models, the authors implemented seven different models including the proposed methods: Ngrams, LIWC (Pennebaker et al., 2015), Bag-of-rst (Rubin and Lukoianova, 2015), BiGRNN-CNN (Ren and Zhang, 2016), LSTM and LSTM[w+s] (Karimi and Tang, 2019). Based on the obtained results, the HDSF overcame the other implemented approaches (82.19% of Accuracy). They concluded that discourselevel structure analysis is effectively rich for fake news prediction. In addition, the structures of fake news documents at the discourse level are substantially different from those of true ones, and real news documents indicate more degree of textual coherence.", "filtered_refids": [["b15"], ["b15", "b34", "b31", "b35"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2230, "num_references": 5}
{"corpusid_sectionid": "252461144-s4", "title": "A Survey of Machine Translation Tasks on Nigerian Languages", "date": 2022, "section_title": "Machine Translation Techniques", "section": "Machine translation has been extensively studied for decades (Bahdanau et al., 2016;Luong et al., 2015;Koehn et al., 2007b) with neural machine translation providing the most recent state-of-the art results. There are three types of machine translation techniques that have been explored in the literature -Rule-based machine translation (RBMT), Statistical machine translation (SMT), and Neural machine translation (NMT). A high level overview of these techniques are outlined as follows:\n\nRule-based Approach: This is one of the oldest form of machine translation technique used. This approach is based on understanding the linguistic properties of the source and target languages using dictionaries and expert knowledge to define grammar rules. This process involves morphology analysis, syntax, and lexical semantics. Linguistic analysis is performed on the source language to identify morphology, parts of speech, phrases, named entity, and word disambiguation. Each word is replaced in the target language using a dictionary which represents mappings between source and target words. In order to preserve sentence semantics across translated languages, most RBMT approach utilizes a combination of finite state machines to develop their knowledge graphs (Forcada et al., 2011;Scott and Barreiro, 2009). (Forcada et al., 2011) utilizes finite-state transducers for lexical processing, Hidden Markov models for part-ofspeech tagging, and multi-stage finite-state chunking for structural transfer. (Eisele et al., 2008) utilizes a modified phrase table with entries from translating various data with rule-based systems. One of the main advantage of this approach is that it does not require as much parallel sentence pairs as with most NMT approaches. Also, translation errors can be corrected by updating the dictionary. This allows for flexibility in updating language constructs. Consequently, one major drawback of this approach is that the translation quality is mostly defined by the strength of the dictionary which requires frequent updates from domain experts. RBMT also tends to produce translations that are more repetitive and less fluid which can be attributed to its mechanical approach of using rules for translation.\n\nStatistical-based Approach: This approach involves the use of statistical techniques such as probability distribution models to provide a means for machine translation between source and target languages. This is achieved by assigning a probability score to word or phrase contained in every target sentence where words or phrases with the highest probability contains the best translation for the target sentence (Koehn et al., 2007b;Brown et al., 1993). SMT can be applied at a word or phrase level and consists of a translation and language model. The translation model is defined as the probability that the source sentence is the translated version of the target word. The language model tries to describe how representative the target sentence is to the natural spoken language. It assigns probabilities to sentence similar to the sentence ordering. One approach utilized in developing the probability distributions is the use of Bayes theorem (Zens et al., 2002) and Hidden Markov Model (Deng and Byrne, 2008;Alkhouli et al., 2016). (Koehn et al., 2007a) developed Moses, an open-source machine translation toolkit which utilizes linguistic information that captures semantics in mapping text phrases and a confusion network decoding for translating ambiguous text inputs.\n\nOne advantage of SMT approach over RBMT is the improved translation quality. It allows for translation that captures not just linguistic morphology but the use of a probability distribution which improves with semantic quality.\n\nNeural-based Approach: This approach is referred to as the state of the art in machine translation as it is widely used and has shown to provide results with higher accuracy as compared to the other approaches (Bahdanau et al., 2016;Luong et al., 2015;Cho et al., 2014). Neural machine translation involves the use of deep learning techniques to provide a means of inferring high level semantics from language translations. A popular neural machine translation approach (Vaswani et al., 2017) utilize transformer based models with encoder-decoder architecture. These models consists of stacks of multiple hidden layers with multi-head attention mechanisms and have been shown (Vaswani et al., 2017) to outperform traditional neural architecture such as Recurrent Neural Networks for machine translation task.\n\nCurrent implementation for language models consists of multilingual language model embeddings (Pires et al., 2019;Lample and Conneau, 2019) where one language model is trained on multiple languages. This allows for zero-shot transfer learning where cross language representation is learned without the need for a parallel language corpus across all language pairs. This has been shown to produce better results than monolingual model training (Conneau et al., 2020) especially for low-resource languages. Supervised neural approach relies heavily on a large corpus of quality translated sentence pairs; as such this poses a limitation to the quality of language translation. There are some approaches that work well with limited datasets (Mikolov et al., 2013;Artetxe et al., 2018) and can provide a means of translating from one language to another based on translations derived from a similar language ( ", "filtered_refids": [["b10", "b34", "b32"], ["b24", "b48", "b20"], ["b6", "b58", "b14", "b31", "b32", "b19"], [], ["b53", "b10", "b34", "b17"], ["b18", "b47", "b33", "b8", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 33, "num_chars": 5464, "num_references": 21}
{"corpusid_sectionid": "252461144-s5", "title": "A Survey of Machine Translation Tasks on Nigerian Languages", "date": 2022, "section_title": "State of the Art", "section": "There has been a limited number of work centered on machine translation of Nigerian languages. Most of the cutting edge research on machine translation utilizes neural machine translation approaches (Stahlberg, 2020). However, most of the machine translation work on Nigerian languages focuses on rule-based approach using context free grammars while a few focuses on neural machine translation techniques such as transformer-based models. We outline the work that has been conducted over the years and categorize each work based on the different approach utilized.\n\n3.1. Rule-based Approach (Ayegba et al., 2014) utilizes a rule-based approach for machine translation of English to Igala language. This approach utilizes noun phrases from English language while performing a series of processes such as parts of speech tagging, morphological analysis which analyzes words based on its root or base form, and comparing noun phrases to components contained in a bilingual dictionary. Their approach was tested on 120 randomly selected English noun phrases and achieves a Bilingual Evaluation Understudy (BLEU) accuracy of 90.9%. (Akinwale et al., 2015) proposed a web-based English to Yoruba translation model utilizing a similar approach as (Ayegba et al., 2014). The translator component utilizes a set of twenty rules which were specified using context free grammar. This approach achieves an accuracy of 90.5%. (Eludiora and Ajibade, 2021) proposed a rule-based model for English to Yoruba translation of Yoruba verbs based on tone changing. It is their intuition that some Yoruba verbs change tone in the bilingual dictionary from low-tone to mid-tone which sometimes changes the meaning of the sentence. Their approach is implemented using 20 tone changing verbs. They evaluate the efficacy of their approach by performing language expert evaluation which entails comparing the output derived from their model with the output generated from Google translation. According to the authors, this approach is very time-consuming but very extensive. In addition, they evaluate their approach using human evaluators. In a total of 70 respondents, 69% of the respondents agree that their system correctly translates verb-phrases while 29% of the respondents agrees that Google translation works efficiently. (Ezeani et al., 2016) developed a model using the Igbo Bible corpus to detect and restore missing didactics in texts at word level toknization. Their approach on didatic replacement utilizes work conducted by (Simard, 1998) which consists of using Hidden Markov Model in which the input text is viewed as a stochastic process. (Onyenwe et al., 2019) develops a parts of speech (POS) tagger for Igbo language. Their approach utilizes a host of post tagging approach including Hidden Markov Model. They achieve an accuracy of of 93.17% to 98.11% on the overall words, and 7.13% to 83.95% on unknown words. (Orife, 2020) developed a neural machine translation model for translating Edoid languages to English. Edoid languages are primarily spoken by the southern Nigeria (Edo and Delta states) consisting of Edo, Esan, Urhobo, and Isoko languages. They utilize transformer models with encoder decoder and multi-head self attention. To evaluate the effectiveness of their approach, they trained their model using JW300 dataset (Agi\u0107 and Vuli\u0107, 2019) consisting of over 100 African languages The training was conducted using tokenization processeses such as Byte-pair encoding (BPE) and word-level tokenization . The results shows that Urhobo and Isoko consists of larger training dataset performed best with higher BLEU scores. BPE tokenization provided a 37% boost for the development and test dataset of Edo and Esan languages and a 32% boost for Urhobo language. However, BPE produced worse results when compared to word-level tokenization for Isoko languages. (Ahia and Ogueji, 2020) developed supervised and unsupervised neural machine translation models to serve as a baseline for future works to come in the translation of Nigerian pidgin. For their approach, they utilized a transformer architecture proposed by (Vaswani et al., 2017) while experimenting with word-level and Byte-Pair encoding subword tokenization. The supervised approach produced a BLEU score of 17.73 while the unsupervised model produced a BLEU score of 5.18 for English to Pidgin Translation. (Nguyen and Chiang, 2018) developed a model that improves the mistranslation of rare words. This approach is based on a modified version of attention based encoderdecoder models. Their approach hones on the premise that the output layer which consists of the inner product of the context vector and all possible word embeddings improperly rewards frequently occurring words. In their approach, instead of using the dot product, the norm vectors are set to a constant value. In addition, they include new terms which provides direct connection from source sentence and this makes the model properly memorize rare word translations. They evaluate their approach on 8 language pairs which includes Hausa to English language pair. (Hedderich et al., 2020) demonstrates that a transfer learning approach through multilingual transformer models (mBERT and XLM-RoBERTa) can be utilized for tasks such as name entity recognition and topic classification on low-resource languages. The approach involves fine-tuning the target language dataset on high-resource language models. Their approach is evaluated on three African languages Hausa, isiXhosa and Yoruba out of which two of the languages (Hausa, and Yoruba) are Nigerian languages. They produce results comparable to the state-of-the-art with as little as 10 or 100 labelled sentences. They achieve at least an improvement of 10 points in the F-1 score for a shared label of named entity recognition. Their result shows promise and is consistent with their hypothesis which also validates work shown in prior research. Their approach however does not produce good results for topic classification. This might be as a result of mismatch in the label set. (Ogueji et al., 2021) developed AfriBERTa, an approach which involves training multilingual models on low-resource language. According to the authors, it is a general assumption that low-resource multilingual language models benefit from being trained in combination with high-resource languages. low-resource multilingual models do not need to be trained in combination with highresource languages and does not require as much dataset used for training high-resource languages. The authors accomplish multilingual model training on low-resource languages with a dataset consisting of 11 African languages of which Igbo, Yoruba, Hausa, and Nigerian Pidgin are Nigerian languages. They also show that the state of the art accuracy can be achieved with training on less than 1GB of data. Furthermore, they apply their pre-trained transformer model on downstream tasks such as name entity recognition and text classification task. Their model outperforms the state of the art multilingual models such as mBERT and XLM-R.", "filtered_refids": [["b50"], ["b44", "b4", "b46", "b9", "b28", "b41", "b49", "b21", "b45", "b22", "b53", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 53, "num_chars": 7092, "num_references": 13}
{"corpusid_sectionid": "252461144-s8", "title": "A Survey of Machine Translation Tasks on Nigerian Languages", "date": 2022, "section_title": "Data Acquisition", "section": "One of the major impediments to corpus-based machine translation of low-resource languages is the quality and quantity of the dataset utilized for model training. However, there has been limited work in generating datasets for machine translation tasks of Nigerian languages. Some of the prominent dataset utilized for this task are outlined below.  developed an open-source dataset of Yoruba speech which consists of over four hours of recordings from 36 male and female volunteers with transcription and disfluency annotation. (Adelani et al., 2021) developed a publicly available parallel corpus known as MENYO-20K which consists of a parallel corpus of texts in English-Yoruba language with over 20,000 sentences obtained from news articles, TED talks, movie and radio transcripts, science and technology texts and short articles from the web which were annotated by professional translators with proficiency in Yoruba language. (Butryna et al., 2020) developed a crowd-sourced speech corpus for low-resource languages which consists of languages in South and Southeast Asia, Africa (South Africa, and Nigeria), Europe and South America. The only Nigerian language supported was Nigerian Pidgin. They achieve this task by partnering with local communities and universities in the region. (Agi\u0107 and Vuli\u0107, 2019) introduces JW300, a parallel corpus of over 300 languages containing around over 100,000 sentences per language pair. The corpus consists of a total of 1,335,376 articles with over 109 million sentences and 1.48 billion tokens. They achieve this by crawling publications from jw.org. OPUS (Tiedemann, 2012), is one of the largest open source parallel corpora repository of translated text. It consists of over 90 languages with a total of 3,800 language pairs comprising of over 40 billion tokens in 2.7 billion parallel units. (Goyal et al., 2021) introduces Flores-101, an open-source benchmark for evaluating lowresource multilingual machine translation task. This dataset consists of 3,000 sentences extracted from Wikipeadia. In addition, the sentences have been converted into 101 languages which includes three major languages in Nigeria (Igbo, Yoruba, and Hausa). (Ezeani et al., 2020) developed a publicly available standard evaluation benchmark dataset for Igbo to English machine translation. This includes over 10,000 high quality English to Igbo sentence pairs which were derived mostly from news (BBC Igbo 2 and PUNCH newspaper 3 ) domains.", "filtered_refids": [["b15", "b25", "b23", "b1", "b51", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2469, "num_references": 6}
{"corpusid_sectionid": "246017121-s16", "title": "Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset", "date": "2022-01-07", "section_title": "Implementation Details", "section": "Data pre-processing. We implement spectral augmentation (SpecAugment), a state-of-the-art audio data augmentation method, which is implemented by masking certain frequency and time values on the spectrogram (Park et al., 2019). We use SpecAugment for the Common Voice zh-HK baseline, where it shows an improvement in overall results. Furthermore, we apply cepstral mean and variance normalisation (CMVN) for all the utterances (Strand and Egeberg, 2004). In Fairseq S2T, pre-processed audio can be used directly or stored in the form of .npy files. The latter is the way in which we store features extracted from Cantonese datasets to achieve faster training. For tokenization of the transcribed data, we use the SentencePiece tokenizer (Kudo and Richardson, 2018) with unigram subword tokenization (Kudo, 2018) and an 8,000-word vocabulary. The vocabulary covers 99.95% of the characters in the MDCC (the default coverage for character-based languages).", "filtered_refids": [["b11", "b21", "b22", "b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 954, "num_references": 4}
{"corpusid_sectionid": "14642384-s3", "title": "DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL", "date": "2004-07-01", "section_title": "Descriptions serving as disguised PROPER", "section": "NAMES, such as The Federal Communications Commission or the Iran-Iraq war. The heuristics for recognizing these definite descriptions were primarily based on capitalization (of the head or the modifiers).\n\n3. PREDICATIVE descriptions, i.e., descriptions semantically functioning as predicates rather than as referring. These include descriptions occurring in appositive position (as in Glenn Cox, the president of Phillips Petroleum) and in certain copular constructions (as in the man most likely to gain custody of all this is a career politician named Dinkins). The heuristics used to recognize these cases examined the syntactic structure of the NP and the clause in which it appeared.\n\n4. Descriptions ESTABLISHED (i.e., turned into functions in context) by restrictive modification, particularly by establishing relative clauses (Loebner, 1987) and prepositional phrases, as in The hotel where we stayed last night was pretty good.\n\nThese heuristics, as well, examined the syntactic structure of the NP. 5. LARGER SITUATION definite descriptions (Hawkins, 1978), i.e., definite descriptions like the sun, the pope or the long distance market which denote uniquely on the grounds of shared knowledge about the situation (these are Loebner's 'situational functions'). Vieira and Poesio's system had a small list of such definites.\n\nThese heuristics were included as tests both of a decision tree concerned only with the task of DN detection, and of decision trees determining the classification of DDs as anaphoric, bridging or discourse new. In both cases, the DN detection tests were intertwined with attempts to identify an antecedent for such DDs. Both hand-coded decision trees and automatically acquired ones (trained using ID3, (Quin-lan, 1986)) were used for the task of two-way classification into discourse-new and anaphoric. Vieira and Poesio found only small differences in the order of tests in the two decision trees, and small differences in performance. The hand-coded decision tree executes in the following order:\n\n1. Try the DN heuristics with the highest accuracy (recognition of some types of semantically functional DDs using special predicates, and of potentially predicative DDs occurring in appositions);\n\n2. Otherwise, attempt to resolve the DD as direct anaphora;\n\n3. Otherwise, attempt the remaining DN heuristics in the order: proper names, descriptions established by relatives and PPs, proper name modification, predicative DDs occurring in copular constructions.\n\nIf none of these tests succeeds, the algorithm can either leave the DD unclassified, or classify it as DN. The automatically learned decision tree attempts direct anaphora resolution first. The overall results on the 195 DDs on which the automatically trained decision tree was tested are shown in  ", "filtered_refids": [[], [], ["b4"], ["b2"], [null], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2799, "num_references": 3}
{"corpusid_sectionid": "14642384-s5", "title": "DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL", "date": "2004-07-01", "section_title": "Ng and Cardie", "section": "The discourse-new detectors proposed by Ng and Cardie are statistical classifiers taking as input 37 features and trained using either C4.5 (Quinlan, 1993) or RIPPER (Cohen, 1995). The 37 features of a candidate anaphoric expression specify, in addition to much of the information proposed in previous work, a few new types of information about NPs.\n\n\u2022 The four boolean so-called LEXICAL features are actually string-level features: for example, str_match is Y if a preceding NP string-matches the anaphoric expression (except for the determiner), and head_match = Y if a preceding NP's head string-matches the anaphoric expression's. embedded=Y if the anaphoric expression is a prenominal modifier.\n\n\u2022 The second group of 11 (mostly boolean) features specifies the type of NP: e.g., pronoun is Y if the anaphoric expression is a pronoun, else N.\n\n\u2022 The third group of 7 features specifies syntactic properties of the anaphoric expression, including number, whether NP j is the first of two NPs in an appositive or predicative construction, whether NP j is pre-or post-modified, whether it contains a proper noun, and whether it is modified by a superlative.\n\n\u2022 The next group of 8 features are mostly novel, and capture information not used by previous DN detectors about the exact composition of definite descriptions: e.g., the_2n=Y if the anaphoric expression starts with determiner the followed by exactly two common nouns, the_num_n=Y if the anaphoric expression starts with determiner the followed by a cardinal and a common noun, and the_sing_n=Y if the anaphoric expression starts with determiner the followed by a singular NP not containing a proper noun.\n\n\u2022 The next group of features consists of 4 features capturing a variety of 'semantic' information, including whether a previous NP is an 'alias' of NP j , or whether NP j is the title of a person (the president).\n\n\u2022 Finally, the last three features capture information about the position in the text in which NP j occurs: the header, the first sentence, or the first paragraph.\n\nNg and Cardie's discourse-new predictor was trained and tested over the MUC-6 and MUC-7 coreference data sets, achieving accuracies of 86.1% and 84%, respectively, against a baseline of 63.8% and 73.2%, respectively. Inspection of the top parts of the decision tree produced with the MUC-6 suggests that head_match is the most important feature, followed by the features specifying NP type, the alias feature, and the features specifying the structure of definite descriptions.\n\nNg and Cardie discuss two architectures for the integration of a DN detector in a coreference system. In the first architecture, the DN detector is run first, and the coreference resolution algorithm is run only if the DN detector classifies that NP as anaphoric. In the second architecture, the system first computes str_match and alias, and runs the anaphoric resolver if any of them is Y; otherwise, it proceeds as in the first architecture. The results obtained on the MUC-6 data with the baseline anaphoric resolver, the anaphoric resolver augmented by a DN detector as in the first architecture, and as in the second architecture (using C4.5), are shown in Table 3. The results for all NPs, pronouns only, proper names only, and common nouns only are shown. 2 As indicated in the Table, running the DN detector first leads to worse results-this is because the detector misclassifies a number of anaphoric NPs as nonanaphoric. However, looking first for a same-head antecedent leads to a statistically significant improvement over the results of the baseline anaphoric resolver. This confirms the finding both of Vieira and Poesio and of Bean and Riloff that the direct anaphora should be called very early.\n\n2 It's not clear to us why the overall performance of the algorithm is much better than the performance on the three individual types of anaphoric expressions considered-i.e., which other anaphoric expressions are handled by the coreference resolver.  Table 3: Evaluation of the three anaphoric resolvers discussed by Ng and Cardie. Uryupina (2003) trained two separate classifiers (using RIPPER, (Cohen, 1995)): a DN detector and a UNIQUENESS DETECTOR, i.e., a classifier that determines whether an NP refers to a unique object. This is useful to identify proper names (like 1998, or the United States of America), semantic definites (like the chairman of Microsoft) and larger situation definite descriptions (like the pope). Both classifiers use the same set of 32 features. The features of an NP encode, first, of all, string-level information: e.g., whether the NP contains capitalized words, digits, or special symbols. A second group of features specifies syntactic information: whether the NP is postmodified, and whether it contains an apposition. Two types of appositions are distinguished, with and without commas. CONTEXT features specify the distance between the NP and the previous NP with the same head, if any. Finally, Uryupina's system computes four features specifying the NP's definite probability. Unlike the definite probability used by Bean and Riloff, these features are computed from the Web, using Altavista. From each NP, its head H and entire NP without determiner Y are determined, and four ratios are then computed:", "filtered_refids": [["b15", "b1"], [], [], [], [], [], [], [], [null], ["b1", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 5284, "num_references": 5}
{"corpusid_sectionid": "14642384-s9", "title": "DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL", "date": "2004-07-01", "section_title": "How much does DN-detection help the Vieira / Poesio algorithm?", "section": "GUITAR (Poesio and Alexandrov-Kabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov's algorithm for pronoun resolution (Mitkov, 1998). It is implemented in Java, takes its input in XML format and returns as output its input augmented with the anaphoric relations it has discovered. GUITAR has been implemented in such a way as to be fully modular, making it possible, for example, to replace the DD resolution method with alternative implementations. It includes a pre-processor incorporating a chunker so that it can run over both hand-parsed and raw text. A version of GUITAR without the DN detection aspects of the Vieira / Poesio algorithm was evaluated on the GNOME corpus (Poesio, 2000;, which contains 554 definite descriptions, of which 180 anaphoric, and 305 third-person pronouns, of which 217 anaphoric. The results for definite descriptions over hand-parsed text are shown in Table 6   Notice that although these results are not particularly good, they are still better than the results reported by Ng and Cardie for pronouns and definite NPs.", "filtered_refids": [["b6", "b10", "b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1162, "num_references": 3}
{"corpusid_sectionid": "237353268-s3", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Visualization", "section": "A simple way to discover the role of a neuron is by visualizing its activations and manually identifying the underlying concept over a set of sentences (Karpathy et al., 2015;Fyshe et al., 2015;Li et al., 2016a). Given that deep NLP models are \u2020 Table 3 in Appendix gives a more comprehensive list. trained using billions of neurons, it is impossible to visualize all the neurons. A number of clues have been used to shortlist the neurons for visualization, for example, selecting saturated neurons, high/low variance neurons, or ignoring dead neurons (Karpathy et al., 2015) when using ReLU activation function. \u2021 Limitation While visualization is a simple approach to find an explanation for a neuron, it has some major limitations: i) it is qualitative and subjective, ii) it cannot be scaled to the entire network due to an extensive human-in-the-loop effort, iii) it is difficult to interpret polysemous neurons that acquire multiple roles in different contexts, iv) it is ineffective in identifying group neurons, and lastly and v) not all neurons are visually interpretable. Visualization nevertheless remains a useful tool when applied in combination to other interpretation methods that are discussed below.", "filtered_refids": [["b11", "b22", "b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1216, "num_references": 3}
{"corpusid_sectionid": "237353268-s7", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Linear Classifiers", "section": "The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.\n\nLimitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.\n\nGaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.\n\nLimitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.", "filtered_refids": [["b21", "b38"], ["b15", "b55"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2535, "num_references": 4}
{"corpusid_sectionid": "237353268-s8", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Causation-based methods", "section": "The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.\n\nAblation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.\n\nLimitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.\n\nKnowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.\n\nLimitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.", "filtered_refids": [[], ["b21", "b22"], [null, "b9", "b56"], ["b26", "b46", "b49", null, "b1"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2745, "num_references": 11}
{"corpusid_sectionid": "263835243-s3", "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances", "date": "2023-10-11", "section_title": "Knowledge Editing", "section": "Since tuning LLMs to learn new knowledge can be prohibitively expensive (Patterson et al., 2021), researchers seek efficient methods to directly update more specific, localized, or fine-grained knowledge that is preserved in LLMs (Mitchell et al., 2022a). Knowledge editing (KE) is an arising and promising research area that aims to alter the parameters of some specific knowledge stored in pre-trained models so that the model can make new predictions on those revised instances while keeping other irrelevant knowledge unchanged (Sinitsin et al., 2020;De Cao et al., 2021;Mitchell et al., 2022a;Meng et al., 2022a;Hase et al., 2023b;.\n\nIn this section, we categorize existing methods into meta-learning, hypernetwork, and locate-and-edit -based methods.\n\nMeta-learning. This line of work generally focuses on the intrinsic editability of the model itself, aiming to modify the model parameters so that they can be easily updated during inference (De Cao et al., 2021;Mitchell et al., 2022a  concept and propose a gradient-based knowledge attribution method to identify these knowledge neurons in FFNs. Further, without fine-tuning, they directly modify the corresponding value slots (e.g., embeddings) in the located knowledge neurons and successfully update or delete knowledge, demonstrating a preliminary potential to edit knowledge in LMs. Other.  propose an evaluation framework and dataset for measuring the effectiveness of knowledge editing of LLMs, as well as the ability to reason with the altered knowledge and cross-lingual knowledge transfer. Similarly, Cohen et al. (2023) evaluate the implications of an edit on related facts and show that existing methods fail to introduce consistent changes in the model's knowledge. Ju and Zhang (2023) propose an evaluation benchmark for locate-and-edit-based methods, aiming to reassess the validity of the locality hypothesis of factual knowledge.  and  take multilingual into account and extend existing knowledge editing methods into cross-lingual scenarios.", "filtered_refids": [[null, "b31"], [], [null, "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 2018, "num_references": 4}
{"corpusid_sectionid": "263835243-s4", "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances", "date": "2023-10-11", "section_title": "Continual Learning", "section": "Continual learning (CL) aims to enable a model to learn from a continuous data stream across time while reducing catastrophic forgetting of previously acquired knowledge (Biesialska et al., 2020). With CL, a deployed LLM has the potential to adapt to the changing world without costly re-training from scratch (Bubeck et al., 2023). In this section, we introduce approaches that employ CL for aligning LLMs with the current world knowledge, including continual pre-training and continual knowledge editing.\n\nContinual Pre-training. Unlike traditional continual learning, which sequentially fine-tunes a pre-trained LM on some specific downstream tasks (e.g., QA, text classification), continual pretraining is used to further pre-train an LM to acquire new knowledge, where the data corpus is usually unsupervised (Gururangan et al., 2020;Ke and Liu, 2023). Since our target is the versatile foundation LLMs (e.g., GPT-4) that can be applied to many different use cases rather than a fine-tuned model designed for a specific task, we focus on the literature on continual pre-training.\n\nEarly works (Gururangan et al., 2020;R\u00f6ttger and Pierrehumbert, 2021;Lazaridou et al., 2021;Dhingra et al., 2022)  1 Regularization. To mitigate forgetting, regularization-based methods apply regulations to penalize the changes of the critical parameters learned from previous data. Chen et al. (2020) improve the traditional EWC (Kirkpatrick et al., 2017) by recalling previously learned knowledge through the pre-trained parameters, and the method continually learns new information using a multitask learning objective.  compute the importance of each unit (i.e., attention head and neuron) to the general knowledge in the LM using a proxy based on model robustness to preserve learned knowledge. When continually learning new domains, the approach prevents catastrophic forgetting of the general and domain knowledge and encourages knowledge transfer via soft-masking and contrastive loss.\n\n2 Replay. These methods generally reduce forgetting by replaying previous training data when learning new data. Assuming that the initial pretraining corpus is available, He et al. (2021b) use a gradual decay mix-ratio to adjust the quantity of the pre-training corpus mixed in the new data when learning sequentially. ELLE  and CT0 (Scialom et al., 2022) also mix the old data while learning new data. However, ELLE starts the pre-training from a newly initialized and relatively small BERT ( ", "filtered_refids": [[null], [null], [null, "b34", "b27"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2475, "num_references": 6}
{"corpusid_sectionid": "264490542-s3", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Paper Databases", "section": "We source papers from eminent databases in the fields of NLP, the rest of CS, and medicine, as these are integral knowledge areas in the study of mental health CA.These databases include the ACL Anthology (referred to as ACL throughout this paper)2 , AAAI3 , IEEE4 , ACM5 , and PubMed6 .ACL is recognized as a leading repository that highlights pioneering research in NLP.AAAI features cuttingedge studies in AI.IEEE, a leading community, embodies the forefront of engineering and technology research.ACM represents the latest trends in Human Computer Interaction (HCI) along with several other domains of CS.PubMed, the largest search engine for science and biomedical topics including psychology, psychiatry, and informatics among others provides extensive coverage of the medical spectrum.\n\nDrawing on insights from prior literature reviews (Valizadeh and Parde, 2022;Montenegro et al., 2019;Laranjo et al., 2018) and discussion with experts from both the CS and medical domains, we opt for a combination of specific keywords.These search terms represent both our areas of focus: conversational agents (\"conversational agent\", \"chatbot\") and mental health (\"mental health\", \"depression\").Furthermore, we limit our search criteria to the paper between 2017 to 2022 to cover the most recent articles.We also apply the \"research article\" filter on ACM search, and \"Free Full Text or Full Text\" for PubMed search.Moreover, we manually add 3 papers recommended by the domain experts (Fitzpatrick et al., 2017;Laranjo et al., 2018;Montenegro et al., 2019).This results in 534 papers.", "filtered_refids": [[], ["b69", "b132", "b44", "b88"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1580, "num_references": 4}
{"corpusid_sectionid": "264490542-s8", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Target Demographic", "section": "Most of the papers (>65%) do not specify the target demographic of users for their CAs.The target demographic distribution is shown in Table 4.An advantage of the models proposed in these papers is that they could potentially offer support to a broad group of users irrespective of the underlying mental health condition.Papers without a target demographic and a target mental health category focus on proposing methods such as using generative language models for psychotherapy (Das et al., 2022a), or to address specific modules of the CAs such as leveraging reinforcement learning for response generation (Saha et al., 2022b).\n\nOn the other hand, 31% papers focus on one specific user group such as young individuals, students, women, older adults, etc, to give advanced assistance.Young individuals, including adolescents and teenagers, received the maximum attention (Rahman et al., 2021).Several papers also focus on the mental health care of women, for instance in prenatal and postpartum women (Green et al., 2019;Chung et al., 2021) and sexual abuse survivors (Maeng and Lee, 2022;Park and Lee, 2021).Papers targeting older adults are mainly designed for companionship and supporting isolated elders (Sidner et al., 2018;Razavi et al., 2022).", "filtered_refids": [["b114", "b29"], ["b79", "b21", null, "b123", "b110", "b106"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1251, "num_references": 8}
{"corpusid_sectionid": "264490542-s9", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Model Technique", "section": "Development of Large Language Models such as GPT-series (Radford et al., 2019;Brown et al., 2020) greatly enhanced the performance of generative models, which in turn made a significant impact on the development of CAs (Das et al., 2022b;Nie et al., 2022).However, as shown in Table 5, LLMs are yet to be utilized in the development of mental health CAs (as of the papers reviewed in this study), especially in medicine.No paper from PubMed in our final list dealt with generative models, with the primary focus being rule-based and retrieval-based CAs.\n\nRule-based models operate on predefined rules and patterns such as if-then statements or decision trees to match user inputs with predefined responses.The execution of Rule-based CAs can be straightforward and inexpensive, but developing and maintaining a comprehensive set of rules can be challenging.Retrieval-based models rely on a predefined database of responses to generate replies.They use techniques like keyword matching (Daley et al., 2020), similarity measures (Collins et al., 2022), or information retrieval (Morris et al., 2018)  and Eliza (Weizenbaum, 1966).\n\ntures.While they can often generate more diverse and contextually relevant responses compared to rule-based or retrieval-based models, they could suffer from hallucination and inaccuracies (Azaria and Mitchell, 2023).", "filtered_refids": [["b105", "b30", "b16", "b93"], ["b22", "b139", "b26", "b89"], ["b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1347, "num_references": 9}
{"corpusid_sectionid": "264305746-s5", "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis", "date": "2023-10-18", "section_title": "Affect or Feeling", "section": "Author's disposition towards a specific theme (Birjali et al., 2021) 3-D polarity Framework with 3 dimensions of polarities: Subjective\\Objective, Positive\\Negative, Strength (Sebastiani and Esuli, 2006) Emoticons Emoticons as sentiment indicators (Lou et al., 2020) Object's orientation Measure of the attitude towards individual aspects of an entity (Mowlaei et al., 2020) Implicit Emotional tendencies implied by commonsense knowledge of the effect of concepts or events (Zhang and Liu, 2011) Human Annotation Sentiment ratings collected from experts or crowd-sourced data collection (Kenyon-Dean et al., 2018) Table 2: Frameworks of Sentiment and corresponding definitions in Sentiment Analysis (2002) experimented with using the semantic orientation of words to find whether product reviews are positive or negative.Readily available data in the form of product reviews on e-commerce websites influenced early SA works and firmly established it to almost exclusively mean opinion mining, with sentiment defined as: 'overall opinion towards the subject matter' (Pang et al., 2002).Following this, Read (2005) proposed the use of emoticons as a proxy for ground truth data to measure sentiment in text.They defined SA as the method to 'identify a piece of text according to its author's general feeling toward their subject, be it positive or negative.'This marked a stark deviation of SA from 'opinion mining.'This expansion of the meaning of sentiment can also be seen in the work of Wilson et al. (2005b) where they defined SA as 'the task of identifying positive and negative opinions, emotions, and evaluations '. Subsequently, Sebastiani and Esuli (2006) proposed that SA consists of three dimensions: subjective-objective polarity, positive-negative polarity, and strength of polarity.\n\nThe first use of SA as a sociotechnical system is marked by Go et al. (2009)'s approach to train a SA model using data from a social media platform, namely Twitter.While most prior work still treated SA as a method to extract an author's subjective or objective opinion regarding an entity or an object, Go et al. (2009) defined sentiment from the perspective of a general feeling or emotion in text.Their definition of sentiment as 'a personal positive or negative feeling or opinion', is a marked deviation that influenced much of the literature in SA.Maas et al. (2011)'s work recognized sentiment as a 'complex, multi-dimensional concept' and attempted to operationalize it through a vector representation.Similarly, Zhang and Liu (2011) defined sentiment as an 'emotional tendency im-plied by commonsense knowledge of the effect of concepts or events' to define an implicit form of sentiment.To quantify sentiment from a 'human perspective', Kenyon-Dean et al. (2018) used human annotation, as a methodology to define and measure sentiment, using crowd-sourced data.\n\nTable 2 tabulates the multifarious frameworks encountered in SA.Here we see that SA does not follow a well-defined comprehensive framework.With the evolution of the field, different researchers adapted SA in dissimilar ways while not making a clear distinction between concepts such as emotions, opinions, and attitudes.We posit that there is a need for a nuanced, socially informed, and theoretically motivated framework for sentiment in SA.\n\nTo understand sentiment from an interdisciplinary perspective and draw out an interdisciplinary framework, we examine its meaning from a sociological perspective.", "filtered_refids": [["b115", "b140", "b92", "b107", "b179", "b258", "b80", null, "b125", "b169"], ["b80", "b54", "b179", "b234"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3476, "num_references": 15}
{"corpusid_sectionid": "264305746-s6", "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis", "date": "2023-10-18", "section_title": "The Social Perception of Sentiment", "section": "A notable distinction exists between computational and psycho-linguistic perspectives on sentiment.In psychology, sentiment is often defined as \"socially constructed patterns of sensations, expressive gestures, and cultural meanings organized around a relationship to a social object, usually another person or group such as a family.\"(Gordon, 1981).While sentiment is most commonly categorized as positive, negative, or neutral in computational literature, it encompasses a broader spectrum, ranging from mild to intense (Taboada, 2016;Jo et al., 2017).Furthermore, sentiment (in psychology) is captured through physiological indicators, like facial expressions and heart rate variability (Wiebe et al., 2005;Plutchik, 2001).\n\nPsychological research widely recognizes that a simplistic positive-negative dichotomy is inade-quate for capturing the intricate range of human emotions (Hoffmann, 2018).This is evident in the distinction between seemingly negative emotions such as sadness and fear, which exhibit significant differences in their physiological and psychological effects (Plutchik, 2001).\n\nWe have seen that three primary and interrelated themes are commonly linked to sentiment: opinions, emotions/feelings, and subjectivity.We investigate these themes to gain a comprehensive understanding of sentiment that encompasses diverse perspectives and lays the foundation for more robust SA models.\n\nOpinions: From a psychological perspective, opinion is an individual's stance regarding an object or issue, formed after an evaluation through their own lens or perspective (Vaidis and Bran, 2019).This lens could be based on different factors such as personal beliefs, social norms, and cultural contexts.Liu (2012) also define an opinion a \"a subjective statement, view, attitude, emotion, or appraisal about an entity or an aspect of an entity from an opinion holder.\"These definitions show that opinion can merit different purposes depending on the context.\n\nFeelings/Emotions: Izard (2010) posit that the word emotion has both a descriptive definition i.e. based on its use in everyday life and a prescriptive definition i.e. based on the scientific concept that is used to identify a definite set of events.Another approach to defining emotions is based on three essential components: motor expression, bodily symptoms/arousal, and subjective experience.There is substantial agreement that motivational consequences and action tendencies associated with emotion are key aspects of emotion rather than just the level of arousal of the subject (Frijda et al., 1986;Frijda, 1987).\n\nSubjectivity: Banfield (2014) referred to sentences that take a character's psychological point of view as subjective, contrasted against sentences that narrate an event in a definite but yielding manner.Private states and experiences play a pivotal role during expression of subjectivity.Here private states could refer to intellectual factors, such as believing, wondering, knowing; or emotive factors, such as hating, being afraid; and perceptual ones, such as seeing or hearing something (Wiebe, 1994).Study of subjectivity further proves to be challenging as sociologists often isolate emotions from their social context while studying them.\n\nTerms like opinion, emotion, and subjectivity hold distinct meanings and are studied separately.Therefore, they are not synonymous with sentiment.Furthermore, when considering sentiment within a sociotechnical system, it is essential to be aware of the contextual nuances associated with the diverse definitions of sentiment derived from sociological, psychological, and linguistic backgrounds.Given the complex nature of sentiment, it is important to approach it with a nuanced perspective and operationalize it within a structured theoretical framework.Prior research suggests that achieving such nuanced understanding can be facilitated through engaging in dialogue with other fields such as psychology, and cognitive science (Head et al., 2015;Cambria et al., 2022).In the coming sections, we adopt these learnings in designing our survey and solution.", "filtered_refids": [["b57", "b78", "b149", "b235", "b117"], ["b65", "b117"], [], ["b155", null], ["b48", "b47"], ["b167"], ["b64", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 4095, "num_references": 14}
{"corpusid_sectionid": "264305746-s14", "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis", "date": "2023-10-18", "section_title": "Finance", "section": "Applications developed to comprehend the patterns and dynamics of financial management, creation, and investment analysis.\n\nTable 3: List of applications, defined through thematic analysis, their corresponding definitions, and frequency of papers categorized to the groups.\n\nanalysis (Vaismoradi et al., 2013) to uncover the various applications of SA.Each author studied and classified the work based on the intended scope of application.To ensure accuracy and prevent misclassification, this recursive process was employed.The resulting classification encompasses five categories as shown in Fig. 2 and Table 3 1 .Notably, the Health and Medicine domain emerged as the most prominent application area for SA where studies leverage SA to understand individual reactions in diverse medical scenarios (Rodrigues et al., 2016).Following closely, Government and Policy Making emerged as the second most prevalent category, where sentiment analysis plays a pivotal role in comprehending human behavior in governance solutions (Joyce and Deng, 2017).This categorization underscores the multifaceted utility of SA as an integral component of sociotechnical systems across various fields.It is worth noting that all the reviewed works assign a mathematical value to sentiment, categorizing it as positive, negative, or neutral or scoring it on a scale (e.g., -1 to +1).Most of the reviewed works lack clear definitions of sentiment or SA.Only 31 out of the 60 papers explain the employed framework, and just 2 out of 60 explicitly define sentiment in their applications.Only one takes an interdisciplinary perspective, defining sentiment in the context of finance for understanding market behavior (Kraaijeveld and De Smedt, 2020).Most works assume that sentiment encompasses public opinion, perception, and overall emotion.Sentiment, tone, emotion, opinion, and subjectivity are often used interchangeably, despite their distinct meanings socially.\n\nThe lack of precise sentiment definitions can result in misrepresented measurements.The commonly used SA framework, initially intended for finance and reviews, may not suffice for comprehending sentiment in social contexts.Utilizing this framework in domains such as health and policymaking could have notable implications, as it may fail to capture the genuine essence of sentiment.", "filtered_refids": [[], [], ["b79", "b128", "b197", "b156"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2327, "num_references": 4}
{"corpusid_sectionid": "258832362-s3", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Word Representations", "section": "A well-known challenge in NLP is creating continuous dense vector representations of words in high-dimensional spaces to capture their semantic and syntactic meaning. The most widely used algorithm for creating word embeddings is Word2Vec (Mikolov et al., 2013). Traditional approaches to representing words before Word2Vec, like one-hot encoding or bag-ofwords, have a number of drawbacks: They require a lot of memory to hold sparse vectors and fail to capture the links between words or their meaning. By using a neural network to learn word embeddings, Word2Vec solved these issues. The model trains neural networks using a large corpus of text as input to predict the likelihood of a word given its context or vice versa. The weights of the network are changed during training to reduce the discrepancy between the expected and actual probabilities. The network weights are employed as the word embeddings after training is finished. It has been widely used and inspired other models such as GloVe (Pennington et al., 2014) and fastText (Joulin et al., 2016).", "filtered_refids": [["b15", "b8", "b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1064, "num_references": 3}
{"corpusid_sectionid": "258832362-s7", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Natural Language Inference", "section": "Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence). The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013). NLI serves as a proxy for evaluating natural language understanding. According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.\n\nIn \u00a7 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019). There are two noteworthy components to this model. First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model. The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019). This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings. In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks. Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.\n\nMeanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks. Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings. They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder. This is one of the first studies that shows the utility of generative models for obtaining sentence representations.", "filtered_refids": [[null], ["b22", "b4"], ["b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2248, "num_references": 4}
{"corpusid_sectionid": "258832362-s11", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Surface Level", "section": "To create a sentence that carries the same meaning as another, one can modify the words or characters in a way that retains the sentence's semantic value. Recent research Wu et al., 2022d) suggests certain transformations that preserve the semantic meaning.  propose randomly flipping the case of some tokens, while  mask spans of tokens to get positive instances and Wu et al. (2022d) suggest to repeat certain words or subwords. Representations generated by transformer networks are biased towards the frequency of tokens, the case of words and subwords, and the length of the sentence (Wu et al., 2022d). For example, researchers found that avoiding to use high-frequency tokens can result in better sentence representations . These transformation help in overcoming such biases.\n\nHowever altering the surface characteristics of sentences can lead to models relying on shortcuts rather than learning the semantics of the sentences (Du et al., 2021). To address this issue, Wu et al. (2022a) propose the use of multiple augmentation strategies rather than a single transformation. They use shuffling, repeating, and dropping words as transformation strategies to improve model robustness. Additionally, they implement mechanisms to enhance learning from multiple positive examples.", "filtered_refids": [["b37"], [null, "b34"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1283, "num_references": 3}
{"corpusid_sectionid": "3254638-s1", "title": "A Critical Survey of the Methodology for IE Evaluation", "date": 2004, "section_title": "IE Evaluation Methodology", "section": "The MUC conferences can be considered the starting point of the IE evaluation methodology as currently defined. The MUC participants borrowed the Information Retrieval concepts of precision and recall for scoring filled templates. Given a system response and an answer key prepared by a human, the system's precision was defined as the number of slots it filled correctly, divided by the number of fills it attempted. Recall was defined as the number of slots it filled correctly, divided by the number of possible correct fills, taken from the human-prepared key. All slots were given the same weight. F-measure, a weighted combination of precision and recall, was also introduced to provide a single figure to compare different systems' performances.\n\nApart from the definition of precise evaluation measures, the MUC conferences made other important contributions to the IE field: the availability of large amount of annotated data (which have made possible the development of Machine Learning based approaches), along with the evaluation software (i.e., the MUC scorer (Douthat, 1998)), the emphasis on domain-independence and portability, and the identification of a number of different tasks which can be evaluated separately (Hirschman, 1998).\n\nIt should be noticed that MUC evaluation concentrated mainly on IE from relatively unrestricted text, i.e. newswire articles. In independent efforts, other researchers developed and made available annotated corpora developed from somewhat more constrained texts. Califf compiled and annotated a set of 300 job postings from the Internet (Califf, 1998), and Freitag compiled corpora of seminar announcements and university web pages, as well as a corporate acquisitions corpus from newswire texts (Freitag, 1998). Several of these corpora are available from the RISE repository (RISE, 1998) where a number of tagged corpora have been made available by researchers in Machine Learning for IE. Freitag (1998) uses the term Information Extraction in a more restricted sense than MUC. In the Seminar Announcement collection, the templates are simple and include slots for the seminar speaker, location, start time, and end time. This is in strong contrast with what happened in MUC where templates might be nested (i.e., the slot of a template may take another template as its value), or there might be several templates from which to choose, depending on the type of document encountered. In addition, MUC domains include irrelevant documents which a correctly behaving extraction system must discard. A template slot may be filled with a lower-level template, a set of strings from the text, a single string, or an arbitrary categorical value that depends on the text in some way (a so-called \"set fill\"). Califf (1988) takes an approach that is somewhat inbetween Freitag's approach and more complex MUC extraction tasks. All of the documents are relevant to the task, and the assumption is that there is precisely one template per document, but that many of the slots in the template can have multiple fillers.\n\nAlthough the tasks to be accomplished are different, the methodology adopted by (Freitag, 1998) and (Califf, 1998) is similar to the one used in the MUC competition: precision, recall, and F-measure are employed as measures of the performances of the systems.", "filtered_refids": [[], ["b12", "b8"], [null, "b9", "b3"], ["b9", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3322, "num_references": 7}
{"corpusid_sectionid": "3254638-s2", "title": "A Critical Survey of the Methodology for IE Evaluation", "date": 2004, "section_title": "Problematic Issues in IE Evaluation", "section": "In Section 2. we have summarized the current status of the methodology adopted in IE. However, the definition of an evaluation methodology and the availability of standard annotated corpora do not guarantee that the experiments performed with different approaches and algorithms proposed in the literature can be reliably compared. Some of the problems are common to other NLP tasks (e.g., see (Daelemans and Hoste, 2002)): the difficulty of exactly identifying the effects on performances of the data used (the sample selection and the sample size), of the information sources used (the features selected), and of the algorithm parameter settings.\n\nOne of the most relevant issues is that of the exact split between training set and test set, considering both the numerical proportions between the two sets (e.g., a 50/50 vs. a 80/20 split) and the procedure adopted to partition the documents (e.g., n repeated random splits vs. n-fold crossvalidation).\n\nFurthermore, the question of how to formalize the learning-curve sampling method and its associated costbenefit trade-off may cloud comparison further. For example, the following two approaches have been used: (1) For each point on the learning curve, train on some fraction of the available data and test on the remaining fraction; or (2) Hold out some fixed test set to be used for all points on the learning curve. The second approach is generally preferable: with the first procedure, points on the \"high\" end of the learning curve will have a larger variance than points on the \"low\" end.\n\nAnother important issue concerns the features used by the algorithm and their contribution to the performances of the algorithm. In IE, for instance, it would be relevant to extensively investigate the effectiveness of the use of simple orthographic features with respect to the use of more complex linguistic features such as PoS tags or semantic labels extracted from gazetteers (Ciravegna, 2001b).\n\nApart from those problematic issues mentioned above, there are some others that are specific to IE evaluation. A first issue concerns how to deal with issues related to tokenization, which is often considered something obvious and non problematic but it is not so and can affect the performance of the IE algorithms.\n\nA second issue is related to how to evaluate an extracted fragment -e.g., if an extra comma is extracted should it count as correct, partial or wrong? This issue is related to the question of how relevant is the exact identification of the boundaries of the extracted items. (Freitag, 1998) proposes three different criteria for matching reference instances and extracted instances:\n\nExact The predicted instance matches exactly an actual instance.", "filtered_refids": [["b6"], [], [], ["b5"], [], ["b9"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2720, "num_references": 3}
{"corpusid_sectionid": "3254638-s3", "title": "A Critical Survey of the Methodology for IE Evaluation", "date": 2004, "section_title": "Contains", "section": "The predicted instance strictly contains an actual instance, and at most k neighboring tokens.\n\nOverlap The predicted instance overlaps an actual instance.\n\nEach of these criteria can be useful, depending on the situation, and it can be interesting to observe how performance varies with changing criteria. (De Sitter and Daelemans, 2003) mention such criteria and present the results of their algorithm for all of them.\n\nA third issue concerns which software has been used for the evaluation. The only publicly available tool for such aim is the MUC scorer. Usually IE researchers have implemented their own scorer, relying on a number of implicit assumptions that have a strong influence on performance's evaluation.\n\nWhen multiple fillers are possible for a single slot, there is an additional ambiguity -usually glossed over in papers -that can influence performance. For example, (Califf and Mooney, 2003) remark that there are differences in counting between RAPIER (Califf, 1998), SRV (Freitag, 1998), and WHISK (Soderland, 1999). In his test on Job Postings (Soderland, 1999) does not eliminate duplicate values. When applied to Seminar Announcements SRV and RAPIER behave differently: SRV assumes only one possible answer per slot, while RAPIER makes no such assumption since it allows for the possibility of needing to extract multiple independent strings.\n\nDe Sitter and Daelemans (2003) also discuss this question and claim that in such cases there are two different ways of evaluating performance in extracting slot fillers: to find all occurrences (AO) of an entity (e.g. every mention of the job title in the posting) or only one occurrence for each template slot (one best per document, OBD). The choice of one alternative over the other may have an impact on the performance of the algorithm. (De Sitter and Daelemans, 2003) provide results for the two alternative ways of evaluating performances. This issue is often left underspecified in papers and, given the lack of a common software for evaluation, this further amplifies the uncertainty about the reported results.\n\nNote that there are actually three ways to count:\n\none answer per slot (where \"2pm\" and \"2:00\" are considered one correct answer)\n\none answer per occurrence in the document (each individual appearance of a string to be extracted in the document where two separate occurrences of \"2pm\" would be counted separately)\n\none answer per different string (where two separate occurrences of \"2pm\" are considered one answer, but \"2:00\" is yet another answer)\n\nFreitag takes the first approach, Soderland takes the second, and Califf takes the third.\n\nTo summarize, an information extraction task should specify all of the following: 1. A set of fields to extract.\n\n2. The legal numbers of fillers for each field, such as \"exactly one value\", \"zero or one values\", \"zero or more values\", or \"one or more values\". For example, in Seminar Announcements, the fields stime, etime and location are \"0-1\", speaker is \"1+\"; for Job Postings, title is \"0-1 or 0+\", required programming languages is \"0+\", etc. Thus, in the following seminar announcement:\n\nSpeakers will be Joel S. Birnbaum and Mary E.S. Loomis.\n\nif the task specifies that there should be one or more speaker, then to be 100% correct the algorithm must extract both names, while if the task specifies that zero or more speakers are allowed, then extracting either name would result in 100% correct performance.\n\n3. The possibility of multiple varying occurrences of any particular filler. For example, a seminar announcement with 2 speakers might refer to them each twice, but slightly differently: ", "filtered_refids": [[], [], ["b7"], [], ["b14", "b9", "b2", "b3"], ["b7"], [], [], [], [], [], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3637, "num_references": 6}
{"corpusid_sectionid": "260063224-s7", "title": "How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques", "date": 2023, "section_title": "Complete Training", "section": "Betti et al. (2020) propose a text GAN composed of one generator and two discriminators. The generator is a Relational Memory with self-attention (Santoro et al., 2018) with the objective to generate text consistent with the specified control attribute. The syntax discriminator distinguishes between real and generated sentences, while the semantic discriminator assesses whether the generated sentence expresses the control attribute, e.g. positive sentiment. To solve the well-known problem of differentiation in GANs applied to text, the Gumbel-softmax trick (Jang et al., 2016) is applied. This approach enables control only for one attribute at a time and it has been evaluated on sentiment and topic control.\n\nIn order to enable multi-attribute control, Qiao et al. (2020) propose a Sentiment-Controllable topic-to-essay generator that deploys a Conditional Variational Auto-Encoder in adversarial training. The model simultaneously controls the topics of the essay and the sentiment of each sentence composing the essay. The topic control is achieved using a Topic Graph Attention, which includes a topic knowledge graph in the generation process. Sentiment control is achieved by injecting the sentiment representation both in the encoder and the decoder.\n\nIn a different direction, Xie et al. (2022) propose a psychology-guided story generation method that controls storytelling as the protagonist's psychological state changes. This technique enables multi-attribute control considering the protagonist of the story (Character), their chain of emotions (Emotion), and chain of needs (Need) representing the evolution of the psychological state of the protagonist. The model is an encoder-decoder architecture with the addition of psychology controllers designed to integrate the local and global psychological state into the story context representation.", "filtered_refids": [["b13"], ["b8"], ["b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1865, "num_references": 3}
{"corpusid_sectionid": "260063224-s10", "title": "How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques", "date": 2023, "section_title": "Modification of Token Distribution", "section": "Dathathri et al. (2019) propose a Plug and Play Language Model (PPLM) which uses external attribute classifiers to guide text generation without requiring any training of the PLM. The PLM is used to obtain the next token distribution, which is fed to external classifiers, called Attribute Models, to assess whether the token correctly expresses the desired attributes. The internal latent representations of the LM are updated with a backward pass using the gradients of the attribute models to increase the likelihood of the desired attributes. Finally, the next token distribution is recomputed taking into account the updated latent representations. This model allows control of multiple attributes at a time, such as sentiment and topic.\n\nInspired by this work, Madotto et al. (2020) propose a variation of PPLMs in which the backward pass is executed n times depending on the desired intensity of the control attribute. Furthermore, they add Residual Adapters (Houlsby et al., 2019) on top of each transformer layer to steer the PLM output distribution without changing its parameters.\n\nGoswamy et al. (2020) propose a different variation of PPLMs based on GPT-2, in which a modified loss is considered to take into account the intensity of the controlled sentiment. Furthermore, instead of considering only positive/negative sentiment, control over 8 emotion categories is enabled.\n\nStarting from PPLMs, Gu et al. (2022a) observe that using a controller alone leads to the trade-off problem, i.e. the controller used to modify the token distribution only focuses on how to make the prefix related to the desired attribute without taking into account the original distribution of the LM. In this way, the controller takes over the LM's control for the next token distribution. In order to alleviate  this problem, they propose a weighted decoding method that adds a regulator module that permits fine-grained adjustment of a bias signal from the controller. At every step, the regulator detects differences between the PLM distribution and the target attribute and it determines whether to suppress or amplify the bias signal. This method is model agnostic and has been evaluated with sentiment, topic, and toxicity attributes. The last two methods propose sampling procedures that can be applied to any LM. Landsman et al. (2022) propose to modify beam search by reweighing the token candidate likelihoods to control different attributes. Diverse beam search (Vijayakumar et al., 2016) is used to decode k candidates, which are then scored using an attribute model. The obtained scores are used to reweigh the original likelihoods to produce a reweighed candidate distribution that considers both fluency and attribute characteristics. The resulting distribution is used to sample the next token.\n\nLastly, Kumar et al. (2022) propose a sampling method combining LM log-likelihoods with arbitrary constraints in a single energy function generating samples in a non-autoregressive manner. The idea is to use a PLM without changing its distribution but sampling from it considering different constraints, i.e. control attributes. The constraints are discriminative classifiers trained from scratch or fine-tuned. This method allows multi-attribute control (sentiment and toxicity).", "filtered_refids": [[], [], [], [null, "b18"], ["b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3285, "num_references": 3}
{"corpusid_sectionid": "260063224-s11", "title": "How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques", "date": 2023, "section_title": "Hybrid", "section": "Hybrid techniques combine two or more Control Implementation techniques. One possibility is to combine Complete Training and Fine-Tuning, for example, designing a model composed of different modules in which some modules are trained from scratch and some are fine-tuned models. In this context, Tian et al. (2022) propose a conversa-tion model that generates empathetic responses and guides the mood of the conversation in a positive direction while acknowledging the user's emotion. The idea is to extract the sentiment from the conversation context using a fine-tuned sentiment evaluator and use both the context and the extracted sentiment to steer the generation of the next response by generating a responding strategy that will be used by the Conditional Conversation model to generate the final response. The proposed method enables only single-attribute control (of sentiment).\n\nAnother way to enable controllability using a hybrid technique is to combine Fine-Tuning and Modification of Token Distribution.  propose a technique to control Story Generation by fine-tuning an encoder that learns the representation of new special tokens identifying the control attributes, thus allowing the model to properly include this information in the generation process. The next token distribution is obtained by combining the decoder distribution and the attention distribution, which allows the model to copy important information from the specified control attributes. The model allows fine-grained control taking into account the characters of the story with their actions and emotions.\n\nIn contrast to  who learn the representation of special tokens during fine-tuning, Liu et al. (2021) propose to modify an LM's token distribution including two fine-tuned versions of the PLM: an expert, focused on the desired attribute, and an anti-expert, focused on the opposite of the desired attribute. The next token distribution is obtained by subtracting the anti-expert distribution from the expert one and combining the result with the distribution of the frozen PLM to maintain fluency. This method enables the control only of one control attribute at a time and it has been tested on sentiment and toxicity attributes.\n\nSimilarly, Krause et al. (2021) propose to con-  trast the desired control attribute and its opposite. Instead of fine-tuning specialised LMs for each attribute, GPT-2 is fine-tuned with control codes to obtain a Class-Conditional LM (CCLM). At each time step, the generation is guided by computing classification probabilities for all possible next tokens via the Bayes rule by normalizing two classconditional distributions: conditioned on the desired attribute and conditioned on the undesired attribute. Like the previous method, it allows the control of one attribute at a time and has been evaluated using sentiment, topic, and toxicity attributes. Liu et al. (2022) also use a CCLM which is finetuned using an external discriminator to generate texts with the desired attributes, supporting multiattribute control. The token distribution is modified based on a contrastive generator that learns effective representations by bringing together positive samples, i.e. samples with desired attributes, and separating negative samples, i.e. samples without desired attributes. The obtained distribution is combined with the distribution of a PLM to maintain the fluency of the generated text. The generated text is fed to the external discriminator to assess whether it contains the desired attributes or not. The model has been tested on the joint control of sentiment and topic.  explore the contrast between desired and undesired attributes proposing a fine-tuned LM incorporating the attribute knowledge of a discriminator, similarly to Liu et al. (2022), to optimize continuous virtual tokens called control-prompts. The learned control-prompts are used as prefixes to steer a fixed conditional LM to generate attribute-specific texts. The LM is finetuned using (i) likelihood training, encouraging the LM to generate tokens with higher probability as scored by the discriminator assessing the desired attribute, and (ii) unlikelihood training, keeping the generated tokens away from lower-probability candidates.", "filtered_refids": [["b16"], [], ["b21"], [null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4241, "num_references": 4}
{"corpusid_sectionid": "264832783-s1", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Background and Notation", "section": "We refine the definition in Wei et al. (2022b), and specify the emergent abilities as the abilities that LLMs can leverage to achieve satisfactory results2 across diverse tasks, with only a few examples or chain-of-thought demonstrations, and without the need for re-training.\n\nFormally, let's define some key variables.D \u2208 T train represents a subset of demonstrations selected from the training set.Q \u2208 T test is the query taken from the test set, and Y stands for the label associated with each query.M represents the LLM with its parameters frozen as \u0398, and F denotes the evaluation metric function.For example, F is typically used to measure accuracy or F1 score in classification tasks, such as sentiment classification, and is often used to represent metrics like ROUGE (Lin, 2004) or BLEU (Papineni et al., 2002) in text generation tasks, such as summarization and machine translation.The concept of emergent ability can be formally expressed using the equation:\n\nwhere M is usually considered to exhibit emergent abilities if the computed value using F exceeds a pre-defined threshold.Under this definition, we can group similar concepts within the few-shot prompting paradigm.CoT can be viewed as a variant of ICL, with the primary distinction being the format of the demonstration.Specifically, ICL demonstrations typically rely on a standard prompt with optional demonstration examples, whereas CoT prompting incorporates an additional textual reasoning process.\n\nAccording to our definition, we organize existing literature (summarized in Table 1) on interpreting emergent capabilities into macro and micro perspectives.Researchers in the macro category focus on factors such as overall loss or the model architecture.Their goal is to establish a connection between the outcome of F and the behavior of M. Conversely, those in the micro category primarily centre their attention on the relationship between the outcome of F and the characteristics of the demonstration set D.", "filtered_refids": [["b62"], ["b43", "b31"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1988, "num_references": 3}
{"corpusid_sectionid": "264832783-s3", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Mechanistic Interpretability", "section": "With the goal of reverse-engineering components of frontier models into more understandable algorithms, Elhage et al. (2021) developed a mathematical framework for decomposing operations within transformers (Vaswani et al., 2017).They initially introduced the concept of \"induction heads\" in a two-layer attention-only model to explain the functioning of ICL within transformers with Circuits (Cammarata et al., 2020).They found that one-layer attention-only models perform relatively basic ICL in a crude manner, whereas two-layer models perform very general ICL using very different algorithms.Specifically, they discovered that one-layer models essentially function as an ensemble of bigram and \"skip-trigram\" models that can be accessed directly from the model weights without running the entire model.Most attention heads in these models allocate significant capacity to copying mechanisms, resulting in very simple ICL.In contrast, the two-layer models manifest a significantly powerful mechanism that employs more advanced, qualitative algorithms at inference time, referred to as \"induction heads\".This allows them to perform ICL in a manner that resembles a computer program executing an algorithm, rather than merely referencing skip-trigrams.Building on this foundation, Olsson et al. (2022) later investigated the internal structures responsible for ICL by extending the concept of \"induction head\" (Elhage et al., 2021).They implemented circuits consist of two attention heads: the \"previous token head\", which copies information from one token to its successor, and the actual \"induction head\", which uses this information to target tokens that precede the current one.Their study revealed a phase change occurring early in the training of LLMs of various sizes.This phase change involves circuits that perform \"fuzzy\" or \"nearest neighbor\" pattern completion in a mechanism similar to the two-layer induction heads.These circuits play a crucial role in implementating most ICL in large models.One pivotal insight from (Olsson et al., 2022) presented six arguments supporting their hypothesis that induction heads may serve as the primary mechanistic source of ICL in a significant portion of LLMs, particularly those based on transformer architectures.\n\nWhile Elhage et al. (2021) and Olsson et al. (2022) contribute to our understanding of ICL by probing the internal architecture of LLMs, it is important to note that their findings represent initial steps towards the comprehensive reverseengineering of LLMs.It becomes particularly intricate when dealing with LLMs characterized by complex structures comprising hundreds of layers and spanning billions to trillions of parameters.This complexity introduces significant challenges.Moreover, a substantial portion of their conclusions relies primarily on empirical correlations, which might be susceptible to confounding from various factors, thereby introducing potential vulnerabilities into their findings.", "filtered_refids": [[null, "b57", "b9", "b40"], [null, "b40"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2976, "num_references": 6}
{"corpusid_sectionid": "264832783-s4", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Regression Function Learning", "section": "Several research studies posited that the emergence of LLMs' competence in ICL can be attributed to their intrinsic capability to approximate regression functions for a novel query Q based on the demonstrations D. Garg et al. (2022) first formally de-fined ICL as a problem of learning functions and explored whether LLMs can be trained from scratch to learn simple and well-defined function classes, such as linear regression functions.To achieve this, they generated examples D using these functions, and trained models to predict the function value for the corresponding query Q.Their empirical findings revealed that trained Transformers exhibited ICL abilities, as they manifested to \"learn\" previously unseen linear functions from examples, achieving an average error comparable to that of the optimal least squares estimator.Furthermore, Garg et al. (2022) demonstrated that ICL can be applied to more complex function classes, including sparse linear functions, decision trees, and two-layer neural networks, and posited that the capability to learn a function class through ICL is an inherent property of the model M \u0398 , irrespective of its training methodology.\n\nLater, Li et al. (2023b) extended Garg et al. (2022) to interpret ICL from a statistical perspective.They derived generalization bounds for ICL, considering two types of input examples: sequences that are independently and identically distributed (i.i.d.) and trajectories originating from a dynamical system.They established a multitask generalization rate of 1/", "filtered_refids": [["b18"], [null, "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1536, "num_references": 3}
{"corpusid_sectionid": "222124957-s2", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Transfer: The Pretrain-Finetune Framework", "section": "While text representations can be learned in any manner, ultimately, they are evaluated using specific target tasks. Historically, the learned representations (e.g. word vectors) were used as initialization for task-specific models. Dai and Le (2015) are credited with using pretrained language model outputs as initialization, McCann et al. (2017) use pretrained outputs from translation as frozen word embeddings, and Howard and Ruder (2018) and Radford et al. (2018) demonstrate the effectiveness of finetuning to different target tasks by updating the full (pretrained) model for each task. We refer to the embeddings produced by the pretrained models (or encoders) as contextualized text representations. As our goal is to discuss the encoders and their representations, we do not cover the innovations in finetuning (Liu et al., 2015;Ruder et al., 2019;Phang et al., 2018;Liu et al., 2019c;Zhu et al., 2020, inter alia).\n\nEvaluation Widely adopted evaluations of text representations relate them to downstream natural language understanding (NLU) benchmarks. This full-stack process necessarily conflates representation power with finetuning strategies. Common language understanding benchmarks include (1) a diverse suite of sentence-level tasks covering paraphrasing, natural language inference, sentiment, and linguistic acceptability (GLUE) and its more challenging counterpart with additional commonsense and linguistic reasoning tasks (Super-GLUE) (Wang et al., 2019c,b;Clark et al., 2019a;De Marneffe et al., 2019;Roemmele et al., 2011;Khashabi et al., 2018;Zhang et al., 2018;Dagan et al., 2006;Bar Haim et al., 2006;Giampiccolo et al., 2007;Bentivogli et al., 2009;Pilehvar and Camacho-Collados, 2019;Poliak et al., 2018;Levesque et al., 2011); (2) crowdsourced questions derived from Wikipedia articles (Rajpurkar et al., 2016, 2018; and (3) multiple-choice reading comprehension (Lai et al., 2017, RACE).", "filtered_refids": [["b63", "b6", "b57", "b40", "b41", "b45", null, "b72"], ["b58", "b60", "b28", null, "b19", "b123", "b70", "b33", "b68"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1921, "num_references": 17}
{"corpusid_sectionid": "222124957-s4", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Token Prediction", "section": "Predicting (or generating) the next word has historically been equivalent to the task of language modeling. Large language models perform impressively on a variety of language understanding tasks while maintaining their generative capabilities (Radford et al., 2018(Radford et al., , 2019Keskar et al., 2019;Brown et al., 2020), often outperforming contemporaneous models that use additional training objectives.\n\nELMo (Peters et al., 2018) is a BiLSTM model with a language modeling objective for the next (or previous) token given the forward (or backward) history. This idea of looking at the full context was further refined as a cloze 3 task (Baevski et al., 2019), or as a denoising Masked Language Modeling (MLM) objective (Devlin et al., 2019, BERT). MLM replaces some tokens with a [mask] symbol and provides both right and left contexts (bidirectional context) for predicting the masked tokens. The bidirectionality is key to outperforming a unidirectional language model on a large suite of natural language understanding benchmarks (Devlin et al., 2019;Raffel et al., 2019).\n\nThe MLM objective is far from perfect, as the use of [mask] introduces a pretrain/finetune vo-cabulary discrepancy. Devlin et al. (2019) look to mitigate this issue by occasionally replacing [mask] with the original token or sampling from the vocabulary. Yang et al. (2019) convert the discriminative objective into an autoregressive one, which allows the [mask] token to be discarded entirely. Naively, this would result in unidirectional context. By sampling permutations of the factorization order of the joint probability of the sequence, they preserve bidirectional context. Similar ideas for permutation language modeling (PLM) have also been studied for sequence generation (Stern et al., 2019;Chan et al., 2019;Gu et al., 2019). The MLM and PLM objectives have since been unified architecturally Bao et al., 2020) and mathematically (Kong et al., 2020).\n\nELECTRA (Clark et al., 2020) replaces [mask] through the use of a small generator (trained with MLM) to sample a real token from the vocabulary. The main encoder, a discriminator, then determines whether each token was replaced.\n\nA natural extension would mask units that are more linguistically meaningful, such as rarer words, 4 whole words, or named entities (Devlin et al., 2019;Sun et al., 2019b). This idea can be simplified to random spans of texts (Yang et al., 2019;Song et al., 2019). Specifically, Joshi et al. (2020) add a reconstruction objective which predicts the masked tokens using only the span boundaries. They find that masking random spans is more effective than masking linguistic units.\n\nAn alternative architecture uses an encoderdecoder framework (or denoising autoencoder) where the input is a corrupted (masked) sequence the output is the full original sequence (Wang et al., 2019d;Lewis et al., 2020;Raffel et al., 2019).", "filtered_refids": [["b15", "b64", "b63", "b17"], ["b65", null, "b56"], [null, "b23", "b119", "b84"], [null], ["b90", "b13", "b82", null, "b119"], ["b29", "b104", "b65"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2900, "num_references": 20}
{"corpusid_sectionid": "222124957-s5", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Nontoken Prediction", "section": "Bender and Koller (2020) argue that for the goal of natural language understanding, we cannot rely purely on a language modeling objective; there must be some grounding or external information that relates the text to each other or to the world. One solution is to introduce a secondary objective to directly learn these biases.\n\nSelf-supervised discourse structure objectives, such as text order, has garnered significant attention. To capture relationships between two sentences, 5 Devlin et al. (2019) introduce the next 4 Clark et al. (2020) report negative results for rarer words. 5 Sentence unfortunately refers to a text segment containing sentence prediction (NSP) objective. In this task, either sentence B follows sentence A or B is a random negative sample. Subsequent works showed that this was not effective, suggesting the model simply learned topic (Yang et al., 2019;. Jernite et al. (2017) propose a sentence order task of predicting whether A is before, after, or unrelated to B, and Wang et al. (2020b) and Lan et al. (2020) use it for pretraining encoders. They report that (1) understanding text order does contribute to improved language understanding; and (2) harder-to-learn pretraining objectives are more powerful, as both modified tasks have lower intrinsic performance than NSP. It is still unclear, however, if this is the best way to incorporate discourse structure, especially since these works do not use real sentences.\n\nAdditional work has focused on effectively incorporating multiple pretraining objectives. Sun et al. (2020a) use multi-task learning with continual pretraining (Hashimoto et al., 2017), which incrementally introduces newer tasks into the set of pretraining tasks from word to sentence to document level tasks. Encoders using visual features (and evaluated only on visual tasks) jointly optimize multiple different masking objectives over both token sequences and regions of interests in the image (Tan and Bansal, 2019). 6 Prior to token prediction, discourse information has been used in training sentence representations. Conneau et al. (2017Conneau et al. ( , 2018a use natural language inference sentence pairs, Jernite et al. (2017) use discourse-based objectives of sentence order, conjunction classifier, and next sentence selection, and  use discourse markers. While there is weak evidence suggesting that these types of objectives are less effective than language modeling (Wang et al., 2019a), we lack fair studies comparing the relative influence between the two categories of objectives.", "filtered_refids": [[], ["b11", "b26", "b119", "b106"], ["b93", "b91", null, "b11"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2554, "num_references": 8}
{"corpusid_sectionid": "52011136-s1", "title": "A review of Spanish corpora annotated with negation", "date": "2018-08-01", "section_title": "Negation in Spanish", "section": "Processing negation is not as easy as using a list of negation markers and applying look-up methods. They can be used to find out potential negation cues but they are not adequate because the presence of a cue does not imply that it acts as a negation. In the sentence \"You bought the car to use it, didn't you?\" the cue \"not\" is not used as a negation but it is used to reinforce the first part of the sentence. Moreover, it is also necessary to identify the scope or part of the sentence affected by the negation and its focus, the part more prominently negated. If we want to advance in the study of this phenomenon, as for most of NLP tasks, the availability of annotated corpora is essential to train algorithms. According to existing resources for English, annotating negation involves the annotation of the following aspects:\n\n\u2022 Negation cue: lexical item(s) that modify the truth value of the propositions that are within its scope.\n\nThere are different types of negation according to the type of the negation cue used: . It is also known as affixal negation.\n\n\u2022 Scope: the part of the sentence affected by the negation cue . The scope can be continuous or discontinuous.\n\n\u2022 Focus: the part of the scope that is most prominently or explicitly negated (Blanco and Moldovan, 2011).\n\n\u2022 Negated event: the event that is directly negated by the negation cue, usually a verb, a noun or an adjective (Kim et al., 2008). This is just a list of the main aspects that have been annotated for negation. However, each language has specific linguistic resources to express negation and specific negation structures, which should also be reflected in the information annotated in corpora. As we will show in Section 4, most existing annotation schemes for Spanish do not account for the complexity of the linguistic structures used to express negation that are present in texts. This happens mainly because of two reasons: first, annotation of negation started with the annotation of clinical reports in English (Chapman et al., 2001;Goldin and Chapman, 2003;Mutalik et al., 2001a), where there is not too much variation of negation structures. Second, corpora have been created for specific purposes, such as extracting negated clinical events, and not with the intention of accounting for all the linguistic complexity of the negation phenomenon.\n\nAn exception to this is the SFU Review SP -NEG corpus (Jim\u00e9nez-Zafra et al., 2018;. The guidelines specify a great variety of negation patterns at the syntactic level that we summarize below. Additionally, the guidelines also specify expressions that involve a negation cue but do not express negation.\n\nOn the one hand, patterns that express negation can be divided into three categories:\n\n1. Simple negation markers, if they are composed of only one single negation marker (i.e. no ['no/not'], nunca ['never']).\n\n3. Negation markers in contrastive constructions, if negation markers are used to counterpose different ideas, to correct something, to introduce new information or to express obligation, rather than to express negation (i.e. No hay m\u00e1s soluci\u00f3n que comprar una lavadora ['There is no other solution than to buy a washing machine']). 4. Negation markers in comparative constructions, if negation markers are used to compare some property with something, that is, negation is used to place an entity below or above another entity on a scale (i.e. No es tan grande como me lo imaginaba ['It is not as big as I imagined']).", "filtered_refids": [[], [], [], [], ["b1"], ["b15", "b9", "b5", "b3"], ["b8"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3479, "num_references": 6}
{"corpusid_sectionid": "52011136-s3", "title": "A review of Spanish corpora annotated with negation", "date": "2018-08-01", "section_title": "UAM Spanish Treebank", "section": "The first Spanish corpus annotated with negation that we are aware of is the UAM Spanish Treebank (Moreno et al., 2003), which was enriched with the annotation of negation cues and their scopes (Sandoval and Salazar, 2013). The initial UAM Spanish Treebank consisted of 1,500 sentences extracted from newspaper articles (El Pa\u00eds Digital and Compra Maestra) that were annotated syntactically. Trees were encoded in a nested structure, including syntactic category, syntactic and semantic features, and constituent nodes, following the Penn Treebank model. Later, this version of the corpus was extended with the annotation of negation and 10.67% of the sentences were found to contain negations (160 sentences).\n\nIn this corpus, syntactic negation was annotated but not lexical nor morphological negation. It was annotated by two experts in corpus linguistics who followed similar guidelines to those of Bioscope corpus Vincze, 2010). They included negation cues within the scope as in Bioscope and NegDDI-DrugBank (Bokharaeian et al., 2014). All the arguments of the negated events were also included in the scope of negation, including the subject, which was excluded from the scope in active sentences in Bioscope. There is no information about inter-annotator agreement.\n\nThe UAM Spanish Treebank corpus is freely available at http://www.lllf.uam.es/ESP/ Treebank.html. It is in XML format, negation cues are tagged with the label Type=\"NEG\" and the scope of negation is tagged with the label Neg=\"YES\" in the syntactic constituent on which negation acts.", "filtered_refids": [["b14"], ["b23", "b2"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1558, "num_references": 3}
{"corpusid_sectionid": "52011136-s5", "title": "A review of Spanish corpora annotated with negation", "date": "2018-08-01", "section_title": "SFU Review SP -NEG", "section": "The SFU Review SP -NEG 9 (Jim\u00e9nez-Zafra et al., 2018) is the first Spanish corpus that includes the event in the annotation of negation and that takes into account discontinuous negation markers. Moreover, it is the first corpus where the effect of the negation on the words that are within its scope is annotated, that is, whether there is a change in the polarity or an increment or reduction of its value. It is an extension of the Spanish part of the SFU Review corpus (Taboada et al., 2006) and it could be considered as the counterpart of the SFU Review Corpus with negation and speculation annotations 10 (Konstantinova et al., 2012).\n\nThe Spanish SFU Review corpus consists of 400 reviews extracted from the website Ciao.es that belong to 8 different domains: cars, hotels, washing machines, books, cell phones, music, computers, and movies. For each domain there are 50 positive and 50 negative reviews, defined as positive or negative based on the number of stars given by the reviewer (1-2=negative; 4-5=positive; 3-star review were not included). Later, it was extended to the SFU Review SP -NEG corpus in which each review was automatically annotated at the token level with POS-tags and lemmas, and manually annotated at the sentence level with negation cues and their corresponding scopes and events. It is composed of 9,455 sentences, out of which 3,022 sentences (31.97%) contain at least one negation marker.\n\nIn this corpus, syntactic negation was annotated but not lexical nor morphological negation, as in the UAM Spanish Treebank corpus. Unlike this one, annotations on the event and on how negation affects the polarity of the words within its scope were included. The annotations were performed by two senior researchers with in-depth experience in corpus annotation who supervised the whole process and two trained annotators who carried out the annotation task. The Kappa coefficient for inter-annotator agreement was of 0.97 for negation cues, 0.95 for negated events and 0.94 for scopes. 11 A detailed discussion of the main sources of disagreements can be found in (Jim\u00e9nez-Zafra et al., 2016).\n\nThe guidelines of the Bioscope corpus were taken into account but after a thorough analysis of negation in Spanish, a typology of Spanish negation patterns was defined . As in Bioscope, NegDDI-DrugBank and UAM Spanish Treebank, negation markers were included within the scope. Moreover, the subject was also included within the scope when the word directly affected by negation is the verb of the sentence, as in ConanDoyle-neg corpus (Morante and Daelemans, 2012). The event was also included in the scope of negation as in ConanDoyle-neg corpus.\n\nThe SFU Review SP -NEG is publicly available and can be downloaded at http://sinai.ujaen. es/sfu-review-sp-neg-2/.", "filtered_refids": [["b21", "b10"], [], ["b7"], ["b13"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2788, "num_references": 4}
{"corpusid_sectionid": "47019063-s1", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "The concept of semantic shifts", "section": "Human languages change over time, due to a variety of linguistic and non-linguistic factors and at all levels of linguistic analysis. In the field of theoretical (diachronic) linguistics, much attention has been devoted to expressing regularities of linguistic change. For instance, laws of phonological change have been formulated (e.g., Grimm's law or the great vowel shift) to account for changes in the linguistic sound system. When it comes to lexical semantics, linguists have studied the evolution of word meaning over time, describing so-called lexical semantic shifts or semantic change, which Bloomfield (1933) defines as \"innovations which change the lexical meaning rather than the grammatical function of a form.\" Historically, much of the theoretical work on semantic shifts has been devoted to documenting and categorizing various types of semantic shifts (Br\u00e9al, 1899;Stern, 1931;Bloomfield, 1933). The categorization found in Bloomfield (1933) is arguably the most used and has inspired a number of more recent studies (Blank and Koch, 1999;Geeraerts, 1997;Traugott and Dasher, 2001). Bloomfield (1933) originally proposed nine classes of semantic shifts, six of which are complimentary pairs along a dimension. For instance, the pair 'narrowing' -'broadening' describes the observation that word meaning often changes to become either more specific or more general, e.g. Old English mete 'food' becomes English meat 'edible flesh,' or that the more general English word dog is derived from Middle English dogge which described a dog of a particular breed. Bloomfield (1933) also describes change along the spectrum from positive to negative, describing the speaker's attitude as one of either degeneration or elevation, e.g. from Old English cniht 'boy, servant' to the more elevated knight.\n\nThe driving forces of semantic shifts are varied, but include linguistic, psychological, sociocultural or cultural/encyclopedic causes (Blank and Koch, 1999;Grzega and Schoener, 2007). Linguistic processes that cause semantic shifts generally involve the interaction between words of the vocabulary and their meanings. This may be illustrated by the process of ellipsis, whereby the meaning of one word is transferred to a word with which it frequently co-occurs, or by the need for discrimination of synonyms caused by lexical borrowings from other languages. Semantic shifts may be also be caused by changes in the attitudes of speakers or in the general environment of the speakers. Thus, semantic shifts are naturally separated into two important classes: linguistic drifts (slow and regular changes in core meaning of words) and cultural shifts (culturally determined changes in associations of a given word). Researchers studying semantic shifts from a computational point of view have shown the existence of this division empirically (Hamilton et al., 2016c). In the traditional classification of Stern (1931), the semantic shift category of substitution describes a change that has a non-linguistic cause, namely that of technologi-cal progress. This may be exemplified by the word car which shifted its meaning from non-motorized vehicles after the introduction of the automobile.\n\nThe availability of large corpora have enabled the development of new methodologies for the study of lexical semantic shifts within general linguistics (Traugott, 2017). A key assumption in much of this work is that changes in a word's collocational patterns reflect changes in word meaning (Hilpert, 2008), thus providing a usage-based account of semantics (Gries, 1999). For instance, Kerremans et al. (2010) study the very recent neologism detweet, showing the development of two separate usages/meanings for this word ('to delete from twitter,' vs 'to avoid tweeting') based on large amounts of web-crawled data. The usage-based view of lexical semantics aligns well with the assumptions underlying the distributional semantic approach (Firth, 1957) often employed in NLP . Here, the time spans studied are often considerably shorter (decades, rather than centuries) and we find that these distributional methods seem well suited for monitoring the gradual process of meaning change. Gulordava and Baroni (2011), for instance, showed that distributional models capture cultural shifts, like the word sleep acquiring more negative connotations related to sleep disorders, when comparing its 1960s contexts to its 1990s contexts.\n\nTo sum up, semantic shifts are often reflected in large corpora through change in the context of the word which is undergoing a shift, as measured by co-occurring words. It is thus natural to try to detect semantic shifts automatically, in a 'data-driven' way. This vein of research is what we cover in the present survey. In the following sections, we overview the methods currently used for the automatic detection of semantic shifts and the recent academic achievements related to this problem.", "filtered_refids": [["b59", null, "b7", "b62", "b5", "b17"], ["b59", "b19", "b23", "b5"], ["b15", "b63", "b18", "b28", "b20", "b33"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 4932, "num_references": 16}
{"corpusid_sectionid": "47019063-s4", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "Training data", "section": "The time unit (the granularity of the temporal dimension) can be chosen before slicing the text collection into subcorpora. Earlier works dealt mainly with long-term semantic shifts (spanning decades or even centuries), as they are easier to trace. One of the early examples is Sagi et al. (2011) who studied differences between Early Middle, Late Middle and Early Modern English, using the Helsinki Corpus (Rissanen and others, 1993).\n\nThe release of the Google Books Ngrams corpus 1 played an important role in the development of the field and spurred work on the new discipline of 'culturomics,' studying human culture through digital media (Michel et al., 2011). Mihalcea and Nastase (2012) used this dataset to detect differences in word usage and meaning across 50-years time spans, while Gulordava and Baroni (2011) compared word meanings in the 1960s and in the 1990s, achieving good correlation with human judgments. Unfortunately, Google Ngrams is inherently limited in that it does not contain full texts. However, for many cases, this corpus was enough, and its usage as the source of diachronic data continued in Mitra et al. (2014) (employing syntactic ngrams), who detected word sense changes over several different time periods spanning from 3 to 200 years.\n\nIn more recent work, time spans tend to decrease in size and become more granular. In general, corpora with smaller time spans are useful for analyzing socio-cultural semantic shifts, while corpora with longer spans are necessary for the study of linguistically motivated semantic shifts. As researchers are attempting to trace increasingly subtle cultural semantic shifts (more relevant for practical tasks), the granularity of time spans is decreasing: for example, Kim et al. (2014) and Liao and Cheng (2016) analyzed the yearly changes of words. Note that, instead of using granular 'bins', time can also be represented as a continuous differentiable value (Rosenfeld and Erk, 2018).\n\nIn addition to the Google Ngrams dataset (with granularity of 5 years), Kulkarni et al. (2015) used Amazon Movie Reviews (with granularity of 1 year) and Twitter data (with granularity of 1 month). Their results indicated that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collection, the New-York Times Annotated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA 2 ), with time slices equal to one decade. Hamilton et al. (2016a) continued the usage of COHA (along with the Google Ngrams corpus). Kutuzov et al. (2017b) started to employ the yearly slices of the English Gigaword corpus (Parker et al., 2011) in the analysis of cultural semantic drift related to armed conflicts.", "filtered_refids": [[null, "b57"], ["b45", "b20", "b44", "b48"], ["b34", "b41", "b55"], ["b39", "b58", "b14", "b50", "b35", "b21", "b69", "b68"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3171, "num_references": 17}
{"corpusid_sectionid": "47019063-s5", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "Test sets", "section": "Diachronic corpora are needed not only as a source of training data for developing semantic shift detection systems, but also as a source of test sets to evaluate such systems. In this case, however, the situation is more complicated. Ideally, diachronic approaches should be evaluated on human-annotated lists of semantically shifted words (ranked by the degree of the shift). However, such gold standard data is difficult to obtain, even for English, let alone for other languages. General linguistics research on language change like that of Traugott and Dasher (2001) and others usually contain only a small number of hand-picked examples, which is not sufficient to properly evaluate an automatic unsupervised system.\n\nVarious ways of overcoming this problem have been proposed. For example, Mihalcea and Nastase (2012) evaluated the ability of a system to detect the time span that specific contexts of a word undergoing a shift belong to (word epoch disambiguation). A similar problem was offered as SemEval-2015 Task 7: 'Diachronic Text Evaluation' (Popescu and Strapparava, 2015). Another possible evaluation method is so-called cross-time alignment, where a system has to find equivalents for certain words in different time periods (for example, 'Obama' in 2015 corresponds to 'Trump' in 2017). There exist several datasets containing such temporal equivalents for English (Yao et al., 2018). Yet another evaluation strategy is to use the detected diachronic semantic shifts to trace or predict real-world events like armed conflicts (Kutuzov et al., 2017b). Unfortunately, all these evaluation methods still require the existence of large manually annotated semantic shift datasets. The work to properly create and curate such datasets is in its infancy.\n\nOne reported approach to avoid this requirement is borrowed from research on word sense disambiguation and consists of making a synthetic task by merging two real words together and then modifying the training and test data according to a predefined sense-shifting function. Rosenfeld and Erk (2018) successfully employed this approach to evaluate their system; however, it still operates on synthetic words, limiting the ability of this evaluation scheme to measure the models' performance with regards to real semantic shift data. Thus, the problem of evaluating semantic shift detection approaches is far from being solved, and practitioners often rely on self-created test sets, or even simply manually inspecting the results.", "filtered_refids": [["b62"], ["b53", "b39", "b68"], ["b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2498, "num_references": 5}
{"corpusid_sectionid": "49587276-s3", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models", "date": "2018-08-01", "section_title": "NER datasets", "section": "Since the first shared task on NER (Grishman and Sundheim, 1996) 1 , many shared tasks and datasets for NER have been created. CoNLL 2002 (Tjong Kim Sang, 2002) 2 and CoNLL 2003(Tjong Kim Sang and De Meulder, 2003 3 were created from newswire articles in four different languages (Spanish, Dutch, English, and German) and focused on 4 entities -PER (person), LOC (location), ORG (organization) and MISC (miscellaneous including all other types of entities).\n\nNER shared tasks have also been organized for a variety of other languages, including Indian languages (Rajeev Sangal and Singh, 2008), Arabic (Shaalan, 2014), German (Benikova et al., 2014), and slavic languages (Piskorski et al., 2017). The named entity types vary widely by source of dataset and language. For example, Rajeev Sangal and Singh (2008)'s southeast Asian language data has named entity types person, designation, temporal expressions, abbreviations, object number, brand, etc. Benikova et al. (2014)'s data, which is based on German wikipedia and online news, has named entity types similar to that of CoNLL 2002 and 2003: PERson, ORGanization, LOCation and OTHer. The shared task 4 or-ganized by Piskorski et al. (2017) covering 7 slavic languages (Croatian, Czech, Polish, Russian, Slovak, Slovene, Ukrainian) also has person, location, organization and miscellaneous as named entity types.\n\nIn the biomedical domain, Kim et al. (2004) organized a BioNER task on MedLine abstracts, focusing on protien, DNA, RNA and cell attribute entity types. Uzuner et al. (2007) presented a clinical note de-identification task that required NER to locate personal patient data phrases to be anonymized. The 2010 I2B2 NER task 5 (Uzuner et al., 2011), which considered clinical data, focused on clinical problem, test and treatment entity types. Segura Bedmar et al. (2013) organized a Drug NER shared task 6 as part of SemEval 2013 Task 9, which focused on drug, brand, group and drug n (unapproved or new drugs) entity types. (Krallinger et al., 2015) introduced the similar CHEMDNER task 7 focusing more on chemical and drug entities like trivial, systematic, abbreviation, formula, family, identifier, etc. Biology and microbiology NER datasets 8 (Hirschman et al., 2005;Bossy et al., 2013;Del\u0117ger et al., 2016) have been collected from PubMed and biology websites, and focus mostly on bacteria, habitat and geolocation entities. In biomedical NER systems, segmentation of clinical and drug entities is considered to be a difficult task because of complex orthographic structures of named entities (Liu et al., 2015).\n\nNER tasks have also been organized on social media data, e.g., Twitter, where the performance of classic NER systems degrades due to issues like variability in orthography and presence of grammatically incomplete sentences (Baldwin et al., 2015). Entity types on Twitter are also more variable (person, company, facility, band, sportsteam, movie, TV show, etc.) as they are based on user behavior on Twitter.\n\nThough most named entity annotations are flat, some datasets include more complex structures. Ohta et al. (2002) constructed a dataset of nested named entities, where one named entity can contain another. Strassel et al. (2003) highlighted both entity and entity head phrases. And discontinuous entities are common in chemical and clinical NER datasets (Krallinger et al., 2015). Eltyeb and Salim (2014) presented an survey of various NER systems developed for such NER datasets with a focus on chemical NER. Grishman and Sundheim (1996) scored NER performance based on type, whether the predicted label was correct regardless of entity boundaries, and text, whether the predicted entity boundaries were correct regardless of the label. For each score category, precision was defined as the number of entities a system predicted correctly divided by the number that the system predicted, recall was defined as the number of entities a system predicted correctly divided by the number that were identified by the human annotators, and (micro) F-score was defined as the harmonic mean of precision and recall from both type and text.", "filtered_refids": [["b72", null, "b25", "b71"], [null, "b67", "b56", "b5"], ["b28", "b66", "b30", "b74", "b42", "b33", "b8", "b73", "b17"], ["b4"], ["b52", "b25", "b20", "b69", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 4128, "num_references": 23}
{"corpusid_sectionid": "49587276-s4", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models", "date": "2018-08-01", "section_title": "NER evaluation metrics", "section": "The exact match metrics introduced by CoNLL (Tjong Kim Sang and De Meulder, 2003;Tjong Kim Sang, 2002) considers a prediction to be correct only when the predicted label for the complete entity is matched to exactly the same words as the gold label of that entity. CoNLL also used (micro) F-score, taking the harmonic mean of the exact match precision and recall.\n\nThe relaxed F1 and strict F1 metrics have been used in many NER shared tasks (Segura Bedmar et al., 2013;Krallinger et al., 2015;Bossy et al., 2013;Del\u0117ger et al., 2016). Relaxed F1 considers a prediction to be correct as long as part of the named entity is identified correctly. Strict F1 requires the character offsets of a prediction and the human annotation to match exactly. In these data, unlike CoNLL, word offsets are not given, so relaxed F1 is intended to allow comparison despite different systems having different word boundaries due to different segmentation techniques (Liu et al., 2015).", "filtered_refids": [["b72", "b71"], ["b66", "b42", "b33", "b8", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 967, "num_references": 7}
{"corpusid_sectionid": "49587276-s6", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models", "date": "2018-08-01", "section_title": "Unsupervised and bootstrapped systems", "section": "Some of the earliest systems required very minimal training data. Collins and Singer (1999) used only labeled seeds, and 7 features including orthography (e.g., capitalization), context of the entity, words contained within named entities, etc. for classifying and extracting named entities. Etzioni et al. (2005) proposed an unsupervised system to improve the recall of NER systems applying 8 generic pattern extractors to open web text, e.g., NP is a <class1>, NP1 such as NPList2. Nadeau et al. (2006) presented an unsupervised system for gazetteer building and named entity ambiguity resolution based on Etzioni et al. (2005) and Collins and Singer (1999) that combined an extracted gazetteer with commonly available gazetteers to achieve F-scores of 88%, 61%, and 59% on MUC-7 (Chinchor and Robinson, 1997) location, person, and organization entities, respectively.\n\nZhang and Elhadad (2013) used shallow syntactic knowledge and inverse document frequency (IDF) for an unsupervised NER system on biology (Kim et al., 2004) and medical (Uzuner et al., 2011) data, achieving 53.8% and 69.5% accuracy, respectively. Their model uses seeds to discover text having potential named entities, detects noun phrases and filters any with low IDF values, and feeds the filtered list to a classifier (Alfonseca and Manandhar, 2002) to predict named entity tags.", "filtered_refids": [["b11", "b22", "b13", "b50"], ["b30", "b1", "b74"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1354, "num_references": 7}
{"corpusid_sectionid": "49215220-s2", "title": "A Survey on Open Information Extraction", "date": "2018-06-14", "section_title": "Learning-based Systems", "section": "The line of work on Open IE begins with TEXTRUNNER (Banko et al., 2007), a self-supervised learning approach consisting of three modules. First, given a small sample of sentences from the Penn Treebank, the learner applies a dependency parser to heuristically identify and label a set of extractions as positive and negative training examples. This data is then used as input to a Naive Bayes classifier which learns a model of trustworthy relations using unlexicalized POS and noun phrase (NP) chunk features. The selfsupervised nature mitigates the need for hand-labeled training data, and unlexicalized features help scale to the multitudes of relations found on the Web. The second component, the extractor, then generates candidate tuples by first identifying pairs of NP arguments and then heuristically designating each word in between as part of a relation phrase or not. Next, each candidate extraction is presented to the classifier, whereupon only those labeled as trustworthy are kept. Restricting to the use of shallow features in this step makes TEXTRUNNER highly efficient. Finally, a redundancy-based assessor assigns a probability to each retained tuple based on the number of sentences from which each extraction was found, thus exploiting the redundancy of information in Web text and assigning higher confidence to extractions that occur multiple times. Figure 2: OLLIE's system architecture (Mausam et al., 2012). OLLIE begins with seed tuples from REVERB, uses them to build a bootstrap learning set, and learns open pattern templates. These are applied to individual sentences at extraction time.\n\nWOE (Wu and Weld, 2010) also learns an open information extractor without direct supervision. It makes use of Wikipedia as a source of training data by bootstrapping from entries in Wikipedia infoboxes, i.e. by heuristically matching infobox attribute-value pairs with corresponding sentences in the article. This data is then used to learn extraction patterns on both POS tags (WOE pos ) and dependency parses (WOE parse ). Former extractor utilizes a linear-chain Conditional Random Field (CRF) to train a model of relations on shallow features which outputs certain text between two NPs when it denotes a relation. Latter approach, in contrast, makes use of dependency trees to build a classifier that decides whether the shortest dependency path between two NPs indicates a semantic relation. By operating over dependency parses, even long-range dependencies can be captured. Accordingly, when comparing their two approaches, Wu and Weld (2010) show that the use of dependency features results in an increase in precision and recall over shallow linguistic features, though, at the cost of extraction speed, hence negatively affecting the scalability of the system. OLLIE (Mausam et al., 2012) follows the idea of bootstrap learning of patterns based on dependency parse paths. However, while WOE relies on Wikipedia-based bootstrapping, OLLIE applies a set of high precision seed tuples from its predecessor system REVERB (see section 2.2) to bootstrap a large training set over which it learns a set of extraction pattern templates using dependency parses (see Figure 2). In contrast to previously presented systems that fully ignore the context of a tuple and thus extract propositions that are not asserted as factual, but are only hypothetical or conditionally true, OLLIE includes a context-analysis step in which contextual information from the input sentence around an extraction is analyzed to expand the output representation by adding attribution and clausal modifiers, if necessary, and thus increasing the precision of the system (see extractions (1) and (2) in Figure 1; for details, see section 2.4). Moreover, OLLIE is the first Open IE approach to identify not only verb-based relations, but also relationships mediated by nouns and adjectives (see extractions (3) and (4) in Figure 1). In that way, it expands the syntactic scope of relational phrases to cover a wider range of relation expressions, resulting in a much higher yield (at comparable precision) as compared to previous systems.\n\nMore recently, Yahya et al. (2014) proposed ReNoun, an Open IE system that entirely focuses on the extraction of noun-mediated relations. Starting with a small set of high-precision seed facts relying on manually specified lexical patterns that are specifically tailored for NPs, a set of dependency parse patterns for the extraction of noun-based relations is learned with the help of distant supervision (Mintz et al., 2009). These patterns are then applied to generate a set of candidate extractions which are assigned a confidence score based on the frequency and coherence of the patterns producing them.", "filtered_refids": [["b20", "b3"], ["b20", "b40"], ["b24", "b42"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 4745, "num_references": 6}
{"corpusid_sectionid": "49215220-s3", "title": "A Survey on Open Information Extraction", "date": "2018-06-14", "section_title": "Rule-based Systems", "section": "The second category of Open IE systems make use of hand-crafted extraction rules. This includes RE-VERB (Fader et al., 2011), a shallow extractor that addresses three common errors of hitherto existing Open IE systems: the output of such systems frequently contains a great many of uninformative extractions (i.e. extractions that omit critical information), incoherent extractions (i.e. extractions where the relational phrase has no meaningful interpretation) and overly-specific relations that convey too much information to be useful in further downstream semantic tasks. REVERB improves over those approaches by introducing a syntactic constraint that is expressed in terms of a simple POS-based regular expres-sion (see Figure 3), covering about 85% of verb-based relational phrases in English text, as a linguistic analysis has revealed. In that way, the amount of incoherent and uninformative extractions is reduced. Moreover, in order to avoid overspecified relational phrases, a lexical constraint is presented which is based on the idea that a valid relational phrase should take many distinct arguments in a large corpus. Besides, while formerly proposed approaches start with the identification of candidate argument pairs, REVERB follows a relation-centric approach by first determining relational phrases that satisfy abovementioned constraints, and then finding a pair of NP arguments for each such phrase. An example output produced by ReVerb can be seen in Figure 1 (6-7). Whereas previously mentioned Open IE systems focus on the extraction of binary relations, commonly leading to extraction errors such as incomplete, uninformative or erroneous propositions, KRAKEN (Akbik and L\u00f6ser, 2012) is the first approach to be specifically built for capturing complete facts from sentences by gathering the full set of arguments for each relational phrase within a sentence, thus producing tuples of arbitrary arity. The identification of relational phrases and their corresponding arguments is based on hand-written extraction rules over typed dependency parses.\n\nEXEMPLAR (Mesquita et al., 2013) applies a similar approach for extracting n-ary relations, using hand-crafted patterns based on dependency parse trees to detect a relation trigger and the arguments connected to it. Based on the task of Semantic Role Labeling (SRL), whose key idea is to classify semantic constituents into different semantic roles (Christensen et al., 2010), it assigns each argument its corresponding role (such as subject, direct object or prepositional object).\n\nA more abstract approach, PROPS, was suggested by , who argue that it is hard to read out from a dependency parse the complete structure of a sentence's propositions, since, amongst others, different predications are represented in a non-uniform manner and proposition boundaries are not easy to detect. Therefore, they introduce a more semantically-oriented sentence representation that is generated by transforming a dependency parse tree into a directed graph which is tailored to directly represent the proposition structure of an input sentence. Consequently, extracting propositions from this novel output format is straightforward. The conversion of the dependency tree into the proposition structure is carried out by a rule-based converter.\n\nPredPatt (White et al., 2016) follows a similar approach. It employs a set of non-lexicalized rules defined over Universal Dependency (UD) parses (Marneffe et al., 2014) to extract predicate-argument structures. In doing so, PredPatt constructs a directed graph, where a special dependency ARG is built between the head token of a predicate and the head tokens of its arguments, while the original UD relations are preserved within predicate and argument phrases. As PredPatt uses language-agnostic patterns on UD structures, it is one of the few Open IE systems that work across different languages.", "filtered_refids": [["b11", "b1"], ["b8", "b23"], [], ["b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3912, "num_references": 5}
{"corpusid_sectionid": "49215220-s5", "title": "A Survey on Open Information Extraction", "date": "2018-06-14", "section_title": "Systems Capturing Inter-Proposition Relationships", "section": "Aforementioned Open IE systems lack the expressiveness needed for a proper interpretation of complex assertions, since they ignore the context under which a proposition is complete and correct. Thus, they do not distinguish between information asserted in a sentence and information that is only hypothetical or conditionally true. For example, extracting the relational tuple the earth; is the center of ; the universe from the sentence \"Early scientists believed that the earth is the center of the universe.\" would be inappropriate, since the input is not asserting it, but only noting that is was believed by early scientists (Mausam, 2016). To properly handle such cases, OLLIE attempts a first solution by additionally extracting an attribution context, denoting a proposition that is reported or claimed by some entity:\n\n( the earth; be the center of ; the university ; AttributedTo believe; Early astronomers)\n\nIn that way, it extends the default Open IE representation of arg 1 , rel, arg 2 with an extra field. Besides, OLLIE pays attention to clausal modifiers, such as: ( Romney; will be elected; President ; ClausalModifier if ; he wins five key states)\n\nBoth types of modifiers are identified by matching patterns with the dependency parse of the sentence. Clausal modifiers are determined by an adverbial-clause edge and filtered lexically (the first word of the clause must match a list of cue terms, e.g. if, when, or although), while attribution modifiers are identified by a clausal-complement edge whose context verb must match one of the terms given in VerbNet's list of common verbs of communication and cognition (Mausam et al., 2012). A similar output is produced by OLLIES's successor OPENIE4 (Mausam, 2016), which combines SRLIE (Christensen et al., 2010) and RELNOUN (Pal and Mausam, 2016). Former is a system that converts the output of a SRL system into an Open IE extraction by treating the verb as the relational phrase, while taking its role-labeled arguments as the Open IE argument phrases related to the relation. Latter, in contrast, represents a rule-based Open IE system that extracts noun-mediated relations, thereby paying special attention to demonyms and compound relational nouns. In addition, OPENIE4 marks temporal and spatial arguments by assigning them a T or S label, respectively. Lately, its successor OPENIE 5.0 was released 1 . It integrates BONIE (Saha et al., 2017) and OpenIEListExtractor 2 . While the former focuses on extracting tuples where one of the arguments is a number or a quantity-unit phrase, the latter targets the extraction of propositions from conjunctive sentences.\n\nSimilar to OLLIE, Bast and Haussmann (2013), who explore the use of contextual sentence decomposition (CSD) for Open IE, advocate to further specify propositions with information on which they depend. Their system CSD-IE is based on the idea of paraphrasing-based approaches described in section 2.3. Using a set of hand-crafted rules over the output of a constituent parser, a sentence is first split into sub-sequences that semantically belong together, forming so-called \"contexts\". Each such context now contains a separate fact, yet it is often dependent on surrounding contexts. In order to preserve such inter-proposition relationships, tuples may contain references to other propositions. However, as opposed to OLLIE, where additional contextual modifiers are directly assigned to the corresponding relational tuples, Bast and Haussmann (2013) represent contextual information in the form of separate, linked propositions. To do so, each extraction is given a unique identifier that can be used in the argument position of an extraction for a later substitution with the corresponding fact by a downstream application. An example for an attribution is shown below (Bast and Haussmann, 2013): #1: The Embassy; said; that #2 #2: 6,700 Americans; were; in Pakistan.\n\nAnother current approach that captures inter-proposition relationships is proposed by Bhutani et al. (2016), who present a nested representation for Open IE that is able to capture high-level dependencies, allowing for a more accurate representation of the meaning of an input sentence. Their system NESTIE uses bootstrapping over a dataset for textual entailment to learn both binary and nested triple representations for n-ary relations over dependency parse trees. These patterns can take on the form of binary triples arg 1 ; rel; arg 2 or nested triples such as arg 1 ; rel; arg 2 ; rel 2 ; arg 3 for n-ary relations. Using a set of manually defined rules, contextual links between extracted propositions are inferred from the dependency parse in order to generate a nested representation of assertions that are complete and closer in meaning to the original statement. Similar to OLLIE, contextual links are identified as clausal complements, conditionals and relation modifiers. Linked propositions are represented by arguments that refer to the corresponding propositions using identifiers, e.g. (Bhutani et al., 2016) (Gashteovski et al., 2017), another recent Open IE system, is built on top of ClausIE, a system that was found to often produce overly specific extractions. Such overly specific constituents that combine multiple, potentially semantically unrelated propositions in a single relational or argument phrase generally hurt the performance of downstream semantic applications, such as question answering or textual entailment. In fact, those approaches benefit from extractions that are as compact as possible. Therefore, MinIE aims to minimize both relational and argument phrases by identifying and removing parts that are considered overly specific. For this purpose, MinIE provides four different minimization modes which differ in their aggressiveness, thus allowing to control the trade-off between precision and recall. Moreover, it semantically annotates extractions with information about polarity, modality, attribution and quantities instead of directly representing it in the actual extractions, as the following example shows (Gashteovski et al., 2017):\n\n\"Pinocchio believes that the hero Superman was not actually born on beautiful Krypton.\" with + and -signifying positive and negative polarity, respectively.\n\nIn that way, the output generated by MinIE is further reduced to its core constituents, producing maximally shortened, semantically enriched extractions.\n\nTo further enhance the expressiveness of extracted propositions and sustain their interpretability in downstream artificial intelligence tasks, Cetto et al. (2018) propose Graphene, an Open IE framework that uses a set of hand-crafted simplification rules to transform complex natural language sentences into clean, compact structures by removing clauses and phrases that present no central information from the input and converting them into stand-alone sentences. In that way, a source sentence is transformed into a hierarchical representation in the form of core facts and accompanying contexts (Niklaus et al., 2016). In addition, inspired by the work on Rhetorical Structure Theory (Mann and Thompson, 1988), a set of syntactic and lexical patterns is used to identify the rhetorical relations by which core sentences and their associated contexts are connected in order to preserve their semantic relationships and return a set of semantically typed and interconnected relational tuples (see extractions (15-17) in Figure 1). Graphene's extraction workflow is illustrated in Figure 5.", "filtered_refids": [["b21"], [], [], ["b20", "b27", "b21", "b30", "b8"], ["b4"], ["b14", "b5"], [], [], ["b26", "b7", "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 7506, "num_references": 12}
{"corpusid_sectionid": "237099284-s3", "title": "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey", "date": 2021, "section_title": "Dialog State Tracking Challenge (DSTC)", "section": "The dialog state tracking challenge (DSTC) is a series of dialogue related challenges that serves as a common test and evaluation suite for dialogue state tracking (Williams and Young, 2007;Williams et al., 2013Williams et al., , 2016b. The challenge was later renamed as dialog system technology challenge to accommodate various other dialogue related tasks. The most widely used datasets in the context of the DST challenge are DSTC2 and DSTC3.\n\nDSTC2 and DSTC3. The dialog state tracking challenges 2 (DSTC2 - (Henderson et al., 2014a)) and 3 (DSTC3 - (Henderson et al., 2014b)) are human-machine conversation dialogue datasets collected using Amazon Mechanical Turk, respectively for the restaurant and the tourist domain.\n\nDSTC2 is a spoken dialogue dataset consisting of automatic speech recognition (ASR) hypotheses and turn-level semantic labels along with the transcriptions. The dataset consists of 1,612 dialogues for training, 506 dialogues for development, and 1,117 dialogues for testing. DSTC3 aims to evaluate DST models on their ability to track unseen slot values and on their adaptability to a new domain. For this purpose, the dataset does not contain training dialogues and consists of 2,265 dialogues for testing. Typically, the models trained on the DSTC2 dataset were evaluated with the DSTC3 dataset to estimate their performance.", "filtered_refids": [["b35", "b32", "b36"], ["b6", "b7"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1355, "num_references": 5}
{"corpusid_sectionid": "237099284-s10", "title": "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey", "date": 2021, "section_title": "Static Ontology DST Models", "section": "The main distinguishing characteristic of DST models, in our opinion, is their capacity to predict dialogue states either from a fixed set of slot-values (i.e., from a static ontology) or from a possible open set of slot-values (i.e., from a dynamic ontology).\n\nStatic ontology models rely on a fixed ontology to predict the dialogue state. This means that the set of slot-values is predefined, and that a model can only predict for those predefined values. These models typically consist of an input layer that transforms each input token into an embedding, of an encoder layer that encodes the input to a hidden state h t , and of an output layer that predicts the slot value based on h t . Considering that the set of possible slot-values is predefined, there are two approaches used for the output layer: i) a feed-forward layer, which receives the input representation and produces scores equal to the # of slot-values; ii) an output layer that receives both the input and the slot-value representations and compares them with each of the slot-value representations providing a score for each slot-value. The obtained score can then be normalized using a non-linear activation function, either softmax, to get a probability distribution over all the slot-value pairs, or sigmoid, to get the individual probability for each slot-value pair. Figure 2 shows the standard architecture of the two approaches.\n\nWe now review few challenges that have been addressed in static ontology models, including delexicalization, data-driven DST, parameter sharing, latency in prediction, and the use of pre-trained language models. Performances of the systems are all reported in Table 2.\n\nDelexicalization. Delexicalization is an effective approach adopted to counter imbalanced training data for slot-values. In this regard, the slot values in the input are replaced with labels corresponding to slot names. For instance, I want Chinese food is delexicalised as I want F.VALUE F.SLOT. It has to be noted that replacing slot-values needs a semantic dictionary listing the possible values for each slot. (Henderson et al., 2014c; has proposed a word-based DST with recurrent neural networks that uses delexicalization on top of an input representation based on Automatic Speech Recognition. This allows to improve the system robustness with respect to the user expressions mentioning slot values.\n\nData-driven DST. Although delexicalization showed to be effective, it requires additional manual feature engineering. An alternative, data-driven methodology, was proposed by the neural belief tracker (NBT) (Mrk\u0161i\u0107 et al., 2017a). Instead of delexicalizing the input, a separate module was learned to represent the slot-value pairs. Then, the slot-value representation and the input representation are passed through a binary decision maker before applying softmax activation. Similarly, a fully statistical NBT was proposed by (Mrk\u0161i\u0107 and Vuli\u0107, 2018), where a statistical update function replaces the rule-based update mechanism in NBT. The experimental results showed the statistical update function to outperform the rule-based update.\n\nParameter sharing. While the previous models consist of a separate encoder for each slot whose values have to be predicted, the DST efficiency crucially depends on the number of model parameters. In this direction, (Ren et al., 2018) proposed   StateNet, a DST sharing the parameters for all slots, thus reducing the number of model parameters.\n\nStateNet combines a n-gram input feature representation with a slot representation, and uses long short term memory (LSTM) to encode them into a single vector. The value representation is then compared with the encoded vector to obtain the score for each slot-value. A semantically specialised Paragram-SL999 (Wieting et al., 2015) was used to encode the tokens. Compared with fully statistical NBT, StateNet achieves high performance even with a rule-based update function.", "filtered_refids": [[], [], [], ["b8"], ["b19", "b18"], ["b26"], ["b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3949, "num_references": 5}
{"corpusid_sectionid": "237099284-s11", "title": "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey", "date": 2021, "section_title": "RNN and latency in DST.", "section": "A relevant issue for DST models is prediction time, due to the number of dialogue states they have to consider at each dialogue turn. (Zhong et al., 2018) combined both a shared representation and a slot-specific representation in the Global-Locally Self Attentive Dialogue State Tracker (GLAD). The GLAD model consists of an RNN-based global module, to learn global features, and a local module that learns slot-specific features. The representations of slot-values and user input are then scored using a scoring module that predicts their probability. However, GLAD needs an RNN for each slot-value representation, this way increasing the latency of the model. Further improvements on latency were proposed in GCE, Globally-Conditioned Encoder (Nouri and Hosseini-Asl, 2018), which uses only the global encoder, and in (Balaraman and Magnini, 2019), proposing a Global encoder and Slot-Attentive decoders (G-SAT). The G-SAT model uses an RNN to encode the user input and slot-specific feedforward networks to represent the slot-values.\n\nEncoders based on pre-trained LM. The use of pre-trained language models, such as BERT (Bidirectional Encoder Representation from Transformers) (Devlin et al., 2019), is meant to increase the DST capacity to capture the semantics of slot and values names. (Lee et al., 2019) proposed a slot-utterance matching belief tracker (SUMBT) using BERT to encode slots, user input, and slotvalues. The representations of the slots and of the user input are combined using multi-head attention (Vaswani et al., 2017) to obtain the input representation of the model, and then compared with the slot-value representation to obtain the probability.", "filtered_refids": [["b21", "b42"], ["b11", "b28"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1674, "num_references": 4}
{"corpusid_sectionid": "9089503-s2", "title": "A Review of Corpus-based Statistical Models of Language Variation", "date": "2015-10-01", "section_title": "Modeling phonetic variation", "section": "This vein of corpus-based language variation research first started with studies on phonetic variationprobably because phonetic features are readily quantifiable. Some of the pioneering works on English pronunciation variation were completed around the turn of the century (Bell et al. 2009;Fosler-Lussier and Morgan 1999;Gregory, et al. 1999;Jurafsky et al. 1998Jurafsky et al. , 2001a, among others), with phonetic data from the Switchboard corpus of telephone conversations (Godfrey et al. 1992), which contains 240 hours of speech (of which 4 hours are phonetically transcribed and used in the statistical models).\n\nThe studies above mostly examined word duration and vowel pronunciation (full vs. reduced) as parameters of pronunciation variation. In addition to describing the general picture of variation, these studies were also deeply interested in the effects of probabilistic factors (e.g. word frequency, contextual probability, etc) on pronunciation variation. The results presented in these studies are cited as empirical support for the general claim that probabilistic relations have profound influence on the representation and production of words in speech (Jurafsky et al., 2001b) Later on, with the completion of the Buckeye corpus (Pitt et al., 2007), which contains 40 hours of phonetically transcribed conversational speech, another batch of corpus-based phonetic variation studies appeared (Johnson, 2004;Gahl et al., 2012;Yao, 2009Yao, , 2011. Since the Buckeye corpus is recorded in a studio, the recording quality is high enough to warrant automatic measurement of VOT (Yao, 2009) and vowel formants . This allows for modeling of gradient vowel dispersion, measured by the distance between a specific vowel token from the center of the vowel space on a F1-F2 plane (Bradlow et al., 1996). Furthermore, some of the variation studies based on the Buckeye corpus (Gahl et al., 2012;Yao, 2011) focused on the effects of a particular lexical measure called phonological neighborhood density. Phonological neighborhood density refers to the number of similar-sounding words given a specific target word. Thus, the models built in these studies had one critical predictor (i.e. phonological neighborhood density), and all the other non-neighborhood predictors were included as control variables. Results from these studies revealed the effects of phonological neighborhood structure in word production when all other factors that could also influence word production were statistically controlled.\n\nIn addition to English, corpus-based pronunciation variation research has also been conducted in other languages (Dutch: Pluymaekers et al., 2005, among others;French: Meunier and Espesser, 2011;Yao and Meunier, 2014;Taiwan Southern Min: Myers and Li, 2009).", "filtered_refids": [["b12", "b10", "b9", "b13", "b7", "b1"], ["b11", "b14", "b22", "b23", "b8", "b2", "b17"], [null, "b26"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2777, "num_references": 15}
{"corpusid_sectionid": "9089503-s3", "title": "A Review of Corpus-based Statistical Models of Language Variation", "date": "2015-10-01", "section_title": "Modeling syntactic variation", "section": "The work on modeling syntactic variation started later than the work on modeling phonetic variation. Most of the pioneering works were done by Bresnan and her colleagues at Stanford (Bresnan, 2007;Bresnan et al., 2007;Bresnan and Ford, 2010;Tily et al., 2009;Wolk et al. 2011, etc) on dative variation (e.g. I gave John a book vs. I gave a book to John) and genitive variation (e.g. John's book vs. the book of John) in English. For the American English data, Bresnan and colleagues also used the Switchboard corpus. Since syntactic variation has a discrete set of variants (i.e. different sentence forms), the phenomenon is modelled by generalized regression models. Bresnan and colleagues' work showed that the choice of the surface form under investigation was predictable from a set of factors relating to different components in the local sentence (e.g. semantic type of the verb, NP accessibility, pronominality, definiteness, syntactic complexity, etc) and the context (e.g. presence of parallel structures). When taking all the factors into consideration, Bresnan et al.'s models can correctly predict the surface dative/gentive form in more than 90% of the cases (compare with a baseline accuracy around 79%). Variation patterns revealed in Bresnan et al.'s works were later confirmed in behavioral experiments (e.g. Bresnan and Ford, 2010).\n\nInspired by Bresnan and colleagues' work on English syntactic variation, there have also been a few studies that apply a similar modeling approach to the study of syntactic variation in Chinese languages (Cantonese: Starr, 2015;Mandarin: Yao, 2014;Yao and Liu, 2010).\n\nIn particular, Yao and colleagues (Yao, 2014;Yao and Liu, 2010) investigated both dative variation and BA-form variation in written Mandarin using data from the Academia Sinica corpus (Chen et al., 1996). Sentence patterns involved in Mandarin dative-variation (e.g. \u6211\u9001\u5c0f \u5f20\u4e00\u672c\u4e66 'I gave Xiaozhang a book' vs. \u6211\u9001\u4e00\u672c \u4e66\u7ed9\u5c0f\u5f20 'I gave a book to Xiaozhang' vs. \u6211\u628a\u4e00 \u672c\u4e66\u9001\u7ed9\u5c0f\u5f20 'I (BA) a book gave to Xiaozhang') are more complicated than those in English. In addition to the two dative constructions similar to those in English, Mandarin Chinese also allows the direct object to be preposed before the verb. Yao and Liu' work showed that the three-way dative variation in Mandarin Chinese can be modeled by a hierarchy of two models: one on the upper level for the pre-verbal vs. post-verbal distinction and the other on the lower level for the dative vs. double object distinction. Yao and Liu' models raise the prediction accuracy by 27% (upper level) and 7% (lower level) compared to the baseline accuracy levels.\n\nFurthermore, to understand the general properties of the pre-verbal vs. post-verbal word order variation, Yao also built general models on syntactic variation between BA and non-BA sentences. The results from this study showed that the surface word order in Mandarin Chinese is most significantly influenced by the prominence (accessibility, definiteness, etc) and length of the NP, as well as the presence of a similar word order in the nearby context (i.e. parallel structure).", "filtered_refids": [["b4", "b20", null, "b5", "b3"], [null, "b25", "b18"], ["b24", "b6", "b25"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 3101, "num_references": 11}
{"corpusid_sectionid": "262460726-s2", "title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning", "date": "2023-09-22", "section_title": "Prompts for Encoder-only Transformer Models (BERT)", "section": "Before the advent of in-context learning, some research efforts have been devoted to studying how to design effective prompts to enhance the performance of BERT models.As depicted in Figure 2, prompts in BERT are usually combined with input to form a cloze-style structure, while for transformer decoder-based models, prompts are more flexible.\n\nNumerous studies have investigated prompt design in BERT.In the work by (Jiang et al., 2020), the authors proposed heuristic-based approaches for designing discrete prompts.Dependency parsing is employed to identify useful prompts from Wikipedia.In (Gao et al., 2021), the authors utilized T5 as a prompt generator with a beam search to create a set of diversified prompts.They then used D dev to select a single prompt with the best performance.In (Shin et al., 2020), a gradient-based prompt search approach was proposed, wherein each prompt token is learned by directly optimizing LMs on the downstream task.\n\nIn addition to prompt designing strategies, other research work focuses on enriching the prompt can-didates and ensembling the output from multiple prompts for the same input.To enrich prompts, (Jiang et al., 2020) employed back-translation to paraphrase prompts.Building on this work, (Haviv et al., 2021) trained a separate BERT model to rewrite prompts using the nearest BERT vector embedding.\n\nThe concept of in-context learning originates from the work by (Brown et al., 2020).However, BERT models can also perform similar tasks by using a single token as output.For example, France's capital is [MASK].\n\nOnly the output for the [MASK] position is used for inference.This characteristic enables the ensemble of answers from different prompts, although it is not apparent for similar practices in GPT-style models.In (Jiang et al., 2020), the authors proposed rank-based ensemble and optimized ensemble methods to aggregate answers generated from different prompts.\n\nAmong the studies designing prompts for BERT models, the majority focus on discrete prompts (i.e., hard prompts).To the best of our knowledge, we did not find any work attempting to generate continuous prompts.In general, optimizing prompts in BERT brings only marginal improvements to the original model.Given the size and structure of BERT, it is more favorable to fine-tune on downstream tasks.", "filtered_refids": [[], ["b24", "b8", "b4"], ["b6", "b8"], [null, "b0"], ["b8"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2327, "num_references": 8}
{"corpusid_sectionid": "262460726-s4", "title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning", "date": "2023-09-22", "section_title": "Continuous Prompt", "section": "Another line of research has focused on optimizing soft prompts, which eliminate the constraint that prompts have to be natural language.Soft prompts can be learned and optimized directly within the same language model.The key difference between soft prompt tuning and fine-tuning is that prompt tuning typically fixes the weights of the language model and only performs gradient updates on the network that generates the prompt.Prefix-Tuning (Li and Liang, 2021) is one of the early works that tunes prompts on GPT-2 with a small amount of data per task, achieving comparable performance to the full data fine-tuning setting.Prefix-Tuning does not use a separate network; instead, it utilizes the same transformer network but only optimizes the input embedding of the prompt.In P-Tuning V1 (Liu et al., 2021b) and V2 (Liu et al., 2022), the authors employ a separate LSTM network to generate the input prompt for the language model.While using soft prompts provides more flexibility in prompt design, it requires access to either the weights of language models or the ability to input vectors into language models.As recent language models are hosted as cloud services and large language models are difficult to access via vector inputs, this practice becomes less feasible when using GPT-3 or PaLM (Chowdhery et al., 2022).", "filtered_refids": [["b16", "b12", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1325, "num_references": 3}
{"corpusid_sectionid": "262460726-s5", "title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning", "date": "2023-09-22", "section_title": "Few-Shot Learning", "section": "In the GPT paper (Brown et al., 2020), fewshot learning demonstrates strong NLP capabilities across various benchmarks.As the title suggests, Language Models are Few-Shot Learners.In the few-shot setting, a task description along with a few examples are presented to the model, which is then asked to complete the task for an unseen example.Numerous studies have been conducted to optimize few-shot examples and prompts to enhance performance.In (Liu et al., 2021a), the authors discovered that GPT-3 generally performs better when in-context examples are similar to the test examples.As a result, they proposed an incontext example algorithm based on example similarities.Similarity is measured using RoBERTa embedding distance in Euclidean space or cosine distance.Other works, such as (Rubin et al., 2021) and (Gutierrez et al., 2022), have adopted similar example selection logic and demonstrated better performance over randomly selected examples.In addition to example selection methods, research efforts like (Wu et al., 2022) and (Kumar and Talukdar, 2021) have been made to optimize the rank and order of retrieved examples.While few-shot learning exhibits remarkable performance, according to the no free lunch(NFL) theorem (Wolpert andMacready, 1995, 1997), providing examples inevitably introduces bias to the prediction algorithm.In cases where out-ofdistribution samples occur, applying few-shot learning can hinder the inference process.", "filtered_refids": [["b15", "b10", "b28", null, "b22", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1452, "num_references": 6}
{"corpusid_sectionid": "261822277-s9", "title": "Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text", "date": "2023-09-14", "section_title": "Methods", "section": "In this section, we report on the various methods proposed for detecting ChatGPT-generated text.The scope of this review does not include the evaluation or comparison of the results obtained from these methods.This limitation primarily arises from the absence of a common experimental setup and the utilization of different datasets and metrics.Table 2 provides an overview of these recent approaches.Some previous works have utilized transformerbased models to classify text generated by Chat-GPT and human-written text, as demonstrated by Mitrovi\u0107 et al. (2023).Their approach consists of two components: a detection model and a framework to explain the decisions made by this model.They first fine-tune an uncased version of Distil-BERT (Sanh et al., 2019) and then employ SHAP to provide local explanations in the form of feature importance scores to gain insights into the significance of different input features of the model's results.As a baseline comparison, they implement a perplexity-based classifier that categorizes text based on its perplexity score, where GPT-2 is used for calculating perplexity scores.Their results show that the DistilBERT-based detector outperforms the perplexity-based classifier.However, its performance decreases when considering the rephrased dataset by ChatGPT.\n\nIn Liao et al. (2023), different models are proposed to detect medical text generated by Chat-GPT: a fine-tuned BERT model (Devlin et al., 2019), a model based on Classification and Regression Trees (CART), an XGBoost model (Chen and Guestrin, 2016) and a perplexity classifier that utilizes BioGPT (Luo et al., 2022) for calculating text perplexity.Predictions by the BERT model are explained by visualizing the local features of the samples, where it can be seen that using conjuncts is an essential feature for the model classifying a medical text as machine-generated.Liu et al. (2023) fine-tune RoBERTa to detect argumentative essays generated by different GPT models, including ChatGPT, and evaluate its performance on document, paragraph, and sentencelevel classification.The essays are broken down into paragraphs and sentences for paragraph and sentence-level classification.They train and compare the performance of SVM models using different linguistic features.These models serve as a baseline to compare with the RoBERTa model and to understand which linguistic features differentiate between human and ChatGPT-generated text.Guo et al. (2023) implement a machine learning and deep learning-based detector.They utilize a logistic regression model trained on the GLTR Test-2 dataset (Gehrmann et al., 2019) and two deep classifiers based on fine-tuning the pre-trained transformer model RoBERTa.One deep classifier is designed explicitly for single-text detection, while the other is intended for QA detection.The authors construct various training and testing datasets versions to assess the models' robustness.They create full-text, sentence-level, and mixed subsets of the collected corpus.Each subset has both a raw version and a filtered version where prominent indicating words referring to humans (such as \"Nope\" and \"Hmm\") or ChatGPT words (such as \"AI assistant\") are removed.The evaluation of the models reveals that the RoBERTa-based models outper- form GLTR in terms of performance and exhibit more robustness against interference.Moreover, the RoBERTa-based models are not influenced by indicating words.\n\nBuilding upon the work of Guo et al. (2023), Antoun et al. (2023a) propose an approach for developing robust detectors able to detect ChatGPT-generated text in different languages, with a focus on French.Their approach consists of fine-tuning pre-trained transformer-based models on English, French, and multilingual datasets.They train RoBERTa and ELECTRA (Clark et al., 2020) models on the English dataset, CamemBERT (Martin et al., 2020) and CamemBERTa (Antoun et al., 2023b) on the French datasets and XLM-R (Conneau et al., 2020) on the combined English and French dataset.They evaluate the robustness of these models against adversarial attacks, such as replacing characters with homoglyphs and adding misspelled words.Considering in-domain text, their results show that French models perform well in detecting machine-generated text.Still, they were outperformed by the English models, while XLM-R provides the best and most resilient performance against adversarial attacks for both English and French.However, this performance decreases when evaluated on out-of-domain text.\n\nAnother method proposed for detecting ChatGPT-generated text is a metric-based approach proposed by Vasilatos et al. (2023) to detect machine-generated student assignments by calculating perplexity scores using GPT-2.They show that having category-wise thresholds (derived from dataset metadata) results in better detection performance than only having one threshold value.", "filtered_refids": [["b43", "b37"], ["b34", "b14", "b20", "b32", null, "b19", "b7"], ["b10", "b9", "b20", "b35", "b1", "b2"], ["b48"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 4894, "num_references": 16}
{"corpusid_sectionid": "261822277-s10", "title": "Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text", "date": "2023-09-14", "section_title": "Analysis of Human and", "section": "ChatGPT-Generated Text\n\nThe textual characteristics of ChatGPT-generated text as well as its syntactic and linguistic features, are of significant focus in the works we reviewed.These linguistic and stylistic features are compared to the human-written texts in the datasets.In this section, we summarize and provide an overview of the findings of such analyses for the different domains and datasets we reviewed.\n\n\u2022 Medical domain: Medical texts generated by ChatGPT have lower text perplexity and are more fluent, neutral, positive, and logical but more general in content and language style, while medical texts written by humans are more diverse and specific (Liao et al., 2023).\n\n\u2022 English argumentative essays: ChatGPT produces syntactically more complex sentences than English language learners, but ChatGPT-authored essays tend to have lower lexical diversity (Liu et al., 2023).\n\n\u2022 Multi-domain question answering: Chat-GPT writes in an organized and neutral way, offers less bias and harmful information, and refuses to answer questions where it believes it does not know.ChatGPT answers are formal, less emotional, and more objective than human answers (Guo et al., 2023).\n\n\u2022 Scientific abstracts: ChatGPT has a better choice of vocabulary, can generate more unique words, uses more connecting words, and has fewer grammatical errors (Yu et al., 2023).\n\n\u2022 Language-agnostic characteristics: The linguistic and syntactic characteristics of ChatGPT-generated text tend to be languageagnostic.Text generated in different languages, such as English, French, and Chinese, shows similar characteristics where ChatGPT tends to produce didactic and impersonal text without errors.Such errors can indicate human text, like grammatical, spelling or punctuation mistakes (Antoun et al., 2023a;Guo et al., 2023).", "filtered_refids": [[], [], [null], ["b32"], ["b20"], ["b52"], ["b1", "b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1810, "num_references": 6}
{"corpusid_sectionid": "252683270-s5", "title": "A Decade of Knowledge Graphs in Natural Language Processing: A Survey", "date": "2022-09-30", "section_title": "Classification Scheme and Data Extraction", "section": "According to our RQs, the included papers had to be categorized with respect to three facets: task, research type, and contribution. Established classification schemes from Wieringa et al. (2006) and Shaw (2003) were adapted for the research and contribution type as presented in Appendix A. For classifying tasks, we constructed a task taxonomy, following the iterative procedure suggested by Petersen et al. (2008), in which an initial classification scheme derived from keywords continuously evolves through adding, merging, or splitting categories during the classification process. Our task taxonomy is based on existing schemes from Paulheim (2017) Once the initial schemes were set up, all papers were sorted into the classes as part of the data extraction process. The 507 included studies were divided between two of the authors. In regular sessions, they discussed changes to the classification schemes or clarified uncertain labels. While each paper got assigned one label for the research type assigned, multiple labels were possible with regard to tasks and contributions. To assess the reliability of the inter-annotator agreement, the two authors independently classified a random sample of 50 papers. We calculated Cohen's Kappa coefficient of these annotations for each facet (Cohen, 1960). ", "filtered_refids": [["b9", "b61", "b50", "b40"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1308, "num_references": 4}
{"corpusid_sectionid": "252683270-s8", "title": "A Decade of Knowledge Graphs in Natural Language Processing: A Survey", "date": "2022-09-30", "section_title": "<HDU", "section": "Another finding of the data extraction process concerns the diverse application areas of KGs in NLP. We observed that the number of domains explored in the research literature grew rapidly in parallel with the annual count of papers. To reveal the great variety of areas, we list all 20 discovered domains and their subdomains in Table 6 in the Appendix. In Figure 3, the ten most frequent domains are displayed. It is striking that health is by far the most prominent domain. The latter appears more than twice as often as the scholarly domain, which ranks second. Other popular areas are engineering, business, social media, or law. In view of the domain diversity, it becomes evident that KGs are naturally applicable to many different contexts, as has been stated in prior work (Abu-Salih, 2021;Ji et al., 2021;Zou, 2020).  omy shown in Figure 1. The two top-level categories consist of knowledge acquisition and knowledge application. Knowledge acquisition contains NLP tasks to construct KGs from unstructured text (knowledge graph construction) or to conduct reasoning over already constructed KGs (knowledge graph reasoning). KG construction tasks are further split into two subcategories: knowledge extraction, which is used to populate KGs with entities, relations, or attributes, and knowledge integration, which is used to update KGs. Knowledge application, being the second top-level concept, encompasses common NLP tasks, which are enhanced through structured knowledge from KGs. As might be expected, the frequency of occurrence in the literature for the tasks from our taxonomy varies greatly. While Table 2 gives an overview of the most popular tasks, Figure 5 compares their popularity over time. Figure 4 displays the number of detected domains for the most prominent tasks. It shows that certain tasks are adopted to more domain-specific contexts than others. ", "filtered_refids": [["b70", null, "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 1880, "num_references": 3}
{"corpusid_sectionid": "252683270-s9", "title": "A Decade of Knowledge Graphs in Natural Language Processing: A Survey", "date": "2022-09-30", "section_title": "Knowledge Graph Construction", "section": "The task of entity extraction is a starting point in constructing KGs and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020). Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a). A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018). Entity linking is a task of linking entities recognized in some text to already existing entities in KGs (Moon et al., 2018;. Since synonymous or similar entities often exist in different KGs or in different languages, entity alignment can be performed to reduce redundancy and repetition in future tasks (Gangemi et al., 2016;Chen et al., 2018). Coming up with the rules and schemes of KGs, i.e., their structure and format of knowledge presented in it, is done with the task of ontology construction (Haussmann et al., 2019).", "filtered_refids": [["b6", "b46", "b29", "b32", "b19", "b1", "b65", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1025, "num_references": 8}
{"corpusid_sectionid": "252762171-s6", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Triples: Crowd-Sourcing of Facts", "section": "Popular large-scale KGs, like Wikidata (Vrandecic and Kr\u00f6tzsch, 2014) and DBpedia (Auer et al., 2007) are the products of continuous crowdsourcing efforts. Both of these examples are closely related to Wikipedia, where the top five languages (English, Cebuano, German, Swedish, and French) constitute 35% of all articles on this platform. 3 It can be said that Wikipedia is Euro-centric in tendency. Moreover, the majority of authors are white males. 4 As a result, the data transport a particular homogeneous set of interests and knowledge (Beyt\u00eda et al., 2022;Wagner et al., 2015). This sampling bias affects the geospatial coverage of information (Janowicz et al., 2018) and leads to higher barriers for female personalities to receive a biographic entry (Beyt\u00eda et al., 2022). In an experiment, Demartini (2019) asked crowd contributors to provide a factual answer to the (politically charged) question of whether or not Catalonia is a part of Spain. The diverging responses indicated that participants' beliefs of what counts as true differed largely. This is an example of bias that is beyond a subliminal psychological level. In this case, structural aspects like consumed media and social discourse play an important role. To counter this problem, Demartini (2019) suggests actively asking contributors for evidence supporting their statements, as well as keeping track of their demographic backgrounds. This makes underlying motivations and possible sources for bias traceable.", "filtered_refids": [["b42", "b6", "b85", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1486, "num_references": 4}
{"corpusid_sectionid": "252762171-s7", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Ontologies: Manual Creation of Rules", "section": "Ontologies determine rules regarding allowed types of entities and relations or their usage. They are often hand-made and a source of bias (Janowicz et al., 2018) due to the influence of opinions, motivations, and personal choices (Keet, 2021): Factors like scientific opinions (e.g., historical ideas about race), socio-culture (e.g., how many people a person can be married to), or political and religious views (e.g., classifying a person of type X as a terrorist or a protestor) can proximately lead to an encoding of social bias. Also structural constraints like the ontologies' granularity levels can induce bias (Keet, 2021). Furthermore, issues can arise from the types of information used to characterize a person entity. Whether one attributes the person with their skin color or not could theoretically determine the emergence of racist bias in a downstream application (Paparidis and Kotis, 2021). Geller and Kollapally (2021) give a practical example for detection and alleviation of ontology bias in a real-world scenario. The authors discovered that ontological gaps in the medical context lead to an under-reporting of racespecific incidents. They were able to suggest countermeasures based on a structured analysis of real incidents and external terminological resources.", "filtered_refids": [["b45", "b42", "b30", "b64"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1288, "num_references": 4}
{"corpusid_sectionid": "252762171-s8", "title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "date": "2022-10-07", "section_title": "Extraction: Automated Extraction of Information", "section": "Natural language processing (NLP) methods can be used to recognize and extract entities (named entity recognition; NER) and their relations (relation extraction; RE), which are then represented as [head entity, relation, tail entity] tuples (or as [subject, predicate, object], respectively). Mehrabi et al. (2020) showed that the NER system CoreNLP  exhibits binary gender bias. They used a number of template sentences, like \"<Name> is going to school\" or \"<Name> is a person\" using male and female names 5 from 139 years of census data. The model returned more erroneous tags for female names. Similarly, Mishra et al. (2020) created synthetic sentences from adjusted Winogender (Rudinger et al., 2018) templates with names associated with different ethnicities and genders. A range of different NER systems were evaluated (bidirectional LSTMs with Conditional Random Field (BiLSTM CRF) (Huang et al., 2015) on GloVe (Pennington et al., 2014), ConceptNet (Speer et al., 2017) and ELMo (Peters et al., 2017) embeddings, CoreNLP, and spaCy 6 NER models). Across models, nonwhite names yielded on average lower performance scores than white names. Generally, ELMo exhibited the least bias. Although ConceptNet is debiased for gender and ethnicity 7 , it was found to produce strongly varied accuracy values. Gaut et al. (2020) analyzed binary gender bias in a popular open-source neural relation extraction (NRE) model, OpenNRE (Han et al., 2019). For this purpose, the authors created a new dataset, named WikiGenderBias (sourced from Wikipedia and DBpedia). All sentences describe a gendered subject with one of four relations: spouse, hypernym, birthData, or birthPlace (DBpedia mostly uses occupation-related hypernyms). The most notable bias found was the spouse relation. It was more reliably predicted for male than female entities. This observation stands in contrast to the predominance of female instances with spouse relation in WikiGenderBias. The authors experimented with three different mitigation strategies: downsampling the training data to equalize the number of male and female instances, augmenting the data by artificially introducing new female instances, and finally word embedding debiasing (Bolukbasi et al., 2016). Only downsampling facilitated a reduction of bias that did not come at the cost of model performance.\n\nNowadays, contextualized transformer-based en-coders are used in various NLP applications, including NER and NRE. Several works have analyzed the various societal biases encoded in large-scale word embeddings (like word2vec (Mikolov et al., 2013;Bolukbasi et al., 2016) or BERT (Devlin et al., 2019;Kurita et al., 2019)) or language models (like GPT-2 (Radford et al., 2019;Kirk et al., 2021) and GPT-3 (Brown et al., 2020;Abid et al., 2021)). Thus, it is likely that these biases also affect the downstream tasks discussed here.  used two types of tasks to analyze bias in BERT-based RE on the newly created Wiki80 and TACRED (Zhang et al., 2017) benchmarks. For the first task, they masked only entity names with a special token (masked-entity; ME), whereas for the second task, only the entity names were given (onlyentity; OE). The model maintained higher performances in the OE setting, indicating that the entity names were more informative of the predicted relation than the contextual information. This hints at what the authors call semantic bias.\n\nA Note on Reporting Bias Generally, when extracting knowledge from text, one should be aware that the frequency with which facts are reported is not representative of their real-world prevalence. Humans tend to mention only events, outcomes, or properties that are out of their perceived ordinary (Gordon and Van Durme, 2013) (e.g., \"a banana is yellow\" is too trivial to be reported). This phenomenon is called reporting bias and likely stems from a need to be as informative and non-redundant as possible when sharing knowledge.", "filtered_refids": [["b67", "b60", "b56", "b40", "b78", "b28", "b9", "b66", null, "b73", "b37"], ["b90", "b9", "b47", "b50", "b59", null, "b19", "b69", "b0"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 3933, "num_references": 20}
{"corpusid_sectionid": "252992688-s3", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Output Uncertainty", "section": "Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schr\u00f6der et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).\n\nAnother way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).", "filtered_refids": [["b11", "b12", "b10", "b14", "b91", "b25", "b49", null, "b19"], ["b72", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1258, "num_references": 11}
{"corpusid_sectionid": "252992688-s6", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Performance Prediction", "section": "Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.\n\nRecently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.\n\nA similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).", "filtered_refids": [[null], ["b53", null, "b98"], [null, "b38", "b62", "b106"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2155, "num_references": 8}
{"corpusid_sectionid": "252992688-s9", "title": "A Survey of Active Learning for Natural Language Processing", "date": "2022-10-18", "section_title": "Discriminative 3", "section": "Another direction is to select instances that are different from already labeled instances. Again, for NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005;Bloodgood and Callison-Burch, 2010;Erdmann et al., 2019). Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006;). Another interesting idea is to train a model to discriminate the labeled and unlabeled sets. Gissin and Shalev-Shwartz (2019) directly train a classifier for this purpose, while naturally adversarial training can be also adopted (Sinha et al., 2019;. In domain adaptation scenarios, the same motivation leads to the usage of a domain separator to filter instances (Rai et al., 2010).", "filtered_refids": [["b91", "b34", "b2", "b106"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 847, "num_references": 4}
{"corpusid_sectionid": "256461177-s1", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "What is Frame or Framing?", "section": "This section provides a conceptual understanding of framing. A classic example of framing concerns a debate over whether to permit Ku Klux Klan to hold a public rally. One news story with the headline \"Ku Klux Klan Tests OSU's Commitment to Free Speech\" reported the rally as a free speech issue, while another one with the headline \"Possible Ku Klux Klan Rally Raises Safety Concerns\" reported it as a disruption of public order. As reflected in the headlines, the two stories used different frames. People who read the free speech news story expressed higher tolerance toward KKK's rally compared to those who read the public order news story (Nelson et al., 1997, p. 581). Scholars are not agreed upon any unified framing definition (Hertog and McLeod, 2001;Van Dijk, 2016). However, a prominent definition, widely used in both traditional and computational framing studies, was provided by Entman (1993). He says:\n\nTo frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described. (p. 52) As per this definition, a frame is largely determined by its outcome effects, such as four functions: a) defining problems, b) diagnosing causes, c) making judgments, and d) suggesting remedies. The functions depend on how some selected aspects of \"perceived\" reality are made salient. In 2003, he defined it a bit differently, \"Framing entails selecting and highlighting some facets of events or issues, and making connections among them so as to promote a particular interpretation, evaluation, and/or solution\" (Entman, 2003, p. 417). This definition seems to have made a few shifts, such as from \"causal interpretation\" to \"interpretation,\" from \"moral evaluation\" to \"evaluation,\" and from \"treatment recommendation\" to \"solution.\" The salient aspects are also interconnected.\n\nWhile approaching frames as cultural phenomena, Hertog and McLeod (2001) identified a frame as a cultural \"[structure] of meaning that includes a set of core concepts and ideas,\" including \"conflicts, metaphors, myths, and narratives\" (p. 160). A frame has also been explained as \"a central organizing idea. . . for making sense of relevant events, suggesting what is at issue\" (Gamson and Modigliani, 1989, p. 3). Reese et al. (2001) defined a frame from the sociological perspective and focused on six aspects (italicize): \"Frames are organizing principles that are socially shared and persistent over time, that work symbolically to meaningfully structure the social world\" (p. 11). In a recent definition, D'angelo (2018) defined news framing as \"how journalists, their sources, and audiences work within conditions that shape the messages they construct as well as the ways they understand and interpret these messages\" (p. xxiv).\n\nTo describe a frame's aspect highlighting some selected facets of an issue or event, Fairhurst (2005) utilized an analogy that \"choosing language to frame people's actions and events is like moving a telescope into position\" (p. 125). The selected aspects are then coherently organized in a way to make an argument, which finally promotes a particular interpretation, evaluation, and solution. This organization of selected aspects could even be subtle, as framing also \"refers to subtle alterations in the statement or presentation of judgment and choice problems\" (Iyengar, 1994, p. 11). Another crucial aspect of framing is \"to choose one particular meaning (or set of meanings) over another\" (Fairhurst and Sarr, 1996, p. 3) that is also supported by Entman (1993), who says a frame \"operates by selecting and highlighting some features of reality while omitting others\" (p. 53).", "filtered_refids": [["b58", null, "b26"], [null], [null, "b26", "b46"], [null, "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3798, "num_references": 9}
{"corpusid_sectionid": "256461177-s2", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "Contexts in Framing.", "section": "A frame is considered context-sensitive. It is shaped in four locations: i) communicator, ii) texts, iii) receiver, and iv) culture (Entman, 1993). The culture is the stock of commonly invoked frames and explained as (a part of) contexts. A news report's content is fully comprehensible when its contextual information is at the disposal of readers. They interpret a frame and its meaning following contextual information (Baden and D'Angelo, 2018;Tewksbury and Riles, 2018).\n\nFraming Devices. Framing devices can be defined as tools that are used to make a piece of information more salient, which is, in other words, \"making a piece of information more noticeable, meaningful, or memorable to audiences\" (Entman, 1993, p. 53). While conceptualizing a frame, we accumulated framing devices (see Table 1). To make the list concise and convenient, we combined similar devices and put them into four groups: a) content, b) action, c) context, and d) communicator. The devices or tools can be used to provide either higher or lower salience to selected aspects of reality. In some cases, multiple devices can be applied together as a new device. For example, jargon, metaphors, and contrast can together be used to develop a \"story\" (Fairhurst and Sarr, 1996). ", "filtered_refids": [["b14", "b2", "b55"], [null, "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1258, "num_references": 5}
{"corpusid_sectionid": "256461177-s7", "title": "A Survey of Computational Framing Analysis Approaches", "date": 2022, "section_title": "Policy Frames Codebook", "section": "Boydstun et al. (2013) and Boydstun et al. (2014) proposed a codebook named \"policy frames codebook\" (PFC). The PFC consists of 14 categories of \"frame dimensions\" and an \"other\" category. The dimensions include \"economic frames,\" \"capacity and resources frames,\" \"morality frames,\" etc. For example, a news report is labeled as an economic frame if it focuses on \"the costs, benefits, or monetary/financial implications of the issue (to an individual, family, community, or to the economy as a whole)\" (Boydstun et al., 2014, p. 6).\n\nThey developed the codebook through brainstorming and iteration of applying it to random texts. With the codebook, they deployed 3,033 coders to manually code three sets of articles on immigration, tobacco, and same-sex marriage. Using the labeled documents, they finally developed a logistic regression binary text classifiers (i.e., present or absent) (Boydstun et al., 2013(Boydstun et al., , 2014.", "filtered_refids": [[null, "b7"], ["b8", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 936, "num_references": 4}
{"corpusid_sectionid": "1509090-s3", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Use", "section": "Cases. The article focuses on two use cases of MWE processing at the heart of language technology for which the correct handling of MWEs is equally crucial: parsing and MT. These uses were chosen because they are representative of past and current efforts to develop MWE-aware applications. There have been some attempts to integrate MWE processing into other applications such as information retrieval and sentiment analysis, but we do not cover them in this survey.\n\nParsing is generally concerned with the definition of algorithms that map strings to grammatical structures. Although MWE-aware parsers represent only a small portion of the total parsing literature, we argue that proper MWE identification can improve parser performance (Cafferkey, Hogan, and van Genabith 2007). In particular, complex function words that have a key role in syntax may be ambiguous (e.g., by the way). Failing to identify MWEs will lead to parsing errors. Clearly, a key characteristic of all MWE-aware parsing algorithms is that they must in some way have access to preexisting MWE resources. There are many ways to represent such resources and to incorporate them into the parsing process and this gives rise to the observed variation in the design of such algorithms (Section 4).\n\nMT is more complex than parsing insofar as it involves not only the identification of source MWEs but also their translation into the target language. Although phrase-based approaches aimed at capturing the translation of multiword units and may in principle handle contiguous MWE categories such as compounds, these approaches will certainly not be able to handle discontiguous MWEs, and neither will they cater for variants of MWEs, unseen in the training data. Attempts at MWE-aware MT have shown variable results, according to the category of MWE under consideration and the given language pair, but have proved beneficial in a number of cases (Pal, Naskar, and Bandyopadhyay 2013;Cap et al. 2015). As with parsing, pre-existing resources are necessary and there are several ways to integrate such resources in the translation process (Section 5).\n\nBecause the properties of MWEs represent challenges to one process, but opportunities for another (Section 1.2), they induce a complex pattern of bidirectional interactions. Figure 1 gives an overview of the main support relations between the two processes involved in MWE processing and our two selected use cases.\n\nThe single arrows in Figure 1 indicate a support relationship. So the arrow from discovery to identification means that discovery supports identification in virtue of the lexical resources that discovery yields. Similarly, the arrows from MT and parsing to discovery indicate that the outputs of both parsing and MT have been shown to support discovery. Syntactic analysis can help deal with discontiguity, as exemplified above, and non-literal translations can serve as a cue for ranking non-compositional MWEs for discovery. 9 The bidirectional arrows indicate two-way support. Parsing can support identification, for example, when a grammatical relationship must hold between MWE components. Translation can also support identification on the target side given a pair of parallel texts. The converse relations also hold. Identification can support parsing in that the identified MWE legitimates special treatment by the parser. It can also support the correct translation of an MWE identified on the source side.\n\nNote that this picture shows the main support relations found in previous work only. Additional arrows are possible and in Section 6 we argue for a large-scale evaluation over a systematic set of experiments that cover the less populated areas of the interaction landscape as well.\n\n1.3.3 Orchestration. This complex set of interactions, and in particular the directions in which they operate, give rise to a variety of architectures that differ in how MWE processing is scheduled with respect to the use case. More precisely, they define whether MWE processing is done before (as preprocessing), after (as postprocessing), or during (jointly with) the given use case. Although joint systems perform both tasks simultaneously, preprocessing and postprocessing can be seen as pipelines, in which the output of one process constitutes the input of the next one.\n\nThe following sections on MWE discovery, MWE identification, parsing, and MT further explain how the core tasks of MWE processing are incorporated into the use cases and vice versa. In particular, they develop the notion of orchestration, the effort of trying to find the best entry-point for one process to help the other-for example, the optimum moment to introduce MWE identification into the parsing pipeline.\n\nThe parsing literature reveals that authors have chosen different entry-points for MWE identification in the process. The choice of MWE identification before parsing, where methods are used to partly annotate words and sequences in advance, can reduce the search space of the parsing algorithm. Otherwise one can opt to do MWE identification after parsing, allowing it to benefit from the available syntactic analysis. MWE identification during parsing has the benefit that several alternatives can be maintained and resolved with joint learning models.\n\nWe see alternative approaches to orchestration in the literature on MT as well. On the one hand, we find MWE identification before translation methods (the so-called static approaches) that concatenate MWEs as a preprocessing step or conversely split compositional closed compounds in Germanic languages to distinguish them from noncompositional compounds. On the other hand, we find MWE identification during the translation process itself (so-called dynamic approaches).", "filtered_refids": [[], ["b37"], [null, "b41"], [], [null], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 39, "num_chars": 5761, "num_references": 4}
{"corpusid_sectionid": "1509090-s4", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "Resources.", "section": "Much of the glue that holds together the network of interactions shown in Figure 1 is composed of resources, which in the case of MWEs fall into three basic categories: lexical resources, (parallel) corpora, and treebanks. Lexical resources are essentially databases, and include MWE lexicons and general-purpose dictionaries containing MWE material. Both are useful for handling specific categories of MWE, such as multiword named entities (Steinberger et al. 2013), multiword terms, or idioms. Lexical resources are particularly effective for the identification of highly fixed MWEs. Otherwise, they may be combined with rules describing possible syntactic constraints within MWEs.\n\nCorpora consist of natural text, and may be annotated in different ways. Minimally, tags are simply used to delimit MWEs. Further information, concerning MWE categories, for example, can be added as tag features. Progressively more refined information can approach the level of expressiveness found in treebanks. Examples of annotated corpora with MWE tags include Wiki50 (Vincze, Nagy, and Berend 2011), STREUSLE (Schneider et al. 2014b), and the PARSEME shared task corpora (Savary et al. 2017). Two or more corpora can also be set in correspondence. For example, parallel corpora in different languages include sentence-level alignment and are used to detect manyto-many, one-to-many, or many-to-one translations. An example of MWE-annotated parallel corpus is the English-Hungarian SzegedParallelFX corpus (Vincze 2012).\n\nFinally, treebanks are special corpora that include syntactic relations between nodes over text segments and are arguably the most valuable resources for data-driven parsing systems and syntax-aware MT systems. In the literature, there exist different opinions on whether syntactically regular but semantically idiomatic MWEs should be identified in syntactic treebanks. Although the Penn Treebank designers prefer not to annotate verbal MWEs (Marcus, Marcinkiewicz, and Santorini 1993), these are annotated in the Prague Treebank (Bej\u010dek et al. 2012). For example, whereas the syntactic structure within light-verb constructions such as to make a decision is annotated with special MWE relations in the Prague Treebank, they are annotated as regular verb-object pairs in the Penn Treebank. Although, at the time of writing this survey, there is still not a universal standard for MWE annotation, one of the main goals of the PARSEME network is to develop annotation guidelines for MWE representation in both constituency and dependency treebanks. For an up-to-date status of the current annotations for different languages, see Ros\u00e9n et al. (2015Ros\u00e9n et al. ( , 2016. Appendix A provides a complementary list of resources and tools for MWE processing.\n\nIdentification relies on lexical resources that can be either the fruit of discovery or hand-built. Both parsing and MT rely on lexical resources as well, either through a separate identification step or by using them internally. For example, MWE lexicons are important for MT within preprocessing, postprocessing, and translation phases of different paradigms: They are mainly used to delimit MWEs, replacing them by either a single token, a sense identifier, or by a translation equivalent before alignment takes place. In addition to lexicons, statistical parsing depends on treebanks annotated with MWEs, and MT relies on parallel corpora (or treebanks for syntax-aware MT) to retrieve translation equivalents for the MWEs it has identified.", "filtered_refids": [["b190"], ["b177", "b203", "b204", "b172"], ["b22", "b113", "b164", "b165"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 3511, "num_references": 9}
{"corpusid_sectionid": "1509090-s7", "title": "Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2017-12-01", "section_title": "MWE Discovery", "section": "Our survey focuses on interactions of MWE processing with parsing and MT. However, we cannot discuss these interactions without providing an overview of approaches in discovery and identification. Other surveys on these tasks have been previously published (Baldwin and Kim 2010;Seretan 2011;. Our main contributions are to cover the latest advances and group references across languages and MWE categories according to each method's characteristics. Hence, the goal of this section is to define MWE discovery and provide a concise overview of the current state of affairs.\n\nThis section describes existing approaches for MWE discovery. As defined in Section 1.3, discovery is a process that takes as input a text and generates a list of MWE candidates, which can be further filtered by human experts before their integration into lexical resources. This process is depicted in Figure 2.\n\nAutomatic MWE discovery (hereafter simply referred to as discovery) has been an active research topic since the end of the 1980s when a number of seminal papers were published (Choueka 1988;Church and Hanks 1990). The famous \"pain-in-the-neck\" paper  and the related MWE workshops (Bond et al. 2003) have put discovery in focus as one of the main bottlenecks of NLP technology. Since then, considerable progress has been made, notably in the context of national and international research projects like PARSEME (Savary et al. 2015).\n\nWhereas discovery methods generate lists of MWE types out of context, MWE identification marks MWE tokens in running text. However, several terms have been used to designate what we have defined as discovery in our conceptual framework (Section 1.3), such as identification, extraction, acquisition, dictionary induction, and learning. Because one of the aims of this article is to clearly delineate the tasks of, on the one hand, discovering MWE types, and on the other, identifying MWE tokens in running text (Section 3), discovery seemed the most suitable term at the right level of specificity. Our survey focuses on empirical strategies for MWE discovery as opposed to expert lexicon construction by human language experts. Empirical methods try to automatically learn lexical information from textual data. In practice, empirical and expert methods", "filtered_refids": [["b184", "b13"], [], ["b48", null, "b165"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2277, "num_references": 5}
{"corpusid_sectionid": "226283737-s1", "title": "An Empirical Survey of Unsupervised Text Representation Methods on Twitter Data", "date": "2020-11-01", "section_title": "Text Representation Methods", "section": "In this section, we briefly introduce the methods used in our survey, sorted from oldest to newest. For word embedding methods like word2vec, GloVe, and fastText, which dot not explicitly support sentence embeddings, we average the word embeddings to get sentence embeddings. For deep models like ELMo, BERT, ALBERT, and XLNet, we take the average of the hidden state of the last layer on the input sequence axis. Note that some other works use the hidden state of the first token ([CLS]), but in our experiments, we use the pretrained model without fine-tuning, in this case, the hidden state of [CLS] is not a good sentence representation. Note that we use all these deep neural models without fine-tuning. This is because finetuning is usually based on specific downstream tasks which bias the information in the hidden states, weakening the general representation. Note that when we refer to n-gram models we mean models that capture all grams up to and including the n-gram (e.g., bigram models will include bigrams and unigrams).\n\n1. bag-of-words (BoW). This is a representation of text that describes the occurrence of words within a document. In our experiments, we use a random sample of 5 million tweets collected from the Internet Archive Twitter dataset 1 (IAT) to create a vocabulary. We also remove stop words from the tweets. We try unigram, bigram, and trigram models. 2. TF-IDF.. Term frequency-inverse document frequency (TF-IDF) reflects how important a word is with respect to documents in a collection or corpus. We use a similar experimental setup as BoW. 3. LDA (Hoffman et al., 2010). Latent Dirichlet allocation (LDA) is a generative statistical model for capturing the topic distribution of documents in a corpus. We train this model on the IAT dataset. We also remove stop-words and train models with 5, 10, 20, and 100 topics. 4. word2vec (Mikolov et al., 2013). word2vec is a distributed representation of words based on a model trained on predicting the current word from surrounding context words (CBOW). We train unigram, bigram, and trigram word2vec models using the IAT dataset. 5. doc2vec (Le and Mikolov, 2014). This model extends word2vec by adding another document vector based on ID. Our model is trained on the IAT dataset. 6. GloVe (Pennington et al., 2014). This model combines global matrix factorization and local context window methods for training distributed representations. We use the 200-dimensional version that was pre-trained on 2 billion tweets. 7. fastText (Joulin et al., 2016). fastText is another word embedding method that extends word2vec by representing each word as an n-gram of characters. We use the 300-dimensional off-the-shelf version which was pre-trained on Wikipedia. 8. Tweet2vec (Dhingra et al., 2016). This model finds vector-space representations of whole tweets by learning complex, non-local dependencies in character sequences. In our experiments, we use the pre-trained best model provided by the authors. 2 1 https://archive.org/search.php? query=collection%3Atwitterstream&sort= -publicdate 2 https://github.com/bdhingra/ tweet2vec/tree/master/tweet2vec/best_ model There is another tweet2vec model that uses a character-level cnn-lstm encoder-decoder (Vosoughi et al., 2016), but for the sake of brevity we only show the results for one of the tweet2vec models. 9. Universal Sentence Encoder (USE) (Cer et al., 2018). USE encodes sentences into high dimensional vectors. The pre-trained encoder comes in two versions, one trained with deep averaging network (DAN) (Iyyer et al., 2015) and one with Transformer. We use the DAN version of USE. 10. ELMo (Peters et al., 2018). This method provides context-dependent word representations based on bidirectional language models. We use the version pre-trained on the One Billion Word Benchmark. 11. BERT (Devlin et al., 2018). BERT is a largescale Transformer-based language representation model (Vaswani et al., 2017). We use two off-theshelf pre-trained versions BERT-base and BERTlarge, which are pre-trained on the BooksCorpus and English Wikipedia respectively. 12. ALBERT (Lan et al., 2019). This is a lite version of BERT, with far fewer parameters. We use two off-the-shelf versions, ALBERT-base and ALBERT-large, which are pre-trained on the BooksCorpus and English Wikipedia respectively. 13. XLNet (Yang et al., 2019). This is an autoregressive Transformer-based language model. Like BERT, XLNet is a large-scale language model with millions of parameters. We use the off-the-shelf versions pre-trained on the BooksCorpus and English Wikipedia. 14. Sentence-BERT (Reimers and Gurevych, 2019). Sentence-BERT modifies BERT by using siamese and triplet network structures to derive semantically meaningful sentence embeddings. We use five off-the-shelf versions provided by the authors, Sentence-BERT-base, Sentence-BERT-large, Sentence-Distilbert, Sentence-RoBERTa-base, and Sentence-RoBERTa-large, all pre-trained on NLI data.", "filtered_refids": [[null], ["b11", "b12", "b17", "b6", "b9", "b13", "b21", "b19", "b7", "b5", "b1", "b8", "b2", "b0", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 48, "num_chars": 4962, "num_references": 16}
{"corpusid_sectionid": "226283737-s3", "title": "An Empirical Survey of Unsupervised Text Representation Methods on Twitter Data", "date": "2020-11-01", "section_title": "Evaluation Metrics", "section": "We use a total of six metrics for evaluating the \"goodness\" of our clusters, described below. Except for the Silhouette score, all other metrics rely on ground-truth labels.\n\nSilhouette score (Rousseeuw, 1987): A good clustering will produce clusters where the elements inside the same cluster are close to each other and the elements in different clusters are far from each other. The Silhouette score takes both these factors into account. The score goes from -1.0 to 1.0, where higher values mean better clustering. Homogeneity, Completeness, and V-measure, (Rosenberg and Hirschberg, 2007): If clusters contain only data points that are members of a single class, in other words, high homogeneity, this usually indicates good clustering. Similarly, if all members of a given class are assigned to the same cluster, in other words, high completeness, this usually indicates good clustering. The Homogeneity and Completeness scores are between 0.0 and 1.0, where higher values correspond to better clustering. The V-measure score is the harmonic mean of Homogeneity and Completeness. Adjusted Rand Index (ARI) (Hubert and Arabie, 1985): The Rand Index can be used to compute the similarity between generated clusters and groundtruth labels. This is done by considering all pairs of samples and seeing whether their label agreement (i.e., belonging to the same ground-truth cluster or not) matches the generated cluster agreement (i.e., belonging to the same generated cluster or not). The raw RI score is then \"adjusted for chance\" into the ARI. score using the following formula: The ARI score can be between -1.0 and 1.0, where random clusterings have an ARI close to 0.0 and 1.0 stands for perfect clustering. Adjusted Mutual Information (AMI) (Vinh et al., 2010): The Mutual Information (MI) score is an information-theoretic metric that measures the amount of \"shared information\" between two clusterings. The Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information (MI) score to account for chance. It accounts for the fact that the MI is generally higher for two cluster- ings with a larger number of clusters, regardless of whether there is actually more information shared. The AMI score can be between 0.0 and 1.0, where random clusterings have an AMI close to 0.0 and 1.0 stands for perfect clustering.", "filtered_refids": [[], ["b18", "b14", "b4"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2336, "num_references": 3}
{"corpusid_sectionid": "251196750-s3", "title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking", "date": "2022-07-29", "section_title": "Datasets", "section": "Many public datasets have been published to advance machine learning approaches for DST. The evolution of these datasets is marked by increasing dialogue complexity, especially with the advent of multidomain corpora. We distinguish two main approaches for collecting dialogue datasets: (i) Wizard-of-Oz (WOZ or H2H for human-to-human) approaches where two humans (asynchronously) play the roles of user and agent according to a task description. This approach allows for natural and varied dialogues, but the subsequent annotation can be a source of errors. (ii) Simulation-based approaches (M2M for machine-to-machine) where two systems play the roles of user and agent and interact with each other to generate conversation templates that are then paraphrased by humans. The advantage of this method is that the annotations are obtained automatically. However, the complexity of the task and linguistic diversity are often limited because the dialogue is simulated. Table 1 lists the main datasets as well as recent datasets relevant to the problems discussed in Sec-  between paid participants and various telephone dialogue systems (H2M collection). The user needs to find a restaurant by specifying constraints such as the type of cuisine and can request specific information such as the phone number.\n\nMultiWOZ 9 (Budzianowski et al., 2018) The first large-scale multidomain corpus and currently the main benchmark for DST. It contains dialogues between a tourist and a travel clerk that can span several domains. A major problem related to the way the data was collected is the inconsistency and errors of annotation, which was crowdsourced. Four more versions were later released to try and fix these errors. (Eric et al., 2019;Han et al., 2021;Ye et al., 2021a). Multilingual versions 10 were obtained by a process of machine translation followed by manual correction (Gunasekara et al., 2020;Zuo et al., 2021).\n\nSGD 11 (Rastogi et al., 2020b) The Schema-Guided Dataset was created to elicit research on domain independence through the use of schemas. Schemas describe domains, slots, and intents in natural language and can be used to handle unseen domains. The test set includes unseen schemas to encourage model generalization. SGD-X is an extension designed to study model robustness to different schema wordings (Lee et al., 2021b).", "filtered_refids": [[], [null, "b58", "b46"], ["b30", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2345, "num_references": 5}
{"corpusid_sectionid": "251196750-s5", "title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking", "date": "2022-07-29", "section_title": "Modern Approaches", "section": "One feature that categorizes DST models is the way they predict slot values. The prediction can be made either from a predefined set of values (fixed ontology) or from an open set of values (open vocabulary). What follows is a description of these approaches. A selection of recent models is presented in Table 2 based on this taxonomy.\n\nFixed ontology Following the discriminative approaches that preceded them, traditional neural approaches are based on a fixed ontology and treat DST as a multiclass classification problem (Henderson et al., 2014b;Mrk\u0161i\u0107 et al., 2017). Predictions for a given slot are estimated by a probability distribution over a predefined set of values, restricting the prediction field to a closed vocabulary and thus simplifying the task considerably. The performance of this approach is therefore relatively high (Chen et al., 2020), however, its cost is proportional to the size of the vocabulary as all potential values have to be evaluated. In practice, the number of values can be large and a predefined ontology is rarely available.\n\nOpen vocabulary To overcome these limitations, approaches to predict on an open set of values have been proposed. The first method consists in extracting values directly from the dialogue history, e.g. by formulating DST as a reading comprehension task (Gao et al., 2019). This method depends solely on the dialogue context to extract value spans, however, slot values can be implicit or have different wordings (e.g. the value \"expensive\" may be expressed as \"high-end\"). An alternative is to generate slot values using an encoder-decoder architecture. For instance, TRADE uses a copy mechanism to generate a value for each slot based on a representation of the dialogue history (Wu et al., 2019). A common current approach is to decode the dialogue state using a pretrained autoregressive language model (Hosseini-Asl et al., 2020).\n\nHybrid methods A trade-off seems to exist between the level of value independence in a model and DST performance. Some works have sought to combine fixed ontology approaches with open vocabulary prediction to benefit from the advantages of both methods. This approach is based on the distinction between categorical slots for which a set of values is predefined, and non-categorical slots with an open set of values (Goel et al., 2019;Heck et al., 2020).  ", "filtered_refids": [[], [null, "b23"], [null, "b44"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2359, "num_references": 5}
{"corpusid_sectionid": "251196750-s8", "title": "\"Do you follow me?\": A Survey of Recent Approaches in Dialogue State Tracking", "date": "2022-07-29", "section_title": "Adapting PLMs to Dialogues", "section": "Though now commonly used for DST, existing PLMs are pretrained on free-form text using language modeling objectives. Their ability to model dialogue context and multi-turn dynamics is therefore limited. It has been shown that adapting a PLM to the target domain or task by continuing self-supervised learning can lead to performance gains (Gururangan et al., 2020). This method has been applied to TOD systems and DST.\n\nThere are two underlying questions with this approach: the selection of adaptation data and the formulation of self-supervised training objectives to learn better dialogue representations for the downstream task.  gather nine TOD corpora and continue BERT's pretraining with masked language modeling and next response selection. The obtained model TOD-BERT provides an improvement over a standard BERT model on several TOD tasks including DST. With a similar setup, Zhu et al. (2021) contrast these results and find that such adaptation is most beneficial when little annotated data is available. Based on TOD-BERT, Hung et al. (2022) show that it is advantageous not only to adapt a PLM to dialogues but also to the target domain. To do so, they use conversational data from Reddit filtered to contain terms specific to the target domain. Finally,  introduce two objective functions designed to inject inductive biases into a PLM in order to jointly represent dynamic dialogue utterances and ontology structure. They evaluate their method on conversational semantic parsing tasks including DST.", "filtered_refids": [[null], ["b4", "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1515, "num_references": 3}
{"corpusid_sectionid": "53593090-s3", "title": "A Review of Standard Text Classification Practices for Multi-label Toxicity Identification of Online Content", "date": 2018, "section_title": "Neural Network Classification Models", "section": "While word embeddings are a semantic representation of words, bidirectional neural networks are the technology known for generating a semantic representation for a given sequence of words. Bidirectional recurrent neural networks learn the meaning of a sentence not only from the individual words but by processing the dependencies of the surrounding words through forward and backward connections. Both bi-LSTM (Chen et al., 2016) and bi-GRU (Chung et al., 2015) architectures are shown to perform well in sentence representation. LSTM and GRU layers have a proficient learning ability for long text, because they can control how much information should be received in the current step, how much should be forgotten, and how much information should be passed back. Attention layers (Parikh et al., 2016;Felbo et al., 2017) are mechanisms suitable for converting sequence representations, which are usually in the form of matrices, to a vector representation that is tailored for the desired classification tasks. We investigated the impact of leveraging these technologies by training and testing of two neural network structures shown in Figures 2a and b. Pre-trained fasttext embeddings are used and stop words are not removed, since we want the LSTM and attention layer learn the complete sequences. The neural network shown in Figure 2a which contains two layers of biLSTM to encode the information of sequences achieves 0.9842 and the one shown in Figure 2b which uses attention mechanism to combine the context information from embedding layer and the sequence information from each biL-STM layer to get a summary vector of the sentence, reaches 0.9844 in AUC.", "filtered_refids": [[null, "b7", "b5", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1666, "num_references": 4}
{"corpusid_sectionid": "53593090-s4", "title": "A Review of Standard Text Classification Practices for Multi-label Toxicity Identification of Online Content", "date": 2018, "section_title": "Stacking of Classifiers", "section": "Stacking of classifiers is a standard way of increasing the accuracy of a classification task by combining the predictions of multiple classifiers to- gether (Merz, 1999). In this method, a supervisor model is trained and learns how to combine the predictions of different types of models that differ in their variance, bias and capability of dealing with noise (Sluban and Lavra\u010d, 2015). Figure 3 describes the stacking method applied in this work. We used a Light Gradient Boosting Machine (LGBM) stacking model which is a gradient boosting library implemented by Microsoft (Ke et al., 2017).\n\nLGBM is an implementation of fast gradient boosting on decision trees. Given a set of features, this classifier learns a linear combination of the predictions of preliminary classifiers to predict the label. The output of softmax layer from both classifiers (probabilities predicted for 6 classes) is fed to the LGBM. Also, the length of the text, frequency of exclamation marks and frequency of capital letters are considered as LGBM features. The LGBM classifier reached a 0.9847 score. In this section, we investigate the impact of pseudo-labeling as a semi-supervised training method (Lee, 2013). Simply put, we split the test dataset into 10 folds. We then trained the two classifiers described in Section 4, in a supervised fashion, with both training set and 9 folds of test set. For test set, pseudo-labels are used which are the predictions calculated by the best classifier (the LGBM model) as if they were true labels. The trained classifier is tested on the 10th fold and the experiment is repeated for all 10 folds. This method has shown to be equivalent to entropy regularization (Grandvalet and Bengio, 2005) and makes up for dissimilarities of distributions between test and train dataset. Semi-supervised training of classifier-1 and classifier-2 improves the AUC score to 0.9860 and 0.9862 respectively.", "filtered_refids": [["b12", "b10", "b17"], ["b11", "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 1917, "num_references": 5}
{"corpusid_sectionid": "265068198-s3", "title": "ChatGPT for translators: a survey", "date": 2023, "section_title": "As a translation engine", "section": "Even though ChatGPT was not trained explicitly to translate texts, it proved capable of translating between languages.Initial experiments used simple prompts like Translate the following sentences to [TARGET LANGUAGE] and showed that commercial translation engines like Google Translate and DeepL perform significantly better than ChatGPT (Jiao et al., 2023).More recent work, focused on prompt engineering to improve the quality of the translation (Gao et al., 2023).ChatGPT was also used as a translation tool that can help avoid gender bias, with better results than Neural Machine Translation (Castilho et al., 2023).A general observation is that the quality of translation is very different from one language pair to another.For example, Gao et al. (2023) report better results when the target language is English, whilst Castilho et al. (2023) observe poor results when Irish, a low resourced language, is involved.Small scale experiments conducted by the author of this paper seem to suggest that ChatGPT can translate noisy social media texts better than existing translation engines.", "filtered_refids": [["b1", "b0", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 1092, "num_references": 3}
{"corpusid_sectionid": "252624550-s1", "title": "IMTVault: Extracting and Enriching Low-resource Language Interlinear Glossed Text from Grammatical Descriptions and Typological Survey Articles", "date": 2022, "section_title": "Low Resource Languages and Diversity Linguistics", "section": "While the NLP community has not produced structured datasets for these low/no resource languages, structured data does indeed exist within the field of Diversity Linguistics. Diversity Linguistics is the field which concerns itself with the variety of languages spoken in the world. This concerns in-depth treatment of a particular language (grammatical description) as well as large-scale comparison of a given phenomenon (e. g. position of the verb before or after the object) in hundreds or thousands of languages. This comparative work can be found in articles in journals or edited volumes, in monographs, or also in databases.\n\n1 https://glottolog.org/glottolog/ glottologinformation 2 https://en.unesco.org/idil2022-2032\n\nWe can name AUTOYP 3 or the CLLD datasets (WALS,4 APiCS 5 ), of which there are 19 as of 2022. The academic inquiry is complemented by language archives where audiovisual data are stored, some of them transcribed, translated and glossed, in varying percentages. We can name ELAR, 6 AILLA, 7 TLA, 8 Paradisec. 9 See (Nordhoff, 2020a) for a breakdown of their accessible holdings. These different data sources have been tapped into over time: academic books and articles ( (Lewis and Xia, 2010;Xia et al., 2014)), typological databases ( (Chiarcos and Ionov, 2019;Ionov, 2021)), and language archives ( (Nordhoff, 2020a;Nordhoff, 2020b;von Prince and Nordhoff, 2020)), producing structured data which allows for programmatic and quantitative approaches.", "filtered_refids": [[], [], ["b11", "b21", null, "b19", "b16", "b8", "b0", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1480, "num_references": 8}
{"corpusid_sectionid": "252624550-s3", "title": "IMTVault: Extracting and Enriching Low-resource Language Interlinear Glossed Text from Grammatical Descriptions and Typological Survey Articles", "date": 2022, "section_title": "Data Modelling", "section": "The interlinear sentence has received quite some theoretical treatment. The first technical approach was the implementation in the program Shoebox, which would later become Toolbox. 10 The representation used therein was actually never intended to be used in a productive environment, but turned out to become the mainstay for language documenters for more than two decades. Shoebox/Toolbox was developed by SIL, who discontinued development in favour of FLEx, an XML based tool. 11 In parallel, ELAN 12 ( (Wittenburg et al., 2006)) is another XML-based tool for the representation of correpondences and part-whole relations 10 https://software.sil.org/toolbox 11 https://software.sil.org/fieldworks 12 https://archive.mpi.nl/tla/elan in glossed texts ( (Nordhoff, 2020a)). While XML suggest a good perspectives for programmatic extraction of data, (Nordhoff, 2020a) reports that while syntactically valid XML, the ELAN files retrieved from language archives are semantically wildly heterogeneous, making a principled approach very difficult (also compare (Cimiano et al., 2020, 4)). On a more theoretical level, (Drude, 2002) proposed a very elaborate model with a multiplicity of tiers. The XML Interlinear Glossed Text (XIGT, (Goodman et al., 2015)) format has a recursive structure instead, allowing for an arbitrary number of tiers ( (Xia et al., 2014)). (Chiarcos and Ionov, 2019) and (Ionov, 2021) developed a Linked Data version of XIGT, called LIGT, also used in (Nordhoff, 2020a;Nordhoff, 2020b). For the purposes of this paper, a very simple data model distinguishing the tiers of \"utterance\" and \"word\", with respective translations, is sufficient; the level of \"morpheme\" is disregarded. Basic storage is done in JSON, while transformations into JSON-LD, RDF, and CLDF are also made available. An additional morpheme tier could also have been made available, but it was determined that data consumers could easily create such more granular structures easily themselves should the need arise and that it was not necessary to provide an artificially inflated dataset.", "filtered_refids": [["b20", "b21", null, "b0", "b16", "b8", "b2", "b5", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2078, "num_references": 9}
{"corpusid_sectionid": "252624550-s4", "title": "IMTVault: Extracting and Enriching Low-resource Language Interlinear Glossed Text from Grammatical Descriptions and Typological Survey Articles", "date": 2022, "section_title": "Data Sources", "section": "Extraction of interlinear examples from documents has a comparatively long history. The ODIN project ( (Lewis and Xia, 2010;Xia et al., 2014)) 13 crawled the web for pdfs and tried to extract the examples. Copyright problems and the generally poor extraction facilities, however, posed great challenges for this endeavour. While ODIN is still up and running, it uses meanwhile outdated technology (eg HTML framesets), has encoding issues and does not provide dereferenceable URIs for the examples (Figure 2). Another source for interlinearized texts are crosslinguistic databases. The Atlas of Pidgin and Creole Figure 2: A screenshot of the ODIN website, showing an example of the Aari language. Note the URL, which does not give the ID, and the encoding problems. The example given has the \"Verified\" rating \"highest\". There is also \"high\", \"auto\" and \"low\", with presumably worse quality.\n\nLanguage Structures (APiCS 14 , (Michaelis et al., 2013)) offers its example sentences for download in the CLDF format ( (Forkel et al., 2018)). These examples were parsed by (Chiarcos and Ionov, 2019), who used them to develop the LIGT format. The APiCS data have the advantage of being available under a free license. (von Prince and Nordhoff, 2020) and (Nordhoff, 2020a;Nordhoff, 2020b) downloaded data from a variety of language archives, which store ELAN files. ELAN is an XML-format with explicit correspondences between morphemes, words, and sentences. These ELAN files were than converted to the RDF LIGT format, drawing on previous work by (Nordhoff et al., 2016). Published books, most databases and most of the language archives share the problem of unclear copyright status, which hinders dissemination and reuse. Enter Language Science Press.", "filtered_refids": [["b11", "b21"], ["b3", "b12", "b14", "b16", "b0", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 1748, "num_references": 8}
{"corpusid_sectionid": "252200083-s1", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Theoretical Research in Irony 2.1 Irony Theories", "section": "Various definitions have been given to irony. Early studies suggested that irony is the expression whose real meaning is contradictory to its literal meaning (Grice, 1975). The Merriam-Webster Dictionary, The Oxford English Dictionary, and The Collins English Dictionary all adopted this definition and used the words \"opposite\" or \"contrary\" to explain the relationship between the literal and contextual meanings of irony.\n\nHowever, more research into various types of ironic examples revealed that the contextual meaning of irony does not have to be \"opposite\" or \"contrary\" to the literal one. According to Sperber and Wilson (1986); Wilson and Sperber (2012), some expressions have no \"literal meaning\" to be challenged because no \"literal meaning\" is mentioned in the context, based on which they raised relevance theory and the \"echoic\" concept. They considered irony as \"an echoic use of language in which the speaker tacitly dissociates herself from an attributed utterance or thought\" (Wilson, 2006). That is, if the \"echoic use\" is incongruous in some ways, the expression can be ironic. Based on this theory, Seto (1998) put forward that there are some \"echo-markers\" like definitely, really, and indeed.\n\nLi and Huang (2020) provided instances to show that \"incongruity\" does not have to be between the literal and contextual meanings of irony in certain circumstances. They believed that irony's true nature is a psychological activity as much as a verbal representation. The speaker or listeners must finish the \"reversal\" process on a psychological level for it to be completed. When compared to the concepts of \"echoic\" and \"incongruity,\" \"reversal\" is concerned not only with the results but also with the psychological processes that the speakers/listeners go through.", "filtered_refids": [["b13"], ["b69", "b54", "b51", "b68"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1787, "num_references": 5}
{"corpusid_sectionid": "252200083-s4", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Irony Markers and Constructions", "section": "Although most of the studies saw irony as a pragmatics phenomenon, people also considered that it can be reflected on the verbal, grammatical, or semantic level. For example, on the verbal level, people often use words like thank, congratulate, welcome, happy, and interesting to express ironic meanings. Laszlo (2017) found 15 core evaluative words which often show in ironic expressions. She generated patterns from these core evaluative words to extract ironic sentences from the corpus. For example, when the word love is in the pattern \"NP + would/ 'd/ wouldn't + love\", it is highly possible to be an ironic expression. On a grammatical level, people often use the subjunctive when they intend to be ironic. Besides that, semantic conflict is the most direct way to express ironic meaning. The incompatibility between the main words of the proposition leads to the ridiculousness of the proposition (e.g. It's very considerate of you to make such a loud noise while I was asleep). Besides, (Ghosh and Muresan, 2018) also categorized irony markers according to trope, morphosyntactic, and typographic types. Li (2021) considered that ironies are often expressed by specific \"constructions\", especially in short discourses. Larger than \"core evaluative words\" in Laszlo (2017), the \"constructions\" mentioned in Li (2021) are mostly in the form of idioms or phrases. The crucial feature of them is the lack of predictability. Most of them do not have to rely on too much contextual information, they themselves can provoke the process of reversal for readers or listeners (e.g. \u8d35\u4eba\u591a\u5fd8\u4e8b (honorable people frequently forget things)).", "filtered_refids": [["b24", "b29", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1632, "num_references": 3}
{"corpusid_sectionid": "252200083-s5", "title": "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives", "date": "2022-09-10", "section_title": "Irony in Communication", "section": "Researchers claim that by using ironies, people have several kinds of intentions.\n\nBe polite: According to Brown et al. (1987), when unfavorable attitudes such as resistance, criticism, and complaints are stated with irony, the threat to the listener's reputation is reduced. The irony, as stated in Giora (1995), is an indirect negation. People prefer to utilize indirect negation to be polite to their listeners because direct negation can generate great unhappiness;\n\nEase criticisms: As reported by Dews and Winner (1995), irony helps to ease the expression's evaluative function. They believe that the incompatibility between literal meaning and contextual meaning can make it difficult to articulate negative feelings. However, Toplak and Katz (2000) argued that, while irony literally avoids conflict, it is more aggressive from the perspective of the speaker's goal;\n\nSelf-protection: Sperber and Wilson (1986) proposed the \"echoic\" idea, which stated that irony is a detached utterance that is simply an echo of another people's thought. It's a self-protection tactic, especially when the speakers are members of marginalized groups. According to , the irony is an \"off-record\" statement that allows speakers to deny their true intentions and avoid being challenged;\n\nBe amusing:  reported that when young people intend to be humorous, 50% of their communication is ironic. It can assist people in creating a dialogue platform on which speakers and listeners can agree and communicate more easily.", "filtered_refids": [[], [null, "b10"], ["b60", "b2"], ["b54"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1506, "num_references": 5}
{"corpusid_sectionid": "251719280-s3", "title": "Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect", "date": "2022-08-22", "section_title": "Single-Domain Datasets", "section": "Single-domain text-to-SQL datasets typically collect question-SQL pairs for a single database in some real-world tasks, including early ones such as Academic (Li and Jagadish, 2014), Advising (Finegan-Dollak et al., 2018), ATIS (Price, 1990;Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), Yelp and IMDB (Yaghmazadeh et al., 2017), Scholar (Iyer et al., 2017) and Restaurants (Tang and Mooney, 2000;Popescu et al., 2003), as well as recent ones such as SEDE (Hazoom et al., 2021), ESQL (Chen et al., 2021a) and MIMICSQL (Wang et al., 2020d). These single-domain datasets, particularly the early ones, are usually limited in size, containing only a few hundred to a few thousand examples. Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models that are trained on these single-domain datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other unless otherwise specified.\n\ndomains (Finegan-Dollak et al., 2018;Yu et al., 2018c). However, since these datasets are adapted from real-life applications, most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020). Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.\n\nAppendix B gives a detailed discussion on domain knowledge and dataset convention, and concrete text-to-SQL examples.\n\nLarge Scale Cross-domain Datasets Large cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) are proposed to better evaluate deep neural models. Wik-iSQL uses tables extracted from Wikipedia and lets annotators paraphrase questions generated for the tables. Compared to other datasets, WikiSQL is an order of magnitude larger, containing 80,654 natural utterances in total (Zhong et al., 2017). However, WikiSQL contains only simple SQL queries, and only a single table is queried within each SQL query (Yu et al., 2018c). Yu et al. (2018c) propose Spider, which contains 200 databases with an average of 5 tables for each database, to test models' performance on complicated unseen SQL queries and their ability to generalize to new domains. Furthermore, researchers expand Spider to study various issues of their inter-est (Lei et al., 2020;Zeng et al., 2020;Gan et al., 2021b;Taniguchi et al., 2021;Gan et al., 2021a).\n\nBesides, researchers build several large-scale text-to-SQL datasets in different languages such as CSpider (Min et al., 2019a), TableQA (Sun et al., 2020), DuSQL (Wang et al., 2020c) in Chinese, ViText2SQL (Tuan Nguyen et al., 2020) in Vietnamese, and PortugueseSpider (Jos\u00e9 and Cozman, 2021) in Portuguese. Given that human translation has shown to be more accurate than machine translation (Min et al., 2019a), these datasets are annotated mainly by human experts based on the English Spider dataset. These Spider-based datasets can serve as potential resources for multi-lingual text-to-SQL research.\n\nOther Datasets Several context-dependent textto-SQL datasets have been proposed, which involve user interactions with the text-to-SQL system in English (Price, 1990;Dahl et al., 1994;Yu et al., 2019a,b) and Chinese (Guo et al., 2021). In addition, researchers collect datasets to study questions in text-to-SQL being answerable or not (Zhang et al., 2020), lexicon-level mapping (Shi et al., 2020b) and cross-domain evaluation for real Web databases (Lee et al., 2021).\n\nAppendix C.1 discusses more details about datasets mentioned in \u00a7 2.", "filtered_refids": [["b30", null], [null, "b28", "b9"], [], [null, "b28", "b13", "b32"], ["b11", null], [null, "b6", "b34"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3649, "num_references": 14}
{"corpusid_sectionid": "251719280-s7", "title": "Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect", "date": "2022-08-22", "section_title": "Graph-based Methods", "section": "Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures. As summarized in \u00a7 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing. As a result, modeling DB schema receives little attention. Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas. Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure. In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL  encodes more relationships for DB schemas such as \"both columns are from the same table\" in their graph.\n\nGraphs have also been used to encode questions together with DB schema. Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema. Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.\n\nFinally, graph-based techniques are also exploited in context-dependent text-to-SQL. For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.\n\nSelf-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.\n\nRAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns. DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.\n\nAdapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task. PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas. The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021). Other methods adjust the embeddings by PLMs. On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings. Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.\n\nHydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on. After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction. Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question. They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question. Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns. By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.\n\nPre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task. For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT. Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered. GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.", "filtered_refids": [[null, "b36"], [null], [], [null, "b40"], ["b15", "b38"], [null, "b31", "b40"], ["b42"], ["b43"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 34, "num_chars": 5296, "num_references": 12}
{"corpusid_sectionid": "251719280-s8", "title": "Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect", "date": "2022-08-22", "section_title": "Decoding", "section": "Various methods have been proposed for decoding to achieve a fine-grained and easier process for SQL generation and bridge the gap between natural language and SQL queries. As shown in Table 3 Table 9 in Appendix D. IR: Intermediate Representation. and other technologies.\n\nTree-based Seq2Tree (Dong and Lapata, 2016) employs a decoder that generates logical forms in a top-down manner. The components in the sub-tree are generated conditioned on their parents apart from the input question. Note that the syntax of the logical forms is implicitly learned from data for Seq2Tree. Similarly, Seq2AST (Yin and Neubig, 2017) uses an abstract syntax tree (AST) for decoding the target programming language, where the syntax is explicitly integrated with AST. Although both Seq2Tree (Dong and Lapata, 2016) and Seq2AST (Yin and Neubig, 2017) do not study text-to-SQL datasets, their uses of trees inspire tree-based decoding in text-to-SQL. SyntaxSQL-Net (Yu et al., 2018b) employs a tree-based decoding method specific to SQL syntax and recursively calls modules to predict different SQL components.\n\nSketch-based SQLNet (Xu et al., 2017) designs a sketch aligned with the SQL grammar, and SQL-Net only needs to fill in the slots in the sketch rather than predict both the output grammar and the content. Besides, the sketch captures the dependency of the predictions. Thus, the prediction of one slot is only conditioned on the slots it depends on, which avoids issues of the same SQL query with varied equivalent serializations. Dong and Lapata (2018) decompose the decoding into two stages, where the first decoder predicts a rough sketch, and the second decoder fills in the lowlevel details conditioned on the question and the sketch. Such coarse-to-fine decoding has also been adopted in other works such as IRNet (Guo et al., 2019). To address the complex SQL queries with nested structures, RYANSQL (Choi et al., 2021) recursively yields SELECT statements and uses a sketch-based slot filling for each of the SELECT statements.\n\nBottom-up Both the tree-based and the sketchbased decoding mechanisms can be viewed as top-down decoding mechanisms. Rubin and Berant (2021) use a bottom-up decoding mechanism. Given K trees of height t, the decoder scores trees with height t + 1 constructed by SQL grammar from the current beam, and K trees with the highest scores are kept. Then, a representation of the new K trees is generated and placed in the new beam.\n\nAttention Mechanism To integrate the encoderside information at decoding, an attention score is computed and multiplied with hidden vectors from the encoder to get the context vector, which is then used to generate an output token (Dong and Lapata, 2016;Zhong et al., 2017). Variants of the attention mechanism have been used to better propagate the information encoded from questions and DB schemas to the decoder. SQLNet (Xu et al., 2017) designs column attention, where it uses hidden states from columns multiplied by embeddings for the question to calculate attention scores for a column given the question. Guo and Gao (2018)  Copy Mechanism Seq2AST (Yin and Neubig, 2017) and Seq2SQL (Zhong et al., 2017) employ the pointer network (Vinyals et al., 2015) to compute the probability of copying words from the input. Wang et al. (2018a) use types (e.g., columns, SQL operators, constant from questions) to explicitly restrict locations in the query to copy from and develop a new training objective to only copy from the first occurrence in the input. In addition, the copy mechanism is also adopted in context-dependent text-to-SQL task (Wang et al., 2020b).\n\nIntermediate Representations Researchers use intermediate representations to bridge the gap between natural language and SQL queries. Inc-SQL (Shi et al., 2018)  However, the intermediate representations are usually designed for a specific dataset and cannot be easily adapted to others (Suhr et al., 2020). To construct a more generalized intermediate representation, Herzig et al. (2021) propose to omit tokens in the SQL query that do not align to any phrase in the utterance.\n\nInspired by the success of text-to-SQL task, intermediate representations are also studied for SPARQL, another executable language for database systems (Saparina and Osokin, 2021;Herzig et al., 2021).\n\nOthers PICARD (Scholak et al., 2021b) and UniSAr (Dou et al., 2022) set constraints to the decoder to prevent generating invalid tokens. Several methods adopt an execution-guided decoding mechanism to exclude non-executable partial SQL queries from the output candidates (Wang et al., 2018b;. Global-GNN (Bogin et al., 2019b) employs a separately trained discriminative model to rerank the top-K SQL queries in the decoder's output beam, which is to reason about the complete SQL queries instead of considering each word and DB schemas in isolation.  (2018); Lee (2019) use separate submodules to predict different SQL components, easing the difficulty of generating a complete SQL query. Chen et al. (2020b) employ a gate to select between the output sequence encoded for the question and the output sequence from the previous decoding steps at each step for SQL generation. Inspired by machine translation, M\u00fcller and Vlachos (2019) apply byte-pair encoding (BPE) (Sennrich et al., 2016) to compress SQL queries to shorter sequences guided by AST, reducing the difficulties in SQL generation.", "filtered_refids": [[], ["b1", "b22", "b26"], [null], [], ["b16", null, "b22"], [null, "b9", "b5"], [null], [null, "b26"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 33, "num_chars": 5403, "num_references": 13}
{"corpusid_sectionid": "252571112-s3", "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability", "date": 2022, "section_title": "Shapley Values Approximation and SHAP", "section": "The idea of utilizing Shapley values to compute feature attribution scores precedes the SHAP framework (Lipovetsky and Conklin, 2001;Song et al., 2016). In this case, the outcome val of the game is the prediction of a machine learning model f and Shapley values \u03d5 f (i) measure the influence that each feature i has based on its current value. The early literature also worked on approximation strategies, as the exponential number of coalitions renders the exact estimation of Shapley values unfeasible (\u0160trumbelj and Kononenko, 2014;Datta et al., 2016). The main idea from these works is to compute \u03d5 f (i) only for a smaller selection of subsets S \u2286 F and to estimate the effect of removing a feature by integrating over training samples. This eliminates the need to retrain the model for each choice of S.\n\nThe work from Lundberg and Lee (2017) introduces a new perspective that unifies Shapley value estimation with popular explainability methods such as LIME (Ribeiro et al., 2016), LRP (Binder et al., 2016), and DeepLIFT (Shrikumar et al., 2017). Furthermore, they propose SHAP values as a unified measure of feature importance and prove them to be the unique solution respecting the criteria of local accuracy, missingness, and consistency. The authors contribute a library of methods to efficiently approximate SHAP values in a variety of settings:\n\nKernelSHAP: Adaptation of LIME-hence model-agnostic-to approximate SHAP values. As it works for any model f , it cannot make any assumption on its structure and is thus the slowest within the framework.\n\nLinearSHAP: Specific to linear models, uses the model's weight coefficients and optionally accounts for inter-feature correlations.\n\nDeepSHAP: Adaptation of DeepLIFT-hence specific to neural networks-to approximate SHAP values. Considerably faster than its model-agnostic counterpart as it makes assumptions about the model's compositional nature.\n\nWhile not initially presented in Lundberg and Lee (2017), the following algorithms were later  GradientSHAP: An extension of the Integrated Gradients (IG) method (Sundararajan et al., 2017)again specific to neural networks-that aggregates gradients over the difference between the expected model output and the current output.\n\nTreeSHAP: A fast method for computing exact SHAP values for both trees and ensembles (Lundberg et al., 2020a). In comparison to KernelSHAP, it also accounts for interactions among features.\n\nOther minor approaches-PermutationSHAP, SamplingSHAP, ExactSHAP, and MimicSHAPare also available in the official library 1 . To avoid confusion, we point out that the implementations have slightly different names: they use \"Explainer\" instead of \"SHAP\". For instance, KernelSHAP and DeepSHAP are implemented with the names of KernelExplainer and DeepExplainer respectively. Figure 2 sketches an explanation generated with SHAP.", "filtered_refids": [["b45", "b44", "b25", "b12"], ["b42", "b39", "b5"], [], [], [], ["b47"], ["b26"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2859, "num_references": 9}
{"corpusid_sectionid": "252571112-s6", "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability", "date": 2022, "section_title": "Review: SHAP-Based Approaches", "section": "Several works proposed methods based on SHAP, or more generally on Shapley values, following the contribution from Lundberg and Lee (2017). While the changes and variations introduced have been at times criticized for not being as rigorous as SHAP in following its core assumptions (Sundararajan and Najmi, 2020), SHAP-based methods continue to increase in both quantity and popularity.\n\nOur review categorizes SHAP-based approaches available to date based on how they differ from and how they improve on the original SHAP framework. We identify five broad categories in the existing literature, each one of them describing a different research direction pursued by its members:\n\n(C1) Tailored to Different Input Data: This category contains approaches specialized on specific input data structures such as graphs , structured text (Chen et al., 2020), and images (Teneggi et al., 2021). In some cases, approaches are used complementary for applications dealing with multimodal inputs Mosca et al., 2022b).\n\n(C2) Explaining Different Models: Methods in this class are specifically designed to explain predictions from particular types of machine learning models such as random forests (Lundberg et al., 2018;Labreuche and Fossier, 2018) and neural networks (Ghorbani and Zou, 2021). Hence, these are model-specific.\n\n(C3) Modifying Core Assumptions: SHAP treats features as independent. Newer methods offer the possibility to account for dependencies between features (Frye et al., 2019) and for causal structures behind their interactions (Heskes et al., 2020).\n\n(C4) Producing Different Explanations Types: SHAP is a framework for local featureattribution explanations, i.e. it attributes scores to input components based on their instance-level contributions. Methods in this category have a different scope and generate explanations that convey a different type of information. This can vary from global explanations (Covert et al., 2020) to counterfactual explanations (Singal et al., 2019) and concept explanations (Yeh et al., 2020). Clearly, these categories are not designed to be exclusive. Therefore, an approach can fall in more than one if it differs from SHAP in multiple aspects. Table 1 provides an overview of all approaches with their main characteristics. As one can observe, the majority of approaches are identified as part of more categories, i.e. research directions.", "filtered_refids": [["b46"], [], ["b49", "b34", "b7"], ["b28", "b23", "b17"], ["b15", "b18"], ["b11", "b43", "b56"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2390, "num_references": 12}
{"corpusid_sectionid": "252571112-s7", "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability", "date": 2022, "section_title": "Approaches Tailored to Different Inputs", "section": "SHAP does not make strong assumptions on the target model's input. While this suggests that it is suitable for all input types, its lack of specificity results in limitations when applied directly to different inputs than tabular data.  For text data, only measuring each individual feature's effect is an oversimplification, as words present strong interactions and their meaning and contribution heavily rely on the context. Thus, when it comes to text data, only considering single words as features is quite restrictive and relevance scores should be applied to multi-level tokens or even to entire sentences. Hierarchical Explanation via Divisive GEneration (HEDGE) (Chen et al., 2020) is an example of a SHAP-based method addressing this issue for (long) texts. Based on the weakest token interactions, it iteratively divides the text into shorter phrases and words in a topdown fashion. At each level, a relevance score is attributed to each token, resulting in a hierarchical explanation (Chen et al., 2020). PartitionSHAP, recently added to the official SHAP repository 3 , follows a similar strategy by creating hierarchical features coalitions and measuring their interactions.  (Bhatt et al., 2020) neighbors to explain a given instance n.a. ASV (C1) (C3) Relaxes the symmetry axiom of Shapley values Potentially Applicable (Frye et al., 2019) to incorporate causal structure into explanations R BShap (C4) (C5) Baseline approach to facilitate comparison Adaptable (Sundararajan and Najmi, 2020) between different Shapley value based methods n.a. C-and L-Shapley (C3) (C5) Efficient feature attribution method that models data Ready Off-the-Shelf (Chen et al., 2018) as a graph by considering only neighboring features TensorFlow CASV   Figure 3 sketches an example of a hierarchical explanation for text data. For models trained on graph data, especially graph DNNs, Yuan et al. (2021) proposed to explain predictions by using Shapley values as a measure of subgraph importance. The resulting method-named SubgraphX-also captures the interactions between different subgraphs.\n\nOn images, SHAP can face computational limitations as the number of features, i.e. pixels, can become extremely large. h-SHAP (Teneggi et al., 2021) efficiently retrieves exact Shapley values by hierarchically excluding irrelevant image areas from the computation. This is done following the observation that, if a certain area in the image is uninformative, so are its constituent sub-areas, which are therefore not worth exploring.", "filtered_refids": [["b15", "b57", "b46", "b4", "b9", "b7"], ["b49"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2523, "num_references": 7}
{"corpusid_sectionid": "252819333-s1", "title": "A Survey of Automatic Text Summarization using Graph Neural Networks", "date": 2022, "section_title": "Why graph neural networks ?", "section": "Contemporary solutions to the task of ATS suffer from a number of issues, chiefly an inconsistent evaluation protocol and, somewhat, a lack of progress, as noted by Kry\u015bci\u0144ski et al. (2019). In recent years GNNs have been successfully applied to a number of downstream NLP tasks such as classification    and translation (Xu et al., 2021) (Yin et al., 2020). Although GNNs may not be able to solve all problems related to the task of ATS, we believe that they can at least give a new perspective to this task. Generally GNNs bring a number of advantages to ATS which we believe to be significant enough to warrant further research, and this survey. In particular we want to highlight the following aspects of GNNs:\n\n\u2022 Scalability and Flexibility. A vast number of ATS models are based on BERT (Devlin et al., 2019). However, the computational complexity of BERT-based ATS models grows quadratic with the input length; due to the selfattention operation. This fact renders them impractical for long, or even medium sized text documents. Recently some work has been done in order to circumvent this limiting factor (Ding et al., 2020) (Zhang et al., 2021). In contrast, GNNs can scale by their nature to graphs of thousands of nodes and more. This is in part due to the linear scaling of the memory cost with regards to the input size. The total memory cost of a GNN model depends on the size of the graph, the number of layers and the feature vector size of the nodes present. Formally, for L layers and an input of N nodes with each node's feature vector being of size H the memory complexity is O(LN H). But even for very large graphs on the scale of millions of nodes one can utilize GNNs. This can be achieved using methods such as neighbour sampling or distributing the graph over multiple GPUs, as done for example by Jia et al. (2020b). We recommend the paper by  for insights as to how one can train large and very deep GNNs. As the input of a GNN is a graph, the input can vary in size, therefore GNNs are also able to cope with changing text sizes and structures. Both of these aspects combined allow GNNs to produce summaries which are not restricted by hard-coded limits related to input or output size.\n\n\u2022 Understanding and Explainability. It is often difficult to understand why a model arrived at a certain conclusion. Additionally it is often difficult to see how the model aggregates information. This is not the case with GNNs, as with the help of methods such as GNN Explainer (Ying et al., 2019) one can understand which nodes were used by the model to reach its output. This removes a layer of the blackbox magic present in many current non-GNN models. We recommend the survey by Yuan et al. (2020) for an overview of methods for generating explanations for GNNs.", "filtered_refids": [["b45", "b42", "b16"], ["b14", "b48", "b6", "b7"], ["b47", "b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 2783, "num_references": 9}
{"corpusid_sectionid": "252819333-s6", "title": "A Survey of Automatic Text Summarization using Graph Neural Networks", "date": 2022, "section_title": "Spatial Convolution and Message Passing", "section": "One can view spatial convolution as used in GNNs as a generalization of the convolution used in neu-ral networks such as CNNs. As an example, in the case of images, one can imagine 2D convolution as being applied to a regular grid of nodes where each node represents a pixel in the image. The resulting 2D convolution applied to one target node is then the weighted average of node (pixel) values of the neighbours of the target node. Generalizing this idea to a non-regular grids leads to spatial convolution. However, different to images and regular grids, in graphs, the neighbours of each target node are unordered and can vary in number and their feature vector representation. The major challenge with this extension consists therefore in dealing with the unordered and inconsistent neighbourhood sizes inherent to homogeneous and heterogeneous graphs, with an additional challenge being posed by the differing feature vector representations in heterogeneous graphs.\n\nDirectly translating the above description of convolution into a mathematical formulation leads to a valid information propagation scheme. However, such a description suffers from scalability issues due to it directly operating over the entire graph. As such modern GNNs use, what is commonly referred to as, message passing. In practice, this means that nodes within the graph exchange messages (perform convolutions) with their neighbours for a number of iterations. Thereby the network is able to diffuse information throughout the graph. Consequently, the more iterations, the further outwards information is propagated throughout the graph. In the terminology of CNNs one would say that the more message passing iterations, the larger the receptive field of the convolution. Formally, one can define message passing (Grattarola and Alippi, 2021) for each time step t as two equations:\n\nThis first equation describes how messages are generated. A differentiable function \u03d5 generates messages m for each edge which connects nodes using the node features and edge feature present.\n\nThe above equation is the core of the message passing framework and describes how each node feature is updated. The first part consists in the application of a permutation-invariant reduction function \u03c1. This function aggregates all incoming messages to a node. Then another differen-tiable function \u03c8 combines the reduced messages received with the previous state. Using these two equations one can utilize message passing for learnable layers.\n\nThe convolution layer for a GNN is then defined with a learnable weight W such that the message per edge is m t+1 i,j = x t j and the aggregation is the normalized sum of messages, i.e.\n\nwhere M (i) represents the set of messages received by node i, \u03c3 is the activation function, b is the bias, and c j,i is an appropriate scaling factor, e.g., the square root of the node degree. Note how it is important for the reduction function, in this case a sum function, to be permutation-invariant as otherwise GNNs could not handle the unordered nature of graphs.\n\nThe above presented convolution layer does not allow the model to filter unimportant neighbours. Inspired by the attention mechanism popularized by transformer networks (Vaswani et al., 2017), graph attention networks (GAT) (Veli\u010dkovi\u0107 et al., 2018) assign attention scores to each neighbour. A schematic depiction of the two variants of spatial convolution can be seen in Figure 1 with GAT depicted on the lower part of the figure. The introduction of attention scores to the spatial convolution allows the model to explicitly assign importance to certain nodes and their messages. Just as in transformers GAT is formulated with multi-head attention. The modification to the previously presented convolution layer follows closely the common attention formulation. Formally, x t+1\n\nwhere \u03b1 i,j is the attention score between node i and node j and K denotes the number of concatenated heads. The attention scores are computed with\n\n. This score is then normalized to obtain the attention score per edge \u03b1 i,j = sof tmax i (r i,j ). We want to highlight here a recent development which Brody et al. (2021) simply denote as GATv2. Their main improvement aims at the fact that in the above calculation both learnable parameters a and W effectively fold into a single linear layer, thus the expressive power of the layer is less than what it could be. The fix introduced by GATv2 pulls the two parameters apart, thus achieving more expressive power while not increasing computational complexity. Taking the above description the attention score for GATv2 is modified as follows\n\n). In both synthetic and real datasets this modification shows superior performance, which is supported by a theoretical analysis of the authors.\n\nThere are numerous modifications and extensions to the basic convolution presented here. However, for ATS models, GAT layers are dominating as the workhorse for most models. The reasoning for their dominance can be explained by the similar success that attention transformers have had in conventional neural networks for AST. We expect that GATv2 will continue this trend as it is an attractive and simple improvement for the currently dominating GAT. Although the authors of GATv2 note that it is not yet entirely clear which tasks would benefit the most from the usage of GATv2 over GAT, which will require more research and models to use GATv2.\n\nIn ATS the graphs used are in nearly all cases not homogeneous. However, the equations presented here do not work for heterogeneous graphs. The solution for this problem involves defining one convolution layer for each node type combination occurring within the graph. In the case of the already discussed sentence and word node graph there would be four possible combinations of types, and four convolution layers which would have to be defined if the graph were fully connected.\n\nConvolution is a central aspect of GNNs, but pooling also presents an important and common operation, especially whenever GNNs are used jointly with other models. Pooling in GNNs is achieved by generating a global representation of the graph, or a subset of the graph, by pooling together features of nodes. This is usually done with some function f where f is commonly the mean, max or sum.\n\nWe want to explicitly point out to the reader that the construction of GNNs does not require special datasets. All GNN models for ATS use the common benchmark single-document and multidocument summarization datasets such as DUC 2004 or CNN/Dailymail. The only requirement for any ATS, or textual dataset, is for the designer to find an appropriate way of encoding sentences, words, subwords etc. into feature vectors, and finding a sensible way of connecting them.", "filtered_refids": [[], ["b11"], [], [], [], [], ["b31", "b32"], [], ["b3"], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 52, "num_chars": 6773, "num_references": 4}
{"corpusid_sectionid": "252819333-s8", "title": "A Survey of Automatic Text Summarization using Graph Neural Networks", "date": 2022, "section_title": "Standalone GNNs", "section": "We will start our discussion of standalone GNN models with HeterSumGraph (HSG), a model proposed by . We will do so due to the fact that this model illustrates concepts and ideas seen throughout GNNs models used for ATS. An illustration of the general concepts presented here can be seen in Figure 3.\n\nThe HSG model encodes each text into a graph with three node types, sentence nodes, word nodes and document nodes. The connection between these nodes is decided by inclusion i.e. if the word represented by a word node occurs in a sentence then their respective nodes are connected by an edge. The same principle applies to document nodes which are connected depending on whether a word, represented by a word node, occurs within the document. This is a flexible structure, as it can be used in a single-document but also multidocument setting.  Figure 3: General architecture of standalone GNNs with word nodes and sentence nodes, encoders for both node types and a sentence selection mechanism. Inspired by HSG.\n\nThe feature vectors for all nodes are obtained by encoders and the edge weights are obtained by computing the TF-IDF score for each word. The neural network consists of a modified GAT layer. The GAT is modified to consider the TF-IDF value of the connecting edge. Additionally a positionwise feed-forward (FFN) layer consisting of two linear transformations is applied to the hidden state after the convolution. In total three convolution layers are used, word-sentence, sentence-word and word-document. The model is then trained on a node-based binary classification task that is predicting whether a sentence node is to be included for the summary or not.\n\nThe classification itself is done by a single linear layer. The model then does not directly use the predicted nodes to produce the summary. Instead trigram blocking (Paulus et al., 2018) is used during sentence selection in order to ensure sparsity of the generated summary.\n\nThe results for this model are quite impressive as it outperforms non-BERT based models on both single-document and multi-document summarization for the CNN/DailyMail dataset. One should especially note the flexibility and ability to use this model for two tasks.\n\nAn older model by Muratore et al. (2010) can be considered a precursor to this architecture. A simple extension to the HSG model is proposed by Ya et al. (2021). In their extension they modify the model for query constraints for the summary. This is achieved by adding a query node to the graph structure. Additionally, they introduce a mu-tual information maximization mechanism during training.\n\nA model which further follows this structure is the one by Linmei et al. (2019). The authors there extend the attention mechanism by adding another layer of attention, allowing it to include information about the type of the node during convolution. The GNN model by Jing et al. (2021) encodes even more information into the graph by considering the relation between sentences on a number of different levels. In particular, they encode the semantic and syntactical relationship between sentences within the graph.\n\nThis idea of encoding additional information into the graph is also followed by Antognini and Faltings (2019). They introduce an additional universal feature vector which is added to each sentence node embedding. This universal feature vector is learned from a large unrelated and general corpus. This model is also unique in that it focuses on the summarization of very small texts, on average less than 100 words.\n\nTaking this basic structure and idea even further is the model called HAHSum by Jia et al. (2020a). The construction of the input graph for HAHSum is more involved as it aims to significantly reduce semantic sparsity by utilizing named entities. The model uses three types of nodes, named entity nodes, word nodes and sentence nodes, with the named entity nodes being anonymized tokens. The graph is then built as follows, word nodes are connected with a directed edge to a sentence node if they occur within the sentence. Two named entities are connected with an undirected edge if they represent the same entity and two sentence nodes are connected with an undirected edge if they share a trigram. Additionally, sequentially occurring words and entities are connected with a directed edge. This setup shows how one can encode a substantial amount of implicit information in an explicit manner.\n\nHAHSum uses a GAT for each of the five node type combinations found within the graph. Just as in HSG, a FFN is applied after the multi-head attention and again as in the previous model a linear layer is used to perform the binary classification of the sentence nodes.\n\nThe results for HAHSum show that GNNs can perform very well. The authors of the paper tested the model on the CNN/Daily Mail, Newsroom and NYT dataset. The model outperforms very pow- erful models such as MATCHSUM (Zhong et al., 2020) and is even able to compete in some metrics with leading abstractive models such as PEGASUS . The results of an Amazon Mechanical Turk experiment corroborate these results and show that for human readers HAHSum produce summaries with superior fluency and conciseness. Another recent GNN model which has achieved great performance in the task of multi-document summarization is the SgSum model by . Different to the approaches outlined above, the SgSum model uses graph pooling to extract sub-graphs from encoded documents. That is, it first transforms the documents into a large graph, then generates a number of sub-graphs via pooling and convolution. These sub-graphs are then ranked and thereby selected for a summary. This is quite an innovative approach as it casts the problem of multi-document summarization as a simple subgraph selection problem. Additionally, it outputs an integral summary, that is the entire summary is output by the model in the form of the sub-graph of sentences.", "filtered_refids": [[], [], [], ["b30"], [], ["b43", "b29"], ["b43", "b22"], ["b1"], [null], [], ["b52"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 51, "num_chars": 5942, "num_references": 8}
{"corpusid_sectionid": "232320384-s2", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Text", "section": "Due to the availability of large amounts of textual content, research on the text modality is comparatively richer than for other modalities. Notable work in this direction covers fake news spread on social media (Vosoughi et al., 2018b), fake news and fact-checking on news media (Rashkin et al., 2017), fact-checking such as fact-checked URL recommendation model (Vo and Lee, 2018) (Sathe et al., 2020). There have also been recent efforts for factchecking from political debates (Shaar et al., 2020(Shaar et al., , 2022(Shaar et al., , 2021Nakov et al., 2022b,a), fact-checking with evidence reasoning (Si et al., 2021;Jiang et al., 2021;Wan et al., 2021) and fact-checking by claim matching (Kazemi et al., 2021). Given that there have been surveys on the text modality for fake news/disinformation detection and fact-checking, here we will not go into more detail about the individual studies.", "filtered_refids": [["b52", "b58", "b50", "b45", null, "b69", "b74", "b42", "b51", "b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 898, "num_references": 10}
{"corpusid_sectionid": "232320384-s3", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Image", "section": "Text with visual content (e.g., images) in social media is more prominent as it is more intuitive; thus, it is easier to consume, it spreads faster, it gets 18% more clicks, 89% more likes, and 150% more retweets . Due to the growing number of claims disseminated with images, in the current literature, there have been various studies that address the visual content with text for predicting misleading information (Volkova et al., 2019), fake images (Gupta et al., 2013), images shared with misinformation in political groups (Garimella and Eckles, 2020), and fauxtography Wang et al., 2021). Some of these studies attempt to understand how two different modalities are used. Their analyses show that the extension of text with images increases the effectiveness of misleading content. Gupta et al. (2013) highlighted the role of Twitter to spread fake images. This study reports that 86% tweets spreading fake images are retweets. Garimella and Eckles (2020) manually annotated a sample of 2,500 images collected from public WhatsApp groups, and labeled them as misinformation, not misinformation, misinformation already fact-checked, and unclear; however, experiments were conducted with binary labels: misinformation vs. not-misinformation. The authors found that violent and graphic images spread faster. Nakamura et al. (2020) developed a multimodal dataset containing 1M posts including text, images, metadata, and comments collected from Reddit. The dataset was labeled with 2, 3, and 6-ways labels. Volkova et al. (2019) proposed models for detecting misleading information using images and text.\n\nFauxtography is defined as \"visual images, especially news photographs, which convey a questionable (or outright false) sense of the events they seem to depict\" (Cooper, 2007). It is also commonly used in social media in different forms such as a fake image with false claims, a true image with false claims.  defined that \"a post is a fauxtography if the image of the post (i) directly supports a false claim, or (ii) conveys misinformation of a true claim.\" An example is shown in Figure 2 (in Appendix A).  developed FauxBuster to detect fauxtographic social media content, which uses social media comments in addition to the content in the images and the texts. Zlatkova et al. (2019) investigated the factuality of claims with respect to images and compared the performance of different feature groups between text and images. Wang et al. (2021) analyzed fauxtography images in social media posts and found that posts with doctored images increase user engagement in the form of re-shares, likes, and comments, specifically in Twitter and Reddit. They pointed out that doctored images are often used as memes to mislead or as a means of satire, and that they have a 'clickbait' power to drive engagement.", "filtered_refids": [["b70", null, "b25"], ["b93", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2817, "num_references": 5}
{"corpusid_sectionid": "232320384-s4", "title": "A Survey on Multimodal Disinformation Detection", "date": "2021-03-13", "section_title": "Speech/Audio", "section": "There have been attempts to use acoustic signals to predict the factuality of claims in political de-bates (Kopev et al., 2019;Shaar et al., 2020), leftcenter-right bias in YouTube channels (Dinkov et al., 2019), and deception in speech (Hirschberg et al., 2005). Kopev et al. (2019) found that the acoustic signal helps in improving the performance compared to using only textual and metadata features. Similarly, Dinkov et al. (2019) reported that the use of speech signal improves the performance of the system for detecting the political bias (i.e., left, center, right) of Youtube channels. Moreover, a large body of work was done on deception detection using the acoustic signal. Hirschberg et al. (2005) created the Columbia-SRI-Colorado (CSC) corpus by eliciting within-speaker deceptive and non-deceptive speech. Their experiments consist of the use of acoustic, prosodic, and a variety of lexical features including 68 LIWC categories, filled pauses, and paralinguistic information (e.g., speaker information, gender, field-pause). Using the same corpus, an evaluation campaign was organized, where different multimodal approaches were proposed, such as fusion of different acoustic, prosodic, lexical, and phonotactics representations (Levitan et al., 2016;Kaya and Karpov, 2016).", "filtered_refids": [["b52", null, "b9", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1291, "num_references": 4}
{"corpusid_sectionid": "9840922-s2", "title": "Word Meaning Inducing via Character Ontology: A Survey on the Semantic Prediction of Chinese Two-Character Words", "date": 2005, "section_title": "Morpho-Semantic Description", "section": "As known, \"bound roots\" are the largest classes of morpheme types in Chinese morphology, and they are very productive and represent lexical rather than grammatical information (Packard 2000). This morphological phenomena leads many Chinese linguists to view the word components (i.e., characters) as building blocks in the semantic composition process of dis-or multisyllabic words. In many empirical studies (Tseng and Chen (2002); Tseng (2003); Lua (1993); Chen (2004)), this view has been confirmed repeatedly.\n\nIn the semantic studies of Chinese word formation, many descriptive and cognitive semantic approaches have been proposed, such as argument structure analysis (Chang 1998) and the frame-based semantic analysis (Chu 2004). However, among these qualitative explanation theoretical models, problems often appear in the lack of predictability on the one end of spectrum, or overgeneration on the other. 1 Empirical data have also shown that in many cases, -e.g., the abundance of phrasal lexical units in any natural language, -the principle of compositionality in a strict sense, that is, \"the meaning of a complex expression can be fully derivable from the meanings of its component parts, and from the schemas which sanction their combination\"(Taylor 2002), which is taken to be a fundamental proposition in some of morpho-semantically motivated analysis, is highly questionable.\n\nThis has given to the consideration of the embeddedness of linguistic meanings within broader conceptual structures. In what follows, we will argue that an ontology-based approach would provide an interesting and efficient prospective toward the character-triggered morpho-semantic analysis of Chinese words.", "filtered_refids": [["b12", null, "b8", "b2"], [null, "b3"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1702, "num_references": 6}
{"corpusid_sectionid": "9840922-s4", "title": "Word Meaning Inducing via Character Ontology: A Survey on the Semantic Prediction of Chinese Two-Character Words", "date": 2005, "section_title": "A Shift Toward Character Ontology", "section": "In prior studies, it is widely presumed that the category (be it syntactical or semantic) of a word, is somehow strongly associated with that of its composing characters. The semantic compositionality underlying two-character words appears in different terms in the literature. 2 Word semantic similarity calculation techniques have been commonly used to retrieve the similar compositional patterns based on semantic taxonomic thesaurus. However, one weak point in these studies is that they are unable to separate conceptual and semantic levels. Problem raises when words in question are conceptually correlated are not necessarily semantically correlated, viz, they might or might not be physically close in the CILIN thesaurus (Mei et al 1998). On closer observations, we found that most synonymic words (i.e., with the same CILIN semantic class) have characters which carry similar conceptual information. This could be best illustrated by examples. Table 1 shows the conceptual distribution of the modifiers of an example of VV compound by presuming the second character \u00a6 as a ceptable to native speakers. 2 Using statistical techniques, Lua (1993) found out that each Chinese two-character word is a result of 16 types of semantic transformation patterns, which are extracted from the meanings of its constituent characters. In Chen (2004), the combination pattern is referred to as compounding semantic template.\n\nhead. The first column is the semantic class of CILIN (middle level), the second column lists the instances with lower level classification number, and the third column lists their conceptual types adopted from a character ontology we will discuss later. As we can see, though there are 12 resulting semantic classes for the * \u00a6 compounds, the modifier components of these compounds involve only 4 concept types as follows:\n\nWe defined these patterns as conceptual aggregate pattern in compounding. Unlike statistical measure of the co-occurrence restrictions or association strength, a concept aggregate pattern provides a more knowledge-rich scenario to represent a specific manner in which concepts are aggregated in the ontological background, and how they affect the compounding words. We will propose that the semantic class prediction of Chinese two-character words could be improved by making use of their conceptual aggregate pattern of head/modifier component.", "filtered_refids": [["b8", "b13", "b2"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2392, "num_references": 3}
{"corpusid_sectionid": "9840922-s5", "title": "Word Meaning Inducing via Character Ontology: A Survey on the Semantic Prediction of Chinese Two-Character Words", "date": 2005, "section_title": "Semantic Prediction of Unknown Two-Character Words", "section": "The practical task intended to be experimented here involves the automatic classification of Chinese two-character words into a predetermined number of semantic classes. Difficulties encountered in previous researches could be summarized as follows: First, many models (Chen and Chen 1998;2000) cannot deal with the issue of \"incompleteness\" of characters in the lexicon, for these models depend heavily on CILIN, a Chinese Thesaurus containing only about 4,133 monosyllabic morphemic components (characters). As a result, if unknown words contain characters that are not listed in CILIN, then the prediction task cannot be performed automatically. Second, the ambiguity of characters is often shunned by  manual pre-selection of character meaning in the training step, which causes great difficulty for an automatic work. Third, it has long been assumed (Lua 1997;Chen and Chen 2000) that the overwhelming majority of Chinese compounds are more or less endocentric, where the compounds denote a hyponym of the head component in the compound. E.g, \u00das (\"electric-mail\"; e-mail) is a kind of mail. So the process of identifying semantic class of a compound boils down to find and to determine the semantic class of its head morpheme. However, there is also an amount of exocentric and appositional compounds 3 where no straightforward criteria can be made to determine the head component. For example, in a case of VV compound o\u00bd (\"denounce-scold\", drop-on), it is difficult (and subjective) to say which character is the head that can assign a semantic class to the compound. To solve above-mentioned problems, Chen (2004) proposed a non head-oriented charactersense association model to retrieve the latent senses of characters and the latent synonymous compounds among characters by measuring similarity of semantic template in compounding by using a MRD. However, as the author remarked in the final discussion of classification errors, the performance of this model relies much on the productivity of compounding semantic templates of the target compounds. To correctly predict the semantic category of a compound with an unproductive semantic template is no doubt very difficult due to a sparse existence of the template- 3 Lua reports a result of 14.14% (Z3 type). similar compounds. In addition, the statistical measure of sense association does not tell us any more about the constraints and knowledge of conceptual combination.\n\nIn the following, we will propose that a knowledge resource at the morpheme (character) level could be a straightforward remedy to these problems. By treating characters as instances of conceptual primitives, a character ontology thereof might provide an interpretation of conceptual grounding of word senses. At a coarse grain, the character ontological model does have advantages in efficiently defining the conceptual space within which character-grounded concept primitives and their relations, are implicitly located.", "filtered_refids": [["b12", null, "b9", "b1"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2959, "num_references": 4}
{"corpusid_sectionid": "229376920-s6", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "How to Define Style?", "section": "Linguistic Definition of Style. An intuitive notion of style refers to the manner in which the semantics is expressed (McDonald and Pustejovsky 1985). Just as everyone has their own signatures, style originates as the characteristics inherent to every person's utterance, which can be expressed through the use of certain stylistic devices such as metaphors, as well as choice of words, syntactic structures, and so on. Style can also go beyond the sentence level to the discourse level, such as the stylistic structure of the entire piece of the work, for example, stream of consciousness, or flashbacks. Beyond the intrinsic personal styles, for pragmatic uses, style further becomes a protocol to regularize the manner of communication. For example, for academic writing, the protocol requires formality and professionalism. Hovy (1987) defines style by its pragmatic aspects, including both personal (e.g., personality, gender) and interpersonal (e.g., humor, romance) aspects. Most existing literature also takes these well-defined categories of styles.\n\nData-Driven Definition of Style as the Scope of this Survey. This survey aims to provide an overview of existing neural TST approaches. To be concise, we will limit the scope to the most common settings of existing literature. Specifically, most deep learning work on TST adopts a data-driven definition of style, and the scope of this survey covers the styles in currently available TST datasets. The data-driven definition of style is different from the linguistic or rule-based definition of style, which theoretically constrains what constitutes a style and what not, such as a style guide (e.g., American Psychological Association 2020) that requires that formal text not include any contraction, e.g., \"isn't.\" The distinction of the two defintions of style is shown in Figure 1.\n\nWith the rise of deep learning methods of TST, the data-driven definition of style extends the linguistic style to a broader concept-the general attributes in text. It regards \"style\" as the attributes that vary across datasets, as opposed to the characteristics that stay invariant (Mou and Vechtomova 2020). The reason is that deep learning models (which are the focus of this survey) need large corpora to learn the style from, but not all styles have well-matched large corpora. Therefore, apart from the very few manually annotated datasets with linguistic style definitions, such as formality (Rao and Tetreault 2018) and humor & romance (Gan et al. 2017), many recent dataset collection works automatically look for meta-information to link a corpus to a certain attribute. A typical example is the widely used Yelp review dataset (Shen et al. 2017), where reviews with low ratings are put into the negative corpus, and reviews with high ratings are put into the positive corpus, although the negative vs. positive opinion is not a style that belongs to the linguistic definition, but more of a content-related attribute.\n\nMost methods mentioned in this survey can be applied to scenarios that follow this data-driven definition of style. As a double-edged sword, the prerequisite for most methods is that there exist style-specific corpora for each style of interest, either parallel or non-parallel. Note that there can be future works that do not take such an assumption, which will be discussed in Section 6.3.\n\nComparison of the Two Definitions. There are two phenomena rising from the data-driven definition of style as opposed to the linguistic style. One is that the data-driven definition of style can include a broader range of attributes including content and topic preferences of the text. The other is that data-driven styles, if collected through automatic classification by meta-information such as ratings, user information, and source of text, can be more ambiguous than the linguistically defined styles. As shown in Jin et al. (2019, Section 4.1.1), some automatically collected datasets have a concerningly high undecideable rate and inter-annotator disagreement rate when the annotators are asked to associate the dataset with human-defined styles such as political slant and gender-specific tones.\n\nThe advantage of the data-driven style is that it can marry well with deep learning methods because most neural models learn the concept of style by learning to distinguish the multiple style corpora. For the (non-data-driven) linguistic style, although it is under-explored in the existing deep learning works of TST, we provide in Section 6.3 a discussion of how potential future works can learn TST of linguistics styles with no matched data.", "filtered_refids": [["b59", "b119"], [], ["b174", "b123", "b148", "b32"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 29, "num_chars": 4620, "num_references": 6}
{"corpusid_sectionid": "229376920-s8", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "Existing Subtasks with Datasets", "section": "We list the common subtasks and corresponding datasets for neural TST in Table 3. The attributes of interest vary from style features (e.g., formality and politeness) to content preferences (e.g., sentiment and topics). Each task of which will be elaborated below.\n\nFormality. Adjusting the extent of formality in text was first proposed by Hovy (1987). It is one of the most distinctive stylistic aspects that can be observed through many linguistic phenomena, such as more full names (e.g., \"television\") instead of abbreviations (e.g., \"TV\"), and more nouns (e.g., \"solicitation\") instead of verbs (e.g., \"request\"). The formality dataset, Grammarly's Yahoo Answers Formality Corpus (GYAFC) (Rao and Tetreault 2018), contains 50K formal-informal pairs retrieved by first getting 50K informal sentences from the Yahoo Answers corpus, and then recruiting crowdsource workers to rewrite them in a formal way. Briakou et al. (2021b) extend the formality dataset to a multilingual version with three more languages, Brazilian Portuguese, French, and Italian.\n\nPoliteness. Politeness transfer (Madaan et al. 2020) aims to control the politeness in text. For example, \"Could you please send me the data?\" is a more polite expression than \"send me the data!\". Madaan et al. (2020) compiled a dataset of 1.39 million automatically labeled instances from the raw Enron corpus (Shetty and Adibi 2004). As politeness is culture-dependent, this dataset mainly focuses on politeness in North American English. Table 3 List of common subtasks of TST and their corresponding attribute values and datasets. For datasets with multiple attribute-specific corpora, we report their sizes by the number of sentences of the smallest of all corpora. We also report whether the dataset is parallel (Pa?). ", "filtered_refids": [[], ["b59", "b148", "b6"], ["b112", "b175"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1783, "num_references": 5}
{"corpusid_sectionid": "229376920-s9", "title": "Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2020-11-01", "section_title": "162", "section": "Gender. Linguistic phenomena related to gender is a heated research area (Trudgill 1972;Lakoff 1973;Tannen 1990;Argamon et al. 2003;Boulis and Ostendorf 2005). The gender-related TST dataset is proposed by Prabhumoye et al. (2018), who compiled 2.5M reviews from Yelp Dataset Challenge that are labeled with the gender of the user.\n\nHumor & Romance. Humor and romance are some artistic attributes that can provide readers with joy.  first propose to borrow the FlickrStyle stylized caption dataset (Gan et al. 2017) from the computer vision domain. In the FlickrStyle image caption dataset, each image has three captions, with a factual, a humorous, and a romantic style, respectively. By keeping only the captions of the three styles,  created a subset of the FlickrStyle dataset of 5K parallel (factual, humorous, romantic) triplets.\n\nBiasedness. Wiki Neutrality Corpus (Pryzant et al. 2020) is the first corpus of biased and neutralized sentence pairs. It is collected from Wikipedia revisions that adjusted the tone of existing sentences to a more neutral voice. The types of bias in the biased corpus include framing bias, epistemological bias, and demographic bias.\n\nToxicity. Another important use of TST is to fight against offensive language. Tran, Zhang, and Soleymani (2020) collect 350K offensive sentences and 7M non-offensive sentences by crawling sentences from Reddit using a list of restricted words.\n\nAuthorship. Changing the tone of the author is an artistic use of TST. Xu et al. (2012) created an aligned corpus of 18K pairs of Shakespearean English and their modern English translation. Carlson, Riddell, and Rockmore (2018) collected 28M parallel data from English versions of the Bible by different translators.\n\nSimplicity. Another important use of TST is to lower the language barrier for readers, such as translating legalese, medical jargon, or other professional text into simple English, to avoid discrepancies between expert wordings and lay understanding (Tan and Goonawardene 2017). Common tasks include converting standard English Wikipedia into Simple Wikipedia, whose dataset contains 108K samples (Zhu, Bernhard, and Gurevych 2010). Another task is to simplify medical descriptions to patient-friendly text, including a dataset with 2.2K samples (den Bercken, Sips, and Lofi 2019), another non-parallel dataset with 59K free-text discharge summaries compiled from MIMIC-III (Weng, Chung, and Szolovits 2019), and a more recent parallel dataset with 114K samples compiled from the health reference Merck Manuals (MSD), where discussions on each medical topic has one version for professionals, and the other for consumers .\n\nSentiment. Sentiment modification is the most popular task in previous work on TST. It aims to change the sentiment polarity in reviews, for example, from a negative review to a positive review, or vice versa. There is also work on transferring sentiments on finegrained review ratings (e.g., 1-5 scores). Commonly used datasets include Yelp reviews (Shen et al. 2017) and Amazon product reviews (He and McAuley 2016).\n\nTopic. There are a few works that cover topic transfer. For example, Huang et al. (2020) form a two-topic corpus by compiling Yahoo! Answers under two topics, entertainment and politics, respectively. There is also a recent dataset with 21 text styles such as Sciences, Sport, Politics, and others (Zeng, Shoeybi, and Liu 2020 Combined Attributes. Lample et al. (2019) propose a more challenging setting of text attribute transfer: multi-attribute transfer. For example, the source sentence can be a positive review on an Asian restaurant written by a male reviewer, and the target sentence is a negative review on an American restaurant written by a female. Each of their datasets has 1-3 independent categories of attributes. Their first dataset is FYelp, which is compiled from the Yelp Dataset Challenge, labeled with sentiment (positive or negative), gender (male or female), and eatery category (American, Asian, Mexican, bar, or dessert). Their second dataset, Amazon, which is based on the Amazon product review dataset , contains the following attributes: sentiment (positive or negative), and product category (book, clothing, electronics, movies, or music). Their third dataset, Social Media Content dataset, collected from internal Facebook data that is private, contains gender (male or female), age group (18-24 or 65+), and writerannotated feeling (relaxed or annoyed).", "filtered_refids": [["b141", "b91", null, "b191", "b1", "b186"], ["b32"], ["b142"], [], ["b13", "b213"], ["b185", "b201", "b230"], ["b174", "b52"], ["b64", "b94", "b219"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 4465, "num_references": 18}
{"corpusid_sectionid": "237371890-s1", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Language Pair Speakers (approx) Parallel Sentences", "section": "English-French 267M 280M English-Myanmar 30M 0.7M English-Fon 2M 0.035M Table 1: Examples of language pairs with different levels of resources. The number of speakers is obtained from Ethnologue, and the parallel sentence counts are from Opus.\n\nIn Table 1 we select three language pairs that display a range of resource levels. We show the estimated number of first language speakers 1 of the non-English language, together with the number of parallel sentences available in Opus (Tiedemann 2012), the largest collection of publicly available translated data. 2,3 Although there is typically a correlation between speaker numbers and size of available resources, there are many exceptions where either widely spoken languages have little parallel data, or languages with very small speaker numbers are richly resourced (mainly European languages). For one of the world's most spoken languages, French, there are nearly 280 million parallel sentences of English-French in OPUS. However when we search for English-Myanmar, we find only around 700,000 parallel sentences, despite Myanmar having tens of millions of speakers. If we consider Fon, which has around 2 million speakers, then we find far fewer parallel sentences, only 35,000 4 . Developing MT systems for these three language pairs will require very different techniques.\n\nThe lack of parallel data for most language pairs is only one part of the problem. Existing data is often noisy or from a very specialised domain. Looking at the resources that are available for Fon-English, we see that the only corpus available in Opus is extracted from Jehovah's Witness publications (Agi\u0107 and Vuli\u0107 2019, JW300) 5 . For many language pairs, the only corpora available are those derived from religious sources (e.g. Bible, Koran) or from IT localisation data (e.g. from open-source projects such as GNOME and Kubuntu). Not only is such data likely to be in a very different domain from the text that we would like to translate, but such large-scale multilingual automatically extracted corpora are often of poor quality (Caswell et al. 2021) and this problem is worse for low-resource language pairs. This means that low-resource language pairs suffer from multiple compounding problems: lack of data, out-of-domain data and noisy data. And the difficulty is not just with parallel data, low-resource languages often lack good linguistic tools, and even basic tools like language identification do not exist or are not reliable.\n\nPartly in response to all these challenges, there has been an increasing interest in the research community in exploring more diverse languages, and language pairs that do not include English. This survey paper presents a high-level summary of approaches to low-resource MT, with focus on neural machine translation (NMT) techniques, which should be useful for researchers interested in this broad and rapidly evolving field. There are currently a number of other survey papers in related areas, for example a survey of monolingual data in low-resource NMT (Gibadullin et al. 2019) and a survey of multilingual NMT (Dabre, Chu, and Kunchukuttan 2020). There have also been two very recent surveys of low-resource MT, which have been written concurrently with this survey (Ranathunga et al. 2021;Wang et al. 2021). Our survey aims to provide the broadest coverage of existing research in the field and we also contribute an extensive overview of the tools and techniques validated across 18 low-resource shared tasks that ran between 2018 and 2021.\n\nOne of the challenges of surveying the literature on low-resource MT is how to define what a low-resource language pair is. This is hard, because \"resourced-ness\" is a continuum and any criterion must be arbitrary. We also note that the definition of low-resource can change over time. We could crawl more parallel data, or we could find better ways of using related language data or monolingual data which means that some language pairs are no longer so resource-poor. We maintain that for research to be considered to be on \"low-resource MT\", there should be some way in which the research should either aim to understand the implications of the lack of data, or propose methods for overcoming the lack of data. We do not take a strict view of what to include in this survey though; if the authors consider that they are studying low-resource MT, then that is sufficient. We do feel however that it is important to distinguish between simulated low-resource settings (where a limited amount of data from otherwise highresource language pairs is used) and genuinely low-resource languages (where additional difficulties apply). We also discuss some papers that do not explicitly consider low-resource MT but which present important techniques and we mention methods that we think have the potential to improve low-resource MT. Even though there has been a lot of interest recently in low-resource NLP, the field is limited to languages where some textual data is freely available. This means that so far low-resource MT has only considered 100-odd languages, and there is a long tail of languages that is still unexplored.\n\nIn Figure 1 we show how we structure the diverse research methods addressing low-resource MT, and this paper follows this structure. We start the survey by looking at work that aims to increase the amount and quality of parallel and monolingual data available for low-resource MT (Section 2). We then look at work that uses other types of data: monolingual data (Section 3), parallel data from other language pairs (Section 4), and other types of linguistic data (Section 5). Another avenue of important research is how to make better use of existing, limited resources through better training or modelling (Section 6). In Section 7 we pause our discussion on methods to improve low-resource MT, to consider how to evaluate these improvements. In our final section, we look at efforts in the community to build research capacity through shared tasks and  language-specific collectives (Section 8), providing a practical summary of commonly used approaches and other techniques often used by top-performing shared task systems. This survey aims to provide researchers and practitioners interested in low-resource MT with an overview of the area, and we hope it will be especially useful with those that are new to low-resource MT, and looking to quickly assimilate the recent research directions. We assume that our readers have prior knowledge of MT techniques and are already familiar with basic concepts, including the main architectures used. We therefore do not redefine them in this survey and refer the interested reader to other resources such as (Koehn 2020).", "filtered_refids": [[], ["b191"], [null], [null, "b202", "b34"], [], ["b74"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 41, "num_chars": 6722, "num_references": 6}
{"corpusid_sectionid": "237371890-s4", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Web-crawling for Parallel Data", "section": "Once freely available sources of parallel data have been exhausted, one avenue for improving low-resource NMT is to obtain more parallel data by web-crawling. There is a large amount of translated text available on the web, ranging from small-scale tourism websites to large repositories of government documents. Identifying, extracting and sentence-aligning such texts is not straightforward, and researchers have considered many techniques for producing parallel corpora from web data. The links between source texts and their translations are rarely recorded in a consistent way, so techniques ranging from simple heuristics to neural sentence embedding methods are used to extract parallel documents and sentences.\n\nParacrawl, 11 a recent large-scale open-source crawling effort (Ba\u00f1\u00f3n et al. 2020) has mainly targeted European languages, only a few of which can be considered as lowresource, but it has created some releases for non-European low-resource language pairs, and the crawling pipeline is freely available. Related to this are other broad guages, where it is affected by class imbalance, similar languages, encoding problems and domain mismatches. Further down the crawling pipeline, common techniques for document alignment and sentence alignment rely on the existence of translation systems (Uszkoreit et al. 2010;Volk 2010, 2011) or sentence embeddings (Artetxe and Schwenk 2019a), which again may not be of sufficient quality in lowresource languages and so we often have to fall back on older, heuristic alignment techniques (Varga et al. 2005) (and even this may perform worse if a bilingual lexicon is not available). The consequence is that the resulting corpora are extremely noisy and require extensive filtering before they can be useful for NMT training.\n\nFiltering of noisy corpora is itself an active area of research, and has been explored in recent shared tasks, which particularly emphasised low-resource settings . In an earlier version of the task (Koehn et al. 2018), dual conditional crossentropy (Junczys-Dowmunt 2018, DCCE) was found to be very effective for English-German. In the 2019 and 2020 editions of the task however, DCCE was much less used possibly indicating that it is less effective for low-resource and/or distant language pairs. Instead, we see that all participants apply some heuristic filtering (e.g. based on language identification and length) and then strong submissions typically used a combination of embedding-based methods (such as LASER (Artetxe and Schwenk 2019b), GPT-2 (Radford et al. 2019) and YiSi (Lo 2019)) with feature-based systems such as zipporah (Xu and Koehn 2017) or bicleaner . Whilst the feature-based methods are much faster than sentence-embedding based methods, both types of methods require significant effort in transferring to a new language pair, especially if no pre-trained sentence embeddings or other models are available.\n\nThe conclusion is that all crawled data sources should be treated with care, especially in low-resource settings as they will inevitably contain errors. A large-scale quality analysis (Caswell et al. 2021) of crawled data has highlighted that many contain incorrect language identification, non-parallel sentences, low quality texts, as well as offensive language, and these problems can be more acute in low-resource languages.", "filtered_refids": [[], ["b197", "b176", "b198"], [null, "b149", "b79"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 3344, "num_references": 7}
{"corpusid_sectionid": "237371890-s6", "title": "Survey of Low-Resource Machine Translation", "date": "2021-09-01", "section_title": "Test Sets", "section": "Obtaining more training data is important, but we should not forget the role of standardised and reliable test sets in improving performance on low-resource translation. Important contributions have come from shared tasks, such as those organised by the WMT Conference on Machine Translation Barrault et al. 2019Barrault et al. , 2020 (Nakazawa et al. 2018(Nakazawa et al. , 2019(Nakazawa et al. , 2020 and the Workshop on Technologies for MT of Low Resource Languages (LoResMT) (Karakanta et al. 2019;Ojha et al. 2020). The test sets in these shared tasks are very useful, but inevitably only cover a small selection of low-resource languages, and usually English is one of the languages in the pair; non-English language pairs are poorly covered. A recent initiative towards rectifying this situation is the FLORES-101 benchmark , which covers a large number of low-resource languages with multi-parallel test sets, vastly expanding on the original FLORES release . Since FLORES-101 consists of the same set of English sentences, translated into 100 other languages, it can also be used for testing translation between non-English pairs. It has the limitation that, for non-English pairs, the two sides are \"translationese\", and not mutual translations of each other, but there is currently no other data set with the coverage of FLORES-101.\n\nWe summarise the data sets described in this section in Table 2.", "filtered_refids": [["b59", null, "b122", "b128"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1409, "num_references": 4}
{"corpusid_sectionid": "21715311-s1", "title": "A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches", "date": "2018-05-01", "section_title": "Background -WordNet Construction", "section": "Generally speaking, WordNets are constructed using one of two approaches (Vossen, 1998):\n\n\u2022 The merge approach -whereby an exhaustive repository of senses (meanings) of each word is compiled, with synsets then created that contain all of the applicable words for a given sense.\n\n\u2022 The expansion approach -whereby existing synsets from a reference WordNet are used as a guide to create corresponding synsets in a new WordNet, by gathering applicable words that represent the meaning of the synset and ordering them by frequency.\n\nSince the introduction of the PWN and the success of early projects such as EuroWordNet that were built around its principles, many projects have focused on building new WordNets in diverse languages using these methods. These endeavours have highlighted various advantages and disadvantages of both the merge and the expansion approaches. Bhattacharyya (2010) describes how the merge approach results in WordNets of high quality, on account of expert lexicographers working in detail on only one language; however, the process is typically very slow. Conversely, the expansion approach can allow the construction of the Word-Net to take place much more quickly, with construction guided by synsets and semantic relationships in the source (or reference) WordNet; however, lexicographers still need to dedicate time to constructing language-specific synsets (meanings or concepts which may not be represented or have a place in the source WordNet), and there is a danger of specific concepts only applicable to the target language being overlooked altogether (Bhattacharyya, 2010). Years of work on constructing new WordNets have also contributed to the development of guidelines and principles for creating them, largely based on leveraging existing knowledge using the expansion approach. The GWA outlines the importance of 'base concepts' 4 -those concepts that occupy a high position in a semantic hierarchy and have many relations to other concepts -as playing a vital role in constructing WordNets. Base concepts are defined by their universality -common (to at least two languages), local (to only one language), or global (across all languages) -with an initial set of 1024 common base concepts being released as part of the EuroWordNet project 5 . As a starting point, the GWA proposes that WordNets be constructed in two steps:\n\n\u2022 A core WordNet of between 5,000 and 10,000 synsets is constructed around the common base concepts,\n\n\u2022 An extended WordNet is built (semi-automatically, given the semantic basis of the core WordNet) to increase the total number of synsets to 20,000 and beyond.\n\nGiven that for many languages these core synsets are readily-constructed, it makes sense to leverage them when constructing new WordNets, and to 'borrow' the semantic relationships that have already been created (Bhattacharyya, 2010). Given the amount of time that can be saved by re-using existing work, there is a tendency to see the expansion approach favoured over the merge approachit also lends itself extremely well to the automatic construction of synsets, where input from lexicographers is minimal to zero. Thus, the research described in this paper (and particularly in section 3.) largely follows the expansion approach, with synsets being constructed by automatically extracting lexical data from a range of resources in order to build a skeleton framework of meanings based on a reference WordNet.", "filtered_refids": [["b16"], [], [], ["b2"], [], [], ["b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 3442, "num_references": 3}
{"corpusid_sectionid": "21715311-s4", "title": "A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches", "date": "2018-05-01", "section_title": "Bilingual Dictionaries", "section": "The most common technique for populating new WordNets automatically has been to leverage the information in bilingual dictionaries in the source and target languages. Use of bilingual dictionaries for this purpose goes back to very early work on building Catalan and Spanish WordNets as part of the EuroWordNet project. In this work, translations of English words in the source WordNet were found, and these translations classified by features such as polysemy (number of translations for each word), structure (the semantic relationships between translations in the source WordNet), and 'conceptual distance' (length of the path between two words in a graph-based representation of the source WordNet) to create a skeleton WordNet in the target language, which could be extended later using bilingual taxonomies (Farreres et al., 1998). Since then, bilingual dictionaries have continued to be a popular resource for the automatic construction of Word-Nets. A Romanian WordNet was built by using a range of heuristics to:\n\n\u2022 Analyse the relationships between synsets in the source (English) WordNet,\n\n\u2022 Identify semantic relationships in various target language resources,\n\n\u2022 Map these relationships to each other in the target (Romanian) WordNet using a bilingual dictionary (Barbu and Barbu Mititelu, 2005).\n\nThe method was evaluated using 9716 synsets from a preexisting Romanian WordNet that also had entries in PWN, from which these 9716 synsets were extracted and used as the source (English) WordNet -the synsets used were limited to hypernymy and meronymy relations, and all 19,624 literal words within the synsets had an entry in the bilingual dictionary. The resulting automatically-constructed Romanian WordNet contained 9610 synsets connected by approximately 11,969 semantic relationships, which were reported to be 91% accurate when compared to the 9716 synsets from the pre-existing Romanian WordNet (Barbu and Barbu Mititelu, 2005). In more recent work on building a Persian WordNet, a bilingual dictionary was used to extract a group of 'candidate' synsets containing English translations of a given Persian word from a source WordNet (PWN). These candidate synsets were then ranked by calculating the Mutual Information of the given Persian word and its English translations in both source and target language corpora, and based on this ranking the most appropriate candidate synset to use for the target (Persian) WordNet was selected (Montazery and Faili, 2010). An extension of this work specifically aimed at lesser-resourced languages was also described, in which a Persian WordNet is constructed by finding the English translations of Persian words in small corpora using a bilingual dictionary. These translations are then used to perform word sense disambiguation (WSD) on a Persian sentence using a source (English) WordNet, and the English synsets returned by the WSD algorithm are mapped to the target (Persian) WordNet (Taghizadeh and Faili, 2016). Again, these techniques have been shown to be able to automatically construct WordNets with a good degree of accuracy. Montazery and Faili (2010) report that a manual evaluation of 500 synsets from their automatically-constructed target WordNet (which in total covered 29,716 synsets from PWN) resulted in an accuracy of 82.6% (95.8% for synsets whose mapping from source to target WordNet was unambiguous and 76.4% for synsets whose correct mapping had to be decided by ranking multiple candidates). Taghizadeh and Faili (2016) manually evaluated 1,750 word/synset pairs from their target WordNet, and describe how a threshold value (between 0 and 1) used by their WSD algorithm to remove low-scoring candidate synsets had a significant impact on their results. Higher threshold values resulted in the WordNet being more precise (90% with a threshold value of 0.1) but with low recall (fewer synsets in the target WordNet), while lower threshold values resulted in a Word-Net with higher recall (more synsets) but with low precision (74% with the threshold value set to 0).", "filtered_refids": [["b3"], [], [], ["b1"], ["b15", "b1", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 4053, "num_references": 5}
{"corpusid_sectionid": "21715311-s7", "title": "A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches", "date": "2018-05-01", "section_title": "Word Embeddings and WordNet Synsets", "section": "Given the increasing popularity of word embeddings (vector space representations of word meanings based on their distribution within large datasets), it should come as no surprise that the links between embeddings and traditional, WordNet-style representations of word senses have recently been explored. Nayak (2015) demonstrated that word embeddings can be classified according to words and can also be used to predict hypernymy relations between them, while Rothe and Sch\u00fctze (2015) report that sets of embeddings trained not just on words but also on synsets (groups of synonyms) and lexemes (word-synset pairs) achieve state-of-the-art performance on WSD and semantic similarity tasks. This kind of research shows the potential of word embeddings for capturing the kinds of relationships (and being useful in the types of tasks) commonly associated with WordNet-style word senses -potential which is further compounded by the reported high precision with which multi-sense word embeddings can be mapped to WordNetstyle synset entries in Babelnet 9 (Panchenko, 2016).", "filtered_refids": [["b12", "b10", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 3, "num_chars": 1071, "num_references": 3}
{"corpusid_sectionid": "21693867-s0", "title": "The DLDP Survey on Digital Use and Usability of EU Regional and Minority Languages", "date": "2018-05-01", "section_title": "Background and Motivation", "section": "In this paper we present the results of the first survey about the actual usage of four European minority languages and the related needs of their speakers in terms of digital opportunities. The survey is part of the work carried out by the Digital Language Diversity Project (DLDP) (Soria et al., 2016) 1 , a three-year Erasmus+ project started in September 2015. The goal of the DLDP is to help minority language speakers' communities in the acquisition of intellectual and practical skills to create, share, and reuse online digital content in their languages. At the same time we want to define general guidelines and best practices for the promotion of minority languages with poor digital representation, a fact that further prevents their usability on digital media and devices. One of the underlying assumptions of the Digital Language Diversity Project is that the sustainability and preservation of regional and minority languages is closely tied to their being perceived by their speakers as being fully-fledged languages that can be used in any context, the digital one included. Unfortunately, this is far from being a reality not only for regional and minority languages, but for the majority of the world languages. In most cases, the technical or infrastructural impediments for the digital use of European regional and minority languages are modest and fairly easily solvable. Marginalisation and minoritisation of those languages mostly derives from the concurrency of the national and global languages for which digital content and services are more easily available, which further discourages regional and minority language speakers from using those languages digitally. In order to break this vicious circle and make those languages digitally appealing and usable to an extent that can compete with other major lan-guages, it is necessary to approach the problem in terms of \"digital language planning\". In order to be able to plan for digital development, we first need to identify the current and actual extent to which RML are used digitally, the type and frequency of their digital use, the opportunity for their use, and the main obstacles currently preventing it so as to get a clear understanding of the different factors that may affect the digital use of RMLs. Some reports carried out for individual languages and specific media are available, like the Language White Papers (Uszkoreit and Rehm, 2012) published by the META-NET Network that has clearly shown how 30 European languages are at risk of digital extinction because of lack of sufficient support in terms of language technologies. The META-NET work, initially for each of the EU official languages, was then extended to cover as well some regional languages such as Basque (Hern\u00c3\u00a1ez et al., 2012), Catalan (Moreno et al., 2012), Galician (Garc\u00c3a-Mateo and Arza, 2012), and Welsh (Evas, 2013). The reports mostly assessed the status of those languages in terms of language technology support. A general survey covering all regional and minority languages of the EU, the different types of digital media and services available, as well as inquiring about the attitudes and desires to make a digital use of the language is still lacking. The DLDP effort can therefore be seen as a first step towards the design of a survey about digital use of minority languages in both professional and informal contexts, specifically tailored on RMLs in the digital world and structured around a crucial question: is it possible for regional or minority language speakers to have a digital life in those languages? The paper is organised as follows: a description of the methodology underlying the design of the survey; an analysis of the results collected, with a separate section for each language; a summary of the key findings and an indication of the work planned for the future.", "filtered_refids": [[null, "b5", "b8", "b0", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 3859, "num_references": 5}
{"corpusid_sectionid": "258378266-s1", "title": "Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey", "date": 2023, "section_title": "Background", "section": "Neural Ranking (NR) for ODQA Let Q, D and A denote the question, document and answer set. Given a question q \u2208 Q, the NR model assigns a relevance score R(q, d) to each d \u2208 D and selects top-k document D topk \u2208 D with the highest relevance scores. Afterwards, a reader will estimate the score G(a|q, D topk ) to predict the final answer a \u2208 A conditioned on both q and D topk . The NR model can be implemented using various architecture with increasing model complexity. For computational efficiency, normally a bi-encoder architecture (Bromley et al., 1993) is first applied to pre-select top candidates from the whole document set, then a more complex cross-interaction model is applied to provide more accurate relevance scores only for the preselected candidates (Lee et al., 2021). The training objective for the NR model R can be formalized as:\n\nwhere Q \u00d7 D indicates the full set of questiondocument pairs, d + is a positive (relevant) document for q, d \u2212 1\u223cn is the sampled n negative (irrelevant) documents and L is the loss function. A common choice for L is the contrastive loss:\n\nNeural Ranking with Weak Supervision In the standard supervised setting we need relevance annotations for (q, d) \u2192 {+, \u2212} to train R with Eq 1.\n\nObtaining high-quality relevance annotations requires tremendous human labor and is expensive to scale to multiple domains (Del Tredici et al., 2021;Ram et al., 2022). Weak supervision (WS) is a widely-used approach to reduce such cost by leveraging supervision signals from e,g., heuristic rules, knowledge bases or external models . WS signals are cheap to obtain but might contain significant noise which will affect the NR performance. Therefore, understanding their working mechanisms and pros and cons are important to obtain a good NR model. We group WS signals into 3 classes by the resources that they need: (1) Documents: only document collection D is needed;\n\n(2) Documents + Questions: document collection D and question set Q are needed; (3) Documents + QA Pairs: document collection D and QA pairs (Q, A) are needed. In the next section, we will present the three classes of WS signals and discuss their pros and cons.", "filtered_refids": [[null], [], [], [null, "b31"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2169, "num_references": 3}
{"corpusid_sectionid": "258378266-s3", "title": "Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey", "date": 2023, "section_title": "Self Contrastive Learning", "section": "Self contrastive learning relies on heuristics to construct pseudo question-document pairs (q \u2032 , d \u2032+/\u2212 ) from D, then uses them to supervise training of a NR model. The objective is:\n\nwhere L is the ranking loss as in Eq 1. Since negative pairs can be easily constructed by random sampling, the main difficulty is to design good heuristics for constructing positive pseudo pairs (q \u2032 , d \u2032+ ). There are 5 popular heuristics to construct such positive pairs: perturbation-based, summarybased, proximity-based, cooccurence-based and hyperlink-based. An overview is in Table 2.\n\nPerturbation-based heuristics add perturbations to some text, then treat the perturbed text and the original text as a positive pair. The intuition is that perturbed text should still be relevant to the original text. Typical choices of perturbations include word deletion, substitution and permutation (Zhu et al., 2021b;Meng et al., 2021) (Ma et al., 2021a).\n\nProximity-based heuristics utilize the position information in the document to obtain positive pairs based on the intuition that nearby text should be more relevant to each other. The most famous one is the inverse-cloze task (Lee et al., 2019), where a sentence from a passage is treated as the question and the original passage, after removing the sentence, is treated as a positive document. They can be combined with typical noise injection methods like adding drop-out masks (Xu et al., 2022), random word chopping or deletion (Izacard et al., 2021) to further improve the model robustness. Other methods include using spans from the same document (Gao and Callan, 2022; Ma et al., 2022), sentences from the same paragraph, paragraphs from the same document as positive samples (Di Liello et al., 2022), etc.\n\nCooccurrence-based heuristics construct positive samples based on the intuition that sentences containing cooccurred spans are more likely to be relevant (Ram et al., 2021). For example, Glass et al. (2020) constructs a pseudo question with a sentence from the corpus. A term from it is treated as the answer and replaced with a special token. Passages retrieved with BM25 which also contains the answer term are treated as pseudo positive documents. Ram et al. (2022) treat a span and its surrounding context as the pseudo question and use another passage that contains the same span as a positive document.\n\nHyperlink-based heuristics leverage hyperlink information based on the intuition that hyperlinked text are more likely to be relevant . For example, Chang et al.\n\n(2020) takes a sentence from the first section of a page p as a pseudo question because it is often the description or summary of the topic. A passage from another page containing hyperlinks to p is treated as a positive document. Yue et al. (2022a) replace an entity word with a question phrase like \"what/when\" to form a pseudo question. A passage from its hyperlinked document that contains the same entity word is treated as a positive sample. Zhou et al. (2022) build positive samples with two typologies: \"dual-link\" where two passages have hyperlinks pointed to each other, and \"co-mention\" where two passages both have a hyperlink to the same third-party document.  ", "filtered_refids": [[], [], ["b16", "b83", "b10"], ["b64", null, "b74"], ["b30", "b31"], [], ["b70", "b80"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3203, "num_references": 10}
{"corpusid_sectionid": "258378266-s5", "title": "Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey", "date": 2023, "section_title": "Choices of Filter", "section": "Filtering is a crucial part of QG since a significant portion of generated questions could be of low quality and would provide misleading signals when used to train the NR model (Alberti et al., 2019). A typical choice is filtering based on round-trip consistency (Alberti et al., 2019;Dong et al., 2019), where a pre-trained QA system is applied to produce an answer based on the generated question. A question is kept only when the produced answer is consistent with the answer from which the question is generated. We can also relax this strict consistency requirement and manually adjust an acceptance threshold based on the probability from the pre-trained QA system (Zhang and Bansal, 2019;Lewis et al., 2021), LM score from the generator itself (Shakeri et al., 2020;Liang et al., 2020), or an entailment score from a model trained on question-context-answer pairs . Influence functions (Cook and Weisberg, 1982) can be used to estimate the effect on the validation loss of including a synthetic example (Yang et al., 2020), but this does not achieve satisfying performances on QA tasks (Bartolo et al., 2021). Bartolo et al. (2021) propose filtering questions based on ensemble consistency, where an ensemble of QA models are trained with different random seeds and only questions agreed by most QA models are selected. When minimal target-domain annotation is available, we can also learn to reweight pseudo samples based on the validation loss , or use RL to select samples that lead to validation performance gains (value estimation) (Yue et al., 2022b).", "filtered_refids": [["b25", "b71", "b66", "b77", null, "b1"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1565, "num_references": 6}
{"corpusid_sectionid": "258378191-s3", "title": "A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models", "date": 2023, "section_title": "Pre-trained Language Models", "section": "Recently, pre-trained language models have advanced the state-of-the-art in many NLP tasks ranging from textual similarity to text summarization (Zhang et al., 2019;Liu and Lapata, 2019;Zhong et al., 2020) and named entity recognition (Zhou et al., 2021). State-of-the-art pre-trained models include LSTM-based language models (e.g., ELMo (Peters et al., 2018)) and Transformer-based language models (e.g., BERT 2 (Devlin et al., 2019) and RoBERTa ). Specifically, the transformer-based models learn bidirectional representations for words based on a masked language model and sentence adjacency training objective (Devlin et al., 2019). Simply using contextualized embeddings obtained from the transformerbased pre-trained language models in place of traditional embeddings has resulted in state-of-the-art performance on a range of NLP tasks. Therefore, pre-trained language models have been employed as encoders for obtaining word-, sentence-, and document-level representations to assist the downstream tasks.", "filtered_refids": [["b72", "b46", "b13", "b70", "b33", "b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1013, "num_references": 6}
{"corpusid_sectionid": "258378191-s4", "title": "A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models", "date": 2023, "section_title": "Keyphrase Extraction Dataset", "section": "Since the first shared task on KE (Turney, 1999), many shared tasks and benchmark datasets for KE have been created. Specifically, OpenKP (Xiong et al., 2019), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, Se-mEval2017 (Augenstein et al., 2017), and KP20k (Meng et al., 2017) were created from scientific articles in English.\n\nCompared with other datasets, KP20k contains a large amount of annotation data, so it is often used as the dataset to train the neural-based KE models recently. Meanwhile, in recent papers (Sun et al., 2020a;Song et al., 2021), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010, and SemEval2017 (Augenstein et al., 2017)  datasets are often used as the zero-shot test sets to verify the robustness of the KE models trained by the KP20k dataset. Furthermore, KE tasks have also been organized on newswire articles in English, e.g., DUC2001 (Wan and Xiao, 2008b). Table 1 summarizes the statistics of several commonly used benchmark datasets.", "filtered_refids": [["b43", "b29", "b66", null, "b22", "b38", "b2"], ["b43", "b52", "b29", null, "b22", "b51", "b61", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1112, "num_references": 15}
{"corpusid_sectionid": "258378191-s7", "title": "A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models", "date": 2023, "section_title": "Two-Stage Unsupervised Keyphrase Extraction Models", "section": "As noted before, unsupervised keyphrase extraction systems generally extract a set of phrases from the source document as candidates by using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum (Hasan and Ng, 2014). The main steps of the commonly used candidate keyphrases extraction methods for the recent unsupervised keyphrase extraction models are as follows, (1) tokenizing the document and tagging the document with partof-speech (POS) tags via the StanfordCoreNLP Tools 3 ; (2) extracting candidate phrases based on part-of-speech tags by the regular expression via the python package NLTK 4 . Furthermore, different pruning heuristics have been designed for pruning candidates that are unlikely to be keyphrases to obtain a better candidate set (Huang et al., 2006;Kumar and Srinathan, 2008;El-Beltagy and Rafea, 2009;Newman et al., 2012;You et al., 2009). After obtaining candidates, keyphrases are determined by estimating the importance of each candidate through various strategies. Here, to facilitate the introduction, we divide the methods of importance estimation into two categories, namely, traditional methods and embedding-based methods. Traditional unsupervised keyphrase extraction systems can be mainly divided into statistics-based (Jones, 2004;Campos et al., 2018b), topic-based (Liu et al., 2009;Jardine and Teufel, 2014), and graph-based (Mihalcea and Tarau, 2004;Wan and Xiao, 2008b;Bougouin et al., 2013;Florescu and Caragea, 2017b) methods. Generally, these models primarily use different features of documents (e.g., word frequency, position, linguistic properties, topic, length, the relationship between words, external knowledge-based information, etc.) to estimate the importance of each candidate phrase and discriminate whether a candidate phrase is a keyphrase (Hasan and Ng, 2014; Papagiannopoulou and Tsoumakas, 2019).\n\nHowever, these traditional unsupervised models estimate the importance scores of candidate phrases based on the surface-level features, ignoring the high-level features (e.g., syntactic and semantic information) of natural languages, which leads to extract wrong keyphrases. Therefore, recent studies focus on embedding-based models (Wang et al., 2015;Mahata et al., 2018a;Papagiannopoulou and Tsoumakas, 2018;Sahrawat et al., 2020;Kulkarni et al., 2022;Song et al., 2022b), which leverage pretrained embeddings (containing high-level features) to obtain phrase and document embeddings and calculate the importance scores of candidate phrases for extracting keyphrases. Wang et al. (2015) is the first work to explore utilizing word embedding and frequency to generate weighted edges between words, then using the weighted PageRank algorithm to compute and rank candidate scores. Key2vec (Mahata et al., 2018a) proposes an effective way of processing text documents for training multi-word phrase embeddings that are used for topic representations of scientific articles and ranking of keyphrases extracted from them using the topic-weighted PageRank algorithm. Mahata et al. (2018b) uses a combination of theme-weighted personalized PageRank algorithm and neural phrase embeddings for extracting and ranking keyphrases. EmbedRank (Bennani-Smires et al., 2018) ranks candidate phrases by measuring the semantic similarity between each candidate phrase and document embeddings.\n\nWith the development of pre-trained language models (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERta ), SIFRank 5 (Sun et al., 2020b) improves candidate phrase and document embeddings from EmbedRank with the pre-trained language model ELMo (Peters et al., 2018) and achieves better performance. JointGL 6 (Liang et al., 2021) integrates boundary-aware phrase centrality (the semantic similarities are calculated between all candidate phrases for identifying which candidate is better) and phrase-document relevance (the semantic similarities are calculated between candidate phrases and their corresponding document) from both local and global views, then used both jointly to determine the importance of each candidate. Attention-Rank 7 (Ding and Luo, 2021) adopts a pre-trained language model to calculate the self-attention of a candidate within the context of a sentence, and the cross-attention between a candidate and sentences within the source document to evaluate the local and global importance of each candidate. MDERank 8  proposes to rank candidates using the similarity between the BERT embeddings of the source document and the masked document. Totally, these models achieve state-ofthe-art performance in the unsupervised keyphrase extraction task, benefiting from the development of representation learning.", "filtered_refids": [["b15", "b67", "b39", "b31", "b41", "b27", "b35", "b21", "b61", "b24", "b8", "b5", "b17"], ["b44", "b47", "b50", "b30", "b62", "b37", "b36"], ["b53", "b13", "b46", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 4752, "num_references": 24}
{"corpusid_sectionid": "246863418-s1", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing", "date": "2022-02-15", "section_title": "Skimming", "section": "Skimming techniques, as summarized in Table 1, skip some time steps or allocate different computation on them. Intuitively, skimming matches how human beings efficiently read text and extract information from it (Li et al., 2019). By em-phasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies. The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.\n\nSkipping and Early Stopping Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early. LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU). At each step, the current states are used to compute a \"jumping softmax\", which decides how many steps to jump forward and whether to stop reading. LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions. The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6\u00d7 speed-up. Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update. If the gate decides to skip a time step, the hidden states will be directly copied without any update.\n\nTo stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level. Jumper  first splits a paragraph to several sub-sentences and encodes them into sentence embeddings. They then apply early stopping at a sentence when the policy network decides to stop reading. Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text. They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early. Yu et al. (2018) add a rereading operation to LSTM-Jump (Yu et al., 2017) which allows the model to stay on the current time step, allocating more computation to important information.\n\nThe aforementioned techniques can only go forward, which makes it impossible to regret if hav-", "filtered_refids": [["b30"], ["b64", "b3"], ["b30", "b50", "b64", "b65"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2627, "num_references": 7}
{"corpusid_sectionid": "246863418-s2", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing", "date": "2022-02-15", "section_title": "Method Decision based on", "section": "Operation options LSTM-Jump (Yu et al., 2017) hidden states skip multiple steps; stop reading Skip RNN (Campos et al., 2018) states of the update gate; hidden states skip a single step ReasoNet (Shen et al., 2017) hidden states stop reading Jumper  input sentence; hidden states stop reading RIM (Li et al., 2019) input sentence; hidden states skip a single step; stop reading Yu et al. (2018) hidden states skip multiple steps; stop reading; re-read LSTM-Shuttle (Fu and Ma, 2018) hidden states skip multiple steps; jump back multiples steps Struc. Jump-LSTM (Hansen et al., 2019) hidden states stop reading; jump to next (,;) or (.!?)\n\nPoWER (Goyal et al., 2020) attention drop tokens TR-BERT (Ye et al., 2021) hidden states forward tokens LAT (Kim and Cho, 2021) attention forward tokens LTP  attention drop tokens Transkimmer (Guan et al., 2022) hidden states forward tokens VCRNN (Jernite et al., 2017) input token; hidden states partial update with zero-masked weights Skim-RNN (Seo et al., 2018) input token; hidden states partial update with a small RNN HM-RNN (Chung et al., 2017) states of the gates skip a single step; \"flush\" FHRNN (Ke et al., 2018) query; hidden states update the upper RNN layer ing jumped over important information. LSTM-Shuttle (Fu and Ma, 2018) proposes a bidirectional shuttling mechanism, which can jump multiple time steps both forward and backward, allowing the model to ignore unimportant information and recover lost information if needed. Structural information that naturally exists in sentences can also play a role in skimming. Structural Jump-LSTM (Hansen et al., 2019) can jump to the next word, next sub-sentence separator (a comma or colon), next sentence end symbols (a period, exclamation mark or question mark), or to the end of the text (i.e., stop reading).\n\nIn the era of Transformers, there have been works attempting to reduce computation by either skip tokens at higher layers or forward tokens to higher layers. The PoWER-BERT model (Goyal et al., 2020) reduces the number of tokens processed by each Transformer layer based on their attention scores. The number of tokens to be dropped, referred to as the \"schedule,\" is optimized by combining the sparsity of a soft mask layer with the original loss function. This results in an improved balance between accuracy and processing time. TR-BERT (Ye et al., 2021) uses a dynamic approach to determine which tokens to skip, using reinforcement learning to train the model with a reward system that prioritizes classifier confidence while also penalizing the number of tokens retained. In contrast to PoWER-BERT, TR-BERT passes the skipped tokens to the final layer rather than discarding them. The Length-Adaptive Transformer (LAT, Kim and Cho, 2021) utilizes LengthDrop to randomly skip tokens during pretraining, aiming to close the gap between pretraining and fine-tuning. The schedule for LAT is found through an evolutionary search algorithm. LTP ) trains a threshold for each Transformer layer, instead of following a predetermined schedule. It simply drops tokens with attention scores lower than the learned threshold. Transkimmer (Guan et al., 2022) incorporates a skim predictor module, consisting of a small MLP and Gumbel-Softmax reparameterization, before each layer. This module outputs a mask to determine whether a token should be dropped, and a skim loss is used to optimize the ratio of skipped tokens to total tokens, promoting sparsity.\n\nComputation Reduction Different from skipping, computation reduction applies a reduced computational workload for some time steps instead of skipping it completely. VCRNN (Jernite et al., 2017) explores a scheduler to decide which proportion of computation to use for each time step. Upon making the decision, only the corresponding proportion of the weight matrix will be used to update the hidden states while the rest part of the weight matrix will be masked out with zero.\n\nInstead of using part of weights to update the hidden states, Skim-RNN (Seo et al., 2018) has a big RNN and a separate small RNN. At each time step, the model decides whether to read or skim based on hidden states from the last time step and the input token. If the model decides to skim, the small RNN will update only a fraction of the hid-den states. Otherwise, a regular full update will be conducted by the big RNN.\n\nDynamic Hierarchical RNN Different from the aforementioned two categories of skimming, dynamic hierarchical RNN can increase computation by calling the upper layer RNN when needed. HM-RNN (Chung et al., 2017) automatically discovers the hierarchical multi-scale structure in the data for a hierarchical RNN architecture. In addition to the update and copy operations as in Skip RNN (Campos et al., 2018), they add a flush operation which ejects the summarized representation of the current time step to the upper layer and reinitializes the states for the next time step.\n\nIn question answering, only a small portion of tokens are relevant and can be used to answer the question while the rest can be safely skimmed. Based on this observation, Focused Hierarchical RNN (Ke et al., 2018) aims to only pick up information that is relevant to the query for question answering. It applies a binary gate to control whether to update the upper layer of the RNN, based on the current hidden states of the lower-level RNN and the question embedding.", "filtered_refids": [["b15", "b10", "b50", "b30", "b64", "b65", "b3"], ["b15", "b63", "b10", "b4", "b14", "b47", "b13", "b19", "b23", "b24"], ["b24", "b14", "b63", "b13"], ["b19"], ["b47"], ["b4", "b3"], ["b23"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 31, "num_chars": 5405, "num_references": 26}
{"corpusid_sectionid": "246863418-s4", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing", "date": "2022-02-15", "section_title": "MoE Layers with Learned Routing", "section": "A straightforward idea to implement MoE is to learn a router to allocate inputs to experts. Sparsely-Gated MoE layer  contains up to thousands of feed-forward sub-networks with a trainable gating network which determines a sparse combination of these experts to use for each example. There are two major challenges to address: (1) Sparsity. The gating network predicts a softmax weight for the experts based on the input. The gating network is trained by simple back-propagation, together with other parts of the model. Then, only the top-k experts in the layer will be activated based on the softmax prediction of the gating network. They insert one MoE layer between stacked LSTM layers and achieve improvement on language modeling and machine translation tasks.\n\n(2) Load balancing.  observe a self-reinforcing phenomenon that the gating network tends to converge to a state where it always produces large weights for the same few experts. They resolve the problem by defining the importance of an expert relative to a batch of training examples to be batch-wise sum of the gate values for that expert. Then, they introduce an additional loss, the square of the coefficient of variation of the set of importance values, to encourage a more balanced update during training. Besides encouraging a balanced update, the authors also introduce a loss function with a smooth estimator that estimate the number of examples assigned to each expert for a batch of inputs, to encourage experts to receive roughly equal numbers of training examples.\n\nGShard  enables scaling up multilingual neural machine translation Transformer beyond 600 billion parameters. It adapts Sparsely-Gated MoE  to Transformer (Vaswani et al., 2017) by replacing every other feed forward layer with an MoE layer, which routes to top-2 experts. When scaling to multiple devices, the MoE layer is sharded across devices, i.e., each device has different allocated experts, while all other layers are replicated. To achieve workload balance, GShard employs a threshold, namely expert capacity, to limit the maximum number of tokens processed by one single expert. They also introduce a local group dispatching mechanism, which partitions all tokens in a training batch evenly into groups to be processed independently in parallel, to balance the overall workload. Following , they use an additional loss to enforce even top-2 expert capacity; local group dispatching; auxiliary loss; random routing Switch (Fedus et al., 2021) Transformer (T5) top-1 expert capacity; auxiliary loss BASE (Lewis et al., 2021) Transformer (GPT) top-1 linear assignment M6-T  Transformer (M6) k top-1 expert capacity DTS (Nie et al., 2021) Transformer (GPT) dynamic sparsity scheduler\n\nHash (Roller et al., 2021) Transformer hash deterministic hash THOR (Zuo et al., 2022) Transformer (NMT) random random selection allocation for experts. Additionally, they propose a random routing mechanism, which only routes to the second-best expert with probability proportional to its weight, to simplify sparse training. Switch Transformer (Fedus et al., 2021) aims to simplify the Sparsely-Gated MoE  for efficiency and performance. They propose a Switch Layer which only routes to one expert at a time, to reduce gating computation, batch size and communication costs. Switch Transformer inherits expert capacity and an auxiliary load balancing loss from GShard . Combined with low-precision training, compared to T5-Base and T5-Large (Raffel et al., 2020), Switch Transformer obtains up to 7\u00d7 increases in pretraining speed with the same computational resources. They further scale Switch Transformer to more than 1.5 trillion parameters and achieve 4\u00d7 speed-up over T5-XXL.\n\nThe Balanced Assignment of Sparse Experts (BASE) layer (Lewis et al., 2021) formulates token-to-expert allocation as a linear assignment problem and solves it with the auction algorithm (Bertsekas, 1992). This allows an optimal assignment in which each expert receives an equal number of tokens, improving efficiency and getting rid of the expert capacity and auxiliary loss in previous works. The experiments show that BASE layers are more efficient for training compared to Sparsely-Gated MoE layers  and Switch Layers (Fedus et al., 2021), and can successfully learn a good balanced routing without any auxiliary balancing loss.\n\nM6 ) is a multi-modal multitask Transformer, trained in the same way as Switch Transformer (Fedus et al., 2021), scaling up to 100B parameters. Following this, M6-T  splits experts into k prototypes (i.e., groups of experts). In each forward pass, each token is sent to the k prototypes, within which the top-1 routing is done lo-cally. The experiments demonstrate this \"k top-1\" strategy outperforms the top-1 routing in Switch Transformer (Fedus et al., 2021) while being more computation-efficient than \"top-k\" routing. They also claim that the load balancing loss may be ineffective for improving the performance of an MoE model, although it can indeed help balance the workload. They subsequently train a 1 trillion parameter model with the finding.\n\nDense-to-Sparse gate (Nie et al., 2021) begins as a dense gate that routes tokens to all experts then gradually learns to become sparser and route tokens to fewer experts, demonstrating higher training efficiency in experiments. Their experiments confirm the finding in  that an auxiliary load balancing loss does not improve the model performance.\n\nMoE Layer with Unlearnable Routing Although learning-based routing has shown effectiveness only with the help of complicated load balancing mechanisms, recent studies have attempted to get rid of those. Hash Layer (Roller et al., 2021) simplifies routing by using a parameter-free hashing function to route tokens to specific experts. This design eliminates the need for a load balancing loss and sophisticated assignment algorithms. They also study the performance of different hashing techniques, hash sizes and input features, and conclude that balanced and random hashes focused on the most local features work best. The experiments show that a Hash Layer achieves comparable performance with a Switch Layer (Fedus et al., 2021) and BASE Layer (Lewis et al., 2021).\n\nTHOR (Zuo et al., 2022) is a special form of MoE layer, which completely discards the conditional routing mechanism and instead optimizes the consistency between a randomly selected pair of experts. During inference, one expert will be randomly selected to be activated.", "filtered_refids": [[], [], ["b53", "b9", "b28", null], ["b43", "b41", "b9", "b73"], ["b1", "b28", "b9"], ["b9"], [null], ["b43", "b9", "b28"], ["b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 41, "num_chars": 6497, "num_references": 17}
{"corpusid_sectionid": "254877753-s1", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "What is Reasoning?", "section": "Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018). Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information. Although \"reasoning\" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things. To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized:\n\nDeductive reasoning. Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises. In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true. For example:\n\n\u2022 Premise: All mammals have kidneys. \u2022 Premise: All whales are mammals. \u2022 Conclusion: All whales have kidneys.\n\nInductive reasoning. Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence. The conclusion is likely to be true based on the available evidence, but it is not necessarily certain. For example:\n\n\u2022 Observation: Every time we see a creature with wings, it is a bird. \u2022 Observation: We see a creature with wings. \u2022 Conclusion: The creature is likely to be a bird.\n\nAbductive reasoning. Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations. The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain. For example:\n\n\u2022 Observation: The car cannot start and there is a puddle of liquid under the engine. \u2022 Conclusion: The most likely explanation is that the car has a leak in the radiator.\n\nOther types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.\n\nFormal Reasoning vs Informal Reasoning. Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic. Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life. Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable. We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.\n\nReasoning in Language Models. The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails. In the literature, the term \"reasoning\" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia). Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia). In this paper, we encompass various forms of reasoning, with a particular focus on \"informal deductive reasoning\" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.", "filtered_refids": [["b75", null, "b34", "b76"], [], [], [], [], [], [], [], [null], ["b80", null, "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 3943, "num_references": 8}
{"corpusid_sectionid": "254877753-s3", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "Fully Supervised Finetuning", "section": "Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets. For example, Rajani et al.\n\n(2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019). Talmor et al. (2020) train RoBERTa (Liu et al., 2019) to perform reasoning/inference based on both implicit pre-trained knowledge and explicit free-text statements. Hendrycks et al. (2021) finetune pretrained 2 It is important to note that the term \"reasoning\" in this paper does not necessarily imply that LLMs are truly capable of reasoning or that they are able to reason in the same way that humans do. We will discuss this issue in more detail in \u00a76. language models to solve competition mathematics problems by generating full step-by-step solutions, though the accuracy is relatively low. Nye et al. (2022) train language models to do multi-step reasoning for program synthesis/execution by generating \"scratchpads\", i.e., intermediate computations, before producing the final answers. We refer the reader to Helwe et al. (2021); Bhargava and Ng (2022)'s survey for more studies in this line.\n\nThere are two major limitations of fully supervised finetuning. First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create. Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.", "filtered_refids": [[], ["b40", "b28", "b69", "b51", "b68"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1828, "num_references": 5}
{"corpusid_sectionid": "254877753-s5", "title": "Towards Reasoning in Large Language Models: A Survey", "date": "2022-12-20", "section_title": "Chain of Thought and Its Variants", "section": "To encourage LLMs to engage in reasoning rather than simply providing answers directly, we may guide LLMs to generate \"reasoning\" explicitly. One approach for doing this is chain-of-thought prompting, proposed by Wei et al. (2022b). This approach involves providing a few examples of \"chain of thought\" (CoT), which are intermediate natural language reasoning steps, in the prompt to LLMs ( Figure 2). Specifically, in CoT prompting, \u27e8input, output\u27e9 demonstrations are replaced with \u27e8input, chain of thought, output\u27e9 triples, e.g., \"[input] Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? [chain of thought] Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. [output] The answer is 11.\" In this way, given a target question, the model learns to generate explicit ratio- Input* Figure 2: An illustration of Chain-of-Thought Prompting and Rationale Engineering, where asterisk (*) denotes the target problem to be solved.\n\nnale before producing the final answer. Experimental results show that this simple idea can improve LLMs' few-shot performance on arithmetic, symbolic, and commonsense reasoning tasks, sometimes to a striking degree.\n\nThere are several variants of chain-of-thought prompting that have been proposed in the literature, in a different form or to solve a specific problem.\n\nDifferent Form: Kojima et al. (2022) introduce Zero-shot-CoT, in which LLMs are simply prompted with the phrase \"Let's think step by step\" after the input, in order to elicit reasoning without the need for few-shot demonstrations. Madaan et al. Specific Problem/Setting: Before chain of thought, Nye et al. (2022) also try to use intermediate computations, named \"scratchpads\", to improve language models' reasoning performance in both finetuning and few-shot regimes, with a particular focus on programs. Shi et al. (2022) attempt to solve multilingual reasoning tasks with CoT in the native language, CoT in English (regardless of the problem language), and CoT in English (with the problem translated to English). Chen (2022) apply CoT to table-based reasoning, finding that LLMs can achieve strong performance on table tasks with only one exemplar. Prystawski et al. (2022) demonstrate that CoT can improve LLMs' performance on paraphrase selection for metaphors. Lu et al. (2022) apply chain of thought to solve multimodal science questions.", "filtered_refids": [["b78"], [], [], ["b40", "b29", "b49", "b62", "b16"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2460, "num_references": 6}
{"corpusid_sectionid": "254854317-s11", "title": "The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges", "date": "2022-12-19", "section_title": "Linguistic-Driven Approaches", "section": "Equivalence Constraint In a well-formed codeswitched sentence, the switching takes place at those points where the grammatical constraints of both languages are satisfied (Poplack, 1980). Fung (2012, 2013) incorporate this syntactic constraint to a statistical code-switch language model (LM) and evaluate the model on Chinese-English code-switched speech recognition. On the same line of work, Pratapa et al. (2018a); Pratapa and Choudhury (2021) implement the same constraint to Hindi-English CSW data by producing parse trees of parallel sentences and matching the surface order of child nodes in the trees. Winata et al. (2019c) apply the constraint to generate synthetic CSW text and find that combining the real CSW data with synthetic CSW data can effectively improve the perplexity. They also treat parallel sentences as a linear structure and only allow switching on non-crossing alignments.\n\nMatrix-Embedded Language Framework (MLF) Myers-Scotton (1997) proposed that in bilingual CSW, there exists an asymmetrical relationship between the dominant matrix language and the subordinate embedded language.\n\nMatrix language provides the frame of the sentence by governing all or most of the most of the grammatical morphemes as well as word order, whereas syntactic elements that bear no or only limited grammatical function can be provided by the embedded language (Johanson, 1999;Myers-Scotton, 2005 Functional Head Constraint Belazi et al. (1994) posit that it is impossible to switch languages between a functional head and its complement because of the strong relationship between the two constituents. Li and Fung (2014) use the constraint of the LM by first expanding the search network with a translation model and then using parsing to restrict paths to those permissible under the constraint.", "filtered_refids": [[null, "b18"], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1809, "num_references": 4}
{"corpusid_sectionid": "254854317-s13", "title": "The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges", "date": "2022-12-19", "section_title": "The Era of Statistical Methods", "section": "The research on CSW is also influenced by the progress and development of machine learning. According to Figure 5, starting in 2006, statistical methods have been adapted to CSW research, while before that year, the approaches were mainly rule-based. There are common statistical methods for text classification used in the literature, such as Naive Bayes (Solorio and Liu, 2008a) and Support Vector Machine (SVM) (Solorio and Liu, 2008b). Conditional Random Field (CRF) (Sutton et al., 2012) is also widely seen in the literature for sequence labeling, such as Part-of-Speech (POS) tagging (Vyas et al., 2014), Named Entity Recognition (NER), and word-level language identification (Lin et al., 2014;Chittaranjan et al., 2014;Jain and Bhat, 2014). HMM-based models have been used in speech-related tasks, such as speech recognition (Weiner et al., 2012a;Li and Fung, 2013) and text synthesis (Qian et al., 2008;Shuang et al., 2010;He et al., 2012).", "filtered_refids": [["b42", null, "b6"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 949, "num_references": 3}
{"corpusid_sectionid": "254854317-s14", "title": "The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges", "date": "2022-12-19", "section_title": "Utilizing Neural Networks", "section": "Following general NLP trends, we see the adoption of neural methods and pre-trained models growing in popularity over time. In contrast, the statistical and rule-based approaches are diminishing. Compared to ISCA, we see more adaptation of the pre-training model. This is because ACL work is more text-based focused, where pre-trained LMs are more widely available.\n\nNeural-Based Models Figure 5 shows that the trend of using neural-based models started in 2013, and the usage of rule/linguistic constraint and statistical methods diminished gradually through time, but they are still used even with a low percentage. RNN and LSTM architectures are commonly used in sequence modeling, such as language modeling (Adel et al., 2013;Vu and Schultz, 2014;Adel et al., 2014c;Winata et al., 2018a;Garg et al., 2018a;Winata et al., 2019c) and CSW identification (Samih et al., 2016a). DNN-based and hybrid HMM-DNN models are used in speech recognition models .\n\nPre-trained Embeddings Pre-trained embeddings are used to complement neural-based approaches by initializing the embedding layer. Common pre-trained embeddings used in the literature are monolingual subword-based embeddings, Fast-Text (Joulin et al., 2016), and aligned-embeddings MUSE (Conneau et al., 2017). A standard method to utilize monolingual embeddings is to concatenate or sum two or more embeddings from different languages (Trivedi et al., 2018). A more recent approach is to apply an attention mechanism to merge embeddings and form metaembeddings (Winata et al., 2019a,b). Characterbased embeddings have also been explored in the literature to address the out-of-vocabulary issues on word-embeddings (Winata et al., 2018b;Attia et al., 2018;Aguilar et al., 2021). Another approach is to train bilingual embeddings using real and synthetic CSW data (Pratapa et al., 2018b). In the speech domain, Lovenia et al. (2022) utilize wav2vec 2.0 (Baevski et al., 2020) as a starting model before fine-tuning.\n\nLanguage Models Many pre-trained model approaches utilize multilingual LMs, such as mBERT or XLM-R to deal with CSW data (Khanuja et al., 2020b;Aguilar and Solorio, 2020;Pant and Dadu, 2020;Patwa et al., 2020;Winata et al., 2021a). These models are often fine-tuned with the downstream task or with CSW text to better adapt to the languages. Some downstream fine-tuning approaches use synthetic CSW data due to a lack of available datasets. Aguilar et al. (2021) propose a character-based subword module (char2subword) of the mBERT that learns the subword embedding that is suitable for modeling the noisy CSW text. Winata et al. (2021a) compare the performance of the multilingual LM versus the language-specific LM for CSW context. While XLM-R provides the best result, it is also computationally heavy. There needed to be more exploration of larger models. We see that pre-trained LMs provide better empirical results on current benchmark tasks and enables an end-to-end approach. Therefore, one can theoretically work on CSW tasks without any linguistic understanding of the language, assuming the dataset for model finetuning is available. However, the downside is that there is little understanding of how and when the LMs would fail, thus we encourage more interpretability work on these LMs in CSW setting.  et al., 2023), especially on different CSW variations. CSW style can vary in different regions of the world, and it would be interesting to gather more datasets on unexplored and unknown styles, which can be useful for further research and investigation on linguistics and NLP. Therefore, one future direction is to broaden the language scope of CSW research.", "filtered_refids": [[], [null, "b18", "b17"], [null, "b19"], [null, "b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 3645, "num_references": 7}
{"corpusid_sectionid": "259108815-s1", "title": "Mapping Brains with Language Models: A Survey", "date": "2023-06-08", "section_title": "Datasets", "section": "To infer a mapping between language models and brains, researchers rely on datasets in which brain activity is recorded in response to linguistic stimuli. In some studies, the stimuli are single words (Mitchell et al., 2008;Anderson et al., 2017) or sentences displayed on a screen (Pereira et al., 2018). In others, participants read longer stories (Wehbe et al., 2014a;Bhattasali et al., 2020;Nastase et al., 2021) or listened to speech or podcasts (Huth et al., 2016;Antonello et al., 2021). Table 1 lists publicly available datasets that have been used in the context of mapping language models to and from recordings of brain response. Differences between the datasets -the number of participants, the equipment, the experimental setup, pre-processing steps, and probabilistic corrections -should lead us to expect some variation in what researchers have concluded (Hollenstein et al., 2020).\n\n3 How to predict brain activity?\n\nIn this section, we survey work in which neural responses are predicted from linguistic representations. Such work typically aims to shed light on how language functions in the brain. One of the earliest studies exploring the mapping between brain and language representations is by Mitchell et al. (2008), who trained a linear regression model on a set of word representations extracted from 60 nouns using 115 semantic features based on cooccurrence statistics, to predict the corresponding fMRI representations of the same nouns. They use pair-wise matching accuracy evaluation, extracting two words w and w \u2032 for evaluation, and showed that the predicted fMRI for a word w was closer to the real fMRI image for w than to the real fMRI image for w \u2032 , at above-chance levels. Mitchell et al. (2008) also report percentile rank results, ranking predicted fMRI images by similarity with the real image of w. We discuss how the metrics relate in \u00a76.\n\nThe dataset of Mitchell et al. (2008)is also used by Murphy et al. (2012), who extract linguistic features from part-of-speech taggers, stemmers, and dependency parsers, showing that dependency parsers are the most successful in predicting brain activity. They also use leave-2-out pair-matching as their performance metric.\n\nLater on, Wehbe et al. (2014a) moved on to predicting brain activation patterns for entire sentences rather than for isolated words. They recorded fMRI neural response measurements while participants read a chapter from Harry Potter and the Sorcerer's Stone, then extracted a set of 195 features for each word (ranging from semantic, syntactic properties to visual and discourse-level features) to train a comprehensive generative model that would then predict the time series of the fMRI activity observed when the participants read that passage. Leave-2-out pair-matching accuracy is used for evaluation. Huth et al. (2016), in contrast, use fMRI recordings of participants listening to spoken narrative stories, representing each word in the corpus as a 985-dimensional vector encoding semantic information driven by co-occurrence statistics. They train per-voxel linear regression models and evaluate their predicted per-word fMRI images by their per-voxel Pearson correlation with the real fMRI images, showing that 3-4 dimensions explained a significant amount of variance in the FMRI data. Wehbe et al. (2014b) are among the first to use neural language models, using recurrent models to compute contextualized embeddings, hidden state vectors of previous words, and word probabilities.  of participants reading Harry Potter, obtained in a follow-up study to Wehbe et al. (2014a). From the three sets of representations, they then train linear regression models to predict the MEG vectors corresponding to each word, and the regression models are then evaluated by computing pair-matching accuracy.\n\nSimilarly, S\u00f8gaard (2016) evaluates static word embeddings on the data from Wehbe et al. (2014a), learning linear transformation from word embeddings into an fMRI vector space. The predictions are evaluated through mean squared error (MSE).\n\nJain and Huth (2018) evaluate recurrent language models against the fMRI dataset from Huth et al. (2016). Their findings show that contextual language model representations align significantly better (to brain response) compared to static word embedding models. Their evaluation metric is the total sum of explained variance 1 Following this, Schwartz et al. (2019) use attention-based transformer language models for brain mapping. They finetune BERT (Devlin et al., 2019)to predict neural response measurements from the Harry Potter dataset, showing that the fine-tuned models have representations that encode more brain-activity-relevant language information than the non-finetuned models. They rely on pair-matching accuracy as their performance metric.\n\nAs in S\u00f8gaard (2016), Zhang et al. (2020) map static word embeddings into the vector space of the neural response measurements (fMRI). They introduce a new dataset of such measurements from subjects listening to natural stories. They rely on explained variance as their performance metric.\n\nToneva and Wehbe (2019) evaluate word and sequence embeddings from 4 recurrent and attention-based transformer language models, using the Harry Potter fMRI dataset. They evaluate models across layers, context lengths, and attention types, using pairwise matching accuracy as their performance metric. In a later study, Toneva et al. (2022a) induce compositional semantic representations of \"supra-word meaning\" which they then use to predict neural responses across regions of interest, evaluating their models using Pearson correlation.\n\nAlso using the Harry Potter data,  evaluate five models, one static and four contextualized, relying on a variant of representational similarity analysis (Kriegeskorte et al., 2008). The results suggest that models provide representations of local contexts that are well-aligned to neural measurements. However, as information from further away context is integrated by the models, representations become less aligned to neural measurements.\n\nIn a large-scale study, Schrimpf et al. (2021) examine the relationships between 43 diverse stateof-the-art neural network models (including embedding models, recurrent models, and transformers) across three datasets (two fMRI, one electrocardiography). They rely on a metric they term Brain Score which involves normalising the Pearson correlation by a noise ceiling. Their results show that transformer-based models perform better than recurrent or static models, and larger models perform better than smaller ones.\n\nSimilarly, in , the Schoffelen et al. (2019) fMRI and MEG datasets are used to compare a variety of transformer architectures. They study how architectural details, training settings, and the linguistic performance of these models independently account for the generation of brain correspondent representations. The results suggest that the better language models are at predicting words from context, the better their activations linearly map onto those of the brain. Antonello et al. (2021) evaluate three static and five attention-based transformer models, in combination with four fine-tuning tasks and two machine translation models. They train linear regression models to evaluate their word-level representations against a new fMRI dataset from participants listening to podcast stories. They find a low-dimensional structure in language representations that can predict brain responses. In a similar setting, Antonello and Huth (2022) examine why some features fit the brain data better arguing that the reason is that they capture various linguistic phenomena.\n\nReddy and Wehbe (2021) evaluate syntactic features in conjunction with BERT representations, finding that syntax explains additional variance in brain activity in various parts of the language system, even while controlling for complexity metrics that capture processing load.\n\nIn a series of studies Caucheteux et al. (2021Caucheteux et al. ( , 2022b investigate GPT2's activations in predicting brain signals using the Nastase et al. (2021) dataset. Their evaluation metric is Brain Score (Schrimpf et al., 2018). To determine which factors affect the brain encoding Pasquiou et al. (2022) examine the impact of test loss, training corpus, model architecture, and fine-tuning in various models using the Li et al. (2022) dataset. They evaluate model performance using Pearson Correlation.\n\nOota et al. (2022a) study the impact of context size in language models on how they align with neural response measurements. They use the Nastase et al. (2021) dataset and evaluate recurrent and attention-based transformer architectures. In a later study, Oota et al. (2022b) use the Pereira et al. (2018) dataset and evaluate BERTbase models (fine-tuned for various NLP tasks). They showed that neural response predictions from ridge regression with BERT-base models fine-tuned for coreference resolution, NER, and shallow syntactic parsing explained more variance for Pereira et al. (2018) response measurements. On the other hand, tasks such as paraphrase generation, summarization, and natural language inference led to better encoding performance for the Nastase et al. (2021) data (audio). Using the same dataset, in Oota et al. (2022c) it is shown that the presence of surface, syntactic, and semantic linguistic information is crucial for the alignment across all layers of the language model. They use pairwise matching accuracy and/or Pearson correlation as their performance metrics in these studies.\n\nAw and Toneva (2023) extract feature representations from four attention-based transformer models. They evaluate the impact of fine-tuning on the BookSum dataset (Kryscinski et al., 2021). All models are used to predict brain activity on the Harry Potter data. Pairwise matching accuracy and Pearson correlation are their performance metrics. Merlin and Toneva (2022) focus more narrowly on variants of GPT-2, showing that improvements in alignment with brain recordings are probably not because of the next-word prediction task or wordlevel semantics, but due to multi-word semantics. Their reported metric is Pearson correlation.\n\nIntermediate summary The above studies differ in many respects. Several metrics are used: pairwise-matching accuracy, 2 Pearson correlation (or Brain Score), mean squared error, and representational similarity analysis. Even studies that report the same performance metrics are not directly comparable because they often report on results on different datasets and use slightly different protocols, e.g., Murphy et al. (2012) and Wehbe et al. (2014b). Beinborn et al. (2023) compare various encoding experiments and receive very diverse results for different evaluation metrics. The diversity of metrics and data renders a direct comparison difficult. To remedy this, we consider how the metrics compare in \u00a76.", "filtered_refids": [["b67", "b10", "b40", "b50", "b25", "b27", "b22", "b5", "b3"], [], ["b40"], ["b41", "b40"], ["b67", "b68", "b27"], ["b59", "b67"], [null, "b17"], ["b69"], ["b63"], ["b31"], ["b56"], [null, "b5"], [], ["b12", "b57", "b14", "b48", "b22", "b23"], ["b45", "b22", "b50"], ["b34", "b32"], ["b68", "b8", "b41"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 71, "num_chars": 10929, "num_references": 39}
{"corpusid_sectionid": "259108815-s2", "title": "Mapping Brains with Language Models: A Survey", "date": "2023-06-08", "section_title": "How to predict linguistic stimuli?", "section": "Decoding models work in the other direction and aim to predict linguistic features of the stimuli from recordings of brain response. Pereira et al. (2018) introduce a decoder that predicts stimuli representation of semantic features given fMRI data.They introduce a novel dataset of neural responses aligned with annotation of concrete and abstract semantic categories (such as pleasure, ignorance, cooking etc.). They evaluate static word embeddings by applying ridge regression to predict per-word fMRI vectors. A separate regression model is trained per dimension, allowing for dimension-wise regularization. The model is evaluated in terms of pairwise matching accuracy, but also in terms of percentile rank, adapted to the decoding scenario.\n\nGauthier and Levy (2019) also train linear regression models which map from the response measurements in Pereira et al. (2018), but to representations of the same sentences produced by the BERT language model finetuned on different natural language understanding tasks. The regression models are evaluated using two metrics: mean squared error and average percentile rank. Their results show that fine-tuning with different NLU objectives leads to worse alignment and that, somewhat surprisingly, the only objective which does lead to better alignment is a scrambled language modeling task where the model is trained to predict scrambled sentences.\n\nMinnema and Herbelot (2019) re-examine the work of Pereira et al. (2018) using various metrics (pairwise matching accuracy, percentile rank, cosine distance, R 2 , RSA), comparing decoder models (ridge regression, perceptron, and convolutional neural networks). 3 They show that positive results are only obtained using pairwise matching accuracy. Abdou et al. (2021) investigate whether aligning language models with brain recordings can be improved by biasing their attention with annotations from syntactic or semantic formalisms. They fine-tune the BERT models using several syntactosemantic formalisms and evaluate their alignment with brain activity measurements from the Wehbe et al. (2014a) and Pereira et al. (2018) datasets. Their results -obtained using Pearson correlation as performance metric -are positive for two in three formalisms. Zou et al. (2022) propose a new evaluation method for decoding, a so-called cross-modal cloze task. They generate the data for the task from the neural response measures in Mitchell et al. (2008) and Wehbe et al. (2014a). The task itself amounts to a cloze task in which the context is prefixed by the fMRI image of the masked word. They evaluate models using precision@k. Note how this task is considerably easier than linearly mapping from language model representations into fMRI images, and precision@k results therefore cannot be compared to those obtained in other settings. Their best precision@1 scores are around 0.3, but only marginally (0.03) better than a unimodal LM.\n\nFinally, Pascual et al. (2022) try a more realistic setup by predicting language from fMRI scans of subjects not included in the training. They use the (Pereira et al., 2018) dataset and evaluate the regression models based on pairwise accuracy and precision@k (or top-k accuracy). They propose evaluating with direct classification as a more demanding setup to evaluate and understand current brain decoding models.\n\nIntermediate summary Decoding studies also differ in many respects. Several metrics are used: pairwise-matching accuracy, Pearson correlation, percentile rank, cosine distance, precision@k, and representational similarity analysis; and several datasets are used. Gauthier and Ivanova (2018) criticize the evaluation techniques of decoding studies and suggest adopting task and mechanism explicit models. It is of particular interest to our study that both Minnema and Herbelot (2019) only report positive results for pairwise matching accuracy compared to other metrics. This suggests pairwise matching accuracy is a less conservative metric (and maybe less reliable).", "filtered_refids": [["b50"], ["b50"], ["b67", "b40", "b50", "b70", "b0"], ["b47", "b50"], ["b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 4016, "num_references": 10}
{"corpusid_sectionid": "259108815-s3", "title": "Mapping Brains with Language Models: A Survey", "date": "2023-06-08", "section_title": "Performance Metrics", "section": "We present the evaluation metrics used in the above studies and discuss how they relate. See Table 2 for a summary of metrics and corresponding studies. Mitchell et al. (2008) introduce pairwise matching accuracy. Because of their small sample size, they use a leave-2-out cross-validation, which later work also adopted. The metric is a binary classification accuracy metric on a balanced dataset, so a random baseline converges toward 0.5. Many studies have relied on this metric, both in encoding and decoding (see Table 2). 4 Pearson correlationPearson correlation is another widely used metric in the studies surveyed above, measuring the linear relationship between variables, and providing insight into the strength and direction of their association. Huth et al. (2016), compute Pearson correlation between predicted and actual brain responses using Gaussian random vectors to test statistical significance. Resulting p-values are corrected for multiple comparisons within each subject using false discovery rate (FDR) (Benjamini and Hochberg, 1995). Others have used Bonferroni correction (Huth et al., 2016) or block-wise permutation test (Adolf et al., 2014) to evaluate the statistical significance of the correlation (Zhang et al., 2020). Some report R 2 (explained variance) instead of or in addition to correlation coefficients (Minnema and Herbelot, 2019;Reddy and Wehbe, 2021). Others have adopted a more elaborate extension of Pearson correlation, namely BrainScore (Schrimpf et al., 2018). Brain-Score is estimated on held-out test data, calculating Pearson's correlation between model predictions and neural recordings divided by the estimated ceiling and averaged across voxels and participants.\n\nPercentile rank was first used for encoding (Mitchell et al., 2008), but can also be used for decoding (Pereira et al., 2018;Gauthier and Levy, 2019;Minnema and Herbelot, 2019). In encoding, the predicted brain image for w is ranked along the predicted images for a set of candidate words w \u2032 by their similarity to the real (ground truth) image for w. The average rank is then reported. For decoding, they rank word vectors rather than neural response images. Note the similarity metric is unspecified, but typically cosine distance is used.\n\nMean squared error, the average of the squared differences between word vectors and neural responses, was first used for encoding in S\u00f8gaard (2016) on a held-out test split. It was also used by Gauthier and Levy (2019).\n\nRepresentational similarity analysis (RSA) was introduced in Kriegeskorte et al. (2008) as a non-parametric way to characterize structural alignment between the geometries of representations derived from disparate modalities. RSA abstracts away from activity patterns themselves and instead computes representational similarity matrices (RSMs), which characterize the information carried by a given representation method through global similarity structure. A rank correlation coefficient is computed between RSMs derived from the two spaces, providing a summary statistic indicative of the overall representational alignment between them. Being non-parametric, RSA circumvents many of the various methodological weaknesses (such as over fitting, etc.). Gauthier and Levy (2019), Minnema and Herbelot (2019), and  apply (variations of) RSA to investigate the relations between different model components, and then to study the alignment of these components with brain response.\n\nCosine similarity was used in Mitchell et al. (2008) to select between the candidate images in pairwise matching accuracy, as well as in percentile rank and RSA, but the raw cosine similarities between predicted and real images or embeddings can also be used as a metric. Minnema and Herbelot (2019) use this metric to quantify how close the predicted word vectors are to the target. Finally, Zou et al. (2022) use precision@k, a standard metric in other mapping problems, e.g., cross-lingual word embeddings (S\u00f8gaard et al., 2019).\n\nComparisons Most metrics are used to evaluate both encoding and decoding models (pairwise matching accuracy, Pearson correlation, percentile rank, MSE, RSA, cosine distance). Results for two of the most widely used metrics -pairwise matching accuracy 5 and percentile rank -tend to be around 0.7-0.8 with generally better results for more recent architectures and larger LMs. To draw conclusions across studies relying on different metrics, we need to investigate which metrics are more conservative, and how different metrics relate.\n\nPairwise matching accuracy vs. Pearson correlation It seems that pairwise matching accuracy tends to increase monotonically with Pearson correlation. Consider three sets of distances over corresponding point sets, A, B, and C. If A and B are more strongly linearly correlated than A and C, under an optimal linear mapping \u2126 (minimizing point-wise squared error distance), E[(a \u2212 b\u2126) 2 ] > E[(a \u2212 c\u2126) 2 ]. Even in this conservative setting in our synthetic experiments in Appendix A.1, the correlation between matching accuracy and percentile rank was very high,~0.9.\n\nPairwise matching accuracy vs. percentile rank Both metrics have random baseline scores of 0.5, and they will converge in the limit. If a has a percentile rank of p in a list A, it will be higher than a random member of A p percent of the time. In our experiments in Appendix A.1, the correlation converges toward 1.0, with values consistently higher than 0.8 for N = 100.\n\nPairwise matching accuracy vs. precision@k are also positively correlated. Perfect score in one entails perfect score in the other, but precision@k can of course be very small for very high values of pairwise matching accuracy (especially if the set of candidate words is big). Conversely, we can have 5 When discriminating averages over 20 images (Wehbe et al., 2014b), scores are naturally lower. saturation for high values of k, because matching accuracies higher than n\u2212k n will mean near-perfect precision@k scores. In practice, precision@k (for low values of k) will be much more conservative, however. The correlation coefficient for N = 100 (see Appendix A.1) tends to lie around 0.7.\n\nRelative strength Pairwise Matching Accuracy is a relatively permissible performance metric. To see this, consider the scenario in which all target words can be divided into two equal-sized buckets based on word length (number of characters). Say the neural responses capture nothing but this binary distinction between long and short words, but do so perfectly. Moreover, our mapping method, e.g., linear regression, learns this from training data. Now, from this alone, the pairwise matching accuracy will converge toward \u00b5 = 0.75, since our model will do perfectly (1.0) on half of the data, and exhibit random performance (0.5) on the other half. If the neural responses tracked word length (and not just the distinction between short and long words), performance would be even better. In other words, Pairwise Matching Accuracy scores around 0.7-0.8 (observed in the studies above) may only reflect very shallow processing characteristics. The fact that Minnema and Herbelot (2019) only observed good results with this metric, led them to adopt a rather critical stance, for good reasons.\n\nOther metrics are clearly more conservative. For a set of n candidate words, a random mapping will induce a precision@1-score of 1 n . While hubs may inflate scores for larger values, the metric is extremely conservative for small values of k. However, only Zou et al. (2022) use this metric, and they modify the experimental protocol substantially, making the task much easier by providing additional input to a non-linear model. The small improvement from adding neural response input is interesting, but could potentially be explained by shallow processing characteristics.\n\nThey argue that analogy testing would provide a better evaluation protocol: one would ideally use standard metrics such as semantic relatedness judgment tasks, analogy tasks, etc.\n\n[but] this is not possible due to the limited vocabulary sizes of the available brain datasets Such evaluation is possible on small scale, though, and increasingly larger fMRI datasets are becoming available (see above). Zhang et al. (2020) have identified analogical reasoning in fMRI brain activation spaces. The analogies are computed using vector offset and probe the systematicity of how semantic relations are encoded. If a model encodes the capital-of relation systematically, we can retrieve the capital of Germany by subtracting the fMRI vector for 'Paris' from the sum of our the fMRI vectors for Germany and France. This is the same kind of analogical reasoning found in language models (Mikolov et al., 2013). Garneau et al. (2021) show that the more language models satisfy analogies, the more isomorphic they are.\n\nSo far, it seems that, with the possible exception of Zhang et al. (2020), there is little evidence for structural similarities, beyond what could be induced by shallow processing characteristics, but what about all the studies that report strong Pearson correlations? Per-voxel correlation coefficients are low on average, but across the above studies, typically only around 4-40% of the voxels exhibit significant correlations (Huth et al., 2016;. Since these correlations have been replicated across different datasets, they are generally not disputed, but could still reflect rather shallow processing characteristics.\n\nOn a more positive note, several studies show that larger (and better) language models align better with neural response measurements (Schrimpf et al., 2021;. This suggests that language models in the future may align even better with such measurements, possibly reflecting properties of deep processing. Such correlations with model quality and size are positive, making the results reported above more credible.\n\nGenerally, the conclusions we can draw from the above studies are somewhat vague. There are two reasons for this: (i) Past studies have relied on permissible (pairwise matching accuracy) and ambiguous (Pearson correlation) performance metrics; and (ii) past studies have relied on small-sized datasets. We believe that this calls for a meta-analysis of the above studies. To provide grounds for such a meta-analysis, we have in this section taken steps to compare the metrics used in these studies. We leave it for future work to explore various ways effect sizes can be computed across these studies.", "filtered_refids": [["b57", "b40", "b9", "b27", null, "b69", "b53", "b38", "b2"], ["b20", "b38", "b50", "b40"], ["b20"], ["b20", "b38", "b31"], ["b70", "b38", "b60", "b40"], [], [], [], ["b68"], ["b38"], ["b70"], [], ["b69", "b18", "b35"], ["b69", "b27"], ["b56"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 74, "num_chars": 10491, "num_references": 30}
{"corpusid_sectionid": "264426545-s3", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "Prosodic Errors", "section": "Prosodic features encompass elements that influence the pronunciation of an entire word or sentence, including stress, rhythm, and intonation.Errors related to prosodic features involve the production of larger sound units.For intelligibility, prosodic features particularly play a significant role (Raux and Kawahara, 2002).This is especially true for tonal languages (Dahmen et al., 2023) where variation in the pitch can lead to words with different meanings.Prosodic errors are often languagedependent and categorized by: stress (lexical and sentence), rhythm, and intonation.Accent PCC: 68% (Rasipuram et al., 2015) ERJ (Minematsu et al., 2004) * English Japanese /68,000 200 # Utterance PCC (Luan et al., 2012).Word Intelligibility (Minematsu et al., 2011).Phoneme Errors (Ito et al., 2005) CU-CHLOE (Meng et al., 2007a Stress is the emphasis placed on certain syllables in a word or sentence.It is articulated by increasing the loudness, duration, and pitch of the stressed syllable.It can be categorized as lexical stress, if the stress is placed on syllables within the word, or sentence stress if the stress is placed on words within sentences.Mandarin learners of English have contrastive stress at the word-level that is absent in Korean, Mandarin speakers can have an advantage over Korean speakers in stress processing of English words (Wang, 2022).\n\nRythm is the pattern of stressed and unstressed syllables in a word or sentence.A language can be classified as either stress-timed or syllable-timed (Ohata, 2004;Matthews, 2014).In stress-timed languages, the duration of stressed syllables tends to dominate the overall time required to complete a sentence.Conversely, in syllable-timed languages, each syllable receives an equal amount of time during production.\n\nIntonation refers to the melodic pattern and pitch variations in speech.L2 learners of Vietnamese and Mandarin Chinese encounter significant difficulty in acquiring distinct tones, particularly if their native language lacks tonality.Such tonal languages rely on different pitch patterns to convey distinct meanings, making it challenging for learners to accurately grasp and reproduce these tonal variations (Nguyen et al., 2014;Chen et al., 2015).", "filtered_refids": [["b94", "b97", "b98", "b113", "b26", "b58", "b91", "b112", "b138"], ["b93", "b106"], [null, "b104"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2230, "num_references": 13}
{"corpusid_sectionid": "264426545-s4", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "Pronunciation Constructs", "section": "The motivation behind mastering L2 pronunciation is to communicate properly in the target language.Most of the time, these successes are measured using three pronunciation constructs (Uchihara, 2022) -Intelligibility, Comprehensibility, and Accentedness.These are perceived measures, that are partially independent with overlapping features.\n\nIntelligibility can be defined using the accuracy of the sound, word, and utterance itself along with utterance-level completeness (Abercrombie, 1949;Gooch et al., 2016).Accuracy refers where the learner pronounces each phoneme, or word in the utterance correctly.In contrast, completeness measures the percentage of words pronounced compared to the total number of words.\n\nComprehensibility, on the other hand, is defined based on the perceived ease or difficulty that listeners experience when understanding L2 speech.Fluency, defined by the smoothness of pronunciation and correct usage of pauses (Zhang et al., 2021b), is observed to be one of the key factors that determine the level of comprehensibility, along with good linguistic-knowledge and discourse-level organization (Trofimovich and Isaacs, 2012;Saito et al., 2016).\n\nAmong the three constructs, accentedness, which is defined as \"listeners' perceptions of the degree to which L2 speech is influenced by their native language and/or colored by other non-native features\" (Saito et al., 2016).It is often confused with both comprehensibility and intelligibility, influencing pronunciation assessment.The accent is an inherent trait that defines a person's identity and is one of the first things that a listener notices.It is often observed that most of the unintelligible speech is identified as highly accented whereas highly accented speech is not always unintelligible (Derwing and Munro, 1997;Kang et al., 2018;Munro and Derwing, 1995).Thus accents complicate fine-grained pronunciation assessment as it is harder to pinpoint (supra-)segment-level error.", "filtered_refids": [[], ["b49", "b0"], ["b155", "b130", "b119"], ["b59", "b100", "b31", "b119"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1966, "num_references": 9}
{"corpusid_sectionid": "264426545-s7", "title": "Automatic Pronunciation Assessment -A Review", "date": "2023-10-21", "section_title": "Classification based on Acoustic Phonetics", "section": "Classifier-based approaches explored both segmental and prosodic aspects of pronunciation.Segmental approaches involve the use of classifiers targeting specific phoneme pair errors, utilizing different acoustic features such as Mel-frequency cepstral coefficients (MFCCs) along with its first and second derivative, energy, zero-cross, and spectral features (Van Doremalen et al., 2009;Huang et al., 2020), with different techniques such as Linear Discriminant Analysis (LDA) (Truong et al., 2004;Strik et al., 2009), decision trees (Strik et al., 2009).Prosodic approaches focus on detecting lexical stress and tones, utilizing features such as energy, pitch, duration, and spectral characteristics, with classifiers like Gaussian mixture models (GMMs) (Ferrer et al., 2015), support vector machines (SVMs) (Chen and Wang, 2010;Shahin et al., 2016), and deep neural network (DNNs) (Shahin et al., 2016), and multi-distribution DNNs (Li et al., 2018a).", "filtered_refids": [["b12", "b57", "b133", "b121", "b77", "b42", "b131", "b125"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 3, "num_chars": 952, "num_references": 8}
{"corpusid_sectionid": "264451714-s3", "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models", "date": "2023-10-25", "section_title": "Non-optimized Inputs", "section": "Extracting factual knowledge from PLMs depends on providing them with short inputs that indirectly describe the sought-after information.These methods can take various forms (cloze prompts (Taylor, 1953), questions, or entities).Non-optimized inputs represent the simplest case, where the probing 2 For more details refer to Appendix A.1 inputs are not altered in any way.\n\nCloze prompts are widely used across several methods.Petroni et al. (2019) probe PLMs for factual knowledge by manually constructing clozestyle templates for several relations.Onoe et al. (2022) automatically construct cloze prompts from Wikipedia and Wikidata by masking out spans near entities of interest, in order to evaluate PLMs' knowledge about (unseen) entities.Abaho et al. (2022) construct cloze prompts from annotated PubMed abstracts to use PLMs as health outcome predictors.Chen et al. (2022) finetune PLMs using cloze prompts that consist of task descriptions alongside a few examples to elicit more facts.\n\nQuestions are the second input category.Several Question Answering datasets are used to finetune T5 models (Raffel et al., 2020), and evaluate the amount of knowledge implicitly present in their parameters in (Roberts et al., 2020).Multiple choice questions are used in (Hardalov et al., 2020) by providing PLMs with the questions followed by each option individually.The options are masked, and the final answer is selected based on the normalized log probabilities of the predicted tokens for each option.Kalo and Fichtel (2022) present a dataset based on Wikipedia, where inputs consist of several questions and answers, i.e., a few examples to implicitly indicate the task, and a similar question without an answer for evaluation.\n\nEntities are used in methods that infer relational information or generate descriptions based on these entities.Some methods depend on a simple classifier or cosine similarity between the subject and object representations to determine the presence or absence of a relation.For example, to probe for geographical knowledge, Li\u00e9tard et al. ( 2021) use fixed inputs that contain locations (e.g., countries or cities).These inputs are then used to extract representations for the respective locations from PLMs.Using these representations, the authors evaluate based on the ability of a simple classifier to solve certain tasks (e.g., predicting if two countries share border).Dufter et al. (2021) evaluate the amount of knowledge present in static word embeddings by matching a subject entity (the query) to an object entity from a pre-defined set of possible objects based on the cosine similarity between the representations of the subject and object entities.Shi et al. (2021) train generative PLMs to generate entities' descriptions while providing only the en-tities as inputs, and compare them to ground truth descriptions.", "filtered_refids": [["b99"], ["b11", "b77", "b82", "b0"], ["b30", "b86", "b89", "b47"], ["b22", "b95"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2859, "num_references": 11}
{"corpusid_sectionid": "264451714-s4", "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models", "date": "2023-10-25", "section_title": "Optimized Inputs", "section": "Probing inputs contribute substantially to the probing procedure.PLMs are sensitive to the inputs (Petroni et al., 2019;Jiang et al., 2020b;Elazar et al., 2021), and even syntactical variations or distractors, that do not alter the meaning, cause the PLM's predictions to change (Heinzerling and Inui, 2021;Longpre et al., 2021;Pandia and Ettinger, 2021;Podkorytov et al., 2021;Li et al., 2022a).Therefore, depending on the probing inputs, the estimate on factual knowledge we obtain may vary significantly.Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs.\n\nDiversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.This dataset is then used for the final evaluation.\n\nDirect optimization methods aim to directly optimize existing prompts.This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary.Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task.Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradientbased search to sequentially replace the trigger tokens with concrete tokens.These tokens are chosen to increase the probability of predicting the correct object.OPTIPROMPT (Zhong et al., 2021) is sim-ilar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space.In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs.Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts.Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings.These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity.PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs.Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts.(Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts.\n\nGeneration with PLM methods re-write prompts with the help of a secondary PLM.Haviv et al. (2021) re-write manual prompts using another version of the probed model.The re-writing model is trained to produce prompts that help extract more knowledge from the probed one, which is kept unchanged.Zhang et al. (2022) leverage a generative PLM to produce optimized prompts.", "filtered_refids": [["b82", "b79", "b83", "b59", "b23", "b42", "b33", "b65"], ["b42", "b6"], ["b92", "b85", "b61", "b117", "b75", "b53", "b96", "b36"], ["b114", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 4277, "num_references": 20}
{"corpusid_sectionid": "264451714-s6", "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models", "date": "2023-10-25", "section_title": "Vanilla PLMs", "section": "Methods in this category do not induce any changes to the probed PLMs, and depend on pre-training ob-jectives to probe PLMs for factual knowledge.Using the pre-trained parameters is the most straightforward approach and is claimed to preserve the facts learned during pre-training (Elazar et al., 2021;Newman et al., 2022).\n\nMost methods leverage the language modeling objectives from pre-training to probe for factual knowledge (Petroni et al., 2019;Jiang et al., 2020b;Shin et al., 2020;Haviv et al., 2021;Kumar and Talukdar, 2021;Zhong et al., 2021;Kalo and Fichtel, 2022;Newman et al., 2022;Onoe et al., 2022;Saeed and Papotti, 2022).Other methods rely on representations that come from the model's body, discarding task-specific parameters altogether (e.g., the Masked Language Modeling head in BERT-like models) (Li\u00e9tard et al., 2021) or use representations of the subject and object entities in the case of static word embeddings (Dufter et al., 2021).", "filtered_refids": [["b75", "b23"], ["b92", "b47", "b82", "b32", "b42", "b77", "b22", "b62", "b117", "b75", "b53", "b96"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 959, "num_references": 14}
{"corpusid_sectionid": "249204437-s1", "title": "How well do real-time machine translation apps perform in practice? Insights from a literature review", "date": 2022, "section_title": "MT quality assessment", "section": "The quality of MT output has been a hotly debated topic for decades, and a wide variety of methods for its assessment have been proposed (cf. Castilho et al., 2018). When classifying these methods, authors commonly distinguish between automated metrics and human metrics (e.g., Rivera-Trigueros, 2021;Chatzikoumi, 2020). Automated metrics include Word Error Rates (WERs), precision, recall, and BLEU scores, all of which are calculated on the basis of a comparison between MT output and a reference translation created by a professional human translator.\n\nHuman metrics are further subdivided by Chatzikoumi (2020) into metrics in which human experts express a direct judgement concerning the translation quality and metrics in which no direct judgement is expressed. When experts are asked to indicate the adequacy or fluency of a machine translated text on a 5-point scale, for example, they make an explicit quality judgement. When, on the other hand, they classify the translation errors occurring in the MT output, they provide useful information for improving the application without explicitly judging the quality of the output. Measuring the post-editing effort required to reach an acceptable quality level for the target text (e.g. Lacruz et al., 2014) also provides an indirect indication of MT quality.\n\nThere are several reasons why most of the metrics discussed above can be considered less suitable for assessing real-time MT that is used to support synchronous dialogues. First of all, postediting does not occur in such situations, so postediting effort cannot be used as a quality indicator. In the absence of a human-generated reference translation, automated metrics can also not be calculated. Technically speaking, human experts could judge the quality of the output after the dialogue has taken place, but they would be at a disadvantage due to the limited length and disfluent nature of the source texts, particularly when speech input is used (Przybocki et al., 2011).\n\nMoreover, it is important to acknowledge that MT quality assessment can have different purposes. Many of the metrics above were primarily developed to identify areas of improvement for MT applications that are 'under construction' (Dorr et al., 2011). For professionals contemplating the use of real-time MT in their daily professional routines, however, improving the application is not the main priority. They want to know whether using MT will enhance the quality of their interactions with patients, students or business partners who speak a different language. In some cases, they might even wonder whether the use of MT is ethically responsible given the prevalence of errors in MT output and the potentially damaging consequences of such errors in certain contexts (Vieira et al., 2020).\n\nTaken together, these considerations suggest that the evaluation of real-time MT might best be approached from the perspective of 'fitness for purpose', which is achieved when the quality of a translation is 'good enough' for the end user to understand the information content and pragmatic intent of a translated message Directorate General for Translation, 2016). Although this concept has featured prominently in both practical and academic discourse about translation quality for quite some time (Jim\u00e9nez-Crespo, 2018), it is not yet standard practice to ask end users to assess the quality of (post-edited) MT output (cf. Van Egdom & Pluymaekers, 2019).\n\nThis raises the question to what extent existing studies into the performance of real-time MT apps are guided by the concept of fitness for purpose, and how fitness for purpose is operationalized in evaluation methods used in these studies. For the current paper, we are specifically interested in the answers to the following questions:\n\nRQ1: To what extent are real-time MT applications tested in authentic professional situations?\n\nRQ2: Which quality indicators are most commonly used and how are they operationalized? RQ3: Who judges the performance of real-time MT apps? RQ4: Which overall picture concerning the performance of real-time MT apps emerges from the research conducted so far?\n\nWe hope to find these answers by conducting a systematic literature review of prior studies (N = 34) which report an evaluation of a real-time MT app that was or could be used to facilitate a synchronous dialogue between interlocutors who did not speak the same language. More information about our methodology is provided in the next chapter.", "filtered_refids": [["b20", "b2", "b3"], ["b13", "b3"], ["b19"], [null, "b6"], [null, "b27"], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 4490, "num_references": 10}
{"corpusid_sectionid": "249204437-s3", "title": "How well do real-time machine translation apps perform in practice? Insights from a literature review", "date": 2022, "section_title": "Sampling", "section": "In compiling the sample, we followed a multi-step approach (see Figure 1). First, we conducted an initial search in four scientific databases (EBSCOhost, PubMed, Web of Science and Google Scholar), which were selected for reasons of practicality (i.e., accessibility via the university library) as well as quality (cf. Creswell, 2014;Gusenbauer & Haddaway, 2020). In each database, we used the following Boolean combination of search words:\n\n(\"mobile translat*\" OR \"real-time translat*\" OR \"automatic translat*\") OR (\"translat* tool\" OR \"translat* app\") AND (\"quality\" OR \"evaluation\" OR \"usability\") NOT \"knowledge translation\"\n\nDepending on the search functionalities of the database, this query was applied to the abstract, the title and the abstract, or the entire text. The relevance of the articles that came up in the search results was assessed in two steps. On the basis of the abstracts, 23 articles were marked as potentially relevant. After reading the complete articles, we decided that 10 of them indeed corresponded to the inclusion criteria outlined above. In the next step, we expanded the sample by (1) manually adding 4 articles that we had found earlier and (2) investigating studies that were either included in the reference list of one of the articles in the initial set or that referred to one of the articles in the initial set. By doing so, we identified 28 potential additions to the sample, 18 of which met the screening criteria. For the newly added articles (4+18), we repeated the reference check described above, which led to the identification of 2 more articles. After this, saturation was achieved, resulting in a final sample of 34 articles (see Appendix A). More information about the characteristics of these articles (year of publication, the number and types of applications tested, language combinations etc.) will be provided in section 4.1 below.", "filtered_refids": [["b7", "b4"], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1889, "num_references": 3}
{"corpusid_sectionid": "256461385-s2", "title": "Narrative Why-Question Answering: A Review of Challenges and Datasets", "date": 2022, "section_title": "Causality", "section": "Causality is a semantic relationship between events showing that an event occurs or holds due to another event (Mostafazadeh et al., 2016b). Mostafazadeh et al. (2016b) distinguish four types of lexical causality relations: cause, enable, prevent, and cause-to-end based on the works by Wolff and Song (2003), Wolff (2007), and Khemlani et al. (2014). Moreover, causality has temporal implications such that if an event A causes/enables/prevents an event B, then A should start before B, or if an event A causes an event B to end, then B should start before A. Causality relations can hold one of the three temporal implications: before, overlaps, and during (Mostafazadeh et al., 2016b). Thus, while answering a whyquestion, the temporal relation between the events should also be taken into account in addition to the causality relation.\n\nA causal relation is constructed from two components: cause and effect. Based on how the cause and the effect are conveyed in a text, causation can be distinguished into the following categories: explicit vs implicit, marked vs unmarked, and ambiguous vs unambiguous.\n\nExplicit vs Implicit. Causation is explicit if both the cause and the effect are present in the text. Causation is implicit if either the cause or the effect of both are missing from the text (Blanco et al., 2008). For instance, \"She was accepted to a top university after receiving a high score in the state examination\" is explicit, while \"I did not attend the mandatory final exam.\" is implicit because the effect of \"failing the course\" is not explicitly stated.\n\nMarked vs Unmarked. Causation is marked if the text contains the causal signal words that indicate the causal relation (Blanco et al., 2008). For example, \"I was late because of traffic\" is marked, but \"Do not buy any bread. We have already got two at home\" is unmarked.\n\nAmbiguous vs Unambiguous. If the causal relation is presented in the text with causal keywords (e.g., cause, effect, consequence) or with causal signals (e.g., because of, due to, as a result of ), it is considered unambiguous (Girju, 2003). On the other hand, if a causal relation is constructed in the form of an expression containing affect verbs (e.g., affect, change, influence) or link verbs (e.g., link, lead, depend), it is considered ambiguous. Furthermore, if a marked signal always refers to causation (e.g., because), it is unambiguous, while if a marked word occasionally signals causation (e.g., since), it is ambiguous (Blanco et al., 2008).", "filtered_refids": [["b43", "b44", "b16", "b25"], [], ["b2"], ["b2"], ["b12", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2506, "num_references": 8}
{"corpusid_sectionid": "256461385-s5", "title": "Narrative Why-Question Answering: A Review of Challenges and Datasets", "date": 2022, "section_title": "Answer ambiguity", "section": "Answer ambiguity occurs because most questions can have multiple answers belonging to different answer types and because often the desired type is not expressed in the question. Several partially overlapping taxonomies of reasons, which is the cause component of a causal relation, have been proposed (Verberne et al., 2006;Dunietz et al., 2017;Tan et al., 2022). Verberne et al. (2006) distinguish four types of reasons based on Quirk et al. (1985):\n\n\u2022 Cause -a temporal and causal relation without the involvement of the human intention: an event mechanistically leads to another event;\n\n\u2022 Motivation -a temporal and causal relation with an involvement of the human intention: a goal or a motivation of an agent leads to their action;\n\n\u2022 Circumstance -a temporal and causal relation based on conditionality: one event is a condition for another event to occur;\n\n\u2022 Generic purpose -a causal relation stemming from physical functions of the objects.\n\nSimilarly, Dunietz et al. (2017) defines three types of causalities while annotating causal relations: (1) Consequence: similar to the Cause type above, (2) Motivation and (3) Purpose: similar to the Motivation type above. Tan et al. (2022) defines four senses for causality based on Webber et al. (2019) for annotating causal relations: (1) Cause: similar to the Cause type above (2) Purpose: similar to the Motivation type above, (3) Condition and (4) Negative-Condition, which can fit into the Circumstance type above. Although the types of reasons introduced by Verberne et al. (2006) are broader than the taxonomies of Dunietz et al. (2017) and Tan et al. (2022), this list is not complete, as Verberne et al. (2006) demonstrated that not all why-questions can be classified into these categories.\n\nContext: \"He opened the box to take a slice of pizza.\" Question: \"Why did he open the box?\" Answers:\n\n(1) The pizza was in the box.\n\n(2) The box was closed.\n\n(3) He was hungry.\n\n(4) He wanted to eat pizza. (5) He wanted to take a slice of pizza.  (1) and (2) refer to causal reasons, answers (3), (4) and (5) refer to motivational reasons.\n\nValid answers to a why-question about an event or a state can include at least one of the cause, motivation, circumstance, or generic purpose of an event or state according to the above taxonomy. Since a why-question can often be answered with answers falling into several type categories, the necessity to choose the correct answer type creates ambiguity since the desired type is typically not explicitly stated in the question. Furthermore, a whyquestion can be answered with several causes in the causal chain (Verberne et al., 2006), and in that case all these answers can be considered as correct. For instance, consider the example shown in Table 1. For this example question, several potential causes can be the basis for the answer. Consequently, this why-question can be answered according to both mechanistically causal (answers 1, 2) and motivational (answers 3, 4, 5) reasons.", "filtered_refids": [[null, "b9", "b34", "b41"], [], [], [], [], ["b42", null, "b9", "b41"], [], [], [], [], [], ["b41"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2985, "num_references": 9}
{"corpusid_sectionid": "256461385-s6", "title": "Narrative Why-Question Answering: A Review of Challenges and Datasets", "date": 2022, "section_title": "Narratives", "section": "Narratives are texts in which events are causally or thematically linked and develop within a temporal framework (Brewer, 2017). Narratives are generally agent-oriented and their main scope is centered on characters, their actions, and motivations (Sang et al., 2022). In narrative QA, stories, fairytales, books, and (movie) scripts are commonly utilized as narrative texts. Characteristics of narrative texts, such as causality of events and motivations of agents, make narratives a suitable context for asking why-questions. Additionally, fictional narratives can ensure the test of comprehension because they are self-contained, meaning that all elements needed to understand the narrative, such as events, characters, and settings, are present in the text and QA models need to comprehend the narrative in order to answer questions (Dunietz et al., 2020;Richardson et al., 2013;Ko\u010disk\u00fd et al., 2018). Implicitness is a key feature of narratives that makes it different from other types of texts. Length is another characteristic dimension of narratives which is also very important for QA systems. In the following subsections, we will review these characteristics in more detail.", "filtered_refids": [["b18", "b4", "b35", "b8", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1185, "num_references": 5}
{"corpusid_sectionid": "245616858-s2", "title": "The Quality of Lexical Semantic Resources: A Survey", "date": 2021, "section_title": "Polysemy", "section": "LSR, e.g, WordNet organizes the relation between terms and synsets through senses (term-synset pair). A term may have many meanings (one or more senses) which is called polysemous term. For example, head has 33 senses in WordNet which indicates that there are 33 relations between the word head and associated synsets. The ambiguity of a term that can be used (in different contexts) to express two or more different meanings is called polysemy. Due to synonymy and polysemy, the relation between terms and synsets is many-to-many relationship. Really, wrong semantic connection can be occurred in WordNet. A misconstruction that results in wrong assignment of a synset to a term is called Sense enumeration (Freihat et al., 2015).\n\nIn WordNet, a compound-noun which contains two-parts (modifier and modified) causes polysemy this is called compound-noun polysemy. It corresponds to \"the polysemy cases, in which the modified noun or the modifier is synonymous to its corresponding noun compound and belongs to more than one synset\". WordNet contains a substantial amount of this type of ploysemy such as: center and medical center in WordNet (Kim and Baldwin, 2013).\n\nAlso in WordNet, a special case is founded when there are related some senses (synsets) with a specific polysemous term and not connected with it. For example, a hierarchical relation between the meanings of a polysemous term (Freihat et al., 2013b). \"In case of abstract meanings, we say that a meaning A is a more general meaning of a meaning B. We say also that the meaning B is a more specific meaning of the meaning A\" which is called specialization polysemy. In this case, synset connections require reorganizing the semantic structure (using semantic relations) to cover and reflect the (implicit) hierarchical relation between all such senses.\n\nSo, the big challenge in WordNet is polysemy, because it may produce OVERLOAD connections (overload of a number of term-synset pairs). For example wrong assignments of a synset to terms in sense enumeration add overload relations in Word-Net which decrease the synset quality implicitly.", "filtered_refids": [["b0"], ["b6"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2109, "num_references": 3}
{"corpusid_sectionid": "245616858-s3", "title": "The Quality of Lexical Semantic Resources: A Survey", "date": 2021, "section_title": "Missing Senses, Lemmas and Relations", "section": "Despite \"the highpolysemous nature of wordNet, there are substantial amount of missing senses (term-synset pairs) in WordNet\" based on Ciaramita and Johnson's work that cause UNDER-LOAD of term synsets problem which is the opposite of the overload of term synsets. For example, new added words in languages cause missing senses (synsets) for some terms in lexical resources (e.g, WordNet). Such as Crypto Mining sense is missing from the synsets of mining term in Word-Net and only two synsets are founded in WordNet for it (Ciaramita and Johnson, 2003).\n\nAlso, WordNet contains synsets with missing lemmas as shown in (Verdezoto and Vieu, 2011). For example, \"the term brocket denotes two synsets in WordNet, the lemmas of the two synsets are incomplete. This is due to the following: the terms red brocket and Mazama americana which are syn-onyms of the lemmas in (b) are missing. The two synsets do not even include the term brocket deer. (a) brocket: small South American deer with unbranched antlers. (b) brocket: male red deer in its second year\" WordNet relations are \"useful to organize the relations between the synsets, while substantial amount of relationships between the synsets remain implicit or sometimes missing as in the case synset glosses relations. For example, the relation between correctness and conformity is implicit. The relation between fact or truth and social expectations in the following two meanings of the term correctness is missing. A human being may understand that correctness is a hyponym of conformity and fact or truth is a hyponym of social expectations, but this is extremely difficult or impossible for a machine because conformity is neither the hypernym of (a) nor (b). The relation between fact or truth and social expectations is missing because social expectations is not defined in WordNet which makes the two synsets are incorrect (Freihat et al., 2013a).\n\nMissing senses, missing terms or missing Relations may cause UNDERLOAD problem whether UNDERLOAD in connections or UNDERLOAD in synset itself. Therfore, to enhance synset quality, you have to solve the two main problems: OVER-LOAD and UNDERLOAD which are caused by polysemy and missing, respectively.", "filtered_refids": [[null], [null, "b17"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2208, "num_references": 3}
{"corpusid_sectionid": "245616858-s6", "title": "The Quality of Lexical Semantic Resources: A Survey", "date": 2021, "section_title": "Lemmas Validation Methods", "section": "The most famous method for lemmas validation is the work of Ramanand in (Nadig et al., 2008). They presented Validate Synset algorithm, its principle depends on \"dictionary definitions to verify that the words present in a synset are indeed synonymous or NOT\". This is due to the availability of synsets in which some members \"do not belong\". To accomplish their work they discussed the following research questions: \"is a given WordNet complete, how to select one lexico-semantic network over another, and are WordNet synsets INCOM-PLETE (may be many words have been omitted from the synset) and are WordNet synsets COR-RECT (the words in a synset indeed synonyms of each other and the combination of words should indicate the required sense)\". To answer the questions they try to validate the available synsets which are the foundations of a WordNet. \"A WordNet synset is constructed by putting together a set of synonyms that together define a particular sense uniquely. This sense is indicated for human readability by a gloss\". To evaluate the quality of a synset, they begin by looking for validating the synonyms that the synset has them. They follow these subtasks in the synset validation: are the words in a synset indeed synonyms of each other? Are there any words which have been omitted from the synset? And does the combination of words indicate the required sense? In their work, they focus on the quality of content embedded in the synsets; this is by attempting to verify a given a set of words/lemmas if they were synonyms and thus correctly belong to that synset or not synonyms based on the following two principles: \"if two words are synonyms, it is necessary that they must share one common meaning out of all the meanings they could possess. And a condition could be showing that the words replace each other in a context without loss of meaning\" (Nadig et al., 2008).\n\nA simple block diagram for a synset synonym validation using the system is shown in Figure 1. As we notice from the block diagram, the input to the system is: \"a WordNet synset which provides the following information: the synonymous words in the synset, the hypernym(s) of the synset and other linked nodes, gloss, example usages\". The output consists of \"a verdict on each word as to whether it fits in the synset, i.e. whether it qualifies to be the synonym of other words in the synset, and hence, whether it expresses the sense represented by the synset\". They used the following hypothesis: \"if a word is present in a synset, there is a dictionary definition for it which refers to its hypernym or to its synonyms from the synset\" (Nadig et al., 2008). However, dictionary definitions include useful clues for validating and verifying synonymy. The results show that: the algorithm is simple to implement and depends on the nature (the depth and the quality) of the used dictionary. Many words in WordNet are not validated, around 0.18 of total words in WordNet and 0.09 of total WordNet synsets that couldn't be validated. Also, the algorithm cannot detect omissions from a synset. To overcome this shortcoming of the algorithm, they proposed that expanding the validation to the synset gloss, and synset relations; using more dictionaries in validation; running the algorithm to other language WordNets, and applying the algorithm on other parts of speech in English. The same team proposed in (Ramanand and Bhattacharyya, 2007) an automatic method for the synset synonyms and the hypernyms validation based on new rules: 8 rules in synonym validation and 3 rules for hypernyms validation which is the first attempt of automatic evaluations for synsets in WordNet. They focus on the synsets because they are the foundational elements of wordnets and focus on the hypernymy hierarchy this is due to its importance in semantic linkages with other synsets. The quality of the synset and its hypernymy ensure the correctness, the completeness and the usability of the resource. They evaluate the quality of a wordnet by \"examining the validity of its constituent synonyms and its hypernym-hyponym pairs\". The authors defined the synonymy validation as \"the inspection of the words in the synset indeed synonyms of each other or NOT\", and they use the following observation: \"If a word w is present in a synset along with other words w 1 , w 2 , . . . , w k , then there is a dictionary definition of w which refers to one or more of w 1 , w 2 , . . . , w k and/or to the words in the hypernymy of the synset\" which was the hypothesis in the (Nadig et al., 2008) work. In the synonymy validation algorithm, the authors apply 8 rules in order which are the basic steps of the algorithm. Also, omissions from synsets aren't considered. Examples of these are synsets such as: Taylor, Zachary Taylor, President Taylor: no definition for the last multiword. Thus the multiword synonyms do share partial words. To validate such multi-words without dictionary entries, they check for the presence of partial words in their synonyms\". They run the algorithm on the noun synsets (39840 from the available 81426) of PWN, the inputs of the algorithm are synsets with more than one lemma, by running the validator which uses the online dictionary service Dictionary.com in validation, the results show that the percentage of the synsets where all words were validated is (0.701), Pushpak algorithm is simple and acts as a backbone for the synset validation models, also, the applied rules such as: Rule1, Rule2 and Rule7 are the most impact among synonym validation rules, on the other hand Rule4, Rule5 and Rule6 are the lowest. They conclude that many of the words present in PWN aren't validated and those with rare meanings and usages. \"The wordnet contains synsets that have outlier words and/or missing words\". The limiting factors are \"the availability of dictionaries and tools like stemmers for those languages\". They plan to summarize the quality of the synsets into a single number. The results could then be correlated with human evaluation, finally converging to a score that captures the human view of the wordnet. \"The presented algorithm is available only for Princeton WordNet. However, the approach could broadly apply to other language wordnets and other knowledge bases as well. And the algorithm has been executed on noun synsets; they can also be run on synsets from other parts of speech\". Also, in the same area and due to the wide-spread usage of lexical semantic resources, the lexicon quality evaluation became more and more important to tell us how well the applica-tions and operations based on these resources perform, for example, the authors in (Giunchiglia et al., 2017) describe a general approach to improve the quality of the lexical semantic resources by proposing an algorithm to classify the ambiguity words (based on their senses) in the lexical semantic resources to three classifications for a: polyseme, homonym or unclassified. Also, they present \"a set of formal quantitative measures of resource incompleteness\". And apply their work and analysis on \"a large scale resource, called the Universal Knowledge Core (UKC)\". The authors define \"two types of incompleteness, i.e., language incompleteness and concept incompleteness\". Language Incompleteness (in a lexical resource): a set of synsets/words/concepts is not lexicalized in a lexical resource (e.g UKC) by a specific language. A model (language incompleteness measurement) that can be used to measure the count (how much) of omitted synsets/words/concepts in the language is described in (Giunchiglia et al., 2017). The notion of \"concept incompleteness can be thought of as the dual of language incompleteness. If the language incompleteness measures how much of the UKC a language does not cover, the concept incompleteness measures how much a single concept is covered across a selected set of languages. Concept incompleteness: is the complement to 1 of its coverage\". A concept incompleteness model that can be used to measure the concept incompleteness is described in (Giunchiglia et al., 2017). Also in the same research, lexical ambiguity is described (it is happened when one word in a language denotes to more than one concept) and they computed the number of ambiguity instances in UKC, e.g., polysemy or homonymy. As an application example they applied the proposed algorithm to \"checks whether any two concepts denoted by a single word are polysemes of homonyms or NOT on the UKC concepts\". They run the algorithm which consists of 4 steps, and the results showed that, \"the UKC contains 2,802,811 ambiguity instances across its pool of 335 languages, these instances were automatically evaluated by the algorithm which, generated 0.32 polysemes among all the ambiguity instances and 0.22 homonyms across all languages\". They concluded that when the language coverage increases then the average ambiguity coverage decreases, and vice versa. Also, \"increasing the minimal required number of ambiguity instances consistently increases the percentage of polysemes (up to the 0.74), decreases the percentage of homonyms (down to the 0.11) as well as the percentage of unclassified instances (down to around the 0.15)\". Giunchiglia's group presented the language incorrectness evaluation method in UKC in (Giunchiglia et al., 2018), the authors proposed that \"the languages in the UKC are far from being complete, i.e., from containing all the words and synsets used in the everyday spoken or written interactions. And far from being correct, i.e., from containing only correct senses, namely, only correct associations from words and concepts to synsets\". These limiting factors impact the lexical resource quality. Language Incorrectness is the number of psycholinguistic mistakes in a language in a lexical resource per the number of total of concepts in that language in the same resource. They proposed a model to measure the language Incorrectness in (Giunchiglia et al., 2018). Furthermore, this work solves the problem of synset incomplete through presenting a model that transforms the semantic relations nodes from synsets to concepts. This is based the fact that is some words have multiple meanings, and each word is codified as a synset, consisting of a (possibly incomplete) set of synonymous words. The proposed approach describes the UKC design as three-layers: words, synsets and concepts. \"Word layer, stores what we call the universal lexicon, the synset layer, stores the world languages, and the concept layer, stores the world (mental) model(s), as represented by the CC\". This work makes an improvement in the UKC that influences on its quality; this due the work that becomes a language independent and handles the problem of each synset is associated with one and only one language.", "filtered_refids": [["b13"], ["b16", "b1", "b13", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 66, "num_chars": 10802, "num_references": 5}
{"corpusid_sectionid": "32461868-s1", "title": "A Survey on Intelligent Poetry Generation: Languages, Features, Techniques, Reutilisation and Evaluation", "date": "2017-09-01", "section_title": "Languages", "section": "Poetry is an artistic expression of language. Humans have produced poetry in many languages and, due to their specificities, different languages happen to follow different poetic traditions, often focused on different forms. While the majority of poetry generation systems targets English and produces text in this language, there are systems of this kind in other languages, enumerated in this section.\n\nWell-known early attempts to poetry generation included French (Queneau, 1961;Oulipo, 1981), but Spanish was one of the first languages where this topic was explored in the context of AI, and related issues were discussed (Gerv\u00e1s, 2000;Gerv\u00e1s, 2001). For Portuguese, another romance language, song lyrics have been automatically generated for a given melody (Gon\u00e7alo Oliveira et al., 2007), and poetry has been produced according to user-given structures that would set the number of lines, stanzas, syllables per stanza, or the rhyme pattern (Gon\u00e7alo Oliveira, 2012). In an effort to use the same architecture for generating poetry in different languages, the previous system was extended to cover also Spanish and English (Gon\u00e7alo Oliveira et al., 2017). Another poetry generator originally developed for English was also adapted to produce poetry in Spanish (Ghazvininejad et al., 2016).\n\nTraditional eight-line Basque poems, aiming to be sung, have also been produced automatically (Agirrezabal et al., 2013). Although, as Portuguese and Spanish, Basque is spoken in the Iberian Peninsula, it has different origins and is significantly different from romance languages. Toivanen et al. (2012)'s system produced poetry in Finnish, another European language.\n\nAsian languages have also been targeted, some of which with specific tonal and rhythm requirements in poetry generation. This includes the generation of song lyrics in Tamil (Ramakrishnan A et al., 2009;Ramakrishnan A and Devi, 2010), a phonetic language; ancient Chinese classic poetry (Yan et al., 2013;Zhang and Lapata, 2014;Yan, 2016), with strict tonal and rhythm requirements; follow-up lines in Bengali (Das and Gamb\u00e4ck, 2014), matching the rhythm of a user-given line; and poetry inspired by news articles, in Indonesian (Rashel and Manurung, 2014).", "filtered_refids": [[], ["b11", "b12", "b10", "b9", null, "b24", "b8"], ["b0"], ["b28", "b41", "b29", "b30", null, "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2224, "num_references": 14}
{"corpusid_sectionid": "32461868-s2", "title": "A Survey on Intelligent Poetry Generation: Languages, Features, Techniques, Reutilisation and Evaluation", "date": "2017-09-01", "section_title": "Form features", "section": "Despite all the levels of language involved in poetry, form is a key feature for, at the first glance, recognis-ing the resulting text as poetic. Most common formrelated features are, without a doubt, a regular metre and rhymes. When alone, both of them are quite straightforward to handle by computer programs, especially when compared with content features.\n\nMetre is generally modelled with the number of syllables each line has, sometimes also considering the stress patterns (e.g. Manurung (2003), Gerv\u00e1s (2001), Tobing and Manurung (2015)), which indicate the position of the stressed syllables. Rhyme results from the repetition of certain sounds (e.g. in great and mate). End-rhymes, the most typical, occur when two lines end in the same sound. But some systems consider other kinds of rhyme, such as assonance or alliteration, which respectively involve the repetition of the same vowel or of a consonant sound throughout the poem.\n\nFor less phonetic languages, such as Portuguese (Gon\u00e7alo Oliveira et al., 2007) or Spanish (Gerv\u00e1s, 2001), it is often enough to design a set of orthography-based rules to handle metre and rhyme. For English, poetry generators (e.g. Manurung (2003), , Tobing and Manurung (2015)) typically resort to a pronunciation dictionary for this purpose (e.g. CMU's 1 ). Yet, automatic methods for the automatic scansion of poetry have also been developed (Agirrezabal et al., 2016).\n\nMetre and rhymes are often organised according to a well-known poetry form and some systems are designed to produce only poems of specific forms. Haikus traditionally have 3 lines, respectively with 5, 7 and 5 syllables (Manurung, 2003;Netzer et al., 2009), but there are modern haikus with a different number (Wong and Chun, 2008). Limericks have five lines, with lines 1, 2 and 5 generally longer, and rhyme of the kind AABBA (Levy, 2001;Manurung, 2003). The sonnet is a classic form of poem with 14 lines, typically with 10-syllables each. Depending on the tradition, it might have different groupings, stress patterns and rhyming schemes, such as ABAB CDCD EFEF GG (Ghazvininejad et al., 2016). Spanish traditional forms (Gerv\u00e1s, 2000;Gerv\u00e1s, 2001) include the romance, lines of 8 syllables, where all even-numbered rhyme together; the cuarteto, a stanza with four 11-syllable lines, where the two outer lines rhyme together; and tercetos encadenados, stanzas of three 11-syllable lines with the pattern ABA BCB CDC... Bertsolaritza is a Basque traditional verse with metre and rhyme constraints, typically sung (Agirrezabal et al., 2013). The generation of classic Chinese poetry has focused mostly on quartrains, four lines of 5 or 7 characters with a rigid tonal pattern where two kinds of tones are interleaved, and a rhyme scheme where the majority of the lines in the same poem end with the same vowel, but not the same character (Yan et al., 2013;Zhang and Lapata, 2014;Yan, 2016).\n\nThe poetry form can be decided from the initial data (Gerv\u00e1s, 2000), while other systems generate poetry in more or less any form, depending on a user-provided template, which might be strictly structural (Gon\u00e7alo Oliveira, 2012) or a poem, possibly with some words stripped (Toivanen et al., 2014). There are also systems focused on generating song lyrics, which have less traditional forms, but where metre is key for matching the rhythm, while other features should still be present. These include melodies where stressed and weak beats are identified (Gon\u00e7alo Oliveira et al., 2007;Ramakrishnan A et al., 2009;Gon\u00e7alo Oliveira, 2015), pop songs (Barbieri et al., 2012), or rap (Malmi et al., 2016;Potash et al., 2015) where, besides rhyme, assonance is modelled as the repetition of vowel phonemes (e.g. in raps and tax). ered and is often only softly satisfied, for instance, by using words that belong to the same semantic domain. This section describes how different poetry generators select their content in order to transmit a meaningful message or, at least, to be, as much as possible, semantically coherent.\n\nIntelligent poetry generation systems often exploit a model of semantics, either a semantic knowledge base, or a statistical model of distributional semantics. The former is usually a more theoretical view on linguistic knowledge, where words are connected according to labelled relations, with different meanings. Poetry generators have used knowledge bases with verbs and their restrictions and ontological categories (Ramakrishnan A and Devi, 2010); semantic networks extracted from dictionaries, that go beyond synonymy and hypernymy, and cover other relations such as causation, property and others (Gon\u00e7alo Oliveira, 2012); WordNet, a lexical knowledge base Agirrezabal et al., 2013;Tobing and Manurung, 2015); and Con-ceptNet, a common sense knowledge base (Das and Gamb\u00e4ck, 2014). Those have been used not only to restrict the generated words to a common semantic domain, but also for increasing the paraphrasing power, towards higher variation and better covering of different metres.\n\nDistributional models of semantics target how language is actually used, in a collection of documents, and consider that words that occur in similar contexts have similar meanings. These include vector space models, either based on words (Wong and Chun, 2008;McGregor et al., 2016), also including word embeddings learned from collections of poems (Yan, 2016) or from Wikipedia (Ghazvininejad et al., 2016), or based on sentences (Malmi et al., 2016), both used to compute the semantic relatedness with the cosine similarity; or word associations (Netzer et al., 2009;Toivanen et al., 2012) which, according to some authors, capture relations in poetic text better than WordNet-like lexical knowledge bases.\n\nIn some systems, text is generated according to a grammar for handling syntax, possibly also considering semantic features (Manurung, 2003). In Gon\u00e7alo Oliveira (2012)'s system, the grammar is tightly related to the semantics, as each rule transmits a known semantic relation and can be instan-tiated with any pair of words sharing relations of that kind (e.g. vehicle-car or fruit-mango, for hypernymy).\n\nYet, in order to enable some kind of interpretation, the poem must actually be about something or, at least, be different for different stimuli, reflected in its content. Stimuli can be given in different forms, with different degrees of precision, namely: a list of semantic predicates (e.g. love(John, Mary)) (Manurung, 2003); one (Netzer et al., 2009;Toivanen et al., 2013;Ghazvininejad et al., 2016) or more (Wong and Chun, 2008;Gon\u00e7alo Oliveira, 2012;Zhang and Lapata, 2014;Yan, 2016) keywords that will, somehow, set a semantic domain and constraint the generation space; a line of text (Das and Gamb\u00e4ck, 2014) or a sequence of lines (Malmi et al., 2016) to be followed; a textual document, which can either be a single sentence with a message (Gerv\u00e1s, 2001), or a longer text from a blog (Misztal and Indurkhya, 2014) or newspaper (D\u00edaz-Agudo et al., 2002;Rashel and Manurung, 2014;Toivanen et al., 2014;Tobing and Manurung, 2015;Gon\u00e7alo Oliveira and Alves, 2016).\n\nIn order to extract meaningful information to be used in the poem, different systems process the input document differently. For instance, Toivanen et al. (2014) acquire novel associations from the document (e.g. bieber and alcohol, in opposition to pop and star), identified by contrast with well-known associations. Tobing and Manurung (2015) extract dependency relations from the document and use them to constrain the generated poem. They argue that, though not a genuine semantic representation, dependency relations are a useful abstraction of the text and end up conveying its semantics. In fact, some dependency relations include semantic relations (e.g. agent-of, subject-of, object-of ). A final example (Gon\u00e7alo Oliveira and Alves, 2016) extracts concept maps from the input document, and uses them as a semantic network.\n\nTowards an improved interpretation, 's system produces natural language commentaries for each generated poem, providing some generation context. A similar feature is presented by Gon\u00e7alo Oliveira and Alves (2016) or Gon\u00e7alo Oliveira et al. (2017). In this case, semantic relation instances explaining the connection between the input keywords and the words used can be provided either in raw format or, if a grammar exists for this purpose, in natural language.\n\nAdditional semantic features captured by poetry generators include sentiment (Gerv\u00e1s, 2000;Gon\u00e7alo Oliveira et al., 2017), which typically involves exploiting a polarity lexicon; or emotion (Misztal and Indurkhya, 2014), in this case achieved with the help of WordNet Affect.\n\nFigurative language is often implicitly present as a consequence of reusing material from humanproduced poetry, but its presence can also be explicitly handled, for instance, by exploiting similes mined from Google n-grams . Veale (2013) points out the importance of contentfeatures and presents a system more relaxed on form but heavily influenced by figurative language. More precisely, similes (e.g. politicians are crooks) are exploited for generating metaphors (e.g. he is a crook) and conceptual blends (e.g. sweet silence).\n\nPoetry generation systems handle a broad range of features both at the formal and at the content level. Dealing with so many constraints may actually turn out to be computationally impractical (see e.g. Tobing and Manurung (2015)). Yet, this also depends on the techniques adopted for handling all the constrains, surveyed in the following section.", "filtered_refids": [[], ["b9", "b20", "b31"], ["b11", "b1", "b9", "b20"], ["b10", "b9", "b20", "b41", null, "b23", "b8", "b0", "b17"], ["b11", "b15", "b34", "b26", "b18", "b29", "b8", "b2"], ["b7", "b31", "b0"], ["b10", "b18", "b32", "b21", null, "b23"], ["b20"], ["b12", "b10", "b34", "b18", "b9", "b13", "b41", "b31", "b30", null, "b22", "b23", "b7", "b33"], ["b34"], ["b14"], ["b14", "b22", "b8"], ["b37"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 49, "num_chars": 9570, "num_references": 54}
{"corpusid_sectionid": "32461868-s3", "title": "A Survey on Intelligent Poetry Generation: Languages, Features, Techniques, Reutilisation and Evaluation", "date": "2017-09-01", "section_title": "Artificial Intelligence Techniques", "section": "Early poetry generators (e.g. Queneau (1961) or Oulipo (1981)) relied heavily on combinatory processes applied to a set of human-created poems. On the other hand, intelligent poetry generation systems consider semantics when selecting content and take advantage of computational techniques that add value to the generation process, with a more efficient exploration of the space of possible generations, also enabling to handle a larger number of features, often towards a predefined intention, and sometimes resulting in poems with higher novelty. This section enumerates some of those techniques, borrowed from the domain of AI.\n\nThe technique of Case-Based Reasoning exploits past solutions for solving new similar problems, in a four-step approach (retrieve, reuse, revise, retain). In the scope of poetry generation (Gerv\u00e1s, 2001;D\u00edaz-Agudo et al., 2002), it has been instantiated as follows: retrieve vocabulary and line examples that suit fragments of a poem draft; reuse the part-ofspeech (POS) structure of the example lines for producing new lines and combine them with the words in the vocabulary; present the resulting draft to the user, for revision; perform a linguistic analysis of the revised poems and retain it for further generations.\n\nChart Parsing is a known technique that employs dynamic programming for parsing text according to a context-free grammar. Chart Generation, used by some poetry generation systems (Manurung, 1999;Manurung, 2003;Tobing and Manurung, 2015;Gon\u00e7alo Oliveira, 2012), is the inverse of chart parsing. Given a grammar, a lexicon, and a meaning (e.g. as a set of predicates), chart generation produces all syntactically well-formed texts that convey the meaning. Charts store complete generated constituents (inactive edges) as well as incomplete (active edges), with dotted rules marking constituent portions yet to be generated.\n\nPoetry composition can be seen as an incremental task, where initial drafts go through several iterations, each ideally better than the previous, until the final poem. The application of evolutionary algorithms to this task (Levy, 2001;Manurung, 2003) is thus natural. The basic idea is to generate an initial population of poems by a simple method, and then evolve it through several generations, towards more suitable poems, assessed by a fitness function that considers a set of relevant features for poetry. Changes in the population are obtained by the application of crossover and mutation operators. Crossover creates new poems from two other poems in the population. This can be achieved by adopting the syntax of the former but the words or the rhyme of the latter (Levy, 2001), or by swapping parts of the former with parts of the latter (Manurung, 2003). Mutation may involve the replacement of some words in all the poem, only in a certain line, or changing the rhyme (Levy, 2001). It may also consist of adding, deleting or changing contents of the poem, possibly considering the target semantics (Manurung, 2003).\n\nGiven the number of constraints involved in poetry generation, it is also natural to have this problem formulated as a Constraint Satisfaction approach (Toivanen et al., 2013;Rashel and Manurung, 2014). For this purpose, a constraint satisfaction solver explores the search space and produces solutions that match the input properties (how poems can be like), represented as predicates that indicate the poem structure and the vocabulary words to be used. Different constraints and their types can be set for different generations. Hard constraints are mandatory (e.g. number of lines, syllables per line), while soft constraints are optional (e.g. rhymes).\n\nLanguage models have been used to generate poetic text, constrained by both a target style and a predefined form. These include Markov models (Barbieri et al., 2012) and models based on Deep Neural Networks (DNNs), including Recurrent Neural Networks (RNNs). Given a sequence of words, a RNN was used to predict the next word in rap lyrics (Potash et al., 2015). Or given the line history, RNNs can be used for generating new lines incrementally, considering their respective phonetics, structure and semantics (Zhang and Lapata, 2014;Yan, 2016). There may be one neural network (NN) for selecting the structure of lines and another for guiding the generation of single words within a line. Towards better poeticness, Yan (2016) goes further and adds poem refinement iterations to the previous process. The RNN language model may also be guided by a Finite-State Acceptor that controls rhyme and metre (Ghazvininejad et al., 2016). Malmi et al. (2016) use a DNN and the RankSVM algorithm to predict the next full line, from a knowledge base of human-produced lyrics, considering rhyme, structure and semantic similarity. Support Vector Machines (SVMs), trained in a poetry corpus, were also used to predict followup lines with certain syllabic and rhyming properties (Das and Gamb\u00e4ck, 2014). And classic NNs were also used to measure the fitness of poems generated by an evolutionary approach (Levy, 2001). In the latter case, the NN was trained on human judgments of creativity in a selection of limericks, half by humans and another half randomly generated.\n\nMisztal and Indurkhya (2014) adopted a Multi-Agent approach where a set of artificial experts, focused on a particular aspect of poetry generation, interact by sharing results on a blackboard. Experts can contribute with words matching a given topic or emotion (word-generating), arrange words in the common pool into phrases (poem-making), or select the best solutions according to given constraints and heuristics (selection experts), among others. Poetry generation has also been tackled as a Generative Summarization framework that incorporates poetic features as constraints to be optimised (Yan et al., 2013). Candidate poems are retrieved for a set of keywords, they are segmented into constituent terms and clustered given their semantics. Lines that conform the structural constraints, each using terms from the same cluster and with some correlation, are then selected. Suitable term replacements are finally made iteratively, in order to improve structure, rhyme, tonality and semantic coherence.", "filtered_refids": [["b24", null], [null, "b9"], ["b12", "b19", "b20", "b31"], ["b20", "b17"], ["b30", "b33"], ["b10", "b26", "b18", "b41", null, "b7", "b2", "b17"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 6234, "num_references": 21}
{"corpusid_sectionid": "2395785-s2", "title": "To what extent is Immediate Constituency Analysis dependency-based? A survey of foundational texts", "date": "2017-09-13", "section_title": "Reification", "section": "In graphical trees, nodes and edges are turned into discrete graphical objects. This encoding operation is called reification (from Lat. r\u0113s 'thing'; hence to reifiy 'to turn into a thing'). Theoretical objects can be expressed by graphical objects, in which case, they are indeed reified (Kahane and Mazziotta, 2015;Mazziotta, 2016b). However, as illustrated by the alternative between the use of arrows or the use of vertically ordered strokes, the fact that diagrams are drawn on a bidimensional plane allows for the configurational expression of theoretical objects. Configurational expression competes with reification -e.g. in phrase structure trees (henceforth PST), words are often linearly ordered, which is a configurational means of expression of their precedence relations; this precedence could be reified by arrows instead.\n\nAs an example of linguistic entities that are conceived as distinct notions in the argumentation but not reified in the diagrams, one can introduce S.W. Clark's diagrams. The diagrams in his Practical grammar (1847), a pedagogical handbook on the grammar of English, do not reify the relations between the words -see Mazziotta's comprehensive study (2016a), although the text acknowledges that some words modify or complete others. In the diagrams, words are depicted as labeled bubbles that are but aggregated to one another ( fig. 3).  (Clark, 1847, 23) It is clear in Clark's diagrams that bubbles in contact correspond to word in syntagmatic relation (cf. section 3.2). Their configuration conveys information about the syntactic analysis they encode. It is possible to reify these contacts and we obtain a diagram that, intuitively, is very similar to a classical dependency tree ( fig. 4) -the only difference is that the connection between the verb and the subject and between the verb and the object are not directed. In the diagrams, the choice of what is reified and what is not is closely bound to the theoretical stance chosen, but, as it will appear, some options are not always taken in full awareness.\n\n3 What does dependency-based mean?\n\nThe difference between constituency and dependency is presented through their use of tree structures under 3.1 and the definitional attributes of dependency trees are reviewed under 3.2.", "filtered_refids": [["b15", "b21"], [null], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2279, "num_references": 3}
{"corpusid_sectionid": "2395785-s4", "title": "To what extent is Immediate Constituency Analysis dependency-based? A survey of foundational texts", "date": "2017-09-13", "section_title": "Connection-basedness.", "section": "Words combine pairwise, they are in a syntagmatic relationship in the sense of de Saussure (2013, 170):\n\nWords as used in discourse, strung together one after another, enter into relations based on the linear character of languages. Linearity precludes the possibility of uttering two words simultaneously. They must be arranged consecutively in spoken sequence. Combinations based on sequentiality may be called syntagmas. The syntagma invariably comprises two or more consecutive units: for example, re-lire ('re-read'), contre tous ('against all'), la vie humaine ('the life of man'), Dieu est bon ('God is good'), s'il fait beau temps, nous sortirons ('if it's fine, we'll go out').\n\nSince the term syntagma has been led astray -this is especially the case in French linguistic: Fr. syntagme has been used to translate phrase (Chomsky, 1969) -, we suggest to use the term connection introduced by Tesni\u00e8re (2015, ch. 1, \u00a7 3-5):\n\nEach word in a sentence is not isolated as it is in the dictionary. The mind perceives connections between a word and its neighbors. The totality of these connections forms the scaffold of the sentence. [. . . ] [A] sentence of the type Alfred speaks is not composed of just the two elements, Alfred and speaks, but rather of three elements, the first being Alfred, the second speaks, and the third the connection that unites them -without which there would be no sentence.\n\nElaborating from this quotation, we call connection the undirected relation underlying any dependency. 3 Hence, in a dependency tree, syntagmatic relations are encoded by edges. By contrast, in a PST, edges represent constituency relations -see also (Mel'\u010duk, 1988, 13-14). Analyses and diagrams that make use of connections to describe the syntactic structure of constructions are connection-based.\n\nBinarity. In a dependency tree, a connection always involves exactly two words. In a PST, a phrase can have more than two immediate constituents. Binarity is a central property of ICA until the 60's and still remains preeminent. 4 It seems that binarity is the consequence of the connectionbasedness of these ICAs. Non-binary structures appear later, cf. fig. 6 (Chomsky, 1965, 65). 5 Figure 6: First PST in (Chomsky, 1965) Headedness. Connections are directed, as explained by Tesni\u00e8re (2015, ch. 2, \u00a7 1-3):\n\nStructural connections establish dependency relations between words. In principle, each connection unites a superior term and an inferior term. The superior term is called the governor, and the inferior term the subordinate. Thus in the sentence Alfred speaks (Stemma 1), speaks is the governor and Alfred is the subordinate. We say that the subordinate depends on the governor and that the governor governs the subordinate. Thus in the sentence Alfred speaks (Stemma 1), Alfred depends on speaks, and speaks governs Alfred.\n\nWe call this property headedness.\n\nIt is noteworthy to mention that although the notion of head is absent from , headedness is considered as a central notion in many early ICA-based presentations, and especially in (Bloomfield, 1933). Bloomfield's work emphasizes constituency relations, but connections are also considered: \"Every syntactic constructions shows us two (or sometimes more free forms combined in a phrase, which may call the resultant phrase.\" ( \u00a7 12.10) This last definition allows Bloomfield to oppose endocentric vs. exocentric constructions, according to the fact that the resultant phrase may belong or not to the \"formclass\" (i.e. distributional class) of one of the constituents (called the head). In a dependency tree, every construction is endocentric, i.e. connections are directed from a governor to a dependent. In a PST, endocentric constructions can be encoded by marking one of their constituents as the head.\n\nFlatness (i.e. absence of stratification). In a dependency tree, dependents that have the same governor are not hierarchized. In a PST, phrases are embedded: if a head word has several complements (or specifiers, or adjuncts), each of them can belong to a different stratum (Kahane, 1997;Kahane and Mazziotta, 2015). E.g., the dependency tree of a sentence such as Mary gives Peter a book represents Mary, Peter and a book as co-dependents of gives that belong to the same level, whereas a PST of the same sentence can attach Mary, Peter and a book at different levels. Stratification remains the main difference between dependency syntax and ICA-based syntax. This point will be developed in Section 4.\n\nNode-to-word mapping. Dependency trees do not encode connections by the means of nodes: these are used exclusively to encode words. 6 As a result, one can state:\n\nA dependency structure for a sentence is a one-to-one mapping between the nodes of a tree (the dependency tree) and the words of the sentence. (Kahane, 1996, 45) By contrast, classical PST use nodes to encode words as well as constituents. Thus the mapping between nodes and words is not one-to-one. As it will appear in the next section, node-to-word mapping does not imply flatness.\n\nAs soon as additional nodes are introduced, labels on these nodes can be used to reify other information. E.g., X-bar syntax (Chomsky, 1970) uses XP vs. X labels to express headedness.  ", "filtered_refids": [["b27"], [], ["b31", "b5"], [null], [null], [null, "b31", "b4"], [], [], ["b2"], ["b15", "b18"], [], [null], ["b6"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 45, "num_chars": 5226, "num_references": 13}
{"corpusid_sectionid": "2395785-s8", "title": "To what extent is Immediate Constituency Analysis dependency-based? A survey of foundational texts", "date": "2017-09-13", "section_title": "Barnard, 1836", "section": "To our knowledge, the first diagram representing an ICA ( fig. 7) appears in Frederick A. P. Barnard's Analytic Grammar with Symbolic Illustrations (1836). Syntactic categories of units are represented by special symbols and braces that indicate in a configurational way that a list of units combine together to form another unit. In his text, Barnard compares man and a rational animal or quadruped and a four-footed animal and says (Barnard, 1836, 243-244):\n\nWe thus construct phrases standing in the places of nouns, and answering all their purpose. [. . . ] Contemplating, then, a noun and its adjective, we say that they constitute, together, a compound noun. Contemplating an adjective and its accompanying adverb, we say, in like manner, that they constitute a compound adjective. E.g., in fig. 7, in and disposition form together a unit with the same category as very and who is mild and in disposition form together a unit with the same category as many. 8 Barnard's diagrams have no discrete means to express individual part-whole relations: the brace Figure 7: Barnard's diagram (1836) is equivalent to Chomsky's rewriting operator as well as the \"+\" symbol, linking a phrase with the entire set of its immediate constituents. There is no independent reification for the two operations. Syntagmatic relations are not represented in a discrete way either. The brace inscribes the whole construction. According to our terms (section 3), such a diagram is thus neither exactly connectionbased nor exactly constituency-based.\n\nAs shown in tab. 3, the diagram is very different from a canonical dependency tree: not a single definitional attribute firmly holds.   fig. 7 with respect to definitional attributes of dependency trees.\n\n4. 3 Nida, 1943;1966 It seems that Barnard's diagram was overlooked by his contemporaries. More than one century passed between this attempt and the next ICA diagram. 9 It appears in Nida's Morphology (1949(1943, 87). 10 Fig. 8 shows the first ICA diagram published by Nida and fig. 9 is a diagram from (Nida, 1966).  (1949(1943)) 9 In the mid time, other diagrams, which are much more dependency-based and that will not be discussed here, have been proposed by several authors (Clark, 1847;Reed and Kellogg, 1876;Kern, 1883;Tesni\u00e8re, 1934). 10 We could not access the fist edition of Nida's Morphology (1943). Figure 9: Nida's diagram (1966) At first glance, it would seem that Nida's first diagram could be interpreted as a PST. It is tempting to consider that fig. 8 is completely equivalent to fig. 10, where constituency relations are reified as distinct graphical entities. Figure 10: Nida, 1943's diagram, reified However, fig. 9, which elaborates on the same rationales as fig. 8, demonstrates that it is not the case. Both diagrams consist of arcs between words and arcs between words and other arcs. Every single node in these diagrams corresponds to a word. Thus, the contact point between strokes are not equivalent to reifications, since they are not discrete graphical entities and they possibly allow for several interpretations.\n\nTo fully understand fig. 9, let us recall that Nida's work was preceded by Bloomfield's seminal text on constructions (section 3.1). Hence, in his fig. 9, arcs bear additional symbols (\">\", \"\u00d7\", \"=\") and the accompanying text clearly explains how to interpret them (Nida, 1966, 17):\n\nIn addition to the usual set of lines used to show relationships between immediate constituents, an additional set of symbols has been employed to mark exocentric, endocentric, and paratactic relationships.\n\nConsequently, the labels over the strokes reify the headedness of the connections. Nida's diagrams are connection-based and not constituency-based. Such a diagram is close to a dependency tree. The only difference between classical dependency trees and Nida's diagrams is that the later are not flat, but stratified: connections are ordered and hierarchized. The consequence of such an analysis is that connections can be connected to one another. From a mathematical perspective, this means that edges can have other edges as vertices -see (Kahane and Mazziotta, 2015) for a formalization of such a structure, that can be called a polygraph.\n\nTab. 4 shows that the evolution between fig. 8  and fig. 9 consists in encoding headedness in the diagram. Fig. 9 is almost a dependency tree: the only attribute that does not hold is flatness.", "filtered_refids": [[null], [null], [], ["b25", "b29", null, "b19", "b23", "b24"], [null], [], ["b15"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 40, "num_chars": 4414, "num_references": 10}
{"corpusid_sectionid": "43985051-s2", "title": "Notion of Semantics in Computer Science A Systematic Literature Review", "date": "2017-12-01", "section_title": "Background, Context & Motivation", "section": "In human computer interaction, there is obviously 1) a human, 2) a computing system and 3) an engagement or interaction between the two. The engagement could either be passive (as in browsing or viewing), or active, as in querying or selecting something on the system. In such scenarios, humans are said to be deriving meaning from the representation presented by the computer. The modality for representation can be text, image, audio, video etc. More interactive representation(al experiences) can be animation, video, user interfaces etc. In the case of interaction (as in inputting or programming by the human), the computing system is also processing data to derive meaning. Apparently, both the human and the system can be seen as two processing agents. 513\n\nThe notion of meaning and semantics can, therefore, be applied to either of the two agents. Our interest, however, is on the human formulating meaning. From an information delivery pointof-view, the idea of how meaning is extracted, constructed or possessed by the human is studied by Psychologists, Cognitive Scientists and Information Processing researchers. On this side, topics like Sense-making (Russell et al., 1993), User Experience, Semantic Interaction (Endert et al., 2012) etc. emerge.\n\nAs a compliment to the human sense making experience, on the computing side, we may also look at how something can be constructed to deliver a particular meaning. Web Accessibility researchers, claim that currently web content is primarily designed for a majority in mind (Prasad et al., 2014). And that it may not suffice for the individualized needs of a minority of users (Prasad, 2017).\n\nA color blind person, for example, may not benefit in the same way as a non-body disabled user. So, in this regard, on the computing system side of the human computer interaction, does there exist a platform that would enable the creation and simultaneous co-existence of multiple representations for the varying needs of a diverse human end users? Is there sufficient motivation for a system that can renarrate and simultaneously have multiple representations of some source text (Prasad, 2017)? That is, a system equally being able to produce colorful content for the majority of users, high contrast and appropriately rendered visuals for the color blind, braille for the visually impaired, in vernacular for the non-English speakers, in tables, diagrams and scientific explanations for the learned etc. These questions form the background context and motivation for our study of semantics in CSE.", "filtered_refids": [[], ["b19", "b46"], ["b43", "b42"], ["b43"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2555, "num_references": 5}
{"corpusid_sectionid": "43985051-s4", "title": "Notion of Semantics in Computer Science A Systematic Literature Review", "date": "2017-12-01", "section_title": "SLR -A Research Tool", "section": "As already stated, our larger goal is to understand how best to represent either information or data on the system so that it may create the right meaning to the human. To that end we wanted to conduct an exploratory Literature Review for such a social applicable, human oriented web application space. SLRs have been popularized as a Evidence Based Software Engineering (EBSE) research tool by Kitchenham et al. in a seminal paper (Kitchenham et al., 2004) presented at ICSE 2004, which is a prominent conference for Software Engineers. In particular SLRs have been suggested as a systematic way of exploring a problem space and thus have been suggested as valuable first step in a PhD research effort (Kitchenham et al., 2004).\n\nWhile SLRs have been popular in the fields of medical sciences, their use in CSE has been limited. However, we are now beginning to find SLRs in various areas of CSE. SLRs are now being published in Information Systems (Okoli and Schabram, 2010) ", "filtered_refids": [[null, "b27"], ["b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 977, "num_references": 3}
{"corpusid_sectionid": "43985051-s14", "title": "Notion of Semantics in Computer Science A Systematic Literature Review", "date": "2017-12-01", "section_title": "RQ3: Human Vs. Computer Semantics", "section": "Through this RQ3 we wanted to uncover a presumption that most of the notion of semantics in CSE was computer oriented and not human oriented. The SLR results confirmed this. We found that 47 out of 50 papers were indeed meant for computers as the processing agent. Only 3 out of the 50 were designed for human as the processing 517  (Boute, 1988) yes -for SDL comp denotational systems S34 (Papaspyrou, 2001) yes -for C comp denotational prog lang S35 (Lobo et al., 1991) yes -Logic comp logic S36 (Broy and Lengauer, 1991) yes -Logic comp predicative, denotational theoretical, logic S37 (Puntigam, 1997) none comp trace prog lang S38 (Jasmin Christian Blanchette, 2008) yes -alternatives presented comp operational prog lang S39 (Thomas Eiter, 2008) yes -for answer sets comp forgetting, stable model theoretical, logic S40 (Ouksel and Sheth, 1999) none comp general Global Info Systems (GIS) S41 (Millard et al., 2005) yes-for hypertext comp general hypertext, logic S42 (Zeng et al., 2006) yes -compatibility comp compatibility prog lang S43 (Wehrman et al., 2008) yes -for ORC comp operational, denotational, timed theoretical; logic S44 (Zeng et al., 2005) yes -compatibility comp compatibility web services S45 (Benthem, 2005) none comp general logic S46 (Kessing et al., 2012) none hum general game S47 (Baroni and Lenci, 2010) none comp spaces, models, similarity distributed memory; database S48 (Abiteboul and Hull, 1987) yes -IFO database model comp general database S49 (da Silva et al., 2012) none comp general workflows; web services S50 (Titov and Klementiev, 2011) yes-bayesian parsing comp general nlp Table 3: Part two, or the remaining listing of 50 sample studies we used in our SLR.\n\nagent. Upon further investigation, these 3 were either using a specialized concept of semantics or were geared towards a social application. For example S15 had to use human understandable terms like Roof, Window, Gate, Shell, Wall etc to link the graphics to urban planning. S13 used a cell component ontology, and S46 focused on real world physics on game word entities.\n\nThis exposed a potential bias for us. It appears that in CSE, most of the ideas related to semantics have indeed been largely designed for computers, and not humans as the processing agent.", "filtered_refids": [["b11", "b12", "b54", "b39", "b44", "b58", "b26", "b4", "b31", "b35", "b50", "b49", "b59", "b23", "b0", "b5", "b37"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2269, "num_references": 17}
{"corpusid_sectionid": "12719479-s2", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Agreement, Reliability, and Validity", "section": "We begin with a quick recap of the goals of agreement studies, inspired by Krippendorff (2004a, Section 11.1). Researchers who wish to use hand-coded data-that is, data in which items are labeled with categories, whether to support an empirical claim or to develop and test a computational model-need to show that such data are reliable.\n\nThe fundamental assumption behind the methodologies discussed in this article is that data are reliable if coders can be shown to agree on the categories assigned to units to an extent determined by the purposes of the study (Krippendorff 2004a;Craggs and McGee Wood 2005). If different coders produce consistently similar results, then we can infer that they have internalized a similar understanding of the annotation guidelines, and we can expect them to perform consistently under this understanding.\n\nReliability is thus a prerequisite for demonstrating the validity of the coding scheme-that is, to show that the coding scheme captures the \"truth\" of the phenomenon being studied, in case this matters: If the annotators are not consistent then either some of them are wrong or else the annotation scheme is inappropriate for the data. (Just as in real life, the fact that witnesses to an event disagree with each other makes it difficult for third parties to know what actually happened.) However, it is important to keep in mind that achieving good agreement cannot ensure validity: Two observers of the same event may well share the same prejudice while still being objectively wrong.", "filtered_refids": [[null], ["b25", "b56"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1532, "num_references": 3}
{"corpusid_sectionid": "12719479-s5", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Agreement Without Chance Correction", "section": "The simplest measure of agreement between two coders is percentage of agreement or observed agreement, defined for example by Scott (1955, page 323) as \"the percentage of judgments on which the two analysts agree when coding the same data independently.\" This is the number of items on which the coders agree divided by the total number of items. More precisely, and looking ahead to the following discussion, observed agreement is the arithmetic mean of the agreement value agr i for all items i \u2208 I, defined as follows:\n\nagr i = 1 if the two coders assign i to the same category 0 if the two coders assign i to different categories Observed agreement over the values agr i for all items i \u2208 I is then:\n\nFor example, let us assume a very simple annotation scheme for dialogue acts in information-seeking dialogues which makes a binary distinction between the categories statement and info-request, as in the DAMSL dialogue act scheme . Two coders classify 100 utterances according to this scheme as shown in Table 1. Percentage agreement for this data set is obtained by summing up the cells on the diagonal and dividing by the total number of items: A o = (20 + 50)/100 = 0.7. Observed agreement enters in the computation of all the measures of agreement we consider, but on its own it does not yield values that can be compared across studies, because some agreement is due to chance, and the amount of chance agreement is affected by two factors that vary from one study to the other. First of all, as Scott (1955, page 322) points out, \"[percentage agreement] is biased in favor of dimensions with a small number of categories.\" In other words, given two coding schemes for the same phenomenon, the one with fewer categories will result in higher percentage agreement just by chance. If two coders randomly classify utterances in a uniform manner using the scheme of Table 1, we would expect an equal number of items to fall in each of the four cells in the table, and therefore pure chance will cause the coders to agree on half of the items (the two cells on the diagonal: 1 4 + 1 4 ). But suppose we want to refine the simple binary coding scheme by introducing a new category, check, as in the MapTask coding scheme (Carletta et al. 1997). If two coders randomly classify utterances in a uniform manner using the three categories in the second scheme, they would only agree on a third of the items ( 1 9 + 1 9 + 1 9 ). The second reason percentage agreement cannot be trusted is that it does not correct for the distribution of items among categories: We expect a higher percentage agreement when one category is much more common than the other. This problem, already raised by Hsu and Field (2003, page 207) among others, can be illustrated using the following example (Di Eugenio and Glass 2004, example 3, pages 98-99). Suppose 95% of utterances in a particular domain are statement, and only 5% are inforequest. We would then expect by chance that 0.95 \u00d7 0.95 = 0.9025 of the utterances would be classified as statement by both coders, and 0.05 \u00d7 0.05 = 0.0025 as inforequest, so the coders would agree on 90.5% of the utterances. Under such circumstances, a seemingly high observed agreement of 90% is actually worse than expected by chance.\n\nThe conclusion reached in the literature is that in order to get figures that are comparable across studies, observed agreement has to be adjusted for chance agreement. These are the measures we will review in the remainder of this article. We will not look at the variants of percentage agreement used in CL work on discourse before the introduction of kappa, such as percentage agreement with an expert and percentage agreement with the majority; see Carletta (1996) for discussion and criticism. 3", "filtered_refids": [[null], [], ["b94", "b18", null], ["b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 3757, "num_references": 5}
{"corpusid_sectionid": "12719479-s6", "title": "Survey Article Inter-Coder Agreement for Computational Linguistics", "date": "2008-12-01", "section_title": "Chance-Corrected Coefficients for Measuring Agreement between Two Coders", "section": "All of the coefficients of agreement discussed in this article correct for chance on the basis of the same idea. First we find how much agreement is expected by chance: Let us call this value A e . The value 1 \u2212 A e will then measure how much agreement over and above chance is attainable; the value A o \u2212 A e will tell us how much agreement beyond chance was actually found. The ratio between A o \u2212 A e and 1 \u2212 A e will then tell us which proportion of the possible agreement beyond chance was actually observed. This idea is expressed by the following formula.\n\nThe three best-known coefficients, S (Bennett, Alpert, and Goldstein 1954), \u03c0 (Scott 1955), and \u03ba (Cohen 1960), and their generalizations, all use this formula; whereas Krippendorff's \u03b1 is based on a related formula expressed in terms of disagreement (see Section 2.6). All three coefficients therefore yield values of agreement between \u2212A e /1 \u2212 A e (no observed agreement) and 1 (observed agreement = 1), with the value 0 signifying chance agreement (observed agreement = expected agreement). Note also that whenever agreement is less than perfect (A o < 1), chance-corrected agreement will be strictly lower than observed agreement, because some amount of agreement is always expected by chance. Observed agreement A o is easy to compute, and is the same for all three coefficients-the proportion of items on which the two coders agree. But the notion of chance agreement, or the probability that two coders will classify an arbitrary item as belonging to the same category by chance, requires a model of what would happen if coders' behavior was only by chance. All three coefficients assume independence of the two coders-that is, that the chance of c 1 and c 2 agreeing on any given category k Table 2 The value of different coefficients applied to the data from Table 1.", "filtered_refids": [[], ["b21", "b94", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1841, "num_references": 3}
{"corpusid_sectionid": "8132278-s1", "title": "A Survey of Idiomatic Preposition-Noun-Verb Triples on Token Level", "date": "2010-05-01", "section_title": "Background", "section": "Most of the research on the extraction of idiomatic MWEs focused on the acquisition of MWE types. The procedures made use of several corpus-observable idiosyncratic properties of MWEs: they were identified either based on their co-occurrence frequency (Evert, 2004), their morphosyntactic fixedness -e.g. (Fazly and Stevenson, 2006), (Bannard, 2007) -or their semantics -e.g. (Lin, 1999), (Baldwin et al., 2003), to name only a few examples. However, most of these approaches operate on lexical type level, stating, e.g. that spill+beans is idiomatic, but not on token level. Contrary to this, we intend to take into account whether a text instance of a potentially idiomatic MWE is actually used idiomatically in a given context or not. In fact, there are a number of idiomatic MWEs that can also have a straightforward literal meaning. It is possible to automatically distinguish the idiomatic from the literal use in the way (Katz and Giesbrecht, 2006) did by using latent semantic analysis. In one of their case-studies they found that two thirds of the occurrences of the German idiom ins Wasser fallen (lit.: \"to fall into the water\", idiom.: \"to be cancelled\") were idiomatic uses, as opposed to one third literal uses. In the case of ins Wasser fallen, the two meanings exhibit the same morpho-syntactic surface form. However, sometimes the surface form may help to distinguish the different idiomatic vs. literal uses. Quite often, morpho-syntactic features also support a separation of \"homonymous\" idioms, which have the same lexical items as components, or of different (idiomatic) readings of a \"polysemous\" idiom (see Section 5.2. below) . An example of homography is the German idiom in Gang kommen which means \"to be set in motion\" when it appears in singular form without determiner, while the same used in plural form with definite article in die G\u00e4nge kommen, bear the meaning \"to get organised\". A literal meaning is also thinkable, e.g. in singular with definite article in den Gang kommen, where it would mean something like \"to reach the hallway\". These examples show that it is not sufficient to handle MWEs solely on the basis of the lemmas of their components, but that their context and surface form has also to be taken into account. To our knowledge, (Katz and Giesbrecht, 2006) were so far the only authors who investigated the automatic identification of idiomatic vs. literal uses of German MWEs. For English, however, there has been some more work in this field recently: this includes unsupervised methods like e.g. (Sporleder and Li, 2009) who make use of lexical cohesion in order to recognise different uses of idiomatic MWEs or (Fazly et al., 2009) who use combined knowledge of canonical forms and context information; there have also been supervised methods like (Diab and Bhutada, 2009), who used the MWEs' context and surface form features in a classification approach based on machine learning.", "filtered_refids": [["b6", "b10", "b14", null, "b7", "b5", "b1", "b0", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2937, "num_references": 9}
{"corpusid_sectionid": "8132278-s2", "title": "A Survey of Idiomatic Preposition-Noun-Verb Triples on Token Level", "date": "2010-05-01", "section_title": "Objectives", "section": "In the present work we do not investigate new methods for MWE classification in context. Instead, we take one step back and present a German resource that could be useful for future supervised methods, similar to e.g. (Diab and Bhutada, 2009) and for evaluation of extraction tools. Inspired by the VNC-Tokens Dataset of (Cook et al., 2008), consisting of ca. 3,000 manually annotated corpus sentences of 53 English verb-noun combinations (VNCs), we created a dataset for German: our set contains the manually analysed results of 77 German preposition-noun-verb triples (PNVs: a frequent pattern among German MWEs) in roughly 9,700 sentences. Each instance is provided along with a detailed morpho-syntactic feature description of the MWE and a classification into either literal or idiomatic use. Some cases cannot be decided, as the context given in the sentence is not sufficient to determine the intended reading. Even though we primarily conceived the dataset to serve as a basis for the development of new supervised MWE classification approaches, we will also discuss some examples based on the quantitative distributions of the different readings and their morpho-syntactic feature preferences. We thereby intend to enhance the awareness of literal uses of presumably idiomatic MWEs. Previous work in this field (for German) includes a corpusbased study (H\u00fcmmer, 2007), where the literal vs. idiomatic meaning of 60 German MWEs (of different structural patterns) was investigated from a linguistic and phraseological point of view. Finally, we are aware of one more dataset for English which is (as (Cook et al., 2008) and ours) also conceived to serve future supervised extraction methods, namely the IDIX corpus of (Sporleder et al., 2010). It covers 50 English idioms (mainly V+NP and V+PP) in roughly 5,000 instances and will be available as an add-on to the BNC XML edition.", "filtered_refids": [["b15", "b9", "b2", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1887, "num_references": 4}
{"corpusid_sectionid": "8132278-s6", "title": "A Survey of Idiomatic Preposition-Noun-Verb Triples on Token Level", "date": "2010-05-01", "section_title": "Parsing", "section": "In order to reliably extract PNVs , a deep syntactic analysis is essential, due to the above mentioned non-adjacency phenomena. Furthermore, as a by-product of parsing, we get a full morpho-syntactic analysis of a PNV's constituent words.\n\nIn the past, we successfully used the dependency parser FSPAR (Schiehlen, 2003) for several different MWE extraction tasks, e.g. (Fritzinger, 2009), (Weller and Heid, 2010). FSPAR is highly efficient and relies on a large lexicon. Its output, given in Figure 1 (b), is to be read as follows:\n\n1 st column: position of a word in the sentence 2 nd column: token 3 rd column: part of speech 2 4 th column: base form (lemma) 5 th column: morpho-syntactic information (case, gender, etc.) 6 th column: dependency relation: position of the word's governor 7 th column: grammatical function (subject, object, etc.)\n\nThe dependency tree representation in Figure 1(a) is not provided by the parser; we inserted it here in order to enhance readability of the example. As can be seen from Figure 1, the noun Raum is dependent on the preposition im: the 6 th column in the noun's row (cf. Fig. 1 (b)) points to sentence position 5, where the preposition is located. Analogously, the preposition im is dependent on the verb steht. In case of structural or labelling ambiguities, the parser provides an underspecified output, as e.g. for the attachment of weiter (1||5) in Figure 1: it is either dependent on steht (1) or im (5).", "filtered_refids": [[], ["b16", "b8", "b13"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1455, "num_references": 3}
{"corpusid_sectionid": "16946379-s2", "title": "Tokenization: Returning to a Long Solved Problem A Survey, Contrastive Experiment, Recommendations, and Toolkit", "date": "2012-07-08", "section_title": "A Contrastive Experiment", "section": "To get an overview of current tokenization methods, we recovered and tokenized the raw text which was the source of the (Wall Street Journal portion of the) PTB, and compared it to the gold tokenization in the syntactic annotation in the treebank. 4 We used three common methods of tokenization: (a) the original 2 See http://www.cis.upenn.edu/~treebank/ tokenization.html for available 'documentation' and a sed script for PTB-style tokenization.\n\n3 \u00d8vrelid et al. (2010) observe that tokenizing with the GE-NIA tagger yields mismatches in one of five sentences of the GENIA Treebank, although the GENIA guidelines refer to scripts that may be available on request (Tateisi & Tsujii, 2006). 4 The original WSJ text was last included with the 1995 release of the PTB (LDC #95T07) and required alignment with the treebank, with some manual correction so that the same text is represented in both raw and parsed formats. PTB tokenizer.sed script; (b) the tokenizer from the Stanford CoreNLP tools 5 ; and (c) tokenization from the parser of Charniak & Johnson (2005). Table 1 shows quantitative differences between each of the three methods and the PTB, both in terms of the number of sentences where the tokenization differs, and also in the total Levenshtein distance (Levenshtein, 1966) over tokens (for a total of 49,208 sentences and 1,173,750 gold-standard tokens).\n\nLooking at the differences qualitatively, the most consistent issue across all tokenization methods was ambiguity of sentence-final periods. In the treebank, final periods are always (with about 10 exceptions) a separate token. If the sentence ends in U.S. (but not other abbreviations, oddly), an extra period is hallucinated, so the abbreviation also has one. In contrast, C&J add a period to all final abbreviations, CoreNLP groups the final period with a final abbreviation and hence lacks a sentence-final period token, and the sed script strips the period off U.S. The 'correct' choice in this case is not obvious and will depend on how the tokens are to be used.\n\nThe majority of the discrepancies in the sed script tokenization come from an under-restricted punctuation rule that incorrectly splits on commas within numbers or ampersands within names. Other than that, the problematic cases are mostly shared across tokenization methods, and include issues with currencies, Irish names, hyphenization, and quote disambiguation. In addition, C&J make some additional modifications to the text, lemmatising expressions such as won't as will and n't. Expression-Based Pre-Processing)-essentially a cascade of ordered finite-state string rewriting rules, though transcending the formal complexity of regular languages by inclusion of (a) full perl-compatible regular expressions and (b) fixpoint iteration over groups of rules. In this approach, a first phase of string-level substitutions inserts whitespace around, for example, punctuation marks; upon completion of string rewriting, token boundaries are stipulated between all whitespace-separated substrings (and only these).\n\nFor a good balance of human and machine readability, REPP tokenization rules are specified in a simple, line-oriented textual form. Figure 1 shows a (simplified) excerpt from our PTB-style tokenizer, where the first character on each line is one of four REPP operators, as follows: (a) '#' for group formation; (b) '>' for group invocation, (c) '!' for substitution (allowing capture groups), and (d) ':' for token boundary detection. 6 In Figure 1, the two rules stripping off prefix and suffix punctuation marks adjacent to whitespace (i.e. matching the tab-separated left-hand side of the rule, to replace the match with its right-hand side) form a numbered group ('#1'), which will be iterated when called ('>1') until none of the rules in the group fires (a fixpoint). In this example, conditioning on whitespace adjacency avoids the issues observed with the PTB sed script (e.g. token boundaries within comma-separated numbers) and also protects against infinite loops in the group. 7 REPP rule sets can be organized as modules, typ-6 Strictly speaking, there are another two operators, for lineoriented comments and automated versioning of rule files. 7 For this example, the same effects seemingly could be obtained without iteration (using greatly more complex rules); our actual, non-simplified rules, however, further deal with punctuation marks that can function as prefixes or suffixes, as well as with corner cases like factor(s) or Ca [2+]. Also in mark-up removal and normalization, we have found it necessary to 'parse' nested structures by means of iterative groups. ically each in a file of its own, and invoked selectively by name (e.g. '>wiki' in Figure 1); to date, there exist modules for quote disambiguation, (relevant subsets of) various mark-up languages (HTML, L A T E X, wiki, and XML), and a handful of robustness rules (e.g. seeking to identify and repair 'sandwiched' inter-token punctuation). Individual tokenizers are configured at run-time, by selectively activating a set of modules (through command-line options). An open-source reference implementation of the REPP framework (in C ++ ) is available, together with a library of modules for English.", "filtered_refids": [[null], [null, "b8", "b0"], [], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 5241, "num_references": 5}
{"corpusid_sectionid": "216552915-s2", "title": "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?", "date": "2020-04-25", "section_title": "Belief Measurement in Social Psychology", "section": "A common approach for measuring beliefs about specific identities is to assume a dimensional representation-that is, to assume a set of distinct dimensions of social meaning can be used to characterize how we think and feel about someone that holds a particular identity. From this dimensional perspective, two primary questions arise.\n\nFirst, what are the dimensions along which beliefs form? Social psychologists have identified three classes of important dimensions: traits, affective meanings, and semantic associations. Traits represent visible-although also socioculturally defined-characteristics like age, gender, and race (Freeman and Ambady, 2011). Affective dimensions of social meaning represent how we feel about a given person and/or identity (Todorov et al., 2015;Fiske et al., 2002;Heise, 2007). Here, we use the three affective dimensions proposed by Heise (2007) and that are popular in sociology (Rogers et al., 2013)-Evaluation (goodness/badness), Potency (strength/weakness), and Activity (active/passive). Finally, social psychologists often characterize beliefs about identities in terms of semantic associations to particular concepts (Freeman andAmbady, 2011) or institutions (MacKinnon andHeise, 2010). For example, people link the identities brother and sister together because they are both associated with the family institution. In the present work, we collect beliefs for seventeen different dimensions of social meaning, incorporating age, race, gender, evaluation, potency, activity, and six institutional associations.\n\nSecond, given a theorized dimension of meaning, how should we measure society-wide beliefs about where particular identities lie on that dimension? Here, we adopt perhaps the most common approach, which uses semantic differential scales on surveys (Osgood et al., 1975). The semantic differential technique asks respondents to place an identity on a sliding scale with two opposing concepts (e.g. weak and strong, see the example in Figure 2A). Finally, it is worth noting that here, like in most social psychology research, we assume that responses from survey participants generalize to American culture writ large. This assumption is built on the well-established culture-as-consensus paradigm in psychological anthropology (Karabatsos and Batchelder, 2003;Batchelder and Romney, 1988), and empirical work showing that people tend to agree on the vast majority of their beliefs about people (Heise, 2007). Nonetheless, many counterexamples exist (Berger et al., 1992;Smith-Lovin and Douglas, 1992). We leave questions about how to address these issues to future work.", "filtered_refids": [[], ["b15", null, "b4"], ["b16", "b1", "b10", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2624, "num_references": 7}
{"corpusid_sectionid": "216552915-s3", "title": "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?", "date": "2020-04-25", "section_title": "Measuring beliefs with embeddings", "section": "Embedding-based approaches to measuring beliefs typically follow a three step process of corpus/embedding selection, dimension selection, and word position measurement.\n\nCorpus/Embedding Selection Several recent works have argued that the corpus used can impact measures of beliefs about people derived from word embeddings (Lauscher and Glava\u0161, 2019;Mirzaev et al., 2019;Sweeney and Najafian, 2019). For example, Brunet et al. (2019) show how to reduce gender bias in embeddings by removing particular documents from a corpus. However, several oth-ers have shown that in their analyses, the corpus used does not significantly impact results (Spirling and Rodriguez, 2019;Garg et al., 2018;Kozlowski et al., 2019;Caliskan et al., 2017). Differences in the embedding model used have also been observed to impact measurements (Chaloner and Maldonado, 2019). Again, though, robustness checks from other studies suggest a limited effect beyond the somewhat general hyperparameters of window size and the number of dimensions estimated (Garg et al., 2018;Kozlowski et al., 2019).", "filtered_refids": [[], [null, "b9", "b2", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1074, "num_references": 4}
{"corpusid_sectionid": "216552915-s4", "title": "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?", "date": "2020-04-25", "section_title": "Dimension Selection", "section": "To measure beliefs, one first must select a dimension along which the belief is assumed to be held. Much of the literature has focused on dimensions related to gender or race. Others, however, have seen value in moving beyond these dimensions (Agarwal et al., 2019;Sweeney and Najafian, 2019). Most relevant is the work of Kozlowski et al. (2019), who study the association of 59 concepts across 20 different dimensions of sociocultural meaning, and that of An et al. (2018), who induce 732 different dimensions using WordNet to study contextual effects of linguistic meaning. While neither work focuses heavily on identities, these efforts compliment our goal of studying a broad range of dimensions of social meaning.\n\nScholars then identify a direction within the embedding that represents this dimension. To do so, an approach similar to the semantic differential idea is used. Terms are selected to represent the two ends of the dimension. For example, to identify the gender direction, words at one end might be he and him, and words at the other end, she and her. Scholarship varies on how these dimensioninducing word sets are selected. For example, several scholars have used demographically gendered and/or racialized names (Bolukbasi et al., 2016;Caliskan et al., 2017), while others have relied on careful extraction of concepts from dictionaries and thesauri (Kozlowski et al., 2019). Kozlowski et al. (2019) find that having more words at each end generally provides better measurements, and others have found a need to use frequently occurring terms (Ethayarajh et al., 2019;Brunet et al., 2019). Beyond these observations, however, scholars have generally found stable results as long as reasonable word sets are selected.\n\nWord Position Measurement Finally, the position of each identity along this direction must be identified. Doing so entails two major deci-sions. First, how should one quantify the direction, given the dimension-inducing words? For example, Bolukbasi et al. (2016) identify the direction by taking the first dimension of a PCA on the full set of direction words. Second, how should one define the position of points along this line? For example, several works use the cosine similarity between the identified \"bias direction\" and the embedding of each identity. Scholars have also recently proposed supervised methods for word position measurement (Sweeney and Najafian, 2019;Agarwal et al., 2019). Such approaches are important, but assume the existence of some training data, which may or may not be available in certain measurement contexts. We therefore do not explore these methods further in the present work.\n\nIn sum, using embeddings to measure beliefs requires a series of decisions, the impacts of which are still debated. Below, we provide the most comprehensive study to date on the importance of these decisions on measurement quality.", "filtered_refids": [[null, "b8", "b2"], [null, "b2"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 2887, "num_references": 6}
{"corpusid_sectionid": "218487374-s1", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "date": "2020-05-01", "section_title": "Applications", "section": "In Table 1, we gather and summarize applications that use text to adjust for potential confounding. This encompasses both (a) text as a surrogate for confounders, or (b) the language itself as confounders. 2 As an example, consider Kiciman et al. (2018) where the goal is to estimate the size of the causal effect of alcohol use (treatment) on academic success (outcome) for college students. Since randomly assigning college students to binge drink is not feasible or ethical, the study instead uses observational data from Twitter, which also has the advantage of a large sample size of over sixty-three thousand students. They use heuristics to identify  the Twitter accounts of college-age students and extract alcohol mentions and indicators of college success (e.g., study habits, risky behaviors, and emotions) from their Twitter posts. They condition on an individual's previous posts (temporally previous to measurements of treatment and outcome) as confounding variables since they do not have demographic data. They represent text as word counts and use stratified propensity score matching to adjust for the confounding bias. The study finds the effects of alcohol use include decreased mentions of study habits and positive emotions and increased mentions of potentially risky behaviors.\n\nText as a surrogate for confounders. Traditionally, causal research that uses human subjects as the unit of analysis would infer demographics via surveys. However, with the proliferation of the web and social media, social research now includes large-scale observational data that would be challenging to obtain using surveys (Salganik, 2017). This type of data typically lacks demographic information but may contain large amounts of text written by participants from which demographics can be extracted. In this space, some researchers are specific about the confounders they want to extract such as an individual's ideology (Sridhar and Getoor, 2019) or mood (Sridhar et al., 2018). Other researchers condition on all the text they have avail-able and assume that low-dimensional summaries capture all possible confounders. For example, researchers might assume that text encodes all possible confounders between alcohol use and college success (Kiciman et al., 2018) or psychiatric medication and anxiety (Saha et al., 2019). We dissect and comment on this assumption in Section 8.\n\nOpen problems: NLP systems have been shown to be inaccurate for low-resource languages (Duong et al., 2015), and exhibit racial and gender disparity (Blodgett and O'Connor, 2017;Zhao et al., 2017). Furthermore, the ethics of predicting psychological indicators, such as mental health status, from text are questionable (Chancellor et al., 2019). It is unclear how to mitigate these disparities when trying to condition on demographics from text and how NLP errors will propagate to causal estimates.\n\nLanguage as confounders. There is growing interest in measuring language itself (e.g. the sentiment or topical content of text) as causal confounders. For example, Roberts et al. (2020) examine how the perceived gender of an author affects the number of citations that an article receives. However, an article's topics (the confounders) are likely to influence the perceived gender of its author (reflecting an expectation that women write about certain topics) and the number of citations of that article (\"hotter\" topics will receive more Figure 2: This chart is a guide to design decisions for applied research with causal confounders from text.\n\nStep 1: Encode domain assumptions by drawing a causal diagram ( \u00a73). If the application does not use text to measure latent confounders, the causal effects are not identifiable or the application is outside the scope of this review.\n\nStep 2: Use NLP to measure confounders from text ( \u00a74).\n\nStep 3: Choose a method that adjusts for confounding in causal estimates ( \u00a75). Evaluation should include (A) sensitivity analysis ( \u00a74), (B) human evaluation of adjustments when appropriate ( \u00a76), and (C) evaluation of recovering the true causal effects ( \u00a77). citations). Other domains that analyze language as a confounder include news (Johansson et al., 2016), social media (De Choudhury et al., 2016;Olteanu et al., 2017), and loan descriptions (Pham and Shen, 2017). See Section 4 for more discussion on the challenges and open problems of inferring these latent aspects of language.", "filtered_refids": [[null, "b31"], ["b67", "b68", "b31", "b72", "b73"], ["b11", null, "b8", "b82"], [], [], [], [null, "b29", "b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 4420, "num_references": 14}
{"corpusid_sectionid": "218487374-s4", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "date": "2020-05-01", "section_title": "Structural causal models framework", "section": "Structural causal models (SCMs) use a graphical formalism that depicts nodes as random variables and directed edges as the direct causal dependence between these variables. The typical estimand of choice for SCMs is the probability distribution of an outcome variable Y given an intervention on a treatment variable T :\n\nin which the do-notation represents intervening to set variable T to the value t and thereby removing all incoming arrows to the variable T .\n\nIdentification. In most cases, Equation 2 is not equal to the ordinary conditional distribution P (Y | T = t) since the latter is simply filtering to the sub-population and the former is changing the underlying data distribution via intervention. Thus, for observational studies that lack intervention, one needs an identification strategy in order to represent P (Y | do(T = t)) in terms of distributions of observed variables. One such identification strategy (assumed by the applications throughout this review) is the backdoor criterion which applies to a set of variables, S, if they (i) block every backdoor path between treatment and outcome, and (ii) no node in S is a descendant of treatment. Without positive identification, the causal effects cannot be estimated and measuring variables from text is a secondary concern.\n\nDrawing the causal graph. Causal graphs help clarify which variables should and should not be conditioned on. The causal graphs in Figure 3 illustrate how the direction of the arrows differentiates confounder, collider, and mediator variables. Identifying the differences in these variables is crucial since, by d-separation, conditioning on a confounder will block the treatment-confounderoutcome path, removing bias. By contrast, conditioning on a collider can create dependence between treatment-collider-outcome 5 (Pearl, 2009a) potentially introducing more bias (Montgomery et al., 2018;Elwert and Winship, 2014). Mediator variables require a different set of adjustments than confounders to find the \"natural direct effect\" between treatment and outcome (VanderWeele, 2015;Pearl, 2014). A practitioner typically draws a causal graph by explicitly encoding theoretical and domain assumptions as well as the results of prior 5 In Pearl et al. (2016)'s example of a collider, suppose scholarships at a college are only given to two types of students: those with unusual musical talents and high grade point averages. In the general population, musical and academic talent are independent. However, if one discovers a person is on a scholarship (conditioning on the collider) then knowing a person lacks musical talent tells us that they are extremely likely to have a high GPA. data analyses. 6 Open Problems: When could text potentially encode confounders and colliders simultaneously? If so, is it possible to use text to adjust exclusively for confounders?", "filtered_refids": [[], [], [], ["b13", "b41", "b50", "b77", "b48", null, "b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2859, "num_references": 7}
{"corpusid_sectionid": "218487374-s5", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "date": "2020-05-01", "section_title": "Measuring confounders via text", "section": "After drawing the causal graph, the next step is to use available text data to recover latent confounders. Some approaches pre-specify the confounders of interest and measure them from text, P (z | x). Others learn confounders inductively and use a low-dimensional representation of text as the confounding variable z in subsequent causal adjustments.\n\nPre-specified confounders. When a practitioner can specify confounders they want to measure from text (e.g., extracting \"occupation\" from text in our smoking example), they can use either (1) lexicons or (2) 2019) also build machine learning classifiers for users' mental states (e.g., depression and anxiety) and apply these classifiers on Twitter posts that are temporally prior to treatment. If these classifiers accurately recover mental states and there are no additional latent confounders, then conditioning on the measured mental states renders treatment independent of potential outcomes.\n\nOpen problems: Since NLP methods are still far from perfectly accurate, how can one mitigate error that arises from approximating confounding variables? Closely related to this question is effect restoration which addresses error from using proxy variables (e.g., a father's occupation) in place of true confounders (e.g, socioeconomic status) (Kuroki and Pearl, 2014;Oktay et al., 2019). Wood-Doughty et al. (2018) build upon effect restoration for causal inference with text classifiers, but there are still open problems in accounting for error arising from other text representations and issues of calibration (Nguyen and O'Connor, 2015) and prevalence estimation (Card and Smith, 2018; Keith and O'Connor, 2018) in conjunction with NLP. Ideas from the large literature on measurement error models may also be helpful (Fuller, 1987;Carroll et al., 2006;Buonaccorsi, 2010).\n\nInductively derived confounders. Other researchers inductively learn confounders in order to condition on all aspects of text, known and unknown. For example, some applications condition on the entirety of news (Johansson et al., 2016) or scientific articles (Veitch et al., 2019;Roberts et al., 2020). This approach typically summarizes textual information with text representations common in NLP. Ideally, this would encode all aspects of language (meaning, topic, style, affect, etc.), though this is an extremely difficult, open NLP problem. Typical approaches include the following. (1) Bag-of-words representations discard word order and use word counts as representations. (2) Topic models are generative probabilistic models that learn latent topics in document collections and represent documents as distributions over topics (Blei et al., 2003;Boyd-Graber et al., 2014;Roberts et al., 2014). (3) Embeddings are continuous, vector-based representations of text. To create vector representations of longer texts, off-the-shelf word embeddings such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) or combined via variants of weighted averaging (Arora et al., 2017) or neural models (Iyyer et al., 2015;Bojanowski et al., 2017;Yang et al., 2016). (4) Recently, fine-tuned, large-scale neural language models such as BERT (Devlin et al., 2019) have achieved state-of-the-art performance on semantic benchmarks, and are now used as text representations. Each of these text representations is a real-valued vector that is used in place of the confounder, z, in a causal adjustment method ( \u00a75)\n\nOpen problems: Estimates of causal effects are contingent on the \"garden of forking paths\" of data analysis, meaning any \"paths\" an analyst did not take could have resulted in different conclusions (Gelman and Loken, 2013). For settings with causal confounders from text, the first fork is the choice of representation (e.g., topic models or embeddings) and the second fork is the pre-processing and hyperparameter decisions for the chosen representations.\n\nWe highlight that these decisions have been shown to alter results in predictive tasks. For instance, studies have shown that pre-processing decisions dramatically change topic models (Denny and Spirling, 2018;Schofield et al., 2017); embeddings are sensitive to hyperparameter tuning (Levy et al., 2015) and the construction of the training corpus (Antoniak and Mimno, 2018); and fine-tuned language model performance is sensitive to random restarts (Phang et al., 2018). Thus, reporting sensitivity analysis of the causal effects from these decisions seems crucial: how robust are the results to variations in modeling specifications?", "filtered_refids": [[], [], ["b44", "b32", "b45", "b80", "b30", null, "b16"], ["b60", "b40", "b78", "b9", "b29", "b27", null, "b7", "b81", "b53", "b61", "b2"], ["b17"], ["b35", "b55", null, "b70", "b1"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4544, "num_references": 25}
{"corpusid_sectionid": "218971825-s5", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "date": "2020-05-28", "section_title": "Techniques", "section": "Papers' techniques are not well grounded in the relevant literature outside of NLP. Perhaps unsurprisingly given that the papers' motivations are often vague, inconsistent, and lacking in normative reasoning, we also found that the papers' proposed quantitative techniques for measuring or mitigating \"bias\" do not effectively engage with the relevant literature outside of NLP. Papers on stereotyping are a notable exception: the Word Embedding Association Test (Caliskan et al., 2017) draws on the Implicit Association Test (Greenwald et al., 1998) from the social psychology literature, while several techniques operationalize the well-studied \"Angry Black Woman\" stereotype Tan and Celis, 2019) and the \"double bind\" faced by women Tan and Celis, 2019), in which women who succeed at stereotypically male tasks are perceived to be less likable than similarly successful men (Heilman et al., 2004). Tan and Celis (2019) also examine the compounding effects of race and gender, drawing on Black feminist scholarship on intersectionality (Crenshaw, 1989).\n\nPapers' techniques are poorly matched to their motivations. We found that although 21% of the papers include allocational harms in their motivations, only four papers actually propose techniques for measuring or mitigating allocational harms.\n\nPapers focus on a narrow range of potential sources of \"bias.\" We found that nearly all of the papers focus on system predictions as the potential sources of \"bias,\" with many additionally focusing on \"bias\" in datasets (e.g., differences in the number of gendered pronouns in the training data ). Most papers do not interrogate the normative implications of other decisions made during the development and deployment lifecycleperhaps unsurprising given that their motivations sometimes include no normative reasoning. A few papers are exceptions, illustrating the impacts of task defnitions, annotation guidelines, and evaluation metrics: Cao and Daum\u00e9 (2019) study how folk conceptions of gender (Keyes, 2018) are reproduced in coreference resolution systems that assume a strict gender dichotomy, thereby maintaining cisnormativity;  focus on the effect of priming annotators with information about possible dialectal differences when asking them to apply toxicity labels to sample tweets, fnding that annotators who are primed are signifcantly less likely to label tweets containing features associated with African-American English as offensive.", "filtered_refids": [["b250", null, "b283"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2452, "num_references": 3}
{"corpusid_sectionid": "218971825-s7", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "date": "2020-05-28", "section_title": "Language and social hierarchies", "section": "Turning frst to (R1), we argue that work analyzing \"bias\" in NLP systems will paint a much fuller picture if it engages with the relevant literature outside of NLP that explores the relationships between language and social hierarchies. Many disciplines, including sociolinguistics, linguistic anthropology, sociology, and social psychology, study how language takes on social meaning and the role that language plays in maintaining social hierarchies. For example, language is the means through which social groups are labeled and one way that beliefs about social groups are transmitted (e.g., Maass, 1999;Beukeboom and Burgers, 2019). Group labels can serve as the basis of stereotypes and thus reinforce social inequalities: \"[T]he label content functions to identify a given category of people, and thereby conveys category boundaries and a position in a hierarchical taxonomy\" (Beukeboom and Burgers, 2019). Similarly, \"controlling images,\" such as stereotypes of Black women, which are linguistically and visually transmitted through literature, news media, television, and so forth, provide \"ideological justifcation\" for their continued oppression (Collins, 2000, Chapter 4).\n\nAs a result, many groups have sought to bring about social changes through changes in language, disrupting patterns of oppression and marginalization via so-called \"gender-fair\" language (Sczesny et al., 2016;Menegatti and Rubini, 2017), language that is more inclusive to people with disabilities (ADA, 2018), and language that is less dehumanizing (e.g., abandoning the use of the term \"illegal\" in everyday discourse on immigration in the U.S. (Rosa, 2019)). The fact that group labels are so contested is evidence of how deeply intertwined language and social hierarchies are. Taking \"gender-fair\" language as an example, the hope is that reducing asymmetries in language about women and men will reduce asymmetries in their social standing. Meanwhile, struggles over language use often arise from dominant social groups' desire to \"control both material and symbolic resources\"-i.e., \"the right to decide what words will mean and to control those meanings\"-as was the case in some white speakers' insistence on using offensive place names against the objections of Indigenous speakers (Hill, 2008, Chapter 3).\n\nSociolinguists and linguistic anthropologists have also examined language attitudes and language ideologies, or people's metalinguistic beliefs about language: Which language varieties or practices are taken as standard, ordinary, or unmarked? Which are considered correct, prestigious, or appropriate for public use, and which are considered incorrect, uneducated, or offensive (e.g., Campbell-Kibler, 2009;Preston, 2009;Loudermilk, 2015;Lanehart and Malik, 2018)? Which are rendered invisible (Roche, 2019)? 3 Language ideologies play a vital role in reinforcing and justifying social hierarchies because beliefs about language varieties or practices often translate into beliefs about their speakers (e.g. Alim et al., 2016;Rosa and Flores, 2017;Craft et al., 2020). For example, in the U.S., the portrayal of non-white speakers' language varieties and practices as linguistically defcient helped to justify violent European colonialism, and today continues to justify enduring racial hierarchies by maintaining views of non-white speakers as lacking the language \"required for complex thinking processes and successful engagement in the global economy\" (Rosa and Flores, 2017).\n\nRecognizing the role that language plays in maintaining social hierarchies is critical to the future of work analyzing \"bias\" in NLP systems. First, it helps to explain why representational harms are harmful in their own right. Second, the complexity of the relationships between language and social hierarchies illustrates why studying \"bias\" in NLP systems is so challenging, suggesting that researchers and practitioners will need to move beyond existing algorithmic fairness techniques. We argue that work must be grounded in the relevant literature outside of NLP that examines the relationships between language and social hierarchies; without this grounding, researchers and practitioners risk measuring or mitigating only what is convenient to measure or mitigate, rather than what is most normatively concerning.\n\nMore specifcally, we recommend that work analyzing \"bias\" in NLP systems be reoriented around the following question: How are social hierarchies, language ideologies, and NLP systems coproduced? This question mirrors Benjamin's (2020) call to examine how \"race and technology are coproduced\"-i.e., how racial hierarchies, and the ideologies and discourses that maintain them, create and are re-created by technology. We recommend that researchers and practitioners similarly ask how existing social hierarchies and language ideologies drive the development and deployment of NLP systems, and how these systems therefore reproduce these hierarchies and ideologies. As a starting point for reorienting work analyzing \"bias\" in NLP systems around this question, we provide the following concrete research questions:\n\n.  (Olteanu et al., 2017)? Are any non-quantitative evaluations performed? . How do NLP systems reproduce or transform language ideologies? Which language varieties or practices come to be deemed good or bad? Might \"good\" language simply mean language that is easily handled by existing NLP systems? For example, linguistic phenomena arising from many language practices (Eisenstein, 2013) are described as \"noisy text\" and often viewed as a target for \"normalization.\" How do the language ideologies that are reproduced by NLP systems maintain social hierarchies? . Which representational harms are being measured or mitigated? Are these the most normatively concerning harms, or merely those that are well handled by existing algorithmic fairness techniques? Are there other representational harms that might be analyzed?", "filtered_refids": [[null], ["b30", "b1", "b47"], [null, "b28", "b20", "b32"], [], [], ["b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 31, "num_chars": 5945, "num_references": 9}
{"corpusid_sectionid": "218971825-s9", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "date": "2020-05-28", "section_title": "Language use in practice", "section": "Finally, we turn to (R3). Our perspective, which rests on a greater recognition of the relationships between language and social hierarchies, suggests several directions for examining language use in practice. Here, we focus on two. First, because language is necessarily situated, and because different social groups have different lived experiences due to their different social positions (Hanna et al., 2020)-particularly groups at the intersections of multiple axes of oppression-we recommend that researchers and practitioners center work analyzing \"bias\" in NLP systems around the lived experiences of members of communities affected by these systems. Second, we recommend that the power relations between technologists and such communities be interrogated and reimagined. Researchers have pointed out that algorithmic fairness techniques, by proposing incremental technical mitigations-e.g., collecting new datasets or training better models-maintain these power relations by (a) assuming that automated systems should continue to exist, rather than asking whether they should be built at all, and ( There are many disciplines for researchers and practitioners to draw on when pursuing these directions. For example, in human-computer interaction, Hamidi et al. (2018) study transgender people's experiences with automated gender recognition systems in order to uncover how these systems reproduce structures of transgender exclusion by redefning what it means to perform gender \"normally.\" Value-sensitive design provides a framework for accounting for the values of different stakeholders in the design of technology (e.g., Friedman et al., 2006;Friedman and Hendry, 2019;Le Dantec et al., 2009;Yoo et al., 2019), while participatory design seeks to involve stakeholders in the design process itself (Sanders, 2002;Muller, 2007;Simonsen and Robertson, 2013;DiSalvo et al., 2013). Participatory action research in education (Kemmis, 2006) and in language documentation and reclamation (Junker, 2018) is also relevant. In particular, work on language reclamation to support decolonization and tribal sovereignty (Leonard, 2012) and work in sociolinguistics focus-ing on developing co-equal research relationships with community members and supporting linguistic justice efforts (e.g., Bucholtz et al., 2014Bucholtz et al., , 2016Bucholtz et al., , 2019 provide examples of more emancipatory relationships with communities. Finally, several workshops and events have begun to explore how to empower stakeholders in the development and deployment of technology (Vaccaro et al., 2019;Givens and Morris, 2020;Sassaman et al., 2020) 4 and how to help researchers and practitioners consider when not to build systems at all (Barocas et al., 2020).\n\nAs a starting point for engaging with communities affected by NLP systems, we therefore provide the following concrete research questions:\n\n. How do communities become aware of NLP systems? Do they resist them, and if so, how? . What additional costs are borne by communities for whom NLP systems do not work well? . Do NLP systems shift power toward oppressive institutions (e.g., by enabling predictions that communities do not want made, linguistically based unfair allocation of resources or opportunities (Rosa and Flores, 2017), surveillance, or censorship), or away from such institutions? . Who is involved in the development and deployment of NLP systems? How do decision-making processes maintain power relations between technologists and communities affected by NLP systems? Can these processes be changed to reimagine these relations?", "filtered_refids": [["b44", "b4", "b40", null, "b81", "b68"], [], ["b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 3596, "num_references": 7}
{"corpusid_sectionid": "253447259-s1", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "The Task", "section": "The definition of a grammatical error is surprisingly difficult. Some types of spelling errors (such as accomodation with a single m) are about equally distributed between native and non-native writers and have no grammatical reflexes, so could be reasonably excluded. Others, such as he eated, are boundary cases as they result from over-regularisation of morphology, whilst he would eated is clearly ungrammatical in the context of a modal auxiliary verb. At the interpretative boundary, infelicitous discourse organisation, such as Kim fell. Sandy pushed him. where the intention is to explain why Kim fell, is not obviously a grammatical error per se but nevertheless can be 'corrected' via a tense change (Sandy had pushed him.) as opposed to a reordering of the sentences. Other tense changes which can span sentences appear more grammatical, such as Kim will We met they talked and left We met, they talked and left Unidiomatic\n\nWe had a big conversation We had a long conversation Multiple I sea the see from the seasoar I saw the sea from the seesaw Table 1 Example error types make Sandy a sandwich. Sandy ate it., as the discourse is incoherent and correction will require a tense change in one or other sentence. In practice, the task has increasingly been defined in terms of what corrections are annotated in corpora used for the shared tasks. These use a variety of annotation schemes but all tend to adopt minimal modifications of errorful texts to create errorfree text with the same perceived meaning. Other sources of annotated data, such as that sourced from the online language learning platform Lang-8 (Mizumoto et al. 2012;Tajiri, Komachi, and Matsumoto 2012), often contain much more extensive rewrites of entire paragraphs of text. Given this resource-derived definition of the task, systems are evaluated on their ability to correct all kinds of mistakes in text, including spelling and discourse level errors that have no or little grammatical reflex. The term 'Grammatical' Error Correction is thus something of a misnomer, but is nevertheless now commonly understood to encompass errors that are not always strictly grammatical in nature. A more descriptive term is Language Error Correction. Table 1 provides a small sample of (constructed) examples that illustrate the range of errors to be corrected and some of the issues that arise with the precise definition and evaluation of the task. Errors can be classified into three broad categories: replacement errors, such as dreamed for dreamt in the second example; omission errors, such as on in the first example; and insertion errors, such as the in the third example. Some errors are complex in the sense that their correction requires a sequence of replacement, omission or insertion steps to correct, as with the syntax example. Sentences may also contain multiple distinct errors that require a sequence of corrections, as in the multiple example. Both the classification and specification of correction steps for errors can be and has been achieved using different schemes and approaches. For instance, correction of the syntax example involves transposing two adjacent words so we could introduce a fourth broad class and correction step of transposition (word order). All extant annotation schemes break these broad classes down into further subclasses based on the part-ofspeech of the words involved, and perceived morphological, lexical, syntactic, semantic or pragmatic source of the error. The schemes vary in the number of such distinctions, ranging from just over two dozen (NUCLE: (Dahlmeier, Ng, and Wu 2013)) to almost a hundred (CLC: (Nicholls 2003)). The schemes also identify different error spans in source sentences and thus suggest different sets of edit operations to obtain the suggested corrections. For instance, the agreement error example might be annotated as She likes him and [kiss \u2192 kisses] him at the token level or simply [\u01eb \u2192 es] at the character level. These differing annotation decisions affected the evaluation of system performance in artefactual ways, so a two-stage automatic standardisation process was developed, ERRANT (Felice, Bryant, and Briscoe 2016;Bryant, Felice, and Briscoe 2017), which maps parallel errorful and corrected sentence pairs to a single annotation scheme using a linguistically-enhanced alignment algorithm and series of error type classification rules. This scheme uses 25 main error type categories, based primarily on part-of-speech and morphology, which are further subdivided into missing (omission), unnecessary (insertion) and replacement errors. This approach allows consistent automated training and evaluation of systems on any or all parallel corpora as well as supporting a more fined-grained analysis of the strengths and weaknesses of systems in terms of different error types.\n\nUltimately however, the correction of errors requires an understanding of the communicative intention of the writer. For instance, the determiner example in Table 1 implicitly assumes a 'neutral' context where the intent is to make a statement about generic icecream rather than a specific instance. In a context where, say, a specific ice-cream dessert is being compared to an alternative dessert, then the determiner is felicitous. Similarly the preposition omission error might not be an error if the writer is describing a context in which a talk was oversubscribed and many attendees had to stand because of a lack of seats. Though annotators will most likely take both the context and perceived writer's intention into account when identifying errors, GEC itself is instead often framed as an isolated sentence-based task that ignores the wider context. This can introduce noise in the task in that errorful sequences in context may appear correct in isolation out of context. A related issue is that correction may not only depend on communicative intent, but also factors such as dialect and genre. For example, correcting dreamed to dreamt may be appropriate if the target is British English, but incorrect for American English.\n\nA larger issue arises with differing possibilities for correction. For example, correcting the tense/aspect example to kissing or to kiss in the context of likes seems equally correct. However, few existing corpora provide more than one possibility which means the true performance of systems is often underestimated. However, the same two corrections are not equally correct as complements of a verb such as try depending on whether the context implies that a kissing event occurred or not. The issue of multiple possible corrections arises with many if not most examples: for instance I haven't the book, We met them, talked and left, We had an important conversation, The sea I see from the seesaw (is calm) are all plausible alternative corrections for some of the examples in Table 1. For this reason, several of the shared tasks have also evaluated performance on grammatical error detection, as this is valuable in some applications. Recently, some work has explored treating the GEC task as one of document-level correction (e.g. Chollampatt, Wang, and Ng (2019); ) which, in principle, could ameliorate some of these issues but is currently hampered by a lack of appropriately structured corpora.", "filtered_refids": [[], ["b98", "b157", "b118", null, "b33"], [], ["b23"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 42, "num_chars": 7287, "num_references": 6}
{"corpusid_sectionid": "253447259-s4", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "Annotation Challenges", "section": "As mentioned in Section 1.1, the notion of a grammatical error is hard to define as different errors may have different scope (e.g. local vs. contextual), complexity (e.g. orthographic vs. semantic) and corrections (e.g. [this books \u2192 this book] vs. [this books \u2192 these books]. Human annotation is thus an extremely cognitively demanding task and so clear annotation guidelines are a crucial component of dataset quality. This section briefly outlines three important aspects of data collection: Minimal vs. Fluent Corrections, Annotation Consistency, and Preprocessing Challenges.\n\nMinimal vs. Fluent Corrections. Most GEC corpora have been annotated on the principle of minimal corrections, i.e. annotators should make the minimum number of changes to make a text grammatical. Sakaguchi et al. (2016) argue, however, that this can often lead to corrections that sound unnatural, and so it would be better to annotate corpora on the principle of fluent corrections instead. Consider the following example:\n\nOriginal I want explain to you some interesting part from my experience.\n\nMinimal I want to explain to you some interesting parts of my experience. Fluent I want to tell you about some interesting parts of my experience.\n\nWhile the minimal correction primarily inserts a missing infinitival to before explain to make the sentence grammatical, the fluent correction also changes explain to tell you about because it is more idiomatic to tell someone about an experience rather than explain an experience. One of the main challenges of this distinction, however, is that it is very difficult to draw a line between what constitutes a minimal correction and what constitutes a fluent correction. This is because minimal corrections (e.g. missing determiners) are a subset of fluent corrections, and so there cannot be fluent corrections without minimal corrections. It is also the case that minimal corrections are typically easier to make than fluent corrections (for both humans and machines), although it is undeniable that fluent corrections are the more desirable outcome. Ultimately, although it is very difficult to precisely define a fluent correction, annotation guidelines should nevertheless attempt to make clear the extent to which annotators are expected to edit.\n\nAnnotation Consistency. A significant challenge of human annotation is that corrections are subjective and there is often more than one way to correct a sentence (Bryant and Ng 2015;Choshen and Abend 2018b). It is nevertheless important that annotators attempt to be consistent in their judgements, especially if they are explicitly annotating edit spans. For example the edit [has eating \u2192 was eaten] can also be represented as [has \u2192 was] and [eating \u2192 eaten], and this choice not only affects data exploration and analysis, but can also have an impact on edit-based evaluation. Similarly, the edit [the informations \u2192 information] can also be represented as [the \u2192 \u01eb] and [informations \u2192 information], but the latter may be more intuitive because it represents two independent edits of clearly distinct types. Explicit error type classification is thus another important aspect of annotator consistency, as an error type framework (if any) not only increases the cognitive burden on the annotator, but also might influence an annotator towards a particular correction given the error types that are available . Ultimately, if annotators are tasked with explicitly defining the edits they make to correct a sentence, annotator guidelines must clearly define the notion of an edit.\n\nPreprocessing Challenges. While human annotators are trained to correct natural text, GEC systems are typically trained to correct word tokenised sentences (mainly for evaluation purposes). This mismatch means human annotations typically undergo several preprocessing steps in order to produce the desired output format (Bryant and Felice 2016). The first of these transformations involves converting character-level edits to token-level edits. While this is often straightforward, it can sometimes be the case that a human-annotated character span does not map to a complete token; e.g. [ing \u2192 ed] to denote the edit [dancing \u2192 danced]. Although such cases can often (but not always) be resolved automatically, e.g., by expanding the character spans of the edit or calculating token alignment, they can also be reduced by training annotators to explicitly annotate longer spans rather than sub-words.\n\nThe second transformation involves sentence tokenisation, which is potentially more complex given human edits may change sentence boundaries; e.g. [A. B, C. \u2192 A, B. C.]. Sentences are nevertheless typically tokenised based solely on the original text, with the acknowledgement that some may be sentence fragments (to be joined with the following sentence) and that edits which cross sentence boundaries are ignored (e.g. [. Because \u2192 ,  because]. It is worth noting that this issue only affects sentence-based GEC systems (the vast majority) but paragraph or document-based systems are unaffected.", "filtered_refids": [[], ["b140"], [], [], [], [null, "b25"], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 5068, "num_references": 5}
{"corpusid_sectionid": "253447259-s7", "title": "Grammatical Error Correction: A Survey of the State of the Art", "date": "2022-11-09", "section_title": "FCE.", "section": "The First Certificate in English (FCE) corpus (Yannakoudakis, Briscoe, and Medlock 2011) is a public subset of the Cambridge Learner Corpus (CLC) (Nicholls 2003) that consists of 1,244 scripts (\u223c531k words) written by international learners of English as a second language (L2 learners). Each script typically contains two answers to a prompt in the style of a short essay, letter, or description, and each answer has been corrected by a single annotator who has identified and classified each edit according to a framework of 88 error types (Nicholls 2003 2001)) and the data is split into a standard training, development and test set. The FCE was used as the official dataset of the HOO-2012 shared task (Dale, Anisimoff, and Narroway 2012), one of the official training datasets of the BEA-2019 shared task (Bryant et al. 2019), and has otherwise commonly been used for grammatical error detection (Rei and Yannakoudakis 2016;Bell, Yannakoudakis, and Rei 2019;). It also contains essay level scores, as well as other limited metadata about the learner, and has been used for automatic essay scoring (AES) (e.g. Ke and Ng (2019)).\n\nNUCLE/CoNLL. The National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier, Ng, and Wu 2013) consists of 1,397 argumentative essays (\u223c1.16m words) written by NUS undergraduate students who needed L2 English language support. The essays, which are approximately C1 level, are written on a diverse range of topics including technology, healthcare, and finance, and were each corrected by a single annotator who identified and classified each edit according to a framework of 28 error types. NUCLE was used as the official training corpus of the CoNLL-2013 and CoNLL-2014 shared tasks  as well as one of the official training datasets of the BEA-2019 shared task (Bryant et al. 2019). The CoNLL-2013 and CoNLL-2014 test sets were annotated under similar conditions to NUCLE and respectively consist of 50 essays each (\u223c30k words) on the topics of i) surveillance technology and population aging, and ii) genetic testing and social media. The CoNLL-2014 test set was also doubly annotated by 2 independent annotators, resulting in 2 sets of official reference annotations; Bryant and Ng (2015) and Sakaguchi et al. (2016) subsequently collected another 8 sets of annotations each for a total of 18 sets of reference annotations. The CoNLL-2013 dataset is now occasionally used as a development set, while the CoNLL-2014 dataset is one of the most commonly used benchmark test sets. One limitation of the CoNLL-2014 test set is that it is not very diverse given that it consists entirely of essays written by a narrow range of learners on only two different topics.\n\nLang-8. The Lang-8 Corpus of Learner English (Mizumoto et al. 2012;Tajiri, Komachi, and Matsumoto 2012) is a preprocessed subset of the multilingual Lang-8 Learner Corpus (Mizumoto et al. 2011), which consists of 100,000 submissions (\u223c11.8m words) to the language learning social network service,  The texts are wholly unconstrained by topic, and hence include the full range of ability levels (A1-C2), and were written by international L2 English language learners with a bias towards Japanese L1 speakers. Although Lang-8 is one of the largest publicly available corpora, it is also one of the noisiest as corrections are provided by other users rather than professional annotators. A small number of submissions also contain multiple sets of corrections, but all annotations are provided as parallel text and so do not contain explicit edits or error types. Lang-8 was also one of the official training datasets of the BEA-2019 shared task (Bryant et al. 2019).\n\nJFLEG. The Johns Hopkins Fluency-Extended GUG corpus (JFLEG) (Napoles, Sakaguchi, and Tetreault 2017) is a collection of 1,501 sentences (\u223c28.1k words) split roughly equally into a development and test set. The sentences were randomly sampled from essays written by L2 learners of English of an unspecified ability level (Heilman et al. 2014) and corrected by crowdsourced annotators on Amazon Mechanical Turk (Crowston 2012). Each sentence was annotated a total of 4 times, resulting in 4 sets of parallel reference annotations, but edits were not explicitly defined or classified. The main innovation of JFLEG is that sentences were corrected to be fluent rather than minimally grammatical (Section 2.1). The main criticisms of JFLEG are that it is much smaller than other test sets, the sentences are presented out of context, and it was not corrected by professional annotators (Napoles, N\u0103dejde, and Tetreault 2019).\n\nW&I+LOCNESS. The Write & Improve (W&I) and LOCNESS corpus (Bryant et al. 2019) respectively consist of 3,600 essays (\u223c755k words) written by international learners of all ability levels (A1-C2) and 100 essays (\u223c46.2k words) written by native British/American English undergraduates. It was released as the official training, development and test corpus of the BEA-2019 shared task and was designed to be more balanced than other corpora such that there are roughly an equal number of sentences at each ability level: Beginner, Intermediate, Advanced, Native. The W&I essays come from submissions to the Write & Improve online essay-writing platform 3 (Yannakoudakis et al. 2018) and the LOCNESS essays, which only comprise part of the development and test sets, come from the LOCNESS corpus (Granger 1998). The training and development set essays were each corrected by a single annotator, while the test set essays were corrected by 5 annotators resulting in 5 sets of parallel reference annotations. Edits were explicitly defined, but not manually classified, so error types were added automatically using the ERRANT framework (Bryant, Felice, and Briscoe 2017). The test set references are not currently publicly available, so all evaluation on this dataset is done via the BEA-2019 Codalab competition platform, 4 which ensures all systems are evaluated in the same conditions.", "filtered_refids": [["b10", "b34", "b118", "b177", "b71", null, "b148"], ["b140", null, "b33", "b148"], ["b99", "b148", "b98", "b157"], ["b115", "b110", "b56"], [null, "b178"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 5987, "num_references": 20}
{"corpusid_sectionid": "221970053-s4", "title": "A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English", "date": "2020-09-27", "section_title": "Rule and Feature Based Methods", "section": "Before the deep learning era, human-designed rules (Hobbs, 1978;Raghunathan et al., 2010), knowledge (Ponzetto and Strube, 2006;Versley et al., 2016), and features (Ng, 2005;Wiseman et al., 2016) dominated the general coreference resolution and PCR tasks. Some rules and features are crucial for correctly resolving pronouns (Lee et al., 2013). For example, 'he' typically refers to males and 'she' typically refers to females; 'it' typically refers to singular objects and 'them' typically refers to plural objects. The performances of these methods heavily rely on the coverage and quality of the manually defined rules and features. Based on these designed features (Bengtson and Roth, 2008), a few more advanced machine learning models were applied to the coreference resolution task. For example, instead of identifying coreference relation pair-wisely, (Clark and Manning, 2015) proposes an entity-centric coreference system that can learn an effective policy for building coreference chains incrementally. Besides that, a novel model was also proposed to predict coreference relations with a deep reinforcement learning framework (Clark and Manning, 2016). Moreover, heuristic rules based on linguistic knowledge can also be incorporated into constraints for machine learning models .", "filtered_refids": [["b44", "b4", "b14", "b29", "b32", "b45", "b21", "b5", "b0", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1291, "num_references": 10}
{"corpusid_sectionid": "221970053-s6", "title": "A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English", "date": "2020-09-27", "section_title": "Further Improvements", "section": "Recently, on top of the end-to-end model, a few improved works were proposed to address different limitations of the original end-to-end model 5 :\n\n1. Higher-order Information: One limitation of the original end-to-end model is that all predictions are based on pairs, which is not sufficient for capturing higher-order coreference relations. To fix this issue, a differentiable approximation module was proposed in  to provide the higher-order coreference resolution inference ability (i.e., leveraging the coreference cluster to better predict the coreference relations). Moreover, this work first incorporates ELMo (Peters et al., 2018), a kind of deep contextualized word representations, as part of the word representation, which is proven very effective.\n\n2. Structured Knowledge: Another limitation of the end-to-end model is that its success heavily relies on the quality and coverage of the training data. However, in real applications, it is labor-intensive and almost impossible to annotate a large-scale dataset to contain all scenar-  (Clark and Manning, 2015) 25.8 62.1 36.5 28.9 64.9 40.0 9.8 6.3 7.6 25.4 59.3 36.5 Deep-RL (Clark and Manning, 2016) 78.6 63.9 70.5 73.3 68.9 71.0 3.7 2.9 5.5 76.4 61.2 68.0\n\nEnd-to-end (Lee et al., 2017) (Zhang et al., 2019c) 80.0 75.6 77.7 81.7 72.2 76.7 50.8 64.6 56.9 77.9 74.0 75.9 + SpanBERT (Joshi et al., 2020) 82   ios. To solve this problem, two works (Zhang et al., 2019b,c) were proposed to inject external structured knowledge into the end-to-end model. Among these two, (Zhang et al., 2019b) requires converting external knowledge into features while (Zhang et al., 2019c) directly uses external knowledge in the format of triplets.", "filtered_refids": [[], ["b30"], ["b5", "b4"], ["b15", "b50", null, "b22", "b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1694, "num_references": 8}
{"corpusid_sectionid": "221970053-s8", "title": "A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English", "date": "2020-09-27", "section_title": "Performances and Analysis", "section": "We follow the experimental setting of (Zhang et al., 2019c) and test the performance 6 of representative models (Raghunathan et al., 2010;Manning, 2015, 2016;Lee et al., 2017;Zhang et al., 6 We use the released codes of different models along with their default hyper-parameters to finish the experiments. For the end2end model, we also include ELMo (Peters et al., 2018) as part of the representation and achieve better performance than the original one in Table 1. 2019c; Joshi et al., 2020) on the CoNLL-2012 dataset (Pradhan et al., 2012). The experiment setting (both detection the mentions and resolving the coreference relations) and evaluation metric are the same as these previous works on CoNLL-2012. From the results in Table 2, we can observe that with the help of the end-to-end model and further modifications, the community has made great progress on the standard evaluation set. For example, the end-to-end model achieves an F1 score over 70 and adding external knowledge (either in a structured way or a representation way) further boost the performance. Among all pronoun types, all models perform better on third personal and possessive pronouns, and relatively poorly on demonstrative ones. This is mainly because of the imbalanced distribution of the dataset (i.e., third personal and possessive pronouns appear much more than demonstrative ones).", "filtered_refids": [["b30", null, "b22", "b33", "b51", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1368, "num_references": 6}
{"corpusid_sectionid": "237941087-s1", "title": "Coreference Resolution for the Biomedical Domain: A Survey", "date": "2021-09-25", "section_title": "Background", "section": "Coreference resolution in the general domain has a long history of being studied from early heuristicbased and rule-based approaches to recent learningbased approaches. Lee et al. (2017) proposed the first end-to-end neural coreference resolution model which uses LSTM encoder. Based on the end-to-end model, many extensions to the model have been proposed. BERT and SpanBERT were proposed to replace the LSTM encoder and achieved better performance on OntoNotes dataset (Joshi et al. 2019, Joshi et al. 2020. Wu et al. (2020) adapted questionanswering framework on coreference resolution, and achieved the state-of-the-art result with 83.1% F1 score on OntoNotes dataset. Ye et al. (2020) proposed a novel language representation model CorefBERT, which can capture the coreferential relations in context. However, these general coreference systems do not work well in the biomedical domain due to the lack of domain knowledge. For example, the end-to-end model (Lee et al., 2017) only achieved 33.85% and 61.25% F1 scores on CRAFT-CR and BioNLP datasets respectively (Trieu et al., 2018), but achieved 68.8% F1 score on OntoNotes dataset (Hovy et al., 2006), which covers multiple genres, such as newswire, broadcast news and web data.", "filtered_refids": [["b52", "b54", "b56", "b18", "b20", "b27", "b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1236, "num_references": 7}
{"corpusid_sectionid": "237941087-s2", "title": "Coreference Resolution for the Biomedical Domain: A Survey", "date": "2021-09-25", "section_title": "Biomedical Coreference Datasets", "section": "Several biomedical datasets with coreference annotations exist, but different document selection criteria, annotation schemes, domains and coreference types were used. The best known include:\n\nMEDSTRACT  is a corpus consisting of MEDLINE abstracts with coreference annotation. It is mainly concerned with two forms of anaphora: pronominal and sortal (definite noun phrase) anaphora. This corpus adapted the MUC-7 annotation scheme (Hirschman, 1997); in addition, semantic types from UMLS (Bodenreider, 2004) were also annotated.\n\nFlySlip (Gasperin et al., 2007) contains anaphoric links among noun phrases, including coreferent and associative relations. Different from MEDSTRACT, full-text biomedical articles were annotated in this corpus. FlySlip was annotated according to a domain-specific annotation scheme.\n\nGENIA-MedCo (Su et al., 2008) is a coreferentially annotated version of the GENIA corpus (Kim et al., 2003), which in turn consists of 1999 MED-LINE abstracts. This corpus follows the MUC-7 annotation scheme, but adds more linguistic based relations.\n\nDrugNerAR (Segura-Bedmar et al., 2010) was created to study anaphoric expressions in the task of extracting drug-drug interactions in pharmacological literature. This corpus consists of 49 fulltext from the DrugBank database, which contains 4900 drug entries.\n\nBioNLP-ST'11 COREF (Nguyen et al., 2011) was created in support of one of the tasks of the BioNLP 2011 shared task, focusing on finding anaphoric protein references, and based on the observation that one of major difficulties in event extraction is coreference resolution. This corpus was derived from three resources: MedCo coreference annotation (Su et al., 2008), Genia event annotation (Kim et al., 2008), and Genia Treebank (Tateisi et al., 2005).\n\nHANAPIN (Batista-Navarro and Ananiadou, 2011) is comprised of 20 full-text articles from biochemistry literature. In addition to nominal and pronominal anaphora, this corpus also annotated abbreviation/acronyms and numerical anaphora. CRAFT-CR (Cohen et al., 2017) consists of 97 full-text biomedical journal articles. Similar to the general domain, this corpus was annotated with coreferent chains in full-text articles, while most other biomedical coreference datasets focuse on annotating the pairwise coreference relation between an anaphor and its antecedent. In addition, all coreference expressions were annotated regardless of semantic type.\n\nThese datasets are summarized in Table 1.", "filtered_refids": [[], ["b3", "b17"], ["b12"], ["b47", "b22"], [], ["b49", "b47", "b23", "b37"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2474, "num_references": 10}
{"corpusid_sectionid": "237941087-s4", "title": "Coreference Resolution for the Biomedical Domain: A Survey", "date": "2021-09-25", "section_title": "Pre-training on biomedical corpora", "section": "Following the success of large-scale pre-training language models (PLMs) in the general domain, several biomedical-domain PLMs have been developed in recent years by pre-training on large-scale biomedical corpora. Most biomedical PLMs conduct continual pretraining of the general domain PLMs and still use vocabulary trained on the general domain text. BioBERT  is the first transformerbased biomedical PLM, pre-trained on PubMed abstracts and PubMed Central full-text articles. Clini-calBERT and Bio_ClinicalBERT (Alsentzer et al., 2019) are pre-trained on MIMIC-III Clinical Notes, whereas BlueBERT (Peng et al., 2019) uses both PubMed and MIMIC-III for pre-training. All these models are pre-trained based on general BERT, except Bio_ClinicalBERT which is initialized from BioBERT.\n\nIn addition to initializing from general BERT, some biomedical PLMs are directly pre-trained on biomedical text from scratch and use domainspecific custom vocabulary. SciBERT (Beltagy et al., 2019) is pre-trained on biomedical and computer science papers from scratch and achieved good performance on many scientific NLP tasks. PubMedBERT (Gu et al., 2020) and BioELECTRA (raj Kanakarajan et al., 2021) are both pre-trained on PubMed abstract and PubMed Central full text articles, but the latter adopts ELECTRA architecture (Clark et al., 2019). BioMegatron (Shin et al., 2020) is a large-scale model based on Megatron (Shoeybi et al., 2019) architecture. It also investigated the effect of vocabulary and corpora domain on the performance of biomedical tasks.", "filtered_refids": [["b38", "b0"], ["b44", "b45", null, "b7", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1547, "num_references": 7}
{"corpusid_sectionid": "11591301-s3", "title": "A Survey on the Role of Negation in Sentiment Analysis", "date": "2010-07-10", "section_title": "Negation and Bag of Words in Supervised Machine Learning", "section": "Several research efforts in polarity classification employ supervised machine-learning algorithms, like Support Vector Machines, Na\u00efve Bayes Classifiers or Maximum Entropy Classifiers. For these algorithms, already a low-level representation using bag of words is fairly effective (Pang et al., 2002). Using a bag-of-words representation, the supervised classifier has to figure out by itself which words in the dataset, or more precisely feature set, are polar and which are not. One either considers all words occurring in a dataset or, as in the case of Pang et al. (2002), one carries out a simple feature selection, such as removing infrequent words. Thus, the standard bag-of-words representation does not contain any explicit knowledge of polar expressions. As a consequence of this simple level of representation, the reversal of the polarity type of polar expressions as it is caused by a negation cannot be explicitly modeled.\n\nThe usual way to incorporate negation modeling into this representation is to add artificial words: i.e. if a word x is preceded by a negation word, then rather than considering this as an occurrence of the feature x, a new feature NOT x is created. The scope of negation cannot be properly modeled with this representation either. Pang et al. (2002), for example, consider every word until the next punctuation mark. Sentence 2 would, therefore, result in the following representation:\n\n8. I do not NOT like NOT this NOT new NOT Nokia NOT model.\n\nThe advantage of this feature design is that a plain occurrence and a negated occurrence of a word are reflected by two separate features. The disadvantage, however, is that these two contexts treat the same word as two completely different entities.\n\nSince the words to be considered are unrestricted, any word -no matter whether it is an actual polar expression or not -is subjected to this negation modification. This is not only linguistically inaccurate but also increases the feature space with more sparse features (since the majority of words will only be negated once or twice in a corpus). Considering these shortcomings, it comes to no surprise that the impact of negation modeling on this level of representation is limited. Pang et al. (2002) report only a negligible improvement by adding the artificial features compared to plain bag of words in which negation is not considered. Despite the lack of linguistic plausibility, supervised polarity classifiers using bag of words (in particular, if training and testing are done on the same domain) offer fairly good performance. This is, in particular, the case on coarse-grained classification, such as on document level. The success of these methods can be explained by the fact that larger texts contain redundant information, e.g. it does not matter whether a classifier cannot model a negation if the text to be classified contains twenty polar opinions and only one or two contain a negation. Another advantage of these machine learning approaches on coarsegrained classification is their usage of higher order n-grams. Imagine a labeled training set of documents contains frequent bigrams, such as not appealing or less entertaining. Then a feature set using higher order n-grams implicitly contains negation modeling. This also partially explains the effectiveness of bigrams and trigrams for this task as stated in (Ng et al., 2006). The dataset used for the experiments in (Pang et al., 2002;Ng et al., 2006) has been established as a popular benchmark dataset for sentiment analysis and is publicly available 1 .", "filtered_refids": [["b17"], ["b17"], [], [], ["b15", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3571, "num_references": 4}
{"corpusid_sectionid": "11591301-s5", "title": "A Survey on the Role of Negation in Sentiment Analysis", "date": "2010-07-10", "section_title": "Contextual Valence Shifters", "section": "The first computational model that accounts for negation in a model that includes knowledge of polar expressions is (Polanyi and Zaenen, 2004). The different types of negations are modeled via contextual valence shifting. The model assigns scores to polar expressions, i.e. positive scores to positive polar expressions and negative scores to negative polar expressions, respectively. If a polar expression is negated, its polarity score is simply inverted (see Example 1).\n\nIn a similar fashion, diminishers are taken into consideration. The difference is, however, that the score is only reduced rather than shifted to the other polarity type (see Example 2).\n\nBeyond that the model also accounts for modals, presuppositional items and even discourse-based valence shifting. Unfortunately, this model is not implemented and, therefore, one can only speculate about its real effectiveness.\n\nKennedy and Inkpen (2005) evaluate a negation model which is fairly identical to the one proposed by Polanyi and Zaenen (2004) (as far as simple negation words and diminishers are concerned) in document-level polarity classification. A simple scope for negation is chosen. A polar expression is thought to be negated if the negation word immediately precedes it. In an extension of this work (Kennedy and Inkpen, 2006) a parser is considered for scope computation. Unfortunately, no precise description of how the parse is used for scope modeling is given in that work. Neither is there a comparison of these two scope models measuring their respective impacts.\n\nFinal results show that modeling negation is important and relevant, even in the case of such simple methods. The consideration of negation words is more important than that of diminishers. Wilson et al. (2005) carry out more advanced negation modeling on expression-level polarity classification. The work uses supervised machine learning where negation modeling is mostly encoded as features using polar expressions. The features for negation modeling are organized in three groups:", "filtered_refids": [["b18"], [], [], ["b7", "b18"], ["b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2039, "num_references": 4}
{"corpusid_sectionid": "11591301-s6", "title": "A Survey on the Role of Negation in Sentiment Analysis", "date": "2010-07-10", "section_title": "Features for Negation Modeling", "section": "\u2022 negation features \u2022 shifter features \u2022 polarity modification features Negation features directly relate to negation expressions negating a polar expression. One feature checks whether a negation expression occurs in a fixed window of four words preceding the polar expression. The other feature accounts for a polar predicate having a negated subject. This frequent long-range relationship is illustrated in Sentence 9. All negation expressions are additionally disambiguated as some negation words do not function as a negation word in certain contexts, e.g. not to mention or not just. Shifter features are binary features checking the presence of different types of polarity shifters. Polarity shifters, such as little, are weaker than ordinary negation expressions. They can be grouped into three categories, general polarity shifters, positive polarity shifters, and negative polarity shifters. General polarity shifters reverse polarity like negations. The latter two types only reverse a particular polarity type, e.g. the positive shifter abate only modifies negative polar expressions as in abate the damage. Thus, the presence of a positive shifter may indicate positive polarity. The set of words that are denoted by these three features can be approximately equated with diminishers. Finally, polarity modification features describe polar expressions of a particular type modifying or being modified by other polar expressions. Though these features do not explicitly contain negations, language constructions which are similar to negation may be captured. In the phrase [disappointed \u2212 hope + ] \u2212 , for instance, a negative polar expression modifies a positive polar expression which results in an overall negative phrase. Adding these three feature groups to a feature set comprising bag of words and features counting polar expressions results in a significant improvement. In (Wilson et al., 2009), the experiments of Wilson et al. (2005) are extended by a detailed analysis on the individual effectiveness of the three feature groups mentioned above. The results averaged over four different supervised learning algorithms suggest that the actual negation features are most effective whereas the binary polarity shifters have the smallest impact. This is consistent with Kennedy and Inkpen (2005) given the similarity of polarity shifters and diminishers.\n\nConsidering the amount of improvement that is achieved by negation modeling, the improvement seems to be larger in (Wilson et al., 2005). There might be two explanations for this. Firstly, the negation modeling in (Wilson et al., 2005) is considerably more complex and, secondly, Wilson et al. (2005) evaluate on a more fine-grained level (i.e. expression level) than Kennedy and Inkpen (2005) (they evaluate on document level). As already pointed out in \u00a73.1, document-level polarity classification contains more redundant information than sentence-level or expression-level polarity classification, therefore complex negation modeling on these levels might be more effective since the correct contextual interpretation of an individual polar expression is far more important 2 . The fine-grained opinion corpus used in (Wilson et al., 2005;Wilson et al., 2009) and all the resources necessary to replicate the features used in these experiments are also publicly available 3 .", "filtered_refids": [["b24", "b6", "b25"], ["b24", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3354, "num_references": 5}
{"corpusid_sectionid": "231839511-s3", "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries", "date": "2021-02-04", "section_title": "Sampling or selection bias", "section": "Sampling bias occurs when observations are drawn from an unrepresentative subset of the population being studied (Marshall, 1996) and applied more widely. In our context, this might arise when selecting communities from which to collect language data, or specific individuals within each community. When sampling communities, bias can be introduced if convenience is prioritized. Communities which are easier to access may not produce language data representative of a larger area or group. This can be illustrated through Uganda's refugee response, which consists of 13 settlements (including the 2nd largest in the world) hosted in 12 districts (UNHCR, 2020). Data collection may be easier in one of the older, established settlements; however, such data cannot be generalised over the entire refugee response due to different cultural backgrounds, length of stay of refugees in different areas, and the varied stages along the humanitarian chain -emergency, recovery or developmentfound therein (Winter, 1983;OECD, 2019). Prioritizing convenience in this case may result in corpora which over-represents the cultural and economic contexts of more established, longer-term refugees. When sampling interviewees, bias can be introduced when certain sub-sets of a community have more data collected than others (Bryman, 2012). This is seen when data is collected only from men in a community due to cultural norms (Nadal, 2017), or only from wealthier people in cell-phone-based surveys (Labrique et al., 2017).", "filtered_refids": [["b28", "b41", "b80", null, "b19", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1510, "num_references": 6}
{"corpusid_sectionid": "231839511-s4", "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries", "date": "2021-02-04", "section_title": "Observer bias", "section": "Observer bias occurs when there are systematic errors in how data is recorded, which may stem from observer viewpoints and predispositions (Gonsamo and D'Odorico, 2014). We identify three key observer biases relevant to our context. Firstly, confirmation bias, which refers to the tendency to look for information which confirms one's preconceptions or hypotheses (Nickerson, 1998). Researchers collecting data in LICs may expect interviewees to express needs or hardships based on their preconceptions. As Kumar (1987) points out, \"often they hear what they want to hear and ignore what they do not want to hear\". A team conducting a needs assessment for a rural electrification project, for instance, may expect a need for electricity, and thus consciously or subconsciously seek data which confirms this, interpret potentially unrelated data as electricity-motivated (Hirmer and Guthrie, 2017), or omit data which contradicts their hypothesis (Peters, 2020). Using such data to train NLP models may introduce unintentional bias towards the original expectations of the researchers instead of accurately representing the community.\n\nSecondly, the interviewer's understanding and interpretation of the speaker's utterances might be influenced by their class, culture and language. Note that, particularly in countries without strong language standardisation policies, consistent semantic shifts can happen even between varieties spoken in neighboring regions (Gordon, 2019), which may result in systematic misunderstanding (Sayer, 2013). For example, in the neighboring Ugandan tribes of Toro and Bunyoro, the same word omunyoro means respectively husband and a member of the tribe. Language data collected in such contexts, if not properly handled, may contain inaccuracies which lead to NLP models that misrepresent these tribes. Rich information communicated through gesture, expression, and tone (i.e. nonverbal data, Oliver et al. (2005)) may also be systematically lost during verbatim transcription, causing inadvertent inconsistencies in the corpora.\n\nThirdly, interviewer bias, which refers to the subjectivity unconsciously introduced into data gathering by the worldview of the interviewer (Frey, 2018). For instance, a deeply religious interviewer may unintentionally frame questions through religious language (e.g. it is God's will, thank God, etc.), or may perceive certain emotions (e.g. thankfulness) as inherently religious, and record language data including this perception. The researcher's attitude and behaviour may also influence responses (Silverman, 2013); for instance, when interviewers take longer to deliver questions, interviewees tend to provide longer responses (Matarazzo et al., 1963). Unlike in internetbased language data collection, where all speakers are exposed to uniform, text-based interfaces, collecting data from illiterate communities necessitates the presence of an interviewer, who cannot always be the same person due to scalability constraints, introducing this inevitable variability and subsequent data bias.", "filtered_refids": [["b6", "b39", "b47", null, "b17"], ["b42"], ["b30", null, "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 3061, "num_references": 9}
{"corpusid_sectionid": "231839511-s5", "title": "Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries", "date": "2021-02-04", "section_title": "Response bias", "section": "Response bias occurs when speakers provide inaccurate or false responses to questions. This is particularly important when working in rural settings, where the majority of data collection is currently related to SD projects. The majority of existing data is biased by the projects for which it has been collected, and any newly collected data for NLP uses is also likely to be used in decision making for SD. This inherent link of data collection to material development outcomes inevitably affects what is communicated. There are five key response biases relevant to our context.\n\nFirstly, recall bias, where speakers recall only certain events or omit details (Coughlin, 1990). This is often as a result of external influences, such as the presence of a data collector who is new to the community. Recall can also be affected by the distortion or amplification of traumatic memories (Strange and Takarangi, 2015); if data is collected around a topic a speaker may find traumatic, recall bias may be unintentionally introduced.\n\nSecondly, social desirability bias, which refers to the tendency of interviewees to provide socially desirable/acceptable responses rather than honest responses, particularly in certain interview contexts (Bergen and Labont\u00e9, 2020). In tight-knit rural communities, it may be difficult to deviate from traditional social norms, leading to biased data. As an illustrative example, researchers in Nepal found that interviewer gender affected the detail in responses to some sensitive questions (e.g. sex and contraception): participants provided less detail to male interviewers (Axinn, 1991). Social desirability bias can produce corpora which misrepresent community social dynamics or under-represent sensitive topics.\n\nThirdly, recency effect or serial-position, which is the tendency of a person to recall the first and last items in a series best, and the middle items worst (Troyer, 2011). This can greatly impact the content of language data. For instance, in the context of data collection to guide development work, it is important to understand current needs and values (Hirmer and Guthrie, 2016); however, if only the most recent needs are discussed, long-term needs may be overlooked. To illustrate, while a community which has just experienced a poor agricultural season may tend to express the importance of improving agricultural output, other needs which are less top-of-mind (i.e. healthcare, education) may be equally important despite being expressed less frequently. If data containing recency bias is used to develop NLP models, particularly for sustainable development applications (such as for Automatic UPV Classifi-cation, Conforti et al. (2020)), these may amplify current needs and under-represent long-term needs.\n\nFourthly, acquiescence bias, also known as \"yea\" saying (Laajaj and Macours, 2017), which can occur in rural developing contexts when interviewees perceive that certain (possibly false) responses will please a data collector and bring benefits to their community. For example, if data collection is being undertaken by a group with a stated desire to build a school may be more likely to hear about how much education is valued.\n\nFinally, priming effect, or the ability of a presented stimulus to influence one's response to a subsequent stimulus (Lavrakas, 2008). Priming is problematic in data collection to inform SD projects; it can be difficult to collect data on the relative importance of simultaneous (or conflicting) needs if the community is primed to focus on one (Veltkamp et al., 2011). An example is shown in Figure 2a; respondents may be drawn to speak more about the most dominant prompts presented in the chart. This is typical of a broader failure in SD to uncover beneficiary priorities without introducing project bias (Watkins et al., 2012). Needs assessments, like the one referenced above linked to a rural electrification project, tend to focus explicitly on project-related needs instead of more broadly identifying what may be most important to communities (Masangwi, 2015;USAID, 2006). As speakers will usually know why data is being collected in such cases, they may be biased towards stating the project aim as a need, thereby skewing the corpora to over-represent this aim.", "filtered_refids": [[], [null, "b58"], [null], ["b65", "b5"], ["b18"], ["b29", "b79", "b22", "b75", "b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 4274, "num_references": 11}
{"corpusid_sectionid": "265158335-s0", "title": "Exploring undergraduate translation students' perceptions towards machine translation: A qualitative questionnaire survey", "date": 2023, "section_title": "Research background", "section": "The term 'machine translation' (MT) includes various activities related to translation that are performed by computers.A widely used definition provided by Hutchins and Sommers (1992, p. 3) is that MT systems are 'computerised systems responsible for the production of translations from one natural language into another, with or without human assistance'.\n\nThe increasing accuracy and fluency of MT have recently led to MT being included in translation programmes in higher education institutions, with specialised courses for students being provided.However, such courses are often offered at the postgraduate level or towards the last year of an undergraduate programme (e.g., Arenas & Moorkens, 2019;Doherty et al., 2012).One of the concerns is that technologies such as MT might be too difficult for undergraduate translation students to learn.The other concern is that the students' translation performances and their translation competence could be negatively impacted by MT because they might not have the ability to evaluate the output of the technology (Bowker, 2015).Therefore, MT training is not usually available to undergraduate students.In addition, teachers or management may formulate policies that forbid students from using MT in their assignments.\n\nThere is a lack of sufficient evidence in academia to conclude that MT has a negative impact on novice translation students.Most of the previous studies have focused on postgraduate students or undergraduate students in the last year of their programmes (e.g., Jia et al., 2019;Wang et al., 2021;Zaretskaya et al., 2016).Little research has targeted undergraduate translation learners in the early stages of their training.\n\nHowever, it has been observed that students have been interacting with MT in contravention of official instructions.With MT systems and abundant information about them being freely available on the internet, it is unlikely that students would be unaware of MT or would not be interested in experimenting with it.As the quality of MT increases, students might have strong intentions to use MT when learning to translate.The author thus argued that novice translation students' knowledge about and experience of MT could be of value in the curriculum design and pedagogical development of MT courses.\n\nIn previous studies, the participants' perceptions were often solicited via closed-ended questions (e.g., Liu et al., 2022;Yang et al., 2021;Yang & Wang, 2019), the answers to which were later analysed as quantitative data.However, as the participants answered questions on a scale or according to the available choices, their views were limited.For example, Yang and Wang (2019)'s study focused exclusively on students' intentions to use MT; their study was based on a technology acceptance model in which an individual's intention to engage with technology was linked directly to their attitude.A questionnaire using a 5-point scale was developed in accordance with the model and was answered by 109 Chinese student translators.The results supported and verified the technology acceptance model in that the students' perceived ease of use and perceived usefulness of MT were correlated positively with their intentions to use MT.\n\nBased on the above discussion, little is known about how undergraduate students in the early stages of translator training perceive and use MT or what their training needs may be.Therefore, this research intended to survey translation students in the early stages of their translator training to solicit their attitudes towards and perceptions of MT via a qualitative questionnaire survey.This research included open-ended questions, thus allowing the participants to express their opinions freely.Furthermore, unlike interviews, questionnaires with open-ended questions could be self-administered without the researcher's presence, thus avoiding researcher bias.", "filtered_refids": [[null], ["b4", "b1", "b0"], ["b11", "b9", "b7"], [], [null, "b10"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3890, "num_references": 9}
{"corpusid_sectionid": "265158335-s2", "title": "Exploring undergraduate translation students' perceptions towards machine translation: A qualitative questionnaire survey", "date": 2023, "section_title": "Methods and data", "section": "After ethical clearance was obtained from the university, 20 students from an Applied Translation Studies programme at a university based in China were recruited.\n\nThe participants were all in the second year of an undergraduate translation programme; they had similar educational backgrounds and hence comparable language proficiency and translation competencies.They had taken three translation courses covering fundamental translation theories and practices.No specialised training in MT, post-editing (PE) or translation technology was provided in the classroom.\n\nThe survey included ten open-ended questions to solicit the participants' knowledge, experience, perceptions of and attitudes to MT.As few previous studies have used open-ended questions, the design of this questionnaire mainly drew on Gonz\u00e1lez Pastor (2021)'s paper, which had a similar design and goal as the current project, and referenced three other relevant papers (\u00c7etiner & \u0130\u015fisa\u011f, 2019;de Faria Pires, 2020;Schmidhofer & Mair, 2018).Gonz\u00e1lez Pastor (2021) investigated students' attitudes to and perceptions of translation technology before and after being taught about translation technology.The questions were adapted and narrowed down to MT-and PE-specific questions.Questions regarding the students' understanding of translation concepts, processes and products were added as answers to these questions provide an alternative perspective to understand the impact of MT and PE on students' translation processes and products.\n\nThis paper mainly analysed answers to questions regarding the students' knowledge, attitudes to and perceptions of MT.Therefore, only the answers to the following six (out of 10) questions were analysed and presented.6. Do you think there are ethical concerns related to MT, such as legal or moral issues, biases, justice or privacy?Can you explain your thoughts in detail?\n\nThe students were told that they could answer the survey in Chinese or English, whichever they felt most comfortable using.The answers in Chinese were translated by the author, a certified translator in China and Australia with over 15 years of professional experience.\n\nAll the data were de-identified with students' names being indicated as \"s + participant number\" and were imported into NVivo 14 for thematic analysis.The author conducted the analysis twice at two different times to ensure the reliability of the results.", "filtered_refids": [[], [], [null, "b2", "b3"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2408, "num_references": 3}
{"corpusid_sectionid": "5738018-s3", "title": "Annotating genericity: a survey, a scheme, and a corpus", "date": "2015-06-01", "section_title": "ACE entity class annotations", "section": "The research objective of the ACE program (1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008) was the detection and characterization of entities, relations and events in natural text (Linguistic Data Consortium, 2000). All entity mentions receive an entity class label indicating their genericity status. Of the corpora described here, the ACE corpora have been the most widely used for recent research on automatically identifying generic NPs (Reiter and Frank, 2010). The annotation guidelines developed over time; we describe both the initial guidelines of ACE-2 and those from ACE-2005.\n\nThe ACE-2 corpus (Mitchell et al., 2003) includes 40106 annotated entity mentions in 520 newswire and broadcast documents. The annotation guidelines give no formal definition of genericity; annotators are asked to determine whether each entity refers to \"any member of the set in question\" (generic) or rather \"some particular, identifiable member of that set\" (specific/non-generic). 1 This leads to a mix of constructions being marked as generic: types of entities (Good students do all the reading), generalizations across a set of entities (Purple houses are really ugly), hypothetical entities (If a person steps over the line,...) and negated mentions (I saw no one). Suggested attributes of entities are marked as generic (John seems to be a nice person), but a 'positive assertion test' leads to marking both NPs (Joe and a nice guy) as specific in examples like (Joe is a nice guy). Neither of these two cases (be a nice person / be a nice guy) is in fact an entity mention; they are rather predicative uses.\n\nThe guidelines for genericity were redefined for annotation of the ACE-2005 Multilingual Training Corpus (Walker et al., 2006), which contains news, broadcast news, broadcast conversation, forum and weblog texts as well as transcribed conversational telephone speech. In contrast to ACE-2, the ACE-2005 annotation manual 2 clearly defines mentions as kind-referring or not, using the labels GEN (generic) and SPC (specific/non-generic) respectively. The new guidelines also introduce two additional entity class labels for non-attributive mentions. Negatively quantified entities that refer to the empty set of the kind mentioned (There are no confirmed suspects yet) receive the label NEG. The label USP (underspecified) is used for non-generic nonspecific reference, these cases include quantified NPs in modal, future, conditional, hypothetical, negated, uncertain or question contexts. USP also covers 'truly ambiguous cases' that have both generic and non-generic readings (The economic boom is providing new opportunities for women in New Delhi), and cases where the author mentions an entity whose identity would be 'difficult to locate' (Officials reported ...). In our opinion, the latter interferes with the definition of SPC as marking cases where the entity referred to is a particular object in the real world, even if the author does not know its identity (At least four people were injured). The breadth of the USP category causes problems with consistency of application (see Section 3).\n\nThe ACE annotation scheme has also been applied in the Newsreader project. 3 The ECB+ corpus (Cybulska and Vossen, 2014) is an extension of EventCorefBank (ECB), a corpus of news articles marked with event coreference information (Bejan and Harabagiu, 2010). ECB+ annotates entity mentions according to ACE-2005, but collapses the three non-GEN labels into a single category. Roughly 12500 event participant mentions are annotated, some doubly and some singly. Agreement statistics for genericity are not reported. 3 www.newsreader-project.eu", "filtered_refids": [["b30", null], ["b26"], ["b32"], [null, "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3667, "num_references": 6}
{"corpusid_sectionid": "5738018-s4", "title": "Annotating genericity: a survey, a scheme, and a corpus", "date": "2015-06-01", "section_title": "Other corpora annotated at the NP-level", "section": "The resources surveyed here apply carefullydefined notions of genericity but are too small to be feasible machine learning training data.\n\nThe question of whether an NP is generic or not arises in the research context of coreference resolution. Some approaches mark coreference only for non-generic mentions (Hovy et al., 2006;Hinrichs et al., 2004); others include generic mentions (Poesio, 2004), or take care not to mix coreference chains between generic and non-generic mentions (Bj\u00f6rkenstam and Bystr\u00f6m, 2012). Bj\u00f6rkelund et al. (2014) mark genericity in a corpus of German with both coreference and information-status annotations. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they provide a complete overview. In short, they argue that a consistent definition of genericity is lacking and report on their annotation scheme for Czech as applied to the Prague Dependency TreeBank (B\u00f6hmov\u00e1 et al., 2003).\n\nThe GNOME corpus (Poesio, 2004) is a coreference corpus with genericity annotations; NPs are marked with the attributes generic-yes or generic-no. Poesio et al. report that their annotators found it hard to decide how to mark references to substances (A table made of wood) and quantified NPs. Similar to our experience, they found it helpful to have annotators first try to identify generic sentences, and then determine this attribute of the NP. They report an agreement of \u03ba = 0.82 on their corpus, which consists of 900 finite clauses from descriptions of museum objects, pharmaceutical leaflets and dialogues.\n\nComing from a formal semantic perspective, Herbelot and Copestake (2010) and Herbelot and Copestake (2011) describe an approach to treating ambiguously quantified NPs. This annotation effort aims to produce resources for the task of determining the extent to which the semantic properties ascribed to a given NP in context apply to the members of that class. For example, the statement Cats are mammals describes a property of all cats, where Cats have four legs is true only for most cats. The scheme, which includes the labels ONE, SOME, MOST, ALL and QUANT (for explicitly quantified NPs), is applied to 300 subject-verb-object triples from sentences randomly extracted from Wikipedia. Annotators are shown the sentence and the triple. \u03ba ranges from 0.88 and 0.81 for QUANT and ONE to values between 0.44 and 0.51 for the other classes. Bhatia et al. (2014b) present an annotation scheme for Communicative Functions of Definiteness, intended to cover the many semantic and pragmatic functions conveyed by choices regarding definiteness across languages of the world. The scheme has been applied to 3422 English NPs contained in texts from four genres. Their typology includes two categories relevant to our survey: GENERIC KIND LEVEL applies to utterances predicating over an entire class, like Dinosaurs are extinct.\n\nGENERIC INDIVIDUAL LEVEL is for predications applying to the individual members of a class or kind, such as Cats have fur. Across 1202 annotated NPs for an interannotator agreement study, the two annotators used the GENERIC INDIVIDUAL LEVEL label 45 times and 30 times, respectively, with agreement in 29 cases. Neither used the GENERIC KIND LEVEL. The entire corpus contains just 131 NPs labeled with GENERIC INDIVIDUAL LEVEL and none with GENERIC KIND LEVEL (Bhatia et al., 2014a).\n\nThe question of genericity has also been addressed in cognitive science (Prasada, 2000). Gelman and Tardif (1998) study the usage of generic NPs cross-linguistically for English and Chinese in child-directed speech. They annotate kind-referring NPs as generic. They report agreement as the fraction of items on which the annotators agreed at over 99%, but given that their data set has fewer than 1% generic NPs, this statistic does not allow us to estimate how well annotators agreed.", "filtered_refids": [[], ["b17", "b18", "b28", "b27", "b5", "b3"], ["b28"], ["b15", "b16", "b2"], ["b1"], ["b29"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3875, "num_references": 12}
{"corpusid_sectionid": "5738018-s6", "title": "Annotating genericity: a survey, a scheme, and a corpus", "date": "2015-06-01", "section_title": "ACE-2005: an agreement study", "section": "In this section we investigate some problems with the ACE annotation scheme via a study of annotator agreement. The data was first labeled by two annotators independently, then adjudicated by a senior annotator. To our knowledge, agreement numbers on this task have not been published to date. In order to assess both the quality of the data and the difficulty of the task, we compute inter-annotator agreement as follows. Using the 533 documents from the adjudicated data set that were marked by two annotators in the first step, we compute Cohen's \u03ba (Cohen, 1960) for entity class annotations over the four labels SPC, GEN, USP and NEG.\n\nIntuitions about NP genericity are most reliable for subject position as other argument positions involve additional difficulties (Link, 1995). To get a better sense of the difficulty of annotating subjects compared to that for other argument positions, we compute agreement over mentions whose (manually marked) head is the grammatical subject of some other node in a dependency graph (including any dependency type containing subj). We obtain dependency graphs using the Stanford parser (Klein and Manning, 2002).\n\nAn additional complication in entity mention annotation is determining the mention span. Because spans are not pre-marked in the ACE corpora but identified independently by each annotator, we compute \u03ba only over all exactly-matching entity mention spans for the two annotators. For all mentions, annotators mark about 90% of spans marked by the other annotator. For subject mentions, this number is even higher, at about 95%. The spans of the remaining mentions overlap for the two annotators. We exclude them from this study as we cannot be sure that the two mention spans refer to the same entity.\n\nDiscussion. Table 2 shows the confusion matrices of labels for the all-mentions-case and the subjectsonly case. In both cases, confusion between SPC and GEN is acceptable, but confusion between USP and both SPC and GEN is rather high. For example, in the case of subjects, annotator 1 tags 652 mentions as GEN that annotator 2 marks USP, but the two of them only agree on 597 mentions to be GEN. Although it may be useful to create a separate category for unclear or underspecified cases, the definition of USP is not yet clear-cut and compounded with lack of specificity, which refers to whether the speaker presumably knows the referent's identity or not. Even if the identity of a referent may be 'difficult to locate' (as in Officials reported...). The clause certainly does not make a statement about the kind 'official'; instead, it expresses an existential statement (There are officials who reported...). The definition of SPC states that the reader does not necessarily have to know the identity of the entity, possibly making the distinction hard for annotators.\n\nAnother difficult case are noun modifiers in compounds (e.g. a subway system); these are marked as GEN in the corpus. Using the automatic parses,  we find that 9.5% of all mentions marked GEN in the adjudicated corpus are one-token mentions modifying another noun via an nn dependency relation. Genericity as reference to kinds is a discourse phenomenon and thus defined as an attribute of referring expressions. Because nominal modifiers do not introduce discourse referents, they should not be treated on the genericity annotation layer. The data shows moderate agreement for the first two passes of entity class annotation (\u03ba = 0.53 for all mentions and \u03ba = 0.50 for subject mentions). Note that \u03ba scores are not directly comparable across different annotation projects (see also Section 5), we give the above scores for the sake of completeness. Observed and expected agreement are 0.83 and 0.65 for the all-mentions case and 0.79 and 0.58 for subject mentions. This indicates that the all-mentions case may contain some trivial cases, one of which is the case of nominal modifiers described above.\n\nIn summary, the ACE scheme problematically fails to treat subject NPs differently from NPs in other syntactic positions, and 'fuzzy' points in the guidelines, particularly concerning the USP label, contribute to disagreements between annotators.", "filtered_refids": [["b8"], ["b23", "b20"], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 4181, "num_references": 3}
{"corpusid_sectionid": "10690796-s0", "title": "A Survey of Ellipsis in Chinese", "date": "2015-08-01", "section_title": "Sluicing", "section": "Sluicing (Ross 1969, Merchant 2001) typically elides everything from a clause except an interrogative expression (wh-element), e.g.\n\n(9) They are hiding something, but While there is a preference for the b-questions, in which the verb is repeated, the a-questions are not clearly bad. This situation clouds the picture, since the marginal a-questions look like the sluicing in direct questions that is frequent in those languages that have sluicing. One might, however, assume that what has actually been elided from the a-questions is the auxiliary sh\u00ec 'be'. On such an account, such examples would, strictly speaking, not count as instances of sluicing as it is commonly understood. Further data speak more clearly against the presence of sluicing in Mandarin. Cases of so-called multiple sluicing are bad in Mandarin. Multiple sluicing occurs when the sluiced clause contains two or more wh-remnants. The following example illustrates multiple sluicing in English:\n\n(13) A: Somebody has a crush on somebody?\n\nhas Who crush a on whom B: Who has a crush on whom?\n\nThe sluiced clause contains the two wh-remnants, who and on whom, identifying it as an instance of multiple sluicing.  (11a) and (12a). This confirms that sluicing as it is commonly understood in English and related languages does not exist in Mandarin.\n\nA number of accounts of sluicing-like data in Mandarin have acknowledged that what at times looks like sluicing is in fact a different mechanism, this mechanism being called pseudosluicing (see for instance Wei 2004, andAdams andTamioka 2014). Pseudosluicing involves the auxiliary sh\u00ec -but at times sh\u00ec can be omitted. The analysis of pseudosluicing put forth in the literature (Adams and Tamioka 2014) is that it involves zero anaphora; a subject pronoun has been dropped, e.g. \u2026w\u01d2men b\u00f9 zh\u012bd\u00e0o (t\u0101) sh\u00ec shu\u00ed, lit. 'we not know it be who' -more about zero anaphora below in Section 8.\n\nThe absence of sluicing in Mandarin is consistent with the absence of sluicing in wh-in-situ languages in general (Merchant 2001: 84f.).", "filtered_refids": [["b13", "b18"], [], [], [], [], [null, "b20", "b0"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2044, "num_references": 6}
{"corpusid_sectionid": "10690796-s5", "title": "A Survey of Ellipsis in Chinese", "date": "2015-08-01", "section_title": "Null complement anaphora", "section": "Null complement anaphora (Hankamer andSag 1976, Depiante 2000) is a mechanism that elides a complement clause, to-phrase, or prepositional phrase, e.g.\n\n(36) Jim promised he would help, and promised Bill also would he help\n\nBill also promised he would help.\n\n(37) Sam refuses to help, and refuses Sue also to help\n\nSue also refuses to help.\n\nThe predicates that license null complement anaphora in English (e.g. ask, know, promise, refuse, try) are limited. Similar predicates that one might expect to also license null complement anaphora fail to do so (e.g. imagine, intend, pretend, say, think, etc. These two examples suggest that the similar predicates across the languages allow for the ellipsis of a complement clause or phrase. However, concluding that Mandarin has null complement anaphora in the same way that English does is difficult. The difficulty is due to the fact that Mandarin seems to freely allow the ellipsis of most all complements that can be easily recovered from context. When the elided complement is a verb phrase, one can acknowl-edge VP-ellipsis as discussed above, and when the elided complement can be interpreted as a definite or indefinite noun phrase, an analysis in terms of zero anaphora is available (see the next section). Thus the extent to which null complement anaphora is present in Mandarin is unclear.", "filtered_refids": [[null, "b5"], [], [], [], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1345, "num_references": 3}
{"corpusid_sectionid": "260899983-s3", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Linguistic Knowledge Competency", "section": "Linguistic knowledge includes grammatical, semantic, and pragmatic knowledge (Fromkin et al., 2018). The grammar of a natural language is its set of structural constraints on speakers' or writers' composition of clauses, phrases, and words. The term can also refer to the study of such constraints, a field that includes domains such as phonology, morphology, and syntax, often complemented by phonetics, semantics, and pragmatics. Semantic (Austin, 1975) studies the meaning of words, phrases, and sentences, focusing on general meanings rather than on what an individual speaker may want them to mean. Pragmatics (Austin, 1975) studies language use and how listeners bridge the gap between sentence meaning and the speaker's meaning. It is concerned with the relationship between semantic meaning, the context of use, and the speaker's meaning.  (Warstadt et al., 2020) evaluates what language models (LMs) know about major grammatical phenomena. Linguistic mappings 3 task aims to explore the depth of linguistic knowledge in enormous language models trained on word prediction. It aims to discover whether such knowledge is structured so as to support the use of grammatical abstractions, both morphological (past tense formation and pluralization) and syntactic (question formation, negation, and pronominalization). The minute mysteries qa 4 is a reading comprehension task focusing on short crime and mystery stories where the goal is to identify the perpetrator and to explain the reasoning behind the deduction and the clues that support it. The metaphor boolean 5 task presents a model with a metaphoric sentence and asks it to identify whether a second sentence is the correct interpretation of the first. The last three are selected from BIG-Bench (Srivastava et al., 2022), containing diverse task topics including linguistics.", "filtered_refids": [[null, "b138", "b40", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1840, "num_references": 4}
{"corpusid_sectionid": "260899983-s4", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "World Knowledge Competency", "section": "World knowledge is non-linguistic information that helps a reader or listener interpret the meanings of words and sentences (Ovchinnikova, 2012). It is also referred to as extra-linguistic knowledge. In this paper, we categorize world knowledge into general knowledge and domain knowledge. The general knowledge includes commonsense knowledge (Davis, 2014) and prevalent knowledge. The commonsense knowledge consists of world facts, such as \"Lemons are sour\", or \"Cows say moo\", that most humans are expected to know. The prevalent knowledge exists at a particular time or place. For example, \"Chinese people are used to drinking boiled water.\" is only known by a part of human beings; \"There were eight planets in the solar system\" is prevalent knowledge until it is overthrown. The domain knowledge (Alexander, 1992) is of a specific, specialized discipline or field, in contrast to general or domain-independent knowledge. People who have domain knowledge, are often considered specialists or experts in the field.\n\nThe bottom group of Table 1 shows some task examples that are used for testing world knowledge. For example, the LexGLUE (Chalkidis et al., 2022) tests whether LLMs perform well in the legal domain; WikiFact  is a fact completion scenario that tests language models' factual knowledge based on Wikipedia. The input will be a partial sentence such as \"The capital of France is \", and the output will be the continuation of the sentence such as \"Paris\"; TruthfulQA (Lin et al., 2022b) comprises questions spanning numerous categories including economics, science, and law. The questions are strategically chosen so humans may also incorrectly answer them based on misconceptions and biases; language models should ideally return accurate and truthful responses; HellaSwag (Zellers et al., 2019) tests commonsense inference and was created through adversarial filtering to synthesize wrong answers. The World knowledge competency, along with linguistic knowledge, serves as the foundation for solving different NLP tasks and is one of the core competencies of LLMs.", "filtered_refids": [["b94", "b26"], ["b15", "b157", "b71"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2081, "num_references": 5}
{"corpusid_sectionid": "260899983-s5", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "date": "2023-08-15", "section_title": "Reasoning", "section": "Reasoning competency is a crucial skill for LLMs to solve complex problems. What's more, from the perspective of intelligent agents, reasoning ability is also one of the core capabilities towards achieving  AGI (Bubeck et al., 2023;Qiao et al., 2022). However, there remains no consensus whether LLMs can really reason, or just simply produce a larger context that increases the likelihood of correctly predicting the missing tokens (Mialon et al., 2023). Although \"reasoning\" itself may currently be an excuse of language, we can still objectively verify the reasoning performance of LLMs through various reasoning competencies. Previous methods mainly focus on the division of reasoning tasks.  divides existing evaluation tasks into three major categories, namely knowledge reasoning, symbolic reasoning, and mathematical reasoning, based on the type of logic and evidence involved in the reasoning process. Zhao et al. (2023) divides reasoning tasks into deductive reasoning and defeasible reasoning according to the reasoning form. In this section, we decompose the reasoning competency into 6 sub-parts from the perspective of model competency, providing a comprehensive overview of existing research efforts and suggesting potential future directions. And Table 2 presents some datasets for evaluating LLM's reasoning competency using this categorization approach.", "filtered_refids": [["b11", null, "b105", "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1371, "num_references": 4}
{"corpusid_sectionid": "2044324-s2", "title": "A Survey of Current Datasets for Vision and Language Research", "date": "2015-06-23", "section_title": "Language Quality", "section": "We define the following criteria for evaluating the captions or instructions of the datasets:\n\n\u2022 Vocabulary Size (#vocab), the number of unique vocabulary words.\n\n2 http://visionandlanguage.net \u2022 Syntactic Complexity (Frazier, Yngve) measures the amount of embedding/branching in a sentence's syntax. We report mean Yngve (Yngve, 1960) and Frazier measurements (Frazier, 1985); each provides a different counting on the number of nodes in the phrase markers of syntactic trees.\n\n\u2022 Part of Speech Distribution measures the distribution of nouns, verbs, adjectives, and other parts of speech.\n\n\u2022 Abstract:Concrete Ratio (#Conc, #Abs, %Abs) indicates the range of visual and non-visual concepts the dataset covers. Abstract terms are ideas or concepts, such as 'love' or 'think' and concrete terms are all the objects or events that are mainly available to the senses. For this purpose, we use a list of most common abstract terms in English (Vanderwende et al., 2015), and define concrete terms as all other words except for a small set of function words.\n\n\u2022 Average Sentence Length (Sent Len.) shows how rich and descriptive the sentences are.\n\n\u2022 Perplexity provides a measure of data skew by measuring how expected sentences are from one corpus according to a model trained on another corpus. We analyze perplexity (Ppl) for each dataset against a 5-gram language model learned on a generic 30B words English dataset. We further analyze pair-wise perplexity of datasets against each other in Section 4.", "filtered_refids": [[], [], ["b10", "b36"], [], ["b32"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1502, "num_references": 3}
{"corpusid_sectionid": "2044324-s7", "title": "A Survey of Current Datasets for Vision and Language Research", "date": "2015-06-23", "section_title": "Crowd-sourced Captions", "section": "\u2022 UIUC Pascal Dataset (Farhadi et al., 2010) is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image.\n\n\u2022 Flickr 30K Images (Young et al., 2014) extends previous Flickr datasets , and includes 158,915 crowd-sourced captions that describe 31,783 images of people involved in everyday activities and events.\n\n\u2022 Microsoft COCO Dataset (MS COCO) (Lin et al., 2014) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled using per-instance segmentations. In total, this dataset contains photos of 91 basic object types with 2.5 million labeled instances in 328k images, each paired with 5 captions. This dataset gave rise to the CVPR 2015 image captioning challenge and is continuing to be a benchmark for comparing various aspects of vision and language research. \u2022 Abstract Scenes Dataset (Clipart) (Zitnick et al., 2013) was created with the goal of representing real-world scenes with clipart to study scene semantics isolated from object recognition and segmentation issues in image processing. This removes the burden of low-level vision tasks. This dataset contains 10,020 images of children playing outdoors associated with total 60,396 descriptions.", "filtered_refids": [["b8"], ["b37"], ["b40", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1293, "num_references": 4}
{"corpusid_sectionid": "2044324-s10", "title": "A Survey of Current Datasets for Vision and Language Research", "date": "2015-06-23", "section_title": "Beyond Visual Description", "section": "Recent work has demonstrated that n-gram language modeling paired with scene-level understanding of an image trained on large enough datasets can result in reasonable automatically generated captions (Fang et al., 2014;Donahue et al., 2014). Some works have proposed to step beyond description generation, towards deeper AI tasks such as question answering (Ren et al., 2015;Malinowski and Fritz, 2014). We present two of these attempts below:\n\n\u2022 Visual Madlibs Dataset (VML) (Yu et al., 2015) is a subset of 10,783 images from the MS COCO dataset which aims to go beyond describing which objects are in the image. For a given image, three Amazon Turkers were prompted to complete one of 12 fill-in-the-blank template questions, such as 'when I look at this picture, I feel -', selected automatically based on the image content. This dataset contains a total of 360,001 MadLib question and answers.\n\n\u2022 Visual Question Answering (VQA) Dataset (Antol et al., 2015) is created for the task of openended VQA, where a system can be presented with an image and a free-form natural-language question (e.g., 'how many people are in the photo?'), and should be able to answer the question. This dataset contains both real images and abstract scenes, paired with questions and answers. Real images include 123,285 images from MS COCO dataset, and 10,000 clip-art abstract scenes, made up from 20 'paperdoll' human models with adjustable limbs and over 100 objects and 31 animals. Amazon Turkers were prompted to create 'interesting' questions, resulting in 215,150 questions and 430,920 answers.\n\n\u2022 Toronto COCO-QA Dataset (CQA) (Ren et al., 2015) is also a visual question answering dataset, where the questions are automatically generated from image captions of MS COCO dataset. This dataset has a total of 123,287 images with 117,684 questions with one-word answer about objects, numbers, colors, or locations.", "filtered_refids": [["b6", "b7", "b18", "b27"], ["b39"], ["b0"], ["b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1903, "num_references": 7}
{"corpusid_sectionid": "258740687-s2", "title": "A Survey on Zero Pronoun Translation", "date": "2023-05-17", "section_title": "Linguistic Phenomenon", "section": "Definition of Zero Pronoun Cohesion is a significant property of discourse, and it occurs whenever \"the interpretation of some element in the discourse is dependent on that of another\" (Halliday and Hasan, 1976). As one of cohesive devices, anaphora is the use of an expression whose inter-pretation depends specifically upon antecedent expression while zero anaphora is a more complex scenario in pro-drop languages. A ZP is a gap in a sentence, which refers to an entity that supplies the necessary information for interpreting the gap (Zhao and Ng, 2007). ZPs can be categorized into anaphoric and non-anaphoric ZP according to whether it refers to an antecedent or not. In pro-drop languages such as Chinese and Japanese, ZPs occur much more frequently compared to nonpro-drop languages such as English. The ZP phenomenon can be considered one of the most difficult problems in natural language processing (Peral and Ferr\u00e1ndez, 2003).", "filtered_refids": [[null, "b10", "b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 938, "num_references": 3}
{"corpusid_sectionid": "258740687-s7", "title": "A Survey on Zero Pronoun Translation", "date": "2023-05-17", "section_title": "Zero Pronoun Resolution", "section": "The task contains three steps: ZP detection, anaphoricity determination and reference linking. Earlier works investigated rich features using traditional ML models (Zhao and Ng, 2007;Kong and Zhou, 2010;Chen andNg, 2013, 2015). Recent studies exploited neural models to achieve the better performance (Chen and Ng, 2016;Yin et al., 2018;Song et al., 2020). The CoNLL2011 and CoNLL2012 2 are commonlyused benchmarks on modeling unrestricted coreference. The corpus contains 144K coreference instances, but dropped subjects only occupy 15%.\n\nZero Pronoun Recovery Given a source sentence, this aims to insert omitted pronouns in proper positions without changing the original meaning (Yang and Xue, 2010;Yang et al., 2015Yang et al., , 2019a. It is different from ZP resolution, which identifies the antecedent of a referential pronoun (Mitkov, 2014). Previous studies regarded ZP recovery as a classification or sequence labelling problem, which only achieve 40\u223c60% F1 scores on closed datasets Song et al., 2020), indicating the difficulty of generating ZPs. It is worth noting that ZP recovery models can work for ZPT task in a pipeline manner: input sentences are labeled with ZPs using an external recovery system and then fed into a standard MT model (Chung and Gildea, 2010;Wang et al., 2016a).\n\nZero Pronoun Translation When pronouns are omitted in a source sentence, ZPT aims to generate ZPs in its target translation. Early studies have investigate a number of works for SMT models (Chung and Gildea, 2010;Le Nagard and Koehn, 2010;Taira et al., 2012;Xiang et al., 2013;Wang et al., 2016a). Recent years have seen a surge of interest in NMT Wang et al., 2018a), since the problem still exists in advanced NMT systems. ZPT is also related to pronoun translation, which aims to correctly translate explicit pronoun in terms of feminine and masculine. The DiscoMT 3 is a commonly-cited benchmark on pronoun translation, however, there was no standard ZPT benchmarks up until now.", "filtered_refids": [["b51", null, "b19", "b55"], ["b47", "b49", "b48", "b19", null, "b38", "b5"], ["b34", "b46", null, "b23", "b38"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1984, "num_references": 16}
{"corpusid_sectionid": "258740687-s10", "title": "A Survey on Zero Pronoun Translation", "date": "2023-05-17", "section_title": "Overview", "section": "Modeling ZPs has so far not been extensively explored in prior research, largely due to the lack of publicly available data sets. Existing works mostly focused on human-annotated, small-scale and single-domain corpora such as OntoNotes (Pradhan et al., 2012;Aloraini and Poesio, 2020) and Treebanks (Yang and Xue, 2010;Chung and Gildea, 2010). We summarize representative corpora as:\n\n\u2022 OntoNotes. 5 This is annotated with structural information (e.g. syntax and predicate argument structure) and shallow semantics (e.g. word sense linked to an ontology and coreference). It comprises various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in English, Chinese, and Arabic languages. ZP sentences are extracted for ZP resolution task (Chen and Ng, 2013, 2016). \u2022 TVSub. 6 This extracts Chinese-English subtitles from television episodes. Its source-side sentences are automatically annotated with ZPs by a heuristic algorithm (Wang et al., 2016a), which was generally used to study dialogue translation and zero anaphora phenomenon (Wang et al., 2018a;Tan et al., 2021). \u2022 CTB. 7 This is a part-of-speech tagged and fully bracketed Chinese language corpus. The text are extracted from various domains including newswire, government documents, magazine articles, various broadcast news and broadcast conversation programs, web newsgroups and weblogs. Instances with empty category are extracted for ZP recovery task (Yang and Xue, 2010;Chung and Gildea, 2010). \u2022 BaiduKnows. The source-side sentences are collected from the Baidu Knows website, 8 which were annotated with ZP labels with boundary tags. It is widely-used the task of ZP recovery Song et al., 2020).  (Yang and Xue, 2010) ZH Human News 10.6K \u2717 \u2713 \u2717 KTB (Chung and Gildea, 2010) KO Human News 5.0K \u2717 \u2713 \u2717 BaiduKnows  ZH Human Baidu Knows 5.0K \u2717 \u2713 \u2717 TVsub (Wang et al., 2018a) ZH, EN Auto Movie Subtitles 2.2M \u2717 \u2717 \u2713 ZAC (Pereira, 2009) PT Human Mixed Sources 0.6K \u2713 \u2717 \u2717 Nagoya (Zhan and Nakaiwa, 2015) JA Auto Scientific Paper 1.2K \u2713 \u2717 \u2717 SKKU (Park et al., 2015) KO Human Dialogue 1.1K \u2713 \u2717 \u2717 UPENN (Prasad, 2000) HI Human News 2.2K \u2713 \u2717 \u2717 LATL (Russo et al., 2012) IT, ES Human Europarl 2.0K \u2713 \u2717 \u2713 UCFV (Bacolini, 2017) HE Human Dialogue 0.1K \u2713 \u2717 \u2717 Table 1: A summary of existing datasets regarding ZP. We classify them according to language (Lang.), annotation type (Anno.) and text domain. We also report the number of sentences (Size). \"Reso.\", \"Reco.\" and \"Trans.\" indicate whether a dataset can be used for specific ZP tasks. The symbol \u2713 or \u2717 means \"Yes\" or \"No\".", "filtered_refids": [["b49", "b12", null], ["b11", "b34", "b9", "b13", "b25", "b49", null, "b19", "b53", "b38", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2586, "num_references": 14}
{"corpusid_sectionid": "258557362-s1", "title": "Large Language Models Meet NL2Code: A Survey", "date": "2022-12-19", "section_title": "Large Language Models for NL2Code", "section": "Given a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.\n\nEarly works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.\n\nThese models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.", "filtered_refids": [[null, "b8", "b25"], [null, "b27"], ["b12", null, "b6", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4198, "num_references": 9}
{"corpusid_sectionid": "258557362-s3", "title": "Large Language Models Meet NL2Code: A Survey", "date": "2022-12-19", "section_title": "Large Model Size", "section": "As shown in Figure 2 and Table 2, recent LLMs for NL2Code exhibit larger sizes and superior performance. This is consistent with prior findings that an increased number of model parameters can enhance model capabilities (Radford et al., 2019;Thoppilan et al., 2022;Chowdhery et al., 2022). We further demonstrate the correlation between model size and performance in Figure 3a, which compares the pass@1 results of 10 representative models on the HumanEval benchmark. It is clear that larger models generally result in better performance. Furthermore, we also find that current models, regardless of size, still have the potential for improvement through further increases in size. Additional results on the HumanEval and MBPP benchmarks can be found in Appendix Figure 7, which also support this conclusion.\n\nAdditionally, we conduct an experiment on the HumanEval benchmark to examine the syntax error rates of the code generated by different models of varying sizes. Specifically, we make the models predict 10 code samples for each programming problem, and then calculate the percentage of code samples that have syntax errors. As shown in Figure 3b, results indicate that larger models tend to have lower syntax error rates. It is noteworthy that the largest version of the CodeGen-Mono model exhibits a remarkably low rate of syntax errors, i.e., 6%. However, as evidenced by Figure 3a and Table 2, the CodeGen-Mono model with 16 billion parameters still has unsatisfactory performance in terms of pass@k , e.g., pass@1 to be 29%. This highlights the fact that the current limitation for large pre-trained models is the generation of semantically correct code.", "filtered_refids": [["b24", "b14", null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1666, "num_references": 3}
{"corpusid_sectionid": "258557362-s4", "title": "Large Language Models Meet NL2Code: A Survey", "date": "2022-12-19", "section_title": "Large and Premium Data", "section": "As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. This highlights the importance of selecting and pre-processing high-quality data. In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.\n\nEarly models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet (Husain et al., 2019), CoST (Zhu et al., 2022b), and XL-CoST (Zhu et al., 2022a). However, manual annotation is labour-intensive and time-consuming. There are also models like GPT-3 (Brown et al., 2020), GPT-Neo (Black et al., 2021), and GPT-J (Wang and Komatsuzaki, 2021)  In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities. One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.", "filtered_refids": [[], ["b43", "b27", null, "b42", "b38", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1629, "num_references": 6}
{"corpusid_sectionid": "254877175-s1", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Mathematical Reasoning Tasks", "section": "In this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?\n\nRationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5\n\nAnswer: (B) 6.5 Figure 2: An example of geometry problems.\n\nMath Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as \"Which kicker kicked the most field goals?\" over the content of paragraphs.\n\nIn this section, we briefly introduce different tasks for mathematical reasoning. A detailed summary and discussion of commonly used datasets can be found in Table 7 and Appendix A. Math Word Problem Solving. Developing algorithms to automatically solve math word problems (MWPs) has been of interest to NLP researchers for decades (Feigenbaum et al., 1963;Bobrow, 1964). An example of a MWP is shown in Table 1. A question involves four basic arithmetic operations with single or multiple operation steps. The challenge posed by MWPs lies in the need for language com-Question: Bod has 2 apples and David has 5 apples. How many apples do they have in total?\n\nRationale: x = 2 + 5 Solution: 7 prehension, semantic parsing, and the application of multiple mathematical reasoning skills. Theorem Proving. Automating theorem proving is a long-standing challenge in AI (Newell et al., 1957;Feigenbaum et al., 1963). The problem is to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof ). Theorem proving tests various skills, such as choosing effective multi-step strategies, using background knowledge, and performing symbolic manipulations. Geometry Problem Solving. Automated geometry problem solving (GPS) is also a long-standing mathematical reasoning task (Gelernter et al., 1960;Wen-Tsun, 1986). As shown in Figure 2, a geometry problem consists of a textual description and a diagram. The multimodal inputs describe the entities, attributes, and relationships of geometric elements, and the goal is to find the numeric solution to an unknown variable. Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5\n\nAnswer: (B) 6.5 Figure 2: An example of geometry problems.\n\nMath Question Answering. There is a wide range of question answering (QA) benchmarks that center around mathematical reasoning, which we refer to as math question answering (MathQA). For example, DROP (Dua et al., 2019) is a MathQA dataset that requires discrete reasoning to answer questions such as \"Which kicker kicked the most field goals?\" over the content of paragraphs.", "filtered_refids": [[null], [null, "b6"], [], [], [null], [null, "b6"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 36, "num_chars": 4182, "num_references": 6}
{"corpusid_sectionid": "254877175-s3", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Seq2Seq-based Networks for Math", "section": "Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).\n\nSequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).", "filtered_refids": [["b21", null, "b0", "b27"], ["b21", null, "b0", "b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 2088, "num_references": 8}
{"corpusid_sectionid": "254877175-s4", "title": "A Survey of Deep Learning for Mathematical Reasoning", "date": "2022-12-20", "section_title": "Graph-based Networks for Math", "section": "Seq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).\n\nSeq2Seq approaches show their advantages of generating mathematical expressions without relying on hand-crafted features. It is noteworthy that mathematical expressions can be represented as tree-based structures, such as abstract syntax trees (ASTs) and graph-based structures, which capture the structural information in the expressions. However, Seq2Seq methods do not explicitly this important information. To address this limitation, graph-based neural networks have been developed to explicitly model the structure within expressions. Sequence-to-tree (Seq2Tree) models explicitly model the tree structure when encoding the output sequences (Xie and Sun, 2019;Wu et al., 2020;Zaporojets et al., 2021;Qin et al., 2021). For example, Liu et al. (2019a) devise a Seq2Tree model to better use information from an equation's AST. Seq2DAG (Cao et al., 2021), instead, applies a sequence-to-graph (Seq2Graph) framework when generating the equations since the graph decoder is able to extract complex relationships among multiple variables. The graph-based information can also be embedded when encoding the input mathematical sequences (Zhang et al., 2020b;Shen and Jin, 2020;Li et al., 2020b;Wu et al., 2021a).", "filtered_refids": [["b10", "b26", "b9", "b29", null, "b19"], ["b10", "b26", "b9", "b29", null, "b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2422, "num_references": 12}
{"corpusid_sectionid": "237532535-s1", "title": "A Survey of Online Hate Speech through the Causal Lens", "date": "2021-09-16", "section_title": "Sociological Causal Impact", "section": "Digital profanity is, first and foremost, a sociological phenomenon that affects many aspects of the virtual world. There are some pronounced directions which require thorough examination, such as the consequences of such actions on the affected communities as well as their targets, or the impact of banning policies aiming at fighting against it. Moreover, there are several other underlying effects that could get in the spotlight, including the ramifications of interventions or the reasons that drive toxic actors. As a result, it is of paramount importance to examine hate speech in a holistic way and concretely quantify the drivers of these outcomes. However, it is impossible to achieve this task without considering causality, because non-causal inferences can never be conclusive. Surprisingly, despite the broad interest of the research community in this topic, very little work has been done on attempting causal links, even on the most prominent tasks related to online hate speech (OHS).\n\nIn the present survey, we classify fundamental sociological outcomes related to OHS and outline the most distinguished body of literature. The classification results into three major pillars, with respect to the following:\n\n\u2022 Digital misbehaviours versus the physical world: we summarise studies concerning the propagation of online hate speech to real life Schwarz, 2018, 2020) as well as the influence of offline events to the dissemi-nation of the issue online (Olteanu et al., 2018;Thomas et al., 2021) 1 .\n\n\u2022 Harmful content versus the individuals: we outline research concerning the impact of toxic behaviours on the targets or passive readers (Saha et al., 2019a) as well as the by-products of web characteristics (such as anonymity) on hate speech producers (von Essen and Jansson, 2020).\n\n\u2022 Effect of interventions: finally, we review works that revolve around quantifying the effect of combating strategies which various platforms adopt. Existing research has focused on limiting policies, censoring and counter-speaking (\u00c1lvarez-Benjumea and Winter, 2018), social sanctioning (Munger, 2017), quarantining (Chandrasekharan et al., 2020) and banning hateful communities (Chandrasekharan et al., 2017;Thomas et al., 2021).\n\nIn all the of the cases, we can consider the role of hate speech both from the position of a phenomenon which is the result of (potentially) multiple causes, but also from the position of the causal root and study its effects. Therefore, in the following sections we review both directions.", "filtered_refids": [[], [], [null, "b22", "b36"], ["b29"], ["b21", "b1", null, "b36"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2526, "num_references": 8}
{"corpusid_sectionid": "237532535-s2", "title": "A Survey of Online Hate Speech through the Causal Lens", "date": "2021-09-16", "section_title": "Digital and Physical World", "section": "The first and most apparent pillar concerns the interrelation of online hate speech with the physical world, in a range of simple dissemination to absolute influence. Towards this direction, Schwarz (2018, 2020) have conducted research on the causal effect of social media on the propagation of hate sentiments offline, whether antirefugee (M\u00fcller and Schwarz, 2018) or more specifically anti-Muslim (M\u00fcller and Schwarz, 2020).\n\nIn (M\u00fcller and Schwarz, 2018), the authors provide evidence that there is significant association between negative content against refugees existing on Facebook and offline hate crimes in Germany on a municipal level, while controlling for multiple potentially confounding factors such as German municipalities' characteristics and overall social media usage. To reach to this conclusion they combine a variety of data sources; online antirefugee sentiment is represented by content from a widespread Facebook page of a German right-wing party, which hosts plenty of far-right content; controlling for the network's popularity in Germany, the authors measure Facebook outages and internet disruptions; finally, to further measure user activity on the network and create controls based on a neutral subject, they explore another broadly popular page of a famous commercial product. The causal framework they implement is a fixed-effects regression model (inspired by Bartik (1991)), which considers the aforementioned panel data combined with a range of controls. They discover that the effect is stronger in areas with higher Facebook usage and demonstrate a robustly strong connection between the activity of the right-wing group and severe hate crimes. Similarly, in (M\u00fcller and Schwarz, 2020), they study the causal impact of Islamophobic social media content on registered crimes and overall negative sentiment against Muslims, and whether former US president Donald Trump's Twitter campaign has contributed to the propagation of Islamophobia. To ensure validity and robustness of their findings, they fuse a number of different data sources and employ a difference-in-differences approach. The data originate mainly from Twitter for the social media information, a survey by FBI to discover hate crimes, data from mass media, demographic information about US counties etc. Their findings provide evidence to associate a 38% larger increase in hate crimes, between 2010 and 2017, with higher exposure to social media. Moreover, consistently with previous research, they also provide evidence which shows a connection amid the start of Trump's presidential campaign with an increase of anti-Muslim sentiments in USA. Both projects illustrate there is strong evidence linking OHS with offline occurrences of hate-related crimes, with the former having a causal effect on the latter. 2 Looking at the opposite direction, online hate speech is frequently affected by events taking place in the offline world. For example, following the September 11 th , 2001 attack to the twin towers of 2 It goes without saying that results from papers that employ causality need to be interpreted with great care. As M\u00fcller and Schwarz (2018) emphasise, for instance, their findings do not indicate that social media can cause crimes against minorities, but rather \"social media can act as a propagating mechanism [...]\" so that \"shifts in exposure to anti-refugee sentiment on social media can increase the number of anti-refugee attacks\". New York City, there seems to be an increase in Islamic terrorist attacks as much as an increase in Islamophobia. It is very well expected that this will affect the online world and the way Muslims are perceived, which seems to actually be the case according to Olteanu et al. (2018). In (Olteanu et al., 2018) they analyse the influence of offline events on online hate speech. More specifically, they study the impact of several attacks related to the Islamic State -both Islamic terrorist attacks and Islamophobic ones -in terms of online hate speech and counter-hate speech. Their findings indicate that terrorist attacks show an increase in OHS, especially towards Muslims and Arabs.\n\nTo calculate the causal effect, they construct time series from Reddit and Twitter -representing the number of posts and unique users involved with the event -and then synthesize counterfactual time series for the same period of time, such as they would be produced had the attacks not happened. The counterfactual data are created by composing timelines of the same event, while adjusting for a temporal shift prior to the event, so that the time series will reflect a similar period of time but in different time windows. The produced timelines, put together with other external data sources, are then fit into a state space model (Brodersen et al., 2015) using maximum likelihood estimation, to predict the synthesized control time series. Comparing the treatment and control series, they calculate relative effects for a number of manually curated terms, which are also annotated across four hate speech dimensions (stance, targets, severity, and framing).", "filtered_refids": [[null, "b19", "b20"], [null, "b19", "b20", "b22"], ["b6"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 5106, "num_references": 8}
{"corpusid_sectionid": "237532535-s3", "title": "A Survey of Online Hate Speech through the Causal Lens", "date": "2021-09-16", "section_title": "Actors and Targets", "section": "Beyond broadly looking at the overall impact of OHS, the next step would be to concentrate on the individuals and speculate how OHS affects and is affected by the participating members, whether these are at the producing or receiving end of such content. For example, in the aforementioned works, Olteanu et al. (2018) and M\u00fcller and Schwarz (2020) have made some general remarks regarding OHS, but in order to effectively focus on the task they narrow the type of hate speech to be racism and, more specifically, Islamophobism. Emphasis, however, is given on understanding the dynamics of diffusion and not on studying the impact on individuals who support Islam.\n\nOn the these grounds, Saha et al. (2019a) study the effect of hate prevalence on the stress levels of US college students, within Reddit college communities. To measure the levels of toxicity, they employ hate lexicons, with the help of which they compute the College Hate Index (CHX), as fractions of hateful keywords in each community compared to other subreddits banned for violating the hate-limiting policies of Reddit. Similarly, they quantify the exposure of users to hateful content based on the threads they have participated and to account for their stress levels they use a binary classifier based on existing models. They examine numerous observable confounders -such as the subreddit and user activity -and apply propensity score matching to calculate the causal effect whilst controlling for the covariates. Their results demonstrate an increase in stress expression caused by exposure to hateful speech.\n\nAdditionally, inherent characteristics of online environments, such as anonymity, ease of access, and size of audience (Brown, 2018) are highly likely to affect the behaviour of online social networks' users and sometimes make it easier for them to misbehave. von Essen and Jansson (2020) discuss the outcomes of the by-products of online world characteristics -in this case anonymityon hate speech actors. In particular, they compare the degree of hatefulness before and after the identities of a large set of users from Flashback, an anonymous Swedish discussion platform similar to Reddit, have been publicly exposed. Their hypothesis suggests that once running the risk of exposure, users decrease the volume of hateful content they post. To detect hate, they implement a machine learning model and make predictions on the data, which will afterwards be used with a difference-in-differences approach to make causal claims. According to their estimates, the reduction of anonymity, as in risk of exposure, leads to a decline in general hate and overall activity, and even more on xenophobic content. Surprisingly, levels of misogyny increase. These empirical findings mostly support the author's original hypothesis.", "filtered_refids": [["b22", "b20"], ["b29"], ["b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2806, "num_references": 4}
{"corpusid_sectionid": "254564474-s4", "title": "A Survey on Natural Language Processing for Programming", "date": "2022-12-12", "section_title": "Classification Tasks", "section": "The classification task detects whether given programs have specific characteristics, e.g., being cloned (clone detection), or being vulnerable (vulnerability identification). They are essential in protecting software from the effects of adhoc reuse (Svajlenko et al., 2014) and cyber attacks (Zhou et al., 2019). The granularity of the input ranges from a coarse-grained software repository (Hovsepyan et al., 2012) to a fine-grained function (Russell et al., 2018;Zhou et al., 2019). Despite the fact that NL does not explicitly occur in either input or output, we include tasks of such form for two reasons. First, PL has been demonstrated to contain abundant statistical properties similar to NL (Mou et al., 2016). Second, most of the ways that PL is processed are derived from NLP, like machine translation techniques (Tufano et al., 2019) in the transcription task ( \u00a7 2.5).", "filtered_refids": [["b63", "b56", "b60", "b44", "b20", "b70"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 881, "num_references": 6}
{"corpusid_sectionid": "254564474-s5", "title": "A Survey on Natural Language Processing for Programming", "date": "2022-12-12", "section_title": "Synthesis Tasks", "section": "The synthesis task generates a program given a context (which can be NL, PL, or their mixture), thus can accelerate the development process. It can be further divided into program synthesis and code completion by the formal completeness of the output. The output of program synthesis is a relatively independent unit, such as a function and a class, while the output of code completion is less restricted, ranging from tokens to code snippets.\n\nProgram synthesis is also called code generation. It is the systematic derivation of a program from a given specification (Manna and Waldinger, 1980). Conventional deductive approaches (Manna and Waldinger, 1980;Polozov and Gulwani, 2015) take logical specifications, which are logically complete but hard to write. Inductive approaches (Lieberman, 2001) list inputoutput examples as specifications, which are more accessible but incomplete. In contrast, an NL specification is sufficient to describe the logic of a program. Meanwhile, it is compatible with inputoutput examples by including them in a docstring. Therefore, it can take advantage of both the deductive and inductive approaches.\n\nCode completion is also called code suggestion in early research (Tu et al., 2014;Hindle et al., 2016). It suggests the next program token given a context and has been widely applied to IDEs . The application scenario includes the completion of method calls, keywords, variables, and arguments. With the bloom of the pre-trained models, the scenario has been extended to punctuations, statements, and even code snippets , further blurring the line between program synthesis and code completion.", "filtered_refids": [[], ["b42", "b33", "b50"], ["b62", "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 1634, "num_references": 5}
{"corpusid_sectionid": "254564474-s6", "title": "A Survey on Natural Language Processing for Programming", "date": "2022-12-12", "section_title": "Transcription Tasks", "section": "The transcription task converts a given program to meet a specific requirement. Concretely, program translation aims to convert between highlevel PL (Roziere et al., 2020;Zhu et al., 2022), e.g., C# and Java. It can accelerate the update of projects written by deprecated PL, and the migration of algorithms implemented by various PLs. Code refinement aims to convert a buggy program into correct one (Wang et al., 2021). It is closely related to vulnerability identification but is required to fix the detected bugs simultaneously. The transcription task differs from the synthesis task in two aspects. First, its input program is formally complete (input program is None or a function header in program synthesis, a partial code snippet in code completion). Second, its output can be strictly aligned with the input in both the format and the content.", "filtered_refids": [["b65", "b55", "b71"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 853, "num_references": 3}
{"corpusid_sectionid": "261100760-s1", "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4", "date": "2023-08-24", "section_title": "Multilingualism", "section": "Chinese Linguistic Test. SuperCLUE benchmarks (Xu et al., 2023) compared ChatGPT-like foundation models in regard to their basic language ability, professional ability, and Chinese-featured ability. By 18th May 2023, they reported that GPT-4 and ChatGPT achieved the second (76.67) and third best (66.18) results after humans (96.50), exceeding other LLMs. GPT-4 and ChatGPT had better basic language ability than the other two metrics in Chinese. Both models achieved human-like accuracy on role-playing, chit-chatting, and coding. However, their ability to understand Chinese poetry, literature, classical Chinese, and couplets was far inferior to that of humans. Huang et al. (2023) also ranked GPT-4 and ChatGPT as top-2 on a multi-discipline (52 subjects) Chinese evaluation.\n\nMultilingual NLP Tasks were examined by Lai et al. (2023), e.g., multilingual part-of-speech (PoS) tagging, NER, relation classification, NLI, QA, commonsense reasoning, and summarization. The researchers analyzed 36 languages and discovered that task-specific fine-tuned models outperformed ChatGPT in the majority of examined tasks, except PoS tagging. ChatGPT exhibited superior performance in English tasks compared to tasks in other languages; for low-and extremely low-resource languages, ChatGPT performed significantly worse than baselines. Noticeably, despite the use of non-English languages in the target tasks, ChatGPT improved its performance with English prompts. Wei et al. (2023) found that direct usage of Chat-GPT could not yield satisfying results in Chinese information extraction. Wang et al. (2023b) tested ChatGPT and GPT-4 on English-to-Chinese and English-to-German summarization, showing that although ChatGPT and GPT-4 exceeded other LLM baselines on a zero-shot setup, they fell behind a fine-tuned mBART-50 (Tang et al., 2021) on most of the examined datasets. Bang et al. (2023) argued that ChatGPT generally yielded weak performance on low-resource languages in language understanding and generation , while achieving higher profi-ciency in comprehending non-Latin scripts compared to its proficiency in generating them.", "filtered_refids": [[null], [null, "b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2133, "num_references": 3}
{"corpusid_sectionid": "261100760-s2", "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4", "date": "2023-08-24", "section_title": "Reasoning", "section": "Logical Reasoning was tested by Bang et al. (2023). They found 56 out of 60 answers correct (with appropriate prompts) for deductive reasoning, i.e. applying general rules to specific situations or cases. This was stronger than other types of reasoning. 26 out of 30 were scored for abductive reasoning, i.e. forming plausible explanations or hypotheses, based on limited evidence or incomplete information. 33 out of 60 were scored for inductive reasoning, i.e., drawing generalized conclusions from examples or specific observations. Commonsense Reasoning. Bang et al. (2023) tested ChatGPT via three commonsense datasets, showing that 80 out of 90 of ChatGPT's predictions were correct. ChatGPT was able to give good explanations of the reasoning steps to support its answer. However, Qin et al. (2023); Laskar et al. (2023) showed that the commonsense reasoning accuracy of ChatGPT was lower than fine-tuned baselines by a large margin. Davis (2023) found significant flaws in common benchmarks for common sense, including the CommonsenseQA dataset used by Bang et al. (2023), which he explicitly addressed. Davis (2023) listed several examples of commonsense and particularly physical reasoning failures that had been found shortly after the release of ChatGPT, and pointed to others. However, there does not exist a thorough assessment of the GPT models' commonsense reasoning ability. More generally, Davis (2023) pointed out that \"many important aspects of commonsense reasoning and commonsense knowledge are not tested in any existing benchmark\". Bubeck et al. (2023) probed a small number of their own real-world physical reasoning tasks with GPT-4, finding that it had good knowledge. They concluded that the model was able to learn an understanding of the real-world environment through the medium of text.\n\nCausal Reasoning. Bang et al. (2023) found that 24 out of 30 causes or effects could be correctly identified.  systematically evaluated event causality identification, causal discovery, and causal explanation generation. Compared to SOTA models, ChatGPT and GPT-4 yielded lower scores in causality identification. They outperformed baseline models on the causal discovery, although the compared models, e.g., BERT-(Devlin et al., 2019) and RoBERTa-base (Liu et al., 2019) were relatively weak. The generation of causal explanations yielded inconsistent findings in terms of AVG-BLEU and ROUGE-l metrics, while the human evaluation affirmed that both GPT models attained a level of accuracy comparable to that of human performance. K\u0131c\u0131man et al. (2023) examined ChatGPT and GPT-4 on the causal discovery, counterfactual reasoning, and actual causality inferring, finding that they outperformed other LLMs and SOTA models largely on the first two tasks.\n\nPsychological Reasoning is the ability of humans to reason about other's unobservable mental states (a.k.a Theory of Mind (ToM)). Kosinski (2023) and Moghaddam and Honey (2023) designed sets of False-Belief questions and quantified results suggested that both ChatGPT and GPT-4 had ToM ability, but that was still inferior to a human's. However, Marcus and Davis (2023) pointed out flaws in the Kosinski (2023) study because the test material was in the training data. Holterman and van Deemter (2023) further tested GPT-3 and GPT-4 on more ToM tasks summarized in Kahneman (2000). They acknowledged the potential problem of the test material being in the training data, so they substituted various nouns in the scenario. However, this is unlikely to be adequate to stop a neural model from generalising from those examples. Borji (2023) found that chatGPT failed on a variant of a classic 'Sally-Anne Test' (used to test children).  found that chatGPT answered correctly on a similar variant Sally-Anne, and they further tested on a range of more advanced ToM scenarios, with probing questions, e.g. to infer the counterfactual impact of actions on mental states. They found that GPT-4 had superior abilities and suggested that GPT-4 had a very advanced level of ToM.\n\nTask-Oriented Reasoning. Qin et al. (2023) evaluated dialogue, logical reasoning, complex yes/no QA, symbolic reasoning (last letter concatenation and coin flip), date understanding-, and tracking shuffled objects-oriented logical reasoning. However, ChatGPT underperformed finetuned baselines on most of the tasks, excluding the logical reasoning tasks. To ascertain whether Chat-GPT relies on profound comprehension of truth and logic in their reasoning or merely exploits shallow memorized patterns, Wang et al. (2023a) proposed a dialectical evaluation task, finding that despite dis-playing high confidence, ChatGPT demonstrated an inability to hold its belief in the truth in a wide range of reasoning tasks, e.g., mathematics, firstorder logic, commonsense, and generic reasoning.\n\nNatural Language Inference (NLI) aims to examine if a statement can be inferred, contradicted, or neutral, compared to another statement. Liu et al. (2023b) compared ChatGPT and GPT-4 with RoBERTa. However, both models encountered difficulties when dealing with novel and out-ofdistribution data. They yielded relatively modest performance on NLI that needed logical reasoning. Qin et al. (2023) also proved that the NLI ability of ChatGPT was lower than that of supervised models. Ambiguity is one of the difficulties of NLI. For example, whether \"John and Anna are not a couple\" contradicts \"John and Anna are married\" depends on whether \"married\" means \"both married\" or \"married to each other\". Given an ambiguous NLI premise,  asked ChatGPT and GPT-4 to either generate disambiguations of a premise with respect to the hypothesis or recognize disambiguation (i.e., deciding whether the disambiguation is an interpretation of an ambiguous premise). Their human evaluation showed that for the first task, GPT-4 achieved correctness at 32%, while, for the second task it was at the level of random guessing, suggesting that resolving tricky ambiguity remained challenging for ChatGPT.", "filtered_refids": [[null, "b10"], [null, "b10"], [null], [], ["b11", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 39, "num_chars": 6019, "num_references": 7}
{"corpusid_sectionid": "261100760-s5", "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4", "date": "2023-08-24", "section_title": "Natural Science", "section": "Physics. ChatGPT was evaluated as if it is a college student who needs to finish homework, clicker questions, programming exercises, and exams in the first-year Calculus-based Physics (Kortemeyer, 2023). Overall, ChatGPT achieved 53.05% after weighing different testing modules. This score met the minimum requirement for course credit, yet it adversely affected the overall grade-point average, falling below the necessary threshold for graduation. ChatGPT showed outstanding performance in clicker and programming questions, achieving scores higher than 90%. However, its performance in homework and exams was subpar. Additionally, ChatGPT's mathematical difficulties in the field of physics lowered its overall score.\n\nChemistry. Clark (2023) asked ChatGPT to finish two real chemistry exams with closed-and open-response questions. 44% closed-response questions were correctly answered by ChatGPT, although this is lower than the student average score (69%). Conversely, when it came to open-response questions, ChatGPT's performance was even lower than that of the least successful student.  (2023) suggested that ChatGPT achieved an impressive final diagnosis accuracy (76.9%), while its initial diagnosis accuracy was just 60.3%.", "filtered_refids": [["b17"], ["b15", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1236, "num_references": 3}
{"corpusid_sectionid": "41310809-s8", "title": "Trends in HLT Research: A Survey of LDC's Data Scholarship Program", "date": "2016-05-01", "section_title": "Successes", "section": "Despite the challenges involved in the LDC Data Scholarship program, we believe it makes a valuable contribution to multiple fields. A goal for the program has been to aid new developments in language-related research and technology, and based on feedback from award recipients, we think that goal has been met. 6 For instance, most reported that they used the data as they intended and received the results they expected. Three students have graduated from their programs and two more expect to graduate in 2016. There have been at least six published papers based on data received in the program. (E.g., Guven, 2012;Harrat, et al., 2013;George, et al., 2015). Most awardees described the data they received as vital to their work. In one case, data awarded through the program was used to build a state-of-the-art speaker recognition system (AMRITATCS) that the awardee and his colleagues submitted to The Speakers in the Wild (SITW) Speaker Recognition Challenge hosted by SRI International. 7 There were some who found that they could not use the data, for instance, because they expected it to contain something it did not, the data set was too small, or their dissertation topic changed. Overall, however, as indicated above, students report positive experiences in the program. We think that the data scholarship program has helped potential new entrants to the field by giving them the experience of working with data as they will be expected to in their careers. It is clear to us, however, that metric-driven evaluation has not spread throughout the field. This is something we as a community should consider addressing to ensure that new entrants are prepared to make meaningful contributions and further progress.", "filtered_refids": [[null, "b1", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1725, "num_references": 3}
{"corpusid_sectionid": "233476148-s3", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Tasks", "section": "Most papers in Table 1 focus on text classification with single input (TC) for a variety of specific problems such as e-mail categorization , topic classification (Kulesza et al., 2015;Teso and Kersting, 2019), spam classification (Koh and Liang, 2017), sentiment analysis (Ribeiro et al., 2018b) and auto-coding of transcripts (Kulesza et al., 2010). By contrast, Zylberajch et al. (2021) targeted natural language inference (NLI) which is a type of text-pair classification, predicting whether a given premise entails a given hypothesis. . Ghai et al. (2021) suggested that most researchers work on TC because, for this task, it is much easier for lay participants to understand explanations and give feedback (e.g., which keywords should be added or removed from the list of top features) 4 . Meanwhile, some other NLP tasks require the feedback providers to have linguistic knowledge such as part-of-speech tagging, parsing, and machine translation. The need for linguists or experts renders experiments for these tasks more difficult and costly. However, we suggest that there are several tasks where the trained models are prone to be buggy but the tasks are underexplored in the EBHD setting, though they are not too difficult to experiment on with lay people. NLI, the focus of (Zylberajch et al., 2021), is one of them. Indeed, McCoy et al. (2019) and Gururangan et al. (2018) showed that NLI models can exploit annotation artifacts and fallible syntactic heuristics to make predictions rather than learning the logic of the actual task. Other tasks and their bugs include: QA, where Ribeiro et al. (2019) found that the answers from models are sometimes inconsistent (i.e., contradicting previous answers); and reading comprehension, where Jia and Liang (2017) showed that models, which answer a question by reading a given paragraph, can be fooled by an irrelevant sentence being appended to the paragraph. These non-TC NLP tasks would be worth exploring further in the EBHD setting.", "filtered_refids": [["b67", "b77", "b22", "b23", null, "b48", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1994, "num_references": 7}
{"corpusid_sectionid": "233476148-s5", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Bug Sources", "section": "Most of the papers in Table 1 experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability. Out of the 15 papers we surveyed, 5 used the 20Newsgroups dataset (Lang, 1995) as a case study, since it has lots of natural artifacts. For example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions. However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild. Apart from classification performance drops, natural artifacts can also cause model biases, as shown in (De-Arteaga et al., 2019;Park et al., 2018) and debugged in (Lertvittayakumjorn et al., 2020;Yao et al., 2021).\n\nIn the absence of strong natural artifacts, bugs can still be simulated using several techniques. First, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance (Kulesza et al., 2010). Second, injecting wrong labels (WL) into the training data can obviously blunt the model quality (Koh and Liang, 2017). Third, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on (Lertvittayakumjorn et al., 2020;Yao et al., 2021). All of these techniques give rise to undesirable model behaviors, requiring debugging. Another technique, not found in Table 1 but suggested in related work (Idahl et al., 2021), is contaminating input texts in the training data with decoys (i.e., injected artifacts) which could deceive the model into predicting for the wrong reasons. This has been experimented with in the computer vision domain (Rieger et al., 2020), and its use in the EBHD setting in NLP could be an interesting direction to explore.", "filtered_refids": [["b43", "b28", "b30", null, "b72"], ["b30", null, "b19", "b23", "b72", "b53"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2172, "num_references": 11}
{"corpusid_sectionid": "233476148-s7", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "date": "2021-04-30", "section_title": "Providing Explanations", "section": "The main role of explanations here is to provide interpretable insights into the model and uncover its potential misbehavior or irrationality, which sometimes cannot be noticed by looking at the model outputs or the evaluation metrics.\n\nExplanation scopes. Basically, there are two main types of explanations that could be provided to feedback providers. Local explanations (L) explain the predictions by the model for individual inputs. In contrast, global explanations (G) explain the model overall, independently of any specific inputs. It can be seen from Table 1 that most existing work use local explanations. One reason for this may be that, for complex models, global explanations can hardly reveal details of the models' inner workings in a comprehensible way to users. So, some bugs are imperceptible in such highlevel global explanations and then not corrected by the users. For example, the debugging framework FIND, proposed by Lertvittayakumjorn et al. (2020), uses only global explanations, and it was shown to work more effectively on significant bugs (such as gender bias in abusive language detection) than on less-obvious bugs (such as dataset shift between product types of sentiment analysis on product reviews). Otherwise, Ribeiro et al. (2018b) presented adversarial replacement rules as global explanations to reveal the model weaknesses only, without explaining how the whole model worked.\n\nOn the other hand, using local explanations has limitations in that it demands a large amount of effort from feedback providers to inspect the explanation of every single example in the training/validation set. With limited human resources, efficient ways to rank or select examples to explain would be required (Idahl et al., 2021). Recently, some work in explainable AI considers generating explanations for a group of predictions (Johnson et al., 2020; Chan et al., 2020) (e.g., for all the false positives of a certain class), thus staying in the middle of the two extreme explanation types (i.e., local and global). This kind of explanation is not too fine-grained, yet it can capture some suspicious model behaviors if we target the right group of examples. So, it would be worth studying in the context of EBHD (to the best of our knowledge, no existing study experiments with it).\n\nGenerating explanations. To generate explanations in general, there are two important questions we need to answer. First, which format should the explanations have? Second, how do we generate the explanations?\n\nFor the first question, we see many possible answers in the literature of explainable NLP (e.g., see the survey by Danilevsky et al. (2020)). For instance, input-based explanations (so called feature importance explanations) identify parts of the input that are important for the prediction. The explanation could be a list of importance scores of words in the input, so called attribution scores or relevance scores (Lundberg and Lee, 2017; Arras et al., 2016). Example-based explanations select influential, important, or similar examples from the training set to explain why the model makes a specific prediction (Han et al., 2020;Guo et al., 2020). Rule-based explanations provide interpretable decision rules that approximate the prediction process (Ribeiro et al., 2018a). Adversarial-based explanations return the smallest changes in the inputs that could change the predictions, revealing the model misbehavior (Zhang et al., 2020a). In most NLP tasks, inputbased explanations are the most popular approach for explaining predictions (Bhatt et al., 2020). This is also the case for EBHD as most selected studies use input-based explanations (Kulesza et al., 2009(Kulesza et al., , 2010Teso and Kersting, 2019;Cho et al. For the second question, there are two ways to generate the explanations: self-explaining methods and post-hoc explanation methods. Some models, e.g., Naive Bayes, logistic regression, and decision trees, are self-explaining (SE) (Danilevsky et al., 2020), also referred to as transparent (Adadi and Berrada, 2018) or inherently interpretable (Rudin, 2019). Local explanations of self-explaining models can be obtained at the same time as predictions, usually from the process of making those predictions, while the models themselves can often serve directly as global explanations. For example, feature importance explanations for a Naive Bayes model can be directly derived from the likelihood terms in the Naive Bayes equation, as done by several papers in Table 1 (Kulesza et al., 2009;Smith-Renner et al., 2020). Also, using attention scores on input as explanations, as done in (Cho et al., 2019), is a self-explaining method because the scores were obtained during the prediction process.\n\nIn contrast, post-hoc explanation methods (PH) perform additional steps to extract explanations after the model is trained (for a global explanation) or after the prediction is made (for a local explanation). If the method is allowed to access model parameters, it may calculate word relevance scores by propagating the output scores back to the input words (Arras et al., 2016) or analyzing the derivative of the output with respect to the input words (Smilkov et al., 2017;Sundararajan et al., 2017). If the method cannot access the model parameters, it may perturb the input and see how the output changes to estimate the importance of the altered parts of the input (Ribeiro et al., 2016;Jin et al., 2020). The important words and/or the relevance scores can be presented to the feedback providers in the EBHD workflow in many forms such as a list of words and their scores (Teso and Kersting, 2019;Ribeiro et al., 2016), word clouds (Lertvittayakumjorn et al., 2020), and a parse tree (Yao et al., 2021). Meanwhile, the influence functions method, used in (Koh and Liang, 2017;Zylberajch et al., 2021), identifies training examples which influence the prediction by analyzing how the prediction would change if we did not have each training point. This is another post-hoc explanation method as it takes place after prediction. It is similar to the other two example-based explanation methods used in (Khanna et al., 2019;Han and Ghosh, 2020).\n\nPresenting explanations. It is important to carefully design the presentation of explanations, taking into consideration the background knowledge, desires, and limits of the feedback providers. In the debugging application by Kulesza et al. (2009), lay users were asked to provide feedback to email categorizations predicted by the system. The users were allowed to ask several Why questions (inspired by Myers et al. (2006)) through either the menu bar, or by right-clicking on the object of interest (such as a particular word). Examples include \"Why will this message be filed to folder A?\", \"Why does word x matter to folder B?\". The system then responded by textual explanations (generated using templates), together with visual explanations such as bar plots for some types of questions. All of these made the interface become more user-friendly. In 2015, Kulesza et al. proposed, as desirable principles, that the presented explanations should be sound (i.e., truthful in describing the underlying model), complete (i.e., not omitting important information about the model), but not overwhelming (i.e., remaining comprehensible). However, these principles are challenging especially when working on non-interpretable complex models.", "filtered_refids": [[], ["b30", "b51"], [null], [], ["b67", "b50", "b25", "b55", null, "b23", "b62", "b24", "b74", "b1", "b8", "b5"], ["b67", "b49", "b30", null, "b19", "b77", "b72", "b16", "b65", "b5"], ["b24", null, "b40"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 44, "num_chars": 7431, "num_references": 28}
{"corpusid_sectionid": "16442276-s1", "title": "Content Models for Survey Generation: A Factoid-Based Evaluation", "date": 2015, "section_title": "Data", "section": "Prior research in automatic survey generation has explored using text from different parts of scientific papers. Some of the recent work has treated survey generation as a direct extension of single paper summarization (Qazvinian and Radev, 2008) and used citing sentences to a set of relevant papers as the input for the summarizer (Mohammad et al., 2009;Qazvinian et al., 2013). However, in our prior work, we have observed that it's difficult to generate coherent and readable summaries using just citing sentences and have proposed the use of sentences from introductory texts of papers that cite a number of important papers on a topic (Jha et al., 2015). The use of full text allows for the use of discourse structure of these documents in framing coherent and readable surveys. Since the content models we explore are meant to be part of a larger system that should be able to generate coherent and readable survey articles, we use the introduction sentences for our experiments as well.\n\nThe corpus we used for extracting our experimental data was the ACL Anthology Network, a comprehensive bibliographic dataset that contains full text and citations for papers in most of the important venues in natural language processing . An oracle method is used for selecting the initial set of papers for each topic. For each topic, the bibliographies of at least three human-written surveys were extracted, and any papers that appeared in more than one survey were added to the target document set for the topic.\n\nThe text for summarization is extracted from introductory sections of papers that cite papers in the target document set. The intuition behind this is that the introductory sections of papers that cite these target document summarize the research in papers from the target document set as well as the relationships between these papers. Thus, these introductions can be thought of as mini-surveys for specific aspects of the topic; combining text from these introductory sections should allow us to generate good comprehensive survey articles for the topic 1 . For our experiments, we sort the citing papers based on the number of papers they cite Input sentence Factoids According to [1] , the corpus based supervised machine learning methods are the most successful approaches to WSD where contextual features have been used mainly to distinguish ambiguous words in these methods. supervised wsd, corpus based wsd Compared with supervised methods, unsupervised methods do not require tagged corpus, but the precision is usually lower than that of the supervised methods.\n\nsupervised wsd, unsupervised wsd Word sense disambiguation (WSD) has been a hot topic in natural language processing, which is to determine the sense of an ambiguous word in a specific context. definition of word sense disambiguation Improvement in the accuracy of identifying the correct word sense will result in better machine translation systems, information retrieval systems, etc.\n\nwsd for machine translation, wsd for information retrieval The SENSEVAL evaluation framework ( Kilgarriff 1998 ) was a DARPA-style competition designed to bring some conformity to the field of WSD, although it has yet to achieve that aim completely. senseval Table 3: Sample input sentences from the topic of word sense disambiguation annotated with factoids.\n\nin the target document set, pick the top 20 papers, and extract sentences from their introductions to form the input text for the summarizer. The seven topics used in our experiments and input size for each topic are shown in Table 2.\n\nOnce the input text for each topic has been extracted, we annotate the sentences in the input text with factoids for that topic. Some annotated sentences in the topic of word sense disambiguation are shown in Table 3. Given this new annotated data, we can compare how the factoids are distributed across different citing sentences (as annotated by Jha et al. (2013)) and introduction sentences that we have annotated. For this, we divide the factoids into five categories: definitions, venue, resources, methodology, and applications. The fractional distribution of factoids in these categories is shown in Table 4. We can see that the distribution of factoids relating to venues, methodology and applications is similar for the two datasets. However, factoids related to definitional sentences are almost completely missing in the citing sentences data. This lack of background information in citing sentences is one of the motivations for using introduction sentences for survey article generation as opposed to previous work.\n\nThe complete set of factoids as well as annotated sentences for all the topics is available for download at http: //clair.si.umich.edu/corpora/ Surveyor_CM_Data.tar.gz.", "filtered_refids": [["b21", "b16", "b22", "b9"], [], [], [], [], [], ["b8"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4771, "num_references": 5}
{"corpusid_sectionid": "225062337-s4", "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios", "date": "2020-10-23", "section_title": "How Low is Low-Resource?", "section": "On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.\n\n(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.\n\nGiven the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.", "filtered_refids": [[], ["b13", "b0"], [null, "b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1450, "num_references": 4}
{"corpusid_sectionid": "225062337-s6", "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios", "date": "2020-10-23", "section_title": "Data Augmentation", "section": "New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019). Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018).\n\nTo go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (\u015eahin and Steedman, 2018;Vania et al., 2019;Dehouck and G\u00f3mez-Rodr\u00edguez, 2020), simplification of sentences by removal of sentence parts (\u015eahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020). For whole sentences, paraphrasing through backtranslation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018). An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019). Back-translation has also been leveraged for text classification (Xie et al., 2020;Hegde and Patil, 2020). This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020). It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. Ding et al. (2020) extend this idea for token level tasks.\n\nAdversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020). They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020). Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score). This approach is often applied on the level of vector representations. For instance, Grundkiewicz et al. (2019) reverse the augmentation setting by applying transformations that flip the (binary) label. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.\n\nOpen Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing. A reason might be that several of the approaches require an indepth understanding of the language. There is not yet a unified framework that allows applying data augmentation across tasks and languages. Recently, Longpre et al. (2020) hypothesised that data augmentation provides the same benefits as pretraining in transformer models. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited.", "filtered_refids": [["b49", null, "b46", "b18"], ["b55", null, "b9", "b46"], [null, "b60"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3788, "num_references": 11}
{"corpusid_sectionid": "225062337-s7", "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios", "date": "2020-10-23", "section_title": "Distant & Weak Supervision", "section": "In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).\n\nWhile distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with \"It was great/bad\" for obtaining binary sentiment labels.\n\nOpen Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.\n\nDistant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.\n\nWhile distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).", "filtered_refids": [["b67", "b20", null, "b22", "b23", "b1", "b38", "b61", "b2"], [null, "b5"], ["b54", null], [null], ["b14", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4273, "num_references": 16}
{"corpusid_sectionid": "256231532-s1", "title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks", "date": "2022-10-11", "section_title": "Social Influence Dialogue Systems", "section": "\"Social influence is a fact of everyday life\" (Gass, 2015). It is the change in thoughts, feelings, attitudes, or behaviors resulting from interaction with an individual or a group (Rashotte, 2007). Influence is measured by quantifiable proxies of the observed change, like the interest to indulge in physical exercise before or after the interaction with a system, or the final deal in a negotiation as opposed to one person taking it all. Social influence dialogue systems act interactively and influence their partners in decision-making and behavioral contexts (Zhang et al., 2020a;Lee et al., 2020). This calls for an active role by the system, distinguishing them from other well-studied scenarios, such as purely task-oriented, where systems passively assist their partners to complete tasks, and opendomain, that target social companionship. Key social influence tasks include persuasion , aiming to change users' attitudes or behaviors, and negotiation, aiming to change the users' perspective to achieve a common ground (Lewis et al., 2017). Conceptual overview: Figure 1 distinguishes between the kinds of conversational content in social influence interactions. The task-oriented content focuses on influencing for a domain-specific goal, like persuading for donation, bargaining with tradeoffs, or encouraging healthier habits. These interactions may also contain social content, such as small talk, empathy, or self-disclosure. The task-oriented content provides a context for social interactions. Depending on the task, social content is optional, but if present, can in turn build rapport and enhance user-system relationship for improved task outcomes (Liao et al., 2021). Connections with task-oriented and opendomain systems: Similar to a task-oriented or an open-domain scenario, social influence dialogue can also be seen as a sequential decision making process with the goal of maximizing the expected reward Gao et al., 2018). Our proposed category is not meant to be disjoint from these traditional categories. However, it still uniquely brings together the tasks that capture social influence, which is fundamentally absent from how we primarily define dialogue tasks in the community. Defining a new category that captures social influence dialogue would foster a dedicated effort towards this important aspect of real-world conversations.\n\nTask-oriented scenarios focus on collaborative information exchange for a common goal of task completion. In social influence tasks, the goals of the system and the user can be different and even conflicting, leading to collaborative or noncollaborative interactions. Further, the goals can go beyond the current task (e.g. multiple therapy interactions, repeated negotiations), leading to social interactions for long-term relationships. If a scenario involves the system's goal to influence its partner, we consider it under social influence in this paper. For instance, He et al. (2018) studied buyerseller price negotiations. The task of the buyer is to negotiate for a reasonable price (arguably making it task-oriented), but achieving it requires social influence skills of engaging in trade-offs and building a rapport with the seller so as to reach an agreement. Measures of Success: The above discussion indicates that a comprehensive evaluation of social influence systems must draw from both task-oriented and open-domain dialogue research. Since there exist surveys that discuss the evaluation in these settings (Deriu et al., 2021;Li et al., 2021), we don't cover them here in detail. However, we define three essential axes for evaluation: 1) Linguistic Performance, or the system's linguistic sophistication based on automatic (e.g. perplexity, BLEU) and human (e.g. fluency, consistency, coherency) evaluation. 2) Influence Outcome, or the ability to influence defined by objective goals like the negotiated price or weight loss after therapy. 3) Partner Perception, or the subjective evaluation of the user, for instance, the user's satisfaction, likeness towards the system, and interest in interacting again. In a buyer-seller negotiation, if the seller hates the buyer in the end, no matter how favorable the deal is for the buyer, one might argue that this is still a failed negotiation for the buyer. Hence, we encourage future work to take all three dimensions into account collectively.", "filtered_refids": [["b60", "b18", "b1", "b25", "b16", "b38", "b0"], ["b52", null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 4377, "num_references": 10}
{"corpusid_sectionid": "256231532-s2", "title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks", "date": "2022-10-11", "section_title": "Social Influence Across Diverse Application Areas", "section": "We now illustrate social influence across numerous domains and application areas. In total, we curated 22 datasets from prior work that capture social influence in various forms, spanning 12 publication venues, 4 languages, and 7 application domains (see Appendix A for details on the compilation process). In general, the datasets capture the following information about an interaction: the non-conversational context for the participants (e.g. negotiation preferences or other role-specific information), the conversation between them, and outcome assessment. Optionally, some datasets also gather participant demographics and personality traits, utterance-level annotations, and subjective evaluations via post-surveys.\n\nTo understand the structural similarities and differences between these datasets, we design a taxonomy with two primary dimensions: Task Structure (Symmetric vs Asymmetric), and Context Definition (Global vs Local). Task Structure captures whether the participant roles are defined in a sym-metric or an asymmetric manner. For instance, a typical multi-issue negotiation is symmetric, in the sense that both parties have their own preferences and goals based on which they actively try to reach a favorable agreement (Lewis et al., 2017). On the other hand, a counseling session between a therapist and a patient is asymmetric, where the therapist attempts to emotionally support the patient by employing social influence skills (Althoff et al., 2016). Context Definition relates to whether the input context before each interaction is defined globally or locally. For instance, the PersuasionFor-Good dataset globally defines the context of persuasion for charity donation, which is kept the same throughout . On the contrary, in a typical debate, although the rules are defined globally, the conversation topic and arguments are local and can vary for each conversation (Durmus and Cardie, 2019). We present this categorization in Table 1. We further categorize the datasets according to their Domain, Source, and the # of parties. We provide key statistics and the available metadata in Appendix B. We now briefly discuss the datasets in each domain.\n\nGames: Strategy games involve social influence dynamics of trust and deception. Diplomacy captures deception in long-lasting relationships, where players forge and break alliances to dominate Europe (Peskov et al., 2020). Catan revolves around the trade of resources for acquiring roads, settlements, and cities (Asher et al., 2016;Boritchev and Amblard, 2021). The players have access to only a subset of resources that they would need, which encourages strategic influence and trade.\n\nMulti-Issue Bargaining Tasks (MIBT): MIBT is a tractable closed-domain abstraction of a typical negotiation (Fershtman, 1990). It is based on a fixed set of issues each with a predefined priority for each player, which essentially governs the goals of the players. If the priorities of the players align, this leads to competitive negotiations, where each party attempts to convince their partner with tradeoffs and persuasive arguments. If they don't, this allows cooperative interactions where the negotiators try to find optimal divisions that benefit everyone. DealOrNoDeal (Lewis et al., 2017) involves negotiations over three issues: books, balls, and hats. Other datasets define a more grounded scenario, such as symmetric CaSiNo (Chawla et al., 2021b) negotiations between two campsite neighbors and asymmetric JobInterview (Yamaguchi et al., 2021)   negotiations between recruiters and applicants. Social Good: Social influence is critical for social good applications. The tactics must be personalized using knowledge that is both relevant and appealing. PersuasionForGood  involves asymmetric interactions led by a persuader who attempts to convince the other participant for charity donation by employing a variety of tactics. For instance, Logical Appeal uses reason and evidence to support the argument, while Emotional Appeal elicits specific emotions. E-commerce: These tasks are typically asymmetric. A buyer influences the seller towards a reasonable price, while the seller tries to maximize their own profit. An effective system must combine price-related reasoning with language realization. CraigslistBargain (He et al., 2018) involves openended price negotiations with rich influence strategies like embellishments, side offers, emotional appeals, and using world knowledge. Another example is customer support interactions in AntiScam dataset (Li et al., 2020), where users defend themselves against attackers who try to steal sensitive personal information with convincing arguments.\n\nTherapy & Support: Effective therapy using social influence aids in the treatment of mental disorders, and substance use disorders, along with changing undesirable behaviors like unhealthy diets. A counselor needs to be adaptive, personalized, should understand the core issues, and should facilitate a change in patient's perspective (Althoff et al., 2016). In SMS counseling, Althoff et al. (2016) found that linguistic influence like pushing the conversation in the desired direction is associated with perspective change. Similar scenarios were captured in other datasets as well (Demasi et al., 2019;Liang et al., 2021). Tanana et al. (2016) collected the Motivational Interviewing dataset where the goal is to elicit and explore the patient's own motivations for behavior change. EmpatheticDialogues (Rashkin et al., 2019) captured empathetic support interactions, which has been associated with rapport and better task outcomes (Kim et al., 2004;Norfolk et al., 2007;Fraser et al., 2018).\n\nArgumentation: In addition to factuality and social proof, a convincing argument must also consider the intensity, valence, authoritativeness, and framing (Chaiken, 1987;Althoff et al., 2014). Tan et al. (2016) released the ChangeMyView logs from Reddit, involving discussions on numerous controversial topics. Other datasets include Debate Dot Org (DDO) debates on diverse topics (Durmus and Cardie, 2019), congressional proceedings (Thomas et al., 2006), and court hearings (Fornaciari and Poesio, 2012;D.-N.-M. et al., 2012;Ji et al., 2020). Conversational Recommendation: Everyday scenarios naturally hold potential for influence via recommendations, for instance, a movie fan per-suading their friends to watch a movie that they adore.  and Dodge et al. (2016) collected movie recommendation datasets. Instead of guiding the conversation towards a specific movie, the goal is simply to provide recommendations based on facts and personal experiences. Nevertheless, they still provide interesting examples of scenarios that can involve social influence.\n\nMiscellaneous: The Target-Guided dataset (Tang et al., 2019) was constructed from the PersonaChat corpus (Zhang et al., 2018).\n\nInstead of being openended, the Target-Guided scenario defines a concrete goal of naturally guiding the conversation to a designated target subject, thereby, making it a social influence setting.", "filtered_refids": [[], [null, "b18"], [null, "b33"], ["b54", "b18", null, "b23", "b5"], ["b47", "b13", "b30", null, "b24"], ["b49", null, "b10", "b46"], ["b48", "b61"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 48, "num_chars": 7055, "num_references": 20}
{"corpusid_sectionid": "256231532-s3", "title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks", "date": "2022-10-11", "section_title": "Methodological Progress", "section": "Having summarized the datasets that capture social influence, we now discuss the modeling approaches developed for social influence dialogue systems. Most domains have seen efforts in analyzing human dialogue behaviors and their impact on task outcomes. Examples include analyzing deception in games (Peskov et al., 2020), the impact of persuasive strategies and dialogue acts on charity donations ( In addition, researchers have targeted various domain-specific subtasks that can be crucial for the eventual development of dialogue systems in this space. This involves research in lie detection methods (Yeh and Ku, 2021;Yu et al., 2015), discourse parsing (Shi and Huang, 2019;Ouyang et al., 2021), strategy prediction (Chawla et al., 2021b;, breakdown detection (Yamaguchi et al., 2021), outcome prediction (Sinha and Dasgupta, 2021;Chawla et al., 2020;Dutt et al., 2020), and argument mining (Dutta et al., 2022).\n\nResearch that directly targets the development of dialogue systems in this space is still nascent. Among other challenges like limited cross-cultural diversity and relatively smaller dataset size, social influence dialogue settings pose a unique challenge: an average human often exhibits sub-optimal strategic behaviors in social influence tasks (Wunderle, 2007;Babcock and Laschever, 2009). This means that standard seq2seq approaches trained on these collected datasets using supervised learning are fundamentally insufficient for developing effective dialogue systems with influence capabilities. Hence, prior work has put a special attention to the system strategy, employing different ways to model the strategy and language together.\n\nWe design a taxonomy of methods developed for social influence tasks, assisting readers to comprehend the progress and reflect on future directions. We organize them based on the system strategy, language generation, partner model, architecture, learning process, and the use of pretrained language models. We present annotations for all the surveyed methods in Table 2 and discuss the common categories in brief below.", "filtered_refids": [["b43", "b54", "b58", "b44", "b32", "b59", null, "b33"], ["b53", null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 2080, "num_references": 10}
{"corpusid_sectionid": "237421091-s2", "title": "(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys", "date": "2021-09-04", "section_title": "Stance Detection and Annotation", "section": "While early efforts to use social media to extract public opinion focused on sentiment analysis (O'Connor et al., 2010;Mitchell et al., 2013;Conrad et al., 2019), more recent work has shifted towards stance detection, which more directly aligns with the goal of public opinion modeling (Sen et al., 2020;Mohammad et al., 2017). In stance detection, the task moves from estimating the positive or negative sentiment of a given text to evaluating whether the authoring individual is for, against, or neutral towards some target concept. In principle, this aligns well with traditional public opinion polling in which respondents self-report along a similar stance scale.\n\nHowever, the growth of stance detection methods has come with many questions about their validity and foundational premises. Sen et al. (2020) showed that a variety of existing stance detection tools for Twitter do not generalize well, even when the target is held constant and test data are reasonably similar to training data. Joseph et al. (2017) showed that how one constructs annotation tasks can significantly impact (supervised) model performance and one's assessment of it. Further, as demonstrated by Shen and Rose (2021) on the closely related task of inferring political ideology, annotator expertise and subjectivity also play an important role in the quality of annotated data.\n\nThe present work complements these prior efforts by delving into other questions of annotator disagreement and inference. Whereas prior work has considered disagreement arising from task differences (Joseph et al., 2017), or properties of the annotators (Shen and Rose, 2021), we control for both of these factors, taking a single task and a relatively homogenous set of expert annotators. Instead, extending recent work studying prediction on multiple targets (van den Berg et al., 2019;, we study how agreement varies depending on the target selected, and how even within a single task design, annotators can come to rely on distinct subsets of information. Second, prior work has largely only assumed that it is possible for annotators to accurately infer a user's stance. The present work tests this assumption by comparing these annotations to self-report data.", "filtered_refids": [["b15", "b19", "b7", "b24", "b17"], ["b24", "b13"], ["b29", "b13", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2228, "num_references": 10}
{"corpusid_sectionid": "233219920-s1", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Preliminaries", "section": "We briefly review preliminaries and assumptions necessary for our survey.\n\nKnowledge bases We use the term \"knowledge base\" (KB) to refer to a relational data structure comprising a set of entities E, relation types R, and triples (s, r, o) \u2208 E \u00d7 R \u00d7 E, where s, o \u2208 E are subject and object entities, respectively. 1 We consider two types of KBs under the umbrella of \"relational world knowledge.\" Encyclopedic KBs store facts about typed, disambiguated entities; a well-known example is the Wikidata KB (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), which, like its sister project Wikipedia, is publicly accessible and collaboratively constructed. By contrast, in commonsense KBs, \"entities\" are typically represented by non-canonicalized free-text phrases. Examples include the publicly accessible, crowdsourced Con-ceptNet (Liu and Singh, 2004;Speer et al., 2017) and ATOMIC (Sap et al., 2019) KBs.\n\nLanguage models Following the contemporary NLP literature, we use the term \"language model\" (LM) to refer to a deep neural network that is trained to learn contextual text representations. LMs generally come pretrained, with parameters pre-initialized for generic text representation via self-supervised training on large corpora, and may be used as-is after pretraining, or further finetuned with supervision on downstream task(s). This work considers LMs based on the Transformer architecture (Vaswani et al., 2017), examples of which include the encoder-only BERT family (Devlin et al., 2019;, the decoder-only GPT family (Brown et al., 2020), and the encoder-decoder T5  and BART  families.", "filtered_refids": [[], [null, "b13", "b35", "b39"], ["b19", null, "b47"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1583, "num_references": 7}
{"corpusid_sectionid": "233219920-s2", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Word-level supervision", "section": "The standard language modeling task is to predict the n-th word in a sequence of n words-that is, a conditional probability estimation task (Radford et al., 2019). While many variants of this task have been proposed to allow LMs to condition their predictions on different inputs (Devlin et al., 2019;, a notable feature of all such approaches is that they operate at the word (and subword) level. If these supervision techniques do not incorporate KBs at all, how are they relevant when considering LMs as relational knowledge representations? The answer is simple. Typical language Prompt handcrafting (Petroni et al., 2019;Dufter et al., 2021) Automatic prompt engineering (Jiang et al., 2020b;Shin et al., 2020;Zhong et al., 2021;Qin and Eisner, 2021) Adversarial prompt modification Poerner et al., 2020; Varying base prompts (Elazar et al., 2021;Heinzerling and Inui, 2021;Jiang et al., 2020a;Kassner et al., 2021) Symbolic rule-based prompting Talmor et al., 2020a) Statement scores ( \u00a7 3.2) Single-LM scoring (Tamborrino et al., 2020;) Dual-LM scoring (Davison et al., 2019Shwartz et al., 2020) modeling corpora like Wikipedia are known to contain KB-like assertions about the world (Da and Kasai, 2019). LMs trained on enough such data can be expected to acquire some KB-like knowledge, even without targeted entity-or relation-level supervision. Therefore, in order to motivate the necessity (if at all) of KB supervision, it is crucial to first understand what relational world \"knowledge\" LMs acquire from word-level pretraining.\n\nIn this section, we cover strategies to extract and utilize this knowledge under the cloze prompting ( \u00a7 3.1) and statement scoring ( \u00a7 3.2) protocols. Table 1 provides a taxonomy for this section, with representative examples and evaluation tasks.", "filtered_refids": [["b43", "b63", "b26", "b28", "b27", "b30", null, "b19", "b38", "b37"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1791, "num_references": 10}
{"corpusid_sectionid": "233219920-s3", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review", "date": "2021-04-12", "section_title": "Cloze prompting", "section": "The cloze prompting protocol (Taylor, 1953 and Figure 2) is a direct approach for extracting and evaluating KB-like knowledge in pretrained LMs. Under this protocol, KB triples are first converted to natural language assertions using (e.g.) relation templates. For each assertion, the token(s) corresponding to the object entity are held out. A frozen pretrained LM then ranks candidate tokens within its vocabulary by the probability that they fill in the empty slot(s). Accuracy is typically measured by the proportion of prompts for which the correct answer appears in the LM's top-k predictions, with the assumption that better performance implies more pretrained knowledge within the LM.\n\nHandcrafted prompts in English with singletoken answers make up LAMA (Petroni et al., 2019), one of the earliest and most widely-used LM cloze probes. LAMA, which is mapped primarily to Wikidata and ConceptNet triples, was initially used to compare pretrained LMs' knowledge to offthe-shelf KB question answering systems. Petroni et al. (2019) showed that pretrained BERT is com- petitive with a supervised relation extraction model that has been provided an oracle for entity linking, particularly for 1-1 queries. Subsequent work has experimented with handcrafted templates for probing the knowledge of both very large (hundredbillion parameter) LMs (Brown et al., 2020) as well as non-contextual word embeddings, i.e., as a simple control baseline for LMs (Dufter et al., 2021). Both studies demonstrate some success, particularly in cases where the probed model is provided a small amount of extra context in the form of conditioning examples (Brown et al., 2020) or entity type information (Dufter et al., 2021).\n\nAutomatic prompt engineering is a promising alternative to prompt handcrafting for knowledge extraction in LMs (Liu et al., 2021a), as prompts engineered using discrete (Jiang et al., 2020b;Shin et al., 2020;Haviv et al., 2021) and continuous (Zhong et al., 2021;Qin and Eisner, 2021;Liu et al., 2021b) optimization have improved LMs' lower-bound performance on LAMA's underlying queries. Note, however, that optimized prompts are not always grammatical or intelligible (Shin et al., 2020). Prompt optimization methods may also confound knowledge probes by overfitting to the probes' answer distributions during train-ing (Zhong et al., 2021;, and often require large validation sets for tuning, which may not be feasible in practice (Perez et al., 2021).\n\nAdversarial modification of LAMA prompts has uncovered weaknesses in pretrained LMs' world \"knowledge,\" for example that BERT's accuracy drops precipitously when irrelevant statements or negation words are added to prompts Lin et al., 2020;, and that it can \"guess\" answers using shallow lexical cues or benchmark artifacts (Poerner et al., 2020;. However, the adversarial robustness of LM knowledge improves greatly with supervision in both the pretraining  and fine-tuning  stages, suggesting that explicit KB-level supervision is a viable remedy to input sensitivity. For the former, it has been found that pretrained BERT-based LMs typically do not output consistent answers for prompt paraphrases, although their consistency can again be greatly improved by targeted pretraining (Elazar et al., 2021;Heinzerling and Inui, 2021). For the latter, initial results on prompts beyond English indicate high variability in pretrained LM performance across languages and poor performance on prompts with multi-token answers (Jiang et al., 2020a;Kassner et al., 2021).\n\nPrompts generated with symbolic rules have been used to test pretrained LMs' abilities to learn, e.g., equivalence, implication, composition, and conjunction. Existing studies vary the degrees of experimental control: Talmor et al. (2020a) use BERT-based models with their publicly-available pretrained weights, whereas  pretrain BERT from scratch on synthetic KB triples only. Both studies observe mixed results, concluding that word-level pretraining alone (at least on BERT) does not lead to strong \"reasoning\" skills.", "filtered_refids": [[], [null, "b26"], ["b52", "b63", "b14", "b28", null, "b22", "b37"], ["b11", null, "b27"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 4057, "num_references": 13}
{"corpusid_sectionid": "243865366-s1", "title": "Natural Language Processing Meets Quantum Physics: A Survey and Categorization", "date": 2021, "section_title": "Quantum Physics Preliminaries", "section": "The simplest quantum mechanical system is a qubit, which has two possible states: |0 and |1 , where '|\u00b7 ' is called the Dirac notation, and a ket |\u03c8 denotes a unit column vector. Similarly, the row vector \u03c8 \u2020 is expressed as a bra \u03c8|, where the dagger ( \u2020) corresponds to the conjugate transpose. A qubit can be represented by the linear combination of states, often called superposition:\n\n(1)\n\nwhere a and b are complex numbers and |a| 2 + |b| 2 = 1. Thus the state of a qubit is a unit vector in a two-dimensional complex vector space. When we measure a qubit we obtain either 0, with probability |a| 2 , or 1, with probability |b| 2 . The superposition state can be used for representing multiple meanings of a word. For example, think of a mouse again as a small rodent and a hand-held pointing device. This two independent latent concepts can be denoted as |rodent and |device . Then, the word 'mouse' can be modeled as a superposition state, i.e. |mouse = a |rodent + b |device .\n\nEntanglement is another elementary and unique resource of quantum mechanics which plays a key role in many interesting applications of quantum computing. Consider the following two-qubit entangled Bell state (Nielsen and Chuang, 2002):\n\nAs discussed earlier, when we measure the first qubit, we obtain two possible results: 0 with probability 1/2 and 1 with probability 1/2. According to Eq.2, a measurement of the second qubit always gives the same outcome as the measurement of the first qubit, because the measurement results of these two entangled qubits are correlated. Coecke et al. (2020) proposed that if words are encoded into quantum states, then the grammatical structure is to entangle these states. Because grammar is what correlates meanings between words. We will explain this in Section 4.2.2.\n\nProjective measurements are the most general form of measurement in quantum mechanics, where the measurement operators are projectors P that satisfy P 2 = P . If the state is |\u03c8 before projective measurement then the probability that result m occurs is given by p(m) = \u03c8| P m |\u03c8 . The state after measurement is:\n\nProjective measurement can be applied to calculate cosine similarity in NLP, which measures the similarities between two vectors. Suppose |A and |B represent word A and B, respectively. Then, the cosine similarity of these two word vectors is\n\nwhere P B = |B B| is a projective measurement operator.\n\nIn addition to state vectors, quantum mechanics can also be formulated using density matrix, which is mathematically equivalent. Suppose that a quantum system is in one of the states |\u03c8 i , where i is an index, with probability p i . The definition of the density matrix is:\n\nMore information about quantum computing can be found in (Nielsen and Chuang, 2002).", "filtered_refids": [[], [], [], ["b32"], [null], [], [], [], [], ["b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 2773, "num_references": 3}
{"corpusid_sectionid": "243865366-s3", "title": "Natural Language Processing Meets Quantum Physics: A Survey and Categorization", "date": 2021, "section_title": "Quantum Algorithms", "section": "In quantum computing, a quantum algorithm is an algorithm that runs on real quantum computers. With regard to representation, Coecke et al.    (Coecke and Kissinger, 2018;Coecke et al., 2020) to demonstrate: (a) A ket |\u03c8 , (b) A bra \u03c8|, (c) Bell state in Eq. 2, (d) (g(|\u03d5 1 \u2297 |\u03d5 2 )) \u2297 I, where matrix multiplication looks like connecting up the inputs and outputs of boxes and tensor product looks like placing boxes side by side.\n\n(2010) constituted a graphical framework (DisCo-Cat) for natural language that combines words and builds the meaning of a sentence instead of thinking of a sentence as a bag of words. They devised a graphical framework from previous work which represents quantum mechanics pictorially by using lines, triangles, and so on (Coecke and Kissinger, 2018). As an example, in Figure 1, we use this graphical framework to demonstrate the ket, bra, and two-qubit entangled states introduced in Section 2.\n\nZeng and Coecke (2016) first discussed whether a quantum computer can be applied to process natural language, showing a quantum algorithm for calculating sentence similarity that, under certain conditions, achieves a quadratic speedup over classical methods (see Table 2). This quadratic speedup, however, requires quantum random access memory (QRAM), which is expensive and remains unrealized (Biamonte et al., 2017). Considering this problem, Meichanetzidis et al. (2020a) and Coecke et al. (2020) proposed quantum algorithms that can potentially be implemented in existing NISQ computers. Wiebe et al. (2019) presented a representation for the linguistic structure which can encode NLP problems into small quantum devices. As a proof-of-concept experiment, Meichanetzidis et al. (2020b) performed the first quantum NLP task using a small dataset on NISQ hardware. To present larger-scale experiments, Lorenz et al. (2021) implemented models that solve sentence classification tasks on NISQ computers for datasets of size \u2265 100 sentences. These works pave the way for practical quantum NLP in the NISQ era. ", "filtered_refids": [[null, "b7"], ["b7"], ["b44", "b4", "b29", "b27", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2040, "num_references": 8}
{"corpusid_sectionid": "243865366-s5", "title": "Natural Language Processing Meets Quantum Physics: A Survey and Categorization", "date": 2021, "section_title": "Classical Algorithms", "section": "Quantum-inspired or quantum-like NLP algorithms have been designed for classical computers, and some of them achieve comparable performance to state-of-the-art models (Jiang et al., 2020;. For the sake of applicability, these classical algorithms borrow mathematical frameworks from quantum mechanics but are not constrained by the quantum computing operations when processing the data.\n\nVan Rijsbergen (2004) first proposed to unify information retrieval models into the mathematical framework of quantum mechanics in Hilbert space. Sordoni et al. (2013) proposed a quantum language model, which models term dependencies using the density matrix. This work indicates that the density matrix may be a more general representation of texts. Based on this, Basile and Tamburini (2017) presented a language model using the evolution of the state which can be implemented in speech recognition.  encoded words as quantum states and sentences as mixed systems.\n\nRecently, in order to improve practicality, some quantum-inspired neural networks for natural language problems have been proposed.  use a density matrix based convolutional network to capture interactions within each utterance, outperforms a number of state-of-the-art sentiment analysis algorithms. Jiang et al. (2020) proposed a quantum interference inspired neural matching model with application to ad-hoc retrieval. The main difference between these quantum-inspired neural models for NLP and the existing neural based models is that the former models use the mathematical framework of quantum theory to describe language features. These features described by quantum theory are then used as the input of the neural network. Using quantum mechanics concepts to describe features have better interpretability, because they have more transparent physical explanations. It is also more beneficial to the subsequent neural network to extract useful information.\n\nThe above quantum-inspired neural networks are mainly for improving end-to-end performance, but still lack a theoretical foundation for the connection between quantum-inspired language model and neural network. Tensor networks, which factorize very large tensors into networks of smaller tensors, can help the theoretical understanding of existing neural networks (Levine et al., 2018). Based on tensor decomposition, Zhang et al. (2018b) proposed a quantum many-body wave function (QMWF) inspired language modeling and showed a mathematical understanding of using convolutional neural network (CNN). More recently,  proposed a tensor network method (namely TextTN) for natural language representation. Tensor network can not only run on a classical computer but also can be transformed into a quantum circuit. In addition, the hyper-parameters of TextTN can be well interpreted by the entanglement entropy .", "filtered_refids": [["b15"], ["b38", "b3"], ["b15"], ["b21", "b50"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2829, "num_references": 6}
{"corpusid_sectionid": "231709697-s2", "title": "English Machine Reading Comprehension Datasets: A Survey", "date": "2021-01-25", "section_title": "Answer Type", "section": "Cloze The question is formulated as a sentence with a missing word or phrase which should be inserted into the sentence or should complete the sentence. The answer candidates may be included as in (1) from ReciteQA (Yagcioglu et al., 2018), and may not, as in (2)  We distinguish cloze multiple choice datasets from other multiple choice datasets. The difference is the form of question: in the cloze datasets, the answer is a missing part of the question context and, combined together, they form a grammatically correct sentence, whereas for other multiple choice datasets, the question has no missing words.\n\nBoolean A Yes/No answer is expected, e.g. (4) from the BoolQ dataset (Clark et al., 2019). Some datasets which we include here have a third \"Cannot be answered\" or \"Maybe\" option, e.g. (5) from PubMedQuestions (Jin et al., 2019).\n\n(4) P: The series is filmed partially in Prince Edward Island as well as locations in ... Q: Is anne with an e filmed on pei? A: Yes (5) P: ... Young adults whose families were abstainers in 2000 drank substantially less across quintiles in 2010 than offspring of nonabstaining families. The difference, however, was not statistically significant between quintiles of the conditional distribution. Actual drinking levels in drinking families were not at all or weakly associated with drinking in offspring. ... Q: Does the familial transmission of drinking patterns persist into young adulthood? A: Maybe\n\nExtractive or Span Extractive The answer is a substring of the passage. In other words, the task is to determine the answer character start and end index in the original passage, as shown in (6)  Generative or Free Form Answer The answer must be generated based on information presented in the passage. Although the answer might be in the text, as illustrated in (7) from Narra-tiveQA (Ko\u010disk\u00fd et al., 2018), no passage index connections are provided.\n\n(7) P: ...Mark decides to broadcast his final message as himself. They finally drive up to the crowd of protesting students, .... The police step in and arrest Mark and Nora.... Q: What are the students doing when Mark and Nora drive up? A: Protesting.", "filtered_refids": [["b4"], [null, "b8"], [], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2154, "num_references": 4}
{"corpusid_sectionid": "231709697-s8", "title": "English Machine Reading Comprehension Datasets: A Survey", "date": "2021-01-25", "section_title": "Quantitative Analysis", "section": "Each dataset's size is shown in Table 1. About onethird of datasets contain 100k+ questions which makes them suitable for training and/or fine tuning a deep learning model. A few datasets contain fewer than 10k samples: MultiRC (9.9k), ARC (7,8k), Shmoop (7.2k), ReClor (6.1k), Open-BookQA(6k), QAngaroo MedHop (2.5k), WikiQA (2k). Every dataset has its own structure and data format. We processed all datasets extracting lists of questions, passages, and answers, including answer candidates, and then use the spaCy 10 tokenizer.\n\nQuestion/Passage/Answer Length The graphs in Fig. 4 provide more insight into the differences between the datasets in terms of answer, question, and passage length, as well as vocabulary size. The outliers are highlighted. 11 The majority of datasets have a passage length under 1500 tokens with the median being 329 tokens but due to seven outliers, the average number of tokens is 1250 ( Fig. 4 (a)). Some datasets (MS MARCO, SearchQA, AmazonYesNo, AmazonQA, MedQA) have a collection of documents as a passage but others contain just a few sentences. 12 The number of tokens in a question lies mostly between 5 and 20. Two datasets, ChildrenBookTest and WhoDid-What, have on average more than 30 tokens per question while WikiReading, QAngaroo MedHop, and WikiHope have only 2 -3.5 average tokens per question ( Fig. 4 (b)). The majority of datasets contain fewer than 8 tokens per answer with the average being 3.5 tokens per answer. The Natu-ralQuestions is an outlier with average 164 tokens per answer 13 (Fig. 4 (c)).\n\nVocabulary Size To obtain a vocabulary size we calculate the number of unique lower-cased token lemmas. A vocabulary size distribution is presented in Fig. 4 (d). There is a moderate correlation 14 between the number of questions in a dataset and its vocabulary size (see Fig. 5). 15 WikiReading has the largest number of questions as well as the richest vocabulary. bAbI is a synthetic dataset with 40k questions but only 152 lemmas in its vocabulary.\n\nLanguage Detection We ran a language detector over all datasets using the pyenchant for American and British English, and langid li- 12 We consider facts (sentences) from ARC and QASC corpora as different passages. 13 We focus on short answers, considering long ones only if the short answer is not available.\n\n14 As the data has a non-normal distribution, we calculated the Spearman correlation: coefficient=0.58, p-value=1.3e-05. 15 The values for the BookTest (Bajgar et al., 2017) and WhoDidWhat (Onishi et al., 2016) are taken from the papers.  braries. 16 In 37 of the 60 datasets, more than 10% of the words are reported to be non-English. 17 We inspected 200 randomly chosen samples from a subset of these. For Wikipedia datasets (Hot-PotQA, QAngoroo WikiHop), around 70-75% of those words are named entities; 10-12% are specific terms borrowed from other languages such as names of plants, animals, etc.; another 8-10% are foreign words, e.g. the word \"dialetto\" from Hot-PotQA \"Bari dialect (dialetto barese) is a dialect of Neapolitan ...\"; about 1.5-3% are misspelled words and tokenization errors. In contrast, for the user-generated dataset, AmazonQA, 67% are tokenization and spelling errors. This aspect of a dataset's vocabulary is useful to bear in mind when, for example, fine-tuning a pre-trained language model which has been trained on less noisy text.\n\nFirst Question Word A number of datasets come with a breakdown of question types based on the first token (Nguyen et al., 2016;Ostermann et al., 2018Ostermann et al., , 2019Ko\u010disk\u00fd et al., 2018;Clark et al., 2019;Xiong et al., 2019;Jing et al., 2019;Bjerva et al., 2020). We inspect the most frequent first word in a dataset's questions excluding cloze-style questions. Table 1 shows the most frequent first word per dataset and Table 2 ", "filtered_refids": [[], [null], [null], [null], [null, "b1"], [null, "b8", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 3825, "num_references": 8}
{"corpusid_sectionid": "231709697-s10", "title": "English Machine Reading Comprehension Datasets: A Survey", "date": "2021-01-25", "section_title": "Evaluation Metrics", "section": "Accuracy is defined as the ratio of correctly answered questions out of all questions. For those datasets where the answer should be found or generated (extractive or generative tasks) accuracy is the same as Exact Match (EM), implying the system answer is exactly the same as the gold answer.\n\nIn contrast with selective and boolean tasks, extractive or generative tasks can have ambiguous, incomplete, or redundant answers. In order to assign credit when the system answer does not exactly match the gold answer, Precision and Recall, and their harmonic mean, F1, can be calculated over words or characters. Accuracy is used for all boolean, multiple choice, and cloze datasets. 19 For extractive and generative tasks it is common to report EM (accuracy) and F1. For cloze datasets, the metrics depends on the form of answer. If there are options available, accuracy can be calculated. If words have to be generated, the F1 measure can also be applied.\n\nOne can view the MRC task from the perspective of Information Retrieval, providing a ranked list of answers instead of one definitive answer. In this case, a Mean Reciprocal Rank (Craswell, 2009) (MRR) and Mean Average Precision (MAP) can be used, as well as the accuracy of the top hit (Hits@1) (single answer) over all possible answers (all entities). 20 All metrics mentioned above work well for welldefined answers but might not reflect performance for generative datasets as there could be several alternative ways to answer the same question. Some datasets provide more than one gold answer. A number of different automatic metrics used in language generation evaluation are also used: Bilingual Evaluation Understudy Score (BLEU) (Papineni et al., 2002), Recall Oriented Understudy for Gisting Evaluation (ROUGE-L) (Lin, 2004), and Metric for Evaluation of Translation with Explicit 19 Except MultiRC as there are multiple correct answers and all of them should be found, and CliCR and ReCoRD which use exact match and F1. This is because even though the task is cloze, the answer should be generated (in case of CliCR) or extracted (ReCoRD). 20 MRR and MAP are used only by (Yang et al., 2015) in the WikiQA dataset, as well as precision, recall and F1. (Miller et al., 2016) in the WikiMovies datasets used the accuracy of the top hit (Hits@1).\n\nORdering (METEOR) (Lavie and Agarwal, 2007). MSMarco, NarrativeQA, and TweetQA are generative datasets which use these metrics. Choi et al. (2018) introduced the human equivalence score (HEQ). It measures the percentage of examples where the system F1 matches or exceeds human F1, implying a system's output is as good as that of an average human. There are two variants: HEQ-Q based on questions and HEQ-D based on dialogues.", "filtered_refids": [[], [null], [null, "b5"], [null, "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 2737, "num_references": 5}
{"corpusid_sectionid": "222133962-s4", "title": "A Survey of Unsupervised Dependency Parsing", "date": "2020-10-04", "section_title": "Models", "section": "A generative approach models the joint probability of the sentence and the corresponding parse tree. Traditional generative models are mostly based on probabilistic grammars. To enable efficient inference, they typically make one or more relatively strict conditional independence assumptions. The simplest assumption (a.k.a. the context-free assumption) states that the generation of a token is only dependent on its head token and is independent of anything else. Such assumptions make it possible to decompose the joint probability into a product of component probabilities or scores, leading to tractable inference. However, they also lead to unavailability of useful information (e.g., context and generation history) in generating each token.\n\nBased on their respective independence assumptions, different generative models specify different generation processes of the sentence and parse tree. Paskin (2002) and Carroll and Charniak (1992) choose to first uniformly sample a dependency tree skeleton and then populate the tokens (words) conditioned on the dependency tree in a recursive root-to-leaf manner. The generation of a child token is conditioned on the head token and the dependency direction. In contrast, Klein and Manning (2004) propose the Dependency Model with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden  propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012;Bisk and Hockenmaier, 2013).\n\nSimilar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity.  use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV.", "filtered_refids": [[], ["b6", "b60", "b44", "b4", "b30", "b2", "b3"], ["b26"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 2939, "num_references": 8}
{"corpusid_sectionid": "222133962-s8", "title": "A Survey of Unsupervised Dependency Parsing", "date": "2020-10-04", "section_title": "Intermediate Representation Encoder Decoder", "section": "Autoencoder CRFAE (Cai et al., 2017) Z P (z|x) P (x|z) D-NDMV (Han et al., 2019a) Deterministic Variant S P (s|x) P (z,x|s)\n\nVariational Autoencoder (Li et al., 2019) Z P (z|x) P (z, x) D-NDMV (Han et al., 2019a) Variational Variant S P (s|x) P (z, x|s) (Corro and Titov, 2018) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation.x is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x.\n\nIn addition to the EM algorithm, the learning objective can also be optimized with gradient descent. Yang et al. (2020) recently observe that gradient descent can sometimes significantly outperform EM when learning neural DMV.\n\nBetter learning results can also be achieved by manipulating the training data. Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences. Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing. Spitkovsky et al. (2013) propose to treat different learning algorithms and configurations as modules and connect them to form a network. Some approaches discussed above, such as Lateen EM and curriculum learning, can be seen as special cases of this approach.", "filtered_refids": [["b20", "b5"], ["b33", "b10", "b20"], ["b60"], ["b57", "b58"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1467, "num_references": 8}
{"corpusid_sectionid": "222133962-s12", "title": "A Survey of Unsupervised Dependency Parsing", "date": "2020-10-04", "section_title": "Variational Autoencoder-Based Approaches", "section": "As mentioned in Section 3.1, the training objective of a generative model is typically the probability of the training sentence and the dependency tree is marginalized as a hidden variable. However, the marginalized probability cannot usually be calculated accurately for more complex models that do not make strict independence assumption. Instead, a variational autoencoder maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability. Since the intermediate representation follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema.\n\nThree unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1). There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure.\n\nRecurrent Neural Network Grammars (RNNG) ) is a transition-based constituent parser, with a discriminative and a generative variant. Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing. Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence. The probability of each operation is calculated by a neural network. Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNNG as the encoder and decoder of a variational autoencoder respectively. However, because RNNG has a strong expressive power, it is prone to overfitting in the unsupervised setting. Li et al. (2019) propose to use posterior regularization to introduce linguistic knowledge as a constraint in learning, thereby mitigating this problem to a certain extent.\n\nThe model proposed by Corro and Titov (2018) is also based on a variational autoencoder. It is designed for semi-supervised dependency parsing, but in principle it can also be applied for unsupervised dependency parsing. The encoder of this model is a conditional random field model while the decoder generates a sentence based on a graph convolutional neural network whose structure is specified by the dependency tree. Since the variational autoencoder needs Monte Carlo sampling to approximate the gradient and the complexity of sampling a dependency tree is very high, Corro and Titov (2018) use Gumbel random perturbation. Jang et al. (2017) use differentiable dynamic programming to design an efficient approximate sampling algorithm.\n\nThe variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector.", "filtered_refids": [[], [], ["b33"], ["b10", "b25"], ["b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3173, "num_references": 4}
{"corpusid_sectionid": "219177284-s2", "title": "Conversational Machine Comprehension: a Literature Review", "date": "2020-06-01", "section_title": "What is Conversational Machine Comprehension?", "section": "The task of CMC is defined as: Given a passage P , the conversation history in the form of questionanswer pairs {Q 1 , A 1 , Q 2 , A 2 , ..., Q i\u22121 , A i\u22121 } and a question Q i , the model needs to predict the answer A i . The answer A i can either be a text span (s i , e i ) (Choi et al., 2018) or a free-form text {a i,1 , a i,2 , ..., a i,j } with evidence R i (Reddy et al., 2019). Single-turn MRC models cannot directly cater to CMC, as the latter is much more challenging to address. The major challenges being:\n\n\u2022 The encoding module needs to encode not only P and A i but also the conversational history.\n\n\u2022 General observation about information-seeking dialog in humans suggests that the starting dialogturns tend to focus on the beginning chunks of the passage and shift focus to the later chunks as the conversation progresses (Choi et al., 2018). The model is thus expected to capture these focal shifts during a conversation and reason pragmatically, instead of only matching lexically or via paraphrasing.\n\n\u2022 Multi-turn conversations are generally incremental and co-referential. These conversational dialogs are either drilling down (the current question is a request for more information about the topic), shifting topic (the current question is not immediately relevant to something previously discussed), returning topic (the current question is asking about a topic again after it had previously been shifted away from), clarification of topic, or definition of an entity (Yatskar, 2019). The model should, therefore, be able to take context from history which may or may not be immediate.", "filtered_refids": [["b6", "b36"], [], ["b6"], ["b42"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1609, "num_references": 4}
{"corpusid_sectionid": "219177284-s4", "title": "Conversational Machine Comprehension: a Literature Review", "date": "2020-06-01", "section_title": "CoQA", "section": "Conversational QA (CoQA) dataset consists of 126k questions sourced from 8k conversations.\n\n\u2022 Dataset preparation: Conversations are prepared over passages collected across 7 different domains, each with its source dataset, such as news articles derived from CNN (Hermann et al., 2015). Amongst the 7 domains, two are used for out-of-domain evaluation (only for evaluation, not training), while the other five aid in-domain evaluation (both training and evaluation). The dialog is prepared in a two annotator setting with one questioning and another answering, both referring to the entire context.\n\n\u2022 Questions: Questions are factoid but require sufficient co-referencing and pragmatic reasoning (Bell, 1999).\n\n\u2022 Answers: Answers are free-form, with their corresponding rationale highlighted in the passage. However, Yatskar (2019) identified that the answers are slightly modified versions of the rationale, and therefore optimizing an extractive model to predict the answer span with maximum F1 overlap to the gold answer can achieve up to 97.8 F1.\n\n\u2022 Dialog features: The dialogs mostly involve drilling-down for details (about 60% of all questions) but lack other dialog features like topic-shift, clarification, or definition.\n\n\u2022 Evaluation: Macro-average F1 score of word overlap is used as an evaluation metric and is computed separately for in-domain and out-of-domain.", "filtered_refids": [[], [null], ["b2"], ["b42"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1378, "num_references": 3}
{"corpusid_sectionid": "219177284-s6", "title": "Conversational Machine Comprehension: a Literature Review", "date": "2020-06-01", "section_title": "Generic Framework of a CMC Model", "section": "(2) reasoning in the neural space to identify the answer vector and (3) decoding the answer vector into a natural language output. Huang et al. (2018a) adapted these steps in CMC by adding conversational history modeling. Qu et al. (2019c) proposed a ConvQA model with separate modules for history selection and modeling. Based on these prior works, we synthesize a generic framework for a CMC model. A typical CMC model is provided with context C, current question Q i and the conversation history\n\n, and needs to generate an output set O i . The CMC framework is provided in Fig. 1. There are four major components of the framework, based on their contribution to the overall CMC flow.\n\n1. History Selection module: With complicated dialog behaviors like topic shift or topic return (Yatskar, 2019), simply selecting immediate turns may not work well. A history selection module, therefore, chooses a subset H i of the history turns H i based on a policy (dynamic or static) that is expected to be more helpful than the others. If the history selection module is based on a dynamic learned policy (e.g. Qu et al. (2019b)), then feedback from the other modules can guide its update.", "filtered_refids": [["b15", "b33"], [], ["b42", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1183, "num_references": 4}
{"corpusid_sectionid": "227230405-s1", "title": "A Survey on Approaches to Computational Humor Generation", "date": 2020, "section_title": "Humor Generation Systems", "section": "Humor generation can be viewed as a special case of automatic text generation. Accordingly, the existing systems can be categorized as belonging to one of two major approaches to text generation: Templates and neural networks. Until Yang and Sheng's publication on an LSTM RNN for joke production in 2017, all of the previous published systems were template-based. While the early template-based systems had to be equipped with handcrafted lexicons, more recent systems have a variety of external lexical resources as source of material for generation.\n\nIt is also worth mentioning that the nature of the humorous texts produced by different systems is rather diverse. While a number of systems was focused on the generation of question-answer jokes (Raskin and Attardo, 1994;Binsted and Ritchie, 1994;Ritchie et al., 2007;Sj\u00f6bergh and Araki, 2008;Hong and Ong, 2009;Labutov and Lipson, 2012), others aimed at creating narrative jokes (Sj\u00f6bergh and Araki, 2009;Yang and Sheng, 2017;Yu et al., 2018). Furthermore, three systems generate humor through lexical replacement, in acronyms (Stock and Strapparava, 2005), proverbs (Sj\u00f6bergh and Araki, 2008) or in SMS (Valitutti et al., 2016), and one system creates witty analogies (Petrovi\u0107 and Matthews, 2013).\n\nIn the following, we will give an overview of existing humor generation systems. Since neural networks have recently achieved state-of-the-art results on many tasks in natural language processing, we will start with the group of systems that apply them for the creation of jokes. This group, however, contains only two systems, whereas the vast majority (ten systems) belongs to the group of templatebased approaches.", "filtered_refids": [[], ["b15", "b51", "b28", "b50", "b35", "b32", "b41", "b45", "b19", "b38", "b37", "b3"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1674, "num_references": 12}
{"corpusid_sectionid": "227230405-s4", "title": "A Survey on Approaches to Computational Humor Generation", "date": 2020, "section_title": "Ontologies for variable selection", "section": "Early generators make use of handcrafted ontologies that are tailored to the specific system. This includes the Light Bulb Joke Generator (LIBJOG, Raskin and Attardo, 1994) that produces jokes of the pattern How many <group name> does it take to screw in a light bulb? <NumberX>. One to <activity 1> and <numberY> to <activity2>. LIBJOG uses a lexicon of social groups and their stereotypical activities to produce jokes such as 'How many Californians does it take to change a light bulb? Twelve. One to screw it in and eleven to share in the experience.'. Over several phases from LIBJOG-1 to LIBJOG-4, the system became incrementally more complex, allowing for more activity fields and less restricted number relations. Binsted and Ritchie (1994) follow a similar approach by basing their punning riddle generator on a manually edited lexicon that stores lexical relationship information, such as synonymy, hyponymy and associated verbs and the phonetical feature homophony. Their system called JAPE-1 uses question-answer templates, as for example What is <adjective> and <verb>?-<noun phrase>. With <noun phrase> as user input, JAPE fills the two remaining slots based on the words' homophony, exploiting phonological ambiguity to induce humorousness. An example of such a joke is: 'What's green and bounces? A spring cabbage!'. Figure 1 illustrates the mechanism of that joke. In total, JAPE has 15 joke templates with schemes.\n\nAnother line of systems uses freely-available ontologies and databases. Resources used by the existing systems are WordNet (Princeton University, 2010) for lexical relationships, ConceptNet (Speer et al., 2017) for semantic relationships and UniSyn (Fitt, 2002) or CMU pronouncing dictionary (Lenzo, 2007) for phonetical relationships. Furthermore, systems make use of thematic word lists for slang words or profanity. With STANDUP, Ritchie et al. (2007) present an extension of JAPE that has a user interface makes use of WordNet and UniSyn instead of the handcrafted lexicon.\n\nHAHAcronym (Stock and Strapparava, 2005) is a system that writes out common acronyms in a humorous manner. For example, the acronym FBI (Federal Bureau of Investigation) becomes Fantastic Bureau of Intimidation. HAHAcronym makes use of WordNet, augmented with domain labels for the entries and the CMU pronunciation dictionary to take into account word rhymes and rhythms. The systems parses a given acronym to detect the syntactical form and the highest-ranking noun phrase, which remains unchanged. The other words get substituted while preserving the initial letter, the word class as well as the overall rhyme and rhythm. A substitution is chosen by exploiting semantic field opposition.\n\nSj\u00f6bergh and Araki's (2009) ambiguous compound generator also leverages WordNet to create jokes, this time however focusing less on inter-word relationships, but more on the provided definitions and example sentences. The system consists of two modules, creating two types of jokes, each making use of possibly ambiguous compound nouns. The first module creates jokes with the template 'I saw a <noun1> <noun2>. She (<noun1>) <WordNet example sentence>.' For the two noun slots, the system selects compound nouns from WordNet where both parts also occur as single entries, the first as noun and the second as noun and verb. For the verb, one of the example sentences is extracted in order to complete the template to a joke such as 'I saw a fish stick. She (the fish) stuck her thumb in the crack. ' The audience initially assumes that fish stick is a composite, as this is the more obvious interpretation. However, this expectation is disrupted in the second sentence, when the rear part of the supposed composite turns out to be a verb. The first sentence is now reanalyzed, and the incongruity is resolved. The Figure 1: Illustrated example of the joke mechanism in JAPE. From: Binsted and Ritchie, 1994 second type of joke Sj\u00f6bergh an Araki's system produces, follows the template '<WordNet definition <noun1> <noun2>>.' For this, compounds are selected when they contain at least one word that is included in WordNet itself, and by changing one letter that word must become another word present in WordNet. This compound is combined with one entry that has the original compound in its WordNet definition. From the exemplary compound 'god of war' this would render 'Ares: (Greek mythology) Greek god of car'. To reduce the number of jokes, the authors implement a measure of funniness and only output jokes above a certain threshold. The measure is calculated by the relation of the frequency of the selected compound between a joke corpus and a non-humorous corpus.\n\nHong and Ong's (2009) system uses WordNet's relation synonymy and UniSyn's pronunciation information alongside with any semantic relationship retrieved from ConceptNet. Their system creates punning riddles similar to Binsted and Ritchie's JAPE. This time, however, the algorithm is designed to first learn question-answer joke patterns from examples and then create new jokes according to the patterns. To create joke patterns, the algorithm called T-PEG marks nouns, adjectives and verbs in every joke from a POS-tagged joke corpus as candidate variables. For every tuple of candidate variables in a joke, T-PEG determines the lexical, phonetical or semantical relationship between the two variables. Those candidate variables with at least one relationship with another candidate are the final variables in the learned template. For example, from the source pun 'Which bird can lift the heaviest weights? The crane.', T-PEG extracts the sentence template 'Which <X1> can <X3> the heaviest <X6>? The <Y1>.' and the word relationships 'X1 ConceptuallyRelatedTo X6', 'X6 ConceptuallyRelatedTo X1', 'Y1 IsA X1', 'X6 CapableOfReceivingAction X3', 'Y1 CapableOf X3' and 'Y1 UsedFor X3'. From the 39 templates the algorithm extracted, the authors found 27 (69 %) to be usable. To generate jokes, T-PEG uses the library of patterns and WordNet, Unisyn and ConceptNet to find fitting words for the slots and a keyword input from the user as a starting point.", "filtered_refids": [["b32", "b3"], ["b11", "b39", "b20", "b35"], ["b41"], [null, "b3"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 51, "num_chars": 6130, "num_references": 9}
{"corpusid_sectionid": "227230405-s7", "title": "A Survey on Approaches to Computational Humor Generation", "date": 2020, "section_title": "Natsu to ieba, natsukashii Nose de, kyuuyuu to waiwai! (Speaking of summer, it will be fun to meet some old friends in my beloved Nose [a town near Osaka]!)", "section": "Similar to Stock and Strapparava's (2005) HAHAcronym, Valitutti et al. (2016) generate humorous texts by word substitution. They substitute a single word in a given short text message (SMS) that serves as a template. Their 2016 study is an enhanced version of a 2013 paper by the same authors (Valitutti et al., 2013). They assume different kinds of lexical constraints that determine the funniness of a substitution: (1) Taboo: The substitution is a taboo word (dirty word e.g.) or connoted with a taboo. (2) Coherence: The substitute forms a coherent compound with its neighbors. (3) Position: The substitution takes place among the last words of the text. (4) Form: The substitute is phonetically or orthographically similar to the original word. They use WordNet, the CMU pronunciation dictionary, the Google n-gram corpus after 1990, two online dictionaries of slang words and an example list of funny autocorrection mistakes from the internet, to find suitable candidates and check them for compliance with the constraints. As input for the manipulation they use an SMS Corpus. An example of a joke that leverages the constraints form and position is 'Tmr u going to school? I meet u in pool?' (school/pool replacement).", "filtered_refids": [["b45", "b41", "b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1226, "num_references": 3}
{"corpusid_sectionid": "7384097-s2", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "Phrase-Based SMT", "section": "Phrase-based SMT (PSMT) is the currently dominant approach in string-based SMT. PSMT ruled out the early word-based SMT framework (Brown et al. 1990(Brown et al. , 1993Berger et al. 1996) thanks to two important novelties: the use of multi-word translation units (Och 1999;Zens, Och, and Ney 2002;Koehn, Och, and Marcu 2003), and the move from a generative to a discriminative modeling framework . The search process (1) in PSMT is guided by the target string e, built from left to right, and the alignment variable b that embeds both segmentation and reordering of the source phrases. This is defined as\n\nsuch that K 1 , . . . , K I are consecutive intervals partitioning the target word positions, and J 1 , . . . , J I are corresponding but not necessarily consecutive intervals partitioning the source word positions. A phrase segmentation for our running example is shown in Figure 2. The use of phrases mainly results in a better handling of ambiguous words and many-to-many word equivalences, but it also makes it possible to capture a considerable amount of local reordering phenomena within a translation unit (intra-phrase 3 Automatic measures of translation quality are discussed in Section 3. ", "filtered_refids": [["b115", "b6", "b161", "b21", "b22", "b95"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1205, "num_references": 6}
{"corpusid_sectionid": "7384097-s6", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "n-gram Based SMT", "section": "n-gram based SMT (Casacuberta and Vidal 2004;Mari\u00f1o et al. 2006) is a string-based alternative to PSMT. In this framework, smoothed n-gram models are learned over sequences of minimal translation units (called tuples), which, like phrase pairs, are pairs of word sequences extracted from word-aligned parallel sentences. Tuples, however, are typically shorter than phrase pairs and are extracted from a unique, monotonic segmentation of the sentence pair. Thus, the problem of spurious phrase segmentation is avoided but non-local reordering becomes an issue. For instance, in Figure 2, a monotonic phrase segmentation could be achieved only by treating the large block [jdd ... AlsAds]-[The ... renewed] as a single tuple. Reordering is then addressed by \"tuple unfolding\" (Crego, Mari\u00f1o, and de Gispert 2005): that is, during training the source words of each translation unit are rearranged in a target-like order so that more, shorter tuples can be extracted. At test time, input sentences have to be pre-ordered for translation. To this end, Crego and Mari\u00f1o (2006) propose to precompute a number of likely permutations of the input using POS-based rewrite rules learned during tuple unfolding. The reorderings thus obtained are used to extend the search graph of a monotonic decoder. 8 Reordering is often considered as a shortcoming of n-gram-based SMT as reordering decisions are largely decoupled from decoding and mostly based on source-side information.", "filtered_refids": [["b105", "b38", "b39", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1464, "num_references": 4}
{"corpusid_sectionid": "7384097-s8", "title": "A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena", "date": "2015-02-17", "section_title": "Syntax-Based", "section": "SMT. An important motivation for using syntax in SMT is that reordering among natural languages very often involves the permutation of whole syntactic constituents (e.g., Fox 2002). For instance, in our running example (Figure 2), knowing the span of the Arabic subject would be enough to predict the reordering of the verb for translation into English.\n\nSyntax-based SMT encompasses a variety of frameworks that use syntactic annotation either on the source or on the target language, or both. So-called tree-to-string methods (Huang, Knight, and Joshi 2006;Liu, Liu, and Lin 2006) use a given input sentence parse tree to restrict the application of translation/reordering rules to word spans that coincide with syntactic constituents of specific categories. For instance, the swap of Alr}ys Alfrnsy may only be dictated by a rule applying to noun phrases composed of a noun and an adjective. On the other hand, string-to-tree methods (Yamada and Knight 2002;Galley et al. 2004;Marcu et al. 2006;Shen, Xu, and Weischedel 2010) use syntax as a way to restrict translation hypotheses to well-formed target language sentences-ruling out, for instance, a translation that fails to reorder the translated verb renewed with respect to its subject. Using syntax on both source and target sides (treeto-tree) (Imamura, Okuma, and Sumita 2005;Ding and Palmer 2005;Smith and Eisner 2006;Watanabe, Tsukada, and Isozaki 2006;Zhang et al. 2008) has proven rather difficult in practice due to the complexity of aligning potentially very different tree topologies and to the large size of the resulting translation grammars. Moreover, the need for highquality parsers in both language sides seriously limits the applicability of this approach.\n\nSyntax-based SMT approaches also differ in the formalism they use to represent the trees. Those based on phrase structure (constituency) grammars typically comply with the principle that each translation/reordering rule should match a complete constituent, whereas those based on dependency grammars opt for a more flexible use of structure. For example, in string-to-dependency SMT (Shen, Xu, and Weischedel 2010) rules can correspond to partial constituents but must be either a single rooted tree, with each child being a complete sub-tree, or a sequence of siblings, each being a complete subtree. Partial dependency rules are then combined during decoding, which means that not all reordering decisions are governed by the translation model.\n\nAn even more flexible use of structure is advocated by the treelet-based SMT framework (Quirk, Menezes, and Cherry 2005), where translation rules can correspond to any connected subgraph of the dependency tree (i.e., treelet). As illustrated by Figure 4, treelet pairs are extracted from pairs of source dependency parse tree and target-side projected trees. Treelets can be seen as phrases that are not limited to sets of adjacent words, but rather to sets of words that are connected by dependency relations, which in turn make it possible to learn non-local reordering patterns. As reordering jdd AlEAhl Almgrby Almlk mHmd AlsAds dEm -h l-m$rwE Alr}ys Alfrnsy! ", "filtered_refids": [["b59"], ["b43", "b141", "b104", "b153", "b79", "b82", "b127", "b102", null, "b163", "b61"], ["b127"], ["b119"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 3144, "num_references": 14}
{"corpusid_sectionid": "10205309-s7", "title": "When Conset meets Synset: A Preliminary Survey of an Ontological Lexical Resource based on Chinese Characters", "date": "2006-07-17", "section_title": "Hanzi-grounded Lexicon and Ontology", "section": "The current lexicon contains over 5000 characters, and 30,000 derived words in total. 3 The building of the lexical specification of the entries in HanziNet includes various aspects of Hanzi:\n\n1. Conset(s): The conceptual code is the core part of the MRD lexicon in HanziNet. Concepts in HanziNet are indicated by means of a label (conset name) with a code form. In order to increase the efficiency, an ideal strategy is to adopt the Huffmann-coding-like method, by encoding the conceptual structure of Hanzi as a pattern of bits set within a bit string. 4 The coding thus refers to the assignment of code sequences to an character. The sequence of edges from the root to any character yields the code for that character, and the number of bits varies from one character to another. Currently, for each conset (309 in total) there are 12 characters assigned on the average; for each character, it is assigned to 3 Since this lexicon aims at establishing an knowledge resource for modern Chinese NLP, characters and words are mostly extracted from the Academia Sinica Balanced Corpus of Modern Chinese (http://www.sinica.edu.tw/SinicaCorpus/), those characters and words which have probably only appeared in classical literary works, (considered ghost words in the lexicography), will be discarded. 4 This is inspired by Chu (1999)'s works. 3. Shallow parts of speech (mainly Nominal(N) and Verbal(V) tags) 4. Gloss of prototypical meaning 5. List of combined words with statistics calculated from corpus, and 6. Further aspects such as character types and cognates: According to ancient study, characters can be compartmentalized into six groups based on the six classical principles of character construction. Character type here means which group the character belongs to. And the term cognate here is defined as characters that share the same CSH or CSM.  The second core component of the proposed resource is a set of hierarchically related Top Concepts called Top-level Ontology (or Upper ontology). This is similar to EuroWordnet 1.2, which is also enriched with the Top Ontology and the set of Base Concepts (Vossen 1998).\n\nAs mentioned, a tentative set of 309 conset, a kind of ontological categories in contrast with synset has been proposed 7 , and over 5000 characters have been used as instances in populating the character ontology.\n\nMethodologically, following the basic line of OntoClear approach (Guarino and Welty (2002)), we use simple monotonic inheritance in our ontology design, which means that each node inherits properties only from a single ancestor, and the inherited value cannot be overwritten at any point of the ontology. The decision to keep the relations to one single parent was made in order to guarantee that the structure would be able to grow indefinitely and still be manageable, i.e. that the transitive quality of the relations between the nodes would not degenerate with size. Figure 4 shows a snapshot of the character ontology. Figure 4: The character ontology: a snapshot", "filtered_refids": [[null], [null, "b3"], [], ["b4"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3015, "num_references": 4}
{"corpusid_sectionid": "239015827-s14", "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models", "date": "2021-10-16", "section_title": "B Bias Attribute Words", "section": "Below, we list the bias attribute words we use for CDA, SentenceDebias, and INLP.\n\nGender (Zhao et al., 2018).\n\n(actor, actress), (actors, actresses), (airman, airwoman), (airmen, airwomen), (uncle, aunt), (uncles, aunts), (boy, girl), (boys, girls), (groom, bride), (grooms, brides), (brother, sister), (brothers, sisters), (businessman, businesswoman), (businessmen, businesswomen), (chairman, chairwoman), (chairmen, chairwomen), (dude, chick), (dudes, chicks), (dad, mom), (dads, moms), (daddy, mommy), (daddies, mommies), (son, daughter), (sons, daughters), (father, mother), (fathers, mothers), (male, female), (males, females), (guy, gal), (guys, gals), (gentleman, lady), (gentlemen, ladies), (grandson, granddaughter), (grandsons, granddaughters), (guy, girl), (guys, girls), (he, she), (himself, herself), (him, her), (his, her), (husband, wife), (husbands, wives), (king, queen), (kings, queens), (lord, lady), (lords, ladies), (sir, maam), (man, woman), (men, women), (sir, miss), (mr., mrs.), (mr., ms.), (policeman, policewoman), (prince, princess), (princes, princesses), (spokesman, spokeswoman), (spokesmen, spokeswomen)\n\nRace. (black, caucasian, asian), (african, caucasian, asian), (black, white, asian), (africa, america, asia), (africa, america, china), (africa, europe, asia)\n\nReligion (Liang et al., 2020).\n\n(jewish, christian, muslim), (jews, christians, muslims), (torah, bible, quran), (synagogue, church, mosque), (rabbi, priest, imam), (judaism, christianity, islam) ", "filtered_refids": [[], [null], [], [], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 3, "num_chars": 1495, "num_references": 3}
{"corpusid_sectionid": "247627890-s2", "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions", "date": "2022-03-22", "section_title": "Initial Instruction", "section": "In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as \"Go upstairs and pass the table in the living room. Turn left and go through the door in the middle.\" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.\n\nSome work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.", "filtered_refids": [["b16", null, "b19", "b25"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1916, "num_references": 5}
{"corpusid_sectionid": "247627890-s3", "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions", "date": "2022-03-22", "section_title": "Coarse-grained Navigation", "section": "In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle). Usually, instructions are more concise and contain merely information of the target goal.\n\nRoomNav (Wu et al., 2018) requires agent navigate according to instruction \"go to X\", where X is a predefined room or object.\n\nIn Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question. The instructions in REVERIE (Qi et al., 2020b) are annotated by humans, and thus more complicated and diverse. The agent navigates through the rooms and differentiates the object against multiple competing candidates. In SOON (Zhu et al., 2021a), an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.\n\nNavigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available). In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple). Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question. Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment. CHAI (Misra et al., 2018) requires the agent to navigate and simply interact with the environments.", "filtered_refids": [[], ["b16"], [null, "b25"], [null, "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1652, "num_references": 5}
{"corpusid_sectionid": "247627890-s9", "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions", "date": "2022-03-22", "section_title": "Semantic Understanding", "section": "Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.\n\nIntra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).\n\nInter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).", "filtered_refids": [[], ["b4", "b28", "b21", null, "b22"], ["b21", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1361, "num_references": 7}
{"corpusid_sectionid": "248426721-s2", "title": "What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification", "date": "2022-04-28", "section_title": "The Relation Extraction Task", "section": "Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.\n\nOne way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taill\u00e9 et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.\n\nPipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taill\u00e9 et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.", "filtered_refids": [[], ["b32", null, "b7", "b3", "b36"], ["b43", "b54", "b44", "b56", null, "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 36, "num_chars": 5215, "num_references": 11}
{"corpusid_sectionid": "235422524-s1", "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP", "date": "2021-06-14", "section_title": "Data Augmentation for NLP", "section": "Data augmentation increases both the amount (the number of data points) and the diversity (the variety of data) of a given dataset (Cubuk et al., 2019). Limited labeled data often leads to overfitting on the training set and data augmentation works to alleviate this issue by manipulating data either automatically or manually to create additional augmented data.Such techniques have been widely explored in the computer vision field, with methods like geometric/color space transformations (Simard et al., 2003;Krizhevsky et al., 2012;Taylor and Nitschke, 2018), mixup (Zhang et al., 2018), and random erasing (Zhong et al., 2020;DeVries and Taylor, 2017). Although the discrete nature of textual data and its complex syntactic and semantic structures make finding labelpreserving transformation more difficult, there nevertheless exists a wide range of methods for augmenting text data that in practice preserve labels. In the following subsections, we describe four broad classes of data augmentation methods:", "filtered_refids": [["b50", null, "b22", "b42", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1012, "num_references": 5}
{"corpusid_sectionid": "235422524-s2", "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP", "date": "2021-06-14", "section_title": "Token-Level Augmentation", "section": "Token-level augmentations manipulate words and phrases in a sentence to generate augmented text while ideally retaining the semantic meaning and labels of the original text.\n\nDesigned Replacement. Intuitively, the semantic meaning of a sentence remains unchanged if some of its tokens are replaced with other tokens that have the same meaning. A simple approach is to fetch synonyms as words for substitutions (Kolomiyets et al., 2011;Yang, 2015;Zhang et al., 2015a;Wei and Zou, 2019;Miao et al., 2020). The synonyms are discovered based on pre-defined dictionaries such as WordNet (Kolomiyets et al., 2011), or similarities in word embedding space (Yang, 2015). However, improvements from this technique are usually minimal (Kolomiyets et al., 2011) and in some cases, performance may even degrade (Zhang et al., 2015a). A major drawback stems from the lack of contextual information when fetching synonyms-especially for words with multiple meanings and few synonyms. To resolve this, language models (LMs) have been used to replace the sampled words given their context (Kolomiyets et al., 2011;Fadaee et al., 2017;Kobayashi, 2018;Kumar et al., 2020). Other work preserves the labels of the text by conditioning on the label when generating the LMs' predictions (Kobayashi, 2018;Wu et al., 2019a). In addition, different sampling strategies for word replacement have been explored. For example, instead of sampling one specific word from candidates by LMs, Gao et al. (2019) propose to compute a weighted average over embeddings of possible words predicted by LMs as the replaced input since the averaged representations could augment text with richer information.\n\nRandom Insertion, Replacement, Deletion and Swapping. While well-designed local modifications can preserve the syntax and semantic meaning of a sentence (Niu and Bansal, 2018), random local modifications such as deleting certain tokens (Iyyer et al., 2015;Wei and Zou, 2019;Miao et al., 2020), inserting random tokens (Wei and Zou, 2019;Miao et al., 2020), replacing non-important tokens with random tokens (Xie et al., 2017(Xie et al., , 2020Niu and Bansal, 2018) or randomly swapping tokens in one sentence (Artetxe et al., 2018;Lample et al., 2018;Wei and Zou, 2019;Miao et al., 2020) can preserve the meaning in practice. Different  (2017)  kinds of operations can be further combined (Wei and Zou, 2019), where each example is randomly augmented with one of insertion, deletion, and swapping. These noise-injection methods can efficiently be applied to training, and show improvements when they augment simple models trained on small training sets. However, the improvements might be unstable due to the possibility that random perturbations change the meanings of sentences (Niu and Bansal, 2018). Also, finetuning large pre-trained models on specific tasks might attenuate improvements due to preexisting generalization abilities of the model (Shleifer, 2019).\n\nCompositional Augmentation. To increase the compositional generalization abilities of models, recent efforts have also focused on compositional augmentations (Jia and Liang, 2016; Andreas, 2020) where different fragments from different sentences are re-combined to create augmented examples. Compared to random swapping, compositional augmentation often requires more carefully-designed rules such as lexical overlap (Andreas, 2020), neural-symbolic stack machines (Chen et al., 2020e), and neural program synthesis (Nye et al., 2020). With the potential to greatly improve the generalization abilities to out-of-distribution data, compositional augmentation has been utilized in sequence labeling (Guo et al., 2020), semantic parsing (Andreas, 2020; Nye et al., 2020;Furrer et al., 2020), language modeling (Andreas, 2020; Shaw et al., 2020), and text generation (Feng et al., 2020).", "filtered_refids": [[], ["b28", "b47", "b29", "b35", null], ["b28", "b31", "b32", null, "b16"], ["b14", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3822, "num_references": 12}
{"corpusid_sectionid": "235422524-s3", "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP", "date": "2021-06-14", "section_title": "Sentence-Level Augmentation", "section": "Instead of modifying tokens, sentence-level augmentation modifies the entire sentence at once.\n\nParaphrasing. Paraphrasing has been widely adopted as a data augmentation technique in various NLP tasks (Yu et al., 2018;Xie et al., 2020;Kumar et al., 2019;He et al., 2020;Chen et al., 2020b,c;Cai et al., 2020), as it generally provides more diverse augmented text with different word choices and sentence structures while preserving the meaning of the original text. The most popular is round-trip translation (Sennrich et al., 2015;Edunov et al., 2018) -Tavor et al., 2020;Zhang and Bansal, 2019;Kumar et al., 2020;). An extra filtering process is often used to ensure high-quality augmented data. For example, in text classification, Anaby-Tavor et al. (2020) first fine-tune GPT-2 (Radford et al., 2019) with the original examples prepended with their labels, and then generate augmented examples by feeding the finetuned model certain labels. Only confident examples as judged by a baseline classifier trained on the original data are kept. Similarly, new answers are generated on the basis of given questions in question answering and are filtered by customized metrics like question answering probability (Zhang and Bansal, 2019) and n-gram diversity . Generative models used in this setting have been based on conditional VAE (Bowman et al., 2016;Hu et al., 2017;Guu et al., 2017;Malandrakis et al., 2019), GAN (Iyyer et al., 2018;Xu et al., 2018) or pre-trained language models like GPT-2 (Anaby- Tavor et al., 2020;Kumar et al., 2020). Overall, these conditional generation methods can create novel and diverse data that might be unseen in the original dataset, but require significant training effort.  (2020) proposes two simple yet effective adversarial transformations that reverse the position of subject and object or the position of premise and hypothesis.", "filtered_refids": [[], ["b13", "b31", "b45", null, "b37", "b33", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1871, "num_references": 7}
{"corpusid_sectionid": "253736389-s1", "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications", "date": "2023-03-01", "section_title": "Terminology and Overview", "section": "We focus on tabular data that come in the form of a where all cells in a column share the same atomic type, for example, the relational table in Table 1.\n\nAn entity table has the same properties, but with a horizontal structure with only one entity whose properties are organized as rows. Spreadsheets, or matrix tables, are the most general kind with information that can be organized both horizontally and vertically, possibly with hierarchies in the header and formatting metadata, such as in financial tables. Tables can have rich metadata, such as attribute types (e.g., DATE), domain constraints, functional dependencies across columns and integrity constraints such as primary keys. In the table sample in Figure 1, column Country is a primary key. Most systems focus on a single table, with or without metadata. However, a few systems, such as GTR (Wang et al., 2021a), GRAPPA , and DTR (Herzig et al., 2021), consume databases, which are collections of relational tables, possibly under referential constraints. We identify as input the table(s) and its context. The context is a text associated to the table. Depending on the dataset and task at hand, it varies from table metadata, the text surrounding the tables or their captions up to questions, expressed in natural language, that can be answered with the tabular data (Badaro and Papotti, 2022).\n\nThe main advantage of the transformer architecture is its ability to generate a LM, a large neural network, with self-supervised pre-training. This pre-trained LM is then usually followed by supervised fine-tuning to adapt it to the target task with a small amount of training data. While transformers have proven to be effective in modeling textual content, tables have a rich structure that comes with its own relationships, such as those across values in the rows and attributes. New solutions are therefore needed to jointly model the characteristics of the table, its text content, and the text in the table context. As shown in Figure 1, we distinguish two main phases to spell out these contributions. First, we focus on the development of tabular LMs by using transformer-based deep neural networks (1). Given a table and its context, the goal is to learn a pre-trained representation of the structured data (cell values, rows, attributes) in a continuous vector space. We then discuss the use of those representations in the downstream tasks (2). Figure 1 shows the reference pipeline and the aspects that we propose to model existing systems.\n\n\u2022 Training Datasets (Sec. 3): the datasets used for pre-training and fine-tuning the models toward specific tasks; datasets for the latter case usually come with annotations and/or labels.\n\n\u2022 Input Processing (Sec. 4): the steps to prepare the data for the model processing, such as the transformation from the two dimensional tabular space to one dimensional input.\n\n\u2022 Transformer-based Encoder (Sec. 5): the pre-training objectives and customization of the typical transformer-based deep learning architecture.\n\n\u2022 Downstream Task Model (Sec. 6): the models consuming the representations or fine-tuning them to tackle downstream tasks.\n\n\u2022 Tabular Language Model (Sec. 7): the output representations, including at the token, row, column, table level, and their usage.\n\nFor the tasks consuming the models, we report on ", "filtered_refids": [[], ["b48", "b99", "b101"], [], [], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 25, "num_chars": 3335, "num_references": 3}
{"corpusid_sectionid": "253736389-s2", "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications", "date": "2023-03-01", "section_title": "Training Datasets", "section": "We present both the datasets used for pre-training and for fine-tuning in the downstream tasks. Pre-training tables are not annotated, in some cases scraped from the web, while data used for fine-tuning have task-dependent annotation labels. The datasets consist of tables and their context, such as table metadata, surrounding texts, claims or questions. To construct large pre-training datasets and in an attempt to reduce bias, multiple sources can be used, independently of the target task at hand. For instance, unlike TAPAS (Herzig et al., 2020), which only uses Wikipedia Tables for QA, and TABULARNET , which only uses Spreadsheets for TMP, TABERT  uses Wikipedia Tables and WDC  for SP; GRAPPA uses Wikipedia Tables, Spider, and WikiSQL for SP; MMR (Kosti\u0107 et al., 2021) uses NQ, OTT-QA, and WikiSQL for TR; MATE  uses Wikipedia Tables and HybridQA for QA; and TUTA  uses Wikipedia Tables, WDC, and spreadsheets for TMP. In general, it is recommended to utilize different data sources for pre-training to ensure covering different kinds and content, and thus, improve the scope of representations. For instance, Wikipedia tables has a large number of relational tables (Bhagavatula et al., 2015), while WDC and Spreadsheets include also entity tables and spreadsheets with complex structure. Table 2 summarizes the main characteristics for the most common datasets. We mark the tasks for which the dataset has been used by \u2714 under the column ''Task''. We note that the top four datasets are mostly used for pre-training, while the others can be used for fine-tuning as well since they include annotations for the target task, for example, questions/answers for QA. The column ''Large Tables'' is a binary indicator, where \u2714 and \u2718 indicate whether or not the tabular corpus include large tables and hence whether or not some pre-processing is needed to reduce table content to meet the limits of the transformer architecture (512 input tokens in most cases). Some works, such as TABERT, TAPEX (Liu et al., 2021a), and CLTR (Pan et al., 2021), apply filtering in any case to reduce noisy input. Finally, the ''Context'' column describes additional text that come with the tables. This can be text describing the table, such as caption or title of the document containing the table; table metadata, such as table orientation, header row, and keys; or questions and claims that can be addressed with the table.", "filtered_refids": [["b11", "b49", "b62", "b74", "b68"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2415, "num_references": 5}
{"corpusid_sectionid": "253736389-s10", "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications", "date": "2023-03-01", "section_title": "Vanilla Transformer", "section": "The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Residual connections and layer-normalization modules are also used. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.\n\nThe transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only , or decoder-only (Radford et al., 2019;Brown et al., 2020) model. The choice of the architecture depends on the final task. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).", "filtered_refids": [["b90", "b97"], ["b78", "b77", "b97", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1335, "num_references": 6}
{"corpusid_sectionid": "251402499-s8", "title": "Abstractive Meeting Summarization: A Survey", "date": "2022-08-08", "section_title": "Evaluation methods", "section": "As noted in Section 2, the subjectivity of meeting summaries, stemming in part from the preference for abstractive summarization, complicates the evaluation task. Evaluation requires both verifying that the content in a system-based summary follows from the original transcript and measuring the overlap with a gold-standard summary. However, given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used.\n\nUnfortunately, the ROUGE metric (Lin, 2004), which remains the standard for both meeting and general text summarization, scores systemproduced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries. If we take the abstractive summary from Figure 1 (\"Some part of the casing will be made of a spongy material\") as a gold example, ROUGE would assign a higher score to a system that produces \"Some part of the casing will be made of broccoli\" than one that output \"A portion of the outer layer will be constructed from a sponge-like material,\" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false).\n\nA reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap. Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text. Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.\n\nDespite their rapid development, recent studies on meta-evaluation of these metrics show mixed results. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate. Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.\n\nClearly, evaluation is itself a very challenging task. And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency. When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight. Otherwise, the resulting summaries are not reliable for an end user. While we do not know of current work that focuses on evaluation of factuality explicitly for the meeting domain, the study of factual consistency in summarization more generally is a budding research area that we discuss in Section 6.\n\nIn the absence of a clear winner for summary evaluation metrics, none of the alternatives has yet to be widely adopted. In our comparison of different summarization systems in Section 5, we therefore stick with ROUGE, which offers the additional advantage of conceptual simplicity and lower computational cost compared to metrics based on contextual embeddings and pretrained language models.", "filtered_refids": [[], ["b17"], ["b52", "b55", "b27", "b49", null], ["b6", "b7"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3903, "num_references": 8}
{"corpusid_sectionid": "251402499-s10", "title": "Abstractive Meeting Summarization: A Survey", "date": "2022-08-08", "section_title": "Interpretation", "section": "Consider the following (fabricated) exchange that we might imagine taking place in the AMI meeting from Figure 1:\n\n(1) a. PM: So what color should the removable cover be? b. ID: I think we should offer a few options. What about raspberry, lime and blueberry and then black for those who don't like color? c. UI: Sounds good to me. d. ME: Me too. e. PM: OK. Let's go with that.\n\nThere is a very clear decision that has been made in (1), but what information allows us to recognize this decision so easily? The acknowledgement in (1-e) explicitly confirms the decision and thus plays a crucial role in inferring that a decision has been made, but it does not tell us what decision has been made.\n\nIn fact, none of the utterances (1-a)-(1-e) alone allow us to infer the decision. We must rather understand that (1-b) provides an answer to the question asked in (1-a) and that (1-e) is an acknowledgement of the suggestion in (1-b) and of the positive answers in (1-c) and (1-d). (If instead of agreeing in (1-c) and (1-d), the UI and ME had presented and defended an alternative proposal, we might have understood (1-e) as an acknowledgement of their suggestion rather than of (1-b).)\n\nBecause how an utterance contributes to the larger conversational context is often crucial for understanding the conversation as a whole, some summarization approaches, which we review in Section 5.1.1, enrich meeting transcripts with ex-plicit representations of those contributions. Other summarization accounts exploit other types of information relevant to discourse interpretation. HMNet (Zhu et al., 2020) and DDAMS (Feng et al., 2020), for instance, use information about speakers and turns (\"who said what\"), drawing on the fact that speaker information can help to convert (frequent) occurrences of first and second person pronouns into third person pronouns, as is necessary for summarization (Luo et al., 2009). Further interpretation-focused methods are studied in Section 5.1.2, in which we take a look at accounts that have opted to augment transcripts with non-linguistic information of multi-modal nature, such as information about the eye-gaze of conversational participants.  (1). Node b 1 represents the first sentence of (1-b), b 2 , the second, and similarly for e 1 and e 2 . The dashed box indicates that the PM's \"OK\" in (1-e) acknowledges and accepts the entire exchange from (1-b) to (1-d).", "filtered_refids": [[], [], [], [], ["b20", null, "b60"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2399, "num_references": 3}
{"corpusid_sectionid": "251402499-s11", "title": "Abstractive Meeting Summarization: A Survey", "date": "2022-08-08", "section_title": "Discursive information", "section": "While most work on abstractive meeting summarization implicitly assumes that conversation is merely a linear sequence of utterances without semantic relations between them, example (1) underscores the importance of semantic relations for conversational understanding. Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization. Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse. Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations. Figure 2 il-  , 2021a). The * indicates summary-level (with sentence split) ROUGE-L score (Lin, 2004).\n\nlustrates a possible SDRT graph for example (1).\n\nTo the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries. They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora. Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices. Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary. A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.\n\nAn alternative approach to discourse interpretation that developed largely independently of RST and SDRT is dialogue act classification. Detailing the differences between the approaches is out of the scope of this paper, but in a nutshell, dialogue acts provide a shallower notion of discourse structure (Jurafsky et al., 1997) in that they do not entail a full graph structure over a conversation. On the other hand, systems for dialogue act labeling, such as DAMSL (Allen and Core, 1997; Core and Allen, 1997) or DiAML (Bunt et al., 2010(Bunt et al., , 2012, place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting or informing.\n\nBoth the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.\n\nDialogue acts have also been used to good effect for summarizing decisions. Fern\u00e1ndez et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution. Then, key fragments of the decision related utterances are retained to form an extractive decision summary. Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007.", "filtered_refids": [["b21", "b1", null, "b17"], [], [null, "b2", "b31"], ["b11", null, "b10", "b0"], [], [null, "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3926, "num_references": 13}
{"corpusid_sectionid": "256662721-s2", "title": "Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems", "date": "2023-02-07", "section_title": "Training Strategies of LMRS", "section": "Given the significant impact that PLMs have had on NLP tasks in the pre-train and fine-tune paradigm, there has been a surge recently in adapting such paradigms to multiple recommendation tasks. As illustrated in Figure 1, there are mainly two classes regarding different training paradigms: pre-train, fine-tune paradigm and prompt learning paradigm. Each class is further classified into subclasses regarding different training efforts on different parts of the recommendation model. This section will go through various training strategies w.r.t. specific recommendation purposes. Figure  2(a) presents the statistics of recent publications of LMRSs grouped by different training strategies and the total number of published research works each year. Figure 2(b) shows the taxonomy and some corresponding representative LMRSs.\n\n4.1 Pre-train, fine-tune paradigm for RS\n\nThe \"pre-train, fine-tune\" paradigm attracts increasing attention from researchers in the recommendation field due to several advantages: 1) Pre-training provides a better model initialization, which usually leads to better generalization on different downstream recommendation tasks, improves recommendation performance from various perspectives, and speeds up convergence on the fine-tuning stage; 2) Pre-training on huge source corpus can learn universal knowledge which can be beneficial for the downstream recommenders; 3) Pre-training can be regarded as a kind of regularization to avoid overfitting on lowresource, and small datasets (Erhan et al., 2010).\n\nPre-train This training strategy can be seen as traditional end-to-end training with domain input. Differently, we only focus on research works adapting LM-based learning objectives into the training phase. Many typical LM-based RSs fall into this category, such as BERT4Rec , which models sequential user behaviour with a bidirectional self-attention network through Cloze task, and Transformers4Rec (de Souza Pereira Moreira et al., 2021) which adopts a haggingface transformer-based architecture as the base model for next-item prediction and explores four different LM tasks, namely Causal LM, MLM, Permutation LM, and Replacement Token Detection during training. These two models laid the foundation for LM-based recommender systems and have become popular baselines for their successors.\n\nPre-train, fine-tune holistic model Under this category, the model is pre-trained and fine-tuned with different data sources, and the fine-tuning process will go through adjusting the whole model parameters. The learning objectives can also vary between the pre-training and fine-tuning stages. Pre-training and fine-tuning with different domains of data sources, also called cross-domain recommendation, can refer to the works of Kang et al. (2021) and Qiu et al. (2021). Kang et al. (2021) pre-trained a GPT model using segmented source API code and fine-tuned it with API code snippets from another library for cross-library recommendation. Wang et al. (2022a) fine-tuned the pre-trained DialoGPT model on domain-specific datasets for conversational recommendation together with an R-GCN model to inject knowledge from DBpedia to enhance recommendation perfor- mance. Xiao et al. (2022) fine-tuned the PTM to learn news embedding together with a user embedding part in an auto-regressive manner for news recommendation. They also explored different fine-tuning strategies like tuning part of the PTM and tuning the last layer of the PTM but empirically found fine-tuning the whole model resulted in better performance, which gives us an insight into balancing the recommendation accuracy and training efficiency.\n\nPre-train, fine-tune partial model Since finetuning the whole model is usually time-consuming and less flexible, many LMRSs choose to fine-tune partial parameters of the model to achieve a balance between training overhead and recommendation performance (Hou et al., 2022;Yu et al., 2022;Wu et al., 2022a). For instance, to deal with the domain bias problem that BERT induces a non-smooth anisotropic semantic space for general texts resulting in a large language gap for texts from different domains of items, Hou et al. (2022) applied a linear transformation layer to transform BERT representations of items from different domains followed by an adaptive combination strategy to derive a universal item representation. Meanwhile, considering the seesaw phenomenon that learning from multiple domainspecific behavioural patterns can be a conflict, they proposed sequence-item and sequence-sequence contrastive tasks for multi-task learning during the pre-training stage. They found only fine-tuning a small proportion of model parameters could quickly adapt the model to unseen domains with cold-start or new items. Pre-train, fine-tune extra part of the model With the increase in the depth of PTMs, the representation captured by them makes the downstream recommendation easier. Apart from the aforementioned two fine-tuning strategies, some works leverage a task-specific layer on top of the PTMs for recommendation tasks. Fine-tuning only goes through such extra parts of the PTMs by optimizing the parameters of the task-specific layer. Shang et al. (2019) pre-trained a GPT and a BERT model to learn patient visit embeddings, which were then used as input to fine-tune the extra prediction layer for medication recommendation. Another approach is to use the PTM to initialize a new model with a similar architecture in the finetuning stage, and the fine-tuned model is used for recommendations. In , a bidirectional Transformer-based model was first pretrained on four different self-supervised learning objectives (associated attribute prediction, masked item prediction, masked attribute prediction and segment prediction) to learn item embeddings. Then, the learned model parameters were adopted to initialize a unidirectional Transformer-based model for fine-tuning with pairwise rank loss for recommendation. In (McKee et al., 2023), the authors leveraged the pre-trained BLOOM-176B to generate natural languages descriptions of music given a set of music tags. Subsequently, two distinct pre-trained models, namely CLIP and the D2T pipeline, were employed to initialize textual, video, and audio representations of the provided music content. Following this, a transformerbased architecture model was fine-tuned for multimodal music recommendation.", "filtered_refids": [[], [], ["b5"], [], ["b45", "b53", "b36", "b17"], ["b15", "b39", "b50", "b32", "b61"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 33, "num_chars": 6410, "num_references": 10}
{"corpusid_sectionid": "256662721-s3", "title": "Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems", "date": "2023-02-07", "section_title": "Prompting paradigm for RSs", "section": "Instead of adapting PLMs to different downstream recommendation tasks by designing specific objective functions, a rising trend in recent years is to use the \"pre-train, prompt, and inference\" paradigm to reformulate downstream recommendations through hard/soft prompts. In this paradigm, fine-tuning can be avoided, and the pretrained model itself can be directly employed to predict item ratings, generate top-k item ranking lists, make conversations, recommend similar libraries for programmers while coding, or even output subtasks related to recommendation targets such as explanations (Li et al., 2023b). Prompt learning breaks through the problem of data constraints and bridges the gap of objective forms between pre-training and fine-tuning. Fixed-PTM prompt tuning Prompt-tuning only requires tuning a small set of parameters for the prompts and labels, which is especially efficient for few-shot recommendation tasks. Despite the promising results achieved through constructing prompt information without significantly changing the structure and parameters of PTMs, it also calls for the necessity of choosing the most appropriate prompt template and verbalizer, which can greatly impact recommendation performance. Prompt tuning can be both in the form of discrete textual templates (Penha and Hauff, 2020), which are more human-readable, and soft continuous vectors (Wang et al., 2022c;Wu et al., 2022b). For instance, Penha and Hauff (2020) manually designed several prompt templates to test the performance of movie/book recommendations on a pre-trained BERT model with a similarity measure. Wu et al. (2022b) proposed a personalized prompt generator tuned to generate a soft prompt as a prefix before the user behaviour sequence for sequential recommendation. Fixed-prompt PTM tuning Fixed-prompt PTM tuning tunes the parameters of PTMs similarly to the \"pre-train, fine-tune\" strategy but additionally uses prompts with fixed parameters to steer the recommendation task. One natural way is to use artificially designed discrete prompt to specify recommendation items. For instance, Zhang et al.\n\n(2021b) designed a prompt \"A user watched item A, item B, and item C. Now the user may want to watch () \" to reformulate the recommendation as a multi-token cloze task during fine-tuning of the LM-based PTM. The prompts can also be one or several tokens/words to seamlessly shift/lead the conversations from various tasks. Deng et al. (2023)  token as a prompt to indicate the start of the recommendation process and to summarize the dialogue context for the conversational recommendation.\n\nTuning-free prompting This training strategy can be referred to as zero-shot recommendations, which directly generate recommendations or/and related subtasks without changing the parameters of the PTMs but based only on the input prompts. Zero-shot recommendation has been shown to be effective in dealing with new users/items in one domain or cross-domain settings (Sileo et al., 2022;Geng et al., 2022c), compared to state-ofthe-art baselines. Specifically, Geng et al. (2022c) learned multiple tasks, such as sequential recommendation, rating prediction, explanation generation, review summarization and direct recommendation, in a unified way with the same Negative Log-likelihood (NLL) training objectives during pre-training. At the inference stage, a series of carefully designed discrete textual template prompts were taken as input, including prompts for recommending items in the new domain (not appearing in the pre-training phase), and the trained model outputs the preferable results without a fine-tuning stage. The reason for the effectiveness of zero-shot recommendation is that the training data and pre-training tasks are able to distil rich knowledge of semantics and correlations from diverse modalities into user and item tokens, which can comprehend user preference behaviours w.r.t. item characteristics (Geng et al., 2022c). Building upon this research, Geng et al. (2023) extended their efforts to train an adapter for diverse multimodal assignments, including sequential recommendations, direct recommendations, and the generation of explanations. In particular, they utilized the pre-trained CLIP component to convert images into image tokens. These tokens were added to the textual tokens of an item to create a personalized multimodal soft prompt. This com-bined prompt was then used as input to fine-tune the adapter in an autoregressive manner. Prompt+PTM tuning In this setting, the parameters include two parts: prompt-relevant parameters and model parameters. The tuning phase involves optimizing all parameters for specific recommendation tasks. Prompt+PTM tuning differs from the \"pre-train, fine-tune the holistic model\" strategy by providing additional prompts that can provide additional bootstrapping at the start of model training. For example, Li et al. (2023b) proposed a continuous prompt learning approach by first fixing the PTM, tuning the prompt to bridge the gap between the continuous prompts and the loaded PTM, and then fine-tuning both the prompt and PTM, resulting in a higher BLUE score in empirical results. They combined both discrete prompts (three user/item feature keywords, such as gym, breakfast, and Wi-Fi) and soft prompts (user/item embeddings) to generate recommendation explanations. Case studies showed improvements in the readability and fluency of generated explanations using the proposed prompts. Note that the Prompt+PTM tuning stage does not necessarily mean the fine-tuning stage but can be any possible stage for tuning parameters from both sides for specific data input. Xin et al. (2022) adapted a reinforcement learning framework as a Prompt+PTM tuning strategy by learning rewardstate pairs as soft prompt encodings w.r.t. observed actions during training. At the inference stage, the trained prompt generator can directly generate soft prompt embeddings for the recommendation model to generate actions (items).", "filtered_refids": [["b48", "b33", "b20", null], ["b3"], ["b15", "b9", "b20", "b41", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 5996, "num_references": 10}
{"corpusid_sectionid": "256662721-s5", "title": "Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems", "date": "2023-02-07", "section_title": "Language modelling objectives to recommendation", "section": "The expensive manual efforts required for annotated datasets have led many language learning objectives to adopt self-supervised labels, converting them to classic probabilistic density estimation problems. Among language modelling objectives, autoregressive, reconstruction, and auxiliary are three categories commonly used (Liu et al., 2023b). Here, we only introduce several language modelling objectives used for RSs.\n\nPartial/ Auto-regressive Modelling (P/AM) Given a text sequence X 1:T = [x 1 , x 2 , \u00b7 \u00b7 \u00b7 x T ], the training objective of AM can be summarized as a joint negative log-likelihood of each variable given all previous variables:\n\nModern LMRS typically utilize popular pretrained left-to-right LMs such as GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al., 2022a,c) as the backbone for explainable and conversational recommendations, respectively, to avoid the laborious task of pre-training from scratch. While auto-regressive objectives can effectively model context dependency, the modelling context can only be accessed from one direction, primarily left-to-right. To address this limitation, PAM is introduced, which extends AM by enabling the factorization step to be a span. For each input X, one factorization order M is sampled. One popular PTM that includes PAM as an objective is UniLMv2 (Bao et al., 2020). The pretrained UniLMv2 model can be utilized to initialize the news embedding model for news recommendation (Yu et al., 2022). Besides directly leveraging PTMs trained on textual inputs, some researchers apply this objective to train inputs with sequential patterns, such as graphs (Geng et al., 2022b) and user-item interactions . These patterns serve as either scoring functions to select suitable paths from the start node/user to the end node/item or detectors to explore novel user-item pairs. Masked Language Modelling (MLM) Taking a sequence of textual sentences as input, MLM first masks a token or multi-tokens with a special token such as [M ASK]. Then the model is trained to predict the masked tokens taking the rest of the tokens as context. The objective is as follows:\n\nwhere M (X) and X M (X) represent the masked tokens in the input sequence X and the rest of the tokens in X respectively. A typical example of MLM training strategy can be found on BERT, which is leveraged as backbone in (Zhang et al., 2021a) to capture user-news matching signals for news recommendation. Concurrently, some research works propose multiple enhanced versions of MLM. RoBERTa  \n\nwhere x and y represent two segments from the input corpus, and c = 1 if x and y are consecutive, otherwise c = 0. The NSP objective involves reasoning about the relationships between pairs of sentences and can be utilized for better representation learning of textual items such as news articles, item descriptions, and conversational data for recommendation purposes. Moreover, it can be employed to model the intimate relationships between two components. Malkiel et al. (2020) used the NSP to capture the relationship between the title and description of an item for next-item prediction. Furthermore, models pre-trained with NSP (such as BERT) can be leveraged for probing the learned knowledge with prompts, which are then infused in the fine-tuning stage to improve model training on adversarial data for conversational recommendation (Penha and Hauff, 2020). Sentence Order Prediction (SOP) as a variation of the NSP takes two consecutive segments from the same document as positive examples, which are then swapped in order as negative examples. SOP has been used to learn the inner coherence of title, description, and code for tag recommendation on StackOverflow (He et al., 2022).\n\nNevertheless, some researchers have questioned the necessity and effectiveness of the NSP and SOP for downstream tasks (He et al., 2022), which highlights the need for further investigation in recommendation scenarios. Replaced Token Detection(RTD) It is used to predict whether a token is replaced given its surrounding context:\n\nMovieLens Link", "filtered_refids": [["b23"], [], [null, "b8", "b61", "b0"], ["b65"], ["b33", "b13", "b31"], ["b13"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4064, "num_references": 10}
{"corpusid_sectionid": "258426970-s5", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Format", "section": "An important decision to make when we want to improve language generation systems through human Example input and output for three tasks (machine translation, summarization, and instruction following) and possible different (example) feedback that can be given.\n\nfeedback is in what format to collect this feedback in. The choice of format has implications on the expressivity of the feedback, the ease of its collection, and how we can use it to improve systems. In particular, the complexity of the feedback format is an important factor: simpler formats are often easier to collect and use as part of the training/decoding process, but contain less information than more \"complex\" formats, and might not be able to capture important information for improving the system. The choice of format also has implications in the difficulty for humans to give feedback, its consistency/agreement, and the level of rationality of said feedback (Ghosal et al., 2023). Types of feedback are summarized in Table 1 with examples.\n\nNumerical Numerical feedback, which takes an input and output and returns a single score (X \u00d7 Y \u2192 N \u2286 R), is one of the simplest feedback formats to collect and use. Kreutzer et al. (2018) studied using categorical feedback, in the form of 5 possible \"stars\" that can be assigned to a translation, which are then averaged to produce a score (N = [1, 5]) and used to improve the model.  and Shi et al. (2021) used even simpler feedback, by asking humans to choose if a given response is good or not (N = {0, 1}). Numerical feedback has also been extensively used for evaluation, albeit not with the explicit goal of improving generation. For example, direct assessments (Graham et al., 2013) in machine translation ask humans to rate translations on a continuous scale, and some works have attempted to use this feedback data to train feedback models (Sellam et al., 2020;Rei et al., 2020a) and improve generation (Freitag et al., 2022a;Fernandes et al., 2022).\n\nAlthough easy to leverage, numerical feedback suffers from some limitations: depending on the complexity of the generation task, reducing feedback to a single score might generally be a hard and ill-defined task for humans, leading to a costly collection process and problems of subjectivity and variance (see \u00a76.2.1). Furthermore, such feedback might not be suited to distinguish between outputs of similar quality.\n\nRanking-based An alternative to asking humans to assign a single score to a given input-output pair is asking them to rank multiple possible alternative outputs h : X \u00d7 Y 1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Y n \u2192 S n where S n represents the set of all permutations/rankings of n elements (optionally allowing ties). This has been used extensively in evaluation (Chaganty et al., 2018). Compared to numerical feedback, this format tends to be easier to collect, and, potentially, for this reason, ranking-based feedback tends to be collected to improve model behavior rather than just for evaluation (since the former tends to require more feedback data). Ziegler et al. (2019) and Stiennon et al. (2020) asked humans to rank alternative summaries of the system they are trying to improve. Similarly, Ouyang et al. (2022) collected rankings of alternative responses to an instruction given to the model. They utilized these rankings to enhance the model's instruction-following capabilities. Subsequent research has also employed ranking-based feedback for the same task (Askell et al., 2021;Bai et al., 2022a,b). Natural Language Both numerical and rankingbased feedback lack the ability to capture detailed information about problems with the output, which can be crucial for improving generation systems. Instead of asking humans to rank or score outputs, we can instead ask for natural language feedback. In such cases, the feedback typically provides more detailed information, either highlighting the shortcomings of the current output or suggesting specific actions for improvement. For example, Li et al. (2017) asked humans to give natural language feedback to a dialogue question answering model, including positive or negative feedback, but also possibly providing the correct answer to the model or hinting about it.  and  gather natural language feedback on errors present in model-generated graphs and the model's interpretation of a given instruction. Scheurer et al. (2022Scheurer et al. ( , 2023 improve summarization capabilities of language models by asking humans to provide natural language feedback of summaries of the model. Li et al. (2022) collect natural language feedback (in addition to numerical feedback) for responses from a Question Answering (QA) system.\n\nOthers Besides these feedback types, other (potentially domain-specific) types of feedback can be used to improve model behavior. Commonly humans are asked to provide multi-aspect feedback (X \u00d7 Y \u2192 R d or F d more generally), scoring an output or ranking multiple outputs with respect to multiple dimensions (B\u00f6hm et al., 2019;Glaese et al., 2022;Madaan et al., 2023;Nguyen et al., 2022). Post-editions ask humans to provide corrections to the output in the form of small edits (e.g., replace X by Y), and post-edition data has been used to directly improve models (Denkowski et al., 2014) or train automatic post edition systems that correct model mistakes (Pal et al., 2016;Mehta and Goldwasser, 2019;Madaan et al., 2021;Talmor et al., 2020;Elgohary et al., 2021). There are also other feedback types that haven't been fully leveraged to improve generation: e.g., Multidimensional Quality Metrics (MQM) (Lommel et al., 2014b), the standard for evaluating translation quality, asks professional translators to identify errors spans in a translation, alongside severity and type of error.", "filtered_refids": [[], ["b3"], ["b60", "b9", "b80", null, "b24", "b76"], [], ["b87", "b29", "b71", "b30", null, "b72"], ["b90", "b40", "b35", null, "b5", "b42", "b38", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 5753, "num_references": 21}
{"corpusid_sectionid": "258426970-s6", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Objective", "section": "The purpose of collecting feedback is to align the model's behavior with some (often ill-defined) goal behavior: we might want our summarization model to generate summaries that contain all core information, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate businesscritical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014;Amodei et al., 2016;Bommasani et al., 2021). In addition, Kenton et al. (2021b) discuss some behavioral issues in language agents (natural language generation models) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective.\n\nBai et al. (2022a) explicitly divided the problem of \"aligning\" a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness factors (such as not producing toxic text or providing information that could lead to harm). 2\n\nHelpfulness Most often, feedback is collected with some helpfulness objective in mind: a necessary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018;Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream applications. Similarly, in summarization, most works leverage feedback related to aspects such as relevance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instructions (Ouyang et al., 2022): the task of instructionfollowing can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021).\n\nHarmlessness Another important alignment objective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (besides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their system, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could increase harmlessness without reducing helpfulness.", "filtered_refids": [[null, "b28"], [], ["b24", null, "b69"], ["b69", null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 3311, "num_references": 8}
{"corpusid_sectionid": "258426970-s8", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation", "date": "2023-05-01", "section_title": "Optimizing for Human Feedback", "section": "Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be \"optimizable\", i.e., possibly formulated as an optimization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f \u2208 R), we can create the following optimization problem:\n\n( 2) Where D is the distribution of possible inputs. Various techniques have been suggested to optimize the model parameters, \u03b8, using the collected human feedback. These can be divided into three main categories based on the training mechanisms, which we will call feedback-based imitation learning, joint-feedback modeling, and reinforcement learning (RL).\n\nThe feedback-based imitation learning approach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled generations together with the corresponding inputs, D + . This can be achieved by minimizing the loss:\n\nAn instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model's answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine translation model on a set of positively-labeled translations, and Glaese et al. (2022) performed supervised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), according to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human utterances as targets to fine-tune the model. Scheurer et al. (2022Scheurer et al. ( , 2023 leverage the fact that LLMs can follow instructions and start by collecting natural language human feedback about the model generations, which often describes what an improved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corresponding feedback. The highest similarity refinements for each generation are then used to finetune the LLM. OpenAI's text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disregard the generations which do not receive positive feedback, which may contain useful information to optimize the model. On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural language). Having D as the dataset of inputs x, generations y, and human feedback f collected, this can be achieved by minimizing the following loss of the form\n\nOver all examples in D. These equation can be factorized as\n\n. Some works simply train the model to predict the feedback given to each generation (Weston, 2016, forward prediction), disregarding the second term of the factorization. One example of this approach is the work of Li et al. (2017), in which the authors asked humans to give natural language feedback (e.g., positive/negative feedback, providing the correct answer to the model, or giving a hint about the correct answer) to a dialogue question answering model. Then, after having collected the feedback, the model is trained to predict it. Hancock et al. (2019) proposed having an auxiliary model predicting the satisfaction of the human speaking with the model. Then, if the satisfaction score is lower than a pre-defined threshold, the model will ask the human for feedback. The model then leverages the natural language feedback humans give by learning to predict it. Yuan et al. (2023); Rafailov et al. (2023) showed that having summarization models predict the rankings of different summaries helps the model generate better summaries, and might even outperform more complicated approaches using feedback models ( \u00a75).\n\nOther works train the model to predict the generations and the corresponding human feedback. Xu et al. (2022) proposed using the DIRECTOR model introduced by Arora et al. (2022) to leverage human feedback. As this model has a unified decoderclassifier architecture, Xu et al. (2022) proposed using positively-labeled examples to train its language modeling head (similarly to feedback-based imitation learning) and using both the positive and negatively-labeled examples to train a classifier head that directs the model away from generating undesirable sequences. Thoppilan et al. (2022a) follow this approach to enforce the model's quality and safety. First, they collect dialogues between crowd-workers and the proposed language model LaMDA, which are annotated with feedback provided by the crowd-workers. This feedback states each response's quality (sensible, specific, and interesting) or safety. Then, LaMDA is fine-tuned to predict the high-quality responses and the rewards given to every response regarding its quality attributes and safety. At inference time, LaMDA is also used to filter out candidate responses for which its safety prediction is below a threshold.\n\nFinally, this can also be achieved by training the model to predict generation and conditioning on the feedback. This corresponds to minimizing the following loss: Liu et al. (2023) proposed prompt-based finetuning, where they create prompts containing previous generations rated by humans, in the order of preference. They also suggest inserting languagebased feedback (e.g., \"... is a worse answer than ...\") to the prompt, between the generations. Then, the model is fine-tuned to maximize the likelihood of generating the most preferred answer.\n\nFinally, reinforcement learning (RL) offers a more versatile approach, allowing for direct optimization of a model's parameters based on human feedback, regardless of the feedback's differentiability. A common RL algorithm used in this context is the REINFORCE algorithm (Williams, 1992), which updates the policy parameters using the following gradient:\n\n(7) Here, D represents the set of inputs x, and p \u03b8 is the policy. This flexibility enables RL to handle various types of feedback and better align the generated output with human preferences. For instance, Kreutzer et al. (2018) proposed using task-based implicit feedback from user queries as a reward signal to train a machine translation model using a word-level variant of minimum risk training (Shen et al., 2016), while Jaques et al. (2019) used implicit human reactions in chat to improve open-domain dialog systems through off-policy Q-learning (Watkins and Dayan, 1992). Given that collecting human feedback can be expensive and time-consuming, learning is done offline from logged data, which is typically more favorable than on-policy settings that need feedback on the fly. Later in \u00a75.2.1, we discuss several works that attempt to optimize feedback models using RL instead of directly optimizing human feedback. In conjuction, these aproaches are commonly known as Reinforcement Learning from Human Feedback (RLHF).", "filtered_refids": [[], [], [], ["b29", "b71", "b24", "b72", "b5"], [], ["b59", null, "b10", "b29"], ["b107", "b94"], ["b32"], ["b104"], ["b24", "b77", "b99", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 42, "num_chars": 7251, "num_references": 17}
{"corpusid_sectionid": "132053537-s2", "title": "A Short Survey on Sense-Annotated Corpora", "date": "2018-02-13", "section_title": "WordNet", "section": "WordNet (Fellbaum, 1998) has been one of the most widely used knowledge resource in lexical semantics. In fact, it is the de-facto standard sense inventory for Word Sense Disambiguation since many years. The core unit in WordNet is the synset. A synset represents a concept or a meaning which is represented by its various lexicalizations (i.e. senses). For example, the synset defined as motor vehicle with four wheels can be expressed by its synonym senses auto, automobile, machine and motorcar. In what follows we list the main WordNet sense-annotated corpora, using WordNet 3.0 as reference sense inventory. SemCor. The first and most prominent example of senseannotated corpora is SemCor (Miller et al., 1993b). Sem-Cor was manually annotated and consists of 352 documents from the Brown Corpus (Kucera and Francis, 1979) and 226,040 sense annotations. SemCor is the largest manually-annotated corpus and the most used in the literature to train WSD supervised systems Figure 1: Overview of sense inventories with their corresponding sense-annotated corpora. Zhong and Ng, 2010;Raganato et al., 2017b;Luo et al., 2018;Loureiro and Jorge, 2019;Huang et al., 2019). SemEval. SemEval datasets provide reliable benchmarks for testing WSD systems. The main datasets from Senseval and SemEval competitions have been compiled and unified by Raganato et al. (2017a). In particular, the datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 task 1 (Snyder and Palmer, 2004), SemEval-2007 task 17 (Pradhan et al., 2007), SemEval-2013 task 12 , and SemEval-2015 task 13 (Moro and Navigli, 2015). These datasets, which have been mainly used as evaluation benchmarks for WSD systems, contain a total of 7,253 sense annotations. MASC-WSA. The MASC Word Sense Annotation (MASC-WSA) corpus (Ide et al., 2010)is an excerpt of the Manually Annotated Sub-Corpus of American English (Ide et al., 2008, MASC) and the Open American National Corpus (Ide et al., 2002, ANC) containing annotations for 45 distinct lexemes, i.e., lemma-pos pairs, for a total of 441 distinct WordNet word senses. 2 Each word occurrence has been manually annotated on Amazon Mechanichal Turk by roughly 25 persons for a total of 1M annotations. Princeton WordNet Gloss. The Princeton WordNet Gloss Corpus 3 is a sense-annotated corpus of textual definitions (glosses) from WordNet synsets. The corpus was tagged semi-automatically: 330,499 instances were annotated manually while the remaining annotations (i.e. 118,856) were obtained automatically. This corpus of disambiguated glosses has already proved to be useful in shtml tasks such as semantic similarity (Pilehvar et al., 2013), domain labeling (Gonz\u00e1lez et al., 2012) and Word Sense Disambiguation (Baldwin et al., 2008;Agirre and Soroa, 2009;Camacho-Collados et al., 2015).\n\nOntoNotes. OntoNotes (Weischedel et al., 2013) is a corpus from the Linguistic Data Consortium which comprises different kinds of explicitly-tagged syntactic and semantic information, including annotations at the sense level. The OntoNotes corpus consists of documents from diverse genres such as news, weblogs and telephone conversation. Its 5.0 released version contains 264,622 sense annotations.\n\nOMSTI. The task of gathering sense annotations has proved expensive and not easily scalable. That is the reason why more recent approaches have attempted to exploit semi-automatic or automatic techniques. OMSTI 4 (Taghipour and Ng, 2015a, One Million Sense-Tagged Instances), which is a semi-automatically constructed corpus annotated with WordNet senses, is a prominent example. It was built by exploiting the alignment-based WSD approach of Chan and Ng (2005) on a large English-Chinese parallel corpus (Eisele and Chen, 2010, MultiUN corpus). OM-STI, coupled with SemCor, has already been successfully leveraged as training data for training supervised systems (Taghipour and Ng, 2015a;Iacobacci et al., 2016;Raganato et al., 2017a).", "filtered_refids": [["b52", "b63", "b6", "b19", "b51", "b36", "b49", "b30", "b21", "b16", "b37", "b56", "b4", "b31", "b32", "b27", null, "b48", "b0"], ["b61"], ["b57", "b9", null, "b24", "b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 3944, "num_references": 25}
{"corpusid_sectionid": "132053537-s4", "title": "A Short Survey on Sense-Annotated Corpora", "date": "2018-02-13", "section_title": "BabelNet", "section": "BabelNet (Navigli and Ponzetto, 2012) is a wide-coverage multilingual semantic network obtained from the integration of various encyclopedias and dictionaries (WordNet and Wikipedia, inter alia). Being a superset of all these resources, BabelNet brings together lexicographic and encyclopedic knowledge, thus containing both named entities and concepts, and, unlike Wikipedia covering only noun instances, instances have diverse Part-Of-Speech (PoS) tags: nouns, verbs, adjectives and adverbs. Given its multilingual nature (i.e. BabelNet covers over 250 languages), Babel-Net has been used as a sense inventory for annotating text in languages other than English.\n\nSenseDefs. SenseDefs 8 (Camacho-Collados et al., 2019) extends the effort from the Princeton WordNet Gloss Corpus project (see Section 2.1) by automatically disambiguating textual definitions from various heterogeneous sources in 263 languages. The underlying idea lies on leveraging the cross-complementarities of definitions of identical concepts from different languages and resources. The approach couples a graph-based disambiguation method (Moro et al., 2014) with a refinement based on distributional similarity (Camacho-Collados et al., 2016). The proposed method was evaluated on four European languages (English, Spanish, French and Italian) with an estimated precision of over 80%.\n\nEuroSense. The construction of EuroSense 9 (Delli Bovi et al., 2017) follows a similar approach to SenseDefs. In this case, parallel corpora is exploited for a single multilingual disambiguation. The output is a sense-annotated corpus for 21 languages for the Europarl parallel corpus (Koehn, 2005). The estimated precision for four languages isover 80% on average, with a peak of almost 90% for German.", "filtered_refids": [["b39"], ["b38", "b7"], ["b29"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1763, "num_references": 4}
{"corpusid_sectionid": "132053537-s5", "title": "A Short Survey on Sense-Annotated Corpora", "date": "2018-02-13", "section_title": "Train-o-Matic.", "section": "Similarly to the previous approach, Train-o-Matic 10 (Pasini and Navigli, 2017, T-o-M) aims at automatically annotating words from a raw corpus with senses. The main difference with respect to EuroSense and OMSTI lies in the fact that T-o-M does not need parallel data in order to annotate the input corpus. While being language independent and fully automatic, it proved to lead supervised systems to high performance, close or even better than those achieved when a manually annotated corpus (e.g. SemCor) is used for training. Moreover, it has also proved effective in languages other than English : Italian, Spanish, French, German and Chinese.\n\nOneSeC. OneSeC 11 (Scarlini et al., 2019) is the most recent work among those aiming at automatically producing semantically-annotated data. Instead of the well-known \"one sense per discourse\" assumption made by Gale et al. (1992), this work makes a more relaxed hypotesis, i.e., \"one sense per Wikipedia Category\". That is, a noun is used always with the same meaning within a Wikipedia Category. By leveraging this conjecture, OneSeC exploits the texts contained within the pages of a given Wikipedia Category to annotate each noun occurrence therein with its most suitable meaning. The corpora for English showed to be of high-quality, leading a supervised English WSD model, i.e., It Makes Sense (Zhong and Ng, 2010, IMS), to achieve results that are higher than those attained by IMS trained on other automatically generated corpora. Furthermore, OneSeC has been used to generate annotated data for four other European languages, namely: Italian, Spanish, German and French.", "filtered_refids": [[null], ["b53", "b18", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1629, "num_references": 4}
{"corpusid_sectionid": "53296658-s3", "title": "A Survey on Natural Language Processing for Fake News Detection", "date": "2018-11-02", "section_title": "Short Claims", "section": "A recent benchmark dataset for fake news detection is LIAR (Wang, 2017). This dataset includes 12,836 real-world short statements collected from PolitiFact, where editors handpicked the claims from a variety of occasions such as debate, campaign, Facebook, Twitter, interviews, ads, etc. Each statement is labeled with six-grade truthfulness. The information about the subjects, party, context, and speakers are also included in this dataset. Vlachos and Riedel (2014) and Ferreira and Vlachos (2016) are the first to study PolitiFact data, but LIAR is orders of magnitude larger and more comprehensive. However, note that the original LIAR paper does not include the editor's justification or evidence due to copyright concerns, and users will need to retrieve the justification/evidence separately using an API. Also, even though both the claims and the evidence are from real-world occasions, they are highly un-structured. Fact-checking remains relatively challenging for this dataset. Fever ) is a dataset providing related evidences for fake news detection. Fever contains 185,445 claims generated from Wikipedia data. Each statement is labeled as Supported, Refuted, or Not Enough Info. They also marked which sentences from Wikipedia they use as evidence. Fever makes it possible to develop a system which can predict the truthfulness of a claim together with the evidence, even though the type of facts and evidence from Wikipedia may still exhibit some major stylistic differences from those in real-world political campaigns.\n\nPOLITIFACT, CHANNEL4.COM 2 , and SNOPES 3 are three sources for manually labeled short claims in news, which is collected and labeled manually. Many datasets, such as Wang (2017) and Rashkin et al. (2017), are created based on these websites.", "filtered_refids": [["b6", "b34", "b36"], ["b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 1780, "num_references": 4}
{"corpusid_sectionid": "53296658-s4", "title": "A Survey on Natural Language Processing for Fake News Detection", "date": "2018-11-02", "section_title": "Posts On Social Networking Services", "section": "In addition to the websites mentioned above, posts on Social Networking Services (SNS), such as Twitter and Facebook, can also be a source of short news statements. There are some datasets for fake news detection focusing on SNS, but they tend to have a limited set of topics and can be less related to news. BUZZFEEDNEWS 4 collects 2,282 posts from 9 news agencies on Facebook. Each post is factchecked by 5 BuzzFeed journalists. The advantages of this dataset are that the articles are collected from both sides of left-leaning and rightleaning organizations, and they are enriched in   Potthast et al. (2017) by adding data such as the linked articles. BUZZFACE (Santia and Williams, 2018) extends the BuzzFeed dataset with the comments related to news articles on Facebook. It contains 2,263 news articles and 1.6 million comments. SOME-LIKE-IT-HOAX 5 (Tacchini et al., 2017) consists of 15,500 posts from 32 Facebook pages, that is, the public profile of organizations (14 conspiracy and 18 scientific organizations). This dataset is labeled based on the identity of the publisher instead of post-level annotations so that it may have imposed a strong assumption. A potential major pitfall for such dataset is that such kind of labeling strategy can result in machine learning models learning characteristics of each publisher, rather than that of the fake news. PHEME (Zubiaga et al., 2016) and CRED-BANK (Mitra and Gilbert, 2015) are two Twitter datasets. PHEME contains 330 twitter threads (a series of connected Tweets from one person) of nine newsworthy events, labeled as true or false according to thread structures and follow-follower relationships. CREDBANK contains 60 million tweets covering 96 days, grouped into 1,049 events with a 30-dimensional vector of truthfulness labels. Each event was rated on a 5-point Likert scale of truthfulness by 30 human annotators. They simply concatenate 30 ratings as a vector because they find it difficult to reduce it to a one-dimensional score.\n\nAs mentioned above, these datasets were created for verifying the truthfulness of tweets. Thus they are limited to a few numbers of topics and can include tweets with no relationship to news. Hence both datasets are not so much ideal for fake news detection so that they are more frequently 5 https://github.com/gabll/some-like-it-hoax used for rumor detection.", "filtered_refids": [["b21", "b39", "b31", "b17"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2364, "num_references": 4}
{"corpusid_sectionid": "53296658-s5", "title": "A Survey on Natural Language Processing for Fake News Detection", "date": "2018-11-02", "section_title": "Entire-Article Datasets", "section": "There are several datasets for fake news detection focusing on fake news detection based on the entire article. For example, FAKENEWSNET (Shu et al., 2017a(Shu et al., ,b, 2018 is an ongoing data collection project for fake news research. It consists of headlines and body texts of fake news articles from BuzzFeed and PolitiFact. It also collects information about social engagements of these articles from Twitter. BS DETECTOR 6 is collected from a browser extension named BS Detector, which indicates its labels are the outputs of BS Detector, not human annotators. BS Detector searches all links on a web page at issue for references to unreliable sources by checking against a manually compiled list of unreliable domains. Note that the major issue with using this dataset is that the machine learning models trained on this dataset are learning the parameters of the BS Detector.\n\nWebsites such as BLUFF THE LISTENER and THE ONION create sarcastic and humorous (Rubin et al., 2015a) fake news intentionally. Note that the types of fake news from these sources are limited. Moreover, it is relatively easy to classify them against traditional new media articles. A dataset consists of articles from various publishers can be better (Rashkin et al., 2017), though individual claims must be checked. We should also note that one must avoid using aggregate labels simply based on website source, as it adds more confounding variables and it is more of a website classification task.", "filtered_refids": [[null, "b29"], ["b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1484, "num_references": 3}
{"corpusid_sectionid": "252819431-s1", "title": "Overview of MSLR2022: A Shared Task on Multi-document Summarization for Literature Reviews", "date": 2022, "section_title": "Task description", "section": "We give a brief description of the datasets, task, evaluation metrics, and submission protocol for the shared task.\n\nDatasets We provided two datasets for model iteration and evaluation. The MS\u02c62 dataset consists of 20k reviews (comprising 470K studies) from the literature to study the task of generating review summaries (DeYoung et al., 2021). Reviews and studies for MS\u02c62 were collected from PubMed. Input studies were filtered from cited articles using keyword heuristics and a SciBERT-based suitability classifier trained on human annotations, and the target summary was extracted from the review abstract using a SciBERT-based sequential sentence classifier trained on manually-labeled sentences from over 200 abstracts (see DeYoung et al. (2021) for details). Target summaries in the test set were manually reviewed and corrected. In addition to the abstracts of input studies and summaries, MS\u02c62 extracts a background section from each review as context for the research question.\n\nThe Cochrane dataset consists of 4.6K reviews from the Cochrane Library (Wallace et al., 2020). 2 The target summaries are the Authors' Conclusions sections of the review abstracts. The Cochrane dataset is smaller and more consistent than the MS\u02c62 dataset since all Cochrane reviews follow a similar process. For more information on dataset construction, please refer to the original dataset papers (DeYoung et al., 2021;Wallace et al., 2020).\n\nTask Given the abstracts of input studies pertaining to a research question (and in the case of MS\u02c62, a background section describing that research question), the task is to produce a summary that synthesizes the information from the input studies. The synthesis of information typically results in an evidence \"direction,\" e.g., the evidence overall suggests that the intervention studied increases/decreases/does not change the outcome measure for the studied population (DeYoung et al., 2020). The direction of the evidence indicated in a good generated summary should agree with that in the reference (gold) summary.\n\nEvaluation We perform automated evaluation using ROUGE (Lin, 2004), BERTScore (Zhang et al., 2020), and the evidence inference (Lehman et al., 2019) divergence metric defined in Wallace et al. (2020) and modified by DeYoung et al. (2021). For ROUGE, we report ROUGE-1, ROUGE-2, and ROUGE-L. For the evidence inference-based metric, we report the average divergence (\u2206EI Avg) and the Macro-F1 (\u2206EI F1) computed using a model trained on the dataset provided by DeYoung et al. (2020).\n\nFor human evaluation, we developed and iterated on an annotation protocol based on the analysis conducted by Otmakhova et al. (2022b). For each annotation task, annotators are shown a gold summary and a generated summary and asked to assess the latter for (i) fluency and (ii) agreement with the gold summary in terms of the \"PICO\" element alignment, 3 evidence inference directional agreement, and alignment regarding the strength of the claims made in summaries. We will provide further details on human annotation results following the shared task meeting.\n\nSubmissions Leaderboards for submissions are provided for the two subtasks: MS\u02c62 4 and Cochrane. 5 Submissions to the leaderboard are judged against the gold summaries in the test splits using the automated metrics described previously.", "filtered_refids": [[], ["b3"], [null, "b18", "b3"], [null], ["b10", "b18", null, "b22", "b8", "b3"], ["b13"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3338, "num_references": 12}
{"corpusid_sectionid": "252819431-s3", "title": "Overview of MSLR2022: A Shared Task on Multi-document Summarization for Literature Reviews", "date": 2022, "section_title": "Participating systems", "section": "We provide brief descriptions of all participating systems. System performance as assessed using automated evaluation metrics are given in Table 1.\n\nITTC (Otmakhova et al., 2022a) The team adapted PRIMERA (Xiao et al., 2022), a model based on Longformer Encoder-Decoder (Beltagy et al., 2020) that has been designed for multidocument summarization, resulting in strong performance on the MSLR Cochrane subtask. In addition to fine-tuning on the entire training sets of the MSLR shared task, the team also experimented with zero-and few-shot learning scenarios. The authors found that ROUGE did not adequately capture the performance drops observed in the zeroand 10-shot settings, where factuality of the generated summaries was poor. The team also experiment with using global attention to highlight PICO elements in the input and target texts. Though ROUGE did not vary significantly between these two settings, the authors found that when PICO spans are given global attention, the resulting summaries tended to be more abstractive.\n\nLongT5-Pubmed (Yu, 2022) The author attempted to finetune a LongT5 model (Guo et  PuneICT (Tangsali et al., 2022) The team experimented with finetuning BART-large, DistillBART, and T5-base for both the MS\u02c62 and Cochrane subtasks. On the MS\u02c62 subtask, finetuned BART-large had the highest performance of the three models based on ROUGE score; on the Cochrane subtask, DistillBART performed best.\n\nSciSpace (Shinde et al., 2022) The team combined a BERT-based extractive method with a Big-Bird PEGASUS-based abstractive summarization model (Zaheer et al., 2020), leading to strong performance on the MSLR Cochrane subtask. For the extractive step, the authors use a Lecture Summarizer model to identify the most important sentences from the input documents; this method encodes input sentences using BERT, then clusters the contextual representations and selects the sentences closest to the cluster centroids. The resulting sentences are used as input into a BigBird PEGA-SUS model pretrained on Pubmed, which is finetuned on the MSLR training data. In analysis, the authors observed that a common error is duplication of statements in the generated summary. The model submitted by the team to the Cochrane subtask leaderboard performs best among submissions based on ROUGE-L, though the authors report that the same training strategy does not lead to good performance on the MS\u02c62 subtask due to the much longer input sequences in MS\u02c62.\n\nLED-base-16k (Giorgi et al., 2022) The team fine-tuned Longformer Encoder-Decoder following a similar protocol to PRIMERA (Xiao et al., 2022), improving performance over baselines in both subtasks. Their input sequence included the titles and abstracts of up to 25 studies, separated by special tokens. No system description was submitted.", "filtered_refids": [[], ["b12", "b19", "b0"], [null, "b20", "b17"], ["b15", "b21"], ["b19", "b6"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2813, "num_references": 10}
{"corpusid_sectionid": "252818977-s5", "title": "Abstractive Approaches To Multidocument Summarization Of Medical Literature Reviews", "date": 2022, "section_title": "Training Details", "section": "For training the models we used the Simple Transformers 2 library, an API used for transformer mod-   els (Vaswani et al., 2017), which provides built-in support for various natural language processing tasks including text summarization. We trained our models on the Nvidia K80 GPU which has a GPU RAM of 15 gigabytes. CUDA was utilized for effective computing, and making the training and evaluation processes faster. All the models were trained on 10 epochs, with training and validation losses measured over time for each epoch.\n\nWe trained the BART-large and the DistilBART-CNN models on the datasets, by instantiating Seq2Seq models (Sutskever et al., 2014) and arguments provided by Simple Transformers. We later modified some of the arguments by making the maximum length for each sequence equal to 140. Due to limited RAM available on the CUDA used, we faced memory errors. Hence, after each epoch, the weights directory was overwritten for memory availability. Maximum sequence length for the tokenized sequences of each input document was set to 512. For T5 (Text-To-Text Transfer Transformer), we used the t5-base models (Raffel et al., 2020b), after providing t5-base tokenization, and trained them with the same aforementioned hyperparameters.\n\nAll the above mentioned hyperparameters were giving the best possible results, and hence we proceeded with the use of the same. We finetuned the basic configurations specified in the Fairseq documentation. 3", "filtered_refids": [["b20"], ["b19", "b17"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1465, "num_references": 3}
{"corpusid_sectionid": "210936331-s2", "title": "A Survey of the Perceived Text Adaptation Needs of Adults with Autism", "date": "2019-10-22", "section_title": "Autism Spectrum Disorder and Reading", "section": "Autism Spectrum Disorder (ASD) is a developmental disorder with neural origin characterised by impairment in communication and social interaction (American Psychiatric Association, 2013) and is known to affect about 1 in 100 people in the UK (Brugha et al., 2011). Language comprehension difficulties in autism cover phenomena such as difficulties in syntax processing of long sentences (Whyte et al., 2014), resolving ambiguity in meaning (Happe, F., and Frith, U, 2006), and identifying pronoun referents (O'Connor and Klein, 2004), as well as having difficulties in figurative language comprehension and making pragmatic inferences (MacKay and Shaw, 2004). These difficulties, together with the specific cognitive profile of individuals with autism (e.g., differences in the Theory of Mind (Baron-Cohen, 2000)) may lead to secondary issues such as challenges with identifying author intent and subtler nuances of meaning. In addition, web users with autism have been consistently shown to have different information searching strategies when processing web pages (Eraslan et al., 2017;Yaneva et al., 2018;Eraslan et al., 2019;, which relate to differences in visual attention. As a result of these difficulties, information contained in online user feedback can be less accessible for people with autism.", "filtered_refids": [["b11", "b6", "b4", "b13", "b20", null, "b3", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1307, "num_references": 8}
{"corpusid_sectionid": "210936331-s3", "title": "A Survey of the Perceived Text Adaptation Needs of Adults with Autism", "date": "2019-10-22", "section_title": "Automatic Text Adaptation for Adults with Autism", "section": "In terms of systems aimed at making text more accessible for autistic individuals who are fairly able, the OpenBook tool 1 is the most comprehensive existing system to date. The tool provides semi-automatic conversion of text documents by reducing syntactic complexity and disambiguating meaning by resolving pronominal reference, performing word sense disambiguation and detecting conventional metaphors Or\u0203san et al., 2018), with some initial efforts towards concept substitutions for images (Barbu et al., 2015). As part of the research project, the tool was evaluated together with end-users with ASD who were shown to find the adapted texts more accessible than the originals. Nevertheless, a major impediment for the automatic evaluation of such systems is the limited amount of userevaluated data. To the best of our knowledge, the only available resources containing a limited amount of such data are the ASD corpus (Yaneva et al., 2016a;Yaneva, 2016), followed by a corpus of easy-to-read documents that were specifically developed for people with cognitive disabilities (Yaneva et al., 2016b) 2 . Constrained by these limitations, some approaches propose to automatically evaluate text simplification systems for people with autism in terms the change in readability of the generated sentences Stajner and Saggion, 2013), the incorporation of user-evaluated data into larger corpora , or the use of corpora containing texts for children and language-learners (\u0160tajner et al., 2014). Therefore, very little is known about the perceptions of adults with high-functioning autism on the usefulness of specific simplification strategies.\n\nIn the following sections we present a survey on the perceptions of adults with high-functioning autism on the accessibility of user reviews.", "filtered_refids": [["b18", "b14", null, "b22", "b23", "b1"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1785, "num_references": 6}
{"corpusid_sectionid": "14225710-s1", "title": "Survey on the Use of Typological Information in Natural Language Processing", "date": "2016-10-11", "section_title": "Overview of Linguistic Typology", "section": "Languages may share universal features on a deep, abstract level, but the structures found in real-world, surface-level natural language vary significantly. This variation is conventionally characterised into 'languages' (e.g. French, Hindi, Korean) 2 , and linguistic typology describes how these languages resemble or differ from one another. The field comprises three pursuits: the definition of language features and their capacity for variance, the measurement and analysis of feature variance across empirical data, and the explanation of patterns observed in this data analysis. Bickel (2007) terms these three pursuits qualitative, quantitative and theoretical typology, respectively. Typological classifications of languages have strict empirical foundations. These classifications do often support theories of causation, such as historical, areal or phylogenetic relations, but importantly, these hypotheses come second to quantitative data (Bickel, 2007). Indeed, patterns of variance may even run contrary to established theories of relations between languages based on geographical or historical proximity. For instance, Turkish and Korean are typically considered to be highly divergent in lexical features, yet their shared syntactic features make the two languages structurally quite similar. Such indications of similarity are of value for NLP which primarily seeks to model (rather than explain) cross-linguistic variation.\n\nTypologists define and measure features according to the task at hand. Early studies, focused on word order, simply classified languages as SVO (Subject, Verb, Object), VSO, SOV, and so forth (Greenberg, 1963). There are now more various and fine-grained studies based on a wide range of features, including phonological, semantic, lexical and morphosyntactic properties (see (Bickel, 2007;Daniel, 2011) for an overview and further references). While a lot of valuable information is contained in these linguistic studies, this information is often not readily usable by NLP due to factors such as information overlap and differing definitions across studies. However, there is also a current trend towards systematically collecting typological information from individual studies in publicly-accessible databases, which are suitable for direct application in NLP (e.g., for defining features and their values). Table 1 presents a selection of current major databases, including the Syntactic Structures of the World's Languages (SSWL) (Collins and Kayne, 2009), the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013), the Phonetics Information Base and Lexicon (PHOIBLE) (Moran et al., 2014), the URIEL Typological Compendium (Littel et al., 2016), the Atlas of Pidgin and Creole Language Structures (APiCS) (Michaelis et al., 2013), and the Lyon-Albuquerque Phonological Systems Database (LAPSyD) (Maddieson et al., 2013). The table provides some basic information about these databases, including type, coverage, and additional notes. From these databases, WALS is currently by far the most commonly-used typological resource in NLP due to its broad coverage of features and languages.\n\nWe next discuss the potential of typological information to guide multilingual NLP and the means by which this can be done.", "filtered_refids": [["b8"], ["b18", "b14", "b28", null, "b42", "b38", "b8", "b37"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3276, "num_references": 9}
{"corpusid_sectionid": "14225710-s2", "title": "Survey on the Use of Typological Information in Natural Language Processing", "date": "2016-10-11", "section_title": "Multilingual NLP and the Role of Typologies", "section": "The recent explosion of language diversity in electronic texts has made it possible for NLP to move increasingly towards multilingualism. The biggest challenge in this pursuit has been resource scarcity. In order to achieve high quality performance, NLP algorithms have relied heavily on manually crafted resources such as large linguistically-annotated datasets (treebanks, parallel corpora, etc.) and rich lexical databases (terminologies, dictionaries, etc.). While such resources are available for key NLP tasks (POS tagging, parsing, etc.) in well-researched languages (e.g. English, German, and Chinese), for the majority of other languages they are lacking altogether. Since resource creation is expensive and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem.\n\nOne avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009;Reichart and Rappoport, 2010;Snyder, 2010;Spitkovsky et al., 2011;Goldwasser et al., 2011;Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T\u00e4ckstr\u00f6m et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010;Zhang et al., 2012;T\u00e4ckstr\u00f6m et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance.\n\nLanguage Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005;McDonald et al., 2011;Petrov et al., 2012;Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008;McDonald et al., 2011;S\u00f8gaard, 2011;Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language.\n\nWhile such a transfer approach outperforms unsupervised learning, it does not achieve optimal performance. One potential reason for this is that the tagset used by a POS tagger may not fit a target language which exhibits significantly different morphological features to a source language for which the tagset was initially developed (Petrov et al., 2012). Although parallel data can be used to give additional guidance which improves transfer (McDonald et al., 2011), such data are only available for some language pairs and cannot be used in truly resource-poor situations.\n\nAn alternative direction that has recently emerged uses typological information as a form of non-parallel guidance in transfer. This direction capitalises on the fact that languages do exhibit systematic crosslingual connections at various levels of linguistic description (e.g. similarities in language structure), despite their great diversity. Captured in typological classifications at the level of generalisation useful for NLP, such information can be used to support multilingual NLP in a variety of ways (Bender, 2011). For example, it can be used to define the similarity between two languages with respect to the linguistic information one hopes to transfer; it can also help to define the optimal degree, level and method of transfer. For example, direct transfer of POS tagging is more likely to succeed when languages are sufficiently similar in terms of morphology in particular (Hana et al., 2004;Wisniewski et al., 2014).\n\nTypological information has been used to guide language transfer mostly in the areas of part-of-speech tagging and parsing, e.g. (Cohen and Smith, 2009;McDonald et al., 2011;Berg-Kirkpatrick and Klein, 2010;Naseem et al., 2012;T\u00e4ckstr\u00f6m et al., 2013). Section 4 surveys such works in more detail.\n\nMultilingual Joint Learning Another approach involves learning information for multiple languages simultaneously, with the idea that the languages will be able to support each other (Snyder, 2010;Navigli and Ponzetto, 2012). This can help in the challenging but common scenario where none of the languages involved has adequate resources. This applies even with English, where annotations needed for training basic tools are primarily available only for newspaper texts and a handful of other domains. In some areas of NLP, e.g. word sense disambiguation (Navigli and Ponzetto, 2012), multilingual learning has outperformed independent learning even for resource-rich languages, with larger gains achieved by increasing the number of languages.\n\nSuccess has also been achieved on morphosyntactic tasks. For example, Snyder (2010) observes that cross-lingual variations in linguistic structure correspond to systematic variations in ambiguity, so that what one language leaves implicit, another one will not. For instance, a given word may be tagged as either a verb or a noun, yet its equivalent in other languages may not present such ambiguity. Together with his colleagues, Snyder exploited this variation to improve morphological segmentation, POS tagging, and syntactic parsing for multiple languages. Naseem et al. (2012) introduced a selective sharing approach to improve multilingual dependency parsing where the model first chooses syntactic dependents from all the training languages and then selects their language-specific ordering to tie model parameters across related languages. Because the ordering decisions are influenced by languages with similar properties, this cross-lingual sharing is modelled using typological features. In such works, typological information has been used to facilitate the matching of structural features across languages, as well as in the selection of languages between which linguistic information should be shared.", "filtered_refids": [[], ["b11", "b75", "b55", "b27", "b66", "b45", null, "b64", "b61"], ["b52", "b57", "b41", "b32", "b62", "b74", "b73"], ["b52", "b41"], ["b30", "b72", "b4"], ["b11", "b46", "b41", "b66", "b5"], ["b47", "b61"], ["b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 6680, "num_references": 29}
{"corpusid_sectionid": "14225710-s3", "title": "Survey on the Use of Typological Information in Natural Language Processing", "date": "2016-10-11", "section_title": "Development of Universal Models", "section": "A long-standing goal that has gained renewed interest recently is the development of language-independent (i.e. universal) models for NLP (Bender, 2011;Petrov et al., 2012). Much of the recent interest has been driven by the Universal Dependencies (UD) initiative. It aims to develop cross-linguistically consistent treebank annotation for many languages for the purposes of facilitating multilingual parser development and cross-lingual learning (Nivre et al., 2016). The annotation scheme is largely based on universal Stanford dependencies (de Marneffe et al., 2014) and universal POS tags (Petrov et al., 2012). UD treebanks have been developed for 40 languages to date. Whilst still biased towards contemporary Indo-European languages, the collection developed by this initiative is gradually expanding to include additional language families.\n\nThe development of a truly universal resource will require taking into account typological variation for optimal coverage. For example, while the current UD scheme allows for language-specific tailoring, in the future, language type-specific tailoring may offer a useful alternative, aligned with the idea of universal modeling (Bender, 2011).", "filtered_refids": [["b52", "b48", "b4", "b21"], ["b4"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1193, "num_references": 5}
{"corpusid_sectionid": "250390996-s4", "title": "The Why and The How: A Survey on Natural Language Interaction in Visualization", "date": 2022, "section_title": "Discover Task", "section": "Using natural language to discover information is one of the most common visualization tasks targeted by V-NLI. Brehmer and Munzner (2013) differentiate between different levels of task granularity such as discover -search -query (see Figure 2). The discovery of concepts, objects, and relationships in a visualization depends on the role that the user and the interface take in the visualizationoriented dialogue, as well as on the concreteness of the user's intent. Intents are formulated in oral or written form. Less concrete user intents lead to a more exploratory character of the search. Concrete intents formalized in a query lead to a specific system response. Vague and fuzzy intents are much more difficult to formalize in a single query and must be inferred by the V-NLI through the application of intelligent recommendations or user guidance. interaction to a multi-turn interactive visual exploration also referred to as analytical conversation. Analytical conversation is the support of visual analysis processes by V-NLI with the aim of inspecting visual features through a visualization-oriented human-machine dialogue, as studied by Turkay and Henkin (2018); Aurisano et al. (2015). In contrast to visualization creation (see section 3.4), where visualizations are generated based on natural language text, the manipulation or composition of a visualization in the query dialog is used in the sense of a speech act. The produced or manipulated visualization can be seen here as a dynamically generated visual response to a user query with the goal of providing information in the dialog. Setlur and Tory (2017); Hoque et al.  , 2015;Yang et al., 2016;Anderson et al., 2018) with the goal to answer questions related to the visual content of images. In VIS, the aim is to answer complex questions related to visual models such as charts or scientific illustrations as in Singh and Shekhar (2020); Chaudhry et al. (2020). Infographics as sophisticated arrangements of visual elements and text are supported by VQA in Mathew et al. (2021). Meeting the high informative standards of response generation required to harness the explanatory purposes of visualizations presents itself as a challenging task.\n\nBrowsing. Browsing supports users with a vague or fuzzy data-related intent in discovering visualizations. The idea is to narrow down the user intent through language interaction using text input, multi-step questions, or dialogue and suggest appropriate next steps in the interaction with the visualization. Luo et al. (2018) use keyword input to execute personalized visualization recommendations. Other approaches leverage auto-completion in text input Dhamdhere et al., 2017) ", "filtered_refids": [[null, "b2", "b4", "b3"], ["b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 2700, "num_references": 5}
{"corpusid_sectionid": "250390996-s5", "title": "The Why and The How: A Survey on Natural Language Interaction in Visualization", "date": 2022, "section_title": "Produce Task", "section": "Brehmer and Munzner (2013) refer to produce as a 'reference to tasks in which the intent is to generate new artifacts'. Artifacts generated through natural language interaction are, e.g., annotations of objects in a visualization, scene descriptions, or task reports as used, e.g., in medical visual analysis.\n\nAnnotation. Annotating areas of interest, comparing them among each other, and sharing them with colleagues is a common language interaction while working with visualizations ( Vanhulst et al. (2021), who propose a classification framework that enables a structured capture and ordering of annotations.\n\nDocumentation. Visualization systems are used by experts, e.g., in the medical domain (Meuschke et al., 2021) to plan and discuss a surgery. Reporting, summarizing, and sharing this visualizationrelated work is an important task that is an additional burden to the surgeon and therefore should be executed by a machine. Nafari and Weaver (2013,2015) generate natural language questions from queries executed on a visualization resulting in a natural language translation of the interaction. This leaves a step-by-step report of the interaction finding usage as a report of done work.\n\nVisualization Creation. Visualization creation considers the production of a visual model from a natural language description -also referred to as text2viz. Rashid et al. (2021) (2016) design an interactive production process for generating timelines from unstructured text input. Language-based 3D scene generation, also referred to as text2scene, which allows users to describe 3D scenes using text without having to learn software tools, is investigated in Coyne and Sproat (2001) ", "filtered_refids": [[], [null], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1684, "num_references": 3}
{"corpusid_sectionid": "250390996-s7", "title": "The Why and The How: A Survey on Natural Language Interaction in Visualization", "date": 2022, "section_title": "NLP Methods", "section": "For each paper in the collection, both the NLP methods used, if any, and if named the specific NLP toolkits used for implementation are elaborated. For the sake of clarity, the methods are roughly divided into two areas: Natural Language Understanding (NLU) and Natural Language Generation (NLG). The majority of the systems apply standard NLP methods like tokenization, stemming or stopword removal to pre-process text inputs, which is why these are not recorded separately. For a detailed inspection, we refer to Appendix C. Figure 3 shows the distribution of applied NLU methods over all papers. Semantic Parsing, which relies on rule-based mapping procedures from recognized input tokens to semantic predicates, is predominantly used. Often, POS-Tagging, Word Embeddings, and Named Entity Recognition (NER) are additionally applied to increase the accuracy of the mapping. For Word Sense Disambiguation WordNet, VerbNet or ConceptNet are leveraged. Speech-to-Text APIs are a common method used in many systems to enable auditory input. A small number of pioneering  Fluid interaction between user and system in real time is a crucial factor for the success of a visualization application. Adopting state-of-the-art deep learning models to real-time interactions in visualization, e.g., by using Knowledge Distillation (Hinton et al., 2015) or Quantization (Jacob et al., 2018) leaves space for future work. Figure 4 shows the distribution of applied NLG methods over all papers. Template-based language generation is used by the majority of the systems followed by a significantly smaller number of deep learning-based Seq2Seq Modeling approaches. Multi-turn systems are predominantly based on rule-based or probabilistic Dialogue Management. Only a few systems use the Text-to-Speech functionality, as most of the generated responses consist of visual elements. In order to advance the adoption of deep learningbased methods in visualization-related text generation, extensive training data sets are required, as pointed out by Kumar et al. (2020a). There is a limited number of data sets for Visualization Description Generation (Obeid and Hoque, 2020), Visual Question Answering (Mathew et al., 2021; and Natural Language Querying (Fu et al., 2020;Srinivasan et al., 2021b;Luo et al., 2021). In particular, the compilation of data sets for emerging dialogue scenarios in Analytical Conversation, Hypothesis Verification or collaborative authoring in Visualization Creation would motivate the use of deep learning based NLP methods in these tasks. Therefore, generating high-quality data sets for the aforementioned visualization tasks leaves room for future work.", "filtered_refids": [["b4", "b9", null, "b2", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2670, "num_references": 5}
{"corpusid_sectionid": "245144787-s1", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Definitions of Robustness in NLP", "section": "Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) \u223c D and its prediction over x as f (x); now given test data (x , y ) \u223c D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )\u223cD [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).\n\nThe above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.", "filtered_refids": [["b40"], ["b18", null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1283, "num_references": 4}
{"corpusid_sectionid": "245144787-s2", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Robustness against Adversarial Attacks", "section": "In one line of research, D is constructed by perturbations around input x to form x (x typically being defined within some proximity of x). This topic has been widely explored in computer vision under the concept of adversarial robustness, which measures models' performances against carefully crafted noises generated deliberately to deceive the model to predict wrongly, pioneered by (Szegedy et al., 2013;Goodfellow et al., 2015), and later extended to NLP, such as (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022) and multilingual adversaries . The generation of adversarial examples primarily builds upon the observation that we can generate samples that are meaningful to humans (e.g., by perturbing the samples with changes that are imperceptible to humans) while altering the prediction of the models for this sample. In this regard, human's remarkable ability in understanding a large set of synonyms (Li et al., 2020) or interesting characteristics in ignoring the exact order of letters  are often opportunities to create adversarial examples. A related line of work such as data-poisoning (Wallace et al., 2021) and weight-poisoning (Kurita et al., 2020) exposes NLP models' vulnerability against attacks during the training process. One can refer to more comprehensive reviews and broader discussions on this topic in Zhang et al. (2020c) and Morris et al. (2020b). Assumptions around Label-preserving and Semantic-preserving Most existing work in vision makes a relatively simplified assumption that the gold label of x remains unchanged under a bounded perturbation over x, i.e., y = y, and a model's robust behaviour should be f (x ) = y (Szegedy et al., 2013;Goodfellow et al., 2015). A similar line of work in NLP follows the same label-preserving assumption with small text perturbations like token and character swapping (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019;Ebrahimi et al., 2018), paraphrasing (Iyyer et al., 2018;Gan and Ng, 2019), semantically equivalent adversarial rules (Ribeiro et al., 2018), and adding distractors (Jia and Liang, 2017). However, this label-preserving assumption might not always hold, e.g., Wang et al. (2021b) studied several existing text perturbation techniques and found that a significant portion of perturbed examples are not label-preserving (despite their label-preserving assumptions), or the resulting labels have a high disagreement among human raters (i.e., can even fool humans). Morris et al. (2020a) also call for more attention to the validity of perturbed examples for a more accurate robustness evaluation.\n\nAnother line of work aims to perturb the input x to x in small but meaningful ways that explicitly change the gold label, i.e., y = y, under which case the robust behaviour of a model should be f (x ) = y and f (x ) = y (Gardner et al., 2020;Kaushik et al., 2019;Schlegel et al., 2021). We believe these two lines of work are complementary to each other, and both should be explored in future research to measure models' robustness more comprehensively.\n\nOne alternative notion is whether the perturbation from x to x is \"semantic-preseving\" (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019) or \"semantic-modifying\" (Shi and Huang, 2020;Jia and Liang, 2017). Note this is slightly different from the above label-preserving assumptions, as it is defined over the perturbations on (x, x ) rather than making an assumption on (y, y ), e.g., semantic-modifying perturbations can be either label-preserving (Jia and Liang, 2017;Huang, 2020) or label-changing (Gardner et al., 2020;Kaushik et al., 2019).", "filtered_refids": [["b52", "b63", "b6", "b50", null, "b80", "b23", "b7", "b24", "b74"], [null, "b1"], [null, "b1"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 3831, "num_references": 14}
{"corpusid_sectionid": "245144787-s3", "title": "Measure and Improve Robustness in NLP Models: A Survey", "date": "2021-12-15", "section_title": "Robustness under Distribution Shift", "section": "Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-juss\u00e0, 2019).", "filtered_refids": [["b67", "b26", "b21", null, "b83", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1535, "num_references": 6}
{"corpusid_sectionid": "173188095-s1", "title": "A Survey on Biomedical Image Captioning", "date": "2019-05-26", "section_title": "Datasets", "section": "Datasets for biomedical image captioning comprise medical images and associated texts. Publicly available datasets contain X-rays (IU X-RAY in Table 1), clinical photographs (PEIR GROSS in Table 1), or a mixture of X-rays and photographs (ICLEF-CAPTION in Table 1). The associated texts may be single sentences describing the images, or longer medical reports based on the images (e.g., as in Figure 1b). Current publicly available datasets are rather small (IU X-RAY, PEIR GROSS) or noisy (e.g., IMAGE-CLEF, which is the largest dataset, was created by automatic means that introduced a lot of noise). We do not include in Table 1 datasets like the one of Wang et al. (2017), because their medical reports are not publicly available. 2 Furthermore, we observe that all three publicly available biomedical image captioning datasets suffer from two main shortcomings:\n\n\u2022 There is a great class imbalance, with most images having no reported findings.\n\n\u2022 The wide range of diseases leads to very scarce occurrences of disease-related terms, making it difficult for models to generalize. -Fushman et al. (2015) presented an approach for developing a collection of radiology examinations, including images and narrative reports by radiologists. The authors suggested an accurate anonymization approach for textual radiology reports and provided public access to their dataset through the Open Access Biomedical Image Search Engine (OpenI). 3 The images are 7,470 frontal and lateral chest X-rays, and each radiology report consists of four sections. The 'comparison' section contains previous information about the patient (e.g., preceding medical exams); the 'indication' section contains symptoms (e.g., hypoxia) or reasons of examination (e.g., age); 'findings' lists the radiology observations; and 'impression' outlines the final diagnosis. A system would ideally generate the 'findings' and 'impression' sections, possibly concatenated (Jing et al., 2018). The 'impression' and 'findings' sections of the dataset of Demner-Fushman et al. (2015) were used to manually associate each report with a number of tags (called manual encoding), which were Medical Subject Heading (MESH) 4 and RadLex 5 terms assigned by two trained coders. Additionally, each report was associated with automatically extracted tags, produced by Medical Text Indexer 6 (called MTI encoding). These tags allow systems to learn to initially generate terms describing the image and then use the image along with the generated terms to produce the caption. Hence, this dataset, which is the only one in the field with manually annotated tags, has an added value. From our processing, we found that 104 reports contained no image, 489 were missing 'findings', 6 were missing 'impression', and 25 were missing both 'findings' and 'impression'; the 40 image-caption-tags triplets corresponding to the latter 25 reports were discarded in our later experiments. We shuffled the instances of the dataset (image-text-tags triplets) and used 6,674 of them as the training set (images from the 90% of the reports), with average caption length 38 words and vocabulary size 2,091. Only 2,745 training captions were unique, because 59% of them were the same in more than one image (e.g., similar images with the same condition). Table 1 provides more information about the datasets and their splits.", "filtered_refids": [["b57"], [], ["b21", null, "b9"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3359, "num_references": 4}
{"corpusid_sectionid": "173188095-s5", "title": "A Survey on Biomedical Image Captioning", "date": "2019-05-26", "section_title": "ICLEF-CAPTION", "section": "This dataset was released in 2017 for the Image Concept Detection and Caption Prediction (ICLEF-CAPTION) task (Eickhoff et al., 2017) of IMAGE-CLEF (de Herrera et al., 2018). The dataset consists of 184,614 biomedical images and their captions, extracted from biomedical articles on PubMed Central (PMC). 11 The organizers used an automatic method, based on a biomedical image 7 http://peir.path.uab.edu/library/ 8 PEIR pathology contains 23 sub-categories, but only 22 contain a gross sub-collection (7,443 images in total). We observe that one image was not included by Jing et al. (2018). 9 Our code is publicly available at https://github. com/nlpaueb/bio_image_caption. 10 We used 10% of the dataset for testing, as the 1k images used by Jing et al. for validation and testing were not released. 11 https://www.ncbi.nlm.nih.gov/pmc/ type hierarchy (M\u00fcller et al., 2012), to classify the 5.8M extracted images as clinical or not and also discard compound ones (e.g., images consisting of multiple X-rays), but their estimation was that the overall noise in the dataset would be as high as 10% or 20% (Eickhoff et al., 2017).\n\nIn 2018, the ICLEF-CAPTION organizers employed a Convolutional Neural Network (CNN), to classify the same 5.8M images based on their type and to extract the non-compound clinical ones, leading to 232,305 images along with their respective captions (de Herrera et al., 2018). Although they reported that compound images were reduced, they noted that noise still exists, with nonclinical images present (e.g., images of maps). Additionally, a wide diversity between the types of the images has been reported (Liang et al., 2017). The length of the captions varies from 1 to 816 words (Su et al., 2018;Liang et al., 2017). Only 1.4% of the captions are duplicates (associated with more than one image), probably due to the wide image type diversity. The average caption length is 21 words and the vocabulary size is 157,256. A further 10k instances were used for testing in 2018, but they are not publicly available. Hence, in our experiments we split the 235,305 instances into training and test subsets ( Table 1).\n\nFor tag annotation, the organizers used QUICK-UMLS (Soldaini and Goharian, 2016) to identify concepts of the Unified Medical Language System (UMLS) in the caption text, extracting 111,155 unique concepts from the 222,305 captions. Each image is linked to 30 UMLS concepts, on average, while fewer than 6k have one or two asso-ciated concepts and there are images associated with even thousands of concepts. The organizers observe the existence of noise and note that irrelevant concepts have been extracted, mainly due to the fully automatic extraction process. Varges et al. (2012) employed Natural Language Generation to assist medical professionals turn cardiological findings (e.g., from diagnostic imaging procedures) into fluent and readable textual descriptions. From a different perspective, Schlegl et al. (2015) used both the image and the textual report as input to a CNN, trained to classify images with the help of automatically extracted semantic concepts from the textual report. Kisilev et al. (2015a,b) employed a radiologist to mark an image lesion, and a semi-automatic segmentation approach to define the boundaries of that lesion. Then, they used structured Support Vector Machines (Tsochantaridis et al., 2004) to generate semantic tags, originating from a radiology lexicon, for each lesion. In subsequent work they used a CNN to rank suspicious regions of diagnostic images and, then, generate tags for the top ranked regions, which can be embedded in diagnostic sentence templates (Kisilev et al., 2016). Shin et al. (2016) were the first to apply a CNN-RNN encoder-decoder approach to generate captions from medical images. They used the IU X-RAY dataset and a Network in Network (Lin et al., 2013) or GoogLeNet (Szegedy et al., 2015) as the encoder of the images, obtaining better results with GoogLeNet. The encoder was pretrained to predict (from the images) 17 classes, corresponding to MESH terms that were frequent in the reports and did not co-occur frequently with other MESH terms. An LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) was used as the RNN decoder to generate image descriptions from the image encodings. In a second training phase, the mean of the RNNs state vectors (obtained while describing each image) was used as an improved representation of each training image. The original 17 classes that had been used to pretrain the CNN were replaced by 57 finer classes, by applying k-means clustering to the improved vector representations of the training images. The CNN was then retrained to predict the 57 new classes and this led to improved BLEU (Papineni et al., 2002) scores for the overall CNN-RNN system. The gen-erated descriptions, however, were not evaluated by humans. Furthermore, the generated descriptions were up to 5 words long and looked more like bags of terms (e.g., 'aorta thoracic, tortuous, mild'), rather than fluent coherent reports. Zhang et al. (2017b) were the first to employ an attention mechanism in biomedical image to text generation, with their MDNET. 12 MDNET used RESNET (He et al., 2016) for image encoding, but extending its skip connections to address vanishing gradients. The image representation acts as the starting hidden state of a decoder LSTM, enhanced with an attention mechanism over the image. (During training, this attention mechanism is also employed to detect diagnostic labels.) The decoder is cloned to generate a fixed number of sentences, as many as the symptom descriptions. This model performed slightly better than a state of the art generic image captioning model (Karpathy and Fei-Fei, 2015) in most evaluation measures. Jing et al. (2018) segment each image to equally sized patches and use   (Simonyan and Zisserman, 2014) to separately encode each patch as a 'visual' feature vector. A Multi-Layer Perceptron (MLP) is then fed with the visual feature vectors of each image (representing its patches) and predicts terms from a pre-determined term vocabulary. The word embeddings of the predicted terms of each image are treated as 'semantic' feature vectors representing the image. The decoder, which produces the image description, is a hierarchical RNN, consisting of a sentencelevel LSTM and a word-level LSTM. The sentencelevel LSTM produces a sequence of embeddings, each specifying the information to be expressed by a sentence of the image description (acting as a topic). For each sentence embedding, the wordlevel LSTM then produces the words of the corresponding sentence, word by word. More precisely, at each one of its time-steps, the sentencelevel LSTM of Jing et al. examines both the visual and the semantic feature vectors of the image. Following previous work on image captioning, that added attention to encoder-decoder approaches (Xu et al., 2015;You et al., 2016;Zhang et al., 2017b), an attention mechanism (an MLP fed with the current state of the sentence-level LSTM and each one of the visual feature vectors of the image) assigns attention scores to the visual feature vectors, and the weighted sum of the visual feature vectors (weighted by their attention scores) becomes a visual 'context' vector, specifying which patches of the image to express by the next sentence. Another attention mechanism (another MLP) assigns attention scores to the semantic feature vectors (that represent the terms of the image), and the weighted sum of the semantic feature vectors (weighted by attention) becomes the semantic context vector, specifying which terms of the image to express by the next sentence. At each time-step, the sentence-level LSTM considers the visual and semantic context vectors, produces a sentence embedding and updates its state, until a stop control instructs it to stop. Given the sentence embedding, the word-level LSTM produces the words of the corresponding sentence, again until a special 'stop' token is generated. Jing et al. showed that their model outperforms models created for general image captioning with visual attention (Vinyals et al., 2015;Donahue et al., 2015;Xu et al., 2015;You et al., 2016).  adopted an approach similar to that of Jing et al. (2018), using a RESNET-based CNN to encode the images and an LSTM decoder to produce image descriptions, but their LSTM is flat, as opposed to the hierarchical LSTM Wang et al. 13 ICLEF-CAPTION run successfully for two consecutive years (Eickhoff et al., 2017;de Herrera et al., 2018) and stopped in 2019. Participating systems (see Table 3) used image similarity to retrieve images similar to the one to be described, then aggregating the captions of the retrieved images; or they employed an encoder-decoder architecture; or they simply classified each image based on UMLS concepts and then aggregated the respective UMLS 'semantic groups' 14 to form a caption. Liang et al. (2017) used a pre-trained VG-GNET CNN encoder and an LSTM decoder, similarly to Karpathy and Fei-Fei (2015). They trained three such models on different caption lengths and used an SVM classifier to choose the most suitable decoder for the given image. Furthermore, they used a 1-Nearest Neighbor method to retrieve the caption of the most similar image and aggregated it with the generated caption. Zhang et al. (2018), who achieved the best results in 2018, used the Lucene Image Retrieval software (LIRE) to retrieve images from the training set and then simply concatenated the captions of the top three retrieved images to obtain the new caption. Abacha et al. (2017) used GoogLeNet to detect UMLS concepts and returned the aggregation of their respective UMLS semantic groups as a caption. Su et al. (2018) and Rahman (2018) also employed different encoder-decoder architectures. Gale et al. (2018) argued that existing biomedical image captioning systems fail to produce a satisfactory medical diagnostic report from an image, and to provide evidence for a medical decision. They focused on classifying hip fractures in pelvic X-rays, and argued that the diagnostic report of such narrow medical tasks could be simplified to two sentence templates; one for positive cases, including 5 placeholders to be filled by descriptive terms, and a fixed negative one. They used DENSENET (Huang et al., 2017) to get image embeddings and a two-layer LSTM, with attention over the image, to generate the constrained textual report. Their results, shown in Table 2, are very high, but this is expected due to the extremely simplified and standardized ground truth reports. (Gale et al. report an improvement of more than 50 BLEU points when employing this assumption.) The reader is also warned that the results of Table 2 are not directly comparable, since they are obtained from very different datasets.  Table 2: Evaluation of biomedical image captioning methods with BLEU-1/-2/-3/-4 (B1, B2, B3, B4), METEOR (MET), ROUGE-L (ROU), and CIDER (CID) percentage scores. Zhang et al. (2017a) and Han et al. (2018) also performed biomedical captioning, but did not provide any evaluation results. Datasets with \u2020 are not publicly available; BDIDR consists of 1,000 pathological bladder cancer images, each with 5 reports; FRONTAL PELVIC X-RAYS comprises 50,363 images, each supplemented with a radiology report, but simplified to a standard template; CHEST X-RAY 14 is publicly available, but without its medical reports.  Table 3: Top-5 participating systems at the ICLEF-CAPTION competition, ranked based on average BLEU (%), the official evaluation measure. Systems used an encoder-decoder (ED), image retrieval (IR), or classified UMLS concepts (CLS). We exclude 2017 systems employing external resources, which may have seen test data during training (Eickhoff et al., 2017). 2018 models were limited to use only pre-trained CNNs.", "filtered_refids": [["b21", null, "b13", "b37"], ["b49", "b29", "b18"], ["b52", "b63", "b10", "b40", "b13", "b50", "b20", "b59", "b19", "b17", "b46", "b18", "b49", "b21", "b7", "b16", "b55", "b45", "b22", "b24", "b53", "b61", "b15", "b47", "b31", "b29", "b48", null, "b62", "b64", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 68, "num_chars": 11881, "num_references": 38}
{"corpusid_sectionid": "173188095-s7", "title": "A Survey on Biomedical Image Captioning", "date": "2019-05-26", "section_title": "Evaluation", "section": "The most common evaluation measures in biomedical image captioning are BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005), which originate from machine translation and summarization. The more recent CIDER measure , which was designed for general image captioning (Kilickaya et al., 2016), has been used in only two biomedical image captioning works (Zhang et al., 2017b;Jing et al., 2018). SPICE (Anderson et al., 2016), which was also designed for general image captioning (Kilickaya et al., 2016), has not been used in any biomedical image captioning work we are aware of. Below, we describe each measure separately and discuss its advantages and limitations with respect to biomedical image captioning. BLEU is the most common measure (Papineni et al., 2002). It measures word n-gram overlap between the generated and the ground truth caption.\n\nA brevity penalty is added to penalize short generated captions. BLEU-1 considers unigrams (i.e., words), while BLEU-2, -3, -4 consider bigrams, trigrams, and 4-grams respectively. The average of the four variants was used as the official measure in ICLEF-CAPTION.\n\nMETEOR (Banerjee and Lavie, 2005) extended BLEU-1 by employing the harmonic mean of precision and recall (F-score), biased towards recall, and by also employing stemming (Porter stemmer) and synonymy (WordNet). To take into account longer subsequences, it includes a penalty of up to 50% when no common n-grams exist between the machine-generated description and the reference. ROUGE-L (Lin et al., 2013) is the ratio of the length of the longest common subsequence between the machine-generated description and the reference human description, to the size of the reference (ROUGE-L recall); or to the generated description (ROUGE-L precision); or a combination of the two (ROUGE-L F-measure). We note that several ROUGE variants exist, based on different ngram lengths, stemming, stopword removal, etc., but ROUGE-L is the most commonly used variant in biomedical image captioning so far. CIDER  measures the cosine similarity between n-gram TF-IDF representations of the two captions (words are also stemmed). This is calculated for unigrams to 4grams and their average is returned as the final evaluation score. The intuition behind using TF-IDF is to reward frequent caption terms while penalizing common ones (e.g., stopwords). However, biomedical image captioning datasets contain many scientific terms (e.g., disease names) that are common across captions (or more gener-ally document collections), which may be mistakenly penalized. We also noticed that the scores returned by the provided CIDER implementation may exceed 100%. 15 We exclude CIDER results, since these issues need to be investigated further. SPICE (Anderson et al., 2016) extracts tuples from the two captions (machine-generated, reference), containing objects, attributes and/or relations; e.g., (patient), (has, pain), (male, patient). Precision and recall are computed using WordNet synonym matching between the two sets of tuples, and the F1 score is returned. The creators of SPICE report improved results over both METEOR and CIDER, but it has been noted that results depend on parsing quality (Kilickaya et al., 2016). When experimenting with the provided implementation 16 of this measure, we noticed that it failed to parse long texts to evaluate them. Similarly to CIDER, we exclude SPICE from further analysis below.\n\nWord Mover's Distance (WMD) (Kusner et al., 2015) computes the minimum cumulative cost required to move all word embeddings of one caption to aligned word embeddings of the other caption. 17 It completely ignores, however, word order, and thus readability, which is one of the main assessment dimensions in the biomedical field (Tsatsaronis et al., 2015). Other previously discussed n-gram based measures also largely ignore word order, but at least consider local order (inside n-grams). WMD scores are included in Table 4 as similarity values WMS = (1 + WMD) \u22121 .", "filtered_refids": [["b40", "b30", "b21", "b23", "b64", "b1", "b3"], [], ["b31", null, "b23", "b1", "b3"], ["b51", "b27"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4018, "num_references": 14}
{"corpusid_sectionid": "208541377-s2", "title": "The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations Conference or Workshop Item The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations", "date": 2019, "section_title": "Rating and Likert scales", "section": "In this section we use illustrative examples to underline the differences between rating and Likert scales. We use the term scale with the following two meanings:\n\n\u2022 Given a statement, the term scale is the group of points making up the options offered to respondents. We refer to the combination of the statement and the scale as an item.\n\n\u2022 In the case of an aggregate scale 2 , such as the Likert scale, we use the term scale to indicate a collection of items.\n\nRating scales: Rating scales are items used in surveys to estimate feeling, opinions or attitudes of responders. The data collected through a rating scale can be interpreted both as ordinal and interval. A rating scale is composed of an n-point scale. Scales with 3, 5, 7, 10 or 11 points are used most often. Rating scales can be both numerical and verbal.\n\nIn a numerical rating scale, a number is associated with each point. A variation of a numerical scale uses label words at the extreme values and leaves the intermediate values with a numerical label, as for example shown in Figure 1. A rat- ing scale that uses words as labels for the points is named a graphic rating scale 3 . An example of this kind of rating scale is pictured in Figure 2. Some- times the points of a graphic rating scale can also be labelled with numbers. Another sort of rating scale is the comparative rating scale. This kind of scale is used to ask respondents to answer a question in terms of a comparison. An example of a comparative rating scale is given in Figure 3.\n\nLikert scale: A Likert scale is an aggregate scale. The items that make a Likert scale are In other words, it is a composite of items which are summed or averaged all together to get an overall positive or negative orientation towards the object under examination in the survey.\n\n3 Sometimes a graphic rating scale is called Likert item or Likert-style scale. However, Likert items and Likert-style scale are particular cases of graphic rating scales. graphic rating scales. In this context, each graphic rating scale is called a Likert item. Likert scales are usually expressed in terms of agreement and disagreement. An example of a Likert scale is shown in Figure 4. The items that make a Lik- ert scales are designed to collectively capture the phenomenon under analysis. Accordingly, they shouldn't be considered in isolation and they should be summed or averaged to produce a total score. However, individual items by themselves are often considered as a single scale. Because of this ambivalent use of the Likert scale and its items, the nature of the Likert scale is highly controversial. Researchers are split between who consider it an interval scale and those who consider it an ordinal scale; see for example Jamieson (2004), Pell (2005), Norman (2010).\n\nThe confusion generated by the ambivalent use of the Likert scale and its items is well illustrated and explained in Joshi et al. (2015), where an image similar to Figure 5 is introduced. Likert scales are built in such a way that respondents express their level of agreement or disagreement with the sentences expressed by the Likert items. Because all the items are presented all together and with the same point labels, it is assumed that each respondent gives the same interpretation to the answer points -that is, as suggested by Likert, the distances between the points in the scale can be considered equal. 4 This assumption licenses use of the scale as an interval scale. Consequently, adding or averaging the items annotated by the same respondent is justified. This raises the interval interpretation, depicted by the vertical arrow of Figure 5.\n\nOtherwise, an item-by-item analysis -that is a separate analysis of a single item extracted from an aggregate scale -cannot justify the assumption that the difference between adjacent points is equal. Indeed, we cannot assume that different respondents perceive the difference between adjacent label points as being of equal distance. The difference between \"agree\" and \"strongly agree\" can be perceived differently from one respondent to another. Consequently, the addition or the average of the items extracted from an aggregate scale is not justified. In such cases, the median or mode can be used as the measure of central tendency. This follows the ordinal interpretation, depicted by the horizontal arrow of Figure 5.\n\nUnfortunately, in many cases there is not a clear understanding of the difference between the horizontal and the vertical direction of the aggregate scale. It is common to see item-by-item analysis (that is the horizontal direction) that makes use of parametric statistics without a justification of this choice. Indeed, as shown in Section 4, the interpretation of Likert items as interval scales has become a common practice. This particularly applies to the use of the mean for measuring the central tendency for the analysis of Likert items. 4 Some authors, for example Jamieson (2004), do not accept such an assumption and do not consider the points as equally distant. In this case the Likert scales themselves, and not only the Likert items, are considered ordinal.\n\nIn this section we use illustrative examples to underline the differences between rating and Likert scales. We use the term scale with the following two meanings:\n\n\u2022 Given a statement, the term scale is the group of points making up the options offered to respondents. We refer to the combination of the statement and the scale as an item.\n\n\u2022 In the case of an aggregate scale 2 , such as the Likert scale, we use the term scale to indicate a collection of items.\n\nRating scales: Rating scales are items used in surveys to estimate feeling, opinions or attitudes of responders. The data collected through a rating scale can be interpreted both as ordinal and interval. A rating scale is composed of an n-point scale. Scales with 3, 5, 7, 10 or 11 points are used most often. Rating scales can be both numerical and verbal.\n\nIn a numerical rating scale, a number is associated with each point. A variation of a numerical scale uses label words at the extreme values and leaves the intermediate values with a numerical label, as for example shown in Figure 1. A rat- ing scale that uses words as labels for the points is named a graphic rating scale 3 . An example of this kind of rating scale is pictured in Figure 2. Some- times the points of a graphic rating scale can also be labelled with numbers. Another sort of rating scale is the comparative rating scale. This kind of scale is used to ask respondents to answer a question in terms of a comparison. An example of a comparative rating scale is given in Figure 3.\n\nLikert scale: A Likert scale is an aggregate scale. The items that make a Likert scale are In other words, it is a composite of items which are summed or averaged all together to get an overall positive or negative orientation towards the object under examination in the survey.\n\n3 Sometimes a graphic rating scale is called Likert item or Likert-style scale. However, Likert items and Likert-style scale are particular cases of graphic rating scales. graphic rating scales. In this context, each graphic rating scale is called a Likert item. Likert scales are usually expressed in terms of agreement and disagreement. An example of a Likert scale is shown in Figure 4. The items that make a Lik- ert scales are designed to collectively capture the phenomenon under analysis. Accordingly, they shouldn't be considered in isolation and they should be summed or averaged to produce a total score. However, individual items by themselves are often considered as a single scale. Because of this ambivalent use of the Likert scale and its items, the nature of the Likert scale is highly controversial. Researchers are split between who consider it an interval scale and those who consider it an ordinal scale; see for example Jamieson (2004), Pell (2005), Norman (2010).\n\nThe confusion generated by the ambivalent use of the Likert scale and its items is well illustrated and explained in Joshi et al. (2015), where an image similar to Figure 5 is introduced. Likert scales are built in such a way that respondents express their level of agreement or disagreement with the sentences expressed by the Likert items. Because all the items are presented all together and with the same point labels, it is assumed that each respondent gives the same interpretation to the answer points -that is, as suggested by Likert, the distances between the points in the scale can be considered equal. 4 This assumption licenses use of the scale as an interval scale. Consequently, adding or averaging the items annotated by the same respondent is justified. This raises the interval interpretation, depicted by the vertical arrow of Figure 5.\n\nOtherwise, an item-by-item analysis -that is a separate analysis of a single item extracted from an aggregate scale -cannot justify the assumption that the difference between adjacent points is equal. Indeed, we cannot assume that different respondents perceive the difference between adjacent label points as being of equal distance. The difference between \"agree\" and \"strongly agree\" can be perceived differently from one respondent to another. Consequently, the addition or the average of the items extracted from an aggregate scale is not justified. In such cases, the median or mode can be used as the measure of central tendency. This follows the ordinal interpretation, depicted by the horizontal arrow of Figure 5.\n\nUnfortunately, in many cases there is not a clear understanding of the difference between the horizontal and the vertical direction of the aggregate scale. It is common to see item-by-item analysis (that is the horizontal direction) that makes use of parametric statistics without a justification of this choice. Indeed, as shown in Section 4, the interpretation of Likert items as interval scales has become a common practice. This particularly applies to the use of the mean for measuring the central tendency for the analysis of Likert items. 4 Some authors, for example Jamieson (2004), do not accept such an assumption and do not consider the points as equally distant. In this case the Likert scales themselves, and not only the Likert items, are considered ordinal.", "filtered_refids": [[], [], [], [], [], [], ["b14", "b5"], ["b7"], [], ["b5"], [], [], [], [], [], [], ["b14", "b5"], ["b7"], [], ["b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 92, "num_chars": 10284, "num_references": 8}
{"corpusid_sectionid": "208541377-s15", "title": "The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations Conference or Workshop Item The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations", "date": 2019, "section_title": "Rating and Likert scales", "section": "In this section we use illustrative examples to underline the differences between rating and Likert scales. We use the term scale with the following two meanings:\n\n\u2022 Given a statement, the term scale is the group of points making up the options offered to respondents. We refer to the combination of the statement and the scale as an item.\n\n\u2022 In the case of an aggregate scale 2 , such as the Likert scale, we use the term scale to indicate a collection of items.\n\nRating scales: Rating scales are items used in surveys to estimate feeling, opinions or attitudes of responders. The data collected through a rating scale can be interpreted both as ordinal and interval. A rating scale is composed of an n-point scale. Scales with 3, 5, 7, 10 or 11 points are used most often. Rating scales can be both numerical and verbal.\n\nIn a numerical rating scale, a number is associated with each point. A variation of a numerical scale uses label words at the extreme values and leaves the intermediate values with a numerical label, as for example shown in Figure 1. A rat- ing scale that uses words as labels for the points is named a graphic rating scale 3 . An example of this kind of rating scale is pictured in Figure 2. Some- times the points of a graphic rating scale can also be labelled with numbers. Another sort of rating scale is the comparative rating scale. This kind of scale is used to ask respondents to answer a question in terms of a comparison. An example of a comparative rating scale is given in Figure 3.\n\nLikert scale: A Likert scale is an aggregate scale. The items that make a Likert scale are In other words, it is a composite of items which are summed or averaged all together to get an overall positive or negative orientation towards the object under examination in the survey.\n\n3 Sometimes a graphic rating scale is called Likert item or Likert-style scale. However, Likert items and Likert-style scale are particular cases of graphic rating scales. graphic rating scales. In this context, each graphic rating scale is called a Likert item. Likert scales are usually expressed in terms of agreement and disagreement. An example of a Likert scale is shown in Figure 4. The items that make a Lik- ert scales are designed to collectively capture the phenomenon under analysis. Accordingly, they shouldn't be considered in isolation and they should be summed or averaged to produce a total score. However, individual items by themselves are often considered as a single scale. Because of this ambivalent use of the Likert scale and its items, the nature of the Likert scale is highly controversial. Researchers are split between who consider it an interval scale and those who consider it an ordinal scale; see for example Jamieson (2004), Pell (2005), Norman (2010).\n\nThe confusion generated by the ambivalent use of the Likert scale and its items is well illustrated and explained in Joshi et al. (2015), where an image similar to Figure 5 is introduced. Likert scales are built in such a way that respondents express their level of agreement or disagreement with the sentences expressed by the Likert items. Because all the items are presented all together and with the same point labels, it is assumed that each respondent gives the same interpretation to the answer points -that is, as suggested by Likert, the distances between the points in the scale can be considered equal. 4 This assumption licenses use of the scale as an interval scale. Consequently, adding or averaging the items annotated by the same respondent is justified. This raises the interval interpretation, depicted by the vertical arrow of Figure 5.\n\nOtherwise, an item-by-item analysis -that is a separate analysis of a single item extracted from an aggregate scale -cannot justify the assumption that the difference between adjacent points is equal. Indeed, we cannot assume that different respondents perceive the difference between adjacent label points as being of equal distance. The difference between \"agree\" and \"strongly agree\" can be perceived differently from one respondent to another. Consequently, the addition or the average of the items extracted from an aggregate scale is not justified. In such cases, the median or mode can be used as the measure of central tendency. This follows the ordinal interpretation, depicted by the horizontal arrow of Figure 5.\n\nUnfortunately, in many cases there is not a clear understanding of the difference between the horizontal and the vertical direction of the aggregate scale. It is common to see item-by-item analysis (that is the horizontal direction) that makes use of parametric statistics without a justification of this choice. Indeed, as shown in Section 4, the interpretation of Likert items as interval scales has become a common practice. This particularly applies to the use of the mean for measuring the central tendency for the analysis of Likert items. 4 Some authors, for example Jamieson (2004), do not accept such an assumption and do not consider the points as equally distant. In this case the Likert scales themselves, and not only the Likert items, are considered ordinal.\n\nIn this section we use illustrative examples to underline the differences between rating and Likert scales. We use the term scale with the following two meanings:\n\n\u2022 Given a statement, the term scale is the group of points making up the options offered to respondents. We refer to the combination of the statement and the scale as an item.\n\n\u2022 In the case of an aggregate scale 2 , such as the Likert scale, we use the term scale to indicate a collection of items.\n\nRating scales: Rating scales are items used in surveys to estimate feeling, opinions or attitudes of responders. The data collected through a rating scale can be interpreted both as ordinal and interval. A rating scale is composed of an n-point scale. Scales with 3, 5, 7, 10 or 11 points are used most often. Rating scales can be both numerical and verbal.\n\nIn a numerical rating scale, a number is associated with each point. A variation of a numerical scale uses label words at the extreme values and leaves the intermediate values with a numerical label, as for example shown in Figure 1. A rat- ing scale that uses words as labels for the points is named a graphic rating scale 3 . An example of this kind of rating scale is pictured in Figure 2. Some- times the points of a graphic rating scale can also be labelled with numbers. Another sort of rating scale is the comparative rating scale. This kind of scale is used to ask respondents to answer a question in terms of a comparison. An example of a comparative rating scale is given in Figure 3.\n\nLikert scale: A Likert scale is an aggregate scale. The items that make a Likert scale are In other words, it is a composite of items which are summed or averaged all together to get an overall positive or negative orientation towards the object under examination in the survey.\n\n3 Sometimes a graphic rating scale is called Likert item or Likert-style scale. However, Likert items and Likert-style scale are particular cases of graphic rating scales. graphic rating scales. In this context, each graphic rating scale is called a Likert item. Likert scales are usually expressed in terms of agreement and disagreement. An example of a Likert scale is shown in Figure 4. The items that make a Lik- ert scales are designed to collectively capture the phenomenon under analysis. Accordingly, they shouldn't be considered in isolation and they should be summed or averaged to produce a total score. However, individual items by themselves are often considered as a single scale. Because of this ambivalent use of the Likert scale and its items, the nature of the Likert scale is highly controversial. Researchers are split between who consider it an interval scale and those who consider it an ordinal scale; see for example Jamieson (2004), Pell (2005), Norman (2010).\n\nThe confusion generated by the ambivalent use of the Likert scale and its items is well illustrated and explained in Joshi et al. (2015), where an image similar to Figure 5 is introduced. Likert scales are built in such a way that respondents express their level of agreement or disagreement with the sentences expressed by the Likert items. Because all the items are presented all together and with the same point labels, it is assumed that each respondent gives the same interpretation to the answer points -that is, as suggested by Likert, the distances between the points in the scale can be considered equal. 4 This assumption licenses use of the scale as an interval scale. Consequently, adding or averaging the items annotated by the same respondent is justified. This raises the interval interpretation, depicted by the vertical arrow of Figure 5.\n\nOtherwise, an item-by-item analysis -that is a separate analysis of a single item extracted from an aggregate scale -cannot justify the assumption that the difference between adjacent points is equal. Indeed, we cannot assume that different respondents perceive the difference between adjacent label points as being of equal distance. The difference between \"agree\" and \"strongly agree\" can be perceived differently from one respondent to another. Consequently, the addition or the average of the items extracted from an aggregate scale is not justified. In such cases, the median or mode can be used as the measure of central tendency. This follows the ordinal interpretation, depicted by the horizontal arrow of Figure 5.\n\nUnfortunately, in many cases there is not a clear understanding of the difference between the horizontal and the vertical direction of the aggregate scale. It is common to see item-by-item analysis (that is the horizontal direction) that makes use of parametric statistics without a justification of this choice. Indeed, as shown in Section 4, the interpretation of Likert items as interval scales has become a common practice. This particularly applies to the use of the mean for measuring the central tendency for the analysis of Likert items. 4 Some authors, for example Jamieson (2004), do not accept such an assumption and do not consider the points as equally distant. In this case the Likert scales themselves, and not only the Likert items, are considered ordinal.", "filtered_refids": [[], [], [], [], [], [], ["b14", "b5"], ["b7"], [], ["b5"], [], [], [], [], [], [], ["b14", "b5"], ["b7"], [], ["b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 92, "num_chars": 10284, "num_references": 8}
{"corpusid_sectionid": "219946512-s2", "title": "Automatic summarization of medical conversations, a review", "date": 2019, "section_title": "Summarization criteria", "section": "In the following lines we will describe two important criteria to summarize text. The first criteria is based on the frequency of the words. The second criteria are focused on the text features to detect the importance of the sentences.\n\nFrequency criteria Over the years, several criteria have been developed to generate extractive summaries. One of the most cited in the literature is TF-IDF. TF (Term-Frequency) was proposed by Luhn (1958) and is the frequency of a word in the document. IDF (Inverse Document Frequency) was proposed by Sparck Jones (1972) and attenuates the weight of words that appear in a lot of the documents of the collection and increases the weight of words that occur in a few of them. The first works in summarization were based on T F \u2212 IDF . For instance, Wang & Cardie (2011) used unsupervised methods like TF-IDF, LDA, topic modeling and supervised clustering to produce a concise abstract of the decisions taken in spoken meetings.\n\nSurface features criteria An alternative to detect the relevance of a sentence is through features of different kind. Yogan et al. (2016)  Machine Learning Approaches In recent years, researchers have proposed methods based-on machine learning to summarize text. Naive Bayes was used by Kupiec et al. (1995) to chose if a sentence belongs or not to a summary. Recently Ramanujam & Kaliappan (2016) extended the application of the Naive Bayes algorithm to automatic summarization in multi-documents. Aside from Nayve Bayes, clustering algorithms have been used by Aliguliyev (2009) and KM & Soumya (2015) to get extractive summaries. Aliguliyev (2009) proposed a method based on sentence clustering, while KM & Soumya (2015) prepared the cluster center using a n-dimensional vector space and the documents similarity is measure by cosine similarity to generate the documents clusters.\n\nBesides, researchers have used Support Vector Machine (SVM). For example, (Schilder & Kondadadi, 2008) work on query-based multi-document summarization and they use SVM to rank all sentences in the topic cluster for summarization. Another algorithm that researchers have been used is genetic algorithms. For instance, Chatterjee et al. (2012) worked on extractive summaries representing the document as a Direct Acyclic grapth (DAC). They used genetic algorithm to maximize the fitness function and get the summary. After few years, Bossard & Rodrigues (2017) used objective function from generic algorithms to explore summaries space and compute the probability distribution of tokens.\n\nOne of the most widely used family of machine learning methods for automatic summarization is Neural Networks (NN). In NLP, the currently most relevant recurrent neural networks are : LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit). LSTM has the ability to add or remove information through structures called gates : forget gate, input gate, candidate gate and output gate layer. Meanwhile, GRU is a variant of LSTM and it combines the forget and input gates into a single update gate. LSTM and GRU present some advantages, such as the ability to store long-term dependencies, they avoid the problem of vanishing gradient and they consider the order of the words.\n\nIn abstractive text summarization, taking into account the order of words is one of the greatest advances because the summaries present more coherence.\n\nKaikhah (2004)  Automatic summarization methods have been applied on various kinds of documents, such as text (news, articles, etc), dialogues, in medical and other domains. In the literature, we can find automatic summarization of dialogues in meetings, recorded conversations in call centers and other events that happen everyday. The majority of works on medical summarization have focused on research articles. However in our case we are interested to get automatic summaries from medical conversations. In sections 3 and 4, we describe independently NLP works on dialogue analysis and the medical domain, with a focus on summarization. ", "filtered_refids": [[], ["b55", "b35"], ["b64", "b25", "b27"], ["b51"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 4012, "num_references": 6}
{"corpusid_sectionid": "219946512-s5", "title": "Automatic summarization of medical conversations, a review", "date": 2019, "section_title": "Medical Conversational Systems", "section": "There are many conversational applications that have been developed to help the health field, such as intelligence agents and health dialog systems. Allen et al. (2006) developed Chester, a prototype of medical advisor. Chester provides information, advises based on prescribed medications and reports back to medical support team, it can answer questions asked by users. In the meantime, de Rosis et al.\n\n(2006) developed a conversational agent to talk to patients to influence them to change their dietary behavior. Migneault et al. (2006) describe the approach of how to write dialogues for TLC (Telephone-Linked Care) telephone systems, whose objective is to offer health-related services. At the same time, (Beveridge & Fox, 2006) studied the automatic generation of dialogue combining knowledge of the structure of tasks and ontological knowledge, the objective is to decide if a patient must be referred or not with a cancer specialist.", "filtered_refids": [["b1"], ["b4", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 943, "num_references": 3}
{"corpusid_sectionid": "51623319-s1", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "The Phenomenon of Non-NA Anaphora", "section": "Example (1) demonstrated a simple case of anaphora, in which the antecedent Maya is a simple noun phrase. When the antecedent is a non-nominal constituent, as in Example (2), we refer to the relation between an anaphor and its non-NA as non-NA anaphora. Semantically, such non-NAs typically denote complex entities, such as propositions, facts, events, or situations. These entities are complex because they can contain a number of other entities and events, as well as the relationships between them.\n\nThere are different terminologies used for the antecedent (i.e., the surface linguistic constituent) and its interpretation. For instance, Byron (2004) uses Luperfoy's (1991) terminology, referring to the antecedent as the sponsor, and Byron (2003) uses the term linguistic anchor to indicate that these are not ordinary nominal antecedents. The actual meaning of an antecedent is referred to as its referent or interpretation. In this article, we will use the term antecedent to refer to the expression that most closely represents the interpretation of the anaphor, so far as it is overtly realized, and referent to refer to the interpretation itself.\n\nA few simple examples of non-NA anaphora are shown in Example (4). Here, the anaphors are: this, this fact, and it; the antecedents are: the sentence Women are a rarity in mathematics and engineering in Example (4a), the sentence They will not win many races in Example (4b), and the verb phrase made her butternut squash recipe in Example (4c); and the referents are: the proposition that women are a rarity in mathematics and engineering in (4a), the fact that those who run with large lateral motion will not win many races in (4b), and the action of making the butternut squash recipe in (4c).\n\n(4) a. Women are a rarity in mathematics and engineering. As a female engineering student, I see this every day. (NYT 4 ) b. Those who run with large lateral motion are not running well; they may be good runners who are very tired, or simply poor runners. They will not win many races, but it is far too simplistic to attribute this fact to the \"extra distance\" that must be covered. (NYT) c. Anna finally made her butternut squash recipe this morning. It took her twenty minutes.\n\nThese examples are relatively simple examples, where the antecedents precede the anaphor, are given explicitly in the text, and are in the close vicinity of the anaphor. None of these circumstances are necessarily the case. In Example (5a), the antecedent follows the anaphor this, creating an instance of cataphora. In Example (5b), there is no clear syntactic constituent representing the antecedent of the anaphor this reason-instead, the antecedent is distributed throughout the preceding text. And in Example (5c), the syntactic constituent representing the antecedent is three sentences away from the anaphor and the actual issue here, that is, the referent is whether to allow some form of audiovisual coverage of court proceedings or not, which does not occur explicitly in the text.\n\n(5) a. This, I now realize, was a very bad idea-suggesting we do whatever Terry Crews wants for the day. 5 b. Because all of us carry some baggage from our past, I seldom arrive in Paris,\n\nwhere work takes me four or five times a year, without some feeling of being an ugly duckling or, at any rate, a small-town person. No doubt it is for this reason-I can think of no other-that I stay in the same hotel, in the same room, and consider the area around the Place Vend\u00f4me my neighborhood. (NYT) c. New York is one of only three states that do not allow some form of audio-visual coverage of court proceedings. Some lawmakers worry that cameras might compromise the rights of the litigants. But a 10-year experiment with courtroom cameras showed that televised access enhanced public understanding of the judicial system without harming the legal process. New York's backwardness on this issue hurts public confidence in the judiciary . . . (NYT) Linguistic accounts of anaphora usually assume that in processing text or utterances, speakers and hearers build a discourse model (Kamp 1979;Webber 1979), a mental model of the current discourse state, which is dynamically updated as new utterances are processed. A discourse model contains representations of entities that have been referred to in the discourse up to now, attributes of these entities, and the relationships between them. The entities are called discourse entities or discourse referents (Karttunen 1976). To determine the referent of a nominal anaphoric expression, a suitable antecedent with the appropriate features, for example, matching gender and number, has to be found, whose discourse referent then serves as the referent of the anaphor. With non-NA anaphoric expressions, on the other hand, there is not necessarily a discourse referent available in the discourse model that the anaphor could refer to. Interpreting non-NA anaphora is therefore often said to involve additional steps of interpretation (Webber 1991) (cf. Section 3.2.1). 6", "filtered_refids": [[], ["b14", "b13", "b71"], [], [null], [], [], ["b52", "b124", "b122", null, "b53"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 32, "num_chars": 5045, "num_references": 9}
{"corpusid_sectionid": "51623319-s2", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "Other Terminologies Describing Similar Phenomena", "section": "The phenomenon we discuss in this article has been a subject of interest for linguists, philosophers, and computational linguists for decades. Consequently, it has been addressed in various contexts from a variety of perspectives, as discussed in the following sections.\n\n2.1.1 Abstract Anaphora. Asher (1993), Navarretta (2007), and Dipper et al. (2011) use the terms abstract anaphora or abstract object anaphora, as in this phenomenon the anaphor refers to an abstract object, such as a fact, an event, a proposition, or a situation, in Ashers's typology of saturated abstract objects (Asher 1993, page 57). contrast to a concrete object, such as a person or a location. Asher formalized the notion of an abstract object by extending Vendler's (1967) approach of using linguistic tests to differentiate various types of abstract objects. The resulting typology ( Figure 1) makes a broad distinction between eventualities (i.e., events and states, which have spatial, temporal, and causal properties and can be observed by the senses) and purely abstract objects (i.e., facts and propositions, which do not have a spatiotemporal location and are not perceivable by the senses but are only mentally conceivable; e.g., Asher 1993, page 57). According to Asher (1993, page 86), eventualities are similar to concrete objects in that they can be directly introduced into the discourse model by some syntactic construction. Whereas concrete objects are introduced by noun phrases (or, more precisely, by their determiners), eventualities are introduced by finite clauses (or, more precisely, by their inflectional marking). In contrast, facts or propositions are introduced by the semantic constraints imposed by specific nouns, such as fact, or verbs, such as believe, which require their arguments (e.g., a that clause) to be of a certain type (Asher 1993, pages 116 and 175). Asher collectively calls the events, states, processes, propositions, facts, and similar entities that populate these two categories saturated abstract objects. They are \"saturated\" in the Fregean sense that they are themselves either true or false, whereas properties or concepts, although abstract, are only true or false as applied to their arguments (Asher 1993, page 15). It is primarily this category of objects-saturated abstract objects-to which non-NA anaphors refer.\n\n2.1.2 Discourse Deixis. Another popular term is discourse deixis (e.g., Webber 1988Webber , 1991Eckert and Strube 2000;Byron 2004;Recasens 2008). 7 Webber (1988Webber ( , 1991, attributing the term to Lakoff (1974), 8 calls non-NA anaphors discourse-deictic because the anaphor deictically points to some part of the discourse model from which it gets its reference. 7 The term deixis refers to the linguistic phenomenon in which an expression's reference is determined in relation to its extra-linguistic context, e.g., the time (now), place (here), or participants (I, you) of the utterance. Such expressions are called deictic (Huddleston andPullum 2002, page 1451). 8 Lakoff (1974) uses the term in a broader sense, including both NA and non-NA anaphora. Webber (1991) states that it makes sense to call the phenomenon discourse deixis because such relations are usually signaled by deictic expressions, that is, demonstratives this and that, compared with it. Cornish (2007) contrasts deixis with anaphora, describing them as the poles of a scale: Whereas anaphora involves the retrieval of an existing discourse entity from the current model, deixis shifts the focus to a new discourse entity or a new aspect of an existing entity.\n\nThe term discourse deixis has also been used in the literature with a different meaning: According to Levinson (1983), discourse deixis occurs when reference is made to the linguistic form of an utterance rather than its referent or when demonstrative expressions refer meta-linguistically to the preceding or following discourse segments (e.g., this section, this chapter). One can argue that the antecedents in such cases (i.e., this chapter and this section) are big chunks of text and therefore non-nominal. However, though these are certainly interesting cases, we do not focus on them in this article.\n\n2.1.3 Impure Textual Deixis. Lyons (1977) distinguishes between three different types of entities: First-order entities are physical objects. Second-order entities are events, states of affairs, and processes (Asher's eventualities), which are located in time and involve first-order entities and interactions between them. Third-order entities are propositions and facts (Asher's purely abstract objects), which have no spatiotemporal location, and involve first-and second-order entities and the interactions between them.\n\nFor anaphoric relations, Lyons introduced the term textual deixis, which describes the deictic relation obtained between a referring expression such as a pronoun and a piece of text. He distinguishes between pure textual deixis, where the referring expression refers to a textual unit as such (similar to Levinson's [1983] notion of discourse deixis), and impure textual deixis, where the expression is related to the third-order entity denoted by a textual unit, such as a fact or a proposition. If the relation involves a second-order entity (e.g., an event), it is not clear whether Lyons considers this relation an instance of impure textual deixis or simply ordinary anaphora.\n\n2.1.4 Situational Reference. Fraurud (1992) uses the term situational reference. She defines situations as entities representing eventualities (e.g., events, processes, and states) and factualities (e.g., facts and propositions). She uses the term antecedent for the clause or sentence that provides the anaphor's referent, but often the anaphor refers to a \"larger situation\"-for example, a whole sequence of events.\n\n2.1.5 Non-nominal Direct and Indirect Anaphora. Gundel, Hedberg, and Zacharski (2004) and Hedberg, Gundel, and Zacharski (2007) use the terms non-nominal direct and indirect anaphora. They operationalize this terminology as follows. An anaphoric relation is direct if the anaphor's referent is the same as the antecedent's referent, and it is indirect if the interpretation of the anaphor depends on that of the antecedent but they are not coreferential because the interpretation involves an additional step. Example (6), from Hedberg, Gundel, and Zacharski (2007), is an example of a direct anaphoric relation because both the anaphor and the antecedent refer to the event of the stock doubling on its first day of trading. In contrast, in Example (7), from Hedberg, Gundel, and Zacharski (2007), the clausal antecedent introduces the state of Giuliani being sleepy and the marked anaphor refers to the fact that he was sleepy, so the anaphor is not coreferential with the antecedent here, and it would be classified as an instance of indirect anaphora.\n\n(6) The winner was Internet Capital Group, a company that invests in other Internet companies. It more than doubled its first day of trading, Aug. 5., and that was just the beginning.", "filtered_refids": [[], [null, "b22", "b5", "b117"], ["b14", "b107", "b124", "b25", "b20", "b66", "b123", null, "b48"], ["b70"], ["b72"], ["b70"], ["b32"], ["b44", "b40"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 7069, "num_references": 19}
{"corpusid_sectionid": "51623319-s3", "title": "Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "date": "2018-09-01", "section_title": "554", "section": "(7) Mayor Rudolph Giuliani, who gave himself the job of ubiquitous master of ceremonies of the city's New Year celebration, said he began his last day of the 1900s at 5:30 a.m. having trouble getting his lights on. \"I was convinced that it was Y2K,\" the mayor said, but \"actually I was sleepy.\" This perhaps explains an interesting mishap. . . Botley (2006) provides the following characteristic properties for indirect anaphora: (a) The antecedent is not nominal and is difficult to define directly, (b) the link between anaphor and antecedent is not one of coreference, and (c) the hearer may have to carry out a complex process of inference to arrive at the antecedent. Botley considers three main types of indirect anaphora: textual deixis 9 (Lyons 1977), situational reference (Fraurud 1992), and labeling (Francis 1986). We discussed textual deixis and situational reference in the previous subsections, and we will discuss labeling (i.e., shell nouns) in Section 3.1.\n\n2.1.6 Complex Anaphora. Consten, Knees, and Schwarz-Friesel (2007) coin the term complex anaphora, where anaphors are nominal expressions referring to propositionally structured referents, such as propositions, states, facts, and events. They define two criteria for complex anaphora: First, the antecedent has to be a syntactically complex entity-it must consist of at least one clause; and second, the antecedent must denote a conceptually complex item. 10 Consten, Knees, and Schwarz-Friesel define a conceptually complex item as a second-or third-order entity, according to Lyons' (1977) hierarchy (see Section 2.1.3).\n\n2.1.7 Extended Reference and Text Reference. Halliday and Hasan (1976, pages 52-53, 66-70) distinguish between two kinds of references of demonstrative pronouns and the pronoun it: extended reference and text reference. 11 An example from Halliday and Hasan (1976, page 52) is given in Example (8). 12 The first instance of it in the example refers to curtseying while you're thinking what to say, which they call extended reference, as the reference is no longer to a person or object but to a whole process or complex phenomenon, and the referent is expressed by a clause or string of clauses instead of a simple noun phrase. In contrast, the second instance of it is a case of text reference because it requires its referent to be transmuted into the fact that curtseying while you're thinking what to say saves time. Alice wondered a little at this, but she was too much in awe of the Queen to disbelieve it. 9 Botley calls this type text/discourse deixis. Discourse deixis is here understood in the sense of Levinson (1983), which is very close to Lyons' pure textual deixis. 10 Both conditions are necessary to distinguish non-NA anaphora from bridging relations (see footnote 6):\n\nExample (ia) is a case of a non-NA anaphor with this incident referring to the biting event reported in the previous sentence. Example (ib) is a case of a bridging relation: The expression the scars does not refer to an event but to a concrete entity, which is inferred from an entity involved in the biting event. ( 555 Table 1 Overview of terminology used for non-NA anaphora.", "filtered_refids": [["b30", "b72", "b10", "b32"], ["b72", "b18"], ["b70", null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3166, "num_references": 9}
{"corpusid_sectionid": "15084403-s2", "title": "A Survey of Text Mining Architectures and the UIMA Standard", "date": "2012-05-01", "section_title": "Parallelization and distribution:", "section": "With the increasing amount of unstructured data, it becomes more and more important to provide an architecture that enables parallelization and distribution. It is analyzed, if the architectures provide methods for parallelization and distribution and in which way they support processing units in using these possibilities. 6. Annotation model: A typical Information Retrieval toolchain enriches unstructured data with relevant informations like part of speech tags. It is analyzed in which way the architectures model and store the information, and if efficient and convenient access structures exist. Typical questions are if annotations are stored inline or as stand-off markup, if they are typed to provide formal verification and declaration and if annotation types can be inherited. Furthermore the architecture should allow annotations as fields of other annotations (e.g. for parse trees) as well as indexing mechanisms, iterators and subiterators for given types.\n\n2.1. TIPSTER TIPSTER describes a common architecture developed to provide means for efficient information retrieval and information extraction to government agencies and general research ( (Grishman, 1996), chapter 1.0). TIPSTER was 5 corpus and document based processing iteratively applied not only designed for multilingual applications in a wide range of software and hardware environments, but also introduced the thought of interchangeable modules from different sources. While being defined in an abstract (yet object-oriented) way, the TIPSTER architecture is implemented in a number of programming languages like C, Tcl and Common Lisp. The TIPSTER architecture can be seen as document centric. Each document may be contained in one or more collections and is the atomic unit of information retrieval which is considered as the repository of any extracted data. It is possible to derive documents from other documents, thereby forming logical documents. Any information about the text is given by stand-off annotations. Each annotation can be defined by the system engineer using arbitrary annotation names with arbitrary attributes. Each annotation name has to be defined in a type declaration, which is merely used for documentation, but intended to serve as a base for formal verifications. It is possible to assign each annotation to one or more spans of text in the document. Attributes allow primitive data types as values as well as references to other annotations or documents, thereby allowing even hierarchical structures such as parse trees. Some annotation types and general annotation attributes are predefined according to the Corpus Encoding Standard (CES, (Ide, 1998)) to facilitate the interchangeability of modules and the usage of the architecture.\n\nAnnotations are managed and indexed to ensure efficient access for different use case scenarios. TIPSTER's strength is the sophisticated typed annotation model -adopted by as well UIMA (Ferrucci and Lally, 2004), GATE (Cunningham et al., 2002) and Ellogon (Petasis et al., 2002) -and the integration of existing standards like CES. The main shortcoming is the sparse specification of processing resources. Besides being able to work with the Tipster document model no further characteristics are defined. There is no parameter or resource management by the architecture, and no sophisticated workflow management. This makes interchangeability, a standardized parallelization and distribution of processing and language resources impossible.", "filtered_refids": [[], ["b11", "b12"], [null, "b10", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 3493, "num_references": 5}
{"corpusid_sectionid": "15084403-s4", "title": "A Survey of Text Mining Architectures and the UIMA Standard", "date": "2012-05-01", "section_title": "GATE", "section": "The General Architecture for Text Engineering (GATE, (Cunningham, 2000)) was developed to provide an infrastructure for a wide range of language engineering activities that also considers the prior infrastructural findings of the scientific community. It was originally released 1996 and is today available in version 5. The current version is implemented completely in Java, uses Unicode as default encoding and is also capable of processing audio-visual content. Besides comfortable GUI editing tools, it comprises two central elements (cp. (Cunningham, 2000), chapter 7):\n\n1. The GATE Document Manager (GDM) is implemented according to the TIPSTER specifications about document management. Therefore the core of the manager is given by a collection of documents containing text and annotations, which -similar to Ellogon -can be modified online. With the GDM being the common interface to all processing resources it is also the central data repository. All processing resources obtain the annotated documents from the GDM and return them for later processing steps. Annotations on documents are organized in so-called annotation graphs (Bird et al., 2000). Except the information about start and end node, every annotation defines an identifier, a type declaration and additional attributes. Annotation schemes similar to TIPSTER define common annotations with their attributes (cp. (Bontcheva et al., 2004)). Although one annotation is determined to refer to only one span of text, the architecture offers the possibility to create multiple annotation graphs per document.", "filtered_refids": [["b9"], ["b2", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1577, "num_references": 3}
{"corpusid_sectionid": "15084403-s5", "title": "A Survey of Text Mining Architectures and the UIMA Standard", "date": "2012-05-01", "section_title": "A Collection of REusable", "section": "Objects for Language Engineering (CREOLE) which can be seen as a library of processing resources, language resources and data structures for general usage (cp. (Cunningham, 2000), chapter 7.2). Users can extend the CREOLE objects by own implementations using the CREOLE API and initialize and run them on documents using the GGI or programmatic access. All necessary information for the processing resource (PR) is provided by the GDM in the form of documents with text and annotations and results are written back respectively.\n\nEvery CREOLE component must specify its configuration to facilitate workflow creation, accessibility by the Gate Graphical Interface (GGI) and interchangeability. This metadata comprises parameters as well as pre-and postconditions (in the form of annotations and attributes). It is expressed in XML or by using Java Annotations (Cunningham et al., 2010), which simplifies inheritance of processing resources significantly.\n\nBesides the infrastructural capabilities GATE offers an exhaustive library of GUI tools, data access structures, language resources and import filters for common document formats. The workflow management offers possibilities for conditional processing and collection level processing. Although language resources may be distributed and applications may run on different machines, there is still no sophisticated workflow management allowing iterative, nested or parallel processing (cp. (Bontcheva et al., 2004), (Cunningham et al., 2010)). An impressive feature is the possibility for finite state processing over annotations based on regular expressions. This Java Annotation Pattern Engine (JAPE) operates on given pattern/action rules which define a pattern of annotations and their features in the input document, and a corresponding action to perform if the pattern is matched. Corresponding actions may also include the creation of new annotations or the modification of the matched ones. GATE can be seen as quite exhaustive. Resources are separated and described using metadata that can be composed in workflows. Inheritance of modules is facilitated using Java Annotations, collection level processing is possible and the document model with typed annotations is as well comprehensive as well defined. Shortcomings of GATE are the lack of a sophisticated workflow management (especially with respect to parallelization) and that formal declarations of resources are not analysis aware -neither pre-or postconditions nor parameters can be defined with respect to annotations and attributes. Although many different kinds of resources can be accessed via predefined structures, there is no formal specification for individual resource management. Type inheritance is not possible.", "filtered_refids": [["b9"], ["b8"], ["b8", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2743, "num_references": 4}
{"corpusid_sectionid": "5853439-s0", "title": "Ontologies of Linguistic Annotation: Survey and Perspectives", "date": "2012-05-01", "section_title": "Background", "section": "The heterogeneity of linguistic annotations has been recognized as a key problem limiting the interoperability and reusability of NLP tools and linguistic data collections. In Natural Language Processing, standard architectures such as UiMA (Egner et al., 2007) and GATE (Cunningham, 2002) address the interoperability of linguistic data structures by providing wrappers around existing NLP components that make use of formalisms that represent input and output of these modules in a tool-independent way. While this approach indeed yields interoperable data structures, and thereby establishes structural interoperability, it is limited insofar as the annotations itself, their content and their meaning, are not standardized in the same way. This problem, the establishment of conceptual interoperability between different linguistic annotations, is addressed here. This problem has long been recognized and numerous initiatives have addressed the problem to represent linguistic annotations in an interoperable way. By now, it is generally agreed upon that repositories of linguistic annotation terminology represent a key element in the establishment of conceptual interoperability. With a terminological reference repository, it is possible to abstract from the heterogeneity of annotation schemes: Reference definitions provide an interlingua that allows to map linguistic annotations from annotation scheme A to annotations in accordance with scheme B. Several repositories of linguistic annotation terminology have been developed by the NLP/computational linguistics community (Aguado de Cea et al., 2004) as well as in the field of language documentation/typology (Saulwick et al., 2005), and their continuous application is expected to enhance the consistency of linguistic metadata and annotations. The General Ontology of Linguistic Description (Farrar and Langendoen, 2010, GOLD) and the ISO TC37/SC4 Data Category Registry (Kemps-Snijders et al., 2009, ISOcat) address both communities. At the moment, however, two problems for the practical application of any of these terminology repositories persist:\n\n\u2022 Different communities develop and maintain independent terminology repositories (e.g., GOLD and ISOcat), and these repositories are not always compatible with respect to the definitions they provide, with re-spect to the technologies employed, or with respect to the underlying philosophy. These problems are actively addressed by the GOLD and ISOcat communities, e.g., in the context of the RELISH project (Kemps-Snijders, 2010). The possible integration between GOLD and ISOcat is, however, expected to by a longer process.\n\n\u2022 There is no commonly agreed formalism to link linguistic annotations to terminology repositories. For GOLD, concrete annotations are linked to reference concepts by means of hand-crafted mapping scripts (Simons et al., 2004). For ISOcat, RDF has been suggested as a means of addressing data categories only recently (Windhouwer and Wright, 2012).\n\nThe Ontologies of Linguistic Annotation (OLiA) have been developed to address both problems in order to facilitate the development of applications that take benefit of a welldefined terminological backbone even before the GOLD and ISOcat repositories have converged into a generally accepted reference terminology. The OLiA ontologies introduce an intermediate level of representation between ISOcat, GOLD and other repositories of linguistic reference terminology and are interconnected with these resources, and they provide not only means to formalize reference categories, but also annotation schemes, and the way that these are linked with reference categories.", "filtered_refids": [["b40", "b18", null, "b0", "b17"], [null, "b31"], ["b45", "b52"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 3664, "num_references": 9}
{"corpusid_sectionid": "5853439-s2", "title": "Ontologies of Linguistic Annotation: Survey and Perspectives", "date": "2012-05-01", "section_title": "Linking Annotations with Reference Categories", "section": "The classic approach to link annotations with reference concepts is to specify rules that define a direct mapping (Teufel, 1995). It is, however, not always possible to find a 1:1 mapping. One problem is conceptual overlap: A common noun may occur as a part of a proper name, e.g., German Palais 'baroque-style palace' in Neues Palais lit. 'new palace', a Prussian royal palace in Potsdam/Germany. Palais is thus both a proper noun (in its function), and a common noun (in its form). Such conceptual overlap is sometimes represented with a specialized tag, e.g., in the TIGER scheme (Brants et al., 2004). ISOcat does currently not provide the corresponding hybrid category, so that Palais is to be linked to both properNoun/DC-1371 and commonNoun/DC-1256 if the information carried by the original annotation is to be preserved. Cliticization and fusion are similar in that multiple word classes can be assigned to (different parts of) one token as represented, for example, by the English gonna, that can be annotated in the PennTreebank tagset as both VBG (gerund, going) and TO (to) (Santorini, 1990). A somewhat different problem is the representation of ambiguity: The SUSANNE (Sampson, 1995) tag ICSt applies to English after both as a preposition and as a subordinating conjunction. The corresponding ISOcat category is thus either preposition/DC-1366 or subordinatingConjunction/DC-1393. Without additional disambiguation, ICSt is to be linked to both data categories.\n\nTechnically, such problems can be solved with a 1:n mapping between annotations and reference concepts. Yet, overlap/contraction and ambiguity differ in their underlying meaning: While overlapping/contracted categories are in the intersection ( ) of reference categories, ambiguous categories are are in their join ( ). This difference is relevant for subsequent processing, e.g., to decide whether disambiguation is necessary. A standard mapping approach, however, fails to distinguish or . Being based on a decidable fragment of first-order predicate logic, OWL/DL represents a formalism that supports the necessary operators and flexibility: With reference concepts and annotation concepts are formalized as OWL classes, the linking between them can be represented by rdfs:subClassOf ( ). OWL/DL provides operators such as owl:intersectionOf ( ), owl:unionOf ( ) and owl:complementOf (\u00ac), and it allows to define properties and restrictions on the respective concepts. An OWL/DL-based formalization has the additional advantage that it can employ existing terminological repositories, e.g., GOLD (native OWL/DL) and ISOcat (with an OWL/DL conversion as described by (Chiarcos, 2010a)). GOLD and ISOcat are, however, under development. The efforts to maintain the linking between annotations and the terminological repository can be reduced if another ontology is introduced that mediates between terminological repositories and annotation schemes: If a major revision of the repository occurs, only the linking between the intermediate ontology and the repository is to be revised, but the linking with not every single tagset. Moreover, this intermediate ontology allows linking annotations to multiple terminological repositories at the same time. The OLiA ontologies implement the idea of an architecture of modular OWL/DL ontologies with an ontology mediating between terminological repositories and annotation schemes.", "filtered_refids": [["b1", "b38", "b39", "b50"], ["b12"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 3405, "num_references": 5}
{"corpusid_sectionid": "5853439-s3", "title": "Ontologies of Linguistic Annotation: Survey and Perspectives", "date": "2012-05-01", "section_title": "A Modular Architecture of OWL/DL Ontologies", "section": "The Ontologies of Linguistic Annotations (Chiarcos, 2008) represent a modular architecture of OWL/DL ontologies that formalize several intermediate steps of the mapping between annotations, a 'Reference Model' and existing terminology repositories ('External Reference Models').\n\nThe OLiA ontologies were developed as part of an infrastructure for the sustainable maintenance of linguis-tic resources (Schmidt et al., 2006), and their primary fields of application include the formalization of annotation schemes and concept-based querying over heterogeneously annotated corpora (Rehm et al., 2007;Chiarcos et al., 2008). In the OLiA architecture, four different types of ontologies are distinguished:\n\n\u2022 The OLIA REFERENCE MODEL specifies the common terminology that different annotation schemes can refer to. It is derived from existing repositories of annotation terminology and extended in accordance with the annotation schemes that it was applied to.\n\n\u2022 Multiple OLIA ANNOTATION MODELs formalize annotation schemes and tagsets. Annotation Models are based on the original documentation, so that they provide an interpretation-independent representation of the annotation scheme.\n\n\u2022 For every Annotation Model, a LINKING MODEL defines relationships between concepts/properties in the respective Annotation Model and the Reference Model. Linking Models are interpretations of Annotation Model concepts and properties in terms of the Reference Model.\n\n\u2022 Existing terminology repositories can be integrated as EXTERNAL REFERENCE MODELs, if they are represented in OWL/DL. Then, Linking Models specify relationships between Reference Model concepts and External Reference Model concepts.\n\nThe OLiA Reference Model specifies classes for linguistic categories (e.g., olia:Determiner) and grammatical features (e.g., olia:Accusative), as well as properties that define relations between these (e.g., olia:hasCase). Far from being yet another annotation terminology ontology, the OLiA Reference Model does not introduce its own view on the linguistic world, but rather, it is a derivative of EAGLES (Leech and Wilson, 1996), MULTEXT/East (Erjavec, 2004), and GOLD (Farrar and Langendoen, 2010) that was introduced as a technical means to interpret linguistic annotations with respect to these terminological repositories, and further enriched with information drawn from the annotation schemes it was applied to. Conceptually, Annotation Models differ from the Reference Model in that they include not only concepts and properties, but also individuals: Individuals represent concrete tags, while classes represent abstract concepts similar to those of the Reference Model.", "filtered_refids": [["b11"], ["b43", "b11", "b36"], [], [], [], [], [null, "b20", "b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2670, "num_references": 7}
{"corpusid_sectionid": "215238860-s3", "title": "More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction", "date": "2020-04-07", "section_title": "Statistical Relation Extraction Models", "section": "As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied.\n\nOne typical SRE approach is feature-based methods (Kambhatla, 2004;Zhou et al., 2005;Jiang and Zhai, 2007;Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers.\n\nDue to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004;Bunescu and Mooney, 2005;Zhao and Grishman, 2005;Mooney and Bunescu, 2006;Zhang et al., 2006b,a;Wang, 2008).\n\nThere are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods Yih, 2002, 2004;Sarawagi and Cohen, 2005;Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations.\n\nInspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings Riedel et al., 2013;Gormley et al., 2015). Furthermore, , Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE.\n\nAlthough SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003;Bach and Badaskar, 2007;Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models.", "filtered_refids": [[], ["b132", "b38", "b40", "b1"], ["b60", "b130", "b13", null, "b7", "b99"], [null, "b116", "b79"], ["b46", "b25", "b101", null, "b75"], ["b69", "b1", "b117"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2082, "num_references": 21}
{"corpusid_sectionid": "215238860-s4", "title": "More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction", "date": "2020-04-07", "section_title": "Neural Relation Extraction Models", "section": "Neural relation extraction (NRE) models introduce neural networks to automatically extract semantic features from text. Compared with SRE models, NRE methods can effectively capture textual information and generalize to wider range of data.\n\nStudies in NRE mainly focus on designing and utilizing various network architectures to capture the relational semantics within text, such as recursive neural networks (Socher et al., 2012;Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; (Zhang and Wang, 2015;Nguyen and Grishman, 2015a;Vu et al., 2016; that can better handle long sequential data, graph neural networks (GNNs) Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016;Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010;Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there are also other works integrating syntactic information into NRE models. Xu et al. (2015a) and Xu et al. (2015b) adopt CNNs and RNNs over shortest dependency paths respectively.  propose a recursive neural network based on augmented dependency paths.  and Cai et al. (2016) utilize deep RNNs to make further use of dependency paths. Besides, there are some efforts combining NRE with universal schemas Riedel et al., 2013). Recently, Transformers (Vaswani et al., 2017) and pre-trained language models (Devlin et al., 2019) have also been explored for NRE (Du et al., 2018;Verga et al., 2018; By concisely reviewing the above techniques, we are able to track the development of RE from pattern and statistical methods to neural models. Comparing the performance of state-of-the-art RE models in years (Figure 2), we can see the vast increase since the emergence of NRE, which demonstrates the power of neural methods.", "filtered_refids": [[], ["b107", "b59", "b83", "b17", "b94", "b97", "b91", "b109", "b89", "b134", "b55", "b119", "b75", "b8", "b15", "b111", "b133", "b122", "b48", "b64"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2371, "num_references": 20}
{"corpusid_sectionid": "215238860-s6", "title": "More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction", "date": "2020-04-07", "section_title": "Utilizing More Data", "section": "Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009;Nguyen and Moschitti, 2011;Min et al., 2013). As shown in Figure 3, for any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme.  Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label \"Bill Gates retired from Microsoft\" with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs.\n\nThe existing methods to alleviate the noise problem can be divided into three major approaches:\n\n(1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017;Han et al., 2018b;Zhang et al., 2019a;Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity Wang et al., 2018).\n\n(3) Many methods tend to utilize sophisticated mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks.  incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018;Zeng et al., 2018) and adversarial training (Wu et al., 2017;Wang et al., 2018;Han et al., 2018a) have also been adopted in DS.\n\nThe researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring:\n\n(1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schemes are still similar to the original one in (Mintz et al., 2009), which just covers the case that the entity pairs are mentioned in the same sentences. To achieve better coverage and less noise, exploring better DS schemes for autolabeling data is also valuable.\n\n(2) Inspired by recent work in adopting pretrained language models Wu and He, 2019;Baldini Soares et al., 2019) and active learning (Zheng et al., 2019) for RE, to perform unsupervised or semi-supervised learning for utilizing large-scale unlabeled data as well as using knowledge from KGs and introducing human experts in the loop is also promising.\n\nBesides addressing existing approaches and future directions, we also propose a new DS dataset to advance this field, which will be released once the paper is published. The most used benchmark for DS, NYT-10 (Riedel et al., 2010), suffers from small amount of relations, limited relation domains and extreme long-tail relation performance. To alleviate these drawbacks, we utilize Wikipedia and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) to construct Wiki-Distant in the same way as Riedel et al. (2010). As demonstrated in Table 1, Wiki-Distant covers more relations and possesses more instances, with a more reasonable N/A proportion. Comparison results of state-of-the-art models on these two datasets 2 are shown in Table 2, indicating that Wiki-Distant is more challenging and there is a long way to resolve distantly supervised RE.", "filtered_refids": [["b66", "b57", "b56"], [], ["b100", "b29", "b71", "b125", "b37"], ["b100", "b97", "b4", "b121", "b28", "b20", "b106"], [], ["b57"], ["b105", "b131", "b2"], ["b74", "b96"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3910, "num_references": 21}
{"corpusid_sectionid": "56657817-s2", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Methods", "section": "The most common approach for associating neural network components with linguistic properties is to predict such properties from activations of the neural network. Typically, in this approach a neural network model is trained on some task (say, MT) and its weights are frozen. Then, the trained model is used for generating feature representations for another task by running it on a corpus with linguistic annotations and recording the representations (say, hidden state activations). Another classifier is then used for predicting the property of interest (say, part-of-speech [POS] tags). The performance of this classifier is used for evaluating the quality of the generated representations, and by proxy that of the original model. This kind of approach has been used in numerous papers in recent years; see Table SM1 for references. 5 It is referred to by various names, including ''auxiliary prediction tasks'' (Adi et al., 2017b), ''diagnostic classifiers'' (Veldhoen et al., 2016), and ''probing tasks' ' (Conneau et al., 2018).\n\nAs an example of this approach, let us walk through an application to analyzing syntax in neural machine translation (NMT) by Shi et al. (2016b). In this work, two NMT models were trained on standard parallel data-English\u2192 French and English\u2192German. The trained models (specifically, the encoders) were run on an annotated corpus and their hidden states were used for training a logistic regression classifier that predicts different syntactic properties. The authors concluded that the NMT encoders learn significant syntactic information at both word level and sentence level. They also compared representations at different encoding layers and found that ''local features are somehow preserved in the lower layer whereas more global, abstract information tends to be stored in the upper layer.'' These results demonstrate the kind of insights that the classification analysis may lead to, especially when comparing different models or model components.\n\nOther methods for finding correspondences between parts of the neural network and certain properties include counting how often attention weights agree with a linguistic property like anaphora resolution (Voita et al., 2018) or directly computing correlations between neural network activations and some property; for example, correlating RNN state activations with depth in a syntactic tree (Qian et al., 2016a) or with Melfrequency cepstral coefficient (MFCC) acoustic features (Wu and King, 2016). Such correspondence may also be computed indirectly. For instance, Alishahi et al. (2017) defined an ABX discrimination task to evaluate how a neural model of speech (grounded in vision) encoded phonology. Given phoneme representations from different layers in their model, and three phonemes, A, B, and X, they compared whether the model representation for X is closer to A or B. This discrimination task enabled them to draw conclusions about which layers encoder phonology better, observing that lower layers generally encode more phonological information.", "filtered_refids": [["b64", null], ["b51"], ["b73", null, "b65"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3056, "num_references": 6}
{"corpusid_sectionid": "56657817-s3", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Linguistic Phenomena", "section": "Different kinds of linguistic information have been analyzed, ranging from basic properties like sentence length, word position, word presence, or simple word order, to morphological, syntactic, and semantic information. Phonetic/phonemic information, speaker information, and style and accent information have been studied in neural network models for speech, or in joint audio-visual models. See Table SM1 for references.\n\nWhile it is difficult to synthesize a holistic picture from this diverse body of work, it appears that neural networks are able to learn a substantial amount of information on various linguistic phenomena. These models are especially successful at capturing frequent properties, while some rare properties are more difficult to learn. Linzen et al. (2016), for instance, found that long short-term memory (LSTM) language models are able to capture subject-verb agreement in many common cases, while direct supervision is required for solving harder cases.\n\nAnother theme that emerges in several studies is the hierarchical nature of the learned representations. We have already mentioned such findings regarding NMT (Shi et al., 2016b) and a visually grounded speech model (Alishahi et al., 2017). Hierarchical representations of syntax were also reported to emerge in other RNN models (Blevins et al., 2018).\n\nFinally, a couple of papers discovered that models trained with latent trees perform better on natural language inference (NLI) (Williams et al., 2018;Maillard and Clark, 2018) than ones trained with linguistically annotated trees. Moreover, the trees in these models do not resemble syntactic trees corresponding to known linguistic theories, which casts doubts on the importance of syntax-learning in the underlying neural network. 6", "filtered_refids": [[], ["b36"], [null, "b51"], ["b72", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1771, "num_references": 5}
{"corpusid_sectionid": "56657817-s6", "title": "Analysis Methods in Neural Language Processing: A Survey", "date": "2018-12-21", "section_title": "Visualization", "section": "Visualization is a valuable tool for analyzing neural networks in the language domain and beyond. Early work visualized hidden unit activations in RNNs trained on an artificial language modeling task, and observed how they correspond to certain grammatical relations such as agreement (Elman, 1991). Much recent work has focused on visualizing activations on specific examples in modern neural networks for language (Karpathy et al., 2015;K\u00e1d\u00e1r et al., 2017;Qian et al., 2016a;Liu et al., 2018) and speech (Wu and King, 2016;Nagamine et al., 2015;Wang et al., 2017b). Figure 1 shows an example visualization of a neuron that captures position of words in a sentence. The heatmap uses blue and red colors for negative and positive activation values, respectively, enabling the user to quickly grasp the function of this neuron.\n\nThe attention mechanism that originated in work on NMT (Bahdanau et al., 2014) also lends itself to a natural visualization. The alignments obtained via different attention mechanisms have produced visualizations ranging from tasks like NLI (Rockt\u00e4schel et al., 2016;Yin et al., 2016), summarization (Rush et al., 2015), MT post-editing (Jauregi Unanue et al., 2018), and morphological inflection (Aharoni and Goldberg, 2017) to matching users on social media (Tay et al., 2018). Figure 2 reproduces a visualization of attention alignments from the original work by Bahdanau et al. Here grayscale values correspond to the weight of the attention between words in an English source sentence (columns) and its French translation (rows). As Bahdanau et al. explain, this visualization demonstrates that the NMT model learned a soft alignment between source and target words. Some aspects of word order may also be  Godin et al., 2018). Saliency can also be computed with respect to intermediate values, rather than input features (Ghaeini et al., 2018). 7 An instructive visualization technique is to cluster neural network activations and compare them to some linguistic property. Early work clustered RNN activations, showing that they organize in lexical categories (Elman, 1989(Elman, , 1990. Similar techniques have been followed by others. Recent examples include clustering of sentence embeddings in an RNN encoder trained in a multitask learning scenario (Brunner et al., 2017), and phoneme clusters in a joint audio-visual RNN model (Alishahi et al., 2017).\n\nA few online tools for visualizing neural networks have recently become available. LSTMVis (Strobelt et al., 2018b) visualizes RNN activations, focusing on tracing hidden state dynamics. 8 Seq2Seq-Vis (Strobelt et al., 2018a) visualizes different modules in attention-based seq2seq models, with the goal of examining model decisions and testing alternative decisions. Another tool focused on comparing attention alignments was proposed by Rikters (2018). It also provides translation confidence scores based on the distribution of attention weights. NeuroX (Dalvi et al., 2019b) is a tool for finding and analyzing individual neurons, focusing on machine translation.\n\nEvaluation As in much work on interpretability, evaluating visualization quality is difficult and often limited to qualitative examples. A few notable exceptions report human evaluations of visualization quality. Singh et al. (2018) showed human raters hierarchical clusterings of input words generated by two interpretation methods, and asked them to evaluate which method is more accurate, or in which method they trust more. Others reported human evaluations for attention visualization in conversation modeling (Freeman et al., 2018) and medical code prediction tasks (Mullenbach et al., 2018).\n\nThe availability of open-source tools of the sort described above will hopefully encourage users to utilize visualization in their regular research and development cycle. However, it remains to be seen how useful visualizations turn out to be.", "filtered_refids": [["b70", null, "b38", "b73"], ["b15", "b12", null, "b75", "b42", "b61", "b2"], ["b54", "b55"], ["b53", null, "b3"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 3905, "num_references": 16}
{"corpusid_sectionid": "49564714-s1", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Overview of Linguistic Typology", "section": "There is no consensus on the precise number of languages in the world. For example, Glottolog provides the estimate of 7748 (Hammarstr\u00f6m et al. 2016), while Ethnologue (Lewis, Simons, and Fennig 2016) refers to 7097. 1 This is due to the fact that defining what constitutes a 'language' is in part arbitrary. Mutual intelligibility, which is used as the main criterion for including different language variants under the same label, is gradient in nature. Moreover, social and political factors play a role in the definition of language.\n\nLinguistic Typology is the discipline that studies the variation among the world's languages through their systematic comparison (Comrie 1989;Croft 2003). The comparison is challenging because linguistic categories cannot be pre-defined (Haspelmath 2007). Rather, cross-linguistically significant categories emerge inductively from the comparison of known languages, and are progressively refined with the discovery of new languages. Crucially, the comparison needs to be based on functional criteria, rather than formal criteria. Typologists distinguish between constructions, abstract and universal functions, and strategies, the type of expressions adopted by each language to codify a specific construction (Croft et al. 2017). For instance, the passive voice is considered a strategy that emphasizes the semantic role of patient: languages like Qawasqar (isolate) lack this strategy and use others strategies to express the construction.\n\nThe classification of the strategies in each language is grounded in typological documentation (Bickel 2007, p. 248). Documentation is empirical in nature and involves collecting texts or speech excerpts, and assessing the features of a language based on their analysis. The resulting information is stored in large databases (see \u00a7 4.1) of attributevalues (this pair is henceforth referred to as typological feature), where usually each attribute corresponds to a construction and each value to the most widespread strategy in a specific language.\n\nAnalysis of cross-lingual patterns reveals that cross-lingual variation is bounded and far from random (Greenberg 1966b). Indeed, typological features can be interdependent: the presence of one feature may implicate another (in one direction or both). This sort of generalizations (called statistical universals) are tendencies rather than actual rules (Corbett 2010). For example, if a language (such as Hmong Njua, Hmong-Mien family) has prepositions, then genitive-like modifiers follow their head. If, instead, a language (such as Slavey, Na-Den\u00e9 family) has postpositions, the order of heads and genitive-like modifiers is swapped. However, there are known exceptions: Norwegian (Indo-European) has prepositions but genitives precede nouns. 2 Moreover, some typological features are rare while others are highly frequent. Interestingly, this also means that some languages are intuitively more plausible than others. Implications and frequencies of features are important as they unravel the deeper explanatory factors underlying the patterns of cross-linguistic variation (Dryer 1998).\n\nCross-lingual variation can be found at all levels of linguistic structure. The seminal works on Linguistic Typology were concerned with morpho-syntax, mainly morphological systems (Sapir 2014(Sapir [1921, p. 128) and word order (Greenberg 1966b). This level of analysis deals with the form of meaningful elements (morphemes and words) and their combination, hence it is called structural typology. As an example, consider the alignment of the nominal case system (Dixon 1994): some languages like Nenets (Uralic) use the same case for subjects of both transitive and intransitive verbs, and a different one for objects (nominative-accusative alignment). Other languages like Lezgian (Northeast Caucasian) group together intransitive subjects and objects, and treat transitive subjects differently (ergative-absolutive alignment).\n\nOn the other hand, semantic typology studies languages at the semantic and pragmatic levels. This area was pioneered by anthropologists interested in kinship (d 'Andrade 1995) and colors (Berlin and Kay 1969), and was expanded by studies on lexical classes (Dixon 1977). The main focus of semantic typology has been to categorize languages in terms of concepts (Evans 2011) in the lexicon, in particular with respect to the 1) granularity, 2) division (boundary location), and 3) membership criteria (grouping and dissection). For instance, consider the event expressed by to open (something). It lacks a precise equivalent in languages such as Korean, where similar verbs overlap in meaning only in part (Bowerman and Choi 2001). Moreover, the English verb encodes the resulting state of the event, whereas an equivalent verb in another language such as Spanish (abrir) rather expresses the manner of the event (Talmy 1991). Although variation in the categories is pervasive due to their partly arbitrary nature, it is constrained crosslingually via shared cognitive constraints (Majid et al. 2007).\n\nSimilarities between languages do not always arise from language-internal dynamics but also from external factors. In particular, similarities can be inherited from a common ancestor (genealogical bias) or borrowed by contact with a neighbor (areal bias) (Bakker 2010). Owing to genealogical inheritance, there are features that are widespread within a family but extremely rare elsewhere (e.g. the presence of click phonemes in the Khoisan languages). As an example of geographic percolation, most languages in the Balkan area (Albanian, Bulgarian, Macedonian, Romanian, Torlakian) have developed, even without a common ancestor, a definite article that is put after its noun simply because of their close proximity.\n\nResearch in linguistic typology has sought to disentangle such factors and to integrate them into a single framework aimed at answering the question \"what's where why?\" (Nichols 1992). Language can be viewed as a hybrid biological and cultural system. The two components co-evolved in a twin track, developing partly independently and partly via mutual interaction (Durham 1991). The causes of cross-lingual variation can therefore be studied from two complementary perspectives -from the perspective of functional theories or event-based theories (Bickel 2015). The former theories involve cognitive and communicative principles (internal factors) and account for the origin of variation, while the latter ones emphasize the imitation of patterns found in other languages (external factors) and account for the propagation (or extinction) of typological features (Croft 1995(Croft , 2000.\n\nExamples of functional principles include factors associated with language use, such as the frequency or processing complexity of a pattern (Cristofaro and Ramat 1999). Patterns that are easy or widespread get integrated into the grammar (Haspelmath 1999, inter alia). On the other hand, functional principles allow the speakers to draw similar inferences from similar contexts, leading to locally motivated pathways of diachronic change through the process known as grammaticalization (Greenberg 1966a(Greenberg , 1978Bybee 1988). For instance, in the world's languages (including English) the future tense marker almost always originates from verbs expressing direction, duty, will, or attempt because they imply a future situation.\n\nThe diachronic and gradual origin of the changes in language patterns and the statistical nature of the universals explain why languages do not behave monolithically. Each language can adopt several strategies for a given construction and partly inconsistent semantic categories. In other words, typological patterns tend to be gradient. For instance, the semantics of grammatical and lexical categories can be represented on continuous multi-dimensional maps (Croft and Poole 2008). Bybee and McClelland (2005) have noted how this gradience resembles the patterns learned by connectionist networks (and statistical machine learning algorithms in general). In particular, they argue that such architectures are sensitive to both local (contextual) information and general patterns, as well as to their frequency of use, similarly to natural languages.\n\nTypological documentation is limited by the fact that the evidence available for each language is highly unbalanced and many languages are not even recorded in a written form. 3 However, large typological databases such as WALS (Dryer and Haspelmath 2013) nevertheless have an impressive coverage (syntactic features for up to 1519 languages). Where such information can be usefully integrated in machine learning, it can provide an alternative form of guidance to manual construction of resources that are now largely lacking for low resource languages. We will discuss the existing typological databases and the integration of their features into NLP models in sections 4 and 5.", "filtered_refids": [[null, "b98"], [null, "b34", "b44", "b83"], [null], ["b54", "b38", "b76"], ["b76", "b53", "b139"], ["b52", "b105", "b153", null, "b24", "b61", "b17"], ["b10"], ["b43", "b59", "b21", null, "b117"], ["b26", null, "b75", "b42", "b73"], ["b45", "b25"], ["b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 58, "num_chars": 8939, "num_references": 34}
{"corpusid_sectionid": "49564714-s2", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Overview of Multilingual NLP", "section": "The scarcity of data and resources in many languages represents a major challenge for multilingual NLP. Many state-of-the-art methods rely on supervised learning, hence their performance depends on the availability of manually crafted datasets annotated with linguistic information (e.g., treebanks, parallel corpora) and/or lexical databases (e.g., terminology databases, dictionaries). Although similar resources are available for key tasks in a few well-researched languages, the majority of the world's languages lack them almost entirely. This gap cannot be easily bridged: the creation of linguistic resources is a time-consuming process and requires skilled labor. Furthermore, the immense range of possible tasks and languages makes the aim of a complete coverage unrealistic.\n\nOne solution to this problem explored by the research community abandons the use of annotated resources altogether and instead focuses on unsupervised learning. This class of methods infers probabilistic models of the observations given some latent variables. In other words, it unravels the hidden structures within unlabeled text data. Although these methods have been employed extensively for multilingual applications (Snyder and Barzilay 2008;Vuli\u0107, De Smet, and Moens 2011;Titov and Klementiev 2012, inter alia), their performance tends to lag behind the more linguistically informed supervised learning approaches (T\u00e4ckstr\u00f6m, McDonald, and Nivre 2013). Moreover, they have been rarely combined with typological knowledge. For these reasons, we will not review them in this chapter.\n\nOther promising ways to overcome data scarcity include transferring models or data from resource-rich to resource-poor languages ( \u00a7 3.1) or learning joint models from annotated examples in multiple languages ( \u00a7 3.2) in order to leverage language inter-dependencies. Early approaches of this kind have relied on universal, high-level delexicalized features, such as PoS tags and dependency relations. More recently, however, the incompatibility of (language-specific) lexica has been countered by mapping equivalent words into the same multilingual semantic space through representation learning ( \u00a7 3.3). This has enriched language transfer and multilingual joint modelling with lexicalized features. In this chapter, we provide an overview of these methods, as they constitute the backbone of the typology-savvy algorithms surveyed in \u00a7 5.", "filtered_refids": [[], ["b160", null, "b149", "b145"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2418, "num_references": 4}
{"corpusid_sectionid": "49564714-s3", "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing", "date": "2018-07-02", "section_title": "Language Transfer", "section": "Linguistic information can be transferred from resource-rich languages to resource-poor languages: these are commonly referred to as source languages and target languages, respectively. Language transfer is challenging as it requires us to match word sequences with different lexica and word orders, or syntactic trees with different (anisomorphic) structures (Ponti et al. 2018a). As a consequence, the information obtained from the source languages typically needs to be adapted, by tailoring it to the properties of the target languages. The methods developed for language transfer include annotation projection, (de)lexicalized model transfer, and translation (Agi\u0107 et al. 2014). We will illustrate them below using dependency parsing as an example.\n\nAnnotation projection was introduced in the seminal work of Yarowsky, Ngai, and Wicentowski (2001) and Hwa et al. (2005). In its original formulation, as illustrated in Figure 1a, a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g. PoS tags and dependency trees) is then projected directly and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Pad\u00f3 and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014;Agi\u0107 et al. 2016) or sets of most likely labels (Khapra et al. 2011;Wisniewski et al. 2014) can be projected instead of single categorical labels. These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or support 'ambiguous learning' on the target language, respectively.\n\nModel transfer instead involves training a model (e.g. a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure  1b. Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (T\u00e4ckstr\u00f6m, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see \u00a7 3.3).\n\nMachine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1c, a source sentence is machine translated into a target language, (Banea et al. 2008) or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translated documents can also be employed to generate multilingual sentence representations, which facilitate language transfer (Zhou, Wan, and Xiao 2016).\n\nSome of these methods are hampered by their resource requirements. In fact, annotation projection and translation need parallel texts to align words and train translation systems, respectively (Agi\u0107, Hovy, and S\u00f8gaard 2015). Moreover, comparisons of state-of-the-art algorithms revealed that model transfer is competitive with machine translation in terms of performance (Conneau et al. 2018). Partly owing to these reasons, typological knowledge has been mostly harnessed in connection with model transfer, as we will discuss in \u00a7 5.2. Moreover, typological features can guide the selection of the best source language to match to a target language for language transfer (Agi\u0107 et al. 2016, inter alia), which benefits all the above-mentioned methods (see \u00a7 5.3).", "filtered_refids": [["b129", "b4"], ["b92", "b88", "b48", "b167", "b172", "b125", "b169", "b3"], ["b150", "b177", "b119", "b173"], ["b11", "b179", "b60"], [null, "b2", "b36"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3651, "num_references": 20}
{"corpusid_sectionid": "9088773-s1", "title": "SURVEY OF THE MESSAGE UNDERSTANDING CONFERENCES", "date": "1993-03-21", "section_title": "REVIEW OF PAST MUCs", "section": "The first Message Understanding Conference (MUC) was held in 1987, used ten narrative paragraphs from naval messages as a training corpus and two others as test data, and had no defined evaluation task or metrics. Researchers from six organizations ran their systems on the test data during the conference, then demonstrated and explained how the systems analyzed the texts. Two years later, the second MUC was held [10]. It made use of a training corpus of 105 naval message narratives of four different types, a dry-run test set of 20 narratives, and a final test set of five. An information extraction task was defined that consisted of identifying ten different pieces of information and representing them as slot fillers in a template resembling a semantic frame. This task emulates an information management application requiring the culling of facts from a large body of free text as a means to generate updates to a formatted database.\n\nA rudimentary set of scoring standards was developed, and the templates produced by the eight systems (including four of the six systems represented at the 1987 evaluation) were scored by hand by comparison with a hand-generated answer key. The nature of the corpus used for the second MUC was difficult enough that grammar coverage and parsing efficiency were serious issues. The domain was complex enough that the knowledge engineering job was greatly facilitated by the availability of documentation presenting much of the essential, declarative domain knowledge in a structured format.\n\nAfter another two-year interval, MUC-3 was held in May, 1991, followed by MUC-4 in June, 1992. There are published proceedings for the third and fourth conferences [8,9], including descriptions and test results of the participating systems (15 for MUC-3, 17 for MUC-4).\n\nA new corpus of 1,400 texts on the subject of Latin American terrorism was used that includes 16 text types (transcribed speeches, newspaper articles, editorial reports, etc.). The template developed for MUC-3 contained slots for 17 pieces of information; the number of informationbearing slots increased to 22 for MUC-4. The scoring metrics were refined and implemented for MUC-3 and MUC-4 in a semiautomated scoring program.\n\nFor MUC-3, a study was carried out to measure the complexity of the MUC-3 terrorism task vis-a-vis the naval task, and the scores obtained in the 1989 evaluation were recomputed using the MUC-3 method of scoring [5]. Although these scores were lower, the conclusion was that significant progress had been made, because the increase in difficulty in the task more than offset the decrease in scores.\n\nIt was possible to conduct a more refined study of the progress from MUC-3 to  that showed that higher levels of performance by nearly all veteran systems were achieved despite the relative difficulty of the MUC-4 test set that was used in the comparison and despite increased strictness of the scoring with respect to spurious data generation. The results of MUC-4 show that higher recall is usually correlated with higher precision 1, which is consistent with the results of previous evaluations and suggests that there is still a variety of techniques with potential for attaining even higher levels of performance in the future. In absolute terms, however, recall and precision scores were still only moderate.\n\nAccording to an analysis of the effectiveness of techniques used by MUC-3 systems [4], pattern-matching techniques (with hand-crafted or automatically acquired patterns) and probabilistic text categorzafion techniques proved successful only when combined with linguistic techniques. The use of robust processing including robust parsing was shown to correlate with the success of the system. In a comparison of MUC-3 and MUC-4 systems, minimal improvement from MUC-3 to MUC-4 was demonstrated by the two systems that did not use linguistically-based processing [12]. Several linguistically-based MUC-3 systems improved considerably via extensions made for MUC-4, as did one MUC-3 system that was converted from a generic text understanding system to an information extraction system that maintains its basis in linguistics but is streamlined for speed and geared specifically to the demands of information extraction. However, other systems which underwent a complete overhaul for MUC-4 showed only slight progress or even a degradation in performance.\n\nError analyses point to the critical need for further research in areas such as discourse reference resolution and inferencing. For example, the inability to reliably determine whether a description found in one part of the text refers or does not refer to something previously described inhibits both recall and precision because it could result in the system either missing information or generating spurious information; the inability to pick up subtle relevance indications (e.g., that persons described as being \"in\" a place that was attacked could be targets of the attack) and not-so-subtle ones (e.g., that a vehicle whose roof collapsed as a result of a bomb explosion was damaged by the explosion) places a limitation on recall because it results in missed information. The ability to take advantage of sophisticated approaches to discourse that have already received computational treatment is limited by a dependence on error-free outputs from earlier stages of processing. Thus, there is a need for renewed attention to robust processing at the sentence level.", "filtered_refids": [[null], [], [null], [], [null], [], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 5478, "num_references": 4}
{"corpusid_sectionid": "44130961-s1", "title": "TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation", "date": "2018-05-11", "section_title": "Prerequisite Chains", "section": "Prerequisite chains refer to edges in a graph describing which topics are dependent on the knowledge of another topic. Prerequisite chains play an important role in curriculum planning and reading list generation. Liu et al. (2016) propose \"Concept Graph Learning\" in order to induce a graph from which they can predict prerequisite relations among university courses. Their framework consists of two graphs: (1) a higher-level graph which consists of university courses and (2) a lowerlevel graph which consists of induced concepts and pair-wise sequential preferences in learning or teaching the concept. Liang et al. (2017) experiment with prerequisite chains on education data but focus on the recovery of a concept graph rather than on predicting unseen course relations as in Liu et al. (2016). They introduce both a synthetic dataset as well as one scraped from 11 universities which includes course prerequisites as well as conceptprerequisite labels. Concept graphs are also used in (Gordon et al., 2016) to address the problem of developing reading lists for students. The concept graph in this case is a labeled graph where nodes represent both documents and concepts (determined using Latent Dirichlet Allocation (LDA) (Blei et al., 2003)), and edges represent dependencies. They propose methods based on cross entropy and information flow for determining edges in the graph. Finally, finding prerequisite relationships has also been used in other contexts such as Massive Open Online Courses (MOOCs) (Pan et al., 2017a,b).", "filtered_refids": [["b12", "b13", null, "b1", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1535, "num_references": 5}
{"corpusid_sectionid": "44130961-s18", "title": "TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation", "date": "2018-05-11", "section_title": "Dataset and Annotation Statistics", "section": "We created reading lists for 182 of the 200 topics we identify in Section 4.2. Resources were not found for 18 topics due to the granularity of the topic (e.g., Radial Basis Function Networks) as well as our intended restriction of the chosen resources to PowerPoint presentations and HTML pages. The average number of resources per reading list for the 182 topics is 3.94. As an extension to the reading lists we collected Wikipedia pages for 184 of the topics and present these urls as part of the dataset. We annotated prerequisite relations for the 200 topics described above. We present a subset of our annotations in Figure 1, which shows the network of topic relations (nodes without incoming edges were not annotated for their prerequisites as part of this shown inter-annotation round). Our network consists of 794 unidirectional edges and 33 bidirectional edges. The presence of bidirectional edges stems from our definition of a prerequisite, which does not preclude bidirectionality (one topic can help explain another and viceversa) as well as the similarity of the topics. The set of bidirectional edges consists of topic pairs (BLEU -ROUGE; Word Embedding -Distributional Semantics; Backpropagation -Gradient descent) which could be collapsed into one topic to create a directed acyclic graph in the future.\n\nFor survey extraction, we automatically split 313 resources into content cards which we annotated for usefulness in survey extraction. These resources are a subset of the reading lists limited in number due to constraints in downloading urls and parsing to our annotation interface. The total number of cards which were not marked as repeats/mis-parsed totals 17,088, with 54.59 per resource. 6,099 cards were labeled as somewhat relevant or relevant for the target topic. The resources marked as non-relevant may be poorly  presented or may not pertain fully to the topic of that survey. These numbers confirm the appropriateness of this survey corpus as a non-trivial information retrieval task.\n\nTo better understand the difficulty of our annotation tasks, we performed inter-annotator agreement experiments for each of our annotations. We randomly sampled twenty-five resources and had annotators label for pedagogical function. Additionally, we sampled twenty-five topics for prerequisite annotations and five topics with reading list lengths of five for survey annotation. We used Fleiss's Kappa (Fleiss et al., 2004), a variant of Cohen's Kappa (Cohen, 1960) designed to measure annotator agreement for more than two annotators. The results are shown in Table 5. Using the scale as defined in Landis and Koch (1977), pedagogical function annotation exhibits substantial agreement while prerequisite annotation and survey extraction annotation show fair agreement. The Kappa score for pedagogical function is comparable to that of Sheng et al. (2017) (0.68) while the prerequisite annotation is slightly lower than the agreement metric used in Gordon et al. (2016) (0.36) although they measure agreement through Pearson correlation. We believe that the sparsity of the labels plays a role in these scores.", "filtered_refids": [[], [], ["b10", null, "b2", "b5", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 3135, "num_references": 5}
{"corpusid_sectionid": "44130961-s19", "title": "TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation", "date": "2018-05-11", "section_title": "Comparison to Similar Datasets", "section": "Our corpus distinguishes itself in its magnitude, manual collection and focus on annotation for educational purposes in addition to research tasks. We use similar categories for classifying pedagogical function as , but our corpus is hand-picked and over four-times larger, while exhibiting similar annotation agreement. Gordon et al. (2016) present a corpus for prerequisite relations among topics, but this corpus differs in coverage. They used LDA topic modeling to generate a list of 300 topics, while we manually create a list of 200 topics based on criteria described above. Although their topics are generated from the ACL Anthology and related to NLP, we find less than a 40% overlap in topics. Additionally, they only annotate a subset of the topics for prerequisite annotations while we focus on broad coverage, annotating two orders of magnitude larger in terms of prerequisite edges while exhibiting fair inter-annotator agreement.\n\nPrevious work and datasets on generating surveys for scientific topics have focused on scientific articles (Jha et al., 2013(Jha et al., , 2015Jaidka et al., 2016) and Wikipedia pages (Sauper and Barzilay, 2009;Liu et al., 2018) as a summarization task. We, on the other hand, view this problem as an information retrieval task and focus on extracting content from manually-collected PowerPoint slides and online tutorials. Sauper and Barzilay (2009) differ in their domain coverage, and while the surveys of Jha et al. (2013Jha et al. ( , 2015 focus on NLP, we collect resources for an order of magnitude larger set of topics. Finally, our focus here in creating surveys, as well as the other annotations, is first and foremost to create a useful tool for students and researchers. Websites such as the ACL Anthology 3 and arXiv 4 provide an abundance of resources, but do not focus on the pedagogical aspect of their content. Meanwhile, websites such as Wikipedia which aim to create a survey of a topic may not reflect the latest trends in rapidly changing fields.", "filtered_refids": [["b5"], ["b6", "b14", "b9", "b21", "b8"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2012, "num_references": 6}
{"corpusid_sectionid": "254854669-s1", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "John F. Kennedy", "section": "Negotiation is one of the crucial abilities in human communication that involves two or more individuals discussing goals and tactics to resolve conflicts, achieve mutual benefit, or find mutually acceptable solutions (Fershtman, 1990;Bazerman and Neale, 1993;Lewicki et al., 2011). It is a common aspect of human interaction, occurring whenever people communicate in order to manage conflict or reach a compromise. Scientifically, one of the long-term goals of dialogue research is to empower intelligent agents with such ability. Agent effectively negotiating with a human in natural language could have significant benefits in many scenarios, from bargaining prices in everyday trade-in (He et al., 2018) to high-stakes political or legal situations (Basave and He, 2016). Negotiation dialogue systems (Lewandowska, 1982;Lambert and Carberry, 1992;Chawla et al., 2021c) is an emerging research field that aims to build intelligent conversational agents that can automatically negotiate with a human in natural languages, e.g., CICERO 1 from Meta AI. Agents negotiate with human through multi-turn interaction using logically reasoning (Sycara and Dai, 2010) over goals (Zhang et al., 2020), strategies (Zhou et al., 2020) and psychology factors (Yang et al., 2021). As illustrated in Figure 1, negotiation dialogue agents interact with the human through multiturn cycles. A successful negotiation process involves efficient information exchange, strategic discussion toward their goals, and a closing section.\n\nDespite the significant amount of research that has been conducted on the task, there is a lack of a systematic review of the topic. In this work, we aim to fill this gap by reviewing contemporary work in the emerging field of negotiation dialogue systems, covering aspects such as benchmarks, evaluation, methodology, and future directions. In recent years, various benchmarks have been proposed for negotiation dialogue systems, ranging from bargaining (Lewis et al., 2017) and game scenarios (Asher et al., 2016) to job interviews  and items exchanging (Chawla et al., 2021c). Our survey will provide an overview of these benchmarks and discuss how they have been used to evaluate the performance of negotiation dialogue systems.\n\nModeling the negotiation process for conversational agents also imposes challenges. Firstly, these agents must be able to reason about and employ various strategies in different situations. In addition to strategy modeling, it is also necessary to model the personalities (e.g., mind, emotion, and behaviors) of the negotiators. Thirdly, an effective policy learning method is essential for the successful use of language. To address these challenges, we can categorize existing solutions into three areas: (1) Personality modeling helps us understand negotiator's preferences, (2) Strategy modeling enables agents to make reasonable decisions based on gathered information, and (3) Policy learning methods utilize information effectively to maximize results.\n\nIn summary, our contributions are three-fold: (1) To the best of our knowledge, we systematically categorize current negotiation dialogue benchmarks from the perspective of distributive and integrative, with each category based on different goal types of negotiation dialogue tasks. (2) We categorize typical evaluation methods and current solutions into an appropriate taxonomy. (3) We pointed out the current limitation and promising research directions in the future.", "filtered_refids": [["b10", "b56", "b4", "b47", "b29", "b32", "b55", "b59", "b24", "b33", "b2", "b17"], ["b1", "b10", "b34"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3479, "num_references": 15}
{"corpusid_sectionid": "254854669-s3", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Negotiation in Human", "section": "Humans negotiate everyday in their daily routines. Negotiation is used to manage conflict and is the primary give-and-take process by which people try to reach an agreement (Fisher et al., 2011;Lewicki et al., 2011). Research on negotiation has been conducted for almost 60 years in the field of psychology, political science, and communication. It has evolved over the past decades from exploring game theory (Walton and McKersie, 1991), behavior decisions driven by the cognitive revolution in psychology (Bazerman and Neale, 1993), to cultural differences in the 2000s (Bazerman et al., 2000). Negotiation research, however, is now forced to confront the implications of human/AI collaborations given recent advancements in machine learning (Sycara and Dai, 2010). Converging efforts from social scientists and data scientists who incorporate insights from both fields will be fruitful in maximizing expectations and outcomes during negotiation processes.\n\nNegotiation is a process by which two or more parties attempt to resolve their opposing interests. Strategy of negotiation can be distributive, such as bargaining (Fershtman, 1990) and integrative, such as maximizing unilateral interests (Bazerman and Neale, 1993), both of which are used in various social situations such as informal, peer to peer, organizational, and diplomatic country to country settings. The implications for enhancing outcomes are thus large and important to understand. Research from psychology demonstrates that the negotiation process can be affected by psychological factors, such as personality (Sharma et al., 2013), relationship (Olekalns and Smith, 2003), social status (Blader and Chen, 2012), and cultural background (Leung and Cohen, 2011).", "filtered_refids": [["b4", "b18", "b47", "b33", "b51", "b3"], ["b43", "b42", "b4", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1734, "num_references": 10}
{"corpusid_sectionid": "254854669-s6", "title": "Let's Negotiate! A Survey of Negotiation Dialogue Systems", "date": "2022-12-18", "section_title": "Integrative Negotiation Datasets", "section": "In integrative negotiations, there is normally more than one issue available to be negotiated. To achieve optimal negotiation goals, the involved players should make trade-offs for multiple issues.\n\nMulti-player Strategy Games The strategy video games provide ideal platforms for people to verbally communicate with other players to accomplish their missions and goals. Asher et al. (2016) propose the STAC benchmark, which is the player dialogue in the game of Catan. In this game, players need to gather resources, including wood, wheat, sheep, and more, with each other to purchase settlements, roads and cities. As each player only has access to their own resources, they have to communicate with each other. To investigate the linguistic strategies used in this situation, STAC also includes an SDRT-styled discourse structure. Boritchev and Amblard (2022) also collect a DinG dataset from French-speaking players in this game. The participants are instructed to focus on the game, rather than talk about themselves. As a result, the collected dialogues can better reflect the negotiation strategy used in the game process.\n\nNegotiation for Item Assignment The item assignment scenarios involve a fixed set of items as well as a predefined priority for each player in the dialogue. As the players only have access to their own priority, they need to negotiate with each other to exchange the items they prefer. Nouri and Traum (2014) propose InitiativeTalking, occurring between the owners of two restaurants. They discuss how to distribute the fruits (i.e., apples, bananas, and strawberries) and try to reach an agreement. Lewis et al. (2017) propose DealorNoDeal, a similar two-party negotiation dialogue benchmark where both participants are only shown their own sets of items with a value for each and both of them are asked to maximize their total score after negotiation. Chawla et al. (2021c) propose CaSiNo, a dataset on campsite scenarios involving campsite neighbors negotiating for additional food, water, and firewood packages. Both parties have different priorities over different items.\n\nNegotiation for Job Interview Another commonly encountered negotiation scenario is job offer negotiation with recruiters. Yamaguchi et al. (2021a) fill this gap and propose the JobInterview dataset. JobInterview includes recruiter-applicant interactions over salary, day off, position, company, and workplace. The participants are shown negotiators' preferences and the corresponding issues and options and are given feedback in the middle of the negotiation.", "filtered_refids": [[], ["b1", "b6"], ["b10", "b34", "b41"], ["b53"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2567, "num_references": 6}
{"corpusid_sectionid": "232320269-s1", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "date": "2021-03-16", "section_title": "Factuality", "section": "Veracity of information has been studied at different levels: (i) claim-level (e.g., fact-checking), (ii) article-level (e.g., \"fake news\" detection), (iii) user-level (e.g., hunting for trolls), and (iv) medium-level (e.g., source reliability estimation). Our primary interest here is in the latter.\n\nAt the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim [Castillo et al., 2011;Kochkina et al., 2018]. A set of web pages and snippets from search engines have also been used as a source of information [Karadzhov et al., 2017]. In either case, the most important information for the claimlevel tasks are stance (does a tweet or a news article agree or disagree with the claim?) and source reliability (do we trust the user who posted the tweet or the medium that published the news article?).\n\nThe problem of source reliability remains largely underexplored.\n\nIn the case of social media and community fora, it concerns modeling the user. In particular, there has been research on finding opinion manipulation trolls, paid [Mihaylov et al., 2015a] or just perceived [Mihaylov et al., 2015b], sockpuppets [Maity et al., 2017], Internet water army [Chen et al., 2013], and seminar users [Darwish et al., 2017]. In the case of the Web, it is about the trustworthiness of the source (the URL domain, the medium). The latter is our focus here.\n\nIn early work, the source reliability of news media has often been estimated automatically based on the general stance of the target medium with respect to known true/false claims, without access to gold labels about the overall medium-level factuality of reporting [Mukherjee and Weikum, 2015].\n\nMore recent work has addressed the task as one on its own right. [Baly et al., 2018] used gold labels from Media Bias/Fact Check, 1 and a variety of information sources: articles published by the medium, what is said about it on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, [Baly et al., 2019] used the same representation to jointly predict media factuality and bias on an ordinal scale, using a multi-task ordinal regression setup. Finally, [Baly et al., 2020b] extended the information sources to include Facebook followers and speech signals from the news medium's channel on YouTube (if any). Finally, [Hounsel et al., 2020] proposed to use domain, certificate, and hosting information of the website infrastructure.", "filtered_refids": [[], ["b19", "b10"], [], ["b14", "b12", "b22"], ["b22"], ["b18", "b2", "b5", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2548, "num_references": 10}
{"corpusid_sectionid": "232320269-s3", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "date": "2021-03-16", "section_title": "A Variety of Dimensions in Media Bias", "section": "Compared to factuality, which is decided by whether a piece of information is true or not, media bias has more complex dimensions. For the last few decades, many scholars have conceptualized media bias in different ways. For instance, a bias can be defined as \"imbalance or inequality of coverage rather than as a departure from truth\" [Stevenson et al., 1973]. They 1 http://mediabiasfactcheck.com particularly note that a departure from truth, as a bias, can be measured only when the accurate record of the event is available (e.g., trial transcript).\n\nA different definition, \"Any systematic slant favoring one candidate or ideology over another\" [Waldman and Devitt, 1998], is proposed to capture various other dimensions rather than coverage imbalance, such as favorability conveyed in visual representations (i.e., news photos). For example, smiling, speaking at the podium, cheering crowd, and eye-level shots are preferred over frowning, sitting, being alone, and shots from above, respectively.\n\nD'Alessio and Allen reviewed 59 quantitative studies about partisan media bias in presidential elections [D'Alessio and Allen, 2000], and based on this analysis, they proposed to categorize media bias into the following three types: (i) gatekeeping bias, where editors and journalists 'select' the stories to report, (ii) coverage bias, where the amount of news coverage (e.g., the length of newspapers articles, or the time given on television) each party receives is systematically biased to one party at the expense of the other one, and (iii) statement bias, where news media interject their attitudes or opinions in the news reporting. Groeling proposed a more relaxed concept of media bias, which is \"a portrayal of reality that is significantly and systematically (not randomly) distorted,\" to take a variety of media bias dimensions into account [Groeling, 2013]. In particular, he focused on two main forms of media bias-selection bias (i.e., what to cover) and presentation bias (i.e., how to cover it)-driven by the choices of newsmakers.\n\nSelection bias or gatekeeping bias, has been studied in various ways, including qualitative interviews or surveys of journalists and editors about the decision making process they use to select the news stories in their newsroom [Tandoc Jr, 2014]. Data-driven research on selection bias follows the common steps: (i) collect news articles (for newspapers or online news) or transcripts (for television news) for a target period, (ii) conduct content analysis to find the news coverage of politicians, parties, events, etc. Sometimes the tone of the news articles can be studied (i.e., negative news stories are more frequently reported or selected by the editors compared to positive news) [Soroka, 2012], and (iii) identify systematic biases by comparing their news coverage. An exhaustive database of news stories is, thus, essential for selection bias research. While commercial databases, such as Lexis Nexis, have been widely used [Soroka, 2012], publicly available datasets, such as GDELT or Google News, start to get attention [Kwak et al., 2014;Boudemagh and Moise, 2017;Kwak et al., 2018] and are getting validated by comparing multiple sources [Weaver and Bimber, 2008;Kwak et al., 2016].", "filtered_refids": [[null], ["b25"], ["b16", "b13"], ["b25", null, "b19", "b23", "b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 3255, "num_references": 9}
{"corpusid_sectionid": "232320269-s5", "title": "A Survey on Predicting the Factuality and the Bias of News Media", "date": "2021-03-16", "section_title": "Framing Bias", "section": "Framing refers to a process that highlights a certain aspect of an event or an issue more than the others [Entman, 1993]. Emphasizing an issue's particular aspect can deliver a distorted view toward the issue even without the use of biased expressions.\n\nFraming biases have been typically studied at issue level. Researchers collect news articles about a particular issue or event, conduct manual content analysis on them, and build a frame detection model [Baumer et al., 2015]. Although this approach successfully characterizes diverse frames, it is not trivial to compare media's framing across different issues.\n\nThe Media Frames Corpus (MFC) was proposed to address this limitation. It contains articles annotated with 15 generic frames (including others) across three policy issues [Card et al., 2015]. Several studies have demonstrated reasonable prediction performance of the general media frames with different datasets [Field et al., 2018;. These 15 general frames were also used for identifying frames in political discourse on social media [Johnson et al., 2017]. General media frames are often customized to a specific issue by adding issue-specific frames [Liu et al., 2019], even though doing so somewhat contradicts the original motivation of using general media frames, namely to be able to compare frames across various issues.", "filtered_refids": [["b14"], ["b8"], [null, "b19", "b22"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1345, "num_references": 5}
{"corpusid_sectionid": "264439179-s3", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Metrics", "section": "Previous studies (Mitchell et al., 2023;Sadasivan et al., 2023) predominantly used the Area Under the Receiver Operating Characteristic (AUROC) score to gauge the effectiveness of detection algorithms.As a binary classification problem, AUROC shows the results under different thresholds, and the F1 score is also helpful.Krishna et al. (2023); Yang et al. (2023b) suggest that AUROC may not consistently provide a precise evaluation, particularly as the AUROC score nears the optimal limit of 1.0 since two detectors with identical AUROC score of 0.99 could exhibit substantial variations in detection quality from a user's perspective.From a practical point of view, ensuring a high True Positive Rate (TPR) is imperative while keeping the False Positive Rate (FPR) to a minimum.As such, current research (Krishna et al., 2023;Yang et al., 2023b) both report TPR scores at a fixed 1% FPR, along with the AUROC.Other work (Sadasivan et al., 2023) also refer to Type I and Type II errors following the binary hypothesis test and even report TPR at 10 \u22126 FPR (Fernandez et al., 2023).", "filtered_refids": [["b57", "b92", "b78", "b30", "b127"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1083, "num_references": 5}
{"corpusid_sectionid": "264439179-s4", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Datasets", "section": "In this section, we discuss the common datasets used for this task.The corpus is usually adopted from previous NLP tasks, and reconstructed by prompting LLMs to generate new outputs as candidate machine-generated text.Usually, there are two prompting methods: 1).prompting LLMs with the questions in some question-answering datasets.2).prompting LLMs with the first 20 to 30 tokens to continue writing in datasets without specific questions.Specifically, several datasets have been compiled and utilized in the field.Some noteworthy datasets include TURINGBENCH (Uchendu et al., 2021), HC3 (Guo et al., 2023), CHEAT (Yu et al., 2023a), Ghostbuster (Verma et al., 2023), OpenGPTText (Chen et al., 2023), M4 (Wang et al., 2023d), MGTBench (He et al., 2023), and MULTI-TuDE (Macko et al., 2023) and some other datasets not explicitly built for detection have also been used, such as C4 (Raffel et al., 2019), shareGPT2 , and alpaca (Taori et al., 2023), as summarized in Table 1.For text detection, we only list datasets explicitly built for detection, while some general datasets like C4 (Raffel et al., 2019) or alpaca (Taori et al., 2023) can also be used.For code detection, we only list datasets that have been used in previous code detection work (Lee et al., 2023;Yang et al., 2023e).And other codegeneration corpora can also be adopted.The detailed description is included in Appendix A.3.\n\nData Contamination.Despite those released standard datasets, we argue that static evaluation benchmarks might not be desirable for this problem with the rapid progress of LLMs trained, tuned, or aligned on large amounts of data across the whole internet.On the one hand, Aaronson (2022) mentioned that some text from Shakespeare or the Bible is often classified as AI-generated because such classic text is frequently used in the training datasets for generative language models.On the other hand, many detectors did not fully disclose their training data, especially commercial tools like GPTZero (Tian, 2023).It is natural to worry that those standard evaluation benchmarks would face a serious test data contamination problem, considering the commercial detectors would consistently improve their products for profits.So, with the rapid evolution of LLMs and detectors, the traditional paradigm of providing standard benchmarks might no longer be suitable for AI-generated text detection.We provide a unique solution to this:\n\nUtilize the most latest human-written content to reduce data contamination problem by collecting such content from the most updated open-source websites, which themselves explicitly forbid posting AI-written posts.", "filtered_refids": [["b115", "b86", "b63", "b103", "b130", "b41", "b134", "b88", "b129", "b74", "b38", "b110"], ["b104", "b0"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2640, "num_references": 14}
{"corpusid_sectionid": "264439179-s15", "title": "A Survey on Detection of LLMs-Generated Content", "date": "2023-10-24", "section_title": "Zero-Shot", "section": "In the zero-shot setting, we do not require extensive training data to train a discriminator.Instead, we can leverage the inherent distinctions between machine-generated and human-written text, making the detector training-free.The key advantage of training-free detection is its adaptability to new data distributions without the need for additional data collection and model tuning.It's worth noting that while watermarking methods can also be considered zero-shot, we treat them as an independent track.Previous work utilizes entropy (Lavergne et al., 2008), average log-probability score (Solaiman et al., 2019), perplexity (Beresneva, 2016), uncommon n-gram frequencies (Grechnikov et al., 2009;Badaskar et al., 2008) obtained from a language model as the judge for determining its origin.However, those simple features fail as LLMs are becoming diverse and high-quality text generators.\n\nSimilarly, there are also black-and white-box detection, as summarized below.", "filtered_refids": [["b98", "b9", "b62", "b5", "b36"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 971, "num_references": 5}
{"corpusid_sectionid": "232075945-s2", "title": "A Survey on Stance Detection for Mis-and Disinformation Identification", "date": "2021-02-27", "section_title": "Source(s) Target", "section": "Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) \u01cc Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) \u01cc Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  \u0240  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, \u01cc News, \u0240ikipedia, Reddit. Evidence: Single, Multiple, Thread.\n\n2 What is Stance?\n\nIn order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as \"a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field\", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by K\u00fc\u00e7\u00fck and Can (2020): \"for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text\" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).\n\nFinally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.", "filtered_refids": [["b12", null, "b57", "b17"], [], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2863, "num_references": 5}
{"corpusid_sectionid": "232075945-s5", "title": "A Survey on Stance Detection for Mis-and Disinformation Identification", "date": "2021-02-27", "section_title": "Stance as a (Mis-/Dis-)information Detection Component", "section": "Fully automated systems can assist in gauging the extent and studying the spread of false information online. This is in contrast to the previously discussed applications of stance detection -as a stand-alone system for detecting mis-and disinformation. Here, we review its potency to serve as a component in an automated pipeline. Figure 1b illustrates the setup, which can also include steps such as modelling the user or profiling the media outlet among others. We discuss in more detail media profiling and misconceptions in Appendix B.\n\nRumors Stance detection can be used for rumour detection and debunking, where the stance of the crowd, media, or other sources towards a claim are used to determine the veracity of a currently circulating story or report of uncertain or doubtful factuality. More formally, for a textual input and a rumour expressed as text, stance detection here is to determine the position of the text towards the rumour as a category label from the set {Support, Deny, Query, Comment}. Zubiaga et al. (2016b) define these categories as whether the author: supports (Support) or denies (Deny) the veracity of the rumour they are responding to, \"asks for additional evidence in relation to the veracity of the rumour\" (Query) or \"makes their own comment without a clear contribution to assessing the veracity of the rumour\" (Comment). This setup was widely explored for microblogs and social media. Qazvinian et al. (2011) started with five rumours and classified the user's stance as endorse, deny, unrelated, question, or neutral. While they were among the first to demonstrate the feasibility of this task formulation, the limited size of their study and the focus on assessing the stance of individual posts limited its real-world applicability. Zubiaga et al. (2016b) analysed how people spread rumours on social media based on conversational threads. They included rumour threads associated with nine newsworthy events, and users' stance before and after the rumours were confirmed or denied. Ferreira and Vlachos (2016) collected claims and news articles from rumour sites with annotations for stance and veracity by journalists as part of the Emergent project. The goal was to use the stance of a news article, summarised into a single sentence, towards a claim as one of the components to determine its veracity. A downside is the need to summarise, in contrast to FNC-1 (Pomerleau and Rao, 2017), where entire news articles were used.  Zubiaga et al. (2016b). Bo\u0161njak and Karan (2019) studied stance detection and claim verification of comments for Croatian news articles.", "filtered_refids": [[], ["b12", "b57", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2609, "num_references": 3}
{"corpusid_sectionid": "232075945-s6", "title": "A Survey on Stance Detection for Mis-and Disinformation Identification", "date": "2021-02-27", "section_title": "Multiple languages", "section": "In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.\n\nFact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.\n\nMohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.\n\nMore recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.\n\nGuderlei and A\u00dfenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).\n\nSome formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.\n\nAnother notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using \"born in/on\". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as \"Sarawak is a ...\", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).\n\nError analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., \"Andrea Pirlo is an American professional footballer.\" vs. \"Andrea Pirlo is an Italian professional footballer who plays for an American club.\", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., \"Terry Crews played on the Los Angeles Chargers.\" (NotE-noughInfo) is classified as refuted, given the sentence \"In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ...\", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, \"The heart beats at a resting rate close to 22 bpm.\" is not classified as refuted based on the evidence sentence \"The heart beats at a resting rate close to 72 bpm.\", and similarly for months.\n\nThreaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.\n\nThese approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).\n\nA major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).\n\nAnother factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.\n\nMulti-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.\n\nEarlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Sch\u00fctze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.", "filtered_refids": [[], [null, "b19"], [], ["b31"], [null], ["b43", "b52", "b47", "b50", "b30", null, "b51", "b8"], [null], [], [null, "b57"], ["b49", null, "b55", "b36"], ["b49", null, "b57"], [null, "b38"], [], ["b24", null, "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 65, "num_chars": 11884, "num_references": 27}
{"corpusid_sectionid": "254043519-s5", "title": "Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources", "date": "2022-11-28", "section_title": "Input and Label Collection", "section": "Input Source: Collection Process for Input Texts (x). We classify input text source into three high-level categories (i.e., generated by human, collected from websites, and extracted from curated sources) and break them down into nine finegrained categories. Table 4 shows the categories and annotation results. While we cover 222 languages, only 40 of them (18%) have input text specifically written by humans for the task. * The news is the most common source, used by 40 of 156 datasets, often in summarization or classification tasks, followed by web corpora * and Wikipedia. Many languages have datasets derived only from Wikipedia text. Figure 6 in appendix shows pertask input source distributions. Overall, we observe that high-resource languages entertain a variety of input sources, while low-resource ones rely on fewer resources such as Wikipedia and news.\n\nLabel Source: Collection Process for Labels (y). Table 5 presents the statistics on how the output labels were collected, split into five categories: annotated by authors or linguists, crowdsourced, automatically induced, derived from linguistic resources, and not mentioned. Label collection methods affects dataset quality. While manually annotated datasets can exhibit artifacts (Gururangan et al., 2018;Poliak et al., 2018), they are often val-* Due to overlap of language covered among datasets, there are 40 unique languages instead of 53 (from Table 4). * The \"web\" fine-grained category refers to the collection of sentences scraped or sampled at large-scale from the web. idated via inter-annotator agreement. In contrast, automatically induced datasets are often introduced without such kind of validation phases and tend to be noisy. For example, bullet points from news article are often considered to be the summary of the article, which can contain missing background information in the rest of the article (Kang and Hashimoto, 2020;Goyal et al., 2022). Fifty-three datasets had automatically induced labels, most commonly seen for summarization and classification tasks, and 95 used manual annotation. This further breakdowns to 27 datasets solely annotated by domain experts and 56 datasets solely annotated by crowdworkers. We investigated annotator pools for non-English languages in Section 5.\n\nLabel Source and Task Types. Figure 3 presents label collection methods per task type. QA with retrieval (e.g., XQA; Liu et al. 2019) and generation tasks show a high proportion of automatically induced datasets. In contrast, structured prediction datasets were rarely automatically induced; they were more often annotated by authors or linguists. Crowdsourcing is commonly used to construct reading comprehension and classification datasets.\n\nLabel Source and Language Diversity. Figure 4 shows the distributions of the label data collection methods for the top 10 languages and for 20 sampled languages. In high-resource languages, a large number of datasets are labeled manually, where in low-resource languages, the percentage of automatically induced datasets increases, with 135 languages have only automatically induced datasets. On (macro-)average, the 10 highest resource languages show 43.4% of their datatsets with only automatically induced labels; however, for all languages, 84.9% of the datasets use only automatically induced labels. Prior work often uses the total number of datasets in a target language as a proxy for resource availability of the language, which our analysis suggests is limited.", "filtered_refids": [[], [null, "b7"], ["b2"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 3499, "num_references": 3}
{"corpusid_sectionid": "254043519-s10", "title": "Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources", "date": "2022-11-28", "section_title": "Pilot Study on Crowdsourcing Multilingual Data on MTurk", "section": "How easy is it to collect multilingual dataset on popular annotation platform (MTurk) at the moment? Crowdsourcing enables large-scale, cost efficient data collection; however, for many languages, the number of language-proficient crowdworkers is limited (Garcia et al., 2021). We quantify the availability of MTurk workers with proficiency in non-English languages. We formulated a four-way sentiment analysis task using the Multilingual Amazon Review Corpus (Keung et al., 2020) and analyzed the annotation quality, cost, and time to finish tasks in English, Spanish, German, French, Japanese and Chinese of crowdworkers.\n\nIn all settings, we asked annotators to translate the same English sentence to assess their actual (rather than professed) language proficiency. We found that many production-level MT systems fail to translate his sentence due to its compositionality. Further, we investigated the newly introduced \"language qualification\" in MTurk, which available for only four aforementioned languages and Brazilian Portuguese as of 2022. For our sentiment analysis task, without the language qualification, the accuracy of human binary classification performance in all non-English languages (55.2%) was significantly worse than that of English (77%). Past recommendations, such as constraining location and HIT acceptance rate, are insufficient (as of 2022) to collect high quality data even for languages considered \"easier\" to crowdsource in prior work (Pavlick et al., 2014). Language qualification improved performance by up to 40% and reduced the prevalence of cheating across all languages. However, with the language qualification, the data collection process usually took more time and cost ($1 per assignment). More pilot study details are in Appendix C.\n\nQuality Control Using Translation Task. We investigate whether crowdworkers relied on automatic machine translation, despite our instruction saying not to use them. We ask native speakers to compare the crowdsourced translation with the translation results from three major translation platforms: Google Translate, * Microsoft Bing Translator, * and DeepL Translator. * Without language qualification, we identified 33% of crowdworkers copy-and-pasted automatic translation outputs (with qualification, 7%). This is significantly higher than what Pavlick et al. (2014) report (10%), suggesting more crowdworkers have started to use MT services.\n\nWe found that we could potentially use the translation task to identify good submissions: if we take only the submissions whose translation (1) do not match translation from MT and (2) valid translated judged by native speakers (labeled as either correct or partially correct), * binary task accuracy rises to 81.0% from 63.5%, matching the binary accuracy of English.\n\nOur pilot study suggests that translation quality can reflect the target task performance if workers who copy from MT systems are filtered, and can be a good proxy for the languages without aforementioned language qualifications.", "filtered_refids": [[null], ["b6"], ["b6"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3023, "num_references": 3}
{"corpusid_sectionid": "259833865-s3", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "MRG Systems and Applications", "section": "Since the seminal work of Kukich (1983), there have been several kinds of medical report systems and applications, such as the generation of psychiatric case notes (Kazi and Kahanda, 2019), the generation of consultation notes from transcripts (Papadopoulos Korfiatis et al., 2022), the generation of radiology reports (Chen et al., 2021), nurse-patient summaries (Liu et al., 2019), counseling (conversation) summarization , discharge summaries or clinical notes (Krishna et al., 2021), and even data augmentation for other medical tasks (Kocabiyikoglu et al., 2021). Joshi et al. (2020) provided a general definition of a medical report in the case of medical dialogue summarization: \"the medical report captures and summarizes the important parts of the medical conversation necessary for clinical decision-making and subsequent follow-ups.\"\n\nDespite the diversity of their tasks, structures and audiences, the main characteristics of MRG remain similar, namely the use of the documentation and the subsequent use of the diagnosis, which can also be used for administration and by institutions, subsequently referenced by clinicians and retained by patients. The main objectives of such systems in clinical practice are to reduce the time spent by clinicians on manual writing and facilitate medical decision-making.", "filtered_refids": [["b12", "b6", "b18", "b4", null, "b7", "b8", "b2"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 1319, "num_references": 8}
{"corpusid_sectionid": "259833865-s5", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "Text-to-text", "section": "There are three main tasks in the Text-to-Text category: 1) summarizing medical dia-logues/conversations, including spoken conversations and online medical conversations; 2) summarizing hospital stays/hospitalizations; and 3) summarizing medical reports, where the original reports may come from different domains, such as radiology reports or general clinical reports.\n\nThe most common work in text-based MRG is that of summarizing medical conversations, where the input source can be either transcripts of clinician-patient spoken conversations (Kazi and Kahanda, 2019;Enarvi et al., 2020;Liu et al., 2019;Krishna et al., 2021;Molenaar et al., 2020;Lacson et al., 2006;Moramarco et al., 2022;Yim and Yetisgen, 2021); or online medical conversations (Chintagunta et al., 2021;Nair et al., 2021;Joshi et al., 2020;Song et al., 2020;Chen et al., 2022).\n\nRegarding the summarization of hospital stays, some work (Di Eugenio et al., 2014;Acharya et al., 2016) used both physician discharge notes (free text) and the structured nursing documentation (such as nursing plans of care) to generate a unique summary. Others generated summaries from longform hospital admissions (Adams et al., 2023).\n\nMoreover, work has also been carried out on summarizing medical reports. For example, Moramarco et al. (2021) used the MTSamples dataset to fill automatically the 'description field' of a medical report based on the information present in the overall report. In addition, radiology report summarization Karn et al., 2022) is intended to produce a concise and easily comprehensible 'IMPRESSIONS' section from the rest of the radiology report. The 'IM-PRESSIONS' section of a radiology report is considered a summary of the radiologist's reasoning and conclusions, which helps the referring physician confirm or exclude certain diagnoses (Karn et al., 2022).", "filtered_refids": [[], ["b12", "b4", "b18", "b9", null, "b33", "b7", "b38", "b2"], [null], [null, "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1848, "num_references": 12}
{"corpusid_sectionid": "259833865-s6", "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports", "date": 2023, "section_title": "Data-to-text", "section": "As presented in Table 1, there are different tasks in the Data-to-Text category: 1) generation of reports from medical images, such as radiology images, brain image data.; 2) generation of text summaries from intensive care data; and 3) generation of medical reports from multimodal inputs; 4) other applications such as the generation of tailored smoking cessation letters based on responses to a smoking questionnaire (Reiter et al., 2003).\n\nIn order to meet the growing demand of imagebased diagnosis from patients using artificial in- Other applications Generation of tailored smoking cessation letters Reiter et al. (2003)  telligence and applying image captioning to the medical field, radiology report generation is the subject of continuous work and growing interest from researchers, which aims to describe radiology images with professional quality reports (Chen et al., 2020(Chen et al., , 2021Lovelace and Mortazavi, 2020;Nooralahzadeh et al., 2021;Yan et al., 2021;Qin and Song, 2022;Miura et al., 2021;Liu et al., 2021). Such research has also been applied to generation of clinician reports from brain imaging data (Jordan et al., 2014). Besides images, the summarization of physiological data as also been the subject of research. In the BabyTalk project, Portet et al. (2009) presented a prototype that generates textual summaries of about 45 minutes of continuous physiological signals and discrete events. Their evaluation with physicians showed that text summaries could be an effective decision-support aids for clinicians.\n\nTo cope with the high workload due to the time required for proper documentation, Maas et al. (2021) presented a real-time automated report of the interaction between care provider and patient, taking multimodal inputs that include audio, video, and sensor data from medical consultations, and in-troducing knowledge graphs -the Patient Medical Graph. They used speech and action recognition technology to first transform multimodal inputs into text before formally representing them and generating reports.", "filtered_refids": [["b26"], ["b11", "b26", "b13", "b20", null, "b22", "b1", "b37"], ["b15"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 2053, "num_references": 10}
{"corpusid_sectionid": "3176028-s2", "title": "Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review", "date": "1981-04-01", "section_title": "Kantor's thesis", "section": "Kantor's main exhibit is the following text:\n\n(2-1) A good share of the amazing revival of commerce must be credited to the ease and security of communications within the empire. 'The Imperial fleet kept the Mediterranean Sea cleared of pirates. In each province, the Roman emperor repaired or constructed a number of skillfully designed roads. They were built for the army but served the merchant class as well. Over them, messengers of the Imperial service, equipped with relays of horses, could average fifty miles a day.\n\nHe claims that the they in the penultimate sentence is hard to comprehend, and that most informants need to reread the previous text to find its referent. Yet the sentence is neither semantically anomalous nor ambiguous --the roads is the only plural NP available as a referent, and it occurs immediately before the pronoun with only a full-stop intervening. To explain this paradox is the task Kantor set himself.\n\nKantor's explanation is based on discourse topic and the listener's expectations. In (2-1), the discourse topic of the first three sentences is ease and security of communication in the Roman empire. In the fourth sentence, there is an improper shift to the roads as the topic: improper, because it is unexpected, and there is no discourse cue to signal it. Had the demonstrative these roads been used, the shift would have been okay.\n\n3 Underlining is used in this and subsequent examples to indicate the anaphor(s) of interest. It does not indicate stress.\n\n(Note that a definite NP such as the roads is not enough.) Alternatively, the writer could have clarified the text by combining the last three sentences with semicolons, indicating that the last two main clauses were to be construed as relating only to the preceding one rather than to the discourse as a whole.\n\nKantor identifies a continuum of factors affecting the comprehension of pronouns. At one end is unrestricted expectation and at the other negative expectation. What this says in effect is that a pronoun is easy to understand if its referent is expected, and difficult if it is unexpected. This is not as vacuous as it at first sounds; Kantor provides an analysis of some subtle factors which affect expectation.\n\nThe most expected pronominalizations are those whose referent is the discourse topic, or something associated with it (though note the qualifications to this below). Consider:\n\n(2-2) The final years of Henry's reign, as recorded by the admiring Hall, were given over to sport and gaiety, though there was little of the licentiousness that characterized the French court. The athletic contests were serious but very popular. Masques, jousts and spectacles followed one another in endless pageantry. He brought to Greenwich a tremendously vital court life, a central importance in the country's affairs, and above all, a great naval connection. 4\n\nIn the last sentence, he is quite comprehensible, despite the distance back to its referent, because the discourse topic in all the sentences is Henry's reign.\n\nAn example of the converse --an unexpected pronoun which is difficult despite recency --can be seen in (2-1) above. Between these two extremes are other cases involving references to aspects of the local topic, changes in topic, syntactic parallelism, and, in topicless instances, recency (though the effect of recency decays very fast). I will not describe these here; the interested reader is referred to Section 2.6.5 of Kantor's dissertation (1977).\n\nKantor then defines the notion of the activatedness of a concept. This provides a continuum of Concept givenness, which contrasts with the simple binary given-new distinction usually accepted in linguistics (for example, Chafe 1970). Kantor also distinguishes activatedness from the similar \"communicative dynamism\" of the Prague school (Firbas 1964). Activated-4 From: Hamilton, Olive and Hamilton, Nigel. Royal Greenwich. Greenwich: The Greenwich Bookshop, 1969. Quoted by Halliday and Hasan (1976:14), quoted by Kantor (1977). ness is defined in terms of the comprehensibility phenomena described above: the more activated a concept is, the easier it is to understand an anaphoric reference to it. Thus activatedness depends upon discourse topic, context, and so forth.", "filtered_refids": [[], [], [], [], [], [], [], [], [], [], [null], ["b20", null, "b7", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 37, "num_chars": 4262, "num_references": 5}
{"corpusid_sectionid": "3176028-s3", "title": "Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review", "date": "1981-04-01", "section_title": "The implications of Kantor's work", "section": "What are the ramifications of Kantor's thesis for focus? Clearly, the notions of activatedness and focus are very similar, though the latter has not generally been thought of as a continuum. It follows that the factors Kantor finds relevant for activatedness and comprehensibility of pronouns are also important for those of us who would maintain focus in computerbased natural language understanding (NLU) systems; we will have to discover discourse topic and topic shifts, generate pronominalization expectations, and so forth.\n\nIn other words, if we could dynamically compute (and maintain) the activatedness of each concept floating around, we would have a measure for the ordering of the focus set by preferability as referent; the referent for any given anaphor would be the most highly activated element which passes basic tests for number, gender and semantic reasonableness. And to find the activatedness of the concepts, we follow Kantor's pointers (which he himself concedes are very tenuous and difficult) to extract and identify the relevant factors from the text.\n\nIt may be objected that by applying Kantor's insights all we have done is produce a mere notational variant of our original problem. This is partly true. One should not gainsay the power of a good notation, however, and what we can buy here even with mere notational variance is the power of Kantor's investigations. And there is more. Previously, it has been suggested that items either are in focus or they aren't, and that at each separate anaphor we need to compute a preference ranking of the focus elements for that anaphor. What Kantor tells us is that such a ranking exists independently of the actual use of anaphors in the text, and that we can find the ranking by looking at things like discourse topic. Some miscellaneous comments on Kantor's work:\n\n1. It can be seen as a generalization albeit a weakening of Grosz's (1977aGrosz's ( , 1977bGrosz's ( , 1978) findings on focus in task-oriented dialogues (where each sub-task becomes the new discourse topic, opening up a new set of possible referents), which are discussed below in Section 3. (Kantor and Grosz were apparently unaware of each other's work; neither cites the other.) 2. It provides an explanation for focus problems that have previously baffled us. For example, in Hirst (1977a) I contemplated the problem of the illformedness of this text:\n\n(2-3) *John left the window and drank the wine on the table. It was brown and round.\n\nI had previously thought this to be due to a syntactic factor --that cross-sentence pronominal reference to an NP in a relative clause or adjectival phrase qualifying an NP was not possible. However, it can also be explained as a grossly inconsiderate pronoun which does not refer to the topic properly --the table occurs only as a descriptor for the wine, and not as a concept \"in its own right\". This would be a major restriction on possible reference to sub-aspects of topics.\n\n3. Like too many other researchers, Kantor makes many claims about comprehensibility and the degree of well-formedness of sentences which others (as he concedes) may not agree with. He uses only himself (and his friends, sometimes) as an informant, and then only at an intuitive level. 5 Claims as strong and subtle as Kantor's cry out for empirical testing.6 Barbara Grosz (1977a, 1977b studied the maintenance of the focus of attention in task-oriented dialogues and its effect on the resolution of definite reference, as part of SRI's speech understanding system project (Walker 1978). By a task-oriented dialogue is meant one which has some single major welldefined task as its goal. For example, Grosz collected and studied dialogues in which an expert guides an apprentice in the assembly of an air compressor. She found that the structure of such dialogues parallels the structure of the task. That is, just as the major task is divided into several well-defined sub-tasks, and these perhaps into sub-sub-tasks and so on, the dialogue is likewise divided into sub-dialogues, sub-sub-dialogues, etc, 7 each corresponding to a task component, much as a well-structured Algol program is composed of blocks within blocks within blocks. As the dialogue progresses, each sub-dialogue in turn is performed in a strict depth-first order corresponding to the order of subtask performance in the task goal (though note that some sub-tasks may not be ordered with respect to 5 For a discussion of the problem of idiosyncratic wellformedness judgments, and a suggested solution, see Sections 4.2 and 7.3 of Hirst (1981). 6 Kantor tells me that he hopes to test some of his assertions by observing the eye movements of readers of considerate and inconsiderate texts, to find out if inconsiderate texts actually make readers physically search back for a referent.", "filtered_refids": [[], [], [], ["b14", "b9", "b8", "b10"], [], [], [null, "b28", "b8", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 4822, "num_references": 8}
{"corpusid_sectionid": "3176028-s7", "title": "Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review", "date": "1981-04-01", "section_title": "Focus in the PAL system", "section": "The PAL personal assistant program (Bullwinkle 1977a) is a system designed to accept natural language requests for scheduling activities. A typical request (from Bullwinkle 1977b:44) is:\n\n(4-1) I want to schedule a meeting with Ira. It should be at 3 pm tomorrow. We can meet in Bruce's office.\n\nThe section of PAL that deals with discourse pragmatics and reference was developed by Candace Sidner [Bullwinkle] (Bullwinkle 1977b;Sidner 1978a). Like Grosz's system, PAL attempts to find a focus of attention in its knowledge structures to use as a focus for reference resolution.\n\nSidner sees the focus as equivalent to the discourse topic; in fact in Bullwinkle (1977b) the word topic is used instead of focus.\n\nThere are three major differences from Grosz's system:\n\n1. PAL does not rely heavily on discourse structures.\n\n2. Knowledge is represented in frames.\n\n3. Focus selection and shifting are handled at a more superficial level.\n\nI will discuss each difference in turn.", "filtered_refids": [["b1"], [], [null, "b2", "b37"], [], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 976, "num_references": 4}
{"corpusid_sectionid": "234093015-s1", "title": "A Survey of Data Augmentation Approaches for NLP", "date": "2021-05-07", "section_title": "Background", "section": "What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hern\u00e1ndez-Garc\u00eda and K\u00f6nig, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.\n\nWhat are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.\n\nRule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.\n\nFurther, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.\n\nKashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical \"run-all-heuristics\" comparison, which can be very time and cost intensive.\n\nInterpretation of DA Dao et al. (2019) note that \"data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles\", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in \u00a76, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.", "filtered_refids": [[null], [null, "b23"], [null, "b1"], [], [], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 22, "num_chars": 3180, "num_references": 6}
{"corpusid_sectionid": "234093015-s3", "title": "A Survey of Data Augmentation Approaches for NLP", "date": "2021-05-07", "section_title": "Rule-Based Techniques", "section": "Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model's feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space \"analogy\" transformations between examples of known classes to augment for novel classes (see \u00a74.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally \"stretch\" an example along the class-manifold. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap. They show improved performance on many text classification tasks. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.\n\nFor paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges. They use balance theory and transitivity to infer augmented sentence pairs from this graph. Motivated by image cropping and rotation, \u015eahin and Steedman (2018) propose dependency tree morphing. For dependency-annotated sentences, children of the same parent are swapped (\u00e0 la rotation) or some deleted (\u00e0 la cropping), as seen in Figure 2. This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic).", "filtered_refids": [[null, "b1", "b7"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1538, "num_references": 4}
{"corpusid_sectionid": "234093015-s4", "title": "A Survey of Data Augmentation Approaches for NLP", "date": "2021-05-07", "section_title": "Example Interpolation Techniques", "section": "Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).\n\nAnother class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, \u015eahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see \"Multimodal challenges\" in \u00a76).\n\nA bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the \"hard\" version samples a binary mask (from a Bernoulli with a \u03b2(\u03b1, \u03b1) prior) and picks from one of two sequences at each token position, while the \"soft\" version softly interpolates between sequences based on a coefficient sampled from \u03b2(\u03b1, \u03b1). The \"soft\" version is found to outperform the \"hard\" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).", "filtered_refids": [[null, "b20"], [null, "b19"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2131, "num_references": 5}
{"corpusid_sectionid": "235790370-s1", "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments", "date": "2021-07-08", "section_title": "Up a Tree", "section": "You are about 10 feet above the ground nestled among some large branches. On the branch is a small birds nest. In the bird's nest is a large egg encrusted with precious jewels, apparently scavenged somewhere by a childless songbird. >take egg\n\nTaken. >climb down tree   (Lebling et al., 1979), frequently used as a benchmark for agent performance. User-entered actions are italicized.\n\nemerged as a recent methodological focus that allow studying many embodied research questions while reducing some of the development costs associated with modeling complex and photorealistic 3D environments (e.g. C\u00f4t\u00e9 et al., 2018). More than simply reducing development costs, Text Worlds also offer paradigms to study developmental knowledge representation, embodied task learning, and transfer learning at a higher level than perceptuallygrounded studies, enabling different research questions that explore these topics in isolation of the open problems of perceptual input, object segmentation, and object classification regularly studied in the vision community (e.g. He et al., 2016c;Szegedy et al., 2017;Zhai et al., 2021).", "filtered_refids": [[], [null], [null, "b9", "b34", "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1116, "num_references": 5}
{"corpusid_sectionid": "235790370-s3", "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments", "date": "2021-07-08", "section_title": "Why use Text Worlds?", "section": "For many tasks, Text Worlds can offer advantages over other embodied environment modelling paradigms -typically in reduced development costs, the ability to model large action spaces, and the ability to study embodied reasoning at a higher level than raw perceptual information.\n\nEmbodied Reasoning: Embodied agents have been proposed as a solution to the symbol grounding problem (Harnad, 1990), or the problem of how concepts acquire real-world meaning. Humans likely resolve symbol grounding at least partially by assigning semantics to concepts through perceptually-grounded mental simulations (Barsalou et al., 1999). Using embodied agents that take in perceptual data and perform actions in real or virtual environments offers an avenue for studying semantics and symbol grounding empirically (Cangelosi et al., 2010;Bisk et al., 2020;Tamari et al., 2020a,b). Text Worlds abstract some of the challenges in perceptual modeling, allowing agents to focus on higher-level semantics, while hybrid worlds that simultaneously render both text and 3D views (e.g. Shridhar et al., 2020b) help control what kind of knowledge is acquired, and better operationalize the study of symbol grounding.\n\nEase of Development: Constructing embodied virtual environments typically has steep development costs, but Text Worlds are typically easier to construct for many tasks. Creating new objects does not require the expensive process of creating new 3D models, or performing visualpercept-to-object-name segmentation or classification (since the scene is rendered linguistically). Similarly, a rich action semantics is possible, and comparatively easy to implement -while 3D environments typically have one or a small number of action commands (e.g. Kolve et al., 2017;Shridhar et al., 2020a), Text Worlds typically implement dozens of action verbs, and thousands of valid Verb-NounPhrase action combinations (Hausknecht et al., 2020).\n\nCompositional Reasoning: Complex reasoning tasks typically require multi-step (or compositional) reasoning that integrates several pieces of knowledge in an action procedure that arrives at a solution. In the context of natural language, compositional reasoning is frequently studied through question answering tasks (e.g. Yang et al., 2018;Khot et al., 2020;Xie et al., 2020;Dalvi et al., 2021) or procedural knowledge prediction (e.g. Dalvi et al., 2018;Tandon et al., 2018;Dalvi et al., 2019). A contemporary challenge is that the number of valid compositional procedures is typically large compared to those that can be tractably annotated as gold, and as such automatically evaluating model performance becomes challenging (Jansen et al., 2021). In an embodied environment, an agent's actions have (generally) deterministic consequences for a given environment state, as actions are grounded in an underlying action language (e.g. McDermott et al., 1998) or linear logic (e.g. Martens, 2015. Embodied environments can offer a more formal semantics to study these reasoning tasks, where correctness of novel procedures could be evaluated directly.\n\nTransfer Learning: Training a text-only agent for embodied tasks allows the agent to learn those tasks in a distilled form, at a high-level. This performance can then be transferred to more realistic 3D environments, where agents pretrained on text versions of the same environment learn to ground their high-level knowledge in low-level perceptual information, and complete tasks faster than when trained jointly (Shridhar et al., 2020b). This offers the possibility of creating simplified text worlds to pretrain agents for challenging 3D tasks that are currently out of reach of embodied agents.", "filtered_refids": [[], [null, "b7"], [null, "b6", "b27"], ["b14", null, "b22", "b25"], ["b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 3676, "num_references": 10}
{"corpusid_sectionid": "235790370-s4", "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments", "date": "2021-07-08", "section_title": "Text World Simulators", "section": "Text World simulators render an agent's world view directly into textual descriptions of their environment, rather than into 2D or 3D graphical renderings. Similarly, actions the agent wishes to take are provided to the simulator as text (e.g. \"read the letter\" in Zork), requiring agent models to both parse input text from the environment, and generate output text to to interact with that environment.\n\nIn terms of simulators, the Z-machine (Infocom, 1989) is a low-level virtual machine originally designed by Infocom for creating portable interactive fiction novels (such as Zork). It was paired with a high-level LISP-like domain-specific language (ZIL) that included libraries for text parsing, and other tools for writing interactive fiction novels. The Z-machine standard was reverse-engineered by others (e.g. Nelson, 2014) in an effort to build their own high-level interactive fiction domain-specific languages, and has since become a standard compilation target due to the proliferation of existing tooling and legacy environments. 1 Inform7 (Nelson, 2006) is a popular high-level language designed for interactive fiction novels that allows environment rules to be directly specified in a simplified natural language, substantially lowering the barrier to entry for creating text worlds. The text generation engine allows substantial variation in the way the environments are described, from dry formulaic text to more natural, varied, conversational descriptions. Inform7 is compiled to Inform6, an earlier object-oriented scripting language with C-like syntax, which itself is compiled to Z-machine code.\n\nCeptre (Martens, 2015) is a linear-logic simulation engine developed with the goal of specifying more generic tooling for operational logics than Inform 7. TextWorld (C\u00f4t\u00e9 et al., 2018) adapt Ceptre's linear logic state transitions for environment descriptions, and add tooling for generative environments, visualization, and RL agent coupling, all of which is compiled into Inform7 source code. Parallel to this, the Jericho environment (Hausknecht et al., 2020) allows inferring relevant vocabulary and template-based object interactions for Z-machine-based interactive fiction games, easing action selection for agents. 1 A variety of text adventure tooling, including the Adventure Game Toolkit (AGT) and Text Adventure Development System (TADS), was developed starting in the late 1980s, but these simulators have generally not been adopted by the NLP ", "filtered_refids": [[], ["b2", "b3"], ["b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2479, "num_references": 3}
{"corpusid_sectionid": "236460206-s2", "title": "Towards Argument Mining for Social Good: A Survey", "date": 2021, "section_title": "Framework", "section": "Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including N\u00e4ive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).\n\nRelation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.\n\nTo simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.\n\nConsider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question \"Does public health demand vaccinations?\" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.\n\nA2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.\n\nHere, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.\n\nA1: Marvel Universe is better than DC Universe.\n\nA2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.\n\nA3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.\n\nA4: This is especially true due to his unfortunate passing.\n\nA5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.\n\nThe seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).\n\nClearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).", "filtered_refids": [["b26", null, "b28", "b29"], ["b49", "b14"], [null, "b44", "b35", "b31"], [], [], [], [], [], [], [], [], [], ["b44"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 36, "num_chars": 5945, "num_references": 11}
{"corpusid_sectionid": "236460206-s3", "title": "Towards Argument Mining for Social Good: A Survey", "date": 2021, "section_title": "Scaling Up Argument Mining", "section": "In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.\n\nSocial media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.\n\nRecent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.\n\nDespite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.\n\nThe linguistic, structural, and logistic complexity and \"openness\" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.\n\nMultilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.\n\nVarious recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.\n\nOther work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.", "filtered_refids": [[], ["b15"], [null], ["b38"], [], [null], ["b16", "b52", null], ["b62", null, "b38", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 5218, "num_references": 11}
{"corpusid_sectionid": "236460206-s4", "title": "Towards Argument Mining for Social Good: A Survey", "date": 2021, "section_title": "Argument Quality: An Integrated Definition", "section": "The second stage in the framework of AM is defined as relation assignment (c.f. Section 2.1); a complex task that aims to predict the relations holding between the arguments defined in the first stage. Being able to model the relations between arguments and components within the structure, for example in argument graphs (Besnard and Hunter, 2014; Craven and Toni, 2016), allows us to actually work with the argumentative text in an applicationbased setting, understand the stance and context of arguments, and develop a story for the consequential impact of arguments on the discourse, among other things. Generally speaking, we can use this task as an approach to analyze argument quality (AQ). However, within the AM community, an open question concerns the adequate definition and operationalization of the notion of AQ. Despite this, to move forward with the task of AQ analysis and to create large corpora with crowd-sourced annotations, some approaches rely on the relative assessment of quality: Given two arguments, which is more convincing? (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020) Thus the natural way of quantifying the success of an argument is in terms of its persuasiveness. Indeed, plenty of previous work has explored the many factors which contribute to the persuasiveness of a message: the linguistic features employed by the authors (Persing and Ng, 2017), the semantic type of claims and premises (Hidey et al., 2017), the different sources of evidence produced to support an argument (Addawood and Bashir, 2016), the effects of the personality traits and prior beliefs on persuasiveness (Lukin et al., 2017;Durmus and Cardie, 2018;Al Khatib et al., 2020), the interaction with other participants (Ji et al., 2018;Egawa et al., 2020), the use of argument invention when debating about unknown topics (Bilu et al., 2019), the structure of the arguments (Li et al., 2020), and the effect of the style of the text in achieving persuasion (El Baff et al., 2020).\n\nPersuasiveness is, however, not the only way to define whether an argument is good -at least not from a deliberation point of view. A good contribution to a debate is one which uncovers a previously unnoticed aspect of a problem, thus generating a perturbation in the discourse (controversies can be productive!). Or else, a good contribution is one that settles an issue, by stating the differences between opposing views and allowing the discourse to stabilize in a series of clusters (convergence on just one position is not necessarily a good outcome).\n\nMost recent research projects (Wachsmuth et al., 2017b) aim to address the challenge of redefining the notion of AQ, away from persuasiveness and towards a more \"situated\" definition which has to do with the needs of argumentation in a real-world scenario. This new definition has been the basis for the creation of new corpora from different domains , where feature-based (Wachsmuth and Werner, 2020) and neural models were tested for automatic prediction . Other aspects of AQ have become the subject of AM research such as the relevance and impact of arguments (Durmus et al., 2019), the verifiability (Park and Cardie, 2018), local acceptability (Yang et al., 2019) and the best \"deliberative move\" (Al-Khatib et al., 2018).\n\nWe argue that this shift is necessary for two reasons: (1) Working with real-world applications of AM naturally forces us into the more heterogeneous realm of data structures, such as social media, in which language, structure, and content are less uniform and confined to the classic notion of logical debate; and (2) In order to encourage deliberation from an open audience of citizens, we need to redefine our concept of AQ and productive discourse such that there is equal worth and participation granted to each contributor of the argument.\n\nDeliberative Quality We therefore propose adapting the definition of quality to integrate the abundant research on the topic from the field of Social Sciences. Here, the quality of a discourse has been investigated in the context of deliberation with the focus on inclusivity: how can the interplay of the different participants in the discourse lead to an optimal outcome for the collective? The focus here is not on the quality of the individual contributions. Instead, an overall quality of the discourse is determined by the fact that the individual quality dimensions are distributed among different contributions (e.g some participants do more rational reasoning, others share personal experiences). We would like to integrate those aspects that focus on inclusivity and cooperation.\n\nSimilar to Wachsmuth et al. (2017b), social scientists have developed a taxonomy, the discourse quality index (DQI), that describes the different desirable aspects of a discourse (Steenbergen et al., 2003). This taxonomy has been used to analyze the quality of deliberation in different contexts, ranging from more formal contexts, such as parliamentary debates (Steiner et al., 2005), to informal discussions in online forums (Tr\u00e9nel, 2004). Both implementations integrate logical coherence as one dimension, cogency in Wachsmuth et al. (2017b), justification in the DQI. Some aspects of inclusivity are also being touched upon in the rhetorical and dialectical dimension of Wachsmuth et al. (2017b), such as using appropriate language (Appropriateness) or whether an argument supports conflict resolution (global relevance). We concentrate on the following dimensions from the DQI, which particularly focus on the collaborative aspect of discourse.\n\n\u2022 Respect: this dimension includes respectful tone, respect for other social groups/backgrounds, and openness towards other opinions.\n\n\u2022 Equality / Participation: it is not desirable that some dominant participants make the bulk of contributions while many others remain passive. All participants should have equal opportunities to contribute and all topics, including those that DQI (Steenbergen et al., 2003)  \u2022 Interactivity: beyond simply sharing opinions, acknowledging other viewpoints and interacting with other participants through listening and responding lead to new perspectives arising -compromises can emerge.\n\n\u2022 Testimoniality / Report of personal accounts:\n\nsharing stories and personal narratives as an alternative form of communication can involve more people in the discourse, especially those who cannot identify themselves with rational argumentation. It can also make other participants aware of other perspectives as it generally increases empathy. Especially when traditional or universal norms need to be questioned, narratives are particularly well suited, as their ambiguity and vagueness creates room for interpretation. This is particularly important when new ideas or perspectives are introduced, since they cannot yet be rationally articulated. Table 1 establishes a direct comparison between discourse quality dimensions of the DQI (Steenbergen et al., 2003;Steiner et al., 2005) and argument quality dimensions as defined in Wachsmuth et al. (2017b). Apart from the potential theoretical insights, the existing guidelines can be applied to annotate new or enrich existing corpora for AM. Despite the small size, the data already annotated based on the DQI can be made usable and extended for NLP. In addition, some of the quality dimensions can be further quantified or approximated using statistical methods. For example, interactivity or equality can be assessed with frequency-based methods, such as frequency of posts by distinct participants and response rate.\n\nSumming up The overview of the definitions of AQ along with the discussion of the potential of the integration of Deliberative Quality features into an AM framework has one strong take-home message:\n\nThe need for the scope of the investigation to go beyond (a) the persuasiveness of a an argumentative text (speeches, forum posts, tweets), and (b) their relation to the immediate preceding discourse. Instead, we pointed out the need to also assess the potential of the impact of that argumentative text on the upcoming discourse: this dimension of quality, inherently related to the interpretation of argumentation as a cooperation challenge, is currently lacking in current approaches to AQ.", "filtered_refids": [["b6", "b47", "b9", "b27", null, "b23", "b36"], [], ["b56", "b55", null, "b33", "b61"], [], [], ["b45", "b46", "b50", "b55"], [], ["b45"], [], ["b45", "b46", "b55"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 41, "num_chars": 8280, "num_references": 20}
{"corpusid_sectionid": "236460241-s1", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies", "date": "2023-01-05", "section_title": "Competing models of C-S", "section": "For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.\n\n2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).\n\n3. I love Horlicks maar hier\u015b niks 'I love Horlicks but there's nothing there '\n\nAlthough it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).\n\nFor many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.\n\n3 Why do speakers code-switch?\n\nIn addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Mysl\u00edn and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojal\u00e1 or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).\n\nAccording to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.", "filtered_refids": [[], ["b43", "b71", null, "b16", "b3"], [], [null, "b37", "b32"], ["b57", "b31", "b32", null, "b53", "b33"], [], ["b57", "b44", "b9", "b82", "b42"], ["b52", "b10", "b79", "b30", null, "b7", "b65", "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 37, "num_chars": 5847, "num_references": 27}
{"corpusid_sectionid": "236460241-s2", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies", "date": "2023-01-05", "section_title": "Code-switching, Borrowing, Transfer, Loan Translation", "section": "While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogru\u00f6z and Backus, 2009). 4.\u0130lkokul-u\u0130stanbul-da yap-t\u0131-m.\n\nprimary.school-ACC\u0130stanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'\n\nIn transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher apr\u00e8s 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.", "filtered_refids": [["b58", null, "b64", "b2", "b37"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2206, "num_references": 5}
{"corpusid_sectionid": "236460241-s3", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies", "date": "2023-01-05", "section_title": "C-S across Languages: European Context", "section": "The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).\n\nWithin an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, J\u00f8rgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.\n\nC-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.\n\nC-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, O\u017ca\u0144ska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and \u00c7 etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogru\u00f6z (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.\n\nIn addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.\n\nWithin the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.\n\nSimilar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.", "filtered_refids": [["b11", "b54", "b39", "b56", "b40", "b29", null, "b17"], [], ["b69", "b75", "b28"], ["b51", "b12", "b52", "b46", "b80", null, "b23", "b24", "b1", "b38"], [], ["b83", "b35"], ["b81"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 33, "num_chars": 5583, "num_references": 24}
{"corpusid_sectionid": "11250379-s1", "title": "A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin", "date": "1996-08-01", "section_title": "Continuous Speech Recognition of Mandarin", "section": "There have been many attempts to find so called distinctive features of speech (e.g. [Fant 1973]) which are invariant to a number of factors. Certain distinctive (phonetic) features, such as nasality and voicing, can be used to represent the place and manner of articulation of speech sounds so that speech can be uniquely identified by detecting the acoustic-phonetic properties of the signal. By organizing such knowledge in a systematic manner, speech recognition can (in theory) be performed by first identifying and labeling the sequence of feature vectors and then identifying the corresponding sounds in the speech signal, followed by decoding the corresponding sequence of words using lexical access to a dictionary of words. This has been demonstrated in spectrogram reading by a human expert who can visually segment and identify some speech sounds based on knowledge of acoustic-phonetics of English. Although the collection of distinctive features, in theory, offers a set of invariant features for speech recognition, it is not generally used in most speech recognitions systems. This is due to the fact that the set of distinctive features are usually difficult to identify in spontaneous continuous speech and the recognition results are generally unreliable.\n\nA more successful approach to automatic speech recognition is to treat the speech signal as a stochastic pattern and to adopt a statistical pattern recognition approach. For this approach we assume a source-channel speech generation model (e.g. [Bahl et al. 1983]) shown in Figure  1, in which the source produces a sequence of words, W. Because of uncertainty and inaccuracy in converting from words to speech, we model the conversion from W to an observed speech waveform, S, as a noisy channel. Speech recognition is then formulated as a maximum a posteriori (MAP) decoding problem, as shown in Figure 1. Instead of working with the speech signal S directly, one way to simplify the problem is to assume that S is first parametrically represented as a sequence of acoustic vectors A. We then use the Bayes rule to reformulate the decoding problemas follows, arg max ( | ) arg max ( | ) ( )\n\nwhere \u0393 is the set of all possible sequences of words, P(A|W)is the conditional probability of the acoustic vector sequence, A, given a particular sequence of words W, and P (W) is the a priori probability of generating the sequence of words W. The first term, P (A|W), is often referred to as an acoustic model, and the second term, P (W), is known as a language model. The noisy channel in Figure 1 is a model jointly characterizing the speech production system, the speaker variability, the speaking environment, and the transmission medium. Since it is not feasible to have a complete knowledge about such a noisy channel, the statistical approach often assumes particular parametric forms for P\u03b8 (A|W) and P\u03c9 (W), i.e. according to specific models. All the parameters of the statistical models (i.e. \u03b8 and \u03c9 ) needed in evaluating the acoustic probability, P\u03b8 (W|A), and the language probability, P\u03c9(W), are usually estimated from a large collection (the so-called training set) of speech and text training data. This process is often referred to as model training or learning. We will discuss this important issue later in the paper.\n\nThere is some recent attempt trying to separate the speech production part from the source-channel model by incorporating knowledge about the human speech production mechanism. Knowledge about the transducers used for capturing speech and the channel used for transmitting speech can also be explicitly modeled. However, the effectiveness of such approaches is yet to be shown.", "filtered_refids": [[null], ["b2"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3688, "num_references": 3}
{"corpusid_sectionid": "11250379-s6", "title": "A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin", "date": "1996-08-01", "section_title": "Speech Analysis and Feature Extraction", "section": "The purpose of the feature analysis module is to parametrize the speech into a parsimonious sequence of feature vectors that contain the relevant (for recognition) information about the sounds within the utterance. Although there is no consensus as to what constitutes the optimal feature analysis, most systems extract spectral features with the following properties: having good discrimination to readily distinguish between similar speech sounds, being easy to model statistically without the need for an excessive amount of training data, and having statistical properties which are somewhat invariant across speakers and over a wide range of speaking environments. To our knowledge there is no single feature set that possesses all the above properties. The features used in speech recognition systems are largely derived from their utility in speech analysis, speech coding, and psycho-acoustics.\n\nFourier analysis is still the most widely used method for extracting spectral features for speech recognition. Implementations of feature extraction include:\n\n\u2022 Language Model features along with its first and second time derivatives (e.g. [Furui 1986]). \u2022 Frequency-Warped Spectral Features: Sometimes non-uniform frequency scales are used in spectral analysis to provide the so-called mel-frequency or bark-scale spectral feature sets (e.g. [Davis andMermelstein 1980;Junqua et al. 1993]). The motivation is to mimic the human auditory system which processes the spectral information on a non-uniform frequency scale.\n\nAlthough many research directions are being pursued to create new feature sets for ASR, most promising ones include: (1) extraction of segment features to allow variable-length speech analysis to incorporate discriminative information from neighboring frames; (2) extraction of articulatory and auditory features to make use of the human speech production and perception mechanism (e.g. [Deng and Sun 1994;Ghitza 1988;Seneff 1988]); (3) extraction of discriminative features which are functions of the data, the task, and the classifiers being used (e.g. [Ney et al. 1992;Biem et al. 1993;Rahim and Lee 1996]).", "filtered_refids": [[], [], ["b24", "b42", null], ["b63", "b10", "b29", "b19", "b84", "b73"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 2135, "num_references": 9}
{"corpusid_sectionid": "11250379-s8", "title": "A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin", "date": "1996-08-01", "section_title": "Continuous Speech Recognition of Mandarin", "section": "\u2022 Units Other than Phones: Units smaller than a phone, such as phone-labeled acoustic states, have been used to reduce the number of states needed to represent the set of speech units. Larger units, including diphones, demisyllables, syllables, whole-words and even phrases, have all been used to better characterize coarticulation between adjacent sounds. Acoustic segment units have also been investigated [Lee 1988].\n\n\u2022 Units with Linguistic Context Dependency: Different ways of incorporting linguistic context in a speech subword unit, such as double context dependent phones (often known as triphones) and generalized triphones, have been proposed (e.g. [Lee 1989]). It has been shown that the recognition accuracy of a task can be increased when linguistic context dependency is properly incorporated to reduce the acoustic variability of the speech units being modeled. In fluent continuous speech it has also been shown that incorporation of interword units takes into account cross-word coarticulation and therefore provides more accurate modeling of speech units than simply using intraword context-dependent units. Word-dependent units have also been used to model poorly articulated speech sounds such as function words like a, the, in, and, etc. (e.g. [Lee 1989]).\n\nFor a given task, high recognition accuracy can be achieved only when the subword unit set contains context-dependent phones which maximally covers the vocabulary and the task language and when these phone units are adequately modeled using a large training set [Hon 1992]. However, the collection of a large amount of task-specific training data for every individual application is not practical. Task and vocabulary independent acoustic training and task-specific vocabulary learning (e.g. [Hon 1992;Lee et al.1996]) are therefore important research topics. Task-independent modeling has also been applied to word spotting for training of acoustic models and rejection models [Rose and Hofstetter 1993;Sukkar and Lee 1996]. However, we do not yet know how to design a task-independent training database suitable for a wide range of vocabularies and applications.", "filtered_refids": [["b45"], ["b51"], ["b77", null, "b33", "b87"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 2144, "num_references": 6}
{"corpusid_sectionid": "264833196-s11", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Transformer encoders", "section": "A transformer encoder is a length-preserving function T : \u03a3 * \u2192 (R  ) * parameterized by the weights of an input layer  and  transformer layers L 1 , . . ., L  .A transformer encoder with postnorm layers is:\n\nwhere each L  is a post-norm layer (1) and \u2022 denotes function composition.A transformer encoder with pre-norm layers is additionally parameterized by the weights of a layernorm N and is defined as:\n\nwhere each L  is a pre-norm layer (2).The encoder's output is a sequence of vectors in (R  ) * .To use it as a language recognizer, we add an output layer that converts T () to a probability\n\nwhere w \u2208 R  ,  \u2208 R, and  is a distinguished position.The encoder accepts if p \u2265 1 2 and rejects if p < 1 2 .Chiang and Cholak (2022) also consider a requirement that an encoder accepts/rejects strings with bounded cross-entropy.That is, we say that an encoder recognizes a language  with crossentropy at most  iff for all strings  \u2208 , we have \u2212 log p \u2264 , and for all strings  \u2209 , we have \u2212 log(1 \u2212 p) \u2264 .\n\nWe are aware of two choices for the distinguished position .Most papers use the last position ( =  \u2212 1) but some (Chiang and Cholak, 2022;Chiang et al., 2023), inspired by binary classifiers based on BERT (Devlin et al., 2019), prepend a special symbol CLS at position 0 and use  = 0.", "filtered_refids": [[], [], [], [], ["b12", "b13", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1291, "num_references": 3}
{"corpusid_sectionid": "264833196-s12", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Transformer decoders", "section": "A transformer decoder generates rather than recognizes strings.The input is the prefix of previouslygenerated symbols,\n\nwhere W \u2208 R |\u03a3| \u00d7 and b \u2208 R |\u03a3| .We assume  0 = BOS and every string ends with EOS, where BOS and EOS are special symbols that do not occur anywhere else.To sample a string, we first sample  1 from p( 1 | BOS), then, for each time step  > 1, sample   from p(  |  < ).The process stops when   = EOS.Because each sampled output symbol becomes part of the input at the next time step, this kind of model is called autoregressive.\n\nTwo different ways have been proposed for defining whether a transformer decoder generates a (weighted) language.\n\nFirst, Hahn (2020) considers a weighted language as a distribution over strings ().For any length , the KL divergence (relative entropy) of the model p() from the true distribution (), for predicting   conditioned on all previous words, is\n\n.\n\nAs Hahn's results are negative, he does not spell out a positive criterion, but he seems to implicitly require that this divergence vanish at infinity:\n\nSecond, let us say that a transformer decoder -generates  iff\n\nThen Yao et al. (2021), following Hewitt et al. (2020), say that a transformer decoder  generates a language  iff there exists an  > 0 such that  -generates .(This means that a transformer decoder may generate more than one language, depending on the  chosen.)They also show that any -generator can be converted into a recognizer.\n\nWhile not specifically focusing on transformers, Lin et al. (2021) demonstrate limitations of autoregressive models for generation; for example, they show that there is a language  \u2208 P that cannot be -generated in polynomial time for any  > 0 if P \u2260 NP.", "filtered_refids": [[], [], [], [], [], [], [], ["b74", "b26"], ["b34"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1708, "num_references": 3}
{"corpusid_sectionid": "264833196-s13", "title": "Transformers as Recognizers of Formal Languages: A Survey on Expressivity", "date": "2023-11-01", "section_title": "Transformer encoder-decoders", "section": "A transformer encoder-decoder combines a transformer encoder and decoder, adding to each layer of the decoder an additional attention sublayer, known as cross attention, which attends to the output of the encoder.\n\nIn the literature surveyed here, only the construction of P\u00e9rez et al. (2021) and related constructions (Bhattamishra et al., 2020b;Wei et al., 2022) employ an encoder-decoder architecture.In these constructions, a string  is fed to the encoder, and the decoder is allowed to run for an arbitrary number of steps.Then  is accepted iff the decoder eventually outputs a vector belonging to a fixed set of accept vectors.\n\nAs we will see ( \u00a77.2.1), this setup vastly increases the model's power.It could be likened to a language model that is allowed to \"think step by step\" (Kojima et al., 2022) before generating a final accept decision.", "filtered_refids": [[], ["b54", "b9", "b71"], ["b32"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 851, "num_references": 4}
{"corpusid_sectionid": "265607947-s7", "title": "Teach Me How to Argue: A Survey on NLP Feedback Systems in Argumentation", "date": 2023, "section_title": "Pedagogy", "section": "Before discussing the four dimensions mentioned a priori, it is essential to know the pedagogy used to teach argumentation and adopted by computational models.This section presents some standard pedagogical methods used in teaching how to argue.\n\nToulmin model The Toulmin model (Toulmin, 1958), often seen as the foundation of teaching argumentation, is a popular framework for constructing, analyzing and evaluating arguments, and can contribute to the improvement of students' argumentative writing (Rex et al., 2010;Yeh, 1998) as well as critical thinking skills (Giri and Paily, 2020).This approach deconstructs an argument into six elements (Appendix, Figure 4), and students are taught to identify each element within an argument.\n\nBy identifying elements from the Toulmin model, models can provide users with rich feedback.\n\nRhetorical structure theory Based on Mann and Thompson (1988), the rhetorical structure theory was originally developed in the context of computer-based text generation in order to attribute a formal structure to a text (Hou et al., 2020).This theory employs graphical representations, such as mind maps or graphs, to illustrate the relationships between different components of the text's architecture.This visual approach can help students visualize the connections between different concepts and enhance their understanding of complex topics (Matsumura and Sakamoto, 2021).The advent of tools like Tiara (Putra et al., 2020) has given rise to the deployment of the rhetorical structure theory, i.e. the generation of visual feedback.\n\nCollaborative argumentation In collaborative argumentation-based learning, also described as CABLE by Baker et al. (2019), individuals work together to construct, refine, and evaluate arguments on a particular topic or issue.The main goal of collaborative argumentation is to foster constructive dialogue, critical thinking, and the exploration of different perspectives.Weinberger and Fischer (2006) differentiate four dimensions of CABLE:\n\n\u2022 Participation: Do learners participate at all? Do they participate on an equal basis?\n\n\u2022 Epistemic: Are learners engaging in activities to solve the task (on-task discourse) or rather concerned with off-task aspect?\n\n\u2022 Argumentative: Are learners following the structural composition of arguments and their sequences?\n\n\u2022 Social: To what extent do learners refer to the contributions of their learning partners?Are they gaining knowledge by asking questions?Veerman et al. (2002); Baker et al. (2019) show CABLE's positive effects on students' argumentation development.Nevertheless, they also highlight the challenges of this method, as not every dialogue can be predicted.By using CABLE, models can generate interactive feedback.", "filtered_refids": [[], ["b100", "b23", "b82", "b71"], [], ["b59", "b68", "b33", "b58"], ["b98", "b7"], [], [], [], ["b86", "b7"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2745, "num_references": 12}
{"corpusid_sectionid": "265607947-s9", "title": "Teach Me How to Argue: A Survey on NLP Feedback Systems in Argumentation", "date": 2023, "section_title": "Richness -What is an Error and Why?", "section": "To improve students' critical thinking skills, we first need to evaluate their argumentative texts, i.e., identify argumentative errors.In this section, we focus on models providing shallow explanations, i.e., models that identify what should be corrected in the arguments.We discuss relevant works that identify properties such as the structure of arguments which is helpful in this process.\n\nComponents Identifying argumentative components is one of the fundamental tasks in argumentation (Teufel, 1999;Stab and Gurevych, 2014;Jo et al., 2020).Such works primarily focus on identifying components such as claims and premises.More recently, the usefulness of identifying such components can be seen in tasks such as counterargument generation.For example, in Alshomary et al. (2021), weak premises are identified and ranked to generate counter-arguments.\n\nRelations After identifying the different components of an argumentative text, it is necessary to distinguish the multiple relations between them, ultimately to assert the arguments' quality.Indeed, supporting or refuting a claim is made of complex logical moves, such as promoting, contradicting, or acknowledging a fact.To identify the different relations patterns, Yuan et al. (2021) focus on finding interactive argument pairs, whereas Mim et al. ( 2022) enables annotating complex attack relations.\n\nSchemes In addition to components and relations, Walton et al. (2008) proposed a set of roughly 80 logical argumentation schemes to categorize the underlying logic.Each scheme has a set of critical questions which provide a template to assess the strength of the argument depending upon the associated scheme.Since the first work on automatically detecting argumentation schemes in argumentative texts (Feng and Hirst, 2011), the use of such schemes has been explored in tasks such as essay scoring (Song et al., 2014).\n\nFallacies Although a good structure with a claim and premises is necessary for a good argument, it is not sufficient.An argument has more complex properties, such as its logical, dialectical, and rhetorical aspects.A fallacy is a logical error or deceptive argument that undermines the validity of a conclusion or reasoning, which poses a substantial issue due to its propensity to generate miscommunication.Towards teaching students to avoid making errors in logical reasoning, logical fallacies have received attention (Habernal et al., 2017;Bonial et al., 2022;Zhivar et al., 2023;Nakpih and Santini, 2020).Motivated by the gamification method made by Habernal et al. (2017), Bonial et al. (2022) aimed to capture similar fallacy types for news articles, but the low distribution of fallacy types in the wild makes identification challenging.However, most natural texts do not have recurrent specific patterns, compared to current datasets, like the Logic and LogicClimate datasets (Jin et al., 2022).Moreover, given the large number of logical fallacies that exist (over 100 types), long arguments can be grouped into multiple fallacies, resulting in difficulties in classification (Goffredo et al., 2022).", "filtered_refids": [[], ["b80", "b81", "b41", "b5"], ["b101"], ["b21", "b89", "b79"], ["b40", "b13", "b104", "b27", "b62", "b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3093, "num_references": 14}
{"corpusid_sectionid": "265607947-s10", "title": "Teach Me How to Argue: A Survey on NLP Feedback Systems in Argumentation", "date": 2023, "section_title": "Debate patterns", "section": "In a case of a debate, an opponent is willing to give a counter-argument synchronously and interactively.Analyzing and evaluating a debate is a difficult task as we need to retrieve not only the argumentation structure of each opponent but also the relations between them.Bao et al. (2022) focuses on argument pair extraction (APE), which consists of finding two interactive arguments from two argumentative passages of a discussion.Although the APE task gives insights into relations between different argumentative texts, it does not indicate complex relations (i.e., how claims, supports, attacks and the intention of the speakers are interrelated).To palliate this issue, Hautli-Janisz et al. ( 2022) identified and analyzed the dialogical argumentative structure of debates using Inference Anchoring Theory (IAT) (Budsziyska et al., 2014).Following the same IAT theory, Kik-teva et al. (2022) showed that the type of questions (e.g., pure, assertive, and rhetorical questions) leads to different argumentative discourse.Focused more on the opponent's side, Naito et al. (2022) propose diagnostic comments for assessing the quality of counter-arguments by providing expressive, informative and unique templates.The feedback is then written by template selection and slot filling.\n\nIn-Depth Explanations Although identifying such argumentative structures (components, relations, and schemes) and properties (fallacies and debate patterns) is important, it has limitations in terms of effective feedback.Identifying a missing claim or a wrong premise is insufficient to understand how to improve the argumentation properly.Thus, we relate the identification of structure and properties to shallow explanations in the sense that users can still benefit from the output of the models.\n\nShallow explanations can be difficult to understand, especially for beginners, as they tend to be minimalist and lack guidance.To explain more effectively the errors in an argument, a model should go a step further, hence by providing in-depth explanations, which attempt to identify the argument's implicit components to explain why it is an error in a particular argument.In Figure 2, we implicitly know that hamburgers belong to the American cuisine, as same as the Cobb salad, a healthy garden salad from California.Therefore, if the model is able to reason out this implicit knowledge, it can better explain the invalid generalization in Figure 2.\n\nImplicit Knowledge and Reasoning in Arguments To provide in-depth explanations, we need to know how to refine the argument, i.e., how to identify implicit information.Recently, many works have focused their attention on this aim.The main goal of such studies is to make the structure and reasoning of arguments explicit to explain the arguments for humans better.Additionally, this focus can eventually help build robust argumentation machines that can be enriched with language understanding capacity.Following the pioneer works of Razuvayevskaya and Teufel (2017), the ExpLAIN project (Becker et al., 2021) and Jo et al. (2021) are one such example that focuses extensively on reconstructing implicit knowledge in arguments by relying on knowledge graphs among others.Taking a step further in this direction, Heinisch et al. (2022) and Saadat-Yazdi et al. (2023) proposed to utilize such implicit information to bridge the im-plicit reasoning gap in arguments to help students explain their arguments better.\n\nLarge annotated corpora are required to improve implicit reasoning detection for models.To address this need, various studies have proposed methods for annotating implicit knowledge, leading to the development of multiple datasets (Becker et al., 2020;Singh et al., 2021Singh et al., , 2022)).In Singh et al. (2021), semi-structured warrants, i.e. links between a claim and evidence (c.f.Appendix Figure 4), were annotated via crowdsourcing, whereas Becker et al. (2020) focus on reconstructing omitted information, semantic clause types, and commonsense knowledge relations through expert annotation.Corpora can be dedicated to a specific domain or sentence patterns.For example, (Singh et al., 2022) focused on domain-specific knowledge using six topics.However, implicit knowledge may take various forms, such as warrants, causal relations, facts, beliefs, or assumed-known arguments.Thus, revealing implicit knowledge in an unknown text through annotated datasets can be challenging.\n\nIn recent years, LLMs have made significant progress in exhibiting reasoning abilities.A comprehensive overview of the current state of reasoning abilities in LLMs is provided in the survey Huang and Chang (2023).The increasing interest in LLMs and implicit reasoning prompted the first ever workshop on natural language reasoning and structured explanations in 2023 (Dalvi Mishra et al., 2023).This workshop discussed that while LLMs have demonstrated good capabilities to find implicit components within an argument, they often cannot correctly explain the logical reasons behind their responses.To bridge this gap, a novel category of explanation techniques has arisen, playing a vital role in shaping the logical reasoning of models.One such example is the chain-of-thought prompting (Wei et al., 2022;Wang et al., 2023a), which employs explanations as a means for LLMs to emulate human reasoning procedures.While the references Huang and Chang (2023) and Dalvi Mishra et al. ( 2023) do not primarily focus on argumentative tasks, they can be a valuable source of inspiration in argumentation.", "filtered_refids": [["b14", null, "b8", "b61"], [], [], ["b10", "b31", null, "b72", "b42"], ["b78", "b77", "b9"], [null, "b97", "b94", "b35"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 29, "num_chars": 5538, "num_references": 16}
