corpusid_sectionid,title,date,section_title,section,question,answer,question_rephrased,answer_rephrased,quesiton_standalone,quesiton_ansvar,quesiton_rephrased_standalone,quesiton_rephrased_ansvar,QA_pair,QA_pair_rephrased,question_has_citation,passed_question_citation_criteria,rephrased_answer_length,passed_answer_length_criteria,passed_all_qa_criteria
8729907-s1,The Language ENvironment Analysis (LENA) System: A Literature Review,2016-11-15,The LENA System,"The LENA System's hardware includes a digital language processor (DLP) that can audio record for up to 16 hours. It measures 3-3/8"" x 2-3/16"" x 1/2"", weighs less than two ounces, and consists of a display screen, a USB port for uploading, and two buttons for powering and recording. The processor is held in a specially designed t-shirt or vest with a pocket on the front to secure the device. The audio quality is a 16-bit channel at a 16kHz sample rate (Ford, Baer, Xu, Yapanel, & Gray, 2008). Once the recording is complete it can be uploaded to the LENA software. Recordings are stored in the software by participant, allowing repeated recordings of one participant to be saved and compared over time. Once uploaded and recharged, the same participant or a new participant can use the DLP again without affecting the data stored in the software. The LENA System automatically segments the recordings into 12 categories including speakers, environmental sounds, and silence using Gaussian mixture models. A daylong audio file typically consists of 20,000 to 50,000 segments (VanDam et al., 2016). The software then estimates: adult word count (AWC), child vocalization count (CVC), and conversational turn count (CTC). The amount of background noise, electronic sounds, meaningful speech, and silence that were part of the child's listening environment are reported as percentages of the total sound present in the day and are displayed in user-friendly LENA generated graphs along with the AWC, CVC, and CTC. Additional details can be extracted using ADEX software provided by the LENA Foundation (Ford, et al., 2008;VanDam, Ambrose, & Moeller, 2012).

In addition to the raw data counts, Richards, Gilkerson, Paul, & Xu (2008) discuss the Automatic Vocalization Assessment (AVA) generated by the LENA System, which is correlated with traditional expressive language standard scores including those from the Preschool Language Scale -4 th Edition (PLS-4) (Zimmerman, Steiner, & Pond, 2002) and the Receptive-Expressive Emergent Language Test -3 rd Edition (REEL-3) (Bzoch, League, & Brown, 2003). To learn more about the LENA hardware and software, consult Ford et al. (2008) and .

In order to establish reliability, human transcribers coded 70 full day English recordings and their results were compared with those obtained by the automated software (Xu, Yapanel, Gray, & Baer, 2008). This data was collected as part of the Natural Language Study (NLS), the LENA Foundation's normative study . The LENA System correctly identified 82 and 76 percent of the segments humans coded as adult speech and child vocalizations respectively, indicating reasonable levels of agreement Warren et al., 2010;Xu et al., 2008;). Validity has also been shown in Spanish, French, Mandarin, Korean, and Vietnamese (Canault, Le Normand, Foudil, Loundon, & Thai- Van, 2015;Ganek & Eriks-Brophy, in revision;Gilkerson et al., 2015;Pae et al., 2016;Weisleder & Fernald, 2013). Although these studies show high fidelity, recording in a child's natural environment can produce a degraded auditory signal that may negatively impact validation. Possible causes of interference might include environmental factors such as background noise, overlapping speech, and reverberation, speaker variation like pitch or voice quality, and hardware variability. Although LENA clothing has been rigorously tested, fabric sound absorption rates may also impact accuracy ).",How does the LENA System process audio recordings to estimate language exposure and interaction metrics?,"The LENA System's hardware includes a digital language processor (DLP) that can audio record for up to 16 hours. Once the recording is complete, it can be uploaded to the LENA software. The LENA System automatically segments the recordings into 12 categories including speakers, environmental sounds, and silence using Gaussian mixture models. A daylong audio file typically consists of 20,000 to 50,000 segments (VanDam et al., 2016). The software then estimates: adult word count (AWC), child vocalization count (CVC), and conversational turn count (CTC). The amount of background noise, electronic sounds, meaningful speech, and silence that were part of the child's listening environment are reported as percentages of the total sound present in the day and are displayed in user-friendly LENA generated graphs along with the AWC, CVC, and CTC. Additional details can be extracted using ADEX software provided by the LENA Foundation (Ford, et al., 2008; VanDam, Ambrose, & Moeller, 2012).",How does the LENA System analyze audio recordings to determine language exposure and interaction metrics?,"The LENA System utilizes a digital language processor (DLP) capable of recording audio for up to 16 hours. After completing a recording session, the audio data is uploaded to the LENA software for analysis. The system employs Gaussian mixture models to automatically segment the recordings into 12 distinct categories, such as speakers, environmental sounds, and silence. A typical daylong audio file is divided into approximately 20,000 to 50,000 segments (VanDam et al., 2016). The software then calculates key metrics, including adult word count (AWC), child vocalization count (CVC), and conversational turn count (CTC). Additionally, it reports the proportions of background noise, electronic sounds, meaningful speech, and silence within the child's listening environment as percentages of the total sound recorded throughout the day. These metrics are presented in user-friendly graphs generated by LENA, alongside the AWC, CVC, and CTC. For further analysis, additional details can be extracted using the ADEX software provided by the LENA Foundation (Ford et al., 2008; VanDam, Ambrose, & Moeller, 2012).",7,2,7,2,"Question:

How does the LENA System process audio recordings to estimate language exposure and interaction metrics?

Answer:

The LENA System's hardware includes a digital language processor (DLP) that can audio record for up to 16 hours. Once the recording is complete, it can be uploaded to the LENA software. The LENA System automatically segments the recordings into 12 categories including speakers, environmental sounds, and silence using Gaussian mixture models. A daylong audio file typically consists of 20,000 to 50,000 segments (VanDam et al., 2016). The software then estimates: adult word count (AWC), child vocalization count (CVC), and conversational turn count (CTC). The amount of background noise, electronic sounds, meaningful speech, and silence that were part of the child's listening environment are reported as percentages of the total sound present in the day and are displayed in user-friendly LENA generated graphs along with the AWC, CVC, and CTC. Additional details can be extracted using ADEX software provided by the LENA Foundation (Ford, et al., 2008; VanDam, Ambrose, & Moeller, 2012).","Question:

How does the LENA System analyze audio recordings to determine language exposure and interaction metrics?

Answer:

The LENA System utilizes a digital language processor (DLP) capable of recording audio for up to 16 hours. After completing a recording session, the audio data is uploaded to the LENA software for analysis. The system employs Gaussian mixture models to automatically segment the recordings into 12 distinct categories, such as speakers, environmental sounds, and silence. A typical daylong audio file is divided into approximately 20,000 to 50,000 segments (VanDam et al., 2016). The software then calculates key metrics, including adult word count (AWC), child vocalization count (CVC), and conversational turn count (CTC). Additionally, it reports the proportions of background noise, electronic sounds, meaningful speech, and silence within the child's listening environment as percentages of the total sound recorded throughout the day. These metrics are presented in user-friendly graphs generated by LENA, alongside the AWC, CVC, and CTC. For further analysis, additional details can be extracted using the ADEX software provided by the LENA Foundation (Ford et al., 2008; VanDam, Ambrose, & Moeller, 2012).",NO,True,1113,True,True
263153015-s2,"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future",2023-09-27,Background,"In recent years, with the continuous expansion of computing power, large-scale language models have sprung up (Brown et al., 2020;OpenAI, 2023;Touvron et al., 2023a;Scao et al., 2022;Touvron et al., 2023b;Zhao et al., 2023b), and as the model size continues to grow, many new capabilities have emerged, such as in-context learning and chain-ofthought reasoning (Brown et al., 2020;Wei et al., 2022b,a;Schaeffer et al., 2023).Brown et al. (2020) finds that large-scale language models have excellent in-context learning (ICL) ability.ICL incorporates input-output demonstrations into the prompt text.With ICL, off-the-shelf LLMs can be employed without additional fine-tuning while achieving comparable performance.Nevertheless, this end-to-end approach tends to underperform when faced with complex reasoning tasks.Wei et al. (2022b) finds that the reasoning ability of LLMs can be improved by adding step-by-step reasoning processes to the demonstration, which is known as chain-of-thought prompting.CoT prompting enables the model to gain a more precise understanding of both the question's intricacies and the reasoning process.Furthermore, the model generates a sequence of reasoning steps, which grants us a transparent view of the model's cognitive process, further enhancing interpretability.",How does chain-of-thought prompting enhance the reasoning ability and interpretability of large language models?,"Chain-of-thought prompting enhances the reasoning ability of large language models by adding step-by-step reasoning processes to the demonstration, as found by Wei et al. (2022b). This approach enables the model to gain a more precise understanding of both the question's intricacies and the reasoning process. Furthermore, the model generates a sequence of reasoning steps, which grants a transparent view of the model's cognitive process, further enhancing interpretability.",In what ways does chain-of-thought prompting improve the reasoning capabilities and interpretability of large language models?,"Chain-of-thought prompting improves the reasoning capabilities of large language models by incorporating step-by-step reasoning processes into the demonstration, as highlighted by Wei et al. (2022b). This method allows the model to develop a more precise understanding of the complexities of the question and the associated reasoning process. Additionally, by generating a sequence of reasoning steps, the model provides a transparent view of its cognitive process, thereby enhancing interpretability.",8,4,8,7,"Question:

How does chain-of-thought prompting enhance the reasoning ability and interpretability of large language models?

Answer:

Chain-of-thought prompting enhances the reasoning ability of large language models by adding step-by-step reasoning processes to the demonstration, as found by Wei et al. (2022b). This approach enables the model to gain a more precise understanding of both the question's intricacies and the reasoning process. Furthermore, the model generates a sequence of reasoning steps, which grants a transparent view of the model's cognitive process, further enhancing interpretability.","Question:

In what ways does chain-of-thought prompting improve the reasoning capabilities and interpretability of large language models?

Answer:

Chain-of-thought prompting improves the reasoning capabilities of large language models by incorporating step-by-step reasoning processes into the demonstration, as highlighted by Wei et al. (2022b). This method allows the model to develop a more precise understanding of the complexities of the question and the associated reasoning process. Additionally, by generating a sequence of reasoning steps, the model provides a transparent view of its cognitive process, thereby enhancing interpretability.",NO,True,501,True,True
251307817-s8,Rhetorical Structure Approach for Online Deception Detection: A Survey,2022,Karimi and Tang (2019),"Discourse-level structure analysis of deceptive and truthful news is a tremendous challenge, mainly due to existing methods for capturing discourse-level structure rely on annotated corpora, which are not available for fake news datasets (Karimi and Tang, 2019). In this proposal, the authors provide a new dependency parsing approach, titled ""Hierarchical Discourse-level Structure for Fake news detection"". The HDSF consists of an automated manner to learn a discourselevel structure for a given document through an approach based on the dependency parsing at the sentence level. It should be noted that in this approach, sentences are classified as elementary discourse units (EDU's). An example of discourse-level structure of a document (fake news) using the proposed dependency tree is shown in Figure 5. Note that a document is segmented into sentences (S1, S2, S3, S4 and S5), and hierarchically organized. Figure 5: Hierarchical discourse-level structure of a document using a dependency tree. This fake news was extracted from Politifact.

The HDSF framework build a hierarchical structure between sentences without relying on an annotated corpus, as may be seen in Figure 6. Note that the HDSF receives as input a corpus of fake/real news documents (i.e., D). A model M may automatically learn hierarchical and structurally rich representations for documents in D. Meanwhile, given binary labels Y, model M uses the hierarchical representations to automatically predict the labels of unseen news documents. In order to compare the HDSF approach with baseline and state-of-art models, the authors implemented seven different models including the proposed methods: Ngrams, LIWC (Pennebaker et al., 2015), Bag-of-rst (Rubin and Lukoianova, 2015), BiGRNN-CNN (Ren and Zhang, 2016), LSTM and LSTM[w+s] (Karimi and Tang, 2019). Based on the obtained results, the HDSF overcame the other implemented approaches (82.19% of Accuracy). They concluded that discourselevel structure analysis is effectively rich for fake news prediction. In addition, the structures of fake news documents at the discourse level are substantially different from those of true ones, and real news documents indicate more degree of textual coherence.",How do the structures of fake news documents differ from true ones at the discourse level according to the authors?,The structures of fake news documents at the discourse level are substantially different from those of true ones. Real news documents indicate a greater degree of textual coherence.,How do fake news documents differ from true ones at the discourse level?,"At the discourse level, fake news documents differ significantly from true ones. True news documents exhibit a greater degree of textual coherence compared to fake news.",7,4,7,7,"Question:

How do the structures of fake news documents differ from true ones at the discourse level according to the authors?

Answer:

The structures of fake news documents at the discourse level are substantially different from those of true ones. Real news documents indicate a greater degree of textual coherence.","Question:

How do fake news documents differ from true ones at the discourse level?

Answer:

At the discourse level, fake news documents differ significantly from true ones. True news documents exhibit a greater degree of textual coherence compared to fake news.",NO,True,169,False,False
251307817-s12,Rhetorical Structure Approach for Online Deception Detection: A Survey,2022,Discourse-Aware Deception Detection: Main Challenges,"Although the RST has been applied to a wide variety of successful applications, we should not simply see it without any criticism. For instance, there are several vague statements and definitions described in the RST original proposal. Indeed, it has been criticized by various authors mainly concerning the aspects related to the absence of a minimal text unit's granularity. Furthermore, rhetorical relations are also highly ambiguous. Since the author have suggested that a level of ambiguity is completely natural between the relations, there are not any instructions or enough scientific and methodology elements to address the relations ambiguity. According to Schauer and Hahn (2000), the number and nature of the rhetorical relations are faintly defined. For example, could any researcher propose and use a set of coherence relations that suits her purposes, and would be them really rhetorical relations? In spite of the disapproval from various authors, a couple of authors proposed to address these ""open questions"". For example, Maier and Hovy (1993) suggested a taxonomy of three different levels: ideational, interpersonal, and textual in order to group rhetorical relations. In the same setting, Stede et al. (2017), Carlson and Marcu (2001), Vargas et al. (2021) have proposed new rhetorical relations and updated the RST framework. Lastly, another relevant challenge consists of lack of RST-annotated corpus for the deception detection tasks, and low-performance of RST parsers.",What are the main criticisms of the RST original proposal and how have they been addressed by researchers?,"There are several vague statements and definitions described in the RST original proposal. It has been criticized by various authors mainly concerning the aspects related to the absence of a minimal text unit's granularity. Furthermore, rhetorical relations are also highly ambiguous. Since the author have suggested that a level of ambiguity is completely natural between the relations, there are not any instructions or enough scientific and methodology elements to address the relations ambiguity. According to Schauer and Hahn (2000), the number and nature of the rhetorical relations are faintly defined. For example, could any researcher propose and use a set of coherence relations that suits her purposes, and would be them really rhetorical relations? In spite of the disapproval from various authors, a couple of authors proposed to address these ""open questions"". For example, Maier and Hovy (1993) suggested a taxonomy of three different levels: ideational, interpersonal, and textual in order to group rhetorical relations. In the same setting, Stede et al. (2017), Carlson and Marcu (2001), Vargas et al. (2021) have proposed new rhetorical relations and updated the RST framework.","What are the main criticisms of the original RST proposal, and how have researchers addressed these issues?","The original Rhetorical Structure Theory (RST) proposal has faced several criticisms, primarily due to its vague statements and definitions. One major concern is the lack of clarity regarding the granularity of minimal text units. Additionally, the rhetorical relations within RST are often seen as highly ambiguous. The original authors suggested that some level of ambiguity is natural between relations, but they did not provide sufficient instructions or scientific methodology to address this ambiguity. Schauer and Hahn (2000) pointed out that the number and nature of rhetorical relations are not clearly defined, raising questions about whether researchers can propose and use a set of coherence relations that truly qualify as rhetorical relations.

Despite these criticisms, some researchers have attempted to address these ""open questions."" For instance, Maier and Hovy (1993) proposed a taxonomy with three different levels—ideational, interpersonal, and textual—to better categorize rhetorical relations. Similarly, Stede et al. (2017), Carlson and Marcu (2001), and Vargas et al. (2021) have introduced new rhetorical relations and updated the RST framework to enhance its clarity and applicability.",7,4,7,4,"Question:

What are the main criticisms of the RST original proposal and how have they been addressed by researchers?

Answer:

There are several vague statements and definitions described in the RST original proposal. It has been criticized by various authors mainly concerning the aspects related to the absence of a minimal text unit's granularity. Furthermore, rhetorical relations are also highly ambiguous. Since the author have suggested that a level of ambiguity is completely natural between the relations, there are not any instructions or enough scientific and methodology elements to address the relations ambiguity. According to Schauer and Hahn (2000), the number and nature of the rhetorical relations are faintly defined. For example, could any researcher propose and use a set of coherence relations that suits her purposes, and would be them really rhetorical relations? In spite of the disapproval from various authors, a couple of authors proposed to address these ""open questions"". For example, Maier and Hovy (1993) suggested a taxonomy of three different levels: ideational, interpersonal, and textual in order to group rhetorical relations. In the same setting, Stede et al. (2017), Carlson and Marcu (2001), Vargas et al. (2021) have proposed new rhetorical relations and updated the RST framework.","Question:

What are the main criticisms of the original RST proposal, and how have researchers addressed these issues?

Answer:

The original Rhetorical Structure Theory (RST) proposal has faced several criticisms, primarily due to its vague statements and definitions. One major concern is the lack of clarity regarding the granularity of minimal text units. Additionally, the rhetorical relations within RST are often seen as highly ambiguous. The original authors suggested that some level of ambiguity is natural between relations, but they did not provide sufficient instructions or scientific methodology to address this ambiguity. Schauer and Hahn (2000) pointed out that the number and nature of rhetorical relations are not clearly defined, raising questions about whether researchers can propose and use a set of coherence relations that truly qualify as rhetorical relations.

Despite these criticisms, some researchers have attempted to address these ""open questions."" For instance, Maier and Hovy (1993) proposed a taxonomy with three different levels—ideational, interpersonal, and textual—to better categorize rhetorical relations. Similarly, Stede et al. (2017), Carlson and Marcu (2001), and Vargas et al. (2021) have introduced new rhetorical relations and updated the RST framework to enhance its clarity and applicability.",NO,True,1213,True,True
252461144-s4,A Survey of Machine Translation Tasks on Nigerian Languages,2022,Machine Translation Techniques,"Machine translation has been extensively studied for decades (Bahdanau et al., 2016;Luong et al., 2015;Koehn et al., 2007b) with neural machine translation providing the most recent state-of-the art results. There are three types of machine translation techniques that have been explored in the literature -Rule-based machine translation (RBMT), Statistical machine translation (SMT), and Neural machine translation (NMT). A high level overview of these techniques are outlined as follows:

Rule-based Approach: This is one of the oldest form of machine translation technique used. This approach is based on understanding the linguistic properties of the source and target languages using dictionaries and expert knowledge to define grammar rules. This process involves morphology analysis, syntax, and lexical semantics. Linguistic analysis is performed on the source language to identify morphology, parts of speech, phrases, named entity, and word disambiguation. Each word is replaced in the target language using a dictionary which represents mappings between source and target words. In order to preserve sentence semantics across translated languages, most RBMT approach utilizes a combination of finite state machines to develop their knowledge graphs (Forcada et al., 2011;Scott and Barreiro, 2009). (Forcada et al., 2011) utilizes finite-state transducers for lexical processing, Hidden Markov models for part-ofspeech tagging, and multi-stage finite-state chunking for structural transfer. (Eisele et al., 2008) utilizes a modified phrase table with entries from translating various data with rule-based systems. One of the main advantage of this approach is that it does not require as much parallel sentence pairs as with most NMT approaches. Also, translation errors can be corrected by updating the dictionary. This allows for flexibility in updating language constructs. Consequently, one major drawback of this approach is that the translation quality is mostly defined by the strength of the dictionary which requires frequent updates from domain experts. RBMT also tends to produce translations that are more repetitive and less fluid which can be attributed to its mechanical approach of using rules for translation.

Statistical-based Approach: This approach involves the use of statistical techniques such as probability distribution models to provide a means for machine translation between source and target languages. This is achieved by assigning a probability score to word or phrase contained in every target sentence where words or phrases with the highest probability contains the best translation for the target sentence (Koehn et al., 2007b;Brown et al., 1993). SMT can be applied at a word or phrase level and consists of a translation and language model. The translation model is defined as the probability that the source sentence is the translated version of the target word. The language model tries to describe how representative the target sentence is to the natural spoken language. It assigns probabilities to sentence similar to the sentence ordering. One approach utilized in developing the probability distributions is the use of Bayes theorem (Zens et al., 2002) and Hidden Markov Model (Deng and Byrne, 2008;Alkhouli et al., 2016). (Koehn et al., 2007a) developed Moses, an open-source machine translation toolkit which utilizes linguistic information that captures semantics in mapping text phrases and a confusion network decoding for translating ambiguous text inputs.

One advantage of SMT approach over RBMT is the improved translation quality. It allows for translation that captures not just linguistic morphology but the use of a probability distribution which improves with semantic quality.

Neural-based Approach: This approach is referred to as the state of the art in machine translation as it is widely used and has shown to provide results with higher accuracy as compared to the other approaches (Bahdanau et al., 2016;Luong et al., 2015;Cho et al., 2014). Neural machine translation involves the use of deep learning techniques to provide a means of inferring high level semantics from language translations. A popular neural machine translation approach (Vaswani et al., 2017) utilize transformer based models with encoder-decoder architecture. These models consists of stacks of multiple hidden layers with multi-head attention mechanisms and have been shown (Vaswani et al., 2017) to outperform traditional neural architecture such as Recurrent Neural Networks for machine translation task.

Current implementation for language models consists of multilingual language model embeddings (Pires et al., 2019;Lample and Conneau, 2019) where one language model is trained on multiple languages. This allows for zero-shot transfer learning where cross language representation is learned without the need for a parallel language corpus across all language pairs. This has been shown to produce better results than monolingual model training (Conneau et al., 2020) especially for low-resource languages. Supervised neural approach relies heavily on a large corpus of quality translated sentence pairs; as such this poses a limitation to the quality of language translation. There are some approaches that work well with limited datasets (Mikolov et al., 2013;Artetxe et al., 2018) and can provide a means of translating from one language to another based on translations derived from a similar language ( ","How does the Rule-based Approach in machine translation function, and what are its advantages and drawbacks?","The Rule-based Approach in machine translation is one of the oldest forms of machine translation techniques. This approach is based on understanding the linguistic properties of the source and target languages using dictionaries and expert knowledge to define grammar rules. The process involves morphology analysis, syntax, and lexical semantics. Linguistic analysis is performed on the source language to identify morphology, parts of speech, phrases, named entity, and word disambiguation. Each word is replaced in the target language using a dictionary which represents mappings between source and target words. In order to preserve sentence semantics across translated languages, most RBMT approaches utilize a combination of finite state machines to develop their knowledge graphs (Forcada et al., 2011; Scott and Barreiro, 2009). One of the main advantages of this approach is that it does not require as much parallel sentence pairs as with most NMT approaches. Also, translation errors can be corrected by updating the dictionary, allowing for flexibility in updating language constructs. Consequently, one major drawback of this approach is that the translation quality is mostly defined by the strength of the dictionary, which requires frequent updates from domain experts. RBMT also tends to produce translations that are more repetitive and less fluid, which can be attributed to its mechanical approach of using rules for translation.","What is the Rule-based Approach in machine translation, and what are its benefits and limitations?","The Rule-based Approach in machine translation is one of the earliest techniques developed for translating languages. It relies on a deep understanding of the linguistic properties of both the source and target languages, utilizing dictionaries and expert knowledge to establish grammar rules. The process involves several steps, including morphology analysis, syntax, and lexical semantics. Initially, linguistic analysis is conducted on the source language to identify elements such as morphology, parts of speech, phrases, named entities, and word disambiguation. Each word is then translated into the target language using a dictionary that maps source words to their corresponding target words. To maintain the semantic integrity of sentences across languages, most Rule-based Machine Translation (RBMT) systems employ a combination of finite state machines to construct their knowledge graphs (Forcada et al., 2011; Scott and Barreiro, 2009).

One significant advantage of this approach is that it does not require as many parallel sentence pairs as most Neural Machine Translation (NMT) approaches do. Additionally, translation errors can be rectified by updating the dictionary, providing flexibility in adapting to new language constructs. However, a major limitation of this approach is that the quality of translation heavily depends on the robustness of the dictionary, which necessitates frequent updates from domain experts. Furthermore, RBMT often results in translations that are more repetitive and less fluid, a consequence of its mechanical rule-based methodology.",9,2,9,2,"Question:

How does the Rule-based Approach in machine translation function, and what are its advantages and drawbacks?

Answer:

The Rule-based Approach in machine translation is one of the oldest forms of machine translation techniques. This approach is based on understanding the linguistic properties of the source and target languages using dictionaries and expert knowledge to define grammar rules. The process involves morphology analysis, syntax, and lexical semantics. Linguistic analysis is performed on the source language to identify morphology, parts of speech, phrases, named entity, and word disambiguation. Each word is replaced in the target language using a dictionary which represents mappings between source and target words. In order to preserve sentence semantics across translated languages, most RBMT approaches utilize a combination of finite state machines to develop their knowledge graphs (Forcada et al., 2011; Scott and Barreiro, 2009). One of the main advantages of this approach is that it does not require as much parallel sentence pairs as with most NMT approaches. Also, translation errors can be corrected by updating the dictionary, allowing for flexibility in updating language constructs. Consequently, one major drawback of this approach is that the translation quality is mostly defined by the strength of the dictionary, which requires frequent updates from domain experts. RBMT also tends to produce translations that are more repetitive and less fluid, which can be attributed to its mechanical approach of using rules for translation.","Question:

What is the Rule-based Approach in machine translation, and what are its benefits and limitations?

Answer:

The Rule-based Approach in machine translation is one of the earliest techniques developed for translating languages. It relies on a deep understanding of the linguistic properties of both the source and target languages, utilizing dictionaries and expert knowledge to establish grammar rules. The process involves several steps, including morphology analysis, syntax, and lexical semantics. Initially, linguistic analysis is conducted on the source language to identify elements such as morphology, parts of speech, phrases, named entities, and word disambiguation. Each word is then translated into the target language using a dictionary that maps source words to their corresponding target words. To maintain the semantic integrity of sentences across languages, most Rule-based Machine Translation (RBMT) systems employ a combination of finite state machines to construct their knowledge graphs (Forcada et al., 2011; Scott and Barreiro, 2009).

One significant advantage of this approach is that it does not require as many parallel sentence pairs as most Neural Machine Translation (NMT) approaches do. Additionally, translation errors can be rectified by updating the dictionary, providing flexibility in adapting to new language constructs. However, a major limitation of this approach is that the quality of translation heavily depends on the robustness of the dictionary, which necessitates frequent updates from domain experts. Furthermore, RBMT often results in translations that are more repetitive and less fluid, a consequence of its mechanical rule-based methodology.",NO,True,1583,True,True
246017121-s16,Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset,2022-01-07,Implementation Details,"Data pre-processing. We implement spectral augmentation (SpecAugment), a state-of-the-art audio data augmentation method, which is implemented by masking certain frequency and time values on the spectrogram (Park et al., 2019). We use SpecAugment for the Common Voice zh-HK baseline, where it shows an improvement in overall results. Furthermore, we apply cepstral mean and variance normalisation (CMVN) for all the utterances (Strand and Egeberg, 2004). In Fairseq S2T, pre-processed audio can be used directly or stored in the form of .npy files. The latter is the way in which we store features extracted from Cantonese datasets to achieve faster training. For tokenization of the transcribed data, we use the SentencePiece tokenizer (Kudo and Richardson, 2018) with unigram subword tokenization (Kudo, 2018) and an 8,000-word vocabulary. The vocabulary covers 99.95% of the characters in the MDCC (the default coverage for character-based languages).",How is data pre-processed and tokenized in the study of Cantonese automatic speech recognition datasets?,"Data pre-processing involves implementing spectral augmentation (SpecAugment), which is an audio data augmentation method that masks certain frequency and time values on the spectrogram (Park et al., 2019). SpecAugment is used for the Common Voice zh-HK baseline, where it shows an improvement in overall results. Cepstral mean and variance normalization (CMVN) is applied for all the utterances (Strand and Egeberg, 2004). In Fairseq S2T, pre-processed audio can be used directly or stored in the form of .npy files, which is the method used to store features extracted from Cantonese datasets to achieve faster training. For tokenization of the transcribed data, the SentencePiece tokenizer (Kudo and Richardson, 2018) with unigram subword tokenization (Kudo, 2018) and an 8,000-word vocabulary is used. The vocabulary covers 99.95% of the characters in the MDCC, which is the default coverage for character-based languages.",How is data pre-processed and tokenized in Cantonese automatic speech recognition datasets?,"In the study of Cantonese automatic speech recognition datasets, data pre-processing begins with the implementation of spectral augmentation, known as SpecAugment (Park et al., 2019). This method enhances audio data by masking specific frequency and time values on the spectrogram, which has been shown to improve results in the Common Voice zh-HK baseline. Additionally, cepstral mean and variance normalization (CMVN) is applied to all utterances to standardize the audio data (Strand and Egeberg, 2004). In the Fairseq S2T framework, pre-processed audio can be utilized directly or stored as .npy files, which facilitates faster training by storing features extracted from Cantonese datasets.

For the tokenization of transcribed data, the SentencePiece tokenizer is employed (Kudo and Richardson, 2018). It uses unigram subword tokenization (Kudo, 2018) with an 8,000-word vocabulary, which effectively covers 99.95% of the characters in the MDCC, ensuring comprehensive coverage for character-based languages.",8,2,7,2,"Question:

How is data pre-processed and tokenized in the study of Cantonese automatic speech recognition datasets?

Answer:

Data pre-processing involves implementing spectral augmentation (SpecAugment), which is an audio data augmentation method that masks certain frequency and time values on the spectrogram (Park et al., 2019). SpecAugment is used for the Common Voice zh-HK baseline, where it shows an improvement in overall results. Cepstral mean and variance normalization (CMVN) is applied for all the utterances (Strand and Egeberg, 2004). In Fairseq S2T, pre-processed audio can be used directly or stored in the form of .npy files, which is the method used to store features extracted from Cantonese datasets to achieve faster training. For tokenization of the transcribed data, the SentencePiece tokenizer (Kudo and Richardson, 2018) with unigram subword tokenization (Kudo, 2018) and an 8,000-word vocabulary is used. The vocabulary covers 99.95% of the characters in the MDCC, which is the default coverage for character-based languages.","Question:

How is data pre-processed and tokenized in Cantonese automatic speech recognition datasets?

Answer:

In the study of Cantonese automatic speech recognition datasets, data pre-processing begins with the implementation of spectral augmentation, known as SpecAugment (Park et al., 2019). This method enhances audio data by masking specific frequency and time values on the spectrogram, which has been shown to improve results in the Common Voice zh-HK baseline. Additionally, cepstral mean and variance normalization (CMVN) is applied to all utterances to standardize the audio data (Strand and Egeberg, 2004). In the Fairseq S2T framework, pre-processed audio can be utilized directly or stored as .npy files, which facilitates faster training by storing features extracted from Cantonese datasets.

For the tokenization of transcribed data, the SentencePiece tokenizer is employed (Kudo and Richardson, 2018). It uses unigram subword tokenization (Kudo, 2018) with an 8,000-word vocabulary, which effectively covers 99.95% of the characters in the MDCC, ensuring comprehensive coverage for character-based languages.",NO,True,1014,True,True
14642384-s3,DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL,2004-07-01,Descriptions serving as disguised PROPER,"NAMES, such as The Federal Communications Commission or the Iran-Iraq war. The heuristics for recognizing these definite descriptions were primarily based on capitalization (of the head or the modifiers).

3. PREDICATIVE descriptions, i.e., descriptions semantically functioning as predicates rather than as referring. These include descriptions occurring in appositive position (as in Glenn Cox, the president of Phillips Petroleum) and in certain copular constructions (as in the man most likely to gain custody of all this is a career politician named Dinkins). The heuristics used to recognize these cases examined the syntactic structure of the NP and the clause in which it appeared.

4. Descriptions ESTABLISHED (i.e., turned into functions in context) by restrictive modification, particularly by establishing relative clauses (Loebner, 1987) and prepositional phrases, as in The hotel where we stayed last night was pretty good.

These heuristics, as well, examined the syntactic structure of the NP. 5. LARGER SITUATION definite descriptions (Hawkins, 1978), i.e., definite descriptions like the sun, the pope or the long distance market which denote uniquely on the grounds of shared knowledge about the situation (these are Loebner's 'situational functions'). Vieira and Poesio's system had a small list of such definites.

These heuristics were included as tests both of a decision tree concerned only with the task of DN detection, and of decision trees determining the classification of DDs as anaphoric, bridging or discourse new. In both cases, the DN detection tests were intertwined with attempts to identify an antecedent for such DDs. Both hand-coded decision trees and automatically acquired ones (trained using ID3, (Quin-lan, 1986)) were used for the task of two-way classification into discourse-new and anaphoric. Vieira and Poesio found only small differences in the order of tests in the two decision trees, and small differences in performance. The hand-coded decision tree executes in the following order:

1. Try the DN heuristics with the highest accuracy (recognition of some types of semantically functional DDs using special predicates, and of potentially predicative DDs occurring in appositions);

2. Otherwise, attempt to resolve the DD as direct anaphora;

3. Otherwise, attempt the remaining DN heuristics in the order: proper names, descriptions established by relatives and PPs, proper name modification, predicative DDs occurring in copular constructions.

If none of these tests succeeds, the algorithm can either leave the DD unclassified, or classify it as DN. The automatically learned decision tree attempts direct anaphora resolution first. The overall results on the 195 DDs on which the automatically trained decision tree was tested are shown in  ",How are decision trees utilized for classifying definite descriptions as discourse-new or anaphoric?,"Decision trees are used for the task of two-way classification into discourse-new and anaphoric. Both hand-coded decision trees and automatically acquired ones (trained using ID3, (Quinlan, 1986)) are employed for this classification task. Vieira and Poesio found only small differences in the order of tests in the two decision trees, and small differences in performance. The hand-coded decision tree executes in the following order: 1. Try the DN heuristics with the highest accuracy (recognition of some types of semantically functional DDs using special predicates, and of potentially predicative DDs occurring in appositions). 2. Otherwise, attempt to resolve the DD as direct anaphora. 3. Otherwise, attempt the remaining DN heuristics in the order: proper names, descriptions established by relatives and PPs, proper name modification, predicative DDs occurring in copular constructions. If none of these tests succeeds, the algorithm can either leave the DD unclassified, or classify it as DN. The automatically learned decision tree attempts direct anaphora resolution first.",How are decision trees used to classify definite descriptions as either discourse-new or anaphoric?,"Decision trees are employed for the two-way classification of definite descriptions into discourse-new and anaphoric categories. This task utilizes both hand-coded decision trees and those automatically acquired through training, such as using the ID3 algorithm (Quinlan, 1986). Vieira and Poesio observed only minor differences in the sequence of tests and performance between the two types of decision trees. The hand-coded decision tree follows a specific order: it first applies the discourse-new (DN) heuristics with the highest accuracy, which includes recognizing certain types of semantically functional definite descriptions using special predicates and identifying potentially predicative definite descriptions in appositions. If these do not apply, the tree attempts to resolve the definite description as direct anaphora. If this also fails, it proceeds with the remaining DN heuristics in the following order: proper names, descriptions established by relatives and prepositional phrases, proper name modification, and predicative definite descriptions in copular constructions. If none of these tests succeed, the algorithm may either leave the definite description unclassified or classify it as discourse-new. In contrast, the automatically learned decision tree prioritizes direct anaphora resolution first.",7,2,7,2,"Question:

How are decision trees utilized for classifying definite descriptions as discourse-new or anaphoric?

Answer:

Decision trees are used for the task of two-way classification into discourse-new and anaphoric. Both hand-coded decision trees and automatically acquired ones (trained using ID3, (Quinlan, 1986)) are employed for this classification task. Vieira and Poesio found only small differences in the order of tests in the two decision trees, and small differences in performance. The hand-coded decision tree executes in the following order: 1. Try the DN heuristics with the highest accuracy (recognition of some types of semantically functional DDs using special predicates, and of potentially predicative DDs occurring in appositions). 2. Otherwise, attempt to resolve the DD as direct anaphora. 3. Otherwise, attempt the remaining DN heuristics in the order: proper names, descriptions established by relatives and PPs, proper name modification, predicative DDs occurring in copular constructions. If none of these tests succeeds, the algorithm can either leave the DD unclassified, or classify it as DN. The automatically learned decision tree attempts direct anaphora resolution first.","Question:

How are decision trees used to classify definite descriptions as either discourse-new or anaphoric?

Answer:

Decision trees are employed for the two-way classification of definite descriptions into discourse-new and anaphoric categories. This task utilizes both hand-coded decision trees and those automatically acquired through training, such as using the ID3 algorithm (Quinlan, 1986). Vieira and Poesio observed only minor differences in the sequence of tests and performance between the two types of decision trees. The hand-coded decision tree follows a specific order: it first applies the discourse-new (DN) heuristics with the highest accuracy, which includes recognizing certain types of semantically functional definite descriptions using special predicates and identifying potentially predicative definite descriptions in appositions. If these do not apply, the tree attempts to resolve the definite description as direct anaphora. If this also fails, it proceeds with the remaining DN heuristics in the following order: proper names, descriptions established by relatives and prepositional phrases, proper name modification, and predicative definite descriptions in copular constructions. If none of these tests succeed, the algorithm may either leave the definite description unclassified or classify it as discourse-new. In contrast, the automatically learned decision tree prioritizes direct anaphora resolution first.",NO,True,1324,True,True
237353268-s8,Neuron-level Interpretation of Deep NLP Models: A Survey,2021-08-30,Causation-based methods,"The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.

Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.

Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

Limitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.","How do causation-based methods, such as ablation, identify and evaluate the importance of neurons in deep NLP models?","Causation-based methods identify neurons with respect to a model's prediction. The central idea behind ablation is to notice the effect of a neuron on the model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observing the change in the network's performance. Ablation has been effectively used to find salient neurons with respect to a model (unsupervised) and salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in the model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which the salient neurons are identified.",How do causation-based methods like ablation determine and assess the significance of neurons in deep NLP models?,"Causation-based methods, such as ablation, are used to determine the significance of neurons in relation to a model's predictions. The core concept of ablation involves altering the value of a neuron to observe its impact on the model's performance. This is typically achieved by setting the neuron's value to zero or a fixed value and then monitoring any changes in the network's performance. Ablation has been successfully employed to identify salient neurons in two contexts: unsupervised and supervised. In the unsupervised context, neurons that cause a significant drop in the model's performance when ablated are identified (Li et al., 2016a). In the supervised context, neurons that lead the model to change its prediction for a specific output class are selected (Lakretz et al., 2019). In this scenario, the output class acts as the concept against which the salient neurons are identified.",7,2,7,4,"Question:

How do causation-based methods, such as ablation, identify and evaluate the importance of neurons in deep NLP models?

Answer:

Causation-based methods identify neurons with respect to a model's prediction. The central idea behind ablation is to notice the effect of a neuron on the model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observing the change in the network's performance. Ablation has been effectively used to find salient neurons with respect to a model (unsupervised) and salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in the model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which the salient neurons are identified.","Question:

How do causation-based methods like ablation determine and assess the significance of neurons in deep NLP models?

Answer:

Causation-based methods, such as ablation, are used to determine the significance of neurons in relation to a model's predictions. The core concept of ablation involves altering the value of a neuron to observe its impact on the model's performance. This is typically achieved by setting the neuron's value to zero or a fixed value and then monitoring any changes in the network's performance. Ablation has been successfully employed to identify salient neurons in two contexts: unsupervised and supervised. In the unsupervised context, neurons that cause a significant drop in the model's performance when ablated are identified (Li et al., 2016a). In the supervised context, neurons that lead the model to change its prediction for a specific output class are selected (Lakretz et al., 2019). In this scenario, the output class acts as the concept against which the salient neurons are identified.",NO,True,899,True,True
237353268-s18,Neuron-level Interpretation of Deep NLP Models: A Survey,2021-08-30,Lexical Concepts,"Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.

Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","How have researchers identified neurons capturing lexical concepts in deep NLP models, and what methods were used?","Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts. Visualizations by Karpathy et al. (2015) found neurons that learn the position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle, and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment, such as in the phrases ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly, they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task, such as electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly, Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g., ""law, legal"" etc., and also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation, providing finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","How have researchers identified neurons that capture lexical concepts in deep NLP models, and what methods have they used?","Researchers have employed various methods to identify neurons that capture lexical concepts in deep NLP models. Visualization techniques, as demonstrated by Karpathy et al. (2015), revealed neurons that learn the position of a word within an input sentence, showing positive activation at the beginning, neutral in the middle, and negative towards the end. Li et al. (2016a) discovered intensification neurons that activate for words enhancing sentiment, such as in ""I like this movie a lot"" or ""the movie is incredibly good,"" and also identified neurons capturing ""negation."" These neurons are particularly relevant for sentiment classification tasks. Kádár et al. (2017) identified neurons capturing related concept groups in a multi-modal image captioning task, such as electronic items like ""camera, laptop, cables"" and salad items like ""broccoli, noodles, carrots."" Similarly, Na et al. (2019) found neurons learning lexical concepts related to legislative terms, such as ""law, legal,"" and neurons learning phrasal concepts. Poerner et al. (2018) enhanced Concept Search through Corpus Generation, providing a finer interpretation of neurons by generating synthetic instances. For instance, they demonstrated that a ""horse racing"" neuron identified via concept search was actually a general ""racing"" neuron by generating novel contexts against it.",7,4,8,4,"Question:

How have researchers identified neurons capturing lexical concepts in deep NLP models, and what methods were used?

Answer:

Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts. Visualizations by Karpathy et al. (2015) found neurons that learn the position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle, and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment, such as in the phrases ""I like this movie a lot"" or ""the movie is incredibly good"". Similarly, they discovered neurons that captured ""negation"". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task, such as electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"". Similarly, Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g., ""law, legal"" etc., and also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation, providing finer interpretation of the neurons by generating synthetic instances. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","Question:

How have researchers identified neurons that capture lexical concepts in deep NLP models, and what methods have they used?

Answer:

Researchers have employed various methods to identify neurons that capture lexical concepts in deep NLP models. Visualization techniques, as demonstrated by Karpathy et al. (2015), revealed neurons that learn the position of a word within an input sentence, showing positive activation at the beginning, neutral in the middle, and negative towards the end. Li et al. (2016a) discovered intensification neurons that activate for words enhancing sentiment, such as in ""I like this movie a lot"" or ""the movie is incredibly good,"" and also identified neurons capturing ""negation."" These neurons are particularly relevant for sentiment classification tasks. Kádár et al. (2017) identified neurons capturing related concept groups in a multi-modal image captioning task, such as electronic items like ""camera, laptop, cables"" and salad items like ""broccoli, noodles, carrots."" Similarly, Na et al. (2019) found neurons learning lexical concepts related to legislative terms, such as ""law, legal,"" and neurons learning phrasal concepts. Poerner et al. (2018) enhanced Concept Search through Corpus Generation, providing a finer interpretation of neurons by generating synthetic instances. For instance, they demonstrated that a ""horse racing"" neuron identified via concept search was actually a general ""racing"" neuron by generating novel contexts against it.",NO,True,1352,True,True
263835243-s4,How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances,2023-10-11,Continual Learning,"Continual learning (CL) aims to enable a model to learn from a continuous data stream across time while reducing catastrophic forgetting of previously acquired knowledge (Biesialska et al., 2020). With CL, a deployed LLM has the potential to adapt to the changing world without costly re-training from scratch (Bubeck et al., 2023). In this section, we introduce approaches that employ CL for aligning LLMs with the current world knowledge, including continual pre-training and continual knowledge editing.

Continual Pre-training. Unlike traditional continual learning, which sequentially fine-tunes a pre-trained LM on some specific downstream tasks (e.g., QA, text classification), continual pretraining is used to further pre-train an LM to acquire new knowledge, where the data corpus is usually unsupervised (Gururangan et al., 2020;Ke and Liu, 2023). Since our target is the versatile foundation LLMs (e.g., GPT-4) that can be applied to many different use cases rather than a fine-tuned model designed for a specific task, we focus on the literature on continual pre-training.

Early works (Gururangan et al., 2020;Röttger and Pierrehumbert, 2021;Lazaridou et al., 2021;Dhingra et al., 2022)  1 Regularization. To mitigate forgetting, regularization-based methods apply regulations to penalize the changes of the critical parameters learned from previous data. Chen et al. (2020) improve the traditional EWC (Kirkpatrick et al., 2017) by recalling previously learned knowledge through the pre-trained parameters, and the method continually learns new information using a multitask learning objective.  compute the importance of each unit (i.e., attention head and neuron) to the general knowledge in the LM using a proxy based on model robustness to preserve learned knowledge. When continually learning new domains, the approach prevents catastrophic forgetting of the general and domain knowledge and encourages knowledge transfer via soft-masking and contrastive loss.

2 Replay. These methods generally reduce forgetting by replaying previous training data when learning new data. Assuming that the initial pretraining corpus is available, He et al. (2021b) use a gradual decay mix-ratio to adjust the quantity of the pre-training corpus mixed in the new data when learning sequentially. ELLE  and CT0 (Scialom et al., 2022) also mix the old data while learning new data. However, ELLE starts the pre-training from a newly initialized and relatively small BERT ( ",How do regularization-based methods and replay methods address the challenge of catastrophic forgetting in continual learning?,"Regularization-based methods apply regulations to penalize the changes of the critical parameters learned from previous data. Chen et al. (2020) improve the traditional EWC (Kirkpatrick et al., 2017) by recalling previously learned knowledge through the pre-trained parameters, and the method continually learns new information using a multitask learning objective. These methods compute the importance of each unit (i.e., attention head and neuron) to the general knowledge in the LM using a proxy based on model robustness to preserve learned knowledge. When continually learning new domains, the approach prevents catastrophic forgetting of the general and domain knowledge and encourages knowledge transfer via soft-masking and contrastive loss. Replay methods generally reduce forgetting by replaying previous training data when learning new data. Assuming that the initial pretraining corpus is available, He et al. (2021b) use a gradual decay mix-ratio to adjust the quantity of the pre-training corpus mixed in the new data when learning sequentially. ELLE and CT0 (Scialom et al., 2022) also mix the old data while learning new data.",How do regularization-based methods and replay methods help mitigate catastrophic forgetting in continual learning?,"Regularization-based methods address catastrophic forgetting by applying penalties to changes in critical parameters learned from previous data. For instance, Chen et al. (2020) enhance the traditional Elastic Weight Consolidation (EWC) approach (Kirkpatrick et al., 2017) by recalling previously acquired knowledge through pre-trained parameters. This method facilitates continual learning of new information using a multitask learning objective. It computes the importance of each unit, such as attention heads and neurons, to the general knowledge in the language model (LM) using a proxy based on model robustness. This helps preserve learned knowledge, prevents catastrophic forgetting of both general and domain-specific knowledge, and encourages knowledge transfer through techniques like soft-masking and contrastive loss.

On the other hand, replay methods mitigate forgetting by replaying previous training data while learning new data. Assuming the initial pretraining corpus is available, He et al. (2021b) employ a gradual decay mix-ratio to adjust the amount of pre-training corpus mixed with new data during sequential learning. Similarly, methods like ELLE and CT0 (Scialom et al., 2022) incorporate old data while learning new data, thereby reducing the risk of forgetting previously learned information.",7,4,7,4,"Question:

How do regularization-based methods and replay methods address the challenge of catastrophic forgetting in continual learning?

Answer:

Regularization-based methods apply regulations to penalize the changes of the critical parameters learned from previous data. Chen et al. (2020) improve the traditional EWC (Kirkpatrick et al., 2017) by recalling previously learned knowledge through the pre-trained parameters, and the method continually learns new information using a multitask learning objective. These methods compute the importance of each unit (i.e., attention head and neuron) to the general knowledge in the LM using a proxy based on model robustness to preserve learned knowledge. When continually learning new domains, the approach prevents catastrophic forgetting of the general and domain knowledge and encourages knowledge transfer via soft-masking and contrastive loss. Replay methods generally reduce forgetting by replaying previous training data when learning new data. Assuming that the initial pretraining corpus is available, He et al. (2021b) use a gradual decay mix-ratio to adjust the quantity of the pre-training corpus mixed in the new data when learning sequentially. ELLE and CT0 (Scialom et al., 2022) also mix the old data while learning new data.","Question:

How do regularization-based methods and replay methods help mitigate catastrophic forgetting in continual learning?

Answer:

Regularization-based methods address catastrophic forgetting by applying penalties to changes in critical parameters learned from previous data. For instance, Chen et al. (2020) enhance the traditional Elastic Weight Consolidation (EWC) approach (Kirkpatrick et al., 2017) by recalling previously acquired knowledge through pre-trained parameters. This method facilitates continual learning of new information using a multitask learning objective. It computes the importance of each unit, such as attention heads and neurons, to the general knowledge in the language model (LM) using a proxy based on model robustness. This helps preserve learned knowledge, prevents catastrophic forgetting of both general and domain-specific knowledge, and encourages knowledge transfer through techniques like soft-masking and contrastive loss.

On the other hand, replay methods mitigate forgetting by replaying previous training data while learning new data. Assuming the initial pretraining corpus is available, He et al. (2021b) employ a gradual decay mix-ratio to adjust the amount of pre-training corpus mixed with new data during sequential learning. Similarly, methods like ELLE and CT0 (Scialom et al., 2022) incorporate old data while learning new data, thereby reducing the risk of forgetting previously learned information.",NO,True,1321,True,True
264490542-s3,An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives,2023-10-25,Paper Databases,"We source papers from eminent databases in the fields of NLP, the rest of CS, and medicine, as these are integral knowledge areas in the study of mental health CA.These databases include the ACL Anthology (referred to as ACL throughout this paper)2 , AAAI3 , IEEE4 , ACM5 , and PubMed6 .ACL is recognized as a leading repository that highlights pioneering research in NLP.AAAI features cuttingedge studies in AI.IEEE, a leading community, embodies the forefront of engineering and technology research.ACM represents the latest trends in Human Computer Interaction (HCI) along with several other domains of CS.PubMed, the largest search engine for science and biomedical topics including psychology, psychiatry, and informatics among others provides extensive coverage of the medical spectrum.

Drawing on insights from prior literature reviews (Valizadeh and Parde, 2022;Montenegro et al., 2019;Laranjo et al., 2018) and discussion with experts from both the CS and medical domains, we opt for a combination of specific keywords.These search terms represent both our areas of focus: conversational agents (""conversational agent"", ""chatbot"") and mental health (""mental health"", ""depression"").Furthermore, we limit our search criteria to the paper between 2017 to 2022 to cover the most recent articles.We also apply the ""research article"" filter on ACM search, and ""Free Full Text or Full Text"" for PubMed search.Moreover, we manually add 3 papers recommended by the domain experts (Fitzpatrick et al., 2017;Laranjo et al., 2018;Montenegro et al., 2019).This results in 534 papers.","How were papers selected for the study of mental health conversational agents, and what criteria were used in the search process?","Papers were sourced from eminent databases in the fields of NLP, the rest of CS, and medicine, as these are integral knowledge areas in the study of mental health conversational agents. These databases include the ACL Anthology, AAAI, IEEE, ACM, and PubMed. The search terms used represent both areas of focus: conversational agents (""conversational agent"", ""chatbot"") and mental health (""mental health"", ""depression""). The search criteria were limited to papers published between 2017 to 2022 to cover the most recent articles. A ""research article"" filter was applied on ACM search, and ""Free Full Text or Full Text"" for PubMed search. Additionally, three papers recommended by domain experts were manually added, resulting in a total of 534 papers.","What was the process for selecting papers in the study of mental health conversational agents, and what criteria were applied during the search?","The selection of papers for the study of mental health conversational agents involved sourcing from prominent databases across key fields such as Natural Language Processing (NLP), computer science, and medicine, which are crucial for this area of research. The databases utilized included the ACL Anthology, AAAI, IEEE, ACM, and PubMed. The search incorporated terms that reflect the dual focus of the study: ""conversational agent"" and ""chatbot"" for conversational agents, and ""mental health"" and ""depression"" for mental health. To ensure the inclusion of the most recent research, the search was restricted to papers published between 2017 and 2022. Specific filters were applied, such as the ""research article"" filter on ACM and ""Free Full Text or Full Text"" on PubMed. Additionally, three papers recommended by domain experts were manually included, culminating in a total of 534 papers.",7,2,7,4,"Question:

How were papers selected for the study of mental health conversational agents, and what criteria were used in the search process?

Answer:

Papers were sourced from eminent databases in the fields of NLP, the rest of CS, and medicine, as these are integral knowledge areas in the study of mental health conversational agents. These databases include the ACL Anthology, AAAI, IEEE, ACM, and PubMed. The search terms used represent both areas of focus: conversational agents (""conversational agent"", ""chatbot"") and mental health (""mental health"", ""depression""). The search criteria were limited to papers published between 2017 to 2022 to cover the most recent articles. A ""research article"" filter was applied on ACM search, and ""Free Full Text or Full Text"" for PubMed search. Additionally, three papers recommended by domain experts were manually added, resulting in a total of 534 papers.","Question:

What was the process for selecting papers in the study of mental health conversational agents, and what criteria were applied during the search?

Answer:

The selection of papers for the study of mental health conversational agents involved sourcing from prominent databases across key fields such as Natural Language Processing (NLP), computer science, and medicine, which are crucial for this area of research. The databases utilized included the ACL Anthology, AAAI, IEEE, ACM, and PubMed. The search incorporated terms that reflect the dual focus of the study: ""conversational agent"" and ""chatbot"" for conversational agents, and ""mental health"" and ""depression"" for mental health. To ensure the inclusion of the most recent research, the search was restricted to papers published between 2017 and 2022. Specific filters were applied, such as the ""research article"" filter on ACM and ""Free Full Text or Full Text"" on PubMed. Additionally, three papers recommended by domain experts were manually included, culminating in a total of 534 papers.",NO,True,891,True,True
264305746-s6,The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis,2023-10-18,The Social Perception of Sentiment,"A notable distinction exists between computational and psycho-linguistic perspectives on sentiment.In psychology, sentiment is often defined as ""socially constructed patterns of sensations, expressive gestures, and cultural meanings organized around a relationship to a social object, usually another person or group such as a family.""(Gordon, 1981).While sentiment is most commonly categorized as positive, negative, or neutral in computational literature, it encompasses a broader spectrum, ranging from mild to intense (Taboada, 2016;Jo et al., 2017).Furthermore, sentiment (in psychology) is captured through physiological indicators, like facial expressions and heart rate variability (Wiebe et al., 2005;Plutchik, 2001).

Psychological research widely recognizes that a simplistic positive-negative dichotomy is inade-quate for capturing the intricate range of human emotions (Hoffmann, 2018).This is evident in the distinction between seemingly negative emotions such as sadness and fear, which exhibit significant differences in their physiological and psychological effects (Plutchik, 2001).

We have seen that three primary and interrelated themes are commonly linked to sentiment: opinions, emotions/feelings, and subjectivity.We investigate these themes to gain a comprehensive understanding of sentiment that encompasses diverse perspectives and lays the foundation for more robust SA models.

Opinions: From a psychological perspective, opinion is an individual's stance regarding an object or issue, formed after an evaluation through their own lens or perspective (Vaidis and Bran, 2019).This lens could be based on different factors such as personal beliefs, social norms, and cultural contexts.Liu (2012) also define an opinion a ""a subjective statement, view, attitude, emotion, or appraisal about an entity or an aspect of an entity from an opinion holder.""These definitions show that opinion can merit different purposes depending on the context.

Feelings/Emotions: Izard (2010) posit that the word emotion has both a descriptive definition i.e. based on its use in everyday life and a prescriptive definition i.e. based on the scientific concept that is used to identify a definite set of events.Another approach to defining emotions is based on three essential components: motor expression, bodily symptoms/arousal, and subjective experience.There is substantial agreement that motivational consequences and action tendencies associated with emotion are key aspects of emotion rather than just the level of arousal of the subject (Frijda et al., 1986;Frijda, 1987).

Subjectivity: Banfield (2014) referred to sentences that take a character's psychological point of view as subjective, contrasted against sentences that narrate an event in a definite but yielding manner.Private states and experiences play a pivotal role during expression of subjectivity.Here private states could refer to intellectual factors, such as believing, wondering, knowing; or emotive factors, such as hating, being afraid; and perceptual ones, such as seeing or hearing something (Wiebe, 1994).Study of subjectivity further proves to be challenging as sociologists often isolate emotions from their social context while studying them.

Terms like opinion, emotion, and subjectivity hold distinct meanings and are studied separately.Therefore, they are not synonymous with sentiment.Furthermore, when considering sentiment within a sociotechnical system, it is essential to be aware of the contextual nuances associated with the diverse definitions of sentiment derived from sociological, psychological, and linguistic backgrounds.Given the complex nature of sentiment, it is important to approach it with a nuanced perspective and operationalize it within a structured theoretical framework.Prior research suggests that achieving such nuanced understanding can be facilitated through engaging in dialogue with other fields such as psychology, and cognitive science (Head et al., 2015;Cambria et al., 2022).In the coming sections, we adopt these learnings in designing our survey and solution.","How do the definitions of opinion, emotion, and subjectivity differ from sentiment, and why are they not synonymous?","A notable distinction exists between computational and psycho-linguistic perspectives on sentiment. In psychology, sentiment is often defined as ""socially constructed patterns of sensations, expressive gestures, and cultural meanings organized around a relationship to a social object, usually another person or group such as a family"" (Gordon, 1981). While sentiment is most commonly categorized as positive, negative, or neutral in computational literature, it encompasses a broader spectrum, ranging from mild to intense (Taboada, 2016; Jo et al., 2017). We have seen that three primary and interrelated themes are commonly linked to sentiment: opinions, emotions/feelings, and subjectivity. Opinions, from a psychological perspective, are an individual's stance regarding an object or issue, formed after an evaluation through their own lens or perspective (Vaidis and Bran, 2019). Feelings/Emotions are defined by Izard (2010) as having both a descriptive definition based on its use in everyday life and a prescriptive definition based on the scientific concept that is used to identify a definite set of events. Subjectivity, as referred to by Banfield (2014), involves sentences that take a character's psychological point of view, contrasted against sentences that narrate an event in a definite but yielding manner. Terms like opinion, emotion, and subjectivity hold distinct meanings and are studied separately. Therefore, they are not synonymous with sentiment. Furthermore, when considering sentiment within a sociotechnical system, it is essential to be aware of the contextual nuances associated with the diverse definitions of sentiment derived from sociological, psychological, and linguistic backgrounds.","How do the definitions of opinion, emotion, and subjectivity differ from sentiment, and why are they not synonymous?","The distinction between opinion, emotion, subjectivity, and sentiment lies in their definitions and applications across different fields. In psychology, sentiment is defined as ""socially constructed patterns of sensations, expressive gestures, and cultural meanings organized around a relationship to a social object, usually another person or group such as a family"" (Gordon, 1981). In computational literature, sentiment is typically categorized as positive, negative, or neutral, but it actually encompasses a broader spectrum, ranging from mild to intense (Taboada, 2016; Jo et al., 2017).

Opinions, from a psychological perspective, are an individual's stance regarding an object or issue, formed after an evaluation through their own lens or perspective (Vaidis and Bran, 2019). Emotions or feelings, as defined by Izard (2010), have both a descriptive definition based on everyday use and a prescriptive definition based on scientific concepts used to identify a definite set of events. Subjectivity, according to Banfield (2014), involves sentences that take a character's psychological point of view, contrasted against sentences that narrate an event in a definite but yielding manner.

These terms—opinion, emotion, and subjectivity—hold distinct meanings and are studied separately, which is why they are not synonymous with sentiment. Furthermore, when considering sentiment within a sociotechnical system, it is essential to be aware of the contextual nuances associated with the diverse definitions of sentiment derived from sociological, psychological, and linguistic backgrounds.",9,4,9,4,"Question:

How do the definitions of opinion, emotion, and subjectivity differ from sentiment, and why are they not synonymous?

Answer:

A notable distinction exists between computational and psycho-linguistic perspectives on sentiment. In psychology, sentiment is often defined as ""socially constructed patterns of sensations, expressive gestures, and cultural meanings organized around a relationship to a social object, usually another person or group such as a family"" (Gordon, 1981). While sentiment is most commonly categorized as positive, negative, or neutral in computational literature, it encompasses a broader spectrum, ranging from mild to intense (Taboada, 2016; Jo et al., 2017). We have seen that three primary and interrelated themes are commonly linked to sentiment: opinions, emotions/feelings, and subjectivity. Opinions, from a psychological perspective, are an individual's stance regarding an object or issue, formed after an evaluation through their own lens or perspective (Vaidis and Bran, 2019). Feelings/Emotions are defined by Izard (2010) as having both a descriptive definition based on its use in everyday life and a prescriptive definition based on the scientific concept that is used to identify a definite set of events. Subjectivity, as referred to by Banfield (2014), involves sentences that take a character's psychological point of view, contrasted against sentences that narrate an event in a definite but yielding manner. Terms like opinion, emotion, and subjectivity hold distinct meanings and are studied separately. Therefore, they are not synonymous with sentiment. Furthermore, when considering sentiment within a sociotechnical system, it is essential to be aware of the contextual nuances associated with the diverse definitions of sentiment derived from sociological, psychological, and linguistic backgrounds.","Question:

How do the definitions of opinion, emotion, and subjectivity differ from sentiment, and why are they not synonymous?

Answer:

The distinction between opinion, emotion, subjectivity, and sentiment lies in their definitions and applications across different fields. In psychology, sentiment is defined as ""socially constructed patterns of sensations, expressive gestures, and cultural meanings organized around a relationship to a social object, usually another person or group such as a family"" (Gordon, 1981). In computational literature, sentiment is typically categorized as positive, negative, or neutral, but it actually encompasses a broader spectrum, ranging from mild to intense (Taboada, 2016; Jo et al., 2017).

Opinions, from a psychological perspective, are an individual's stance regarding an object or issue, formed after an evaluation through their own lens or perspective (Vaidis and Bran, 2019). Emotions or feelings, as defined by Izard (2010), have both a descriptive definition based on everyday use and a prescriptive definition based on scientific concepts used to identify a definite set of events. Subjectivity, according to Banfield (2014), involves sentences that take a character's psychological point of view, contrasted against sentences that narrate an event in a definite but yielding manner.

These terms—opinion, emotion, and subjectivity—hold distinct meanings and are studied separately, which is why they are not synonymous with sentiment. Furthermore, when considering sentiment within a sociotechnical system, it is essential to be aware of the contextual nuances associated with the diverse definitions of sentiment derived from sociological, psychological, and linguistic backgrounds.",NO,True,1597,True,True
264305746-s14,The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis,2023-10-18,Finance,"Applications developed to comprehend the patterns and dynamics of financial management, creation, and investment analysis.

Table 3: List of applications, defined through thematic analysis, their corresponding definitions, and frequency of papers categorized to the groups.

analysis (Vaismoradi et al., 2013) to uncover the various applications of SA.Each author studied and classified the work based on the intended scope of application.To ensure accuracy and prevent misclassification, this recursive process was employed.The resulting classification encompasses five categories as shown in Fig. 2 and Table 3 1 .Notably, the Health and Medicine domain emerged as the most prominent application area for SA where studies leverage SA to understand individual reactions in diverse medical scenarios (Rodrigues et al., 2016).Following closely, Government and Policy Making emerged as the second most prevalent category, where sentiment analysis plays a pivotal role in comprehending human behavior in governance solutions (Joyce and Deng, 2017).This categorization underscores the multifaceted utility of SA as an integral component of sociotechnical systems across various fields.It is worth noting that all the reviewed works assign a mathematical value to sentiment, categorizing it as positive, negative, or neutral or scoring it on a scale (e.g., -1 to +1).Most of the reviewed works lack clear definitions of sentiment or SA.Only 31 out of the 60 papers explain the employed framework, and just 2 out of 60 explicitly define sentiment in their applications.Only one takes an interdisciplinary perspective, defining sentiment in the context of finance for understanding market behavior (Kraaijeveld and De Smedt, 2020).Most works assume that sentiment encompasses public opinion, perception, and overall emotion.Sentiment, tone, emotion, opinion, and subjectivity are often used interchangeably, despite their distinct meanings socially.

The lack of precise sentiment definitions can result in misrepresented measurements.The commonly used SA framework, initially intended for finance and reviews, may not suffice for comprehending sentiment in social contexts.Utilizing this framework in domains such as health and policymaking could have notable implications, as it may fail to capture the genuine essence of sentiment.","How is sentiment defined in finance, and what assumptions do most works make about sentiment?","Only one work takes an interdisciplinary perspective, defining sentiment in the context of finance for understanding market behavior (Kraaijeveld and De Smedt, 2020). Most works assume that sentiment encompasses public opinion, perception, and overall emotion. Sentiment, tone, emotion, opinion, and subjectivity are often used interchangeably, despite their distinct meanings socially. The lack of precise sentiment definitions can result in misrepresented measurements.","How is sentiment defined in the context of finance, and what are the common assumptions made about it in most studies?","In the realm of finance, sentiment is often defined with an interdisciplinary approach to better understand market behavior, as highlighted by Kraaijeveld and De Smedt (2020). Most studies assume that sentiment includes elements such as public opinion, perception, and overall emotion. Terms like sentiment, tone, emotion, opinion, and subjectivity are frequently used interchangeably, even though they have distinct meanings in a social context. This lack of precise definitions for sentiment can lead to misrepresented measurements in financial analyses.",8,4,8,4,"Question:

How is sentiment defined in finance, and what assumptions do most works make about sentiment?

Answer:

Only one work takes an interdisciplinary perspective, defining sentiment in the context of finance for understanding market behavior (Kraaijeveld and De Smedt, 2020). Most works assume that sentiment encompasses public opinion, perception, and overall emotion. Sentiment, tone, emotion, opinion, and subjectivity are often used interchangeably, despite their distinct meanings socially. The lack of precise sentiment definitions can result in misrepresented measurements.","Question:

How is sentiment defined in the context of finance, and what are the common assumptions made about it in most studies?

Answer:

In the realm of finance, sentiment is often defined with an interdisciplinary approach to better understand market behavior, as highlighted by Kraaijeveld and De Smedt (2020). Most studies assume that sentiment includes elements such as public opinion, perception, and overall emotion. Terms like sentiment, tone, emotion, opinion, and subjectivity are frequently used interchangeably, even though they have distinct meanings in a social context. This lack of precise definitions for sentiment can lead to misrepresented measurements in financial analyses.",NO,True,556,True,True
258832362-s3,Beyond Words: A Comprehensive Survey of Sentence Representations,2023-05-22,Word Representations,"A well-known challenge in NLP is creating continuous dense vector representations of words in high-dimensional spaces to capture their semantic and syntactic meaning. The most widely used algorithm for creating word embeddings is Word2Vec (Mikolov et al., 2013). Traditional approaches to representing words before Word2Vec, like one-hot encoding or bag-ofwords, have a number of drawbacks: They require a lot of memory to hold sparse vectors and fail to capture the links between words or their meaning. By using a neural network to learn word embeddings, Word2Vec solved these issues. The model trains neural networks using a large corpus of text as input to predict the likelihood of a word given its context or vice versa. The weights of the network are changed during training to reduce the discrepancy between the expected and actual probabilities. The network weights are employed as the word embeddings after training is finished. It has been widely used and inspired other models such as GloVe (Pennington et al., 2014) and fastText (Joulin et al., 2016).",How did Word2Vec address the limitations of traditional word representation methods in NLP?,"A well-known challenge in NLP is creating continuous dense vector representations of words in high-dimensional spaces to capture their semantic and syntactic meaning. The most widely used algorithm for creating word embeddings is Word2Vec (Mikolov et al., 2013). Traditional approaches to representing words before Word2Vec, like one-hot encoding or bag-of-words, have a number of drawbacks: They require a lot of memory to hold sparse vectors and fail to capture the links between words or their meaning. By using a neural network to learn word embeddings, Word2Vec solved these issues. The model trains neural networks using a large corpus of text as input to predict the likelihood of a word given its context or vice versa. The weights of the network are changed during training to reduce the discrepancy between the expected and actual probabilities. The network weights are employed as the word embeddings after training is finished.",How did Word2Vec overcome the limitations of traditional word representation methods in natural language processing?,"In natural language processing (NLP), a significant challenge is developing continuous dense vector representations of words in high-dimensional spaces that effectively capture their semantic and syntactic meanings. Before the advent of Word2Vec, traditional methods like one-hot encoding or bag-of-words were commonly used for word representation. However, these approaches had several drawbacks: they required substantial memory to store sparse vectors and were unable to capture the relationships between words or their meanings. Word2Vec, introduced by Mikolov et al. in 2013, addressed these limitations by employing a neural network to learn word embeddings. The model is trained on a large corpus of text, using either the context to predict a word or vice versa. During training, the network's weights are adjusted to minimize the difference between the predicted and actual probabilities. Once training is complete, these network weights serve as the word embeddings, effectively capturing the semantic and syntactic relationships between words.",9,2,9,2,"Question:

How did Word2Vec address the limitations of traditional word representation methods in NLP?

Answer:

A well-known challenge in NLP is creating continuous dense vector representations of words in high-dimensional spaces to capture their semantic and syntactic meaning. The most widely used algorithm for creating word embeddings is Word2Vec (Mikolov et al., 2013). Traditional approaches to representing words before Word2Vec, like one-hot encoding or bag-of-words, have a number of drawbacks: They require a lot of memory to hold sparse vectors and fail to capture the links between words or their meaning. By using a neural network to learn word embeddings, Word2Vec solved these issues. The model trains neural networks using a large corpus of text as input to predict the likelihood of a word given its context or vice versa. The weights of the network are changed during training to reduce the discrepancy between the expected and actual probabilities. The network weights are employed as the word embeddings after training is finished.","Question:

How did Word2Vec overcome the limitations of traditional word representation methods in natural language processing?

Answer:

In natural language processing (NLP), a significant challenge is developing continuous dense vector representations of words in high-dimensional spaces that effectively capture their semantic and syntactic meanings. Before the advent of Word2Vec, traditional methods like one-hot encoding or bag-of-words were commonly used for word representation. However, these approaches had several drawbacks: they required substantial memory to store sparse vectors and were unable to capture the relationships between words or their meanings. Word2Vec, introduced by Mikolov et al. in 2013, addressed these limitations by employing a neural network to learn word embeddings. The model is trained on a large corpus of text, using either the context to predict a word or vice versa. During training, the network's weights are adjusted to minimize the difference between the predicted and actual probabilities. Once training is complete, these network weights serve as the word embeddings, effectively capturing the semantic and syntactic relationships between words.",NO,True,1054,True,True
258832362-s12,Beyond Words: A Comprehensive Survey of Sentence Representations,2023-05-22,Model Level,"Another approach to generating positive examples is by leveraging the distinctive characteristics of the backbone model utilized in contrastive learning. These characteristics might be the architectural choices or using representation from certain components of the model.

Dropout is a regularization technique used in deep learning to prevent overfitting of a model. During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times. These different representations can be used as positive examples for sentence representations. Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy. Several other works have also incorporated this technique and improved upon it: promoting decorrelation between different dimensions (Klein and Nabi, 2022) and adding it as a part of the transformation arsenal (Wu et al., 2022a,d).

Specific components of language models can be trained to generate semantically similar representations. One example is the use of prefix modules (Li and Liang, 2021), which are small, trainable modules added to a pretrained language model. In a recent study, Wang and Lu (2022) trained prefix modules on NLI data before using them to generate positive examples for unsupervised sentence representations. a",How can dropout be utilized to generate positive examples for sentence representations in contrastive learning?,"Dropout is a regularization technique used in deep learning to prevent overfitting of a model. During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times. These different representations can be used as positive examples for sentence representations. Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy.",How can dropout be used to create positive examples for sentence representations in contrastive learning?,"Dropout is a regularization technique in deep learning designed to prevent model overfitting by randomly deactivating some neurons in a layer during training. This process results in slightly different representations each time the same training instance is processed by the model. These varied representations can serve as positive examples for sentence representations in contrastive learning. Recent studies, such as those by Gao et al. (2021), have shown the effectiveness of using dropout as an augmentation strategy in this context.",7,2,7,2,"Question:

How can dropout be utilized to generate positive examples for sentence representations in contrastive learning?

Answer:

Dropout is a regularization technique used in deep learning to prevent overfitting of a model. During training, some neurons in the layer are randomly deactivated, resulting in slightly different representations when the same training instance is passed through the model multiple times. These different representations can be used as positive examples for sentence representations. Recent studies, such as Gao et al. (2021), have demonstrated the effectiveness of using dropout as an augmentation strategy.","Question:

How can dropout be used to create positive examples for sentence representations in contrastive learning?

Answer:

Dropout is a regularization technique in deep learning designed to prevent model overfitting by randomly deactivating some neurons in a layer during training. This process results in slightly different representations each time the same training instance is processed by the model. These varied representations can serve as positive examples for sentence representations in contrastive learning. Recent studies, such as those by Gao et al. (2021), have shown the effectiveness of using dropout as an augmentation strategy in this context.",NO,True,538,True,True
258832362-s15,Beyond Words: A Comprehensive Survey of Sentence Representations,2023-05-22,Alternative Loss and Objectives,"In § 2, we discuss Contrastive loss, which is widely used in machine learning. However, this loss suffers from several limitations: for instance it only considers binary relationships between instances, lacks a mechanism to incorporate ""hard negatives"". To overcome these drawbacks, researchers have explored supplementary losses that can be used in conjunction with the Contrastive loss. Moreover, they have proposed alterations to the loss function that make it more effective for learning, as well as alternative loss functions for contrastive learning of sentence representations. This section provides an overview of these approaches.

To improve the learning of sentence representations using the Contrastive loss, researchers have introduced several supplementary losses. These include: (1) the hinge loss proposed by Jiang et al. (2022b), which enhances discrimination between positive and negative pairs; (2) losses suggested by Wu et al. (2022b) for reconstructing the original sentence from its representation to better capture sentence semantics; (3) a loss developed by Chuang et al. (2022) to identify masked words and improve sensitivity to meaningless semantic transformations; and (4) a loss proposed by Chen et al. (2022a) to minimize redundant information from transformations by minimizing information entropy.

Besides such supplementary losses, some studies have suggested modifications to the original contrastive loss for better effectiveness. For instance, Wu et al. (2022c) have proposed an additional term that incorporates random noise from a Gaussian distribution as negative instances. In the same vein, Zhang et al. (2022d) have introduced two lossesthe angular loss and margin-based triplet loss-to better address the intricacies of similarity between pairs of examples.

However, recent research has moved away from using the contrastive loss and has employed different loss functions to learn sentence representations. For instance, Zhang et al. (2020) maximize the mutual information between a local and a global representation of a sentence using .  identify an alternative sub-manifold within the sentence representation space that considers the geometric structure of sentences to compute semantic similarity.",What are some supplementary losses introduced to enhance the effectiveness of Contrastive loss in sentence representation learning?,"To improve the learning of sentence representations using the Contrastive loss, researchers have introduced several supplementary losses. These include: (1) the hinge loss proposed by Jiang et al. (2022b), which enhances discrimination between positive and negative pairs; (2) losses suggested by Wu et al. (2022b) for reconstructing the original sentence from its representation to better capture sentence semantics; (3) a loss developed by Chuang et al. (2022) to identify masked words and improve sensitivity to meaningless semantic transformations; and (4) a loss proposed by Chen et al. (2022a) to minimize redundant information from transformations by minimizing information entropy.",What supplementary losses have been introduced to enhance the effectiveness of Contrastive loss in sentence representation learning?,"To enhance the effectiveness of Contrastive loss in sentence representation learning, researchers have introduced several supplementary losses. One such loss is the hinge loss proposed by Jiang et al. (2022b), which improves discrimination between positive and negative pairs. Additionally, Wu et al. (2022b) suggested losses aimed at reconstructing the original sentence from its representation, thereby capturing sentence semantics more effectively. Another approach, developed by Chuang et al. (2022), involves a loss designed to identify masked words, enhancing sensitivity to meaningless semantic transformations. Lastly, Chen et al. (2022a) proposed a loss that minimizes redundant information from transformations by reducing information entropy.",7,4,7,4,"Question:

What are some supplementary losses introduced to enhance the effectiveness of Contrastive loss in sentence representation learning?

Answer:

To improve the learning of sentence representations using the Contrastive loss, researchers have introduced several supplementary losses. These include: (1) the hinge loss proposed by Jiang et al. (2022b), which enhances discrimination between positive and negative pairs; (2) losses suggested by Wu et al. (2022b) for reconstructing the original sentence from its representation to better capture sentence semantics; (3) a loss developed by Chuang et al. (2022) to identify masked words and improve sensitivity to meaningless semantic transformations; and (4) a loss proposed by Chen et al. (2022a) to minimize redundant information from transformations by minimizing information entropy.","Question:

What supplementary losses have been introduced to enhance the effectiveness of Contrastive loss in sentence representation learning?

Answer:

To enhance the effectiveness of Contrastive loss in sentence representation learning, researchers have introduced several supplementary losses. One such loss is the hinge loss proposed by Jiang et al. (2022b), which improves discrimination between positive and negative pairs. Additionally, Wu et al. (2022b) suggested losses aimed at reconstructing the original sentence from its representation, thereby capturing sentence semantics more effectively. Another approach, developed by Chuang et al. (2022), involves a loss designed to identify masked words, enhancing sensitivity to meaningless semantic transformations. Lastly, Chen et al. (2022a) proposed a loss that minimizes redundant information from transformations by reducing information entropy.",NO,True,753,True,True
3254638-s2,A Critical Survey of the Methodology for IE Evaluation,2004,Problematic Issues in IE Evaluation,"In Section 2. we have summarized the current status of the methodology adopted in IE. However, the definition of an evaluation methodology and the availability of standard annotated corpora do not guarantee that the experiments performed with different approaches and algorithms proposed in the literature can be reliably compared. Some of the problems are common to other NLP tasks (e.g., see (Daelemans and Hoste, 2002)): the difficulty of exactly identifying the effects on performances of the data used (the sample selection and the sample size), of the information sources used (the features selected), and of the algorithm parameter settings.

One of the most relevant issues is that of the exact split between training set and test set, considering both the numerical proportions between the two sets (e.g., a 50/50 vs. a 80/20 split) and the procedure adopted to partition the documents (e.g., n repeated random splits vs. n-fold crossvalidation).

Furthermore, the question of how to formalize the learning-curve sampling method and its associated costbenefit trade-off may cloud comparison further. For example, the following two approaches have been used: (1) For each point on the learning curve, train on some fraction of the available data and test on the remaining fraction; or (2) Hold out some fixed test set to be used for all points on the learning curve. The second approach is generally preferable: with the first procedure, points on the ""high"" end of the learning curve will have a larger variance than points on the ""low"" end.

Another important issue concerns the features used by the algorithm and their contribution to the performances of the algorithm. In IE, for instance, it would be relevant to extensively investigate the effectiveness of the use of simple orthographic features with respect to the use of more complex linguistic features such as PoS tags or semantic labels extracted from gazetteers (Ciravegna, 2001b).

Apart from those problematic issues mentioned above, there are some others that are specific to IE evaluation. A first issue concerns how to deal with issues related to tokenization, which is often considered something obvious and non problematic but it is not so and can affect the performance of the IE algorithms.

A second issue is related to how to evaluate an extracted fragment -e.g., if an extra comma is extracted should it count as correct, partial or wrong? This issue is related to the question of how relevant is the exact identification of the boundaries of the extracted items. (Freitag, 1998) proposes three different criteria for matching reference instances and extracted instances:

Exact The predicted instance matches exactly an actual instance.",What are the specific challenges in evaluating extracted fragments in Information Extraction (IE) systems?,"A first issue concerns how to deal with issues related to tokenization, which is often considered something obvious and non-problematic but it is not so and can affect the performance of the IE algorithms. A second issue is related to how to evaluate an extracted fragment - e.g., if an extra comma is extracted should it count as correct, partial, or wrong? This issue is related to the question of how relevant is the exact identification of the boundaries of the extracted items.",What challenges arise in evaluating extracted fragments in Information Extraction (IE) systems?,"One significant challenge in evaluating extracted fragments in Information Extraction (IE) systems is dealing with tokenization issues. Tokenization is often assumed to be straightforward and non-problematic, but in reality, it can significantly impact the performance of IE algorithms. Another challenge is determining how to evaluate an extracted fragment. For instance, if an extra comma is extracted, should it be considered correct, partial, or wrong? This challenge is closely related to the importance of accurately identifying the boundaries of the extracted items.",7,4,7,6,"Question:

What are the specific challenges in evaluating extracted fragments in Information Extraction (IE) systems?

Answer:

A first issue concerns how to deal with issues related to tokenization, which is often considered something obvious and non-problematic but it is not so and can affect the performance of the IE algorithms. A second issue is related to how to evaluate an extracted fragment - e.g., if an extra comma is extracted should it count as correct, partial, or wrong? This issue is related to the question of how relevant is the exact identification of the boundaries of the extracted items.","Question:

What challenges arise in evaluating extracted fragments in Information Extraction (IE) systems?

Answer:

One significant challenge in evaluating extracted fragments in Information Extraction (IE) systems is dealing with tokenization issues. Tokenization is often assumed to be straightforward and non-problematic, but in reality, it can significantly impact the performance of IE algorithms. Another challenge is determining how to evaluate an extracted fragment. For instance, if an extra comma is extracted, should it be considered correct, partial, or wrong? This challenge is closely related to the importance of accurately identifying the boundaries of the extracted items.",NO,True,573,True,True
260063224-s10,How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques,2023,Modification of Token Distribution,"Dathathri et al. (2019) propose a Plug and Play Language Model (PPLM) which uses external attribute classifiers to guide text generation without requiring any training of the PLM. The PLM is used to obtain the next token distribution, which is fed to external classifiers, called Attribute Models, to assess whether the token correctly expresses the desired attributes. The internal latent representations of the LM are updated with a backward pass using the gradients of the attribute models to increase the likelihood of the desired attributes. Finally, the next token distribution is recomputed taking into account the updated latent representations. This model allows control of multiple attributes at a time, such as sentiment and topic.

Inspired by this work, Madotto et al. (2020) propose a variation of PPLMs in which the backward pass is executed n times depending on the desired intensity of the control attribute. Furthermore, they add Residual Adapters (Houlsby et al., 2019) on top of each transformer layer to steer the PLM output distribution without changing its parameters.

Goswamy et al. (2020) propose a different variation of PPLMs based on GPT-2, in which a modified loss is considered to take into account the intensity of the controlled sentiment. Furthermore, instead of considering only positive/negative sentiment, control over 8 emotion categories is enabled.

Starting from PPLMs, Gu et al. (2022a) observe that using a controller alone leads to the trade-off problem, i.e. the controller used to modify the token distribution only focuses on how to make the prefix related to the desired attribute without taking into account the original distribution of the LM. In this way, the controller takes over the LM's control for the next token distribution. In order to alleviate  this problem, they propose a weighted decoding method that adds a regulator module that permits fine-grained adjustment of a bias signal from the controller. At every step, the regulator detects differences between the PLM distribution and the target attribute and it determines whether to suppress or amplify the bias signal. This method is model agnostic and has been evaluated with sentiment, topic, and toxicity attributes. The last two methods propose sampling procedures that can be applied to any LM. Landsman et al. (2022) propose to modify beam search by reweighing the token candidate likelihoods to control different attributes. Diverse beam search (Vijayakumar et al., 2016) is used to decode k candidates, which are then scored using an attribute model. The obtained scores are used to reweigh the original likelihoods to produce a reweighed candidate distribution that considers both fluency and attribute characteristics. The resulting distribution is used to sample the next token.

Lastly, Kumar et al. (2022) propose a sampling method combining LM log-likelihoods with arbitrary constraints in a single energy function generating samples in a non-autoregressive manner. The idea is to use a PLM without changing its distribution but sampling from it considering different constraints, i.e. control attributes. The constraints are discriminative classifiers trained from scratch or fine-tuned. This method allows multi-attribute control (sentiment and toxicity).",How do recent methods modify token distribution to control sentiment in text generation?,"Dathathri et al. (2019) propose a Plug and Play Language Model (PPLM) which uses external attribute classifiers to guide text generation without requiring any training of the PLM. The PLM is used to obtain the next token distribution, which is fed to external classifiers, called Attribute Models, to assess whether the token correctly expresses the desired attributes. The internal latent representations of the LM are updated with a backward pass using the gradients of the attribute models to increase the likelihood of the desired attributes. Finally, the next token distribution is recomputed taking into account the updated latent representations. This model allows control of multiple attributes at a time, such as sentiment and topic. Inspired by this work, Madotto et al. (2020) propose a variation of PPLMs in which the backward pass is executed n times depending on the desired intensity of the control attribute. Furthermore, they add Residual Adapters (Houlsby et al., 2019) on top of each transformer layer to steer the PLM output distribution without changing its parameters. Goswamy et al. (2020) propose a different variation of PPLMs based on GPT-2, in which a modified loss is considered to take into account the intensity of the controlled sentiment. Furthermore, instead of considering only positive/negative sentiment, control over 8 emotion categories is enabled. Starting from PPLMs, Gu et al. (2022a) observe that using a controller alone leads to the trade-off problem, i.e. the controller used to modify the token distribution only focuses on how to make the prefix related to the desired attribute without taking into account the original distribution of the LM. In this way, the controller takes over the LM's control for the next token distribution. In order to alleviate this problem, they propose a weighted decoding method that adds a regulator module that permits fine-grained adjustment of a bias signal from the controller. At every step, the regulator detects differences between the PLM distribution and the target attribute and it determines whether to suppress or amplify the bias signal. This method is model agnostic and has been evaluated with sentiment, topic, and toxicity attributes. The last two methods propose sampling procedures that can be applied to any LM. Landsman et al. (2022) propose to modify beam search by reweighing the token candidate likelihoods to control different attributes. Diverse beam search (Vijayakumar et al., 2016) is used to decode k candidates, which are then scored using an attribute model. The obtained scores are used to reweigh the original likelihoods to produce a reweighed candidate distribution that considers both fluency and attribute characteristics. The resulting distribution is used to sample the next token. Lastly, Kumar et al. (2022) propose a sampling method combining LM log-likelihoods with arbitrary constraints in a single energy function generating samples in a non-autoregressive manner. The idea is to use a PLM without changing its distribution but sampling from it considering different constraints, i.e. control attributes. The constraints are discriminative classifiers trained from scratch or fine-tuned. This method allows multi-attribute control (sentiment and toxicity).",How are recent methods modifying token distribution to control sentiment in text generation?,"Recent advancements in text generation have introduced several methods to modify token distribution for sentiment control. One notable approach is the Plug and Play Language Model (PPLM) by Dathathri et al. (2019), which leverages external attribute classifiers to guide text generation without retraining the pre-trained language model (PLM). The process involves obtaining the next token distribution from the PLM and feeding it to Attribute Models, which evaluate whether the token aligns with the desired attributes. The internal latent representations of the language model are then updated through a backward pass using the gradients from the attribute models, enhancing the likelihood of the desired attributes. The next token distribution is recalculated, considering these updated representations, allowing for control over multiple attributes like sentiment and topic.

Building on this, Madotto et al. (2020) introduced a variation where the backward pass is executed multiple times, depending on the desired intensity of the control attribute. They also incorporated Residual Adapters (Houlsby et al., 2019) atop each transformer layer to adjust the PLM output distribution without altering its parameters. Another variation by Goswamy et al. (2020) involves a modified loss function in a GPT-2-based PPLM to account for the intensity of controlled sentiment, extending control to eight emotion categories beyond just positive/negative sentiment.

Gu et al. (2022a) identified a trade-off issue where the controller might overshadow the original LM distribution. To address this, they proposed a weighted decoding method with a regulator module that fine-tunes the bias signal from the controller. This regulator detects discrepancies between the PLM distribution and the target attribute, deciding whether to suppress or amplify the bias signal. This model-agnostic method has been tested with attributes like sentiment, topic, and toxicity.

Further innovations include Landsman et al. (2022), who modified beam search by reweighing token candidate likelihoods to control different attributes. They employed diverse beam search (Vijayakumar et al., 2016) to decode multiple candidates, which are then scored using an attribute model. These scores reweigh the original likelihoods, producing a candidate distribution that balances fluency and attribute characteristics for sampling the next token.

Lastly, Kumar et al. (2022) proposed a sampling method that combines LM log-likelihoods with arbitrary constraints in a single energy function, generating samples non-autoregressively. This approach uses a PLM without altering its distribution, sampling from it under various constraints, such as control attributes. The constraints are discriminative classifiers trained from scratch or fine-tuned, enabling multi-attribute control, including sentiment and toxicity.",7,4,7,6,"Question:

How do recent methods modify token distribution to control sentiment in text generation?

Answer:

Dathathri et al. (2019) propose a Plug and Play Language Model (PPLM) which uses external attribute classifiers to guide text generation without requiring any training of the PLM. The PLM is used to obtain the next token distribution, which is fed to external classifiers, called Attribute Models, to assess whether the token correctly expresses the desired attributes. The internal latent representations of the LM are updated with a backward pass using the gradients of the attribute models to increase the likelihood of the desired attributes. Finally, the next token distribution is recomputed taking into account the updated latent representations. This model allows control of multiple attributes at a time, such as sentiment and topic. Inspired by this work, Madotto et al. (2020) propose a variation of PPLMs in which the backward pass is executed n times depending on the desired intensity of the control attribute. Furthermore, they add Residual Adapters (Houlsby et al., 2019) on top of each transformer layer to steer the PLM output distribution without changing its parameters. Goswamy et al. (2020) propose a different variation of PPLMs based on GPT-2, in which a modified loss is considered to take into account the intensity of the controlled sentiment. Furthermore, instead of considering only positive/negative sentiment, control over 8 emotion categories is enabled. Starting from PPLMs, Gu et al. (2022a) observe that using a controller alone leads to the trade-off problem, i.e. the controller used to modify the token distribution only focuses on how to make the prefix related to the desired attribute without taking into account the original distribution of the LM. In this way, the controller takes over the LM's control for the next token distribution. In order to alleviate this problem, they propose a weighted decoding method that adds a regulator module that permits fine-grained adjustment of a bias signal from the controller. At every step, the regulator detects differences between the PLM distribution and the target attribute and it determines whether to suppress or amplify the bias signal. This method is model agnostic and has been evaluated with sentiment, topic, and toxicity attributes. The last two methods propose sampling procedures that can be applied to any LM. Landsman et al. (2022) propose to modify beam search by reweighing the token candidate likelihoods to control different attributes. Diverse beam search (Vijayakumar et al., 2016) is used to decode k candidates, which are then scored using an attribute model. The obtained scores are used to reweigh the original likelihoods to produce a reweighed candidate distribution that considers both fluency and attribute characteristics. The resulting distribution is used to sample the next token. Lastly, Kumar et al. (2022) propose a sampling method combining LM log-likelihoods with arbitrary constraints in a single energy function generating samples in a non-autoregressive manner. The idea is to use a PLM without changing its distribution but sampling from it considering different constraints, i.e. control attributes. The constraints are discriminative classifiers trained from scratch or fine-tuned. This method allows multi-attribute control (sentiment and toxicity).","Question:

How are recent methods modifying token distribution to control sentiment in text generation?

Answer:

Recent advancements in text generation have introduced several methods to modify token distribution for sentiment control. One notable approach is the Plug and Play Language Model (PPLM) by Dathathri et al. (2019), which leverages external attribute classifiers to guide text generation without retraining the pre-trained language model (PLM). The process involves obtaining the next token distribution from the PLM and feeding it to Attribute Models, which evaluate whether the token aligns with the desired attributes. The internal latent representations of the language model are then updated through a backward pass using the gradients from the attribute models, enhancing the likelihood of the desired attributes. The next token distribution is recalculated, considering these updated representations, allowing for control over multiple attributes like sentiment and topic.

Building on this, Madotto et al. (2020) introduced a variation where the backward pass is executed multiple times, depending on the desired intensity of the control attribute. They also incorporated Residual Adapters (Houlsby et al., 2019) atop each transformer layer to adjust the PLM output distribution without altering its parameters. Another variation by Goswamy et al. (2020) involves a modified loss function in a GPT-2-based PPLM to account for the intensity of controlled sentiment, extending control to eight emotion categories beyond just positive/negative sentiment.

Gu et al. (2022a) identified a trade-off issue where the controller might overshadow the original LM distribution. To address this, they proposed a weighted decoding method with a regulator module that fine-tunes the bias signal from the controller. This regulator detects discrepancies between the PLM distribution and the target attribute, deciding whether to suppress or amplify the bias signal. This model-agnostic method has been tested with attributes like sentiment, topic, and toxicity.

Further innovations include Landsman et al. (2022), who modified beam search by reweighing token candidate likelihoods to control different attributes. They employed diverse beam search (Vijayakumar et al., 2016) to decode multiple candidates, which are then scored using an attribute model. These scores reweigh the original likelihoods, producing a candidate distribution that balances fluency and attribute characteristics for sampling the next token.

Lastly, Kumar et al. (2022) proposed a sampling method that combines LM log-likelihoods with arbitrary constraints in a single energy function, generating samples non-autoregressively. This approach uses a PLM without altering its distribution, sampling from it under various constraints, such as control attributes. The constraints are discriminative classifiers trained from scratch or fine-tuned, enabling multi-attribute control, including sentiment and toxicity.",NO,True,2879,True,True
264832783-s9,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,2023-11-01,Pre-training Data,"Some studies have suggested that factors related to pre-traning data such as data domain, data term frequency, and data distribution (Chan et al., 2022;Razeghi et al., 2022), are crucial elements influencing the development of emergent abilities.

Data Domain Shin et al. (2022) conducted a study to explore the variations of ICL performance concerning the domain source and the size of the pre-training corpus, focusing primarily on the Korean lexicon.They utilized seven subcorpora from the HyperCLOVA corpus (Kim et al., 2021) to pretrain various language models and evaluated these models on Korean downstream tasks.Interestingly, Shin et al. (2022) found that the size of the pretraining corpus does not always determine the emergence of ICL.Instead, the domain source of the corpus significantly influences ICL performance.For example, language models trained with subcorpora constructed from blog posts exhibited the best ICL capability.This phenomenon may be attributed to the greater token diversity presented in the blog posts corpus compared with other sources like news.Moreover, their experiments highlighted that combining multiple corpora can lead to the emergence of ICL, even if individual corpora did not produce such learning on their own.Surprisingly, Shin et al. (2022) also found that a language model pre-trained with a corpus related to a downstream task did not always guarantee competitive ICL performance.For instance, a model trained on a news-related dataset (Park et al., 2021) showed superior performance in zero-shot news topic classification, but its few-shot performance was not superior.In a similar vein, The authors focused particularly on a crucial type of reasoning in LLMs -numerical reasoning in fewshot settings; and examined the extend to which the frequency of terms from the pre-training data correlates with model performance in these situations.Their analysis focused on the prevalence of numerical reasoning tasks within the training instances and established a connection between frequencies and reasoning performance.This connection is quantified by introducing the ""performance gap"", which is defined as the accuracy of terms appearing more than 90% of the time minus the accuracy of terms appearing less than 10% of the time.They conducted their experiments using GPTbased language models trained on the Pile dataset (Gao et al., 2021), ranging in size from 1.3B to 6B parameters.Evaluation was carried out on 11 datasets spanning three types of mathematical reasoning tasks: Arithmetic, Operation Inference and Time-Unit Conversion.The findings consistently show that models perform better in instances where terms from the pre-training data are more prevalent (Razeghi et al., 2022).In some scenarios, there is a substantial performance gap of over 70% between the most frequently occurring terms and the least frequently occurring terms.The significant performance difference raises questions about the actual generalization capabilities of these models beyond their pre-training data.Razeghi et al. (2022)'s observations suggest that the more prevalent content included in the pre-training data may exert an influ-ence on the emergent abilities, and it is possible that these language models are not actually reasoning to solve arithmetic tasks.In line with this research, Kandpal et al. (2023) (2023) theoretically demonstrated that unseen tasks can be efficiently learned via ICL when the pretraining data distribution comprises a mixture of latent tasks.",How does the frequency of terms in pre-training data affect numerical reasoning performance in few-shot settings?,"The frequency of terms from the pre-training data correlates with model performance in numerical reasoning tasks in few-shot settings. The analysis focused on the prevalence of numerical reasoning tasks within the training instances and established a connection between frequencies and reasoning performance. This connection is quantified by introducing the ""performance gap"", which is defined as the accuracy of terms appearing more than 90% of the time minus the accuracy of terms appearing less than 10% of the time. The findings consistently show that models perform better in instances where terms from the pre-training data are more prevalent (Razeghi et al., 2022). In some scenarios, there is a substantial performance gap of over 70% between the most frequently occurring terms and the least frequently occurring terms. The significant performance difference raises questions about the actual generalization capabilities of these models beyond their pre-training data. Razeghi et al. (2022)'s observations suggest that the more prevalent content included in the pre-training data may exert an influence on the emergent abilities, and it is possible that these language models are not actually reasoning to solve arithmetic tasks.",How does the frequency of terms in pre-training data influence numerical reasoning performance in few-shot settings?,"The frequency of terms in pre-training data significantly influences model performance in numerical reasoning tasks, especially in few-shot settings. Research by Razeghi et al. (2022) highlights a correlation between the prevalence of numerical reasoning tasks in training instances and the model's reasoning performance. This relationship is quantified through the ""performance gap,"" defined as the difference in accuracy between terms appearing more than 90% of the time and those appearing less than 10% of the time. Findings consistently show that models perform better when terms from the pre-training data are more prevalent. In some cases, there is a substantial performance gap of over 70% between the most frequently occurring terms and the least frequently occurring ones. This significant performance difference raises questions about the models' actual generalization capabilities beyond their pre-training data. The observations by Razeghi et al. (2022) suggest that the more prevalent content in the pre-training data may influence the models' emergent abilities, indicating that these language models might not be genuinely reasoning to solve arithmetic tasks.",7,4,7,4,"Question:

How does the frequency of terms in pre-training data affect numerical reasoning performance in few-shot settings?

Answer:

The frequency of terms from the pre-training data correlates with model performance in numerical reasoning tasks in few-shot settings. The analysis focused on the prevalence of numerical reasoning tasks within the training instances and established a connection between frequencies and reasoning performance. This connection is quantified by introducing the ""performance gap"", which is defined as the accuracy of terms appearing more than 90% of the time minus the accuracy of terms appearing less than 10% of the time. The findings consistently show that models perform better in instances where terms from the pre-training data are more prevalent (Razeghi et al., 2022). In some scenarios, there is a substantial performance gap of over 70% between the most frequently occurring terms and the least frequently occurring terms. The significant performance difference raises questions about the actual generalization capabilities of these models beyond their pre-training data. Razeghi et al. (2022)'s observations suggest that the more prevalent content included in the pre-training data may exert an influence on the emergent abilities, and it is possible that these language models are not actually reasoning to solve arithmetic tasks.","Question:

How does the frequency of terms in pre-training data influence numerical reasoning performance in few-shot settings?

Answer:

The frequency of terms in pre-training data significantly influences model performance in numerical reasoning tasks, especially in few-shot settings. Research by Razeghi et al. (2022) highlights a correlation between the prevalence of numerical reasoning tasks in training instances and the model's reasoning performance. This relationship is quantified through the ""performance gap,"" defined as the difference in accuracy between terms appearing more than 90% of the time and those appearing less than 10% of the time. Findings consistently show that models perform better when terms from the pre-training data are more prevalent. In some cases, there is a substantial performance gap of over 70% between the most frequently occurring terms and the least frequently occurring ones. This significant performance difference raises questions about the models' actual generalization capabilities beyond their pre-training data. The observations by Razeghi et al. (2022) suggest that the more prevalent content in the pre-training data may influence the models' emergent abilities, indicating that these language models might not be genuinely reasoning to solve arithmetic tasks.",NO,True,1175,True,True
264832783-s18,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,2023-11-01,Causality rather than Correlation,"As demonstrated by Razeghi et al. (2022) and Power et al. (2022), intervening on the pre-training dataset, particularly with an emphasis on the emergence of reasoning abilities, offers a promising path for gaining deeper into the question of whether LLMs indeed possess reasoning abilities.Moreover, as highlighted by Chan et al. (2022), delving into the intricacies of in-context and in-weights learning deserves further investigation, especially concerning how prior knowledge is signaled.It is crucial to make comparison between transformers and recurrent architectures, particularly in understanding their in-context learning capacities.What's more, there is a need for a more comprehensive interpretation of the impact of the ""Reversal Curse"" in extensive pre-training datasets for LLMs, considering the varying frequencies of reversed information.",Why is it important to compare transformers and recurrent architectures in terms of their in-context learning capacities?,"It is crucial to make a comparison between transformers and recurrent architectures, particularly in understanding their in-context learning capacities.",Why is it important to compare the in-context learning capacities of transformers and recurrent architectures?,"Comparing the in-context learning capacities of transformers and recurrent architectures is crucial for understanding their respective strengths and limitations. This comparison helps in identifying which architecture is more effective for specific tasks, thereby guiding the development and application of machine learning models.",9,7,7,6,"Question:

Why is it important to compare transformers and recurrent architectures in terms of their in-context learning capacities?

Answer:

It is crucial to make a comparison between transformers and recurrent architectures, particularly in understanding their in-context learning capacities.","Question:

Why is it important to compare the in-context learning capacities of transformers and recurrent architectures?

Answer:

Comparing the in-context learning capacities of transformers and recurrent architectures is crucial for understanding their respective strengths and limitations. This comparison helps in identifying which architecture is more effective for specific tasks, thereby guiding the development and application of machine learning models.",NO,True,331,False,False
222124957-s2,Which *BERT? A Survey Organizing Contextualized Encoders,2020-10-02,Transfer: The Pretrain-Finetune Framework,"While text representations can be learned in any manner, ultimately, they are evaluated using specific target tasks. Historically, the learned representations (e.g. word vectors) were used as initialization for task-specific models. Dai and Le (2015) are credited with using pretrained language model outputs as initialization, McCann et al. (2017) use pretrained outputs from translation as frozen word embeddings, and Howard and Ruder (2018) and Radford et al. (2018) demonstrate the effectiveness of finetuning to different target tasks by updating the full (pretrained) model for each task. We refer to the embeddings produced by the pretrained models (or encoders) as contextualized text representations. As our goal is to discuss the encoders and their representations, we do not cover the innovations in finetuning (Liu et al., 2015;Ruder et al., 2019;Phang et al., 2018;Liu et al., 2019c;Zhu et al., 2020, inter alia).

Evaluation Widely adopted evaluations of text representations relate them to downstream natural language understanding (NLU) benchmarks. This full-stack process necessarily conflates representation power with finetuning strategies. Common language understanding benchmarks include (1) a diverse suite of sentence-level tasks covering paraphrasing, natural language inference, sentiment, and linguistic acceptability (GLUE) and its more challenging counterpart with additional commonsense and linguistic reasoning tasks (Super-GLUE) (Wang et al., 2019c,b;Clark et al., 2019a;De Marneffe et al., 2019;Roemmele et al., 2011;Khashabi et al., 2018;Zhang et al., 2018;Dagan et al., 2006;Bar Haim et al., 2006;Giampiccolo et al., 2007;Bentivogli et al., 2009;Pilehvar and Camacho-Collados, 2019;Poliak et al., 2018;Levesque et al., 2011); (2) crowdsourced questions derived from Wikipedia articles (Rajpurkar et al., 2016, 2018; and (3) multiple-choice reading comprehension (Lai et al., 2017, RACE).",How are text representations evaluated in relation to downstream natural language understanding benchmarks?  ,"Text representations are evaluated using specific target tasks. Historically, the learned representations, such as word vectors, were used as initialization for task-specific models. Pretrained language model outputs have been used as initialization, and pretrained outputs from translation have been used as frozen word embeddings. Finetuning has been demonstrated to be effective for different target tasks by updating the full pretrained model for each task. The embeddings produced by the pretrained models, or encoders, are referred to as contextualized text representations. Widely adopted evaluations of text representations relate them to downstream natural language understanding (NLU) benchmarks. This process conflates representation power with finetuning strategies. Common language understanding benchmarks include a diverse suite of sentence-level tasks covering paraphrasing, natural language inference, sentiment, and linguistic acceptability (GLUE) and its more challenging counterpart with additional commonsense and linguistic reasoning tasks (Super-GLUE). Other benchmarks include crowdsourced questions derived from Wikipedia articles and multiple-choice reading comprehension.",How are text representations evaluated in relation to downstream natural language understanding benchmarks?,"Text representations are evaluated through specific target tasks that assess their effectiveness in natural language understanding (NLU) benchmarks. Historically, learned representations like word vectors were used to initialize task-specific models. More recently, pretrained language model outputs have been employed as initialization, and outputs from translation models have been used as frozen word embeddings. Finetuning has proven effective across various target tasks by updating the entire pretrained model for each specific task. The embeddings generated by these pretrained models, or encoders, are known as contextualized text representations. Evaluations of these text representations often involve their application to downstream NLU benchmarks, which combine representation power with finetuning strategies. Common benchmarks include the General Language Understanding Evaluation (GLUE), which covers a diverse range of sentence-level tasks such as paraphrasing, natural language inference, sentiment analysis, and linguistic acceptability. Its more challenging counterpart, Super-GLUE, includes additional tasks requiring commonsense and linguistic reasoning. Other benchmarks involve crowdsourced questions from Wikipedia articles and multiple-choice reading comprehension tasks.",8,4,8,4,"Question:

How are text representations evaluated in relation to downstream natural language understanding benchmarks?  

Answer:

Text representations are evaluated using specific target tasks. Historically, the learned representations, such as word vectors, were used as initialization for task-specific models. Pretrained language model outputs have been used as initialization, and pretrained outputs from translation have been used as frozen word embeddings. Finetuning has been demonstrated to be effective for different target tasks by updating the full pretrained model for each task. The embeddings produced by the pretrained models, or encoders, are referred to as contextualized text representations. Widely adopted evaluations of text representations relate them to downstream natural language understanding (NLU) benchmarks. This process conflates representation power with finetuning strategies. Common language understanding benchmarks include a diverse suite of sentence-level tasks covering paraphrasing, natural language inference, sentiment, and linguistic acceptability (GLUE) and its more challenging counterpart with additional commonsense and linguistic reasoning tasks (Super-GLUE). Other benchmarks include crowdsourced questions derived from Wikipedia articles and multiple-choice reading comprehension.","Question:

How are text representations evaluated in relation to downstream natural language understanding benchmarks?

Answer:

Text representations are evaluated through specific target tasks that assess their effectiveness in natural language understanding (NLU) benchmarks. Historically, learned representations like word vectors were used to initialize task-specific models. More recently, pretrained language model outputs have been employed as initialization, and outputs from translation models have been used as frozen word embeddings. Finetuning has proven effective across various target tasks by updating the entire pretrained model for each specific task. The embeddings generated by these pretrained models, or encoders, are known as contextualized text representations. Evaluations of these text representations often involve their application to downstream NLU benchmarks, which combine representation power with finetuning strategies. Common benchmarks include the General Language Understanding Evaluation (GLUE), which covers a diverse range of sentence-level tasks such as paraphrasing, natural language inference, sentiment analysis, and linguistic acceptability. Its more challenging counterpart, Super-GLUE, includes additional tasks requiring commonsense and linguistic reasoning. Other benchmarks involve crowdsourced questions from Wikipedia articles and multiple-choice reading comprehension tasks.",NO,True,1296,True,True
222124957-s5,Which *BERT? A Survey Organizing Contextualized Encoders,2020-10-02,Nontoken Prediction,"Bender and Koller (2020) argue that for the goal of natural language understanding, we cannot rely purely on a language modeling objective; there must be some grounding or external information that relates the text to each other or to the world. One solution is to introduce a secondary objective to directly learn these biases.

Self-supervised discourse structure objectives, such as text order, has garnered significant attention. To capture relationships between two sentences, 5 Devlin et al. (2019) introduce the next 4 Clark et al. (2020) report negative results for rarer words. 5 Sentence unfortunately refers to a text segment containing sentence prediction (NSP) objective. In this task, either sentence B follows sentence A or B is a random negative sample. Subsequent works showed that this was not effective, suggesting the model simply learned topic (Yang et al., 2019;. Jernite et al. (2017) propose a sentence order task of predicting whether A is before, after, or unrelated to B, and Wang et al. (2020b) and Lan et al. (2020) use it for pretraining encoders. They report that (1) understanding text order does contribute to improved language understanding; and (2) harder-to-learn pretraining objectives are more powerful, as both modified tasks have lower intrinsic performance than NSP. It is still unclear, however, if this is the best way to incorporate discourse structure, especially since these works do not use real sentences.

Additional work has focused on effectively incorporating multiple pretraining objectives. Sun et al. (2020a) use multi-task learning with continual pretraining (Hashimoto et al., 2017), which incrementally introduces newer tasks into the set of pretraining tasks from word to sentence to document level tasks. Encoders using visual features (and evaluated only on visual tasks) jointly optimize multiple different masking objectives over both token sequences and regions of interests in the image (Tan and Bansal, 2019). 6 Prior to token prediction, discourse information has been used in training sentence representations. Conneau et al. (2017Conneau et al. ( , 2018a use natural language inference sentence pairs, Jernite et al. (2017) use discourse-based objectives of sentence order, conjunction classifier, and next sentence selection, and  use discourse markers. While there is weak evidence suggesting that these types of objectives are less effective than language modeling (Wang et al., 2019a), we lack fair studies comparing the relative influence between the two categories of objectives.",How have self-supervised discourse structure objectives been utilized in pretraining encoders for natural language understanding?,"Self-supervised discourse structure objectives, such as text order, have garnered significant attention. To capture relationships between two sentences, Devlin et al. (2019) introduce the next sentence prediction (NSP) objective. In this task, either sentence B follows sentence A or B is a random negative sample. Subsequent works showed that this was not effective, suggesting the model simply learned topic. Jernite et al. (2017) propose a sentence order task of predicting whether A is before, after, or unrelated to B, and Wang et al. (2020b) and Lan et al. (2020) use it for pretraining encoders. They report that understanding text order does contribute to improved language understanding, and harder-to-learn pretraining objectives are more powerful, as both modified tasks have lower intrinsic performance than NSP. It is still unclear, however, if this is the best way to incorporate discourse structure, especially since these works do not use real sentences. Additional work has focused on effectively incorporating multiple pretraining objectives. Sun et al. (2020a) use multi-task learning with continual pretraining, which incrementally introduces newer tasks into the set of pretraining tasks from word to sentence to document level tasks. Encoders using visual features jointly optimize multiple different masking objectives over both token sequences and regions of interests in the image (Tan and Bansal, 2019).",In what ways have self-supervised discourse structure objectives been applied in the pretraining of encoders for natural language understanding?,"Self-supervised discourse structure objectives, such as text order, have gained significant attention in the pretraining of encoders for natural language understanding. Devlin et al. (2019) introduced the next sentence prediction (NSP) objective to capture relationships between two sentences. In this task, either sentence B follows sentence A, or B is a random negative sample. However, subsequent research indicated that NSP was not effective, as the model primarily learned topic rather than discourse structure. To address this, Jernite et al. (2017) proposed a sentence order task that predicts whether sentence A is before, after, or unrelated to sentence B. This approach was further utilized by Wang et al. (2020b) and Lan et al. (2020) for pretraining encoders, demonstrating that understanding text order contributes to improved language understanding. These studies also found that more challenging pretraining objectives are more powerful, as both modified tasks have lower intrinsic performance than NSP. Despite these advancements, it remains unclear if this is the optimal method for incorporating discourse structure, especially since these works do not use real sentences. Additional research has focused on effectively integrating multiple pretraining objectives. For instance, Sun et al. (2020a) employed multi-task learning with continual pretraining, incrementally introducing newer tasks from word to sentence to document level. Furthermore, encoders using visual features jointly optimize multiple masking objectives over both token sequences and regions of interest in images (Tan and Bansal, 2019).",7,4,7,7,"Question:

How have self-supervised discourse structure objectives been utilized in pretraining encoders for natural language understanding?

Answer:

Self-supervised discourse structure objectives, such as text order, have garnered significant attention. To capture relationships between two sentences, Devlin et al. (2019) introduce the next sentence prediction (NSP) objective. In this task, either sentence B follows sentence A or B is a random negative sample. Subsequent works showed that this was not effective, suggesting the model simply learned topic. Jernite et al. (2017) propose a sentence order task of predicting whether A is before, after, or unrelated to B, and Wang et al. (2020b) and Lan et al. (2020) use it for pretraining encoders. They report that understanding text order does contribute to improved language understanding, and harder-to-learn pretraining objectives are more powerful, as both modified tasks have lower intrinsic performance than NSP. It is still unclear, however, if this is the best way to incorporate discourse structure, especially since these works do not use real sentences. Additional work has focused on effectively incorporating multiple pretraining objectives. Sun et al. (2020a) use multi-task learning with continual pretraining, which incrementally introduces newer tasks into the set of pretraining tasks from word to sentence to document level tasks. Encoders using visual features jointly optimize multiple different masking objectives over both token sequences and regions of interests in the image (Tan and Bansal, 2019).","Question:

In what ways have self-supervised discourse structure objectives been applied in the pretraining of encoders for natural language understanding?

Answer:

Self-supervised discourse structure objectives, such as text order, have gained significant attention in the pretraining of encoders for natural language understanding. Devlin et al. (2019) introduced the next sentence prediction (NSP) objective to capture relationships between two sentences. In this task, either sentence B follows sentence A, or B is a random negative sample. However, subsequent research indicated that NSP was not effective, as the model primarily learned topic rather than discourse structure. To address this, Jernite et al. (2017) proposed a sentence order task that predicts whether sentence A is before, after, or unrelated to sentence B. This approach was further utilized by Wang et al. (2020b) and Lan et al. (2020) for pretraining encoders, demonstrating that understanding text order contributes to improved language understanding. These studies also found that more challenging pretraining objectives are more powerful, as both modified tasks have lower intrinsic performance than NSP. Despite these advancements, it remains unclear if this is the optimal method for incorporating discourse structure, especially since these works do not use real sentences. Additional research has focused on effectively integrating multiple pretraining objectives. For instance, Sun et al. (2020a) employed multi-task learning with continual pretraining, incrementally introducing newer tasks from word to sentence to document level. Furthermore, encoders using visual features jointly optimize multiple masking objectives over both token sequences and regions of interest in images (Tan and Bansal, 2019).",NO,True,1624,True,True
222124957-s8,Which *BERT? A Survey Organizing Contextualized Encoders,2020-10-02,Training,"One area of research decreases wall-clock training time through more compute and larger batches. You et al. (2020) reduce the time of training BERT by introducing the LAMB optimizer, a large batch stochastic optimization method adjusted for attention models. Rajbhandari et al. (2020) analyze memory usage in the optimizer to enable parallelization of models resulting in higher throughput in training. By reducing the training time, models can be practically trained for longer, which has also been shown to lead to benefits in task performance Lan et al., 2020, inter alia).

Another line of research reduces the compute through attention sparsification (discussed in §4.2) or increasing the convergence rate (Clark et al., 2020). These works report hardware and estimate the reduction in floating point operations (FPOs). 8 These kinds of speedup are orthogonal to hardware parallelization and are most encouraging as they pave the path for future work in efficient training.

Note that these approaches do not necessarily affect the latency to process a single example nor the compute required during inference, which is a function of the size of the computation graph.",How do recent research efforts aim to decrease wall-clock training time for BERT models?,"One area of research decreases wall-clock training time through more compute and larger batches. You et al. (2020) reduce the time of training BERT by introducing the LAMB optimizer, a large batch stochastic optimization method adjusted for attention models. Rajbhandari et al. (2020) analyze memory usage in the optimizer to enable parallelization of models resulting in higher throughput in training. By reducing the training time, models can be practically trained for longer, which has also been shown to lead to benefits in task performance (Lan et al., 2020, inter alia). Another line of research reduces the compute through attention sparsification or increasing the convergence rate (Clark et al., 2020). These works report hardware and estimate the reduction in floating point operations (FPOs). These kinds of speedup are orthogonal to hardware parallelization and are most encouraging as they pave the path for future work in efficient training. Note that these approaches do not necessarily affect the latency to process a single example nor the compute required during inference, which is a function of the size of the computation graph.",What strategies are recent research efforts employing to reduce the wall-clock training time for BERT models?,"Recent research efforts to reduce the wall-clock training time for BERT models focus on two main strategies: increasing computational efficiency and optimizing training processes. One approach involves using more compute resources and larger batch sizes. For instance, You et al. (2020) introduced the LAMB optimizer, a large batch stochastic optimization method specifically adjusted for attention models, which significantly reduces BERT training time. Additionally, Rajbhandari et al. (2020) explored memory usage in optimizers to enable model parallelization, resulting in higher training throughput. These improvements allow models to be trained for longer periods, which has been shown to enhance task performance (Lan et al., 2020, among others).

Another line of research focuses on reducing computational requirements through techniques like attention sparsification and increasing convergence rates, as discussed by Clark et al. (2020). These methods report hardware usage and estimate reductions in floating point operations (FPOs), offering speedups that are complementary to hardware parallelization. Such advancements are promising as they pave the way for more efficient training in the future. It's important to note that these strategies primarily target training efficiency and do not necessarily impact the latency of processing a single example or the compute required during inference, which depends on the size of the computation graph.",7,4,8,6,"Question:

How do recent research efforts aim to decrease wall-clock training time for BERT models?

Answer:

One area of research decreases wall-clock training time through more compute and larger batches. You et al. (2020) reduce the time of training BERT by introducing the LAMB optimizer, a large batch stochastic optimization method adjusted for attention models. Rajbhandari et al. (2020) analyze memory usage in the optimizer to enable parallelization of models resulting in higher throughput in training. By reducing the training time, models can be practically trained for longer, which has also been shown to lead to benefits in task performance (Lan et al., 2020, inter alia). Another line of research reduces the compute through attention sparsification or increasing the convergence rate (Clark et al., 2020). These works report hardware and estimate the reduction in floating point operations (FPOs). These kinds of speedup are orthogonal to hardware parallelization and are most encouraging as they pave the path for future work in efficient training. Note that these approaches do not necessarily affect the latency to process a single example nor the compute required during inference, which is a function of the size of the computation graph.","Question:

What strategies are recent research efforts employing to reduce the wall-clock training time for BERT models?

Answer:

Recent research efforts to reduce the wall-clock training time for BERT models focus on two main strategies: increasing computational efficiency and optimizing training processes. One approach involves using more compute resources and larger batch sizes. For instance, You et al. (2020) introduced the LAMB optimizer, a large batch stochastic optimization method specifically adjusted for attention models, which significantly reduces BERT training time. Additionally, Rajbhandari et al. (2020) explored memory usage in optimizers to enable model parallelization, resulting in higher training throughput. These improvements allow models to be trained for longer periods, which has been shown to enhance task performance (Lan et al., 2020, among others).

Another line of research focuses on reducing computational requirements through techniques like attention sparsification and increasing convergence rates, as discussed by Clark et al. (2020). These methods report hardware usage and estimate reductions in floating point operations (FPOs), offering speedups that are complementary to hardware parallelization. Such advancements are promising as they pave the way for more efficient training in the future. It's important to note that these strategies primarily target training efficiency and do not necessarily impact the latency of processing a single example or the compute required during inference, which depends on the size of the computation graph.",NO,True,1458,True,True
222124957-s9,Which *BERT? A Survey Organizing Contextualized Encoders,2020-10-02,Inference,"Reducing model size without impacting performance is motivated by lower inference latency, hardware memory constraints, and the promise that naively scaling up dimensions of the model will improve performance. Size reduction techniques produce smaller and faster models, while occasionally improving performance. Rogers et al. (2020) survey BERT-like models and present in Table 1 the differences in sizes and performance across several models focused on inference efficiency.

Architectural changes have been explored as one avenue for reducing either the model size or inference time. In Transformers, the self-attention pattern scales quadratically in sequence length. To reduce the asymptotic complexity, the self-attention can be sparsified: each token only attending to a small ""local"" set (Vaswani et al., 2017;Child et al., 2019;Sukhbaatar et al., 2019). This has further been applied to pretraining on longer sequences, resulting in sparse contextualized encoders Ye et al., 2019;Kitaev et al., 2020;Beltagy et al., 2020, inter alia). Efficient Transformers is an emerging subfield with applications beyond NLP; Tay et al. (2020) survey 17 Transformers that have implications on efficiency.

Another class of approaches carefully selects weights to reduce model size. Lan et al. (2020) use low-rank factorization to reduce the size of the embedding matrices, while Wang et al. (2019f) factorize other weight matrices. Additionally, parameters can be shared between layers (Dehghani et al., 2019;Lan et al., 2020) or between an encoder and decoder (Raffel et al., 2019). However, models that employ these methods do not always have smaller computation graphs. This greatly reduces the usefulness of parameter sharing compared to other methods that additionally offer greater speedups relative to the reduction in model size.

Closely related, model pruning (Denil et al., 2013;Han et al., 2015;Frankle and Carbin, 2018) during training or inference has exploited the overparameterization of neural networks by removing up to 90%-95% parameters. This approach has been successful in not only reducing the number of parameters, but also improving performance on downstream tasks. Related to efforts for pruning deep networks in computer vision (Huang et al., 2016), layer selection and dropout during both training and inference have been studied in both LSTM (Liu et al., 2018a) and Transformer (Fan et al., 2020) based encoders. These also have a regularization effect resulting in more stable training and improved performance. There are additional novel pruning methods that can be performed during training (Guo et al., 2019;. These successful results are corroborated by other efforts (Gordon et al., 2020) showing that low levels of pruning do not substantially affect pretrained representations. Additional successful efforts in model pruning directly target a downstream task (Sun et al., 2019a;Michel et al., 2019;McCarley, 2019;Cao et al., 2020a). Note that pruning does not always lead to speedups in practice as sparse operations may be hard to parallelize.

Knowledge distillation (KD) uses an overparameterized teacher model to rapidly train a smaller student model with minimal loss in performance (Hinton et al., 2015) and has been used for translation (Kim and Rush, 2016), computer vision (Howard et al., 2017), andadversarial examples (Carlini andWagner, 2016). This has been applied to ELMo  and BERT Sun et al., 2020b, inter alia). KD can also be combined with adaptive inference, which dynamically adjusts model size (Liu et al., 2020b), or performed on submodules which are later substituted back into the full model .

Quantization with custom low-precision hardware is also a promising method for both reducing the size of models and compute time, albeit it does not reduce the number of parameters or FPOs (Shen et al., 2020;Zafrir et al., 2019). This line of work is mostly orthogonal to other efforts specific to NLP.","What are the methods for reducing model size and inference time in BERT-like models, and their limitations?","Reducing model size without impacting performance is motivated by lower inference latency, hardware memory constraints, and the promise that naively scaling up dimensions of the model will improve performance. Size reduction techniques produce smaller and faster models, while occasionally improving performance. Architectural changes have been explored as one avenue for reducing either the model size or inference time. In Transformers, the self-attention pattern scales quadratically in sequence length. To reduce the asymptotic complexity, the self-attention can be sparsified: each token only attending to a small ""local"" set (Vaswani et al., 2017; Child et al., 2019; Sukhbaatar et al., 2019). This has further been applied to pretraining on longer sequences, resulting in sparse contextualized encoders (Ye et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020). Another class of approaches carefully selects weights to reduce model size. Lan et al. (2020) use low-rank factorization to reduce the size of the embedding matrices, while Wang et al. (2019f) factorize other weight matrices. Additionally, parameters can be shared between layers (Dehghani et al., 2019; Lan et al., 2020) or between an encoder and decoder (Raffel et al., 2019). However, models that employ these methods do not always have smaller computation graphs. This greatly reduces the usefulness of parameter sharing compared to other methods that additionally offer greater speedups relative to the reduction in model size. Closely related, model pruning (Denil et al., 2013; Han et al., 2015; Frankle and Carbin, 2018) during training or inference has exploited the overparameterization of neural networks by removing up to 90%-95% parameters. This approach has been successful in not only reducing the number of parameters, but also improving performance on downstream tasks. Note that pruning does not always lead to speedups in practice as sparse operations may be hard to parallelize. Knowledge distillation (KD) uses an overparameterized teacher model to rapidly train a smaller student model with minimal loss in performance (Hinton et al., 2015). Quantization with custom low-precision hardware is also a promising method for both reducing the size of models and compute time, albeit it does not reduce the number of parameters or FPOs (Shen et al., 2020; Zafrir et al., 2019).","What are the techniques for reducing the size and inference time of BERT-like models, and what are their limitations?","Reducing the size of BERT-like models without compromising performance is crucial for achieving lower inference latency and addressing hardware memory constraints. This is particularly important given the common belief that simply scaling up model dimensions will enhance performance. Various techniques have been developed to produce smaller and faster models, sometimes even improving performance.

One approach involves architectural changes, particularly in the self-attention mechanism of Transformers, which scales quadratically with sequence length. To address this, self-attention can be sparsified, allowing each token to attend only to a small ""local"" set (Vaswani et al., 2017; Child et al., 2019; Sukhbaatar et al., 2019). This method has been extended to pretraining on longer sequences, resulting in sparse contextualized encoders (Ye et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020).

Another strategy focuses on weight selection to reduce model size. For instance, Lan et al. (2020) employ low-rank factorization to shrink embedding matrices, while Wang et al. (2019f) apply factorization to other weight matrices. Additionally, parameters can be shared between layers (Dehghani et al., 2019; Lan et al., 2020) or between an encoder and decoder (Raffel et al., 2019). However, these methods do not always result in smaller computation graphs, which limits the effectiveness of parameter sharing compared to other techniques that offer greater speedups relative to model size reduction.

Model pruning is another related approach, where during training or inference, the overparameterization of neural networks is exploited by removing up to 90%-95% of parameters (Denil et al., 2013; Han et al., 2015; Frankle and Carbin, 2018). This not only reduces the number of parameters but can also enhance performance on downstream tasks. However, pruning does not always lead to practical speedups, as sparse operations can be challenging to parallelize.

Knowledge distillation (KD) is a technique where an overparameterized teacher model is used to train a smaller student model rapidly, with minimal performance loss (Hinton et al., 2015). Quantization, which involves using custom low-precision hardware, is also promising for reducing model size and compute time, although it does not decrease the number of parameters or floating-point operations (Shen et al., 2020; Zafrir et al., 2019).",8,4,8,4,"Question:

What are the methods for reducing model size and inference time in BERT-like models, and their limitations?

Answer:

Reducing model size without impacting performance is motivated by lower inference latency, hardware memory constraints, and the promise that naively scaling up dimensions of the model will improve performance. Size reduction techniques produce smaller and faster models, while occasionally improving performance. Architectural changes have been explored as one avenue for reducing either the model size or inference time. In Transformers, the self-attention pattern scales quadratically in sequence length. To reduce the asymptotic complexity, the self-attention can be sparsified: each token only attending to a small ""local"" set (Vaswani et al., 2017; Child et al., 2019; Sukhbaatar et al., 2019). This has further been applied to pretraining on longer sequences, resulting in sparse contextualized encoders (Ye et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020). Another class of approaches carefully selects weights to reduce model size. Lan et al. (2020) use low-rank factorization to reduce the size of the embedding matrices, while Wang et al. (2019f) factorize other weight matrices. Additionally, parameters can be shared between layers (Dehghani et al., 2019; Lan et al., 2020) or between an encoder and decoder (Raffel et al., 2019). However, models that employ these methods do not always have smaller computation graphs. This greatly reduces the usefulness of parameter sharing compared to other methods that additionally offer greater speedups relative to the reduction in model size. Closely related, model pruning (Denil et al., 2013; Han et al., 2015; Frankle and Carbin, 2018) during training or inference has exploited the overparameterization of neural networks by removing up to 90%-95% parameters. This approach has been successful in not only reducing the number of parameters, but also improving performance on downstream tasks. Note that pruning does not always lead to speedups in practice as sparse operations may be hard to parallelize. Knowledge distillation (KD) uses an overparameterized teacher model to rapidly train a smaller student model with minimal loss in performance (Hinton et al., 2015). Quantization with custom low-precision hardware is also a promising method for both reducing the size of models and compute time, albeit it does not reduce the number of parameters or FPOs (Shen et al., 2020; Zafrir et al., 2019).","Question:

What are the techniques for reducing the size and inference time of BERT-like models, and what are their limitations?

Answer:

Reducing the size of BERT-like models without compromising performance is crucial for achieving lower inference latency and addressing hardware memory constraints. This is particularly important given the common belief that simply scaling up model dimensions will enhance performance. Various techniques have been developed to produce smaller and faster models, sometimes even improving performance.

One approach involves architectural changes, particularly in the self-attention mechanism of Transformers, which scales quadratically with sequence length. To address this, self-attention can be sparsified, allowing each token to attend only to a small ""local"" set (Vaswani et al., 2017; Child et al., 2019; Sukhbaatar et al., 2019). This method has been extended to pretraining on longer sequences, resulting in sparse contextualized encoders (Ye et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020).

Another strategy focuses on weight selection to reduce model size. For instance, Lan et al. (2020) employ low-rank factorization to shrink embedding matrices, while Wang et al. (2019f) apply factorization to other weight matrices. Additionally, parameters can be shared between layers (Dehghani et al., 2019; Lan et al., 2020) or between an encoder and decoder (Raffel et al., 2019). However, these methods do not always result in smaller computation graphs, which limits the effectiveness of parameter sharing compared to other techniques that offer greater speedups relative to model size reduction.

Model pruning is another related approach, where during training or inference, the overparameterization of neural networks is exploited by removing up to 90%-95% of parameters (Denil et al., 2013; Han et al., 2015; Frankle and Carbin, 2018). This not only reduces the number of parameters but can also enhance performance on downstream tasks. However, pruning does not always lead to practical speedups, as sparse operations can be challenging to parallelize.

Knowledge distillation (KD) is a technique where an overparameterized teacher model is used to train a smaller student model rapidly, with minimal performance loss (Hinton et al., 2015). Quantization, which involves using custom low-precision hardware, is also promising for reducing model size and compute time, although it does not decrease the number of parameters or floating-point operations (Shen et al., 2020; Zafrir et al., 2019).",NO,True,2410,True,True
222124957-s17,Which *BERT? A Survey Organizing Contextualized Encoders,2020-10-02,Input Manipulation 13,"Input manipulation draws conclusions by recasting the probing task format into the form of the pre-training task and observing the model's predictions. As discussed in §3, word prediction (cloze task) is a popular objective. This method has been used to investigate syntactic and semantic knowledge (Goldberg, 2019; Ettinger, 2020; Kassner and Schütze, 2019). For a specific probing task, Warstadt et al. (2019) show that cloze and diagnostic classifiers draw similar conclusions. As input manipulation is not affected by variables introduced by probing tasks and is as interpretable than inspection, we suggest more focus on this method: either by creating new datasets (Warstadt et al., 2020) or recasting existing ones (Brown et al., 2020) into this format. A disadvantage of this method (especially for smaller models) is the dependence on both the pattern used to elicit an answer from the model and, in the few-shot case where a couple examples are provided first, highly dependent on the examples (Schick and Schütze, 2020).",What are the advantages and disadvantages of using input manipulation for probing tasks in contextualized encoders?,"Input manipulation draws conclusions by recasting the probing task format into the form of the pre-training task and observing the model's predictions. As input manipulation is not affected by variables introduced by probing tasks and is as interpretable than inspection, it is suggested to focus more on this method: either by creating new datasets or recasting existing ones into this format. A disadvantage of this method, especially for smaller models, is the dependence on both the pattern used to elicit an answer from the model and, in the few-shot case where a couple examples are provided first, highly dependent on the examples.",What are the pros and cons of using input manipulation for probing tasks in contextualized encoders?,"Input manipulation involves transforming the probing task into a format similar to the pre-training task and then analyzing the model's predictions. One of the main advantages of this approach is that it avoids the variables introduced by probing tasks, making it as interpretable as inspection methods. This suggests a focus on input manipulation, either by developing new datasets or by adapting existing ones to fit this format. However, a notable disadvantage, particularly for smaller models, is the method's reliance on the specific pattern used to extract an answer from the model. Additionally, in few-shot scenarios where only a few examples are provided initially, the method is highly dependent on those examples.",7,4,7,4,"Question:

What are the advantages and disadvantages of using input manipulation for probing tasks in contextualized encoders?

Answer:

Input manipulation draws conclusions by recasting the probing task format into the form of the pre-training task and observing the model's predictions. As input manipulation is not affected by variables introduced by probing tasks and is as interpretable than inspection, it is suggested to focus more on this method: either by creating new datasets or recasting existing ones into this format. A disadvantage of this method, especially for smaller models, is the dependence on both the pattern used to elicit an answer from the model and, in the few-shot case where a couple examples are provided first, highly dependent on the examples.","Question:

What are the pros and cons of using input manipulation for probing tasks in contextualized encoders?

Answer:

Input manipulation involves transforming the probing task into a format similar to the pre-training task and then analyzing the model's predictions. One of the main advantages of this approach is that it avoids the variables introduced by probing tasks, making it as interpretable as inspection methods. This suggests a focus on input manipulation, either by developing new datasets or by adapting existing ones to fit this format. However, a notable disadvantage, particularly for smaller models, is the method's reliance on the specific pattern used to extract an answer from the model. Additionally, in few-shot scenarios where only a few examples are provided initially, the method is highly dependent on those examples.",NO,True,724,True,True
52011136-s1,A review of Spanish corpora annotated with negation,2018-08-01,Negation in Spanish,"Processing negation is not as easy as using a list of negation markers and applying look-up methods. They can be used to find out potential negation cues but they are not adequate because the presence of a cue does not imply that it acts as a negation. In the sentence ""You bought the car to use it, didn't you?"" the cue ""not"" is not used as a negation but it is used to reinforce the first part of the sentence. Moreover, it is also necessary to identify the scope or part of the sentence affected by the negation and its focus, the part more prominently negated. If we want to advance in the study of this phenomenon, as for most of NLP tasks, the availability of annotated corpora is essential to train algorithms. According to existing resources for English, annotating negation involves the annotation of the following aspects:

• Negation cue: lexical item(s) that modify the truth value of the propositions that are within its scope.

There are different types of negation according to the type of the negation cue used: . It is also known as affixal negation.

• Scope: the part of the sentence affected by the negation cue . The scope can be continuous or discontinuous.

• Focus: the part of the scope that is most prominently or explicitly negated (Blanco and Moldovan, 2011).

• Negated event: the event that is directly negated by the negation cue, usually a verb, a noun or an adjective (Kim et al., 2008). This is just a list of the main aspects that have been annotated for negation. However, each language has specific linguistic resources to express negation and specific negation structures, which should also be reflected in the information annotated in corpora. As we will show in Section 4, most existing annotation schemes for Spanish do not account for the complexity of the linguistic structures used to express negation that are present in texts. This happens mainly because of two reasons: first, annotation of negation started with the annotation of clinical reports in English (Chapman et al., 2001;Goldin and Chapman, 2003;Mutalik et al., 2001a), where there is not too much variation of negation structures. Second, corpora have been created for specific purposes, such as extracting negated clinical events, and not with the intention of accounting for all the linguistic complexity of the negation phenomenon.

An exception to this is the SFU Review SP -NEG corpus (Jiménez-Zafra et al., 2018;. The guidelines specify a great variety of negation patterns at the syntactic level that we summarize below. Additionally, the guidelines also specify expressions that involve a negation cue but do not express negation.

On the one hand, patterns that express negation can be divided into three categories:

1. Simple negation markers, if they are composed of only one single negation marker (i.e. no ['no/not'], nunca ['never']).

3. Negation markers in contrastive constructions, if negation markers are used to counterpose different ideas, to correct something, to introduce new information or to express obligation, rather than to express negation (i.e. No hay más solución que comprar una lavadora ['There is no other solution than to buy a washing machine']). 4. Negation markers in comparative constructions, if negation markers are used to compare some property with something, that is, negation is used to place an entity below or above another entity on a scale (i.e. No es tan grande como me lo imaginaba ['It is not as big as I imagined']).","What are the main aspects involved in annotating negation in English, and how do they differ from Spanish?","Annotating negation in English involves the annotation of several aspects. The first aspect is the negation cue, which refers to lexical item(s) that modify the truth value of the propositions within its scope. There are different types of negation according to the type of the negation cue used, including affixal negation. The second aspect is the scope, which is the part of the sentence affected by the negation cue and can be continuous or discontinuous. The third aspect is the focus, which is the part of the scope that is most prominently or explicitly negated (Blanco and Moldovan, 2011). The fourth aspect is the negated event, which is the event directly negated by the negation cue, usually a verb, a noun, or an adjective (Kim et al., 2008). In Spanish, the complexity of linguistic structures used to express negation is not fully accounted for in most existing annotation schemes. This is mainly because annotation of negation started with clinical reports in English, where there is less variation in negation structures, and because corpora have been created for specific purposes, such as extracting negated clinical events, rather than accounting for all linguistic complexity. An exception is the SFU Review SP-NEG corpus, which specifies a variety of negation patterns at the syntactic level and includes expressions that involve a negation cue but do not express negation. Patterns that express negation in Spanish can be divided into categories such as simple negation markers, negation markers in contrastive constructions, and negation markers in comparative constructions.","What are the key elements involved in annotating negation in English, and how do these differ from the approach used for Spanish?","Annotating negation in English involves several key elements. The first is the negation cue, which consists of lexical items that alter the truth value of propositions within their scope. These cues can vary, including types like affixal negation. The second element is the scope, which refers to the part of the sentence affected by the negation cue and can be either continuous or discontinuous. The third element is the focus, which is the part of the scope that is most prominently or explicitly negated (Blanco and Moldovan, 2011). The fourth element is the negated event, which is the event directly negated by the negation cue, typically involving a verb, noun, or adjective (Kim et al., 2008).

In contrast, the annotation of negation in Spanish presents additional complexity due to the diverse linguistic structures used to express negation. Most existing annotation schemes do not fully account for this complexity, as they originated from clinical reports in English, where negation structures are less varied. Additionally, many corpora have been developed for specific purposes, such as extracting negated clinical events, rather than capturing the full linguistic complexity. An exception is the SFU Review SP-NEG corpus, which specifies a variety of negation patterns at the syntactic level and includes expressions that involve a negation cue but do not express negation. In Spanish, negation patterns can be categorized into simple negation markers, negation markers in contrastive constructions, and negation markers in comparative constructions.",7,4,7,4,"Question:

What are the main aspects involved in annotating negation in English, and how do they differ from Spanish?

Answer:

Annotating negation in English involves the annotation of several aspects. The first aspect is the negation cue, which refers to lexical item(s) that modify the truth value of the propositions within its scope. There are different types of negation according to the type of the negation cue used, including affixal negation. The second aspect is the scope, which is the part of the sentence affected by the negation cue and can be continuous or discontinuous. The third aspect is the focus, which is the part of the scope that is most prominently or explicitly negated (Blanco and Moldovan, 2011). The fourth aspect is the negated event, which is the event directly negated by the negation cue, usually a verb, a noun, or an adjective (Kim et al., 2008). In Spanish, the complexity of linguistic structures used to express negation is not fully accounted for in most existing annotation schemes. This is mainly because annotation of negation started with clinical reports in English, where there is less variation in negation structures, and because corpora have been created for specific purposes, such as extracting negated clinical events, rather than accounting for all linguistic complexity. An exception is the SFU Review SP-NEG corpus, which specifies a variety of negation patterns at the syntactic level and includes expressions that involve a negation cue but do not express negation. Patterns that express negation in Spanish can be divided into categories such as simple negation markers, negation markers in contrastive constructions, and negation markers in comparative constructions.","Question:

What are the key elements involved in annotating negation in English, and how do these differ from the approach used for Spanish?

Answer:

Annotating negation in English involves several key elements. The first is the negation cue, which consists of lexical items that alter the truth value of propositions within their scope. These cues can vary, including types like affixal negation. The second element is the scope, which refers to the part of the sentence affected by the negation cue and can be either continuous or discontinuous. The third element is the focus, which is the part of the scope that is most prominently or explicitly negated (Blanco and Moldovan, 2011). The fourth element is the negated event, which is the event directly negated by the negation cue, typically involving a verb, noun, or adjective (Kim et al., 2008).

In contrast, the annotation of negation in Spanish presents additional complexity due to the diverse linguistic structures used to express negation. Most existing annotation schemes do not fully account for this complexity, as they originated from clinical reports in English, where negation structures are less varied. Additionally, many corpora have been developed for specific purposes, such as extracting negated clinical events, rather than capturing the full linguistic complexity. An exception is the SFU Review SP-NEG corpus, which specifies a variety of negation patterns at the syntactic level and includes expressions that involve a negation cue but do not express negation. In Spanish, negation patterns can be categorized into simple negation markers, negation markers in contrastive constructions, and negation markers in comparative constructions.",NO,True,1565,True,True
47019063-s7,Diachronic word embeddings and semantic shifts: a survey,2018-06-09,Comparing vectors across time,"It is rather straightforward to train separate word embedding models using time-specific corpora containing texts from several different time periods. As a consequence, these models are also time-specific. However, it is not that straightforward to compare word vectors across different models.

It usually does not make sense to, for example, directly calculate cosine similarities between embeddings of one and the same word in two different models. The reason is that most modern word embedding algorithms are inherently stochastic and the resulting embedding sets are invariant under rotation. Thus, even when trained on the same data, separate learning runs will produce entirely different numerical vectors (though with roughly the same pairwise similarities between vectors for particular words). This is expressed even stronger for models trained on different corpora. It means that even if word meaning is completely stable, the direct cosine similarity between its vectors from different time periods can still be quite low, simply because the random initializations of the two models were different. To alleviate this, Kulkarni et al. (2015) suggested that before calculating similarities, one should first align the models to fit them in one vector space, using linear transformations preserving general vector space structure. After that, cosine similarities across models become meaningful and can be used as indicators of semantic shifts. They also proposed constructing the time series of a word embedding over time, which allows for the detection of 'bursts' in its meaning with the Mean Shift model (Taylor, 2000). Notably, almost simultaneously the idea of aligning diachronic word embedding models using a distance-preserving projection technique was proposed by Zhang et al. (2015). Later, Zhang et al. (2016) expanded on this by adding the so called 'local anchors': that is, they used both linear projections for the whole models and small sets of nearest neighbors for mapping the query words to their correct temporal counterparts.

Instead of aligning their diachronic models using linear transformations, Eger and Mehler (2016) compared word meaning using so-called 'second-order embeddings,' that is, the vectors of words' similarities to all other words in the shared vocabulary of all models. This approach does not require any transformations: basically, one simply analyzes the word's position compared to other words. At the same time, Hamilton et al. (2016a) and Hamilton et al. (2016c) showed that these two approaches can be used simultaneously: they employed both 'second order embeddings' and orthogonal Procrustes transformations to align diachronic models.

Recently, it was shown in Bamler and Mandt (2017) ('dynamic skip-gram' model) and Yao et al. (2018) ('dynamic Word2Vec' model) that it is possible to learn the word embeddings across several time periods jointly, enforcing alignment across all of them simultaneously, and positioning all the models in the same vector space in one step. This develops the idea of model alignment even further and eliminates the need to first learn separate embeddings for each time period, and then align subsequent model pairs. Bamler and Mandt (2017) additionally describe two variations of their approach: a) for the cases when data slices arrive sequentially, as in streaming applications, where one can not use future observations, and b) for the cases when data slices are available all at once, allowing for training on the whole sequence from the very beginning. A similar approach is taken by Rosenfeld and Erk (2018) who train a deep neural network on word and time representations. Word vectors in this setup turn into linear transformations applied to a continuous time variable, and thus producing an embedding of word w at time t.

Yet another way to make the models comparable is made possible by the fact that prediction-based word embedding approaches (as well as RI) allow for incremental updates of the models with new data without any modifications. This is not the case for the traditional explicit count-based algorithms, which usually require a computationally expensive dimensionality reduction step. Kim et al. (2014) proposed the idea of incrementally updated diachronic embedding models: that is, they train a model on the year y i , and then the model for the year y i+1 is initialized with the word vectors from y i . This can be considered as an alternative to model alignment: instead of aligning models trained from scratch on different time periods, one starts with training a model on the diachronically first period, and then updates this same model with the data from the successive time periods, saving its state each time. Thus, all the models are inherently related to each other, which, again, makes it possible to directly calculate cosine similarities between the same word in different time period models, or at least makes the models more comparable.

Several works have appeared recently which aim to address the technical issues accompanying this approach of incremental updating. Among others, Peng et al. (2017) described a novel method of incrementally learning the hierarchical softmax function for the CBOW and Continuous Skipgram algorithms. In this way, one can update word embedding models with new data and new vocabulary much more efficiently, achieving faster training than when doing it from scratch, while at the same time preserving comparable performance. Continuing this line of research, Kaji and Kobayashi (2017) proposed a conceptually similar incremental extension for negative sampling, which is a method of training examples selection, widely used with prediction-based models as a faster replacement for hierarchical softmax.

Even after the models for different time periods are made comparable in this or that way, one still has to choose the exact method of comparing word vectors across these models. Hamilton et al. (2016a) and Hamilton et al. (2016c) made an important observation that the distinction between linguistic and cultural semantic shifts is correlated with the distinction between global and local embedding comparison methods. The former take into account the whole model (for example, 'second-order embeddings,' when we compare the word's similarities to all other words in the lexicon), while the latter focus on the word's immediate neighborhood (for example, when comparing the lists of k nearest neighbors). They concluded that global measures are sensitive to regular processes of linguistic shifts, while local measures are better suited to detect slight cultural shifts in word meaning. Thus, the choice of particular embedding comparison approach should depend on what type of semantic shifts one seeks to detect.",How can the challenge of comparing word vectors across different time-specific models be addressed?,"It is rather straightforward to train separate word embedding models using time-specific corpora containing texts from several different time periods. However, it is not that straightforward to compare word vectors across different models. Most modern word embedding algorithms are inherently stochastic and the resulting embedding sets are invariant under rotation. Thus, even when trained on the same data, separate learning runs will produce entirely different numerical vectors. To alleviate this, Kulkarni et al. (2015) suggested that before calculating similarities, one should first align the models to fit them in one vector space, using linear transformations preserving general vector space structure. After that, cosine similarities across models become meaningful and can be used as indicators of semantic shifts. Zhang et al. (2015) proposed aligning diachronic word embedding models using a distance-preserving projection technique. Zhang et al. (2016) expanded on this by adding 'local anchors': they used both linear projections for the whole models and small sets of nearest neighbors for mapping the query words to their correct temporal counterparts. Instead of aligning their diachronic models using linear transformations, Eger and Mehler (2016) compared word meaning using 'second-order embeddings,' that is, the vectors of words' similarities to all other words in the shared vocabulary of all models. Hamilton et al. (2016a) and Hamilton et al. (2016c) showed that these two approaches can be used simultaneously: they employed both 'second order embeddings' and orthogonal Procrustes transformations to align diachronic models. Bamler and Mandt (2017) and Yao et al. (2018) showed that it is possible to learn the word embeddings across several time periods jointly, enforcing alignment across all of them simultaneously, and positioning all the models in the same vector space in one step. Kim et al. (2014) proposed the idea of incrementally updated diachronic embedding models: they train a model on the year y_i, and then the model for the year y_i+1 is initialized with the word vectors from y_i. This can be considered as an alternative to model alignment: instead of aligning models trained from scratch on different time periods, one starts with training a model on the diachronically first period, and then updates this same model with the data from the successive time periods, saving its state each time. Thus, all the models are inherently related to each other, which makes it possible to directly calculate cosine similarities between the same word in different time period models.",How can we effectively compare word vectors from different time-specific models?,"Training separate word embedding models using time-specific corpora is relatively straightforward, but comparing word vectors across these models presents a challenge. Modern word embedding algorithms are inherently stochastic, and the resulting embeddings are invariant under rotation. This means that even when trained on the same data, different runs can produce entirely different numerical vectors. To address this, Kulkarni et al. (2015) suggested aligning the models using linear transformations that preserve the general vector space structure before calculating similarities. This alignment allows cosine similarities across models to become meaningful indicators of semantic shifts. Zhang et al. (2015) proposed a distance-preserving projection technique for aligning diachronic word embedding models, and Zhang et al. (2016) expanded on this by introducing 'local anchors,' which use both linear projections for entire models and small sets of nearest neighbors for mapping query words to their correct temporal counterparts.

Instead of using linear transformations for alignment, Eger and Mehler (2016) compared word meanings using 'second-order embeddings,' which are vectors of words' similarities to all other words in the shared vocabulary of all models. Hamilton et al. (2016a) and Hamilton et al. (2016c) demonstrated that both 'second-order embeddings' and orthogonal Procrustes transformations can be used simultaneously to align diachronic models. Bamler and Mandt (2017) and Yao et al. (2018) showed that it is possible to learn word embeddings across several time periods jointly, enforcing alignment across all of them simultaneously and positioning all models in the same vector space in one step.

Kim et al. (2014) introduced the concept of incrementally updated diachronic embedding models. In this approach, a model is trained on data from year y_i, and then the model for year y_i+1 is initialized with the word vectors from y_i. This method serves as an alternative to model alignment: instead of aligning models trained from scratch on different time periods, one starts with training a model on the earliest period and then updates it with data from successive periods, saving its state each time. This inherently relates all models to each other, allowing for direct calculation of cosine similarities between the same word in different time period models.",7,4,8,4,"Question:

How can the challenge of comparing word vectors across different time-specific models be addressed?

Answer:

It is rather straightforward to train separate word embedding models using time-specific corpora containing texts from several different time periods. However, it is not that straightforward to compare word vectors across different models. Most modern word embedding algorithms are inherently stochastic and the resulting embedding sets are invariant under rotation. Thus, even when trained on the same data, separate learning runs will produce entirely different numerical vectors. To alleviate this, Kulkarni et al. (2015) suggested that before calculating similarities, one should first align the models to fit them in one vector space, using linear transformations preserving general vector space structure. After that, cosine similarities across models become meaningful and can be used as indicators of semantic shifts. Zhang et al. (2015) proposed aligning diachronic word embedding models using a distance-preserving projection technique. Zhang et al. (2016) expanded on this by adding 'local anchors': they used both linear projections for the whole models and small sets of nearest neighbors for mapping the query words to their correct temporal counterparts. Instead of aligning their diachronic models using linear transformations, Eger and Mehler (2016) compared word meaning using 'second-order embeddings,' that is, the vectors of words' similarities to all other words in the shared vocabulary of all models. Hamilton et al. (2016a) and Hamilton et al. (2016c) showed that these two approaches can be used simultaneously: they employed both 'second order embeddings' and orthogonal Procrustes transformations to align diachronic models. Bamler and Mandt (2017) and Yao et al. (2018) showed that it is possible to learn the word embeddings across several time periods jointly, enforcing alignment across all of them simultaneously, and positioning all the models in the same vector space in one step. Kim et al. (2014) proposed the idea of incrementally updated diachronic embedding models: they train a model on the year y_i, and then the model for the year y_i+1 is initialized with the word vectors from y_i. This can be considered as an alternative to model alignment: instead of aligning models trained from scratch on different time periods, one starts with training a model on the diachronically first period, and then updates this same model with the data from the successive time periods, saving its state each time. Thus, all the models are inherently related to each other, which makes it possible to directly calculate cosine similarities between the same word in different time period models.","Question:

How can we effectively compare word vectors from different time-specific models?

Answer:

Training separate word embedding models using time-specific corpora is relatively straightforward, but comparing word vectors across these models presents a challenge. Modern word embedding algorithms are inherently stochastic, and the resulting embeddings are invariant under rotation. This means that even when trained on the same data, different runs can produce entirely different numerical vectors. To address this, Kulkarni et al. (2015) suggested aligning the models using linear transformations that preserve the general vector space structure before calculating similarities. This alignment allows cosine similarities across models to become meaningful indicators of semantic shifts. Zhang et al. (2015) proposed a distance-preserving projection technique for aligning diachronic word embedding models, and Zhang et al. (2016) expanded on this by introducing 'local anchors,' which use both linear projections for entire models and small sets of nearest neighbors for mapping query words to their correct temporal counterparts.

Instead of using linear transformations for alignment, Eger and Mehler (2016) compared word meanings using 'second-order embeddings,' which are vectors of words' similarities to all other words in the shared vocabulary of all models. Hamilton et al. (2016a) and Hamilton et al. (2016c) demonstrated that both 'second-order embeddings' and orthogonal Procrustes transformations can be used simultaneously to align diachronic models. Bamler and Mandt (2017) and Yao et al. (2018) showed that it is possible to learn word embeddings across several time periods jointly, enforcing alignment across all of them simultaneously and positioning all models in the same vector space in one step.

Kim et al. (2014) introduced the concept of incrementally updated diachronic embedding models. In this approach, a model is trained on data from year y_i, and then the model for year y_i+1 is initialized with the word vectors from y_i. This method serves as an alternative to model alignment: instead of aligning models trained from scratch on different time periods, one starts with training a model on the earliest period and then updates it with data from successive periods, saving its state each time. This inherently relates all models to each other, allowing for direct calculation of cosine similarities between the same word in different time period models.",NO,True,2391,True,True
47019063-s9,Diachronic word embeddings and semantic shifts: a survey,2018-06-09,Diachronic semantic relations,"Word embedding models are known to successfully capture complex relationships between concepts, as manifested in the well-known word analogies task (Mikolov et al., 2013a), where a model must 'solve' equations of the form 'A is to B is as C is to what?' A famous example is the distributional model capturing the fact that the relation between 'man' and 'woman' is the same as between 'king' and 'queen' (by adding and subtracting the corresponding word vectors). Thus, it is a natural development to investigate whether changes in semantic relationships across time can also be traced by looking at the diachronic development of distributional models. Zhang et al. (2015) considered the temporal correspondences problem, wherein the objective is to identify the word in a target time period which corresponds to a query term in the source time period (for example, given the query term iPod in the 2000s, the counterpart term in the 1980s time period is Walkman). This is proposed as a means to improve the results of information retrieval from document collections with significant time spans. Szymanski (2017) frames this as the temporal word analogy problem, extending the word analogies concept into the temporal dimension. This work shows that diachronic word embeddings can successfully model relations like 'word w 1 at time period t α is like word w 2 at time period t β '. To this end, embedding models trained on different time periods are aligned using linear transformations. Then, the temporal analogies are solved by simply finding out which word vector in the time period t β is the closest to the vector of w 1 in the time period t α . A variation of this task was studied in Rosin et al. (2017), where the authors learn the relatedness of words over time, answering queries like 'in which time period were the words Obama and president maximally related'. This technique can be used for a more efficient user query expansion in generalpurpose search engines. Kutuzov et al. (2017a) modeled a different semantic relation: 'words w 1 and w 2 at time period t α are in the same semantic relation as words w 3 and w 4 at time period t β '. To trace the temporal dynamics of these relations, they re-applied linear projections learned on sets of w 1 and w 2 pairs from the model for the period t n to the model trained on the subsequent time period t n+1 . This was used to solve the task of detecting lasting or emerging armed conflicts and the violent groups involved in these conflicts.",How do diachronic word embeddings model and solve temporal analogies across different time periods?,"Word embedding models are known to successfully capture complex relationships between concepts, as manifested in the well-known word analogies task (Mikolov et al., 2013a), where a model must 'solve' equations of the form 'A is to B is as C is to what?' Zhang et al. (2015) considered the temporal correspondences problem, wherein the objective is to identify the word in a target time period which corresponds to a query term in the source time period. Szymanski (2017) frames this as the temporal word analogy problem, extending the word analogies concept into the temporal dimension. This work shows that diachronic word embeddings can successfully model relations like 'word w1 at time period tα is like word w2 at time period tβ'. To this end, embedding models trained on different time periods are aligned using linear transformations. Then, the temporal analogies are solved by simply finding out which word vector in the time period tβ is the closest to the vector of w1 in the time period tα.",How do diachronic word embeddings address and solve temporal analogies across different time periods?,"Diachronic word embeddings are designed to capture the evolution of word meanings over time, allowing them to address temporal analogies effectively. This concept extends the traditional word analogy task, where models solve equations like 'A is to B as C is to what?' (Mikolov et al., 2013a). In the context of temporal analogies, the goal is to identify a word in a target time period that corresponds to a query term from a source time period. Zhang et al. (2015) explored this as the temporal correspondences problem, while Szymanski (2017) framed it as the temporal word analogy problem, extending the analogy concept into the temporal dimension. Diachronic word embeddings achieve this by aligning models trained on different time periods using linear transformations. This alignment allows the model to find the word vector in the target time period that is closest to the vector of the query word from the source time period, effectively solving the temporal analogy.",7,4,7,6,"Question:

How do diachronic word embeddings model and solve temporal analogies across different time periods?

Answer:

Word embedding models are known to successfully capture complex relationships between concepts, as manifested in the well-known word analogies task (Mikolov et al., 2013a), where a model must 'solve' equations of the form 'A is to B is as C is to what?' Zhang et al. (2015) considered the temporal correspondences problem, wherein the objective is to identify the word in a target time period which corresponds to a query term in the source time period. Szymanski (2017) frames this as the temporal word analogy problem, extending the word analogies concept into the temporal dimension. This work shows that diachronic word embeddings can successfully model relations like 'word w1 at time period tα is like word w2 at time period tβ'. To this end, embedding models trained on different time periods are aligned using linear transformations. Then, the temporal analogies are solved by simply finding out which word vector in the time period tβ is the closest to the vector of w1 in the time period tα.","Question:

How do diachronic word embeddings address and solve temporal analogies across different time periods?

Answer:

Diachronic word embeddings are designed to capture the evolution of word meanings over time, allowing them to address temporal analogies effectively. This concept extends the traditional word analogy task, where models solve equations like 'A is to B as C is to what?' (Mikolov et al., 2013a). In the context of temporal analogies, the goal is to identify a word in a target time period that corresponds to a query term from a source time period. Zhang et al. (2015) explored this as the temporal correspondences problem, while Szymanski (2017) framed it as the temporal word analogy problem, extending the analogy concept into the temporal dimension. Diachronic word embeddings achieve this by aligning models trained on different time periods using linear transformations. This alignment allows the model to find the word vector in the target time period that is closest to the vector of the query word from the source time period, effectively solving the temporal analogy.",NO,True,975,True,True
47019063-s10,Diachronic word embeddings and semantic shifts: a survey,2018-06-09,Applications,"Applications of diachronic word embeddings approaches can generally be grouped into two broad categories: linguistic studies which investigate the how and why of semantic shifts, and event detection approaches which mine text data for actionable purposes.

The first category generally involves corpora with longer time spans, since linguistic changes happen at a relatively slow pace. Some examples falling into this category include tracking semantic drift of particular words (Kulkarni et al., 2015) or of word sentiment (Hamilton et al., 2016b), identifying the breakpoints between epochs (Sagi et al., 2011;Mihalcea and Nastase, 2012), studying the laws of semantic change at scale (Hamilton et al., 2016c) and finding different words with similar meanings at different points in time (Szymanski, 2017). This has been held up as a good use case of deep learning for research in computational linguistics (Manning, 2015), and there are opportunities for future work applying diachronic word embeddings not only in the field of historical linguistics, but also in related areas like sociolinguistics and digital humanities.

The second category involves mining texts for cultural semantic shifts (usually on shorter time spans) indicating real-world events. Examples of this category are temporal information retrieval (Rosin et al., 2017), predicting civil turmoils (Kutuzov et al., 2017b;Mueller and Rauh, 2017), or tracing the popularity of entities using norms of word vectors (Yao et al., 2018). They can potentially be employed to improve user experience in production systems or for policy-making in governmental structures.

We believe that the near future will see a more diverse landscape of applications for diachronic word embeddings, especially related to the real-time analysis of large-scale news streams. 'Between the lines,' these data sources contain a tremendous amount of information about processes in our world, manifested in semantic shifts of various sorts. The task of researchers is to reveal this information and make it reliable and practically useful.","What are the two broad categories of applications for diachronic word embeddings, and what do they involve?","Applications of diachronic word embeddings approaches can generally be grouped into two broad categories: linguistic studies which investigate the how and why of semantic shifts, and event detection approaches which mine text data for actionable purposes. The first category generally involves corpora with longer time spans, since linguistic changes happen at a relatively slow pace. Some examples falling into this category include tracking semantic drift of particular words (Kulkarni et al., 2015) or of word sentiment (Hamilton et al., 2016b), identifying the breakpoints between epochs (Sagi et al., 2011; Mihalcea and Nastase, 2012), studying the laws of semantic change at scale (Hamilton et al., 2016c) and finding different words with similar meanings at different points in time (Szymanski, 2017). The second category involves mining texts for cultural semantic shifts (usually on shorter time spans) indicating real-world events. Examples of this category are temporal information retrieval (Rosin et al., 2017), predicting civil turmoils (Kutuzov et al., 2017b; Mueller and Rauh, 2017), or tracing the popularity of entities using norms of word vectors (Yao et al., 2018).","What are the two main categories of applications for diachronic word embeddings, and what do they entail?","Applications of diachronic word embeddings can be broadly divided into two main categories: linguistic studies and event detection. The first category, linguistic studies, focuses on understanding the how and why of semantic shifts over time. This typically involves analyzing corpora that span longer time periods, as linguistic changes occur relatively slowly. Examples in this category include tracking the semantic drift of specific words (Kulkarni et al., 2015) or word sentiment (Hamilton et al., 2016b), identifying breakpoints between different epochs (Sagi et al., 2011; Mihalcea and Nastase, 2012), studying the laws of semantic change on a large scale (Hamilton et al., 2016c), and finding words with similar meanings at different times (Szymanski, 2017).

The second category, event detection, involves mining text data for cultural semantic shifts that often indicate real-world events, usually over shorter time spans. Examples of this category include temporal information retrieval (Rosin et al., 2017), predicting civil unrest (Kutuzov et al., 2017b; Mueller and Rauh, 2017), and tracing the popularity of entities using norms of word vectors (Yao et al., 2018).",7,2,8,2,"Question:

What are the two broad categories of applications for diachronic word embeddings, and what do they involve?

Answer:

Applications of diachronic word embeddings approaches can generally be grouped into two broad categories: linguistic studies which investigate the how and why of semantic shifts, and event detection approaches which mine text data for actionable purposes. The first category generally involves corpora with longer time spans, since linguistic changes happen at a relatively slow pace. Some examples falling into this category include tracking semantic drift of particular words (Kulkarni et al., 2015) or of word sentiment (Hamilton et al., 2016b), identifying the breakpoints between epochs (Sagi et al., 2011; Mihalcea and Nastase, 2012), studying the laws of semantic change at scale (Hamilton et al., 2016c) and finding different words with similar meanings at different points in time (Szymanski, 2017). The second category involves mining texts for cultural semantic shifts (usually on shorter time spans) indicating real-world events. Examples of this category are temporal information retrieval (Rosin et al., 2017), predicting civil turmoils (Kutuzov et al., 2017b; Mueller and Rauh, 2017), or tracing the popularity of entities using norms of word vectors (Yao et al., 2018).","Question:

What are the two main categories of applications for diachronic word embeddings, and what do they entail?

Answer:

Applications of diachronic word embeddings can be broadly divided into two main categories: linguistic studies and event detection. The first category, linguistic studies, focuses on understanding the how and why of semantic shifts over time. This typically involves analyzing corpora that span longer time periods, as linguistic changes occur relatively slowly. Examples in this category include tracking the semantic drift of specific words (Kulkarni et al., 2015) or word sentiment (Hamilton et al., 2016b), identifying breakpoints between different epochs (Sagi et al., 2011; Mihalcea and Nastase, 2012), studying the laws of semantic change on a large scale (Hamilton et al., 2016c), and finding words with similar meanings at different times (Szymanski, 2017).

The second category, event detection, involves mining text data for cultural semantic shifts that often indicate real-world events, usually over shorter time spans. Examples of this category include temporal information retrieval (Rosin et al., 2017), predicting civil unrest (Kutuzov et al., 2017b; Mueller and Rauh, 2017), and tracing the popularity of entities using norms of word vectors (Yao et al., 2018).",NO,True,1179,True,True
49587276-s4,A Survey on Recent Advances in Named Entity Recognition from Deep Learning models,2018-08-01,NER evaluation metrics,"The exact match metrics introduced by CoNLL (Tjong Kim Sang and De Meulder, 2003;Tjong Kim Sang, 2002) considers a prediction to be correct only when the predicted label for the complete entity is matched to exactly the same words as the gold label of that entity. CoNLL also used (micro) F-score, taking the harmonic mean of the exact match precision and recall.

The relaxed F1 and strict F1 metrics have been used in many NER shared tasks (Segura Bedmar et al., 2013;Krallinger et al., 2015;Bossy et al., 2013;Delėger et al., 2016). Relaxed F1 considers a prediction to be correct as long as part of the named entity is identified correctly. Strict F1 requires the character offsets of a prediction and the human annotation to match exactly. In these data, unlike CoNLL, word offsets are not given, so relaxed F1 is intended to allow comparison despite different systems having different word boundaries due to different segmentation techniques (Liu et al., 2015).",How do relaxed F1 and strict F1 metrics differ in evaluating NER predictions?,"The relaxed F1 and strict F1 metrics have been used in many NER shared tasks (Segura Bedmar et al., 2013; Krallinger et al., 2015; Bossy et al., 2013; Delėger et al., 2016). Relaxed F1 considers a prediction to be correct as long as part of the named entity is identified correctly. Strict F1 requires the character offsets of a prediction and the human annotation to match exactly. In these data, unlike CoNLL, word offsets are not given, so relaxed F1 is intended to allow comparison despite different systems having different word boundaries due to different segmentation techniques (Liu et al., 2015).",What are the differences between relaxed F1 and strict F1 metrics in evaluating NER predictions?,"The relaxed F1 and strict F1 metrics are commonly used in Named Entity Recognition (NER) shared tasks, as seen in studies by Segura Bedmar et al. (2013), Krallinger et al. (2015), Bossy et al. (2013), and Delėger et al. (2016). The key difference between these metrics lies in how they define a correct prediction. Relaxed F1 considers a prediction correct if any part of the named entity is identified accurately. In contrast, strict F1 demands an exact match between the character offsets of the prediction and the human annotation. This distinction is particularly important in datasets where word offsets are not provided, unlike in CoNLL datasets. The relaxed F1 metric is designed to facilitate comparison across different systems that may use varying segmentation techniques, as noted by Liu et al. (2015).",7,2,8,2,"Question:

How do relaxed F1 and strict F1 metrics differ in evaluating NER predictions?

Answer:

The relaxed F1 and strict F1 metrics have been used in many NER shared tasks (Segura Bedmar et al., 2013; Krallinger et al., 2015; Bossy et al., 2013; Delėger et al., 2016). Relaxed F1 considers a prediction to be correct as long as part of the named entity is identified correctly. Strict F1 requires the character offsets of a prediction and the human annotation to match exactly. In these data, unlike CoNLL, word offsets are not given, so relaxed F1 is intended to allow comparison despite different systems having different word boundaries due to different segmentation techniques (Liu et al., 2015).","Question:

What are the differences between relaxed F1 and strict F1 metrics in evaluating NER predictions?

Answer:

The relaxed F1 and strict F1 metrics are commonly used in Named Entity Recognition (NER) shared tasks, as seen in studies by Segura Bedmar et al. (2013), Krallinger et al. (2015), Bossy et al. (2013), and Delėger et al. (2016). The key difference between these metrics lies in how they define a correct prediction. Relaxed F1 considers a prediction correct if any part of the named entity is identified accurately. In contrast, strict F1 demands an exact match between the character offsets of the prediction and the human annotation. This distinction is particularly important in datasets where word offsets are not provided, unlike in CoNLL datasets. The relaxed F1 metric is designed to facilitate comparison across different systems that may use varying segmentation techniques, as noted by Liu et al. (2015).",NO,True,813,True,True
49587276-s9,A Survey on Recent Advances in Named Entity Recognition from Deep Learning models,2018-08-01,Character level architectures,"In this model, a sentence is taken to be a sequence of characters. This sequence is passed through an RNN, predicting labels for each character (Figure 2). Character labels transformed into word labels via post processing. The potential of character NER neural models was first highlighted by Kim et al. (2016) using highway networks over convolution neural networks (CNN) on character sequences of words and then using another layer of LSTM + softmax for the final predictions.

This model was implemented by Pham and Le-Hong (2017) for Vietnamese NER and achieved 80.23% F-score on Nguyen et al. (2016)'s Vietnamese test data. Character models were also used in various other languages like Chinese (Dong et al., 2016) where it has achieved near state of the art performance. Kuru et al. (2016) proposed CharNER 11 which implemented the character RNN model for NER on 7 different languages. In this character model, tag prediction over characters were converted to word tags using Viterbi decoder (Forney, 1973) Ling et al. (2015) proposed word representation using RNN (Bi-LSTM) over characters of the word and achieved state of the art results on POS task using this representation in multiple languages including 97.78% accuracy on English PTB (Marcus et al., 1993). Gillick et al. (2015) implemented sequence to sequence model (Byte to Span-BTS) using encoder decoder architecture over sequence of characters of words in a window of 60 characters. Each character was encoded in bytes and BTS achieved high performance on CoNLL 2002 and 2003 dataset without any feature engineering. BTS achieved 82.95%, 82.84%,86.50%,76.22% Fscore on Spanish, Dutch, English and German CoNLL datasets respectively.",How do character-level architectures transform character labels into word labels in NER models?,"In character-level architectures for NER models, a sentence is taken to be a sequence of characters. This sequence is passed through an RNN, predicting labels for each character. Character labels are then transformed into word labels via post-processing. In one character model, tag prediction over characters is converted to word tags using a Viterbi decoder (Forney, 1973).",How are character labels converted into word labels in character-level architectures for NER models?,"In character-level architectures for Named Entity Recognition (NER) models, the process begins by treating a sentence as a sequence of individual characters. This character sequence is then processed through a Recurrent Neural Network (RNN), which predicts labels for each character. To transform these character-level labels into word-level labels, a post-processing step is employed. One approach involves using a Viterbi decoder (Forney, 1973) to convert the predicted character tags into corresponding word tags.",7,2,7,2,"Question:

How do character-level architectures transform character labels into word labels in NER models?

Answer:

In character-level architectures for NER models, a sentence is taken to be a sequence of characters. This sequence is passed through an RNN, predicting labels for each character. Character labels are then transformed into word labels via post-processing. In one character model, tag prediction over characters is converted to word tags using a Viterbi decoder (Forney, 1973).","Question:

How are character labels converted into word labels in character-level architectures for NER models?

Answer:

In character-level architectures for Named Entity Recognition (NER) models, the process begins by treating a sentence as a sequence of individual characters. This character sequence is then processed through a Recurrent Neural Network (RNN), which predicts labels for each character. To transform these character-level labels into word-level labels, a post-processing step is employed. One approach involves using a Viterbi decoder (Forney, 1973) to convert the predicted character tags into corresponding word tags.",NO,True,516,True,True
49587276-s10,A Survey on Recent Advances in Named Entity Recognition from Deep Learning models,2018-08-01,Character+Word level architectures,"Systems combining word context and the characters of a word have proved to be strong NER systems that need little domain specific knowledge or resources. There are two base models in this category. The first type of model represents words as a combination of a word embedding and a convolution over the characters of the word, follows this with a Bi-LSTM layer over the word representations of a sentence, and finally uses a softmax or CRF layer over the Bi-LSTM to generate labels. The architecture diagram for this model is same as Figure 3 but with the character Bi-LSTM replaced with a CNN 12 . Ma and Hovy (2016) implemented this model to achieve 91.21% F1 score on the CoNLL 2003 English dataset and 97.55% POS-tagging accuracy on the WSJ portion of PTB (Marcus et al., 1993). They also showed lower performance by this model for out of vocabulary words.

Chiu and Nichols (2015)  This model has also been utilized for NER in languages like Japanese where Misawa et al. (2017) showed that this architecture outperformed other neural architectures on the organization entity class. Limsopatham and Collier (2016) implemented a character+word level NER model for Twitter NER (Baldwin et al., 2015) by concatenating a CNN over characters, a CNN over orthographic features of characters, a word embedding, and a word orthographic feature embedding. This concatenated representation is passed through another Bi-LSTM layer and the output is given to CRF for predicting. This model achieved 65.89% F score on segmentation alone and 52.41% F score on segmentation and categorization. Santos and Guimaraes (2015) implemented a model with a CNN over the characters of word, concatenated with word embeddings of the central word and its neighbors, fed to a feed forward network, and followed by the Viterbi algorithm to predict labels for each word. The model achieved 82.21% F score on Spanish CoNLL 2002 data and 71.23% F score on Portuguese NER data (Santos and Cardoso, 2007).

The second type of model concatenates word embeddings with LSTMs (sometimes bi-directional) over the characters of a word, passing this representation through another sentence-level Bi-LSTM, and predicting the final tags using a final softmax or CRF layer (Figure 3). Lample et al. (2016) 13 introduced this architecture and achieved 85.75%, 81.74%, 90.94%, 78.76% Fscores on Spanish, Dutch, English and German NER dataset respectively from CoNLL 2002 and 2003. Dernoncourt et al. (2017) implemented this model in the NeuroNER toolkit 14 with the main goal of providing easy usability and allowing easy plotting of real time performance and learning statistics of the model. The BRAT annotation tool 15 is also integrated with NeuroNER to ease the development of NN NER models in new domains. NeuroNER achieved 90.50% F score on the English CoNLL 2003 data. Habibi et al. (2017) implemented the model for various biomedical NER tasks and achieved higher performance than the majority of other participants. For example, they achieved 83.71 F-score on the CHEMDNER data (Krallinger et al., 2015). Bharadwaj et al. (2016) 16 utilized phonemes (from Epitran) for NER in addition to characters and words. They also utilize attention knowledge over sequence of characters in word which is concatenated with the word embedding and character representation of word. This model achieved state of the art performance (85.81% F score) on Spanish CoNLL 2002 dataset. A slightly improved system focusing on multi-task and multi-lingual joint learning was proposed by Yang et al. (2016) where word representation given by GRU (Gated Recurrent Unit) cell over characters plus word embedding was passed through another RNN layer and the output was given to CRF models trained for different tasks like POS, chunking and NER. Yang et al. (2017) Kim Sang, 2002;Cucerzan and Yarowsky, 2002(Tjong Kim Sang and De Meulder, 2003 and for biomedical NER (Saha et al., 2009), but had not been used in neural NER systems. They extended the Lample et al. (2016) character+word model to learn affix embeddings 17 alongside the word embeddings and character RNNs (Figure 4). They considered all n-gram prefixes and suffixes of words in the training corpus, and selected only those whose frequency was above a threshold, T . Their word+character+affix model achieved 87.26%, 87.54%, 90.86%, 79.01% on Spanish, Dutch, English and German CoNLL datasets respectively. Yadav et al. (2018) also showed that affix embeddings capture complementary information to that captured by RNNs over the characters of a word, that selecting only high frequency (realistic) affixes was important, and that embedding affixes was better than simply expanding the other embeddings to reach a similar number of hyper-parameters. Table 1 shows the results of all the different categories of systems discussed in section 6 on the CoNLL 2002 and 2003 datasets. The table also indicates, for each model, whether it makes use of external knowledge like a dictionary or gazetteer. Table 2 presents a similar analysis on the DrugNER dataset from SemEval 2013 task 9 (Segura Bedmar et al., 2013). Our first finding from the survey is that feature-inferring NN systems outperform feature-engineered systems, despite the latter's access to domain specific rules, knowledge, features, and lexicons. For example, the best feature-engineered system for Spanish, Agerri and Rigau (2016), is 1.59% below the best feature-inferring neural network system, (Lample et al., 2016), and 1.65% below the best neural network system that incorporates lexical resources (Bharadwaj et al., 2016). Similarly, the best featureengineered system for German, Agerri and Rigau (2016), is 2.34% below the best feature-inferring neural network system, Lample et al. (2016). The differences are smaller for Dutch and English, but in neither case is the best feature-engineered model better than the best neural network model. In DrugNER, the word+character NN model outperforms the feature engineered system by 8.90% on MedLine test data and 3.50% on the overall dataset.",How do feature-inferring neural network systems compare to feature-engineered systems in NER tasks across different languages?,"Feature-inferring neural network systems outperform feature-engineered systems, despite the latter's access to domain-specific rules, knowledge, features, and lexicons. For example, the best feature-engineered system for Spanish, Agerri and Rigau (2016), is 1.59% below the best feature-inferring neural network system, (Lample et al., 2016), and 1.65% below the best neural network system that incorporates lexical resources (Bharadwaj et al., 2016). Similarly, the best feature-engineered system for German, Agerri and Rigau (2016), is 2.34% below the best feature-inferring neural network system, Lample et al. (2016). The differences are smaller for Dutch and English, but in neither case is the best feature-engineered model better than the best neural network model. In DrugNER, the word+character NN model outperforms the feature-engineered system by 8.90% on MedLine test data and 3.50% on the overall dataset.",How do neural network systems that infer features compare to those that rely on feature engineering in Named Entity Recognition (NER) tasks across various languages?,"Neural network systems that infer features generally outperform those that rely on feature engineering, even though the latter have access to domain-specific rules, knowledge, features, and lexicons. For instance, in Spanish NER tasks, the best feature-engineered system by Agerri and Rigau (2016) lags behind the top feature-inferring neural network system by Lample et al. (2016) by 1.59%. It also falls short by 1.65% compared to the best neural network system that incorporates lexical resources, as demonstrated by Bharadwaj et al. (2016). Similarly, for German, the feature-engineered system by Agerri and Rigau (2016) is 2.34% less effective than the leading feature-inferring neural network system by Lample et al. (2016). Although the performance gap is smaller for Dutch and English, the best feature-engineered models still do not surpass the top neural network models. In the DrugNER domain, the word+character neural network model outperforms the feature-engineered system by 8.90% on MedLine test data and by 3.50% on the overall dataset.",7,6,7,4,"Question:

How do feature-inferring neural network systems compare to feature-engineered systems in NER tasks across different languages?

Answer:

Feature-inferring neural network systems outperform feature-engineered systems, despite the latter's access to domain-specific rules, knowledge, features, and lexicons. For example, the best feature-engineered system for Spanish, Agerri and Rigau (2016), is 1.59% below the best feature-inferring neural network system, (Lample et al., 2016), and 1.65% below the best neural network system that incorporates lexical resources (Bharadwaj et al., 2016). Similarly, the best feature-engineered system for German, Agerri and Rigau (2016), is 2.34% below the best feature-inferring neural network system, Lample et al. (2016). The differences are smaller for Dutch and English, but in neither case is the best feature-engineered model better than the best neural network model. In DrugNER, the word+character NN model outperforms the feature-engineered system by 8.90% on MedLine test data and 3.50% on the overall dataset.","Question:

How do neural network systems that infer features compare to those that rely on feature engineering in Named Entity Recognition (NER) tasks across various languages?

Answer:

Neural network systems that infer features generally outperform those that rely on feature engineering, even though the latter have access to domain-specific rules, knowledge, features, and lexicons. For instance, in Spanish NER tasks, the best feature-engineered system by Agerri and Rigau (2016) lags behind the top feature-inferring neural network system by Lample et al. (2016) by 1.59%. It also falls short by 1.65% compared to the best neural network system that incorporates lexical resources, as demonstrated by Bharadwaj et al. (2016). Similarly, for German, the feature-engineered system by Agerri and Rigau (2016) is 2.34% less effective than the leading feature-inferring neural network system by Lample et al. (2016). Although the performance gap is smaller for Dutch and English, the best feature-engineered models still do not surpass the top neural network models. In the DrugNER domain, the word+character neural network model outperforms the feature-engineered system by 8.90% on MedLine test data and by 3.50% on the overall dataset.",NO,True,1052,True,True
49215220-s6,A Survey on Open Information Extraction,2018-06-14,Evaluation,"Though a multitude of systems for Open IE have been developed over the last decade, a clear formal specification of what constitutes a valid relational tuple is still missing. This lack of a well-defined, generally accepted task definition prevented the creation of an established, large-scale annotated corpus serving as a gold standard dataset for an objective and reproducible cross-system comparison. As a consequence, to date, Open IE systems were predominantly evaluated by hand on small-scale corpora that consist of only a few hundred sentences, thereby ignoring one of the fundamental goals of Open IE: scalability to large amounts of text. Moreover, none of the datasets that were used for assessing the  Figure 5: Graphene's extraction workflow for an example sentence (Cetto et al., 2018).

performance of different systems is widely agreed upon. As can be seen in Table 2, the corpora compiled by Del Corro and Gemulla (2013), Xu et al. (2013), Fader et al. (2011 and Banko et al. (2007) are occasionally re-used. However, new datasets are still collected, hindering a fair comparison of the proposed approaches. Besides, although Open IE methods are targeted at being domain independent and able to cope with heterogeneous datasets, the corpora used in the evaluation process are restricted to the news, Wikipedia and Web domains for the most part. Accordingly, no clear statement about the portability of the approaches to various genres of text is possible. In addition, most evaluation procedures described in the literature focus on precision-oriented metrics, while either completely ignoring recall or using some kind of proxy, such as yield, i.e. the total number of extractions labeled as correct, or coverage, i.e. the percentage of text from the input that is contained in at least one of the extractions. Hence, the absence of a standard evaluation procedure makes it hard to replicate and compare the performance of different Open IE systems. Table 2 provides a detailed overview of both the datasets and measures used for intrinsically evaluating the various approaches described above, while Table 3 shows the tasks that were used for an extrinsic evaluation of a small set of Open IE systems. In order to address aforementioned difficulties,  recently made a first attempt in standardizing the Open IE evaluation by providing a large gold benchmark corpus. It is based on a set of consensual guiding principles that underly most Open IE approaches proposed so far, as they have identified. Those principles cover the core aspects of the task of Open IE, allowing for a clearer formulation of the problem to be solved. The three key features to consider are the following:

Assertedness. The assertedness principle states that extracted propositions should be asserted by the original sentence. Usually, instead of inferring propositions out of implied statements, e.g. the tuple Sam; convinced; John out of Sam; succeeded in convincing; John , Open IE systems tend to extract the full relational phrase ( Sam; succeeded in convincing; John ), incorporating matrix verbs (""succeeded"") and other elements, such as negotiations or modals (e.g. John; could not join; the band ).

Minimal Propositions. In order to serve for semantic tasks, it is beneficial for Open IE systems to extract compact, self-contained propositions that do not combine several unrelated facts. Therefore, systems should aim to generate valid propositions with minimal spans for both relation and argument slots, while preserving the meaning of the input. As an example, the coordination in the sentence ""Bell distributes electronic and building products"" should ideally yield the two propositions: Bell; distributes; electronic products and Bell; distributes; building products .

Completeness and Open Lexicon. The completeness and open lexicon principle aims to extract all relations that are asserted in the input text. This principle was one of the fundamental ideas that have been introduced in the work of Banko et al. (2007) together with the Open IE terminology. In their work, the Open IE task was defined as a domain-independent task which extracts all possible   relations from heterogeneous corpora, instead of only extracting a set of pre-specified classes of relations. The majority of current Open IE systems realize this challenge by considering all possible verbs as potential relations. Accordingly, their scope is often limited to the extraction of verbal predicates, while ignoring relations mediated by more complex syntactic constructs, such as nouns or adjectives.

Realizing that above-mentioned requirements are subsumed by the task of Question Answering (QA) driven Semantic Role Labeling (SRL) (He et al., 2015),  converted the annotations of a QA-SRL dataset to an Open IE corpus, resulting in more than 10,000 extractions over 3,200 sentences from Wikipedia and the Wall Street Journal.

In addition, Schneider et al. (2017) presented RelVis, another benchmark framework for Open IE that allows for a large-scale comparative analysis of Open IE approaches. Besides Stanovsky and Dagan (2016)'s benchmark suite, it comprises the n-ary news dataset proposed in Mesquita et al. (2013), Banko et al. (2007)'s Web corpus and the Penn sentences from Xu et al. (2013). Similar to the toolkit proposed in , RelVis supports a quantitative evaluation of the performance of Open IE systems in terms of precision, recall and F 2 -score. In addition, it facilitates a manual qualitative error analysis. For this purpose, six common error classes are distinguished to which inaccurate extractions can be assigned: (1) wrong boundaries, where the relational or argument phrase is either too long or too small; (2) redundant extraction, where the proposition asserted in an extraction is already expressed in another extraction; (3) uninformative extraction, where critical information is omitted; (4) missing extraction, i.e. a false negative, where either a relation is not detected by the system or the argumentfinding heuristics choose the wrong arguments or none argument at all; (5) wrong extraction, where no meaningful interpretation of the proposition is possible; and (6) out of scope extraction, where a system yields a correct extraction that was not recognized by the authors of the gold dataset.","What are the key challenges in evaluating Open IE systems, and how do they impact system comparison?","Though a multitude of systems for Open IE have been developed over the last decade, a clear formal specification of what constitutes a valid relational tuple is still missing. This lack of a well-defined, generally accepted task definition prevented the creation of an established, large-scale annotated corpus serving as a gold standard dataset for an objective and reproducible cross-system comparison. As a consequence, to date, Open IE systems were predominantly evaluated by hand on small-scale corpora that consist of only a few hundred sentences, thereby ignoring one of the fundamental goals of Open IE: scalability to large amounts of text. Moreover, none of the datasets that were used for assessing the performance of different systems is widely agreed upon. New datasets are still collected, hindering a fair comparison of the proposed approaches. Although Open IE methods are targeted at being domain independent and able to cope with heterogeneous datasets, the corpora used in the evaluation process are restricted to the news, Wikipedia, and Web domains for the most part. Accordingly, no clear statement about the portability of the approaches to various genres of text is possible. Most evaluation procedures described in the literature focus on precision-oriented metrics, while either completely ignoring recall or using some kind of proxy, such as yield or coverage. The absence of a standard evaluation procedure makes it hard to replicate and compare the performance of different Open IE systems.","What are the main challenges in evaluating Open Information Extraction (Open IE) systems, and how do these challenges affect the ability to compare different systems?","Evaluating Open IE systems presents several key challenges that significantly impact the ability to compare different systems effectively. One major issue is the absence of a clear formal specification for what constitutes a valid relational tuple. This lack of a well-defined and generally accepted task definition has hindered the development of an established, large-scale annotated corpus that could serve as a gold standard dataset for objective and reproducible cross-system comparisons. As a result, Open IE systems have predominantly been evaluated manually on small-scale corpora, typically consisting of only a few hundred sentences. This approach overlooks one of the fundamental goals of Open IE: scalability to large volumes of text.

Furthermore, there is no consensus on the datasets used to assess the performance of different systems, with new datasets continually being collected. This ongoing collection process complicates fair comparisons of the proposed approaches. Although Open IE methods aim to be domain-independent and capable of handling heterogeneous datasets, the evaluation corpora are mostly limited to the news, Wikipedia, and Web domains. Consequently, it is challenging to make definitive statements about the portability of these approaches to various text genres.

Most evaluation procedures described in the literature focus on precision-oriented metrics, often neglecting recall or using proxies like yield or coverage. The absence of a standard evaluation procedure makes it difficult to replicate and compare the performance of different Open IE systems.",8,4,7,4,"Question:

What are the key challenges in evaluating Open IE systems, and how do they impact system comparison?

Answer:

Though a multitude of systems for Open IE have been developed over the last decade, a clear formal specification of what constitutes a valid relational tuple is still missing. This lack of a well-defined, generally accepted task definition prevented the creation of an established, large-scale annotated corpus serving as a gold standard dataset for an objective and reproducible cross-system comparison. As a consequence, to date, Open IE systems were predominantly evaluated by hand on small-scale corpora that consist of only a few hundred sentences, thereby ignoring one of the fundamental goals of Open IE: scalability to large amounts of text. Moreover, none of the datasets that were used for assessing the performance of different systems is widely agreed upon. New datasets are still collected, hindering a fair comparison of the proposed approaches. Although Open IE methods are targeted at being domain independent and able to cope with heterogeneous datasets, the corpora used in the evaluation process are restricted to the news, Wikipedia, and Web domains for the most part. Accordingly, no clear statement about the portability of the approaches to various genres of text is possible. Most evaluation procedures described in the literature focus on precision-oriented metrics, while either completely ignoring recall or using some kind of proxy, such as yield or coverage. The absence of a standard evaluation procedure makes it hard to replicate and compare the performance of different Open IE systems.","Question:

What are the main challenges in evaluating Open Information Extraction (Open IE) systems, and how do these challenges affect the ability to compare different systems?

Answer:

Evaluating Open IE systems presents several key challenges that significantly impact the ability to compare different systems effectively. One major issue is the absence of a clear formal specification for what constitutes a valid relational tuple. This lack of a well-defined and generally accepted task definition has hindered the development of an established, large-scale annotated corpus that could serve as a gold standard dataset for objective and reproducible cross-system comparisons. As a result, Open IE systems have predominantly been evaluated manually on small-scale corpora, typically consisting of only a few hundred sentences. This approach overlooks one of the fundamental goals of Open IE: scalability to large volumes of text.

Furthermore, there is no consensus on the datasets used to assess the performance of different systems, with new datasets continually being collected. This ongoing collection process complicates fair comparisons of the proposed approaches. Although Open IE methods aim to be domain-independent and capable of handling heterogeneous datasets, the evaluation corpora are mostly limited to the news, Wikipedia, and Web domains. Consequently, it is challenging to make definitive statements about the portability of these approaches to various text genres.

Most evaluation procedures described in the literature focus on precision-oriented metrics, often neglecting recall or using proxies like yield or coverage. The absence of a standard evaluation procedure makes it difficult to replicate and compare the performance of different Open IE systems.",NO,True,1595,True,True
237099284-s11,Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey,2021,RNN and latency in DST.,"A relevant issue for DST models is prediction time, due to the number of dialogue states they have to consider at each dialogue turn. (Zhong et al., 2018) combined both a shared representation and a slot-specific representation in the Global-Locally Self Attentive Dialogue State Tracker (GLAD). The GLAD model consists of an RNN-based global module, to learn global features, and a local module that learns slot-specific features. The representations of slot-values and user input are then scored using a scoring module that predicts their probability. However, GLAD needs an RNN for each slot-value representation, this way increasing the latency of the model. Further improvements on latency were proposed in GCE, Globally-Conditioned Encoder (Nouri and Hosseini-Asl, 2018), which uses only the global encoder, and in (Balaraman and Magnini, 2019), proposing a Global encoder and Slot-Attentive decoders (G-SAT). The G-SAT model uses an RNN to encode the user input and slot-specific feedforward networks to represent the slot-values.

Encoders based on pre-trained LM. The use of pre-trained language models, such as BERT (Bidirectional Encoder Representation from Transformers) (Devlin et al., 2019), is meant to increase the DST capacity to capture the semantics of slot and values names. (Lee et al., 2019) proposed a slot-utterance matching belief tracker (SUMBT) using BERT to encode slots, user input, and slotvalues. The representations of the slots and of the user input are combined using multi-head attention (Vaswani et al., 2017) to obtain the input representation of the model, and then compared with the slot-value representation to obtain the probability.","What are the latency improvements proposed for dialogue state tracking models, and how do they address these issues?","A relevant issue for DST models is prediction time, due to the number of dialogue states they have to consider at each dialogue turn. The GLAD model consists of an RNN-based global module, to learn global features, and a local module that learns slot-specific features. However, GLAD needs an RNN for each slot-value representation, this way increasing the latency of the model. Further improvements on latency were proposed in GCE, Globally-Conditioned Encoder (Nouri and Hosseini-Asl, 2018), which uses only the global encoder. Additionally, Balaraman and Magnini (2019) proposed a Global encoder and Slot-Attentive decoders (G-SAT). The G-SAT model uses an RNN to encode the user input and slot-specific feedforward networks to represent the slot-values.","What strategies have been proposed to improve latency in dialogue state tracking (DST) models, and how do these strategies address the latency issues?","Latency in dialogue state tracking (DST) models is a significant concern, primarily due to the extensive number of dialogue states that need to be evaluated at each turn in a conversation. The GLAD model, for instance, employs an RNN-based global module to capture global features and a local module to learn slot-specific features. However, GLAD's requirement for an RNN for each slot-value representation contributes to increased latency. To address this, the Globally-Conditioned Encoder (GCE) proposed by Nouri and Hosseini-Asl in 2018 offers a solution by utilizing only the global encoder, thereby reducing the model's complexity and latency. Additionally, Balaraman and Magnini in 2019 introduced the Global encoder and Slot-Attentive decoders (G-SAT) model, which encodes user input with an RNN and employs slot-specific feedforward networks for slot-value representation, further enhancing latency performance.",7,4,7,4,"Question:

What are the latency improvements proposed for dialogue state tracking models, and how do they address these issues?

Answer:

A relevant issue for DST models is prediction time, due to the number of dialogue states they have to consider at each dialogue turn. The GLAD model consists of an RNN-based global module, to learn global features, and a local module that learns slot-specific features. However, GLAD needs an RNN for each slot-value representation, this way increasing the latency of the model. Further improvements on latency were proposed in GCE, Globally-Conditioned Encoder (Nouri and Hosseini-Asl, 2018), which uses only the global encoder. Additionally, Balaraman and Magnini (2019) proposed a Global encoder and Slot-Attentive decoders (G-SAT). The G-SAT model uses an RNN to encode the user input and slot-specific feedforward networks to represent the slot-values.","Question:

What strategies have been proposed to improve latency in dialogue state tracking (DST) models, and how do these strategies address the latency issues?

Answer:

Latency in dialogue state tracking (DST) models is a significant concern, primarily due to the extensive number of dialogue states that need to be evaluated at each turn in a conversation. The GLAD model, for instance, employs an RNN-based global module to capture global features and a local module to learn slot-specific features. However, GLAD's requirement for an RNN for each slot-value representation contributes to increased latency. To address this, the Globally-Conditioned Encoder (GCE) proposed by Nouri and Hosseini-Asl in 2018 offers a solution by utilizing only the global encoder, thereby reducing the model's complexity and latency. Additionally, Balaraman and Magnini in 2019 introduced the Global encoder and Slot-Attentive decoders (G-SAT) model, which encodes user input with an RNN and employs slot-specific feedforward networks for slot-value representation, further enhancing latency performance.",NO,True,919,True,True
237099284-s12,Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey,2021,Dynamic Ontology DST Models,"The models discussed in Section 4 rely on a fixed slot-value set, which is assumed to be available before making the prediction. This is a severe limitation to domains where compiling the slot-value set . is costly, or the set of possible slot-values is open (e.g., DEPARTURE TIME, RESTAURANT NAME, etc.). For this reason, various studies have focused on developing models that can track slot values even if they are not defined in the ontology. Two major approaches for dynamic ontology models are: i) copy the slot value from the user input to the output; and ii) generate the slot value as the output. Figure 3 presents the schema of a model using the combination of both approaches. One significant difference between static ontology and dynamic ontology models is that while the output vocabulary in the static ontology is limited (i.e., equal to # of slot-values), in a dynamic ontology setting the output vocabulary is much larger.

Copy and pointer networks. Copy mechanism (Gu et al., 2016) and pointer networks (Vinyals et al., 2015) are the main approaches in neural networks to make predictions on the input tokens. They both rely on the attention mechanism (Bahdanau et al., 2015) to obtain scores over the input tokens. (Xu and Hu, 2018) proposed an end-to-end DST architecture based on pointer networks, showing efficient tracking of unseen slot values in a datadriven approach on the DSTC2 dataset. However, since pointer networks can only make predictions on the input tokens, they cannot be directly applied for all slots and require postprocessing of predicted values.  proposed a Transferable Multi-Domain State Generator TRADE, the first generation-based DST that incorporates the copy mechanism with a slot-gate. Figure 3 shows the architecture of the TRADE model. TRADE is based on an encoder-decoder architecture consisting of a three-way classifier that predicts over probabilities ptr, none, and dontcare. If the value is not expressed, it is predicted as none, if no constraint then dontcare and, if the value is expressed in the input, then ptr is predicted by the slot-gate. On ptr prediction, the corresponding value needs to be decoded by the decoder layer (referred as state generator). The state generator layer is initialized with both the domain and the slot representation, and generates the dialogue state using a recurrent architecture. As all the parameters are shared for all slots and domains, TRADE enables the transfer of knowledge from one domain to another, which has opened research directions in zero-shot approaches for DST with promising results.",How does the TRADE model facilitate knowledge transfer across domains in dialogue state tracking?,"TRADE is based on an encoder-decoder architecture consisting of a three-way classifier that predicts over probabilities ptr, none, and dontcare. If the value is not expressed, it is predicted as none, if no constraint then dontcare and, if the value is expressed in the input, then ptr is predicted by the slot-gate. On ptr prediction, the corresponding value needs to be decoded by the decoder layer (referred to as the state generator). The state generator layer is initialized with both the domain and the slot representation, and generates the dialogue state using a recurrent architecture. As all the parameters are shared for all slots and domains, TRADE enables the transfer of knowledge from one domain to another, which has opened research directions in zero-shot approaches for DST with promising results.",How does the TRADE model enable effective knowledge transfer across different domains in dialogue state tracking?,"The TRADE model facilitates knowledge transfer across domains in dialogue state tracking through its encoder-decoder architecture, which includes a three-way classifier. This classifier predicts among three probabilities: ptr, none, and dontcare. If a value is not expressed in the dialogue, it is predicted as none; if there is no constraint, it is predicted as dontcare. When a value is expressed in the input, the slot-gate predicts ptr. Upon ptr prediction, the corresponding value is decoded by the decoder layer, also known as the state generator. This state generator is initialized with both the domain and slot representation and uses a recurrent architecture to generate the dialogue state. By sharing all parameters across all slots and domains, TRADE effectively enables the transfer of knowledge from one domain to another. This capability has spurred research into zero-shot approaches for dialogue state tracking, yielding promising results.",7,2,4,2,"Question:

How does the TRADE model facilitate knowledge transfer across domains in dialogue state tracking?

Answer:

TRADE is based on an encoder-decoder architecture consisting of a three-way classifier that predicts over probabilities ptr, none, and dontcare. If the value is not expressed, it is predicted as none, if no constraint then dontcare and, if the value is expressed in the input, then ptr is predicted by the slot-gate. On ptr prediction, the corresponding value needs to be decoded by the decoder layer (referred to as the state generator). The state generator layer is initialized with both the domain and the slot representation, and generates the dialogue state using a recurrent architecture. As all the parameters are shared for all slots and domains, TRADE enables the transfer of knowledge from one domain to another, which has opened research directions in zero-shot approaches for DST with promising results.","Question:

How does the TRADE model enable effective knowledge transfer across different domains in dialogue state tracking?

Answer:

The TRADE model facilitates knowledge transfer across domains in dialogue state tracking through its encoder-decoder architecture, which includes a three-way classifier. This classifier predicts among three probabilities: ptr, none, and dontcare. If a value is not expressed in the dialogue, it is predicted as none; if there is no constraint, it is predicted as dontcare. When a value is expressed in the input, the slot-gate predicts ptr. Upon ptr prediction, the corresponding value is decoded by the decoder layer, also known as the state generator. This state generator is initialized with both the domain and slot representation and uses a recurrent architecture to generate the dialogue state. By sharing all parameters across all slots and domains, TRADE effectively enables the transfer of knowledge from one domain to another. This capability has spurred research into zero-shot approaches for dialogue state tracking, yielding promising results.",NO,True,956,True,True
9089503-s2,A Review of Corpus-based Statistical Models of Language Variation,2015-10-01,Modeling phonetic variation,"This vein of corpus-based language variation research first started with studies on phonetic variationprobably because phonetic features are readily quantifiable. Some of the pioneering works on English pronunciation variation were completed around the turn of the century (Bell et al. 2009;Fosler-Lussier and Morgan 1999;Gregory, et al. 1999;Jurafsky et al. 1998Jurafsky et al. , 2001a, among others), with phonetic data from the Switchboard corpus of telephone conversations (Godfrey et al. 1992), which contains 240 hours of speech (of which 4 hours are phonetically transcribed and used in the statistical models).

The studies above mostly examined word duration and vowel pronunciation (full vs. reduced) as parameters of pronunciation variation. In addition to describing the general picture of variation, these studies were also deeply interested in the effects of probabilistic factors (e.g. word frequency, contextual probability, etc) on pronunciation variation. The results presented in these studies are cited as empirical support for the general claim that probabilistic relations have profound influence on the representation and production of words in speech (Jurafsky et al., 2001b) Later on, with the completion of the Buckeye corpus (Pitt et al., 2007), which contains 40 hours of phonetically transcribed conversational speech, another batch of corpus-based phonetic variation studies appeared (Johnson, 2004;Gahl et al., 2012;Yao, 2009Yao, , 2011. Since the Buckeye corpus is recorded in a studio, the recording quality is high enough to warrant automatic measurement of VOT (Yao, 2009) and vowel formants . This allows for modeling of gradient vowel dispersion, measured by the distance between a specific vowel token from the center of the vowel space on a F1-F2 plane (Bradlow et al., 1996). Furthermore, some of the variation studies based on the Buckeye corpus (Gahl et al., 2012;Yao, 2011) focused on the effects of a particular lexical measure called phonological neighborhood density. Phonological neighborhood density refers to the number of similar-sounding words given a specific target word. Thus, the models built in these studies had one critical predictor (i.e. phonological neighborhood density), and all the other non-neighborhood predictors were included as control variables. Results from these studies revealed the effects of phonological neighborhood structure in word production when all other factors that could also influence word production were statistically controlled.

In addition to English, corpus-based pronunciation variation research has also been conducted in other languages (Dutch: Pluymaekers et al., 2005, among others;French: Meunier and Espesser, 2011;Yao and Meunier, 2014;Taiwan Southern Min: Myers and Li, 2009).",How did the Buckeye corpus contribute to the study of phonetic variation and what were its key findings?,"The Buckeye corpus, which contains 40 hours of phonetically transcribed conversational speech, contributed to the study of phonetic variation by providing high-quality recordings that allowed for automatic measurement of VOT and vowel formants. This enabled the modeling of gradient vowel dispersion, measured by the distance between a specific vowel token from the center of the vowel space on a F1-F2 plane. Some of the variation studies based on the Buckeye corpus focused on the effects of a particular lexical measure called phonological neighborhood density. Phonological neighborhood density refers to the number of similar-sounding words given a specific target word. The models built in these studies had one critical predictor, phonological neighborhood density, and all other non-neighborhood predictors were included as control variables. Results from these studies revealed the effects of phonological neighborhood structure in word production when all other factors that could also influence word production were statistically controlled.","What role did the Buckeye corpus play in advancing the study of phonetic variation, and what were its significant findings?","The Buckeye corpus, comprising 40 hours of phonetically transcribed conversational speech, played a crucial role in advancing the study of phonetic variation. It provided high-quality recordings that facilitated the automatic measurement of Voice Onset Time (VOT) and vowel formants. This capability allowed researchers to model gradient vowel dispersion, which is assessed by measuring the distance of a specific vowel token from the center of the vowel space on an F1-F2 plane. Studies utilizing the Buckeye corpus often focused on the impact of phonological neighborhood density, a lexical measure that indicates the number of similar-sounding words related to a specific target word. In these studies, phonological neighborhood density served as a critical predictor, while other non-neighborhood predictors were included as control variables. The findings from these studies highlighted the influence of phonological neighborhood structure on word production, demonstrating its effects even when other potential influencing factors were statistically controlled.",7,2,7,2,"Question:

How did the Buckeye corpus contribute to the study of phonetic variation and what were its key findings?

Answer:

The Buckeye corpus, which contains 40 hours of phonetically transcribed conversational speech, contributed to the study of phonetic variation by providing high-quality recordings that allowed for automatic measurement of VOT and vowel formants. This enabled the modeling of gradient vowel dispersion, measured by the distance between a specific vowel token from the center of the vowel space on a F1-F2 plane. Some of the variation studies based on the Buckeye corpus focused on the effects of a particular lexical measure called phonological neighborhood density. Phonological neighborhood density refers to the number of similar-sounding words given a specific target word. The models built in these studies had one critical predictor, phonological neighborhood density, and all other non-neighborhood predictors were included as control variables. Results from these studies revealed the effects of phonological neighborhood structure in word production when all other factors that could also influence word production were statistically controlled.","Question:

What role did the Buckeye corpus play in advancing the study of phonetic variation, and what were its significant findings?

Answer:

The Buckeye corpus, comprising 40 hours of phonetically transcribed conversational speech, played a crucial role in advancing the study of phonetic variation. It provided high-quality recordings that facilitated the automatic measurement of Voice Onset Time (VOT) and vowel formants. This capability allowed researchers to model gradient vowel dispersion, which is assessed by measuring the distance of a specific vowel token from the center of the vowel space on an F1-F2 plane. Studies utilizing the Buckeye corpus often focused on the impact of phonological neighborhood density, a lexical measure that indicates the number of similar-sounding words related to a specific target word. In these studies, phonological neighborhood density served as a critical predictor, while other non-neighborhood predictors were included as control variables. The findings from these studies highlighted the influence of phonological neighborhood structure on word production, demonstrating its effects even when other potential influencing factors were statistically controlled.",NO,True,1067,True,True
262460726-s2,A Practical Survey on Zero-shot Prompt Design for In-context Learning,2023-09-22,Prompts for Encoder-only Transformer Models (BERT),"Before the advent of in-context learning, some research efforts have been devoted to studying how to design effective prompts to enhance the performance of BERT models.As depicted in Figure 2, prompts in BERT are usually combined with input to form a cloze-style structure, while for transformer decoder-based models, prompts are more flexible.

Numerous studies have investigated prompt design in BERT.In the work by (Jiang et al., 2020), the authors proposed heuristic-based approaches for designing discrete prompts.Dependency parsing is employed to identify useful prompts from Wikipedia.In (Gao et al., 2021), the authors utilized T5 as a prompt generator with a beam search to create a set of diversified prompts.They then used D dev to select a single prompt with the best performance.In (Shin et al., 2020), a gradient-based prompt search approach was proposed, wherein each prompt token is learned by directly optimizing LMs on the downstream task.

In addition to prompt designing strategies, other research work focuses on enriching the prompt can-didates and ensembling the output from multiple prompts for the same input.To enrich prompts, (Jiang et al., 2020) employed back-translation to paraphrase prompts.Building on this work, (Haviv et al., 2021) trained a separate BERT model to rewrite prompts using the nearest BERT vector embedding.

The concept of in-context learning originates from the work by (Brown et al., 2020).However, BERT models can also perform similar tasks by using a single token as output.For example, France's capital is [MASK].

Only the output for the [MASK] position is used for inference.This characteristic enables the ensemble of answers from different prompts, although it is not apparent for similar practices in GPT-style models.In (Jiang et al., 2020), the authors proposed rank-based ensemble and optimized ensemble methods to aggregate answers generated from different prompts.

Among the studies designing prompts for BERT models, the majority focus on discrete prompts (i.e., hard prompts).To the best of our knowledge, we did not find any work attempting to generate continuous prompts.In general, optimizing prompts in BERT brings only marginal improvements to the original model.Given the size and structure of BERT, it is more favorable to fine-tune on downstream tasks.","What are the proposed methods for designing discrete prompts in BERT models, and how do they function?","Before the advent of in-context learning, some research efforts have been devoted to studying how to design effective prompts to enhance the performance of BERT models. Numerous studies have investigated prompt design in BERT. In the work by Jiang et al. (2020), the authors proposed heuristic-based approaches for designing discrete prompts. Dependency parsing is employed to identify useful prompts from Wikipedia. In Gao et al. (2021), the authors utilized T5 as a prompt generator with a beam search to create a set of diversified prompts. They then used D dev to select a single prompt with the best performance. In Shin et al. (2020), a gradient-based prompt search approach was proposed, wherein each prompt token is learned by directly optimizing LMs on the downstream task. In addition to prompt designing strategies, other research work focuses on enriching the prompt candidates and ensembling the output from multiple prompts for the same input. To enrich prompts, Jiang et al. (2020) employed back-translation to paraphrase prompts. Building on this work, Haviv et al. (2021) trained a separate BERT model to rewrite prompts using the nearest BERT vector embedding. In Jiang et al. (2020), the authors proposed rank-based ensemble and optimized ensemble methods to aggregate answers generated from different prompts. Among the studies designing prompts for BERT models, the majority focus on discrete prompts (i.e., hard prompts).","What methods have been proposed for designing discrete prompts in BERT models, and how do these methods work?","Before the rise of in-context learning, researchers explored various methods to design effective prompts for enhancing the performance of BERT models. Several studies have focused on this area. For instance, Jiang et al. (2020) proposed heuristic-based approaches for creating discrete prompts, utilizing dependency parsing to extract useful prompts from Wikipedia. Gao et al. (2021) took a different approach by using T5 as a prompt generator, employing beam search to generate a diverse set of prompts, and then selecting the best-performing prompt using a development set. Shin et al. (2020) introduced a gradient-based prompt search method, where each prompt token is learned by directly optimizing language models on the downstream task.

Beyond designing strategies, some research has aimed at enriching prompt candidates and ensembling outputs from multiple prompts for the same input. Jiang et al. (2020) used back-translation to paraphrase prompts, while Haviv et al. (2021) trained a separate BERT model to rewrite prompts using the nearest BERT vector embedding. Additionally, Jiang et al. (2020) proposed rank-based and optimized ensemble methods to aggregate answers generated from different prompts. Most studies in this field focus on discrete prompts, also known as hard prompts.",7,4,7,4,"Question:

What are the proposed methods for designing discrete prompts in BERT models, and how do they function?

Answer:

Before the advent of in-context learning, some research efforts have been devoted to studying how to design effective prompts to enhance the performance of BERT models. Numerous studies have investigated prompt design in BERT. In the work by Jiang et al. (2020), the authors proposed heuristic-based approaches for designing discrete prompts. Dependency parsing is employed to identify useful prompts from Wikipedia. In Gao et al. (2021), the authors utilized T5 as a prompt generator with a beam search to create a set of diversified prompts. They then used D dev to select a single prompt with the best performance. In Shin et al. (2020), a gradient-based prompt search approach was proposed, wherein each prompt token is learned by directly optimizing LMs on the downstream task. In addition to prompt designing strategies, other research work focuses on enriching the prompt candidates and ensembling the output from multiple prompts for the same input. To enrich prompts, Jiang et al. (2020) employed back-translation to paraphrase prompts. Building on this work, Haviv et al. (2021) trained a separate BERT model to rewrite prompts using the nearest BERT vector embedding. In Jiang et al. (2020), the authors proposed rank-based ensemble and optimized ensemble methods to aggregate answers generated from different prompts. Among the studies designing prompts for BERT models, the majority focus on discrete prompts (i.e., hard prompts).","Question:

What methods have been proposed for designing discrete prompts in BERT models, and how do these methods work?

Answer:

Before the rise of in-context learning, researchers explored various methods to design effective prompts for enhancing the performance of BERT models. Several studies have focused on this area. For instance, Jiang et al. (2020) proposed heuristic-based approaches for creating discrete prompts, utilizing dependency parsing to extract useful prompts from Wikipedia. Gao et al. (2021) took a different approach by using T5 as a prompt generator, employing beam search to generate a diverse set of prompts, and then selecting the best-performing prompt using a development set. Shin et al. (2020) introduced a gradient-based prompt search method, where each prompt token is learned by directly optimizing language models on the downstream task.

Beyond designing strategies, some research has aimed at enriching prompt candidates and ensembling outputs from multiple prompts for the same input. Jiang et al. (2020) used back-translation to paraphrase prompts, while Haviv et al. (2021) trained a separate BERT model to rewrite prompts using the nearest BERT vector embedding. Additionally, Jiang et al. (2020) proposed rank-based and optimized ensemble methods to aggregate answers generated from different prompts. Most studies in this field focus on discrete prompts, also known as hard prompts.",NO,True,1295,True,True
262460726-s9,A Practical Survey on Zero-shot Prompt Design for In-context Learning,2023-09-22,Evaluation,"Evaluating prompt design is very challenging.As there is no ground truth dataset for prompt generation, there is no ""best"" prompt but only better prompts.Therefore, the evaluation of the prompt performance for in-context learning usually falls into the following categories.

Conditional Probability (Likelihood): To evaluate the performance of a text generation model, we can measure the probability of the generated text.In our case, we can calculate the conditional probability of ground truth(y) given prompt (p), input(x) or calculate the joint probability of x, y, p averaging over the training data, as shown in (2) P rob (y|x, p) x,y∈X,Y

(2) This is a simple strategy because the models for in-context learning are generative language models which will generate the joint probability (likelihood) automatically.However, this metric sometimes fails to represent the actual performance of the downstream task.

Execution Accuracy: A more direct method to measure the performance of a prompt is to use metrics from the target task (Zhou et al., 2022), as ultimately the performance on the task is what we care about.In addition to measuring the execution accuracy directly on the entire training set, there are ways to efficiently estimate the performance on a subset of training data to save computational cost (Zhou et al., 2022), (Li et al., 2022).

Prompt Transferability is another evaluation metric reported in (Zhou et al., 2022), (Deng et al., 2022) which is used to prove the quality of the prompt generation methods.However, this metric is more useful in selecting the prompt designing method than evaluating the performance of a single prompt.

General Metrics for Language Models should be used when using large language models via zeroshot in-context learning.It is also important to measure the performance from additional aspects.For example, if we are to build a Question-Answering system, we need to measure the risk of hallucination (Ji et al., 2022).If we are to build an email generation system, we may need to measure the toxicity and prevent generating any aggressive content.The work of Holistic Evaluation of Language Models (HELM) (Liang et al., 2022) provides a great example in evaluating the performance for language models via in-context learning.Although various metrics have been reported in HELM for existing models, it is worth noting that the design of our prompt will directly impact the models' performance.",What are the main categories and methods for evaluating prompt performance in in-context learning?,"Evaluating prompt design is very challenging. As there is no ground truth dataset for prompt generation, there is no ""best"" prompt but only better prompts. Therefore, the evaluation of the prompt performance for in-context learning usually falls into the following categories. Conditional Probability (Likelihood): To evaluate the performance of a text generation model, the probability of the generated text can be measured. In this case, the conditional probability of ground truth(y) given prompt (p), input(x) or the joint probability of x, y, p averaging over the training data can be calculated. This is a simple strategy because the models for in-context learning are generative language models which will generate the joint probability (likelihood) automatically. However, this metric sometimes fails to represent the actual performance of the downstream task. Execution Accuracy: A more direct method to measure the performance of a prompt is to use metrics from the target task, as ultimately the performance on the task is what is cared about. In addition to measuring the execution accuracy directly on the entire training set, there are ways to efficiently estimate the performance on a subset of training data to save computational cost. Prompt Transferability is another evaluation metric which is used to prove the quality of the prompt generation methods. However, this metric is more useful in selecting the prompt designing method than evaluating the performance of a single prompt. General Metrics for Language Models should be used when using large language models via zero-shot in-context learning. It is also important to measure the performance from additional aspects. For example, if building a Question-Answering system, the risk of hallucination needs to be measured. If building an email generation system, the toxicity needs to be measured to prevent generating any aggressive content. The work of Holistic Evaluation of Language Models (HELM) provides a great example in evaluating the performance for language models via in-context learning. Although various metrics have been reported in HELM for existing models, it is worth noting that the design of the prompt will directly impact the models' performance.",What are the primary categories and methods for assessing prompt performance in in-context learning?,"Evaluating prompt design in in-context learning is inherently challenging due to the absence of a ground truth dataset for prompt generation. This means there is no definitive ""best"" prompt, only relatively better ones. The evaluation of prompt performance typically falls into several categories:

1. **Conditional Probability (Likelihood):** This method involves measuring the probability of the generated text. Specifically, it calculates the conditional probability of the ground truth (y) given the prompt (p) and input (x), or the joint probability of x, y, and p averaged over the training data. This approach is straightforward because generative language models used in in-context learning automatically generate these probabilities. However, this metric may not always accurately reflect the actual performance on downstream tasks.

2. **Execution Accuracy:** A more direct approach to evaluating prompt performance is to use metrics from the target task itself, as the ultimate goal is to assess task performance. Execution accuracy can be measured directly on the entire training set, or estimated efficiently on a subset to save computational resources.

3. **Prompt Transferability:** This metric assesses the quality of prompt generation methods and is particularly useful for selecting prompt design methods rather than evaluating a single prompt's performance.

4. **General Metrics for Language Models:** When using large language models in zero-shot in-context learning, it's crucial to evaluate performance from additional perspectives. For instance, in a Question-Answering system, the risk of hallucination should be measured, while in an email generation system, toxicity must be assessed to prevent generating aggressive content.

The Holistic Evaluation of Language Models (HELM) provides an excellent example of evaluating language model performance via in-context learning. Although various metrics have been reported in HELM for existing models, it's important to note that prompt design directly impacts model performance.",7,4,7,4,"Question:

What are the main categories and methods for evaluating prompt performance in in-context learning?

Answer:

Evaluating prompt design is very challenging. As there is no ground truth dataset for prompt generation, there is no ""best"" prompt but only better prompts. Therefore, the evaluation of the prompt performance for in-context learning usually falls into the following categories. Conditional Probability (Likelihood): To evaluate the performance of a text generation model, the probability of the generated text can be measured. In this case, the conditional probability of ground truth(y) given prompt (p), input(x) or the joint probability of x, y, p averaging over the training data can be calculated. This is a simple strategy because the models for in-context learning are generative language models which will generate the joint probability (likelihood) automatically. However, this metric sometimes fails to represent the actual performance of the downstream task. Execution Accuracy: A more direct method to measure the performance of a prompt is to use metrics from the target task, as ultimately the performance on the task is what is cared about. In addition to measuring the execution accuracy directly on the entire training set, there are ways to efficiently estimate the performance on a subset of training data to save computational cost. Prompt Transferability is another evaluation metric which is used to prove the quality of the prompt generation methods. However, this metric is more useful in selecting the prompt designing method than evaluating the performance of a single prompt. General Metrics for Language Models should be used when using large language models via zero-shot in-context learning. It is also important to measure the performance from additional aspects. For example, if building a Question-Answering system, the risk of hallucination needs to be measured. If building an email generation system, the toxicity needs to be measured to prevent generating any aggressive content. The work of Holistic Evaluation of Language Models (HELM) provides a great example in evaluating the performance for language models via in-context learning. Although various metrics have been reported in HELM for existing models, it is worth noting that the design of the prompt will directly impact the models' performance.","Question:

What are the primary categories and methods for assessing prompt performance in in-context learning?

Answer:

Evaluating prompt design in in-context learning is inherently challenging due to the absence of a ground truth dataset for prompt generation. This means there is no definitive ""best"" prompt, only relatively better ones. The evaluation of prompt performance typically falls into several categories:

1. **Conditional Probability (Likelihood):** This method involves measuring the probability of the generated text. Specifically, it calculates the conditional probability of the ground truth (y) given the prompt (p) and input (x), or the joint probability of x, y, and p averaged over the training data. This approach is straightforward because generative language models used in in-context learning automatically generate these probabilities. However, this metric may not always accurately reflect the actual performance on downstream tasks.

2. **Execution Accuracy:** A more direct approach to evaluating prompt performance is to use metrics from the target task itself, as the ultimate goal is to assess task performance. Execution accuracy can be measured directly on the entire training set, or estimated efficiently on a subset to save computational resources.

3. **Prompt Transferability:** This metric assesses the quality of prompt generation methods and is particularly useful for selecting prompt design methods rather than evaluating a single prompt's performance.

4. **General Metrics for Language Models:** When using large language models in zero-shot in-context learning, it's crucial to evaluate performance from additional perspectives. For instance, in a Question-Answering system, the risk of hallucination should be measured, while in an email generation system, toxicity must be assessed to prevent generating aggressive content.

The Holistic Evaluation of Language Models (HELM) provides an excellent example of evaluating language model performance via in-context learning. Although various metrics have been reported in HELM for existing models, it's important to note that prompt design directly impacts model performance.",NO,True,2051,True,True
252683270-s9,A Decade of Knowledge Graphs in Natural Language Processing: A Survey,2022-09-30,Knowledge Graph Construction,"The task of entity extraction is a starting point in constructing KGs and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020). Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a). A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018). Entity linking is a task of linking entities recognized in some text to already existing entities in KGs (Moon et al., 2018;. Since synonymous or similar entities often exist in different KGs or in different languages, entity alignment can be performed to reduce redundancy and repetition in future tasks (Gangemi et al., 2016;Chen et al., 2018). Coming up with the rules and schemes of KGs, i.e., their structure and format of knowledge presented in it, is done with the task of ontology construction (Haussmann et al., 2019).",How do entity extraction and relation extraction contribute to the construction of new Knowledge Graphs?,"The task of entity extraction is a starting point in constructing Knowledge Graphs (KGs) and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020). Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a). A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018).",How do entity extraction and relation extraction play a role in building new Knowledge Graphs?,"Entity extraction serves as the initial step in constructing Knowledge Graphs (KGs) by identifying real-world entities from unstructured text (Al-Moslmi et al., 2020). After these entities are identified, relation extraction is employed to discover the relationships and interactions between them (Zhang et al., 2019a). Many studies utilize both entity extraction and relation extraction to develop new KGs, such as those focused on news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018).",8,4,9,4,"Question:

How do entity extraction and relation extraction contribute to the construction of new Knowledge Graphs?

Answer:

The task of entity extraction is a starting point in constructing Knowledge Graphs (KGs) and is used to derive real-world entities from unstructured text (Al-Moslmi et al., 2020). Once the relevant entities are singled out, relationships and interactions between them are found with the task of relation extraction (Zhang et al., 2019a). A lot of papers use both entity extraction and relation extraction to construct new KGs, e.g., for news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018).","Question:

How do entity extraction and relation extraction play a role in building new Knowledge Graphs?

Answer:

Entity extraction serves as the initial step in constructing Knowledge Graphs (KGs) by identifying real-world entities from unstructured text (Al-Moslmi et al., 2020). After these entities are identified, relation extraction is employed to discover the relationships and interactions between them (Zhang et al., 2019a). Many studies utilize both entity extraction and relation extraction to develop new KGs, such as those focused on news events (Rospocher et al., 2016) or scholarly research (Luan et al., 2018).",NO,True,512,True,True
252762171-s16,"The Lifecycle of ""Facts"": A Survey of Social Bias in Knowledge Graphs",2022-10-07,Downstream Task Bias: Link Prediction,"Link prediction is a standard downstream task that targets the prediction of relations between entities in a given KG. Systematic deviations in the relations suggested for entities with different demographics indicate reproduced social bias. For the measurement of fairness or bias in link prediction, Keidar et al. (2021) distinguish between demographic parity versus predictive parity. The assumption underlying demographic parity is that the equality between predictions for demographic counterfactuals (opposite demographics, for example, female versus male in binary understanding) is the ideal state (Dwork et al., 2012). That is, the probability of predicting a label should be the same for both groups. Predictive parity is given, on the other hand, if the probability of true positive predictions (positive predictive value or precision) is equal between groups (Chouldechova, 2017). Hence, this measure factors in the label  Bourli and Pitoura (2020) distribution by demographic. With these metrics, Keidar et al. (2021) analyzed different embedding types, namely TransE, ComplEx, RotatE, and Dist-Mult, each fit on the benchmark datasets FB15k-237 (Toutanova and Chen, 2015) and Wikidata5m (Wang et al., 2021). They averaged the scores across a large set of human-associated relations to detect automatically which relations are most biased. The results showed that position played on a sports team was most consistently genderbiased across embeddings. Arduini et al. (2020) analyzed link prediction parity regarding the relations gender and occupation to estimate debiasing effects on TransH (Wang et al., 2014) and TransD (Ji et al., 2015). The comparability between different forms of vector representations is a strength of downstream metrics. In contrast, measures like the analogy test or projection score (Bourli and Pitoura, 2020) are based on specific distance metrics and TL (Fisher et al., 2020b) was shown to lack transferability across representations (Keidar et al., 2021) (Section 5.3). Du et al. (2022) interpret the correlation between gender and link prediction errors as an indicator of group bias. With this, they found, for example, that engineer and nurse are stereotypically biased in FB5M. However, the ground truth gender ratio was found not predictive of the bias metric (e.g., despite its higher male ratio, animator produced a stronger female bias value). For validation, it was shown that the predicted bias values correlate to the gender distributions of occupations according to U.S. census (again, on TransE). Furthermore, the authors investigated how much single triples contribute to group bias via an influence function. They found that gender bias is mostly driven by triples containing gendered entities and triples of low degree.",How do demographic and predictive parity differ in measuring fairness in link prediction tasks?,"The assumption underlying demographic parity is that the equality between predictions for demographic counterfactuals (opposite demographics, for example, female versus male in binary understanding) is the ideal state (Dwork et al., 2012). That is, the probability of predicting a label should be the same for both groups. Predictive parity is given, on the other hand, if the probability of true positive predictions (positive predictive value or precision) is equal between groups (Chouldechova, 2017). Hence, this measure factors in the label distribution by demographic.",What is the difference between demographic parity and predictive parity in assessing fairness in link prediction tasks?,"Demographic parity and predictive parity are two different approaches to measuring fairness in link prediction tasks. Demographic parity assumes that the ideal state is achieved when predictions are equal across demographic counterfactuals, such as between females and males in a binary context (Dwork et al., 2012). This means that the probability of predicting a certain label should be the same for both demographic groups. On the other hand, predictive parity focuses on ensuring that the probability of true positive predictions, also known as positive predictive value or precision, is equal between groups (Chouldechova, 2017). This approach takes into account the label distribution across different demographics.",7,2,7,2,"Question:

How do demographic and predictive parity differ in measuring fairness in link prediction tasks?

Answer:

The assumption underlying demographic parity is that the equality between predictions for demographic counterfactuals (opposite demographics, for example, female versus male in binary understanding) is the ideal state (Dwork et al., 2012). That is, the probability of predicting a label should be the same for both groups. Predictive parity is given, on the other hand, if the probability of true positive predictions (positive predictive value or precision) is equal between groups (Chouldechova, 2017). Hence, this measure factors in the label distribution by demographic.","Question:

What is the difference between demographic parity and predictive parity in assessing fairness in link prediction tasks?

Answer:

Demographic parity and predictive parity are two different approaches to measuring fairness in link prediction tasks. Demographic parity assumes that the ideal state is achieved when predictions are equal across demographic counterfactuals, such as between females and males in a binary context (Dwork et al., 2012). This means that the probability of predicting a certain label should be the same for both demographic groups. On the other hand, predictive parity focuses on ensuring that the probability of true positive predictions, also known as positive predictive value or precision, is equal between groups (Chouldechova, 2017). This approach takes into account the label distribution across different demographics.",NO,True,721,True,True
252992688-s9,A Survey of Active Learning for Natural Language Processing,2022-10-18,Discriminative 3,"Another direction is to select instances that are different from already labeled instances. Again, for NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005;Bloodgood and Callison-Burch, 2010;Erdmann et al., 2019). Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006;). Another interesting idea is to train a model to discriminate the labeled and unlabeled sets. Gissin and Shalev-Shwartz (2019) directly train a classifier for this purpose, while naturally adversarial training can be also adopted (Sinha et al., 2019;. In domain adaptation scenarios, the same motivation leads to the usage of a domain separator to filter instances (Rai et al., 2010).",How can similarity scores and feature-based metrics be used to select instances in active learning for NLP tasks?  ,"Another direction is to select instances that are different from already labeled instances. For NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005; Bloodgood and Callison-Burch, 2010; Erdmann et al., 2019). Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006).",How can similarity scores and feature-based metrics be applied in active learning to select instances for NLP tasks?,"In active learning for NLP tasks, one effective strategy is to select instances that differ from those already labeled. This can be achieved using simple feature-based metrics, which prioritize instances containing more unseen n-grams or out-of-vocabulary words (Eck et al., 2005; Bloodgood and Callison-Burch, 2010; Erdmann et al., 2019). Additionally, similarity scores can be employed to identify instances that are less similar to the labeled set, further enhancing the diversity of the selected data (Kim et al., 2006).",8,4,8,4,"Question:

How can similarity scores and feature-based metrics be used to select instances in active learning for NLP tasks?  

Answer:

Another direction is to select instances that are different from already labeled instances. For NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005; Bloodgood and Callison-Burch, 2010; Erdmann et al., 2019). Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006).","Question:

How can similarity scores and feature-based metrics be applied in active learning to select instances for NLP tasks?

Answer:

In active learning for NLP tasks, one effective strategy is to select instances that differ from those already labeled. This can be achieved using simple feature-based metrics, which prioritize instances containing more unseen n-grams or out-of-vocabulary words (Eck et al., 2005; Bloodgood and Callison-Burch, 2010; Erdmann et al., 2019). Additionally, similarity scores can be employed to identify instances that are less similar to the labeled set, further enhancing the diversity of the selected data (Kim et al., 2006).",NO,True,524,True,True
252992688-s10,A Survey of Active Learning for Natural Language Processing,2022-10-18,Batch Diversity,"Ideally, only one most useful instance would be selected in each iteration. However, it is more efficient and practical to adopt batch-mode AL (Settles, 2009), where each time a batch of instances is selected. In this case, we need to consider the dissimilarities not only between selected instances and labeled ones but also within the selected batch.

To select a batch of diverse instances, there are two common approaches. 1) Iterative selection collects the batch in an iterative greedy way (Brinker, 2003;. In each iteration, an instance is selected by comparing it with previously chosen instances to avoid redundancy. Some more advanced diversity-based criteria, like coreset (Geifman and El-Yaniv, 2017;Sener and Savarese, 2018) and determinantal point processes , can also be approximated in a similar way. 2) Clustering-based methods partition the unlabeled data into clusters and select instances among them Xu et al., 2003;Nguyen and Smeulders, 2004;Zhdanov, 2019;Yu et al., 2022). Since the chosen instances come from different clusters, diversity can be achieved to some extent.

For the calculation of similarity, in addition to comparing the input features or intermediate neural representations, other methods are also investigated, such as utilizing model-based similarity ), gradients (Ash et al., 2020Kim, 2020), and masked LM surprisal embeddings .",What are the common approaches for selecting a diverse batch of instances in active learning?,"To select a batch of diverse instances, there are two common approaches. 1) Iterative selection collects the batch in an iterative greedy way. In each iteration, an instance is selected by comparing it with previously chosen instances to avoid redundancy. Some more advanced diversity-based criteria, like coreset and determinantal point processes, can also be approximated in a similar way. 2) Clustering-based methods partition the unlabeled data into clusters and select instances among them. Since the chosen instances come from different clusters, diversity can be achieved to some extent.",What are the common strategies for selecting a diverse batch of instances in active learning?,"In active learning, selecting a diverse batch of instances is crucial for improving model performance. Two common strategies are employed to achieve this diversity. The first strategy is iterative selection, where instances are collected in an iterative, greedy manner. During each iteration, an instance is chosen by comparing it with previously selected instances to minimize redundancy. Advanced diversity-based criteria, such as coreset and determinantal point processes, can also be approximated using this method. The second strategy involves clustering-based methods, where the unlabeled data is partitioned into clusters, and instances are selected from these clusters. By choosing instances from different clusters, a certain level of diversity is naturally achieved.",8,4,8,4,"Question:

What are the common approaches for selecting a diverse batch of instances in active learning?

Answer:

To select a batch of diverse instances, there are two common approaches. 1) Iterative selection collects the batch in an iterative greedy way. In each iteration, an instance is selected by comparing it with previously chosen instances to avoid redundancy. Some more advanced diversity-based criteria, like coreset and determinantal point processes, can also be approximated in a similar way. 2) Clustering-based methods partition the unlabeled data into clusters and select instances among them. Since the chosen instances come from different clusters, diversity can be achieved to some extent.","Question:

What are the common strategies for selecting a diverse batch of instances in active learning?

Answer:

In active learning, selecting a diverse batch of instances is crucial for improving model performance. Two common strategies are employed to achieve this diversity. The first strategy is iterative selection, where instances are collected in an iterative, greedy manner. During each iteration, an instance is chosen by comparing it with previously selected instances to minimize redundancy. Advanced diversity-based criteria, such as coreset and determinantal point processes, can also be approximated using this method. The second strategy involves clustering-based methods, where the unlabeled data is partitioned into clusters, and instances are selected from these clusters. By choosing instances from different clusters, a certain level of diversity is naturally achieved.",NO,True,776,True,True
252992688-s19,A Survey of Active Learning for Natural Language Processing,2022-10-18,Directly Reducing Cost,"In addition to better query strategies, there are other ways of directly reducing annotation cost, such as computer-assisted annotation. In AL, models and annotators usually interact in an indirect way where models only query the instances to present to the annotators, while there could be closer interactions.

Pre-annotation is such an idea, where not only the raw data instances but also the model's best or top-k predictions are sent to the annotators to help them make decisions. If the model's predictions are reasonable, the annotators can simply select or make a few corrections to obtain the gold annotations rather than creating from scratch. This method has been shown effective when combined with AL (Baldridge and Osborne, 2004;Vlachos, 2006;Ringger et al., 2008;Skeppstedt, 2013;Cañizares-Díaz et al., 2021). Post-editing for MT is also a typical example (Dara et al., 2014).

Moreover, the models could provide help at real annotating time. For example, Culotta and Mc-Callum (2005) present an interactive AL system where the user's corrections can propagate to the model, which generates new predictions for the user to further refine. Interactive machine translation (IMT) adopts a similar idea, where the annotator corrects the first erroneous character, based on which the model reproduces the prediction. AL has also been combined with IMT to further reduce manual efforts (González-Rubio et al., 2012;Peris and Casacuberta, 2018;.",How can pre-annotation and interactive systems reduce annotation costs in active learning for NLP tasks?  ,"In active learning for NLP tasks, pre-annotation involves sending not only the raw data instances but also the model's best or top-k predictions to the annotators to assist them in making decisions. If the model's predictions are reasonable, annotators can select or make a few corrections to obtain the gold annotations rather than creating them from scratch. This method has been shown to be effective when combined with active learning (Baldridge and Osborne, 2004; Vlachos, 2006; Ringger et al., 2008; Skeppstedt, 2013; Cañizares-Díaz et al., 2021). Interactive systems can also provide help during the actual annotation process. For example, Culotta and Mc-Callum (2005) present an interactive active learning system where the user's corrections can propagate to the model, which generates new predictions for the user to further refine. Interactive machine translation (IMT) adopts a similar idea, where the annotator corrects the first erroneous character, and the model reproduces the prediction based on this correction. Active learning has also been combined with IMT to further reduce manual efforts (González-Rubio et al., 2012; Peris and Casacuberta, 2018).",How do pre-annotation and interactive systems help in reducing annotation costs in active learning for NLP tasks?,"Pre-annotation and interactive systems play a significant role in reducing annotation costs in active learning for NLP tasks. Pre-annotation involves providing annotators with not only the raw data instances but also the model's best or top-k predictions. This assists annotators in making decisions, as they can select or make minor corrections to the model's predictions to obtain the gold annotations, rather than creating them from scratch. This approach has proven effective when combined with active learning, as demonstrated by studies such as Baldridge and Osborne (2004), Vlachos (2006), Ringger et al. (2008), Skeppstedt (2013), and Cañizares-Díaz et al. (2021).

Interactive systems further enhance the annotation process by offering real-time assistance. For instance, Culotta and McCallum (2005) introduced an interactive active learning system where user corrections are fed back into the model, prompting it to generate new predictions for further refinement. A similar concept is applied in interactive machine translation (IMT), where the annotator corrects the first erroneous character, and the model updates its prediction based on this correction. Combining active learning with IMT has been shown to significantly reduce manual efforts, as evidenced by the work of González-Rubio et al. (2012) and Peris and Casacuberta (2018).",8,4,7,4,"Question:

How can pre-annotation and interactive systems reduce annotation costs in active learning for NLP tasks?  

Answer:

In active learning for NLP tasks, pre-annotation involves sending not only the raw data instances but also the model's best or top-k predictions to the annotators to assist them in making decisions. If the model's predictions are reasonable, annotators can select or make a few corrections to obtain the gold annotations rather than creating them from scratch. This method has been shown to be effective when combined with active learning (Baldridge and Osborne, 2004; Vlachos, 2006; Ringger et al., 2008; Skeppstedt, 2013; Cañizares-Díaz et al., 2021). Interactive systems can also provide help during the actual annotation process. For example, Culotta and Mc-Callum (2005) present an interactive active learning system where the user's corrections can propagate to the model, which generates new predictions for the user to further refine. Interactive machine translation (IMT) adopts a similar idea, where the annotator corrects the first erroneous character, and the model reproduces the prediction based on this correction. Active learning has also been combined with IMT to further reduce manual efforts (González-Rubio et al., 2012; Peris and Casacuberta, 2018).","Question:

How do pre-annotation and interactive systems help in reducing annotation costs in active learning for NLP tasks?

Answer:

Pre-annotation and interactive systems play a significant role in reducing annotation costs in active learning for NLP tasks. Pre-annotation involves providing annotators with not only the raw data instances but also the model's best or top-k predictions. This assists annotators in making decisions, as they can select or make minor corrections to the model's predictions to obtain the gold annotations, rather than creating them from scratch. This approach has proven effective when combined with active learning, as demonstrated by studies such as Baldridge and Osborne (2004), Vlachos (2006), Ringger et al. (2008), Skeppstedt (2013), and Cañizares-Díaz et al. (2021).

Interactive systems further enhance the annotation process by offering real-time assistance. For instance, Culotta and McCallum (2005) introduced an interactive active learning system where user corrections are fed back into the model, prompting it to generate new predictions for further refinement. A similar concept is applied in interactive machine translation (IMT), where the annotator corrects the first erroneous character, and the model updates its prediction based on this correction. Combining active learning with IMT has been shown to significantly reduce manual efforts, as evidenced by the work of González-Rubio et al. (2012) and Peris and Casacuberta (2018).",NO,True,1349,True,True
252992688-s23,A Survey of Active Learning for Natural Language Processing,2022-10-18,Learning,"AL can be combined with other advanced learning techniques to further reduce required annotations.

Semi-supervised learning. Since AL usually assumes an unlabeled pool, semi-supervised learning can be a natural fit. Combining these two is not a new idea: (McCallum and Nigam, 1998) adopt the EM algorithm to estimate the outputs of unlabeled data and utilize them for learning. This type of self-training or pseudo-labeling technique is often utilized in AL Majidi and Crane, 2013;Yu et al., 2022). With a similar motivation, (Dasgupta and Ng, 2009) use an unsupervised algorithm to identify the unambiguous instances to train an active learner. For the task of word alignment, which can be learned in an unsupervised manner, incorporating supervision with AL can bring further improvements in a data-efficient way (Ambati et al., 2010b,c).

Transfer learning. AL can be easily combined with transfer learning, another technique to reduce required annotations. Utilizing pre-trained models is already a good example (Ein-Dor et al., 2020;Tamkin et al., 2022) and continual training (Gururangan et al., 2020) can also be applied (Hua and Wang, 2022;Margatina et al., 2022). Moreover, transductive learning is commonly combined with AL by transferring learning signals from different domains (Chan and Ng, 2007;Shi et al., 2008;Rai et al., 2010;Saha et al., 2011;Wu et al., 2017;Kasai et al., 2019; or languages (Qian et al., 2014;Fang and Cohn, 2017;Fang et al., 2017;Chaudhary et al., 2019Moniz et al., 2022). In addition to the task model, the model-based query policy ( §2.1.4) is also often obtained with transfer learning.

Weak supervision. AL can also be combined with weakly supervised learning. Examples include learning from inputs and execution results for semantic parsing (Ni et al., 2020), labeling based on identical structure vectors for entity representations (Qian et al., 2020), learning from gazetteers and dictionaries for sequence labeling  and interactively discovering labeling rules .

Data augmentation. Augmentation is also applicable in AL and has been explored with iterative back-translation , mixup for sequence labeling  and phraseto-sentence augmentation for MT (Hu and Neubig, 2021). As discussed in §2.1.1, augmentation can also be helpful for instance querying (Jiang et al., 2020;Zhang et al., 2022b). Another interesting scenario involving augmentation and AL is query synthesis, which directly generates instances to be annotated instead of selecting existing unlabeled ones. Though synthesizing texts is still a hard problem generally, there have been successful applications for simple classification tasks (Schumann and Rehbein, 2019;Quteineh et al., 2020).",How can active learning be effectively combined with data augmentation techniques to enhance instance querying?,"Augmentation is applicable in active learning and has been explored with iterative back-translation, mixup for sequence labeling, and phrase-to-sentence augmentation for machine translation (MT) (Hu and Neubig, 2021). As discussed, augmentation can also be helpful for instance querying (Jiang et al., 2020; Zhang et al., 2022b). Another interesting scenario involving augmentation and active learning is query synthesis, which directly generates instances to be annotated instead of selecting existing unlabeled ones. Though synthesizing texts is still a hard problem generally, there have been successful applications for simple classification tasks (Schumann and Rehbein, 2019; Quteineh et al., 2020).",How can active learning be effectively combined with data augmentation techniques to enhance instance querying?,"Active learning can be effectively combined with data augmentation techniques to enhance instance querying by leveraging various augmentation strategies. For instance, iterative back-translation, mixup for sequence labeling, and phrase-to-sentence augmentation have been explored in the context of machine translation (MT) (Hu and Neubig, 2021). These techniques can also be beneficial for instance querying, as demonstrated in studies by Jiang et al. (2020) and Zhang et al. (2022b). Additionally, an intriguing approach that combines augmentation with active learning is query synthesis. This method involves directly generating instances to be annotated, rather than selecting from existing unlabeled data. Although synthesizing text remains a challenging task, there have been successful applications in simple classification tasks (Schumann and Rehbein, 2019; Quteineh et al., 2020).",7,4,8,4,"Question:

How can active learning be effectively combined with data augmentation techniques to enhance instance querying?

Answer:

Augmentation is applicable in active learning and has been explored with iterative back-translation, mixup for sequence labeling, and phrase-to-sentence augmentation for machine translation (MT) (Hu and Neubig, 2021). As discussed, augmentation can also be helpful for instance querying (Jiang et al., 2020; Zhang et al., 2022b). Another interesting scenario involving augmentation and active learning is query synthesis, which directly generates instances to be annotated instead of selecting existing unlabeled ones. Though synthesizing texts is still a hard problem generally, there have been successful applications for simple classification tasks (Schumann and Rehbein, 2019; Quteineh et al., 2020).","Question:

How can active learning be effectively combined with data augmentation techniques to enhance instance querying?

Answer:

Active learning can be effectively combined with data augmentation techniques to enhance instance querying by leveraging various augmentation strategies. For instance, iterative back-translation, mixup for sequence labeling, and phrase-to-sentence augmentation have been explored in the context of machine translation (MT) (Hu and Neubig, 2021). These techniques can also be beneficial for instance querying, as demonstrated in studies by Jiang et al. (2020) and Zhang et al. (2022b). Additionally, an intriguing approach that combines augmentation with active learning is query synthesis. This method involves directly generating instances to be annotated, rather than selecting from existing unlabeled data. Although synthesizing text remains a challenging task, there have been successful applications in simple classification tasks (Schumann and Rehbein, 2019; Quteineh et al., 2020).",NO,True,888,True,True
256461177-s12,A Survey of Computational Framing Analysis Approaches,2022,Structural Topic Modeling (STM),"Method. The STM model was also used to explore frames (e.g., Roberts et al., 2014). Compared to LDA topic modeling (Blei, 2012), STM allows including metadata or covariates in the model. With metadata (e.g., political ideology and time) added to the dataset and model, the STM allows researchers to interpret how the topics are associated with those metadata. For example, in terms of political ideology, such as conservatives and lib-erals, researchers might identify a topic as more aligned with conservatives and another topic with liberals. Metadata can also be used in predicting the topics' prevalence by metadata (Gilardi et al., 2021;Nicholls and Culpepper, 2021).

In their study exploring topics in a corpus of newspaper texts, Gilardi et al. (2021)  Review. Like the topic modeling approach (Gilardi et al., 2021), the STM algorithm is also constrained by considering a topic as a frame. So, the STM contains similar limitations in terms of framing analysis. Compared to topic modeling, the STM offers additional insights into the topics or themes through the analysis of covariates. Both methods are based on the bag-of-words idea, indicating the lack of semantic contextualization needed for exploring frames.",How does the inclusion of metadata enhance the interpretability of topics in Structural Topic Modeling (STM)?,"The inclusion of metadata in Structural Topic Modeling (STM) allows researchers to interpret how the topics are associated with those metadata. With metadata such as political ideology and time added to the dataset and model, STM enables the identification of topics aligned with specific ideologies, such as conservatives or liberals. Metadata can also be used in predicting the topics' prevalence by metadata (Gilardi et al., 2021; Nicholls and Culpepper, 2021). Compared to topic modeling, STM offers additional insights into the topics or themes through the analysis of covariates.",How does incorporating metadata improve the interpretability of topics in Structural Topic Modeling (STM)?,"Incorporating metadata into Structural Topic Modeling (STM) significantly enhances the interpretability of topics by allowing researchers to examine how these topics relate to specific metadata attributes. For instance, when metadata such as political ideology or time is included in the dataset and model, STM can identify topics that align with particular ideologies, like conservatism or liberalism. Additionally, metadata can be utilized to predict the prevalence of topics based on these attributes (Gilardi et al., 2021; Nicholls and Culpepper, 2021). Compared to traditional topic modeling, STM provides deeper insights into topics or themes by analyzing covariates, thereby offering a more nuanced understanding of the data.",7,2,7,2,"Question:

How does the inclusion of metadata enhance the interpretability of topics in Structural Topic Modeling (STM)?

Answer:

The inclusion of metadata in Structural Topic Modeling (STM) allows researchers to interpret how the topics are associated with those metadata. With metadata such as political ideology and time added to the dataset and model, STM enables the identification of topics aligned with specific ideologies, such as conservatives or liberals. Metadata can also be used in predicting the topics' prevalence by metadata (Gilardi et al., 2021; Nicholls and Culpepper, 2021). Compared to topic modeling, STM offers additional insights into the topics or themes through the analysis of covariates.","Question:

How does incorporating metadata improve the interpretability of topics in Structural Topic Modeling (STM)?

Answer:

Incorporating metadata into Structural Topic Modeling (STM) significantly enhances the interpretability of topics by allowing researchers to examine how these topics relate to specific metadata attributes. For instance, when metadata such as political ideology or time is included in the dataset and model, STM can identify topics that align with particular ideologies, like conservatism or liberalism. Additionally, metadata can be utilized to predict the prevalence of topics based on these attributes (Gilardi et al., 2021; Nicholls and Culpepper, 2021). Compared to traditional topic modeling, STM provides deeper insights into topics or themes by analyzing covariates, thereby offering a more nuanced understanding of the data.",NO,True,732,True,True
1509090-s7,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,MWE Discovery,"Our survey focuses on interactions of MWE processing with parsing and MT. However, we cannot discuss these interactions without providing an overview of approaches in discovery and identification. Other surveys on these tasks have been previously published (Baldwin and Kim 2010;Seretan 2011;. Our main contributions are to cover the latest advances and group references across languages and MWE categories according to each method's characteristics. Hence, the goal of this section is to define MWE discovery and provide a concise overview of the current state of affairs.

This section describes existing approaches for MWE discovery. As defined in Section 1.3, discovery is a process that takes as input a text and generates a list of MWE candidates, which can be further filtered by human experts before their integration into lexical resources. This process is depicted in Figure 2.

Automatic MWE discovery (hereafter simply referred to as discovery) has been an active research topic since the end of the 1980s when a number of seminal papers were published (Choueka 1988;Church and Hanks 1990). The famous ""pain-in-the-neck"" paper  and the related MWE workshops (Bond et al. 2003) have put discovery in focus as one of the main bottlenecks of NLP technology. Since then, considerable progress has been made, notably in the context of national and international research projects like PARSEME (Savary et al. 2015).

Whereas discovery methods generate lists of MWE types out of context, MWE identification marks MWE tokens in running text. However, several terms have been used to designate what we have defined as discovery in our conceptual framework (Section 1.3), such as identification, extraction, acquisition, dictionary induction, and learning. Because one of the aims of this article is to clearly delineate the tasks of, on the one hand, discovering MWE types, and on the other, identifying MWE tokens in running text (Section 3), discovery seemed the most suitable term at the right level of specificity. Our survey focuses on empirical strategies for MWE discovery as opposed to expert lexicon construction by human language experts. Empirical methods try to automatically learn lexical information from textual data. In practice, empirical and expert methods","How is MWE discovery defined, and what are the key differences between discovery and identification methods?","MWE discovery is defined as a process that takes as input a text and generates a list of MWE candidates, which can be further filtered by human experts before their integration into lexical resources. This process has been an active research topic since the end of the 1980s when a number of seminal papers were published (Choueka 1988; Church and Hanks 1990). Whereas discovery methods generate lists of MWE types out of context, MWE identification marks MWE tokens in running text. Several terms have been used to designate what is defined as discovery, such as identification, extraction, acquisition, dictionary induction, and learning. The article aims to clearly delineate the tasks of discovering MWE types and identifying MWE tokens in running text, with discovery being the most suitable term at the right level of specificity.","What is MWE discovery, and how does it differ from MWE identification methods?","MWE discovery is a process that involves analyzing a text to generate a list of multiword expression (MWE) candidates. These candidates can then be reviewed and filtered by human experts before being integrated into lexical resources. This area of research has been active since the late 1980s, with foundational papers by Choueka (1988) and Church and Hanks (1990) contributing significantly to the field. The key distinction between discovery and identification methods lies in their focus: discovery methods generate lists of MWE types without context, while MWE identification involves marking MWE tokens within running text. Although various terms such as identification, extraction, acquisition, dictionary induction, and learning have been used to describe what is essentially discovery, the article emphasizes the importance of clearly distinguishing between the tasks of discovering MWE types and identifying MWE tokens in text. Discovery is considered the most appropriate term for this process at the right level of specificity.",7,2,7,2,"Question:

How is MWE discovery defined, and what are the key differences between discovery and identification methods?

Answer:

MWE discovery is defined as a process that takes as input a text and generates a list of MWE candidates, which can be further filtered by human experts before their integration into lexical resources. This process has been an active research topic since the end of the 1980s when a number of seminal papers were published (Choueka 1988; Church and Hanks 1990). Whereas discovery methods generate lists of MWE types out of context, MWE identification marks MWE tokens in running text. Several terms have been used to designate what is defined as discovery, such as identification, extraction, acquisition, dictionary induction, and learning. The article aims to clearly delineate the tasks of discovering MWE types and identifying MWE tokens in running text, with discovery being the most suitable term at the right level of specificity.","Question:

What is MWE discovery, and how does it differ from MWE identification methods?

Answer:

MWE discovery is a process that involves analyzing a text to generate a list of multiword expression (MWE) candidates. These candidates can then be reviewed and filtered by human experts before being integrated into lexical resources. This area of research has been active since the late 1980s, with foundational papers by Choueka (1988) and Church and Hanks (1990) contributing significantly to the field. The key distinction between discovery and identification methods lies in their focus: discovery methods generate lists of MWE types without context, while MWE identification involves marking MWE tokens within running text. Although various terms such as identification, extraction, acquisition, dictionary induction, and learning have been used to describe what is essentially discovery, the article emphasizes the importance of clearly distinguishing between the tasks of discovering MWE types and identifying MWE tokens in text. Discovery is considered the most appropriate term for this process at the right level of specificity.",NO,True,1039,True,True
1509090-s10,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Main MWE Discovery Methods,"Some methods are designed to deal with specific categories of MWEs, for instance, focusing on noun compounds (Girju et al. 2005;, lightverb constructions (Stevenson, Fazly, and North 2004), or verb-particle constructions (McCarthy, Keller, and Carroll 2003;Ramisch et al. 2008b). Others are generic and deal uniformly with many MWE categories (da Silva et al. 1999;Seretan 2011). In any case, discovery methods can be differentiated according to the linguistic properties of MWEs that they leverage, some of which have already been discussed (Section 1.2).

r Collocation. Words that are part of an MWE tend to co-occur more often than expected by chance. This property is generally modeled by methods based on association measures such as prominent co-occurrence (Section 2.2.1).

r Non-substitutability. It is not possible to replace part of an expression by a synonym or similar word. This property is generally modeled by variability or fixedness measures (Section 2.2.2).

r Non-compositionality. The meaning of the whole expression cannot be inferred from the meanings of its parts. This property is generally leveraged by models based on vector-space semantic similarity (Section 2.2.3).

r Non-literal translatability. Word-for-word translation tends to generate unnatural, ungrammatical and sometimes nonsensical results. Monolingual and multilingual discovery methods based on translation asymmetries use techniques inspired from MT, and are thus presented later (Section 5.2.1).

These properties are not orthogonal. For example, non-literal translation and non-substitutability are side-effects of non-compositionality, and because noncompositionality is hard to model, such derived properties are additionally used in discovery.

The use of morphosyntactic and syntactic patterns is quite common to generate a first list of MWE candidates 11 of a specific category. For instance, a list of candidate nominal compounds in English can be obtained by looking for nouns preceded by other nouns or adjectives. Justeson and Katz (1995) suggest a limited set of seven bigram and trigram POS patterns, combining nouns and adjectives, in order to discover nominal compound candidates in English. Baldwin (2005) investigates the use of increasingly sophisticated morphosyntactic and syntactic patterns to discover new verb-particle constructions in English.

Because such patterns are used as a preprocessing technique by many discovery strategies presented in the remainder of this section, we do not discuss them in detail. The benefits of using POS-taggers and parsers to help discovery is represented by the unidirectional arrow from parsing to discovery in Figure 1 and will be discussed further in Section 4.2.1.",How are linguistic properties like collocation and non-compositionality used in multiword expression discovery methods?,"Words that are part of a multiword expression (MWE) tend to co-occur more often than expected by chance, a property generally modeled by methods based on association measures such as prominent co-occurrence. Non-compositionality, where the meaning of the whole expression cannot be inferred from the meanings of its parts, is generally leveraged by models based on vector-space semantic similarity. These properties are not orthogonal, as non-literal translation and non-substitutability are side-effects of non-compositionality. Because non-compositionality is hard to model, such derived properties are additionally used in discovery.",How do linguistic properties such as collocation and non-compositionality contribute to methods for discovering multiword expressions?,"In the discovery of multiword expressions (MWEs), linguistic properties like collocation and non-compositionality play crucial roles. Collocation refers to the tendency of words within an MWE to co-occur more frequently than would be expected by chance. This property is typically captured by methods that utilize association measures, focusing on prominent co-occurrence patterns. On the other hand, non-compositionality describes situations where the meaning of the entire expression cannot be deduced from the meanings of its individual components. This property is often leveraged by models that rely on vector-space semantic similarity. It's important to note that these properties are interconnected; for instance, non-literal translation and non-substitutability are often consequences of non-compositionality. Given the challenges in modeling non-compositionality directly, these derived properties are also employed in the discovery process.",7,4,7,4,"Question:

How are linguistic properties like collocation and non-compositionality used in multiword expression discovery methods?

Answer:

Words that are part of a multiword expression (MWE) tend to co-occur more often than expected by chance, a property generally modeled by methods based on association measures such as prominent co-occurrence. Non-compositionality, where the meaning of the whole expression cannot be inferred from the meanings of its parts, is generally leveraged by models based on vector-space semantic similarity. These properties are not orthogonal, as non-literal translation and non-substitutability are side-effects of non-compositionality. Because non-compositionality is hard to model, such derived properties are additionally used in discovery.","Question:

How do linguistic properties such as collocation and non-compositionality contribute to methods for discovering multiword expressions?

Answer:

In the discovery of multiword expressions (MWEs), linguistic properties like collocation and non-compositionality play crucial roles. Collocation refers to the tendency of words within an MWE to co-occur more frequently than would be expected by chance. This property is typically captured by methods that utilize association measures, focusing on prominent co-occurrence patterns. On the other hand, non-compositionality describes situations where the meaning of the entire expression cannot be deduced from the meanings of its individual components. This property is often leveraged by models that rely on vector-space semantic similarity. It's important to note that these properties are interconnected; for instance, non-literal translation and non-substitutability are often consequences of non-compositionality. Given the challenges in modeling non-compositionality directly, these derived properties are also employed in the discovery process.",NO,True,950,True,True
1509090-s11,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Association,"Measures. Prominent co-occurrence, that is, collocation, is the simplest MWE property to model in computational systems. It can be accurately captured by statistical association measures, which estimate the association strength between words in a corpus based on their co-occurrence count and on their individual word counts. Most measures take into account the observed co-occurrence count of a group of n words w 1 , w 2 . . . w n compared with its expected count. The expected co-occurrence count is based on the assumption that words are independent, that is, it equals the product of their individual word probabilities. 12 A popular association measure in MWE discovery is pointwise mutual information. It was first proposed in terminology discovery by Church and Hanks (1990), and can be expressed as the log-ratio between observed and expected counts. Values close to zero indicate independence and the candidate words are discarded, whereas large values indicate probable MWEs. Other measures are based on hypothesis testing. If we assume as null hypothesis that words are independent, their observed and expected counts should be identical. Using a test statistic like Student's t, large values are strong evidence to reject the independence null hypothesis, that is, the candidate words are not independent and probably form an MWE.

More sophisticated test statistics for two-word MWE candidates take into account their contingency table. Examples of such measures are χ 2 and the more robust likelihood ratio (Dunning 1993). Pedersen (1996) suggests using Fisher's exact test in automatic MWE discovery, and this measure is implemented among others in the Text:NSP package. 13 Another measure for MWE discovery is the average and standard deviation of the distance between words, implemented in Xtract (Smadja 1993). Because these measures are based on frequency counts, there have been some studies to use Web hits as an alternative to corpus counts, in order to avoid low-frequency estimates (Keller and Lapata 2003;Ramisch et al. 2008a).

Although association measures work quite well for two-word expressions, they are hard to generalize to arbitrary n-word MWE candidates. One simple approach is to merge two-word MWEs as single tokens and then apply the measure recursively. For instance, in French, the MWE faire un faux pas (lit. to make a false step, 'to make a blunder') can be modeled as the verb faire (to make) combined with the compound faux_pas (blunder), which had been merged due to high association in a previous pass (Seretan 2011). The LocalMaxs algorithm finds optimal MWE boundaries by recursively including left and right context words, stopping when the association decreases (da Silva et al. 1999). 14 A similar approach, using a lexical tightness measure, was proposed to segment Chinese MWEs (Ren et al. 2009).

Association measures can be adapted according to the morphosyntactic nature of lexical elements. Hoang, Kim, and Kan (2009) propose new measures where very frequent words such as prepositions are weighted differently from regular tokens. Comparisons between different association measures have been published, but to date no single best measure has been identified (Pearce 2002;Evert 2005;Pecina 2008;Ramisch, De Araujo, and Villavicencio 2012).",How do statistical association measures estimate the strength of word associations in multiword expression discovery?,"Statistical association measures estimate the association strength between words in a corpus based on their co-occurrence count and on their individual word counts. Most measures take into account the observed co-occurrence count of a group of n words compared with its expected count. The expected co-occurrence count is based on the assumption that words are independent, equaling the product of their individual word probabilities. A popular association measure in MWE discovery is pointwise mutual information, expressed as the log-ratio between observed and expected counts. Values close to zero indicate independence and the candidate words are discarded, whereas large values indicate probable MWEs. Other measures are based on hypothesis testing, where large values of a test statistic like Student's t are strong evidence to reject the independence null hypothesis, indicating that the candidate words probably form an MWE. More sophisticated test statistics for two-word MWE candidates take into account their contingency table, such as χ² and the more robust likelihood ratio. Fisher's exact test is another measure suggested for automatic MWE discovery. Another measure for MWE discovery is the average and standard deviation of the distance between words. These measures are based on frequency counts, and there have been studies to use Web hits as an alternative to corpus counts to avoid low-frequency estimates.",How do statistical association measures determine the strength of word associations in the discovery of multiword expressions (MWEs)?,"Statistical association measures evaluate the strength of word associations in a corpus by analyzing their co-occurrence counts alongside their individual word counts. These measures typically compare the observed co-occurrence count of a group of n words with their expected count, which is calculated under the assumption that the words are independent. This expected count is the product of their individual word probabilities. A widely used measure in MWE discovery is pointwise mutual information, which is expressed as the logarithm of the ratio between observed and expected counts. Values near zero suggest independence, leading to the candidate words being discarded, while larger values indicate potential MWEs.

Other measures involve hypothesis testing, where high values of a test statistic, such as Student's t, provide strong evidence to reject the null hypothesis of independence, suggesting that the candidate words likely form an MWE. More advanced test statistics for two-word MWE candidates consider their contingency table, using methods like χ² and the more robust likelihood ratio. Fisher's exact test is another recommended measure for automatic MWE discovery. Additionally, the average and standard deviation of the distance between words serve as measures for MWE discovery. These methods rely on frequency counts, and some studies have explored using Web hits as an alternative to corpus counts to mitigate low-frequency estimates.",8,2,8,2,"Question:

How do statistical association measures estimate the strength of word associations in multiword expression discovery?

Answer:

Statistical association measures estimate the association strength between words in a corpus based on their co-occurrence count and on their individual word counts. Most measures take into account the observed co-occurrence count of a group of n words compared with its expected count. The expected co-occurrence count is based on the assumption that words are independent, equaling the product of their individual word probabilities. A popular association measure in MWE discovery is pointwise mutual information, expressed as the log-ratio between observed and expected counts. Values close to zero indicate independence and the candidate words are discarded, whereas large values indicate probable MWEs. Other measures are based on hypothesis testing, where large values of a test statistic like Student's t are strong evidence to reject the independence null hypothesis, indicating that the candidate words probably form an MWE. More sophisticated test statistics for two-word MWE candidates take into account their contingency table, such as χ² and the more robust likelihood ratio. Fisher's exact test is another measure suggested for automatic MWE discovery. Another measure for MWE discovery is the average and standard deviation of the distance between words. These measures are based on frequency counts, and there have been studies to use Web hits as an alternative to corpus counts to avoid low-frequency estimates.","Question:

How do statistical association measures determine the strength of word associations in the discovery of multiword expressions (MWEs)?

Answer:

Statistical association measures evaluate the strength of word associations in a corpus by analyzing their co-occurrence counts alongside their individual word counts. These measures typically compare the observed co-occurrence count of a group of n words with their expected count, which is calculated under the assumption that the words are independent. This expected count is the product of their individual word probabilities. A widely used measure in MWE discovery is pointwise mutual information, which is expressed as the logarithm of the ratio between observed and expected counts. Values near zero suggest independence, leading to the candidate words being discarded, while larger values indicate potential MWEs.

Other measures involve hypothesis testing, where high values of a test statistic, such as Student's t, provide strong evidence to reject the null hypothesis of independence, suggesting that the candidate words likely form an MWE. More advanced test statistics for two-word MWE candidates consider their contingency table, using methods like χ² and the more robust likelihood ratio. Fisher's exact test is another recommended measure for automatic MWE discovery. Additionally, the average and standard deviation of the distance between words serve as measures for MWE discovery. These methods rely on frequency counts, and some studies have explored using Web hits as an alternative to corpus counts to mitigate low-frequency estimates.",NO,True,1458,True,True
1509090-s15,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Supervised Learning.,"Supervised learning approaches for discovery use annotated data sets as training material to learn how to distinguish regular word combinations from MWEs. Often, the features used in supervised methods include scores derived from unsupervised methods discussed above, such as association measures and semantic similarity.

The use of these as features has proven to be an effective way to combine scores, giving more weight to more discriminating features and reducing the weight of redundant ones (Ramisch et al. 2008a). It also provides a workaround for the problem of choosing a scoring method for a given data set among dozens of methods proposed in the literature. Furthermore, the learned models can provide insight into features' informativeness (Ramisch et al. 2008b).

One of the first experiments using a supervised approach was proposed by Lapata and Lascarides (2003). The authors use a C4.5 decision tree to classify noun-noun compounds into true MWEs and random co-occurrence. Logistic regression, linear discriminant analysis, support vector machines, and neural networks have been used as classifiers for collocation discovery in Czech and German (Pecina 2008). Rondon, Caseli, and Ramisch (2015) propose an iterative method for the perpetual discovery of novel MWEs. The system requires some initial supervision to build a seed MWE lexicon and classifier, and incrementally enriches it by mining texts in the web and bootstrapping from its results. Yazdani, Farahmand, and Henderson (2015) use light supervision in the form of a list of noun compounds automatically extracted from Wikipedia. They are used as training material to tune their composition function parameters. A similar approach was also used by Farahmand and Henderson (2016) to model MWE substitutability. Supervised methods are generally very precise but cannot be systematically preferred, as they require annotated data sets. Unfortunately, such data sets are usually (1) not readily available, (2) quite small and specific, and (3) not applicable when the target MWEs are highly ambiguous.",What are the limitations of using annotated data sets in supervised methods for multiword expression discovery?,"Supervised methods are generally very precise but cannot be systematically preferred, as they require annotated data sets. Unfortunately, such data sets are usually not readily available. They are quite small and specific. They are not applicable when the target MWEs are highly ambiguous.",What challenges are associated with using annotated data sets in supervised methods for discovering multiword expressions?,"Supervised methods, while generally very precise, face significant challenges when it comes to discovering multiword expressions (MWEs) due to their reliance on annotated data sets. One major limitation is the availability of these data sets, as they are often not readily accessible. Additionally, when they are available, they tend to be quite small and specific, limiting their applicability. Furthermore, these annotated data sets are not suitable for cases where the target MWEs are highly ambiguous, which further restricts their usefulness in diverse linguistic contexts.",7,4,7,4,"Question:

What are the limitations of using annotated data sets in supervised methods for multiword expression discovery?

Answer:

Supervised methods are generally very precise but cannot be systematically preferred, as they require annotated data sets. Unfortunately, such data sets are usually not readily available. They are quite small and specific. They are not applicable when the target MWEs are highly ambiguous.","Question:

What challenges are associated with using annotated data sets in supervised methods for discovering multiword expressions?

Answer:

Supervised methods, while generally very precise, face significant challenges when it comes to discovering multiword expressions (MWEs) due to their reliance on annotated data sets. One major limitation is the availability of these data sets, as they are often not readily accessible. Additionally, when they are available, they tend to be quite small and specific, limiting their applicability. Furthermore, these annotated data sets are not suitable for cases where the target MWEs are highly ambiguous, which further restricts their usefulness in diverse linguistic contexts.",NO,True,578,True,True
1509090-s23,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Rule-Based Methods.,"Rules can be as simple as direct matching, but can also use more sophisticated, context-sensitive constraints encoded as finite-state transducers for instance. Historically, rules based on finite-state transducers offered a simple generic framework to deal with variability, discontiguity, and ambiguity (Gross 1989;Breidt, Segond, and Valetto 1996). Identification methods for contiguous MWEs such as open compounds are generally based on dictionaries compiled into finite-state transducers. Standard pattern matching algorithms for finite-state transducers are then applied to the text to identify MWEs. These approaches have two advantages: Dictionaries are compressed using minimization algorithms and matching is extremely efficient in terms of time complexity.

Nonetheless, all inflected forms of MWEs are listed and usually dictionaries are automatically generated from lists of MWE canonical forms together with inflection rules. An example of this approach is integrated in the MT system Apertium (Forcada et al. 2011). The variable part of MWE lexical entries, as the verb echar in the Spanish expression echar de menos (lit. throw from less, 'to miss'), is inflected according to a regular verbal inflection paradigm. The inflection process may be based on finite-state transducers as in Silberztein (1997), possibly augmented with a unification mechanism for handling agreement between the MWE components (Savary 2009). These approaches are extremely precise, but costly. The manual assignment of inflection rules may be eased by tools like Leximir for predicting inflection classes (Krstev et al. 2013).

Another approach comprises two processing stages: morphological analysis of simple words followed by a composition of regular rules to identify MWEs, as in Oflazer, Çetinoglu, and Say (2004) for Turkish. Breidt, Segond, and Valetto (1996) design regular rules that handle morphological variations and restrictions like the French idiom perdre ADV* :la :tête (lit. lose ADV* :the :head, 'to lose one's mind'), 16 lexical and structural variations (birth date = date of birth). Copestake et al. (2002) design an MWE lexicon for English based on typed feature structures that may rely on analysis of internal words of MWE. Silberztein (1997) also proposes the use of local grammars in the form of equivalence graphs. These approaches are very efficient in dealing with variability and short-distance discontiguity.

Constraints encoded in the lexicon, such as obligatory or forbidden transformations, can be projected on text to disambiguate idiomatic constructions. Hashimoto, Sato, and Utsuro (2006) encode in a lexicon detailed properties of 100 Japanese verb-noun ambiguous idioms such as voice, adnominal modifications, modality, and selectional restrictions. Then, they only classify as idioms those occurrences that match the constraints in a dependency-parsed test set.

More recent approaches to rule-based identification use dictionaries containing canonical MWE forms with no additional constraints. They consist of two stages: (1) POS tagging and lemmatizing the text and (2) performing dictionary lookup (Carpuat and Diab 2010;Ghoneim and Diab 2013). The lookup relies on a maximum forward matching algorithm that locates the longest matching MWE. This simple method handles morphological variants of the MWEs, but tends to overgenerate them. This overgeneration is due to strong morphological constraints on some elements or agreement. For instance, the French idiom prendre la porte (lit. take the door) meaning get sacked has a strong morphological constraint: the noun porte (door) must be in the singular; if it is in the plural, the sequence has its literal meaning. Therefore, using lemmas to identify variants is a potential source of mistakes.

To handle discontiguity, it is possible to apply patterns on such preprocessed texts, including wildcards. For instance, Ramisch, Besacier, and Kobzar (2013) identify discontiguous verb-particle constructions in English made of a verb + at most five words + a particle, adapting the discovery method proposed by Baldwin (2005). General tools for deterministic MWE annotation like the mwetoolkit  17 allow fine tuning of matching heuristics, minimum and maximum gap size, surrounding POS patterns, and local variants. For example, it is possible to identify verb-particle constructions in which the particles up or down appear not further than five words after a content verb, constraining intervening words not to be verbs and the next word not to be the word there (to avoid regular verbs followed by up/down there). Some rule-based identification approaches output ambiguous MWE segmentation, postponing disambiguation until more linguistic context is available (Chanod and Tapanainen 1996). The MWE identification process often generates acyclic finite-state automata representing all possible segmentations for a given input sentence. Some finite state preprocessing tools allow ambiguous lexical analyses, like Intex, Macaon,Nooj,SxPipe,and Unitex. 19 This approach can be used as a preprocessing stage of MWEaware parsing (Section 4) and as a source of features for sequence-tagging identification (Section 3.2.3).","How do rule-based methods handle variability, discontiguity, and ambiguity in multiword expression processing?","Rules based on finite-state transducers offer a simple generic framework to deal with variability, discontiguity, and ambiguity. Identification methods for contiguous MWEs such as open compounds are generally based on dictionaries compiled into finite-state transducers. Standard pattern matching algorithms for finite-state transducers are then applied to the text to identify MWEs. These approaches have two advantages: Dictionaries are compressed using minimization algorithms and matching is extremely efficient in terms of time complexity. Nonetheless, all inflected forms of MWEs are listed and usually dictionaries are automatically generated from lists of MWE canonical forms together with inflection rules. The inflection process may be based on finite-state transducers, possibly augmented with a unification mechanism for handling agreement between the MWE components. These approaches are extremely precise, but costly. Another approach comprises two processing stages: morphological analysis of simple words followed by a composition of regular rules to identify MWEs. Regular rules handle morphological variations and restrictions like idioms with lexical and structural variations. Constraints encoded in the lexicon, such as obligatory or forbidden transformations, can be projected on text to disambiguate idiomatic constructions. More recent approaches to rule-based identification use dictionaries containing canonical MWE forms with no additional constraints. They consist of two stages: (1) POS tagging and lemmatizing the text and (2) performing dictionary lookup. The lookup relies on a maximum forward matching algorithm that locates the longest matching MWE. This simple method handles morphological variants of the MWEs, but tends to overgenerate them. To handle discontiguity, it is possible to apply patterns on such preprocessed texts, including wildcards. General tools for deterministic MWE annotation allow fine-tuning of matching heuristics, minimum and maximum gap size, surrounding POS patterns, and local variants. Some rule-based identification approaches output ambiguous MWE segmentation, postponing disambiguation until more linguistic context is available. The MWE identification process often generates acyclic finite-state automata representing all possible segmentations for a given input sentence.","How do rule-based methods address the challenges of variability, discontiguity, and ambiguity in processing multiword expressions (MWEs)?","Rule-based methods, particularly those utilizing finite-state transducers, provide a robust framework for managing the challenges of variability, discontiguity, and ambiguity in multiword expression (MWE) processing. For contiguous MWEs, such as open compounds, identification methods often rely on dictionaries that are compiled into finite-state transducers. These transducers apply standard pattern matching algorithms to the text, efficiently identifying MWEs. This approach offers two main advantages: the dictionaries are compressed using minimization algorithms, and the matching process is highly efficient in terms of time complexity. However, all inflected forms of MWEs must be listed, and dictionaries are typically generated automatically from lists of canonical MWE forms along with inflection rules. The inflection process itself may utilize finite-state transducers, potentially enhanced with a unification mechanism to manage agreement between MWE components. While these methods are extremely precise, they can be costly.

Another approach involves two stages: first, performing morphological analysis of simple words, followed by the application of regular rules to identify MWEs. These regular rules accommodate morphological variations and restrictions, such as idioms with lexical and structural variations. Constraints encoded in the lexicon, like obligatory or forbidden transformations, can be projected onto the text to disambiguate idiomatic constructions. More recent rule-based identification methods use dictionaries containing canonical MWE forms without additional constraints. This involves two steps: (1) POS tagging and lemmatizing the text, and (2) performing a dictionary lookup using a maximum forward matching algorithm to locate the longest matching MWE. While this method effectively handles morphological variants, it may overgenerate them.

To address discontiguity, patterns can be applied to preprocessed texts, including the use of wildcards. General tools for deterministic MWE annotation allow for fine-tuning of matching heuristics, such as minimum and maximum gap size, surrounding POS patterns, and local variants. Some rule-based identification approaches produce ambiguous MWE segmentation, deferring disambiguation until more linguistic context is available. The MWE identification process often results in acyclic finite-state automata that represent all possible segmentations for a given input sentence.",7,2,7,2,"Question:

How do rule-based methods handle variability, discontiguity, and ambiguity in multiword expression processing?

Answer:

Rules based on finite-state transducers offer a simple generic framework to deal with variability, discontiguity, and ambiguity. Identification methods for contiguous MWEs such as open compounds are generally based on dictionaries compiled into finite-state transducers. Standard pattern matching algorithms for finite-state transducers are then applied to the text to identify MWEs. These approaches have two advantages: Dictionaries are compressed using minimization algorithms and matching is extremely efficient in terms of time complexity. Nonetheless, all inflected forms of MWEs are listed and usually dictionaries are automatically generated from lists of MWE canonical forms together with inflection rules. The inflection process may be based on finite-state transducers, possibly augmented with a unification mechanism for handling agreement between the MWE components. These approaches are extremely precise, but costly. Another approach comprises two processing stages: morphological analysis of simple words followed by a composition of regular rules to identify MWEs. Regular rules handle morphological variations and restrictions like idioms with lexical and structural variations. Constraints encoded in the lexicon, such as obligatory or forbidden transformations, can be projected on text to disambiguate idiomatic constructions. More recent approaches to rule-based identification use dictionaries containing canonical MWE forms with no additional constraints. They consist of two stages: (1) POS tagging and lemmatizing the text and (2) performing dictionary lookup. The lookup relies on a maximum forward matching algorithm that locates the longest matching MWE. This simple method handles morphological variants of the MWEs, but tends to overgenerate them. To handle discontiguity, it is possible to apply patterns on such preprocessed texts, including wildcards. General tools for deterministic MWE annotation allow fine-tuning of matching heuristics, minimum and maximum gap size, surrounding POS patterns, and local variants. Some rule-based identification approaches output ambiguous MWE segmentation, postponing disambiguation until more linguistic context is available. The MWE identification process often generates acyclic finite-state automata representing all possible segmentations for a given input sentence.","Question:

How do rule-based methods address the challenges of variability, discontiguity, and ambiguity in processing multiword expressions (MWEs)?

Answer:

Rule-based methods, particularly those utilizing finite-state transducers, provide a robust framework for managing the challenges of variability, discontiguity, and ambiguity in multiword expression (MWE) processing. For contiguous MWEs, such as open compounds, identification methods often rely on dictionaries that are compiled into finite-state transducers. These transducers apply standard pattern matching algorithms to the text, efficiently identifying MWEs. This approach offers two main advantages: the dictionaries are compressed using minimization algorithms, and the matching process is highly efficient in terms of time complexity. However, all inflected forms of MWEs must be listed, and dictionaries are typically generated automatically from lists of canonical MWE forms along with inflection rules. The inflection process itself may utilize finite-state transducers, potentially enhanced with a unification mechanism to manage agreement between MWE components. While these methods are extremely precise, they can be costly.

Another approach involves two stages: first, performing morphological analysis of simple words, followed by the application of regular rules to identify MWEs. These regular rules accommodate morphological variations and restrictions, such as idioms with lexical and structural variations. Constraints encoded in the lexicon, like obligatory or forbidden transformations, can be projected onto the text to disambiguate idiomatic constructions. More recent rule-based identification methods use dictionaries containing canonical MWE forms without additional constraints. This involves two steps: (1) POS tagging and lemmatizing the text, and (2) performing a dictionary lookup using a maximum forward matching algorithm to locate the longest matching MWE. While this method effectively handles morphological variants, it may overgenerate them.

To address discontiguity, patterns can be applied to preprocessed texts, including the use of wildcards. General tools for deterministic MWE annotation allow for fine-tuning of matching heuristics, such as minimum and maximum gap size, surrounding POS patterns, and local variants. Some rule-based identification approaches produce ambiguous MWE segmentation, deferring disambiguation until more linguistic context is available. The MWE identification process often results in acyclic finite-state automata that represent all possible segmentations for a given input sentence.",NO,True,2460,True,True
1509090-s24,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Sense Disambiguation Methods.,"Methods inspired by word sense disambiguation treat MWE identification as a specialized in-context classification task. Given a candidate combination in context, the classifier must decide whether it is a true MWE or just a regular co-occurrence. Common features for this task include surrounding words, their POS, lemmas, syntactic characteristics, and distributional information. Such methods often do not cover the identification of candidates. They assume that another process pre-identifies potentially idiosyncratic combinations, and instead focus on detecting which of these are true MWEs. We discuss both supervised and unsupervised classifiers. Uchiyama, Baldwin, and Ishizaki (2005) tackle the problem of identifying Japanese verb compounds. Sense labels correspond to the meaning added by the second verb (aspectual, spatial, adverbial) with respect to the first verb. Their support vector machine guesses the possible semantic classes of a given verb combination, using the semantic classes of other co-occurring verbs as features. Then, in a second step, identification proper is done simply by taking the most frequent sense. Hashimoto and Kawahara (2008) propose a supervised disambiguation system able to distinguish literal from idiomatic uses of Japanese idioms. Fothergill and Baldwin (2012) perform an extended evaluation using the same data set and methodology, including new features, a feature ablation study, and cross-idiom tests. Similar approaches based on support vector machines and surface-level features have also been proposed for English light-verb constructions and verb-particle constructions (Tu 2012). Birke and Sarkar (2006) present a nearly unsupervised system capable of distinguishing literal from non-literal verb uses. It uses a clustering strategy that tries to maximize transitive similarity with the seed set of literal or non-literal sentences using standard features. Sporleder and Li (2009) propose a completely unsupervised method based on lexical chains and text cohesion graphs. Their classifier considers an expression as literal if its presence in the sentence does not have a negative impact on cohesion, defined as the similarity between co-occurring words. For instance, play with fire reinforces cohesion in a sentence containing grilling, coals, and cooking but it would reduce lexical cohesion in a chain containing diplomacy, minister, and accuse. Katz and Giesbrecht (2006), detecting idiomatic verb-noun expressions in German, assume that the context of an idiomatic MWE differs from the contexts of its literal uses. Given two distributional vectors representing literal and idiomatic instances, a test instance is classified according to its similarity to the respective vectors. Cook, Fazly, and Stevenson (2007) propose a similar method based on canonical forms learned automatically from large corpora. Once a canonical form is recognized, distributional vectors for canonical and non-canonical forms are learned and then an instance is classified as idiomatic if it is closer to the canonical form vectors. Boukobza and Rappoport (2009) mix the supervised and unsupervised approach of Fazly, Cook, and Stevenson (2009) into a single supervised method to identify English verbal MWEs. In addition to literal and idiomatic uses, they also discuss and model accidental co-occurrence in which the member words of an MWE appear together only by chance, proposing the use of specialized multi-way support vector machines for each target candidate. Besides surface contextual features, they exploit the use of automatically obtained syntactic dependencies as features, which sometimes improves precision.",How do methods inspired by word sense disambiguation handle the identification and classification of multiword expressions?,"Methods inspired by word sense disambiguation treat MWE identification as a specialized in-context classification task. Given a candidate combination in context, the classifier must decide whether it is a true MWE or just a regular co-occurrence. Common features for this task include surrounding words, their POS, lemmas, syntactic characteristics, and distributional information. Such methods often do not cover the identification of candidates. They assume that another process pre-identifies potentially idiosyncratic combinations, and instead focus on detecting which of these are true MWEs. Uchiyama, Baldwin, and Ishizaki (2005) tackle the problem of identifying Japanese verb compounds. Sense labels correspond to the meaning added by the second verb (aspectual, spatial, adverbial) with respect to the first verb. Their support vector machine guesses the possible semantic classes of a given verb combination, using the semantic classes of other co-occurring verbs as features. Then, in a second step, identification proper is done simply by taking the most frequent sense. Hashimoto and Kawahara (2008) propose a supervised disambiguation system able to distinguish literal from idiomatic uses of Japanese idioms. Fothergill and Baldwin (2012) perform an extended evaluation using the same data set and methodology, including new features, a feature ablation study, and cross-idiom tests. Similar approaches based on support vector machines and surface-level features have also been proposed for English light-verb constructions and verb-particle constructions (Tu 2012). Birke and Sarkar (2006) present a nearly unsupervised system capable of distinguishing literal from non-literal verb uses. It uses a clustering strategy that tries to maximize transitive similarity with the seed set of literal or non-literal sentences using standard features. Sporleder and Li (2009) propose a completely unsupervised method based on lexical chains and text cohesion graphs. Their classifier considers an expression as literal if its presence in the sentence does not have a negative impact on cohesion, defined as the similarity between co-occurring words. Katz and Giesbrecht (2006), detecting idiomatic verb-noun expressions in German, assume that the context of an idiomatic MWE differs from the contexts of its literal uses. Given two distributional vectors representing literal and idiomatic instances, a test instance is classified according to its similarity to the respective vectors. Cook, Fazly, and Stevenson (2007) propose a similar method based on canonical forms learned automatically from large corpora. Once a canonical form is recognized, distributional vectors for canonical and non-canonical forms are learned and then an instance is classified as idiomatic if it is closer to the canonical form vectors. Boukobza and Rappoport (2009) mix the supervised and unsupervised approach of Fazly, Cook, and Stevenson (2009) into a single supervised method to identify English verbal MWEs. In addition to literal and idiomatic uses, they also discuss and model accidental co-occurrence in which the member words of an MWE appear together only by chance, proposing the use of specialized multi-way support vector machines for each target candidate. Besides surface contextual features, they exploit the use of automatically obtained syntactic dependencies as features, which sometimes improves precision.",How do methods inspired by word sense disambiguation approach the identification and classification of multiword expressions (MWEs)?,"Methods inspired by word sense disambiguation approach the identification and classification of multiword expressions (MWEs) by treating it as a specialized in-context classification task. These methods focus on determining whether a given candidate combination in context is a true MWE or merely a regular co-occurrence. Common features used in this task include surrounding words, their parts of speech (POS), lemmas, syntactic characteristics, and distributional information. Typically, these methods do not handle the initial identification of candidate expressions; instead, they assume that another process has pre-identified potentially idiosyncratic combinations and focus on detecting which of these are true MWEs.

For instance, Uchiyama, Baldwin, and Ishizaki (2005) address the identification of Japanese verb compounds by using a support vector machine to predict the possible semantic classes of a given verb combination, based on the semantic classes of other co-occurring verbs. The identification is then completed by selecting the most frequent sense. Hashimoto and Kawahara (2008) propose a supervised disambiguation system to distinguish between literal and idiomatic uses of Japanese idioms. Fothergill and Baldwin (2012) extend this evaluation by incorporating new features and conducting a feature ablation study and cross-idiom tests.

Similar approaches have been applied to English light-verb constructions and verb-particle constructions, as demonstrated by Tu (2012). Birke and Sarkar (2006) present a nearly unsupervised system that distinguishes between literal and non-literal verb uses through a clustering strategy that maximizes transitive similarity with a seed set of literal or non-literal sentences. Sporleder and Li (2009) propose a completely unsupervised method based on lexical chains and text cohesion graphs, classifying an expression as literal if its presence does not negatively impact cohesion.

In the context of detecting idiomatic verb-noun expressions in German, Katz and Giesbrecht (2006) assume that the context of an idiomatic MWE differs from its literal uses. They classify a test instance based on its similarity to distributional vectors representing literal and idiomatic instances. Cook, Fazly, and Stevenson (2007) propose a similar method that uses canonical forms learned from large corpora to classify instances as idiomatic if they are closer to the canonical form vectors.

Boukobza and Rappoport (2009) integrate the supervised and unsupervised approaches of Fazly, Cook, and Stevenson (2009) into a single supervised method for identifying English verbal MWEs. They address not only literal and idiomatic uses but also accidental co-occurrences, where the member words of an MWE appear together by chance. They propose using specialized multi-way support vector machines for each target candidate and exploit automatically obtained syntactic dependencies as features, which can sometimes improve precision.",7,4,7,6,"Question:

How do methods inspired by word sense disambiguation handle the identification and classification of multiword expressions?

Answer:

Methods inspired by word sense disambiguation treat MWE identification as a specialized in-context classification task. Given a candidate combination in context, the classifier must decide whether it is a true MWE or just a regular co-occurrence. Common features for this task include surrounding words, their POS, lemmas, syntactic characteristics, and distributional information. Such methods often do not cover the identification of candidates. They assume that another process pre-identifies potentially idiosyncratic combinations, and instead focus on detecting which of these are true MWEs. Uchiyama, Baldwin, and Ishizaki (2005) tackle the problem of identifying Japanese verb compounds. Sense labels correspond to the meaning added by the second verb (aspectual, spatial, adverbial) with respect to the first verb. Their support vector machine guesses the possible semantic classes of a given verb combination, using the semantic classes of other co-occurring verbs as features. Then, in a second step, identification proper is done simply by taking the most frequent sense. Hashimoto and Kawahara (2008) propose a supervised disambiguation system able to distinguish literal from idiomatic uses of Japanese idioms. Fothergill and Baldwin (2012) perform an extended evaluation using the same data set and methodology, including new features, a feature ablation study, and cross-idiom tests. Similar approaches based on support vector machines and surface-level features have also been proposed for English light-verb constructions and verb-particle constructions (Tu 2012). Birke and Sarkar (2006) present a nearly unsupervised system capable of distinguishing literal from non-literal verb uses. It uses a clustering strategy that tries to maximize transitive similarity with the seed set of literal or non-literal sentences using standard features. Sporleder and Li (2009) propose a completely unsupervised method based on lexical chains and text cohesion graphs. Their classifier considers an expression as literal if its presence in the sentence does not have a negative impact on cohesion, defined as the similarity between co-occurring words. Katz and Giesbrecht (2006), detecting idiomatic verb-noun expressions in German, assume that the context of an idiomatic MWE differs from the contexts of its literal uses. Given two distributional vectors representing literal and idiomatic instances, a test instance is classified according to its similarity to the respective vectors. Cook, Fazly, and Stevenson (2007) propose a similar method based on canonical forms learned automatically from large corpora. Once a canonical form is recognized, distributional vectors for canonical and non-canonical forms are learned and then an instance is classified as idiomatic if it is closer to the canonical form vectors. Boukobza and Rappoport (2009) mix the supervised and unsupervised approach of Fazly, Cook, and Stevenson (2009) into a single supervised method to identify English verbal MWEs. In addition to literal and idiomatic uses, they also discuss and model accidental co-occurrence in which the member words of an MWE appear together only by chance, proposing the use of specialized multi-way support vector machines for each target candidate. Besides surface contextual features, they exploit the use of automatically obtained syntactic dependencies as features, which sometimes improves precision.","Question:

How do methods inspired by word sense disambiguation approach the identification and classification of multiword expressions (MWEs)?

Answer:

Methods inspired by word sense disambiguation approach the identification and classification of multiword expressions (MWEs) by treating it as a specialized in-context classification task. These methods focus on determining whether a given candidate combination in context is a true MWE or merely a regular co-occurrence. Common features used in this task include surrounding words, their parts of speech (POS), lemmas, syntactic characteristics, and distributional information. Typically, these methods do not handle the initial identification of candidate expressions; instead, they assume that another process has pre-identified potentially idiosyncratic combinations and focus on detecting which of these are true MWEs.

For instance, Uchiyama, Baldwin, and Ishizaki (2005) address the identification of Japanese verb compounds by using a support vector machine to predict the possible semantic classes of a given verb combination, based on the semantic classes of other co-occurring verbs. The identification is then completed by selecting the most frequent sense. Hashimoto and Kawahara (2008) propose a supervised disambiguation system to distinguish between literal and idiomatic uses of Japanese idioms. Fothergill and Baldwin (2012) extend this evaluation by incorporating new features and conducting a feature ablation study and cross-idiom tests.

Similar approaches have been applied to English light-verb constructions and verb-particle constructions, as demonstrated by Tu (2012). Birke and Sarkar (2006) present a nearly unsupervised system that distinguishes between literal and non-literal verb uses through a clustering strategy that maximizes transitive similarity with a seed set of literal or non-literal sentences. Sporleder and Li (2009) propose a completely unsupervised method based on lexical chains and text cohesion graphs, classifying an expression as literal if its presence does not negatively impact cohesion.

In the context of detecting idiomatic verb-noun expressions in German, Katz and Giesbrecht (2006) assume that the context of an idiomatic MWE differs from its literal uses. They classify a test instance based on its similarity to distributional vectors representing literal and idiomatic instances. Cook, Fazly, and Stevenson (2007) propose a similar method that uses canonical forms learned from large corpora to classify instances as idiomatic if they are closer to the canonical form vectors.

Boukobza and Rappoport (2009) integrate the supervised and unsupervised approaches of Fazly, Cook, and Stevenson (2009) into a single supervised method for identifying English verbal MWEs. They address not only literal and idiomatic uses but also accidental co-occurrences, where the member words of an MWE appear together by chance. They propose using specialized multi-way support vector machines for each target candidate and exploit automatically obtained syntactic dependencies as features, which can sometimes improve precision.",NO,True,2975,True,True
1509090-s27,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Open Issues in MWE Identification,"In spite of receiving less attention than MWE discovery, MWE identification has made significant progress. On the one hand, experiments have shown that it is possible to identify specific MWE categories, especially ambiguous ones such as verbal idioms, using mostly unsupervised models (Katz and Giesbrecht 2006;Boukobza and Rappoport 2009;Fazly, Cook, and Stevenson 2009). On the other hand, supervised taggers have been successfully used to learn more general MWE identification models capable of handling several MWE categories simultaneously (Vincze, Nagy, and Berend 2011;Constant, Sigogne, and Watrin 2012;Schneider et al. 2014a).

Achieving broad-coverage MWE identification is still an open issue for both unsupervised and supervised methods. Unsupervised methods are usually evaluated on small data sets and it is unclear to what extent the proposed models are generalizable. Supervised methods require sufficient training data and do not perform well on rare MWEs, which have not been seen often enough in the training data. Integrating the approaches presented in this section, for example, using unsupervised features in supervised taggers, could be a promising research direction to address this issue.

Moreover, current identification models cannot always properly model and recognize discontiguous and overlapping expressions. As for discontiguous MWEs, the use of parsers can help (Section 4). As for overlap, some approaches can deal with nesting (Schneider et al. 2014a) but other types of overlap are considered sufficiently rare to be safely ignored. For example, partial overlapping like in pay 1 close 2 attention 1,2 containing the expressions pay attention and close attention is usually ignored. Although it is not straightforward to model overlapping MWEs within taggers and parsers, it would be interesting to develop new identification models that can elegantly handle overlap.

The success of MWE identification for languages like English and French has relied heavily on high-quality lexicons and annotated corpora, which are rare resources. Broad-coverage hand-crafted MWE lexicons take years to build, and the use of faster, automatic discovery methods directly for identification, bypassing lexicographers, has not been sufficiently studied. Furthermore, annotated corpora containing MWEs are often constructed for other purposes (e.g., treebanks), and MWE annotation is not always consistent (Green et al. 2011). Even when the annotation was performed explicitly for identification purposes, consistency problems always occur because of the complex nature of MWEs (Hollenstein, Schneider, and Webber 2016). Hence, the development of robust annotation guidelines and coherently annotated corpora is a bottleneck that requires attention in the near future.

The use of end-to-end sequence taggers based on recurrent and/or deep neural networks looks promising (Legrand and Collobert 2016) and remains to be explored. One of the potential advantages of these methods is that they can deal with word vectors (embeddings) that are of a semantic nature. Because MWEs are closely related to semantic compositionality, such models could learn how to tag MWEs when the compositionality of word vectors is breached.",What are the current limitations and challenges in recognizing discontiguous and overlapping MWEs in identification models?,"Achieving broad-coverage MWE identification is still an open issue for both unsupervised and supervised methods. Current identification models cannot always properly model and recognize discontiguous and overlapping expressions. As for discontiguous MWEs, the use of parsers can help. As for overlap, some approaches can deal with nesting (Schneider et al. 2014a) but other types of overlap are considered sufficiently rare to be safely ignored. For example, partial overlapping like in pay 1 close 2 attention 1,2 containing the expressions pay attention and close attention is usually ignored. Although it is not straightforward to model overlapping MWEs within taggers and parsers, it would be interesting to develop new identification models that can elegantly handle overlap.",What are the current limitations and challenges faced by identification models in recognizing discontiguous and overlapping multiword expressions (MWEs)?,"The task of achieving broad-coverage identification of multiword expressions (MWEs) remains an open challenge for both unsupervised and supervised methods. Current identification models often struggle to accurately model and recognize discontiguous and overlapping expressions. For discontiguous MWEs, the use of parsers can provide some assistance. When it comes to overlapping expressions, certain approaches can handle nesting, as noted by Schneider et al. (2014a). However, other types of overlap are considered sufficiently rare and are often ignored. For instance, partial overlapping, such as in the phrase ""pay 1 close 2 attention 1,2,"" which contains the expressions ""pay attention"" and ""close attention,"" is typically overlooked. Although modeling overlapping MWEs within taggers and parsers is not straightforward, developing new identification models that can elegantly handle overlap would be an interesting advancement.",7,4,5,4,"Question:

What are the current limitations and challenges in recognizing discontiguous and overlapping MWEs in identification models?

Answer:

Achieving broad-coverage MWE identification is still an open issue for both unsupervised and supervised methods. Current identification models cannot always properly model and recognize discontiguous and overlapping expressions. As for discontiguous MWEs, the use of parsers can help. As for overlap, some approaches can deal with nesting (Schneider et al. 2014a) but other types of overlap are considered sufficiently rare to be safely ignored. For example, partial overlapping like in pay 1 close 2 attention 1,2 containing the expressions pay attention and close attention is usually ignored. Although it is not straightforward to model overlapping MWEs within taggers and parsers, it would be interesting to develop new identification models that can elegantly handle overlap.","Question:

What are the current limitations and challenges faced by identification models in recognizing discontiguous and overlapping multiword expressions (MWEs)?

Answer:

The task of achieving broad-coverage identification of multiword expressions (MWEs) remains an open challenge for both unsupervised and supervised methods. Current identification models often struggle to accurately model and recognize discontiguous and overlapping expressions. For discontiguous MWEs, the use of parsers can provide some assistance. When it comes to overlapping expressions, certain approaches can handle nesting, as noted by Schneider et al. (2014a). However, other types of overlap are considered sufficiently rare and are often ignored. For instance, partial overlapping, such as in the phrase ""pay 1 close 2 attention 1,2,"" which contains the expressions ""pay attention"" and ""close attention,"" is typically overlooked. Although modeling overlapping MWEs within taggers and parsers is not straightforward, developing new identification models that can elegantly handle overlap would be an interesting advancement.",NO,True,933,True,True
1509090-s29,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Motivations and Challenges for MWE-Aware Parsing,"The motivation for MWE-aware parsing is 3-fold: (1) to improve the syntactic parsing performances on sentences containing MWEs (both on internal MWE structure and on the surrounding sentence structure), (2) to improve MWE identification performance, and (3) to improve MWE discovery performance. The latter two items rely on the fact that processing of some MWEs hinges on their syntactic analysis. Parsing faces different challenges with respect to identification because of non-compositionality and ambiguity (Section 1.2). Paradoxically, both challenges may be tackled using parsing. Besides these, MWE-aware parsing addresses the other MWE-related challenges discussed in Section 1.2.

Ambiguity. An MWE might be ambiguous between accidental co-occurrence and literal and idiomatic uses. An incorrect identification can mislead the parser. In particular, complex function words that have a key role in syntax may be ambiguous (e.g., up to). For instance, in John looked up to the sky, the sequence up to should not be identified as a multiword preposition. If so, it would prevent the right analysis: (John) ((looked) (up to the sky)) instead of (John) ((looked) (up) (to the sky)).

Conversely, combining MWE identification and parsing can help resolve such ambiguities, yielding both better identification and parsing models. Multiword function words such as complex prepositions, conjunctions, and adverbials (up to, now that, by the way) can be disambiguated by their syntactic context (Nasr et al. 2015). For example, the sequence de la in French can be either a compositional sequence (preposition de + determiner la), or a complex partitive determiner, as shown in the following examples and their corresponding syntactic analyses in Figure 5:

(1) Je parle de la voiture I talk about the car (2) Je mange de la soupe I eat some soup MWE-aware parsing is a natural way to solve this ambiguity. The intransitive verb parle (talk) selects the preposition de (about), whereas mange (eat) requires a noun phrase as its object. Furthermore, one of the main challenges of parsing in general is attachment ambiguity. As MWEs tend to form full syntactic constituents, their identification can guide attachment decisions (Wehrli, Seretan, and Nerima 2010).

Non-compositionality. MWEs can be defined as exceptions to regular composition rules. This non-compositionality can take place at different levels: morphological, distributional, syntactic, semantic, and pragmatic. In particular, some expressions display syntactic irregularity in their internal structure-that is, they are irregular with respect to a grammar. Therefore, if the parser is not aware of the existence of such cases, the analysis will fail. For instance, the adverbial by and large is the coordination of a  Syntactic analysis of the discontiguous verbal MWE take into account using the universal dependencies annotation scheme.

preposition and an adjective that is an irregular pattern for a syntactic constituent in English.

Conversely, depending on the type of parser used, some types of syntactic noncompositionality can be captured by a parser as, like by and large, they violate standard grammatical rules. For example, informing a discriminative parser with POS n-gram features may help capture such non-compositionality. Here too, higher levels of noncompositionality are not directly relevant for parsing because they do not interfere with the resulting parse tree, even though information about non-compositional MWEs can guide the parser.

Discontiguity. Some MWEs can appear in discontiguous configurations, like give up in John gave it up, and take into account in Mary took your argument into account. The identification of such discontiguous MWEs can hardly be handled by purely sequential approaches (Section 3.2.3), except maybe for special cases when gaps are short (Schneider et al. 2014a). Because syntactic links can relate non-adjacent words, parsing may help the identification of discontiguous MWEs (Seretan 2011). In Figure 6, representing the syntactic analysis of a discontiguous instance of the expression take into account, the verb take is a syntactic neighbor of the noun account, which should facilitate the identification of the whole expression.

Variability. Flexible MWEs may undergo syntactic variations. For instance, lightverb constructions accept verbal inflection (make/made/making a decision), passivization (John made a decision → a decision was made by John), and insertion of free modifiers (John made an important decision). As parsers provide syntactic structure, they can be useful to capture and aggregate MWE variants for discovery (Seretan 2011). In MWE identification, though, parsers need to take variability into account when matching MWE dictionary entries with their instances in text.

The main challenge for MWE-aware parsing is the orchestration of MWE identification and parsing, that is, the question of when one task should be performed with respect to the other (Section 4.2).",How does MWE-aware parsing address the challenges of non-compositionality and discontiguity in multiword expressions?,"MWEs can be defined as exceptions to regular composition rules, and this non-compositionality can take place at different levels: morphological, distributional, syntactic, semantic, and pragmatic. In particular, some expressions display syntactic irregularity in their internal structure—that is, they are irregular with respect to a grammar. Therefore, if the parser is not aware of the existence of such cases, the analysis will fail. Conversely, depending on the type of parser used, some types of syntactic non-compositionality can be captured by a parser as they violate standard grammatical rules. For example, informing a discriminative parser with POS n-gram features may help capture such non-compositionality. Discontiguity in MWEs can appear in configurations like ""give up"" in ""John gave it up,"" and ""take into account"" in ""Mary took your argument into account."" The identification of such discontiguous MWEs can hardly be handled by purely sequential approaches. Because syntactic links can relate non-adjacent words, parsing may help the identification of discontiguous MWEs. In the syntactic analysis of a discontiguous instance of the expression ""take into account,"" the verb ""take"" is a syntactic neighbor of the noun ""account,"" which should facilitate the identification of the whole expression.",How does MWE-aware parsing effectively tackle the challenges posed by non-compositionality and discontiguity in multiword expressions?,"Multiword expressions (MWEs) often defy regular compositional rules, exhibiting non-compositionality at various levels, including morphological, distributional, syntactic, semantic, and pragmatic. This non-compositionality can manifest as syntactic irregularities within their internal structure, making them exceptions to standard grammatical rules. If a parser is not designed to recognize these irregularities, it may fail to analyze them correctly. However, certain types of parsers, such as discriminative parsers informed by POS n-gram features, can capture some syntactic non-compositionality by identifying violations of standard grammatical rules.

Discontiguity in MWEs presents another challenge, as seen in expressions like ""give up"" in ""John gave it up"" and ""take into account"" in ""Mary took your argument into account."" These discontiguous MWEs are difficult to identify using purely sequential approaches. Parsing, however, can aid in identifying such expressions because syntactic links can connect non-adjacent words. For instance, in the syntactic analysis of the discontiguous expression ""take into account,"" the verb ""take"" is syntactically linked to the noun ""account,"" facilitating the identification of the entire expression.",4,2,4,2,"Question:

How does MWE-aware parsing address the challenges of non-compositionality and discontiguity in multiword expressions?

Answer:

MWEs can be defined as exceptions to regular composition rules, and this non-compositionality can take place at different levels: morphological, distributional, syntactic, semantic, and pragmatic. In particular, some expressions display syntactic irregularity in their internal structure—that is, they are irregular with respect to a grammar. Therefore, if the parser is not aware of the existence of such cases, the analysis will fail. Conversely, depending on the type of parser used, some types of syntactic non-compositionality can be captured by a parser as they violate standard grammatical rules. For example, informing a discriminative parser with POS n-gram features may help capture such non-compositionality. Discontiguity in MWEs can appear in configurations like ""give up"" in ""John gave it up,"" and ""take into account"" in ""Mary took your argument into account."" The identification of such discontiguous MWEs can hardly be handled by purely sequential approaches. Because syntactic links can relate non-adjacent words, parsing may help the identification of discontiguous MWEs. In the syntactic analysis of a discontiguous instance of the expression ""take into account,"" the verb ""take"" is a syntactic neighbor of the noun ""account,"" which should facilitate the identification of the whole expression.","Question:

How does MWE-aware parsing effectively tackle the challenges posed by non-compositionality and discontiguity in multiword expressions?

Answer:

Multiword expressions (MWEs) often defy regular compositional rules, exhibiting non-compositionality at various levels, including morphological, distributional, syntactic, semantic, and pragmatic. This non-compositionality can manifest as syntactic irregularities within their internal structure, making them exceptions to standard grammatical rules. If a parser is not designed to recognize these irregularities, it may fail to analyze them correctly. However, certain types of parsers, such as discriminative parsers informed by POS n-gram features, can capture some syntactic non-compositionality by identifying violations of standard grammatical rules.

Discontiguity in MWEs presents another challenge, as seen in expressions like ""give up"" in ""John gave it up"" and ""take into account"" in ""Mary took your argument into account."" These discontiguous MWEs are difficult to identify using purely sequential approaches. Parsing, however, can aid in identifying such expressions because syntactic links can connect non-adjacent words. For instance, in the syntactic analysis of the discontiguous expression ""take into account,"" the verb ""take"" is syntactically linked to the noun ""account,"" facilitating the identification of the entire expression.",NO,True,1248,True,True
1509090-s31,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Discovery after,"Parsing. Discovery can be fruitfully informed by syntactic structure and can help tackle challenges like discontiguity and variability (Section 2.1), as MWE components usually belong to the same syntactic constituents. Therefore, having the syntactic structure may help capture MWEs with non-adjacent elements. For instance, Seretan (2011) empirically shows that such information enables the extraction of more relevant MWE candidates as compared to standard extraction methods based on fixedsize windows. Moreover, discovery of flexible constructions usually requires linguistic patterns based on parsing: For instance, verbal expressions combining a head verb and a noun or a prepositional phrase (Fazly and Stevenson 2006;Cook, Fazly, and Stevenson 2007;McCarthy, Venkatapathy, and Joshi 2007;Krenn 2008;Seretan 2011), verb-particle constructions (Bannard 2002;McCarthy, Keller, and Carroll 2003), or constructions formed from a head noun and a prepositional phrase (Weller and Heid 2010). For an interesting illustration, Baldwin (2005) proposes different morphosyntactic and syntactic patterns to extract English verb-particle constructions with valence information from raw text. In particular, he shows the effect of using the outputs of a POS tagger, a chunker, a chunk grammar, or a parser, either individually or combined via a classifier. It experimentally appears that the ensemble method significantly outperforms the individual performances. As for individual scores, the use of shallow syntactic information like chunks tends to be prevalent.

It is also possible to use pattern-free approaches like Martens and Vandeghinste (2010) and Sangati and van Cranenburgh (2015), who propose discovery methods not dedicated to a specific MWE category but based on recurring tree fragments and association measures.",What methods are used to discover flexible multiword expressions (MWEs) using linguistic patterns based on parsing?,"Discovery of flexible multiword expressions (MWEs) usually requires linguistic patterns based on parsing. For instance, verbal expressions combining a head verb and a noun or a prepositional phrase are used (Fazly and Stevenson 2006; Cook, Fazly, and Stevenson 2007; McCarthy, Venkatapathy, and Joshi 2007; Krenn 2008; Seretan 2011). Verb-particle constructions are another example (Bannard 2002; McCarthy, Keller, and Carroll 2003). Constructions formed from a head noun and a prepositional phrase are also utilized (Weller and Heid 2010). Baldwin (2005) proposes different morphosyntactic and syntactic patterns to extract English verb-particle constructions with valence information from raw text. He shows the effect of using the outputs of a POS tagger, a chunker, a chunk grammar, or a parser, either individually or combined via a classifier. It experimentally appears that the ensemble method significantly outperforms the individual performances. As for individual scores, the use of shallow syntactic information like chunks tends to be prevalent.",What are the methods used to identify flexible multiword expressions (MWEs) through linguistic patterns based on parsing?,"The identification of flexible multiword expressions (MWEs) often relies on linguistic patterns derived from parsing. For example, verbal expressions that combine a head verb with a noun or a prepositional phrase are commonly used (Fazly and Stevenson 2006; Cook, Fazly, and Stevenson 2007; McCarthy, Venkatapathy, and Joshi 2007; Krenn 2008; Seretan 2011). Another example is verb-particle constructions (Bannard 2002; McCarthy, Keller, and Carroll 2003). Additionally, constructions formed from a head noun and a prepositional phrase are utilized (Weller and Heid 2010). Baldwin (2005) suggests various morphosyntactic and syntactic patterns to extract English verb-particle constructions with valence information from raw text. He demonstrates the impact of using outputs from a POS tagger, a chunker, a chunk grammar, or a parser, either individually or combined through a classifier. Experimentally, it appears that the ensemble method significantly outperforms the individual performances. Among individual methods, the use of shallow syntactic information, such as chunks, tends to be prevalent.",7,4,7,2,"Question:

What methods are used to discover flexible multiword expressions (MWEs) using linguistic patterns based on parsing?

Answer:

Discovery of flexible multiword expressions (MWEs) usually requires linguistic patterns based on parsing. For instance, verbal expressions combining a head verb and a noun or a prepositional phrase are used (Fazly and Stevenson 2006; Cook, Fazly, and Stevenson 2007; McCarthy, Venkatapathy, and Joshi 2007; Krenn 2008; Seretan 2011). Verb-particle constructions are another example (Bannard 2002; McCarthy, Keller, and Carroll 2003). Constructions formed from a head noun and a prepositional phrase are also utilized (Weller and Heid 2010). Baldwin (2005) proposes different morphosyntactic and syntactic patterns to extract English verb-particle constructions with valence information from raw text. He shows the effect of using the outputs of a POS tagger, a chunker, a chunk grammar, or a parser, either individually or combined via a classifier. It experimentally appears that the ensemble method significantly outperforms the individual performances. As for individual scores, the use of shallow syntactic information like chunks tends to be prevalent.","Question:

What are the methods used to identify flexible multiword expressions (MWEs) through linguistic patterns based on parsing?

Answer:

The identification of flexible multiword expressions (MWEs) often relies on linguistic patterns derived from parsing. For example, verbal expressions that combine a head verb with a noun or a prepositional phrase are commonly used (Fazly and Stevenson 2006; Cook, Fazly, and Stevenson 2007; McCarthy, Venkatapathy, and Joshi 2007; Krenn 2008; Seretan 2011). Another example is verb-particle constructions (Bannard 2002; McCarthy, Keller, and Carroll 2003). Additionally, constructions formed from a head noun and a prepositional phrase are utilized (Weller and Heid 2010). Baldwin (2005) suggests various morphosyntactic and syntactic patterns to extract English verb-particle constructions with valence information from raw text. He demonstrates the impact of using outputs from a POS tagger, a chunker, a chunk grammar, or a parser, either individually or combined through a classifier. Experimentally, it appears that the ensemble method significantly outperforms the individual performances. Among individual methods, the use of shallow syntactic information, such as chunks, tends to be prevalent.",NO,True,1102,True,True
1509090-s32,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Identification Before,"Parsing. When MWE identification is performed before parsing, the search space of the parsing algorithm is reduced. Hence, the main advantage of this type of orchestration is that the parsing process becomes less complex. The parser takes as input a sequence of partially analyzed linguistic units. This can be seen as a retokenization process, where the pre-identified MWE is merged into a single token (e.g., by the way → by_the_way). MWE identification prior to parsing has been implemented both in statistical (Cafferkey, Hogan, and van Genabith 2007;Korkontzelos and Manandhar 2010;Constant, Sigogne, and Watrin 2012;de Lhoneux 2015) and rule-based parsers (Brun 1998;Mamede et al. 2012).

This orchestration type has the advantage of simplicity and empirical efficiency. For instance, Cafferkey, Hogan, and van Genabith (2007) show that pre-identifying multiword named entities and prepropositional MWEs improves parsing accuracy in the constituent framework. The best system of the track on MWE-aware dependency parsing in the SPMRL 2013 shared task (Seddah et al. 2013) was the only one that included deterministic pre-identification (Constant, Candito, and Seddah 2013).

Limitations. The pre-identification approach suffers from limitations. First, in this scenario, most of the proposed methods are limited to contiguous MWEs. Handling discontiguous ones may involve word reordering: John gave it up → John gave_up it. In addition, when MWE components are concatenated into a single token, their internal syntactic structure is lost, whereas it may be required for the semantic processing of semi-compositional MWEs. However, this can be performed a posteriori, for instance, by applying simple rules based on POS patterns (Candito and Constant 2014).

Then, the retokenization increases data sparsity that negatively affects parsing performance, because the vocabulary size increases whereas the total amount of training data is the same. Eryigit,İlbay, and Can (2011) showed that the concatenation operation of different MWE categories has different impacts on parsing performance for Turkish. Whereas retokenization of multiword named entities and numerical expressions improved dependency parsing performance, retokenization of light-verb constructions harmed it.

Another disadvantage is that pre-identification is deterministic, so the syntactic parser cannot recover from MWE identification errors. A sentence like He walked by and large tractors passed him cannot be analyzed correctly if by and large is pre-analyzed as a multiword adverb (by_and_large). Errors arise mainly due to challenging aspects like ambiguity (accidental co-occurrence identified as an MWE) and variability (an MWE missed because it is different from the canonical form in the lexicon, see Section 3.1).

Non-Deterministic Approaches. To fully profit from using MWE identification as a preprocessing step, parsing has to be carried out non-deterministically, since several alternatives should be maintained and eventually resolved using a disambiguation model of some kind. Rule-based parsers deal with this problem to some extent by taking as input a lattice of possible POS sequences and MWE segmentations constructed from a lexicon-based preprocessing phase (Villemonte De La Clergerie 2013; Sagot and Boullier 2005). In the statistical paradigm, Constant, Le Roux, and Sigogne (2013) successfully used an MWE tagger based on conditional random fields that generates the most probable outputs in the form of a lattice of lexical units. The lattice is then fed into the parser, which is in charge of selecting the best lexical segmentation, as well as the best syntactic tree. With this rationale, the MWE identification module of Urieli (2013) successfully feeds the initial beam of a transition-based parser with its n-best segmentations, each associated with a score. These proposals suggest that a crucial aspect of MWE identification in MWE-aware parsing systems is whether ambiguous analyses can be handled.",What are the limitations and challenges of pre-identifying MWEs before parsing in NLP systems?,"The pre-identification approach suffers from limitations. First, in this scenario, most of the proposed methods are limited to contiguous MWEs. Handling discontiguous ones may involve word reordering: John gave it up → John gave_up it. In addition, when MWE components are concatenated into a single token, their internal syntactic structure is lost, whereas it may be required for the semantic processing of semi-compositional MWEs. However, this can be performed a posteriori, for instance, by applying simple rules based on POS patterns (Candito and Constant 2014). Then, the retokenization increases data sparsity that negatively affects parsing performance, because the vocabulary size increases whereas the total amount of training data is the same. Eryigit, İlbay, and Can (2011) showed that the concatenation operation of different MWE categories has different impacts on parsing performance for Turkish. Whereas retokenization of multiword named entities and numerical expressions improved dependency parsing performance, retokenization of light-verb constructions harmed it. Another disadvantage is that pre-identification is deterministic, so the syntactic parser cannot recover from MWE identification errors. A sentence like He walked by and large tractors passed him cannot be analyzed correctly if by and large is pre-analyzed as a multiword adverb (by_and_large). Errors arise mainly due to challenging aspects like ambiguity (accidental co-occurrence identified as an MWE) and variability (an MWE missed because it is different from the canonical form in the lexicon).",What are the challenges and limitations associated with pre-identifying multiword expressions (MWEs) before parsing in natural language processing (NLP) systems?,"Pre-identifying MWEs before parsing in NLP systems presents several challenges and limitations. One major limitation is that most methods are restricted to handling contiguous MWEs, which means they struggle with discontiguous ones. For example, handling a phrase like ""John gave it up"" might require reordering to ""John gave_up it."" Additionally, when MWE components are concatenated into a single token, their internal syntactic structure is lost. This structure might be necessary for the semantic processing of semi-compositional MWEs, although it can be reconstructed later using simple rules based on POS patterns (Candito and Constant 2014).

Another issue is that retokenization increases data sparsity, negatively impacting parsing performance. This happens because the vocabulary size increases while the total amount of training data remains the same. Research by Eryigit, İlbay, and Can (2011) demonstrated that the impact of concatenating different MWE categories on parsing performance varies. For instance, retokenizing multiword named entities and numerical expressions improved dependency parsing performance in Turkish, whereas retokenizing light-verb constructions had a detrimental effect.

Furthermore, pre-identification is a deterministic process, meaning that the syntactic parser cannot recover from MWE identification errors. For example, a sentence like ""He walked by and large tractors passed him"" cannot be correctly analyzed if ""by and large"" is mistakenly pre-analyzed as a multiword adverb (by_and_large). Such errors often arise due to challenges like ambiguity, where accidental co-occurrences are misidentified as MWEs, and variability, where an MWE is missed because it differs from its canonical form in the lexicon.",7,4,7,4,"Question:

What are the limitations and challenges of pre-identifying MWEs before parsing in NLP systems?

Answer:

The pre-identification approach suffers from limitations. First, in this scenario, most of the proposed methods are limited to contiguous MWEs. Handling discontiguous ones may involve word reordering: John gave it up → John gave_up it. In addition, when MWE components are concatenated into a single token, their internal syntactic structure is lost, whereas it may be required for the semantic processing of semi-compositional MWEs. However, this can be performed a posteriori, for instance, by applying simple rules based on POS patterns (Candito and Constant 2014). Then, the retokenization increases data sparsity that negatively affects parsing performance, because the vocabulary size increases whereas the total amount of training data is the same. Eryigit, İlbay, and Can (2011) showed that the concatenation operation of different MWE categories has different impacts on parsing performance for Turkish. Whereas retokenization of multiword named entities and numerical expressions improved dependency parsing performance, retokenization of light-verb constructions harmed it. Another disadvantage is that pre-identification is deterministic, so the syntactic parser cannot recover from MWE identification errors. A sentence like He walked by and large tractors passed him cannot be analyzed correctly if by and large is pre-analyzed as a multiword adverb (by_and_large). Errors arise mainly due to challenging aspects like ambiguity (accidental co-occurrence identified as an MWE) and variability (an MWE missed because it is different from the canonical form in the lexicon).","Question:

What are the challenges and limitations associated with pre-identifying multiword expressions (MWEs) before parsing in natural language processing (NLP) systems?

Answer:

Pre-identifying MWEs before parsing in NLP systems presents several challenges and limitations. One major limitation is that most methods are restricted to handling contiguous MWEs, which means they struggle with discontiguous ones. For example, handling a phrase like ""John gave it up"" might require reordering to ""John gave_up it."" Additionally, when MWE components are concatenated into a single token, their internal syntactic structure is lost. This structure might be necessary for the semantic processing of semi-compositional MWEs, although it can be reconstructed later using simple rules based on POS patterns (Candito and Constant 2014).

Another issue is that retokenization increases data sparsity, negatively impacting parsing performance. This happens because the vocabulary size increases while the total amount of training data remains the same. Research by Eryigit, İlbay, and Can (2011) demonstrated that the impact of concatenating different MWE categories on parsing performance varies. For instance, retokenizing multiword named entities and numerical expressions improved dependency parsing performance in Turkish, whereas retokenizing light-verb constructions had a detrimental effect.

Furthermore, pre-identification is a deterministic process, meaning that the syntactic parser cannot recover from MWE identification errors. For example, a sentence like ""He walked by and large tractors passed him"" cannot be correctly analyzed if ""by and large"" is mistakenly pre-analyzed as a multiword adverb (by_and_large). Such errors often arise due to challenges like ambiguity, where accidental co-occurrences are misidentified as MWEs, and variability, where an MWE is missed because it differs from its canonical form in the lexicon.",NO,True,1753,True,True
1509090-s34,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Identification after Parsing.,"Although it is well known that predicting syntactic structure might help tackle challenges like discontiguity and variability, especially for verbal expressions, very few studies have experimented with identification after parsing. Fazly, Cook, and Stevenson (2009) used a parsed text in order to identify verb-noun idiomatic combinations. Nagy and Vincze (2014) also successfully use such an approach to identify verb-particle constructions in English. They positioned a classifier on top of a standard parser in order to select verb-particle constructions from a set of candidates extracted from the parsed text. Baptista et al. (2015) identify verbal idioms in Portuguese using the incremental finite-state parser XIP (Ait-Mokhtar, Chanod, and Roux 2002) as part of the STRING NLP pipeline (Mamede et al. 2012). The finite state parser first recognizes chunks then identifies syntactic relations between them by incrementally applying hand-crafted rules. Then, new rules are added in order to capture verbal idioms based on already predicted lexical and syntactic information. Also, most of the systems proposed in the PARSEME Shared Task (Savary et al. 2017) used tagging supervised models relying on syntactic features in order to identify verbal MWEs.

Instead, most identification methods are based on less complex preprocessing stages (tokenization, POS tagging, lemmatization, etc.), as shown in Section 3. One reason could be that discontiguous MWEs tend to have small gaps, as shown in English by Schneider et al. (2014a). Another reason could be that parsers are error-prone and error-propagation might harm MWE identification (unlike the case for discovery where errors are compensated by large quantities of data). To tackle the problem of errors, an interesting approach is to make use of reranking (Charniak and Johnson 2005). For instance, Constant, Sigogne, and Watrin (2012) used an MWE-dedicated reranker on top of a parser generating the n-best parses (including MWE identification) and showed significant improvement in MWE identification accuracy.",How can reranking be utilized to improve the accuracy of multiword expression identification in parsed text?,"To tackle the problem of errors in multiword expression (MWE) identification, an interesting approach is to make use of reranking (Charniak and Johnson 2005). For instance, Constant, Sigogne, and Watrin (2012) used an MWE-dedicated reranker on top of a parser generating the n-best parses (including MWE identification) and showed significant improvement in MWE identification accuracy.",How can reranking be used to enhance the accuracy of identifying multiword expressions in parsed text?,"Reranking can be an effective strategy to improve the accuracy of multiword expression (MWE) identification in parsed text. This approach involves applying a reranker specifically designed for MWEs on top of a parser that generates the n-best parses, which include MWE identification. For example, Constant, Sigogne, and Watrin (2012) demonstrated that using an MWE-dedicated reranker significantly improved the accuracy of MWE identification. This method builds on the work of Charniak and Johnson (2005), who explored reranking techniques to address parsing errors.",7,2,8,2,"Question:

How can reranking be utilized to improve the accuracy of multiword expression identification in parsed text?

Answer:

To tackle the problem of errors in multiword expression (MWE) identification, an interesting approach is to make use of reranking (Charniak and Johnson 2005). For instance, Constant, Sigogne, and Watrin (2012) used an MWE-dedicated reranker on top of a parser generating the n-best parses (including MWE identification) and showed significant improvement in MWE identification accuracy.","Question:

How can reranking be used to enhance the accuracy of identifying multiword expressions in parsed text?

Answer:

Reranking can be an effective strategy to improve the accuracy of multiword expression (MWE) identification in parsed text. This approach involves applying a reranker specifically designed for MWEs on top of a parser that generates the n-best parses, which include MWE identification. For example, Constant, Sigogne, and Watrin (2012) demonstrated that using an MWE-dedicated reranker significantly improved the accuracy of MWE identification. This method builds on the work of Charniak and Johnson (2005), who explored reranking techniques to address parsing errors.",NO,True,567,True,True
1509090-s35,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Evaluation of MWE-Aware Parsing,"Evaluating a syntactic parser generally consists of comparing the output to reference (gold standard) parses from a manually labeled treebank. In the case of constituency parsing, a constituent is treated as correct if there exists a constituent in the gold standard parse with the same labels and starting and ending points. These parsers are traditionally evaluated through precision, recall, and F-measure metrics (Black et al. 1991;Sekine and Collins 1997).

In standard dependency parsing with the single-head constraint, 23 the number of dependencies produced by the parser should be equal to the number of total dependencies in the gold-standard parse tree. Common metrics to evaluate these parsers include the percentage of tokens with correct head, called unlabeled attachment score (UAS), and the percentage of tokens with correct head and dependency label, called labeled attachment score (LAS) (Buchholz and Marsi 2006;Nilsson, Riedel, and Yuret 2007).

The evaluation of identification and discovery has been discussed in previous sections. However, evaluation of MWE-aware parsers and of whether or not MWE identification helps to improve the parsing quality requires some additional care. In most work where MWE identification is realized before parsing, the MWEs are merged into single tokens (Section 4.2.2). As a result, the common metrics for parsing evaluation given above become problematic for measuring the impact of MWE identification on parsing performance (Eryigit,İlbay, and Can 2011). For example, in dependency parsing, the concatenation of MWEs into single units decrements the total number of evaluated dependencies. It is thus possible to obtain different scores without actually changing the quality of the parser, but simply the representation of the results. Instead of UAS and LAS metrics, the attachment scores on the surrounding structures, namely, UAS surr and LAS surr (i.e., the accuracies on the dependency relations excluding the ones between MWE elements) are more appropriate for the extrinsic evaluation of the impact of MWE identification on parsing. Similar considerations apply to constituency parsing.

Although UAS surr and LAS surr are valuable metrics for measuring the impact of different MWE categories on parsing, they are troublesome with automatic MWE identification when gold-standard MWE segmentation is not available, because erroneously identified MWEs would degrade parsing scores on the surrounding dependencies.

An alternative solution is to detach the concatenated MWE components (if any) into a dependency or constituency subtree (Candito and Constant 2014;Eryigit,İlbay, and Can 2011). In this way, the standard evaluation metrics are still applicable in all different orchestration scenarios and work on both contiguous and non-contiguous cases, thus providing a means to assess the performance of joint syntactic parsing and MWE identification as a whole.",What are the challenges and solutions for evaluating MWE-aware parsers in syntactic parsing?,"Evaluating a syntactic parser generally consists of comparing the output to reference (gold standard) parses from a manually labeled treebank. In most work where MWE identification is realized before parsing, the MWEs are merged into single tokens. As a result, the common metrics for parsing evaluation become problematic for measuring the impact of MWE identification on parsing performance. For example, in dependency parsing, the concatenation of MWEs into single units decrements the total number of evaluated dependencies. It is thus possible to obtain different scores without actually changing the quality of the parser, but simply the representation of the results. Instead of UAS and LAS metrics, the attachment scores on the surrounding structures, namely, UAS surr and LAS surr, are more appropriate for the extrinsic evaluation of the impact of MWE identification on parsing. Although UAS surr and LAS surr are valuable metrics for measuring the impact of different MWE categories on parsing, they are troublesome with automatic MWE identification when gold-standard MWE segmentation is not available, because erroneously identified MWEs would degrade parsing scores on the surrounding dependencies. An alternative solution is to detach the concatenated MWE components into a dependency or constituency subtree. In this way, the standard evaluation metrics are still applicable in all different orchestration scenarios and work on both contiguous and non-contiguous cases, thus providing a means to assess the performance of joint syntactic parsing and MWE identification as a whole.",What are the challenges and solutions in evaluating parsers that are aware of multiword expressions (MWEs) in syntactic parsing?,"Evaluating a syntactic parser typically involves comparing its output to reference (gold standard) parses from a manually labeled treebank. In many cases where MWE identification is performed before parsing, MWEs are merged into single tokens. This merging poses challenges for traditional parsing evaluation metrics, as it affects the measurement of parsing performance. For instance, in dependency parsing, combining MWEs into single units reduces the total number of evaluated dependencies, potentially leading to different scores without any actual change in parser quality, merely altering the representation of results. Instead of using the standard Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS) metrics, it is more appropriate to use attachment scores on the surrounding structures, namely, UAS surr and LAS surr, for evaluating the impact of MWE identification on parsing. However, these metrics can be problematic when automatic MWE identification is used without gold-standard MWE segmentation, as incorrectly identified MWEs can degrade parsing scores on surrounding dependencies. An alternative solution is to separate the concatenated MWE components into a dependency or constituency subtree. This approach allows the standard evaluation metrics to remain applicable across different scenarios, accommodating both contiguous and non-contiguous cases, and thus providing a comprehensive assessment of the performance of joint syntactic parsing and MWE identification.",7,4,7,4,"Question:

What are the challenges and solutions for evaluating MWE-aware parsers in syntactic parsing?

Answer:

Evaluating a syntactic parser generally consists of comparing the output to reference (gold standard) parses from a manually labeled treebank. In most work where MWE identification is realized before parsing, the MWEs are merged into single tokens. As a result, the common metrics for parsing evaluation become problematic for measuring the impact of MWE identification on parsing performance. For example, in dependency parsing, the concatenation of MWEs into single units decrements the total number of evaluated dependencies. It is thus possible to obtain different scores without actually changing the quality of the parser, but simply the representation of the results. Instead of UAS and LAS metrics, the attachment scores on the surrounding structures, namely, UAS surr and LAS surr, are more appropriate for the extrinsic evaluation of the impact of MWE identification on parsing. Although UAS surr and LAS surr are valuable metrics for measuring the impact of different MWE categories on parsing, they are troublesome with automatic MWE identification when gold-standard MWE segmentation is not available, because erroneously identified MWEs would degrade parsing scores on the surrounding dependencies. An alternative solution is to detach the concatenated MWE components into a dependency or constituency subtree. In this way, the standard evaluation metrics are still applicable in all different orchestration scenarios and work on both contiguous and non-contiguous cases, thus providing a means to assess the performance of joint syntactic parsing and MWE identification as a whole.","Question:

What are the challenges and solutions in evaluating parsers that are aware of multiword expressions (MWEs) in syntactic parsing?

Answer:

Evaluating a syntactic parser typically involves comparing its output to reference (gold standard) parses from a manually labeled treebank. In many cases where MWE identification is performed before parsing, MWEs are merged into single tokens. This merging poses challenges for traditional parsing evaluation metrics, as it affects the measurement of parsing performance. For instance, in dependency parsing, combining MWEs into single units reduces the total number of evaluated dependencies, potentially leading to different scores without any actual change in parser quality, merely altering the representation of results. Instead of using the standard Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS) metrics, it is more appropriate to use attachment scores on the surrounding structures, namely, UAS surr and LAS surr, for evaluating the impact of MWE identification on parsing. However, these metrics can be problematic when automatic MWE identification is used without gold-standard MWE segmentation, as incorrectly identified MWEs can degrade parsing scores on surrounding dependencies. An alternative solution is to separate the concatenated MWE components into a dependency or constituency subtree. This approach allows the standard evaluation metrics to remain applicable across different scenarios, accommodating both contiguous and non-contiguous cases, and thus providing a comprehensive assessment of the performance of joint syntactic parsing and MWE identification.",NO,True,1501,True,True
1509090-s37,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,MWE Processing and Machine Translation,"MT systems aim to automatically translate a source text into a target text that retains the meaning and fluency of the source. They must take into account the lexical, morphosyntactic, syntactic, and semantic constraints in source and target languages. The main MT paradigms are summarized here.

Statistical machine translation (SMT) acquires the ability to translate from parallel data using machine learning techniques. Like all such systems, it includes a training phase, which uses the data to build probabilistic models, and a decoding phase, where these models are deployed to actually carry out translation of an unseen source language sentence.

During training, two kinds of probabilistic model are built: a translation model, derived from bilingual corpora, and a language model, from monolingual corpora. Both models assign probabilities: the translation model to source/target language fragments, and the language model to target language word sequences (Koehn 2010).

During decoding, the system generates many hypothetical translations for each source sentence and chooses the most probable hypothesis. This is calculated for each by combining probabilities assigned by the acquired translation and target language models. The effective performance of this calculation requires considerable ingenuity, given the exponential number of possible translations and orderings of translated sentence fragments, the non-trivial computations involved, and the real time and memory constraints.

Variants of these steps take into account contiguous sequences of words in so-called phrase-based SMT (Koehn, Och, and Marcu 2003), syntactic structures in syntax-based SMT (Chiang 2007;Hoang and Koehn 2010), or linguistic annotation layers in factorbased SMT (Koehn and Hoang 2007)).

Phrase-based SMT and its variants build phrase tables-that is, a list of source fragments (words, phrases, subtrees), their translations, and their translation probabilities that take into account word sequences, not only simple words. In principle, therefore, such systems can naturally handle contiguous MWEs. Whether they can handle them correctly in all cases is, of course, a separate question.

More recently, neural machine translation (Kalchbrenner and Blunsom 2013;Cho et al. 2014) proposes alternative methods to compute translation probabilities, by using recurrent neural networks to model the translation task. Most neural translation systems use an encoder-decoder architecture. The input sentence is encoded into a fixedlength or variable-length vector and then one or more decoders use this representation to obtain the target sentence. The probability of the translation of one word is computed on the basis of the translation probabilities of previous words. An attention model is frequently used to represent larger contexts for the translated words and sentences. Indeed, attention models represent source word and larger-context words (using a dot product of vectors or multilayer perceptrons) to generate a target word. Few neural machine translation systems take into account fine linguistic descriptions (Sennrich and Haddow 2016). Neural machine translation obtains impressive improvements of the evaluation scores such as BLEU (Wu et al. 2016).

Rule-based machine translation (RBMT) uses large lexicons and explicit rules describing the syntactic and semantic constraints on both the source and the target language. Transfer rules are used to map source language structures to target language ones and to identify the right translation. These rules are based on formal grammars or intermediate language-independent structures (such as minimal recursion semantics [Oepen et al. 2004]) capable of generating correct translation equivalents.

Finally, example-based machine translation (EBMT) is based mainly on examples in the form of large translation memories (large collections of source/target sentence pairs) but also uses rules to acquire new linguistic knowledge dynamically. EBMT is based on a translation by analogy approach, where at run time translations are obtained by looking up and using examples stored in translation memories. The translation process is organized in three stages: (i) matching of input sentences with translations previously stored, (ii) retrieval of these translations, and finally (iii) adaptation or recombination of the target sentences. An early review of EBMT appears in Somers (1999).

In this introductory section we defined the various approaches used in machine translation and their acronyms. In the following sections we will use the acronyms SMT, RBMT, and EBMT instead of the complete terms to improve text readability.",How do statistical machine translation systems generate and select the most probable translations during decoding?,"Statistical machine translation (SMT) systems acquire the ability to translate from parallel data using machine learning techniques. During the decoding phase, these systems generate many hypothetical translations for each source sentence. The system chooses the most probable hypothesis by combining probabilities assigned by the acquired translation and target language models. This process involves calculating the probabilities for each hypothetical translation using a translation model derived from bilingual corpora and a language model from monolingual corpora. The translation model assigns probabilities to source/target language fragments, while the language model assigns probabilities to target language word sequences (Koehn 2010). The effective performance of this calculation requires considerable ingenuity, given the exponential number of possible translations and orderings of translated sentence fragments, the non-trivial computations involved, and the real-time and memory constraints.",How do statistical machine translation systems determine the most probable translations during the decoding process?,"Statistical machine translation (SMT) systems leverage parallel data and machine learning techniques to develop translation capabilities. During the decoding phase, these systems generate numerous hypothetical translations for each source sentence. To select the most probable translation, the system combines probabilities from both the translation model and the target language model. The translation model, derived from bilingual corpora, assigns probabilities to source and target language fragments. Meanwhile, the language model, built from monolingual corpora, assigns probabilities to sequences of words in the target language (Koehn 2010). This process involves complex calculations due to the exponential number of possible translations and orderings of sentence fragments, as well as the computational challenges posed by real-time and memory constraints. The effective performance of these calculations requires considerable ingenuity.",7,2,9,2,"Question:

How do statistical machine translation systems generate and select the most probable translations during decoding?

Answer:

Statistical machine translation (SMT) systems acquire the ability to translate from parallel data using machine learning techniques. During the decoding phase, these systems generate many hypothetical translations for each source sentence. The system chooses the most probable hypothesis by combining probabilities assigned by the acquired translation and target language models. This process involves calculating the probabilities for each hypothetical translation using a translation model derived from bilingual corpora and a language model from monolingual corpora. The translation model assigns probabilities to source/target language fragments, while the language model assigns probabilities to target language word sequences (Koehn 2010). The effective performance of this calculation requires considerable ingenuity, given the exponential number of possible translations and orderings of translated sentence fragments, the non-trivial computations involved, and the real-time and memory constraints.","Question:

How do statistical machine translation systems determine the most probable translations during the decoding process?

Answer:

Statistical machine translation (SMT) systems leverage parallel data and machine learning techniques to develop translation capabilities. During the decoding phase, these systems generate numerous hypothetical translations for each source sentence. To select the most probable translation, the system combines probabilities from both the translation model and the target language model. The translation model, derived from bilingual corpora, assigns probabilities to source and target language fragments. Meanwhile, the language model, built from monolingual corpora, assigns probabilities to sequences of words in the target language (Koehn 2010). This process involves complex calculations due to the exponential number of possible translations and orderings of sentence fragments, as well as the computational challenges posed by real-time and memory constraints. The effective performance of these calculations requires considerable ingenuity.",NO,True,947,True,True
1509090-s38,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Motivations and Challenges for MWE-Aware Machine Translation,"The main motivation for MWE-aware MT is that none of these paradigms can consistently address the basic features of MWEs, some of which were already mentioned in Section 1.2: ambiguity, discontiguity, non-compositionality, and variability. We briefly review these challenges here.

Ambiguity. Here the components of an MWE can be interpreted and translated literally or idiomatically according to the context, and the two readings are easily confused. For example, the French MWE jeter l'éponge (lit. to throw the sponge) means to resign. However, the expression might be used literally (the person is washing the dishes and literally throws the sponge). The challenge concerns the nature of the information required to make the right choice, and how to represent it. In parsing we should note that this is likely to include more contextual, extra-linguistic, or multilingual information than is available in most MT systems.

Discontiguity. Translation is hampered by alien elements occurring between the components of an MWE. Google Translate renders John picked up the book as John ramassa le livre in French, which is correct, if a little literary. But John picked the book up is translated as John prit le livre jusqu'à, which is ungrammatical. The challenge, again, concerns the information required for such discontiguous MWEs to be recognized, and how that information is brought to bear. A pertinent issue is whether there are special cases to be exploited, as is the case for phrases having a fixed or semi-fixed frame with slots for one or more fillers, such as give [. . . ] a [. . . ] break or on the one hand, [. . . ] on the other hand.

Non-compositionality. This implies that a compositional translation strategy will generally fail. Besides resolving the ambiguity of whether a given case is compositional there is another challenge that arises because compositionality is not necessarily all or nothing: It has degrees. At one extreme, we have non-compositional expressions like red tape, meaning excessive bureaucracy. At the other we have compound nouns like honeymoon for which a compositional translation may be correct (e.g., luna di miele in Italian). In between the two, we have semi-compositional expressions such as to take a decision. The challenge is whether the degree can be predicted and exploited, because if an MWE is mostly compositional, then a mostly compositional strategy stands a chance of producing an acceptable translation. There are special cases where semicompositional translation strategies work. So ask a question should be translated by a pune o întrebare (lit. put a question) in Romanian and not by *a cere o întrebare. The verb to ask could not be translated in Romanian as a cere/to demand but as a pune/to put due to the specific lexical constraints: a cere/to demand could not select the noun întrebare/question, so the other synonym is selected. The challenge is to select the right translation by taking into account all the constraints.

Variability. MT systems must identify and translate all variants of an MWE. Some variants can be challenging to recognize, particularly when they involve syntactic/semantic constraints. For example, the expression he has cooked his goose means that he has fallen into a trap of his own making. But for this reading, he and his must corefer. If they do not, the reading is compositional. Not only is this condition tricky to verify, but there are variants-for example, involving number and gender: she has cooked her goose-whose identification might require hand-made or automatically learned rules. Once these are in place, variants of MWEs can be identified using methods presented in Section 3.2.

Translation asymmetries. These occur when an MWE in the source language is not necessarily translated by an MWE in the target language and vice versa. They represent an additional challenge. In general, we can have different correspondences, exemplified by the following English-Italian examples: many-to-many (to kick the bucket → tirare le cuoia), many-to-one (to kick the bucket → morire), and one-to-many (svegliare → to wake up). There are several challenges. One is to decide whether to choose an MWE target when a one-word alternative exists. Another is to determine what syntactic adjustments must be made to the target to retain fluency with asymmetric output. Another challenge is how best to exploit asymmetry for the purpose of discovery, as discussed further in Section 5.2.1.

Some of these challenges might be indirectly handled by specific types of MT systems with various orchestration strategies. Idiomatic expressions or contiguous MWEs might be correctly translated in phrase-based SMT (Cap et al. 2015) or by neural MT (Wu et al. 2016), which take into account larger contexts. However, ambiguous MWEs are often translated in SMT with their literal meaning because of a larger translation probability. Syntax-based SMT might capture frequent terms or light-verb constructions without distinguishing MWEs from simple noun-noun or verb-noun combinations. Neural MT systems handle some specific compounds by specific segmentation strategies, but no discontiguous MWEs.

EBMT systems handle contiguous MWEs through their specific translation strategies. RBMT designs specific hand-made rules to translate MWEs, but ambiguity is still a problem.

MT systems with various degrees of MWE-awareness have striven to address these challenges to improve the quality of translation (Ren et al. 2009;Kordoni and Simova 2012;Ramisch, Besacier, and Kobzar 2013;Barreiro et al. 2014). The results of these strategies, presented in the next section, vary across language pairs or MWE categories. For example, Ren et al. (2009) report small improvements of the BLEU score for domainspecific terminology, Cap et al. (2015) report significant improvements of the BLEU score for German light-verb constructions, and Pal, Naskar, and Bandyopadhyay (2013) found significant BLEU improvements for English and Bengali multiword named entities. This variation can be explained by the inadequacy of measures used to evaluate MT (e.g., BLEU) for checking the quality of MWE translation. Section 5.2 presents these attempts in terms of some specific orchestration strategies linking MWE processing with MT.",What are the main challenges in translating multiword expressions (MWEs) in machine translation systems?,"The main motivation for MWE-aware MT is that none of these paradigms can consistently address the basic features of MWEs, some of which include ambiguity, discontiguity, non-compositionality, and variability. Ambiguity arises when the components of an MWE can be interpreted and translated literally or idiomatically according to the context, and the two readings are easily confused. Discontiguity occurs when translation is hampered by alien elements occurring between the components of an MWE. Non-compositionality implies that a compositional translation strategy will generally fail, as compositionality is not necessarily all or nothing and has degrees. Variability requires MT systems to identify and translate all variants of an MWE, which can be challenging to recognize, particularly when they involve syntactic/semantic constraints. Translation asymmetries occur when an MWE in the source language is not necessarily translated by an MWE in the target language and vice versa, representing an additional challenge. Some of these challenges might be indirectly handled by specific types of MT systems with various orchestration strategies, but ambiguity remains a problem.",What are the primary challenges faced by machine translation systems when translating multiword expressions (MWEs)?,"Machine translation systems encounter several significant challenges when dealing with multiword expressions (MWEs). One of the primary issues is ambiguity, where the components of an MWE can be interpreted either literally or idiomatically, depending on the context, leading to potential confusion between the two readings. Another challenge is discontiguity, which occurs when unrelated elements appear between the components of an MWE, complicating the translation process. Non-compositionality is also a critical factor, as MWEs often cannot be translated using a straightforward compositional strategy due to varying degrees of compositionality. Additionally, variability poses a challenge, requiring systems to identify and translate all variants of an MWE, which can be difficult, especially when syntactic or semantic constraints are involved. Furthermore, translation asymmetries present another hurdle, as an MWE in the source language may not correspond to an MWE in the target language, and vice versa. While some machine translation systems may address these challenges through specific orchestration strategies, ambiguity remains a persistent problem.",7,4,8,4,"Question:

What are the main challenges in translating multiword expressions (MWEs) in machine translation systems?

Answer:

The main motivation for MWE-aware MT is that none of these paradigms can consistently address the basic features of MWEs, some of which include ambiguity, discontiguity, non-compositionality, and variability. Ambiguity arises when the components of an MWE can be interpreted and translated literally or idiomatically according to the context, and the two readings are easily confused. Discontiguity occurs when translation is hampered by alien elements occurring between the components of an MWE. Non-compositionality implies that a compositional translation strategy will generally fail, as compositionality is not necessarily all or nothing and has degrees. Variability requires MT systems to identify and translate all variants of an MWE, which can be challenging to recognize, particularly when they involve syntactic/semantic constraints. Translation asymmetries occur when an MWE in the source language is not necessarily translated by an MWE in the target language and vice versa, representing an additional challenge. Some of these challenges might be indirectly handled by specific types of MT systems with various orchestration strategies, but ambiguity remains a problem.","Question:

What are the primary challenges faced by machine translation systems when translating multiword expressions (MWEs)?

Answer:

Machine translation systems encounter several significant challenges when dealing with multiword expressions (MWEs). One of the primary issues is ambiguity, where the components of an MWE can be interpreted either literally or idiomatically, depending on the context, leading to potential confusion between the two readings. Another challenge is discontiguity, which occurs when unrelated elements appear between the components of an MWE, complicating the translation process. Non-compositionality is also a critical factor, as MWEs often cannot be translated using a straightforward compositional strategy due to varying degrees of compositionality. Additionally, variability poses a challenge, requiring systems to identify and translate all variants of an MWE, which can be difficult, especially when syntactic or semantic constraints are involved. Furthermore, translation asymmetries present another hurdle, as an MWE in the source language may not correspond to an MWE in the target language, and vice versa. While some machine translation systems may address these challenges through specific orchestration strategies, ambiguity remains a persistent problem.",NO,True,1165,True,True
1509090-s40,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Discovery after MT.,"MWE discovery methods based on parallel corpora use alignment to verify whether a source MWE translates word-for-word or rather as a unit on the target side. Alignment is used for both multilingual discovery, that is, finding new translation correspondences between MWEs in two languages, and monolingual discovery, that is, finding new MWEs in one language based on translation asymmetries. In fact, multilingual data could help monolingual discovery for less-resourced languages. Multilingual discovery. Word-level alignment (Och and Ney 2000) indicating that groups of two or more source words are frequently aligned with a single target word can indicate potential MWE candidates. For instance, the French compound appareil photo will be aligned with the English word camera. MWE candidates can therefore be extracted by using many-to-one alignments as in Caseli et al. (2010). However, because automatic word alignments tend to be very noisy, several techniques have been proposed to filter them. Caseli et al. (2010) used predefined POS patterns and frequency thresholds to obtain bilingual MWE lists, as presented in Section 2. Bouamor, Semmar, and Zweigenbaum (2012b) use an association measure to find the translations of each MWE in the target language counterpart without exploiting the alignment.

Parsed bilingual data has also been used to filter word-aligned MWE candidates. Thus, Zarrieß and Kuhn (2009) propose a method for detecting verb-object MWEs in both source and target languages that are dependency-parsed, only retaining MWEs whose words are bilingually aligned and monolingually linked by syntactic dependencies. Tsvetkov and Wintner (2014) proposed supervised classifiers to distinguish MWEs from non-MWEs, using linguistically motivated features such as literal translatability derived from simple word alignments in parallel corpora. Here, a check is carried out to assess whether the MWE candidate could be literally translated from a bilingual dictionary. Similarly, Rondon, Caseli, and Ramisch (2015) model translatability as the probability of translating the words of the MWE from Portuguese into English and then back to the same Portuguese word.

Monolingual discovery. For monolingual discovery we consider the possibility of using translation asymmetries identified in parallel corpora to compile lists of potential MWE candidates in one specific language without using precomputed word alignments. For example, Sinha (2009) discover Hindi by compiling a list of Hindi light verbs and then looking at mismatches (indicating the use of a verb in a more idiomatic sense) in meaning in the corresponding English counterpart, given a list of literal translations of Hindi light verbs into English.

This approach has also been extended to comparable corpora (texts from the same domain, genre, or type that are not in a translation relation). Morin and Daille (2010) collect bilingual terminology from comparable corpora with the help of a bilingual dictionary. This method applies a compositional word-for-word translation for an MWE candidate and searches the most probable translation in the comparable corpus, identified by a term extraction method. If the compositional translation strategy fails, derivation methods are used to find the nearest word in the dictionary and to find the potential translation.

One advantage of all these methods is that they use relatively well-behaved technologies that exploit translation asymmetries together with non-literal translatability to propose multilingual pairs of MWE candidates as well as monolingual MWEs. Unfortunately, they tend to require large parallel or comparable corpora to find appropriate candidates, and these resources are generally lacking. 5.2.2 Identification before MT. The training process in a standard phrase-based SMT 24 system consists of two steps: word alignment and phrase-table construction. Here, identification takes the form of a preprocessing step (before the training process) that transforms MWEs into an intermediate representation: single units, where component words are concatenated by an underscore character (Carpuat and Diab 2010), 25 a definition from the dictionary (Salton, Ross, and Kelleher 2014), 26 or a paraphrase (Ullman and Nivre 2014). 27 Such preprocessing methods identify MWEs in the monolingual parts of corpora by using external resources (bilingual dictionaries, term databases, or parallel corpora), by applying MWE identification tools (Section 3), or a combination of both (Ghoneim and Diab 2013).

SMT systems. In SMT, MWE replacement takes place before the word alignment step (the ""static"" approach according to Carpuat and Diab [2010]), using rule-based MWE identification and lexicon look-up with monolingual general dictionaries, bilingual dictionaries, and specific bilingual lists containing multiword named entities or terms and their translations.

When such resources are not available or incomplete, lists of MWE candidates obtained by MWE discovery methods can be applied directly to transform MWEs into single tokens. This strategy handles not only contiguous MWEs, but also their inflected variants. Some specific cases of discontiguous MWEs such as light-verb constructions might be handled by a specific annotation of the light verb (Cap et al. 2015).

EBMT and RBMT systems. For both EBMT and RBMT, lexical resources are essential for handling contiguous MWEs (Barreiro 2008). EBMT uses translation memories to properly identify these, for which word sequences, sometimes based on parsed data (Kim, Brown, and Carbonell 2010), are listed as possible translations. RBMT also uses compositional rules that are combined with transfer rules to handle syntactic variants and discontiguity for some MWEs (Forcada et al. 2011). In the compositional approach, MWE handling is obtained by means of tagging and syntactic analysis of the different components of an MWE.

Specific MWE categories. Specific MWE categories such as multiword named entities or multiword terms pose particular challenges to MT systems, because they may require specific translations not directly deducible from the translations of their components. These categories are very productive, that is, new multiword named entities and terms are constantly being created, so it is difficult to have complete and updated lexical resources for use during the translation process. For term identification, bilingual or multilingual term glossaries might be applied to transform terms into an intermediate representation (words concatenated by underscore). But when these resources are missing, for new domains or for under-resourced languages, multiword named entities and multiword terms can be annotated as a single token with the help of specific techniques for named entity recognition (Tan and Pal 2014), or term extraction (Bouamor, Semmar, and Zweigenbaum 2012b) designed for monolingual, parallel, or comparable data (Morin and Daille 2010).

Closed compounds, obtained by concatenating several lexemes with any parts of speech, are typical of Germanic languages and represent another difficult task for MT. This category of expressions can be lexicalized, that is, they belong to the lexicon of a language as a single meaning unit, such as the German word Schwiegereltern (parentsin-law) or non-lexicalized, that is, the individual words keep their meanings when combined, for instance, the German neologism Helikoptereltern (helicopter parents). They are usually translated into several target language words. Their meaning might be more or less compositional. MT systems fail to correctly translate these compounds because of their low frequencies and their variability. Moreover, non-compositional compounds have unpredictable meaning.

Splitting strategies can be applied to cut the compounds into subsequent words to improve translation quality (Fritzinger and Fraser 2010;Stymne, Cancedda, and Ahrenberg 2013). Splitting is done by identifying component words in the corpus or by prefix and suffix identification together with distributional semantics  or by using a morphosyntactic tagger and parser . Oversplitting can also be a problem: Splitting non-compositional compounds may generate erroneous translations. Some methods aim to distinguish between compositional and non-compositional compounds and split only the compositional ones . A postprocessing step is required to merge components back into compounds once a translation is generated using a system trained on split compounds. Some methods replace the compounds by paraphrases (Ullman and Nivre 2014) before translating them.

Preprocessing methods (concatenation or decomposition) tag MWEs in the input data: This strategy is effective for SMT or for RBMT and avoids data sparsity. As a drawback, such methods can only handle contiguous MWEs. Also, without filtering, MWE identification methods can add noise into the MT system by annotating candidates which are not really MWEs. This results in MT performance loss. 5.2.3 Identification During MT. MWE-aware strategies can be best differentiated according to the MT paradigm under discussion.

MWE-aware strategies in SMT. Phrase-based SMT systems build probabilistic models during the training phase, based on simple word alignment and on a phrase table (a list of pairs of n-grams, their n-gram translation, and their translation scores). Then, the core translation process is ultimately determined by the contents of the phrase table-thus, one way to regard MWE identification during MT is in terms of changing the contents of the phrase table adaptively, during the training phrase (Carpuat and Diab 2010). Several observed approaches are: (1) changing the training data dynamically (word alignment or the parallel corpus) to take into account MWEs and then retraining the system; (2) modifying the phrase table directly by including information about MWEs and their translations. In both strategies, the use of MWE identification and discovery tools is essential to improve the quality of the translation.

Modifying training data. A frequent strategy completes simple word alignment with many-to-many, many-to-one, or one-to-many alignments to solve translation asymmetries (Melamed 1997;Carpuat and Diab 2010;Okita 2012). Word alignment completion is based on simple word alignment and on MWE identification tools, designed for specific MWE categories (Tan and Pal [2014] for multiword named entities; Bouamor, Semmar, and Zweigenbaum [2012b] and Okita and Way [2011] for terms; Ramisch, Villavicencio, and Boitet [2010] for general MWEs). Alternatively, MWE identification and alignment is performed using bilingual lexical resources, with translation alongside an n-gram language model to help with disambiguation (Bungum et al. 2013). The resulting many-to-many word alignment is used to retrain the system in order to build a new phrase table. As a consequence, the phrase table takes into account MWEs and their translations.

Alternatively, bilingual dictionaries of MWEs are added as additional training data to the parallel corpus (Babych and Hartley 2010;Tan and Pal 2014).

Modifying the phrase table. Usually, a bilingual list of MWEs and their equivalents is dynamically extracted from the simple word alignment using specific MWE discovery tools (Bouamor, Semmar, and Zweigenbaum 2012b;Kordoni and Simova 2012;Pal, Naskar, and Bandyopadhyay 2013). Then, the phrase table is completed with the bilingual lists of MWEs and the probabilities are modified accordingly (Lambert and Banchs 2005) or added into a new phrase table with the probability set to 1 (Ren et al. 2009).

An alternate strategy consists of adding new features in the phrase table, such as the number of MWEs present in the bilingual aligned phrases (Carpuat and Diab 2010) or the property that the parallel phrase contains a bilingual MWE (Ren et al. 2009). In this way, the translation quality is improved for certain specific MWE categories or languages (Costa-Jussà, Daudaravicius, and Banchs 2010). The modified phrase table contains, indeed, the correct translations of MWEs, thus avoiding an incorrect wordfor-word translation during the decoding phase and helping disambiguation.

More complex models are proposed in syntax-based SMT (Na et al. 2010) or in hierarchical SMT (Chiang 2007). These approaches use grammars to handle discontiguous components and find their translation directly: parsing improves the translation process (according to BLEU and METEOR scores) by providing trees and transfer rules based on parsed data (Wei and Xu 2011).

MWE-aware strategies in EBMT and RBMT. EBMT (Gangadharaiah, Brown, and Carbonell 2006) or RBMT strategies (Anastasiou 2008;Forcada et al. 2011;Monti et al. 2011) dynamically apply rules to handle MWE translations. Some rules are identified from the syntactic tree alignments (Segura and Prince 2011) and integrated into an EBMT system to handle discontiguous MWEs.

RBMT systems use large lexicons to handle contiguous MWEs and apply the correct translation strategy: a simple word-for-word translation strategy or a compositional rule (Wehrli et al. 2009). Discontiguous MWEs are identified using parsing output or some linguistic patterns. Several RBMT systems identify MWEs and generate translations on the basis of formal representations of natural language texts such as parse trees (Wehrli et al. 2009) or intermediate representation languages like minimal recursion semantics (Oepen et al. 2004), a semantico-syntactic abstraction language (Monti et al. 2011;Barreiro et al. 2013). Transfer rules handle MWE variability and discontiguity (Forcada et al. 2011) and are manually defined or automatically learned from parallel corpora (Haugereid and Bond 2011).

Discontiguous or variable MWEs represent an important source of translation errors. These methods have the advantage of handling discontiguous or variable MWEs with the help of rules for RBMT or by completing word alignments dynamically in SMT.",How do multilingual discovery methods identify and filter MWE candidates using parallel corpora?,"MWE discovery methods based on parallel corpora use alignment to verify whether a source MWE translates word-for-word or rather as a unit on the target side. Alignment is used for both multilingual discovery, that is, finding new translation correspondences between MWEs in two languages, and monolingual discovery, that is, finding new MWEs in one language based on translation asymmetries. Word-level alignment (Och and Ney 2000) indicating that groups of two or more source words are frequently aligned with a single target word can indicate potential MWE candidates. MWE candidates can therefore be extracted by using many-to-one alignments as in Caseli et al. (2010). However, because automatic word alignments tend to be very noisy, several techniques have been proposed to filter them. Caseli et al. (2010) used predefined POS patterns and frequency thresholds to obtain bilingual MWE lists. Bouamor, Semmar, and Zweigenbaum (2012b) use an association measure to find the translations of each MWE in the target language counterpart without exploiting the alignment. Parsed bilingual data has also been used to filter word-aligned MWE candidates. Zarrieß and Kuhn (2009) propose a method for detecting verb-object MWEs in both source and target languages that are dependency-parsed, only retaining MWEs whose words are bilingually aligned and monolingually linked by syntactic dependencies. Tsvetkov and Wintner (2014) proposed supervised classifiers to distinguish MWEs from non-MWEs, using linguistically motivated features such as literal translatability derived from simple word alignments in parallel corpora.",How do methods using parallel corpora identify and filter multiword expression (MWE) candidates in multilingual discovery?,"Methods for discovering multiword expressions (MWEs) using parallel corpora primarily rely on alignment techniques to determine whether a source MWE translates directly word-for-word or as a cohesive unit in the target language. This alignment process serves two purposes: multilingual discovery, which involves identifying new translation correspondences between MWEs in two languages, and monolingual discovery, which focuses on finding new MWEs in a single language by examining translation asymmetries. Word-level alignment, as described by Och and Ney (2000), can reveal potential MWE candidates when groups of two or more source words frequently align with a single target word. Consequently, many-to-one alignments can be used to extract MWE candidates, as demonstrated by Caseli et al. (2010).

However, automatic word alignments are often noisy, necessitating various filtering techniques. Caseli et al. (2010) employed predefined part-of-speech (POS) patterns and frequency thresholds to generate bilingual MWE lists. Bouamor, Semmar, and Zweigenbaum (2012b) utilized an association measure to identify translations of each MWE in the target language without relying on alignment. Additionally, parsed bilingual data has been used to refine word-aligned MWE candidates. Zarrieß and Kuhn (2009) introduced a method for detecting verb-object MWEs in both source and target languages that are dependency-parsed, retaining only those MWEs whose words are aligned bilingually and linked by syntactic dependencies monolingually. Tsvetkov and Wintner (2014) proposed using supervised classifiers to differentiate MWEs from non-MWEs, leveraging linguistically motivated features such as literal translatability derived from simple word alignments in parallel corpora.",7,2,7,4,"Question:

How do multilingual discovery methods identify and filter MWE candidates using parallel corpora?

Answer:

MWE discovery methods based on parallel corpora use alignment to verify whether a source MWE translates word-for-word or rather as a unit on the target side. Alignment is used for both multilingual discovery, that is, finding new translation correspondences between MWEs in two languages, and monolingual discovery, that is, finding new MWEs in one language based on translation asymmetries. Word-level alignment (Och and Ney 2000) indicating that groups of two or more source words are frequently aligned with a single target word can indicate potential MWE candidates. MWE candidates can therefore be extracted by using many-to-one alignments as in Caseli et al. (2010). However, because automatic word alignments tend to be very noisy, several techniques have been proposed to filter them. Caseli et al. (2010) used predefined POS patterns and frequency thresholds to obtain bilingual MWE lists. Bouamor, Semmar, and Zweigenbaum (2012b) use an association measure to find the translations of each MWE in the target language counterpart without exploiting the alignment. Parsed bilingual data has also been used to filter word-aligned MWE candidates. Zarrieß and Kuhn (2009) propose a method for detecting verb-object MWEs in both source and target languages that are dependency-parsed, only retaining MWEs whose words are bilingually aligned and monolingually linked by syntactic dependencies. Tsvetkov and Wintner (2014) proposed supervised classifiers to distinguish MWEs from non-MWEs, using linguistically motivated features such as literal translatability derived from simple word alignments in parallel corpora.","Question:

How do methods using parallel corpora identify and filter multiword expression (MWE) candidates in multilingual discovery?

Answer:

Methods for discovering multiword expressions (MWEs) using parallel corpora primarily rely on alignment techniques to determine whether a source MWE translates directly word-for-word or as a cohesive unit in the target language. This alignment process serves two purposes: multilingual discovery, which involves identifying new translation correspondences between MWEs in two languages, and monolingual discovery, which focuses on finding new MWEs in a single language by examining translation asymmetries. Word-level alignment, as described by Och and Ney (2000), can reveal potential MWE candidates when groups of two or more source words frequently align with a single target word. Consequently, many-to-one alignments can be used to extract MWE candidates, as demonstrated by Caseli et al. (2010).

However, automatic word alignments are often noisy, necessitating various filtering techniques. Caseli et al. (2010) employed predefined part-of-speech (POS) patterns and frequency thresholds to generate bilingual MWE lists. Bouamor, Semmar, and Zweigenbaum (2012b) utilized an association measure to identify translations of each MWE in the target language without relying on alignment. Additionally, parsed bilingual data has been used to refine word-aligned MWE candidates. Zarrieß and Kuhn (2009) introduced a method for detecting verb-object MWEs in both source and target languages that are dependency-parsed, retaining only those MWEs whose words are aligned bilingually and linked by syntactic dependencies monolingually. Tsvetkov and Wintner (2014) proposed using supervised classifiers to differentiate MWEs from non-MWEs, leveraging linguistically motivated features such as literal translatability derived from simple word alignments in parallel corpora.",NO,True,1769,True,True
1509090-s41,Multiword Expression Processing: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2017-12-01,Evaluation of MWE-Aware MT,"The evaluation of MWEs translation quality remains an open challenge, whatever MT paradigm is adopted (Monti et al. 2012;Ramisch, Besacier, and Kobzar 2013;Barreiro et al. 2014), because of a lack of shared assessment methodologies, benchmarking resources, and annotation guidelines.

With reference to the assessment methodologies, automatic evaluation metrics such as BLEU (Papineni et al. 2002) do not specifically take MWE translation quality into account. For instance, BLEU is based on shared words between the candidate and the reference translation, and gives only a very general indication about quality. Thus, it cannot be considered as a suitable metric for the kind of more differentiated analysis required to identify specific gaps in the coverage of the system, as is needed for MWEs. There have been a few attempts to adapt automatic evaluation metrics towards a more fine-grained MT error analysis (Babych and Hartley 2010;Stymne, Cancedda, and Ahrenberg 2013;. Extrinsic evaluations in MT have also been performed, mainly for SMT. For instance, Carpuat and Diab (2010) conducted a pilot study for a task-oriented evaluation of MWE translation in SMT, whereas Bouamor, Semmar, and Zweigenbaum (2012a) consider SMT as an extrinsic evaluation of the usefulness of automatically discovered MWEs and explore strategies for integrating them in a SMT system, aiming at a more thorough error analysis of MWE translation.

Another important drawback in this field is represented by the fact that parallel corpora annotated with MWEs, which are important and necessary gold standard resources for the evaluation of MT translation quality, are very scarce. MWE annotation is indeed a complex and time-consuming task. Annotated resources are usually produced manually and require a large number of experts. In addition, annotating MWEs in parallel corpora requires the correct delimitation of MWEs (contiguous vs. discontiguous expressions) to classify and to disambiguate them and to handle not only the multilingual dimension, but also translation asymmetries between languages (Section 5.1). Moreover, each category of MWEs has its own set of properties.

MWE-annotated benchmarking resources useful for translation quality evaluation are usually available for (1) specific MWE categories, (2) specific language pairs, (3) a specific MWE alignment tool or integration strategy in MT systems, or (4) specific approaches to handling MWEs in MT. Evaluation data consist mainly of small parallel corpora, manually built by carefully selecting sentences containing specific categories of MWE to avoid data sparseness, aligned either with human translations collected from the Web or generated by commercial MT systems (Google Translate, Bing, OpenLogos). Previous work that makes use of such resources includes Ramisch, Besacier, and Kobzar (2013) for verb-particle constructions in English and French; Barreiro et al. (2013) for different categories of MWE in language pairs involving English to French, Italian, and Portuguese; Laporte (2014) for French-Romanian verb-noun idioms and collocations; Weller et al. (2014) for compositional noun compounds, compositional verb compounds, and a set of non-compositional compounds in German-English; Barreiro et al. (2014) for light-verb constructions in English to Italian, French, Portuguese, German, and Spanish; and Schottmüller and Nivre (2014) for verb-particle constructions in the German-English language pair. In addition, these linguistic resources are annotated only with a limited set of MWE categories such as, for instance, light-verb constructions (Vincze 2012;Rácz, Nagy, and Vincze 2014). They are of variable granularity, so some annotation schemes consider only MWE categories, whereas others include additional information such as POS and degree of fixedness.

There are only very few instances of parallel corpora annotated with several categories of MWEs and with different types of correspondences (many-to-one, one-tomany, and many-to-many translations), such as those created by Monti, Sangati, and Arcan (2015), Tutin et al. (2015), and Flickinger et al. (2012). Moreover, the lack of homogeneity represents a real obstacle to the effective reuse of existing annotated data.

Concerning MWE annotation guidelines, only very few papers describe the procedures adopted during resource development. Comprehensive approaches to MWE annotation in parallel corpora, that is, which take into account a large inventory of MWE categories, include Monti, Sangati, and Arcan (2015), who developed a parallel English-Italian corpus, and Tutin et al. (2015), who worked on the French part of a parallel French-English corpus.

In conclusion, the evaluation of MWE processing in MT is still an open issue, as we will discuss in the next section.",What are the main challenges and limitations in evaluating MWE translation quality in machine translation?,"The evaluation of MWEs translation quality remains an open challenge, whatever MT paradigm is adopted, because of a lack of shared assessment methodologies, benchmarking resources, and annotation guidelines. Automatic evaluation metrics such as BLEU do not specifically take MWE translation quality into account, as BLEU is based on shared words between the candidate and the reference translation, providing only a very general indication about quality. Thus, it cannot be considered as a suitable metric for the kind of more differentiated analysis required to identify specific gaps in the coverage of the system, as is needed for MWEs. Another important drawback is the scarcity of parallel corpora annotated with MWEs, which are important and necessary gold standard resources for the evaluation of MT translation quality. MWE annotation is a complex and time-consuming task, usually produced manually and requiring a large number of experts. Annotating MWEs in parallel corpora requires the correct delimitation of MWEs to classify and disambiguate them and to handle not only the multilingual dimension but also translation asymmetries between languages. Moreover, each category of MWEs has its own set of properties. MWE-annotated benchmarking resources are usually available for specific MWE categories, specific language pairs, a specific MWE alignment tool or integration strategy in MT systems, or specific approaches to handling MWEs in MT. Evaluation data consist mainly of small parallel corpora, manually built by carefully selecting sentences containing specific categories of MWE to avoid data sparseness. These linguistic resources are annotated only with a limited set of MWE categories and are of variable granularity. There are only very few instances of parallel corpora annotated with several categories of MWEs and with different types of correspondences. Moreover, the lack of homogeneity represents a real obstacle to the effective reuse of existing annotated data. Concerning MWE annotation guidelines, only very few papers describe the procedures adopted during resource development. Comprehensive approaches to MWE annotation in parallel corpora are limited. In conclusion, the evaluation of MWE processing in MT is still an open issue.",What are the primary challenges and limitations in assessing the quality of multiword expression (MWE) translation in machine translation systems?,"Evaluating the translation quality of multiword expressions (MWEs) in machine translation (MT) systems presents several significant challenges. One of the main issues is the absence of standardized assessment methodologies, benchmarking resources, and annotation guidelines specifically for MWEs. Current automatic evaluation metrics, such as BLEU, do not adequately address MWE translation quality. BLEU focuses on shared words between the candidate and reference translations, offering only a broad indication of quality. This makes it unsuitable for the nuanced analysis needed to identify specific gaps in system coverage, particularly for MWEs.

Another major limitation is the scarcity of parallel corpora annotated with MWEs, which are essential as gold standard resources for evaluating MT translation quality. Annotating MWEs is a complex and time-consuming task, typically requiring manual effort from numerous experts. This process involves accurately delimiting MWEs, classifying and disambiguating them, and managing both the multilingual dimension and translation asymmetries between languages. Each MWE category has its own unique set of properties, adding to the complexity.

MWE-annotated benchmarking resources are often limited to specific MWE categories, language pairs, alignment tools, integration strategies in MT systems, or particular approaches to handling MWEs. Evaluation data usually consist of small parallel corpora, manually curated by selecting sentences with specific MWE categories to mitigate data sparsity. These resources are annotated with only a limited set of MWE categories and vary in granularity. There are very few instances of parallel corpora annotated with multiple MWE categories and different types of correspondences. Additionally, the lack of homogeneity in these resources poses a significant barrier to effectively reusing existing annotated data.

Regarding MWE annotation guidelines, only a few papers detail the procedures used during resource development, and comprehensive approaches to MWE annotation in parallel corpora are limited. In summary, evaluating MWE processing in MT remains an unresolved challenge.",7,4,7,4,"Question:

What are the main challenges and limitations in evaluating MWE translation quality in machine translation?

Answer:

The evaluation of MWEs translation quality remains an open challenge, whatever MT paradigm is adopted, because of a lack of shared assessment methodologies, benchmarking resources, and annotation guidelines. Automatic evaluation metrics such as BLEU do not specifically take MWE translation quality into account, as BLEU is based on shared words between the candidate and the reference translation, providing only a very general indication about quality. Thus, it cannot be considered as a suitable metric for the kind of more differentiated analysis required to identify specific gaps in the coverage of the system, as is needed for MWEs. Another important drawback is the scarcity of parallel corpora annotated with MWEs, which are important and necessary gold standard resources for the evaluation of MT translation quality. MWE annotation is a complex and time-consuming task, usually produced manually and requiring a large number of experts. Annotating MWEs in parallel corpora requires the correct delimitation of MWEs to classify and disambiguate them and to handle not only the multilingual dimension but also translation asymmetries between languages. Moreover, each category of MWEs has its own set of properties. MWE-annotated benchmarking resources are usually available for specific MWE categories, specific language pairs, a specific MWE alignment tool or integration strategy in MT systems, or specific approaches to handling MWEs in MT. Evaluation data consist mainly of small parallel corpora, manually built by carefully selecting sentences containing specific categories of MWE to avoid data sparseness. These linguistic resources are annotated only with a limited set of MWE categories and are of variable granularity. There are only very few instances of parallel corpora annotated with several categories of MWEs and with different types of correspondences. Moreover, the lack of homogeneity represents a real obstacle to the effective reuse of existing annotated data. Concerning MWE annotation guidelines, only very few papers describe the procedures adopted during resource development. Comprehensive approaches to MWE annotation in parallel corpora are limited. In conclusion, the evaluation of MWE processing in MT is still an open issue.","Question:

What are the primary challenges and limitations in assessing the quality of multiword expression (MWE) translation in machine translation systems?

Answer:

Evaluating the translation quality of multiword expressions (MWEs) in machine translation (MT) systems presents several significant challenges. One of the main issues is the absence of standardized assessment methodologies, benchmarking resources, and annotation guidelines specifically for MWEs. Current automatic evaluation metrics, such as BLEU, do not adequately address MWE translation quality. BLEU focuses on shared words between the candidate and reference translations, offering only a broad indication of quality. This makes it unsuitable for the nuanced analysis needed to identify specific gaps in system coverage, particularly for MWEs.

Another major limitation is the scarcity of parallel corpora annotated with MWEs, which are essential as gold standard resources for evaluating MT translation quality. Annotating MWEs is a complex and time-consuming task, typically requiring manual effort from numerous experts. This process involves accurately delimiting MWEs, classifying and disambiguating them, and managing both the multilingual dimension and translation asymmetries between languages. Each MWE category has its own unique set of properties, adding to the complexity.

MWE-annotated benchmarking resources are often limited to specific MWE categories, language pairs, alignment tools, integration strategies in MT systems, or particular approaches to handling MWEs. Evaluation data usually consist of small parallel corpora, manually curated by selecting sentences with specific MWE categories to mitigate data sparsity. These resources are annotated with only a limited set of MWE categories and vary in granularity. There are very few instances of parallel corpora annotated with multiple MWE categories and different types of correspondences. Additionally, the lack of homogeneity in these resources poses a significant barrier to effectively reusing existing annotated data.

Regarding MWE annotation guidelines, only a few papers detail the procedures used during resource development, and comprehensive approaches to MWE annotation in parallel corpora are limited. In summary, evaluating MWE processing in MT remains an unresolved challenge.",NO,True,2170,True,True
226283737-s3,An Empirical Survey of Unsupervised Text Representation Methods on Twitter Data,2020-11-01,Evaluation Metrics,"We use a total of six metrics for evaluating the ""goodness"" of our clusters, described below. Except for the Silhouette score, all other metrics rely on ground-truth labels.

Silhouette score (Rousseeuw, 1987): A good clustering will produce clusters where the elements inside the same cluster are close to each other and the elements in different clusters are far from each other. The Silhouette score takes both these factors into account. The score goes from -1.0 to 1.0, where higher values mean better clustering. Homogeneity, Completeness, and V-measure, (Rosenberg and Hirschberg, 2007): If clusters contain only data points that are members of a single class, in other words, high homogeneity, this usually indicates good clustering. Similarly, if all members of a given class are assigned to the same cluster, in other words, high completeness, this usually indicates good clustering. The Homogeneity and Completeness scores are between 0.0 and 1.0, where higher values correspond to better clustering. The V-measure score is the harmonic mean of Homogeneity and Completeness. Adjusted Rand Index (ARI) (Hubert and Arabie, 1985): The Rand Index can be used to compute the similarity between generated clusters and groundtruth labels. This is done by considering all pairs of samples and seeing whether their label agreement (i.e., belonging to the same ground-truth cluster or not) matches the generated cluster agreement (i.e., belonging to the same generated cluster or not). The raw RI score is then ""adjusted for chance"" into the ARI. score using the following formula: The ARI score can be between -1.0 and 1.0, where random clusterings have an ARI close to 0.0 and 1.0 stands for perfect clustering. Adjusted Mutual Information (AMI) (Vinh et al., 2010): The Mutual Information (MI) score is an information-theoretic metric that measures the amount of ""shared information"" between two clusterings. The Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information (MI) score to account for chance. It accounts for the fact that the MI is generally higher for two cluster- ings with a larger number of clusters, regardless of whether there is actually more information shared. The AMI score can be between 0.0 and 1.0, where random clusterings have an AMI close to 0.0 and 1.0 stands for perfect clustering.","How are clustering evaluation metrics adjusted to account for chance, and what do their value ranges indicate?","The Adjusted Rand Index (ARI) adjusts the Rand Index for chance by considering all pairs of samples and seeing whether their label agreement matches the generated cluster agreement. The ARI score can be between -1.0 and 1.0, where random clusterings have an ARI close to 0.0 and 1.0 stands for perfect clustering. The Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information (MI) score to account for chance. The AMI score can be between 0.0 and 1.0, where random clusterings have an AMI close to 0.0 and 1.0 stands for perfect clustering.","How do clustering evaluation metrics account for chance, and what do their value ranges signify?","Clustering evaluation metrics like the Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) are designed to account for chance in their calculations. The ARI adjusts the Rand Index by evaluating all pairs of samples to determine if their label agreement aligns with the generated cluster agreement. The ARI score ranges from -1.0 to 1.0, where a score close to 0.0 indicates random clustering, and a score of 1.0 signifies perfect clustering. Similarly, the AMI is an adjusted version of the Mutual Information (MI) score, which also accounts for chance. The AMI score ranges from 0.0 to 1.0, with a score near 0.0 indicating random clustering and a score of 1.0 representing perfect clustering.",7,2,7,2,"Question:

How are clustering evaluation metrics adjusted to account for chance, and what do their value ranges indicate?

Answer:

The Adjusted Rand Index (ARI) adjusts the Rand Index for chance by considering all pairs of samples and seeing whether their label agreement matches the generated cluster agreement. The ARI score can be between -1.0 and 1.0, where random clusterings have an ARI close to 0.0 and 1.0 stands for perfect clustering. The Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information (MI) score to account for chance. The AMI score can be between 0.0 and 1.0, where random clusterings have an AMI close to 0.0 and 1.0 stands for perfect clustering.","Question:

How do clustering evaluation metrics account for chance, and what do their value ranges signify?

Answer:

Clustering evaluation metrics like the Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) are designed to account for chance in their calculations. The ARI adjusts the Rand Index by evaluating all pairs of samples to determine if their label agreement aligns with the generated cluster agreement. The ARI score ranges from -1.0 to 1.0, where a score close to 0.0 indicates random clustering, and a score of 1.0 signifies perfect clustering. Similarly, the AMI is an adjusted version of the Mutual Information (MI) score, which also accounts for chance. The AMI score ranges from 0.0 to 1.0, with a score near 0.0 indicating random clustering and a score of 1.0 representing perfect clustering.",NO,True,706,True,True
251196750-s3,"""Do you follow me?"": A Survey of Recent Approaches in Dialogue State Tracking",2022-07-29,Datasets,"Many public datasets have been published to advance machine learning approaches for DST. The evolution of these datasets is marked by increasing dialogue complexity, especially with the advent of multidomain corpora. We distinguish two main approaches for collecting dialogue datasets: (i) Wizard-of-Oz (WOZ or H2H for human-to-human) approaches where two humans (asynchronously) play the roles of user and agent according to a task description. This approach allows for natural and varied dialogues, but the subsequent annotation can be a source of errors. (ii) Simulation-based approaches (M2M for machine-to-machine) where two systems play the roles of user and agent and interact with each other to generate conversation templates that are then paraphrased by humans. The advantage of this method is that the annotations are obtained automatically. However, the complexity of the task and linguistic diversity are often limited because the dialogue is simulated. Table 1 lists the main datasets as well as recent datasets relevant to the problems discussed in Sec-  between paid participants and various telephone dialogue systems (H2M collection). The user needs to find a restaurant by specifying constraints such as the type of cuisine and can request specific information such as the phone number.

MultiWOZ 9 (Budzianowski et al., 2018) The first large-scale multidomain corpus and currently the main benchmark for DST. It contains dialogues between a tourist and a travel clerk that can span several domains. A major problem related to the way the data was collected is the inconsistency and errors of annotation, which was crowdsourced. Four more versions were later released to try and fix these errors. (Eric et al., 2019;Han et al., 2021;Ye et al., 2021a). Multilingual versions 10 were obtained by a process of machine translation followed by manual correction (Gunasekara et al., 2020;Zuo et al., 2021).

SGD 11 (Rastogi et al., 2020b) The Schema-Guided Dataset was created to elicit research on domain independence through the use of schemas. Schemas describe domains, slots, and intents in natural language and can be used to handle unseen domains. The test set includes unseen schemas to encourage model generalization. SGD-X is an extension designed to study model robustness to different schema wordings (Lee et al., 2021b).","What are the two main approaches for collecting dialogue datasets, and what are their respective advantages and disadvantages?","Many public datasets have been published to advance machine learning approaches for dialogue state tracking (DST). The evolution of these datasets is marked by increasing dialogue complexity, especially with the advent of multidomain corpora. Two main approaches for collecting dialogue datasets are distinguished: (i) Wizard-of-Oz (WOZ or H2H for human-to-human) approaches where two humans (asynchronously) play the roles of user and agent according to a task description. This approach allows for natural and varied dialogues, but the subsequent annotation can be a source of errors. (ii) Simulation-based approaches (M2M for machine-to-machine) where two systems play the roles of user and agent and interact with each other to generate conversation templates that are then paraphrased by humans. The advantage of this method is that the annotations are obtained automatically. However, the complexity of the task and linguistic diversity are often limited because the dialogue is simulated.","What are the two primary methods for collecting dialogue datasets, and what are the pros and cons of each?","The development of public datasets has significantly contributed to advancing machine learning techniques for dialogue state tracking (DST). Over time, these datasets have evolved to include more complex dialogues, particularly with the introduction of multidomain corpora. There are two primary methods for collecting dialogue datasets: 

1. Wizard-of-Oz (WOZ or H2H for human-to-human) approach: In this method, two humans asynchronously assume the roles of user and agent based on a given task description. This approach facilitates the creation of natural and varied dialogues. However, it can lead to errors during the subsequent annotation process.

2. Simulation-based approach (M2M for machine-to-machine): Here, two systems act as user and agent, interacting with each other to generate conversation templates. These templates are later paraphrased by humans. The main advantage of this method is the automatic generation of annotations. Nonetheless, the dialogues produced often lack complexity and linguistic diversity due to their simulated nature.",9,2,9,2,"Question:

What are the two main approaches for collecting dialogue datasets, and what are their respective advantages and disadvantages?

Answer:

Many public datasets have been published to advance machine learning approaches for dialogue state tracking (DST). The evolution of these datasets is marked by increasing dialogue complexity, especially with the advent of multidomain corpora. Two main approaches for collecting dialogue datasets are distinguished: (i) Wizard-of-Oz (WOZ or H2H for human-to-human) approaches where two humans (asynchronously) play the roles of user and agent according to a task description. This approach allows for natural and varied dialogues, but the subsequent annotation can be a source of errors. (ii) Simulation-based approaches (M2M for machine-to-machine) where two systems play the roles of user and agent and interact with each other to generate conversation templates that are then paraphrased by humans. The advantage of this method is that the annotations are obtained automatically. However, the complexity of the task and linguistic diversity are often limited because the dialogue is simulated.","Question:

What are the two primary methods for collecting dialogue datasets, and what are the pros and cons of each?

Answer:

The development of public datasets has significantly contributed to advancing machine learning techniques for dialogue state tracking (DST). Over time, these datasets have evolved to include more complex dialogues, particularly with the introduction of multidomain corpora. There are two primary methods for collecting dialogue datasets: 

1. Wizard-of-Oz (WOZ or H2H for human-to-human) approach: In this method, two humans asynchronously assume the roles of user and agent based on a given task description. This approach facilitates the creation of natural and varied dialogues. However, it can lead to errors during the subsequent annotation process.

2. Simulation-based approach (M2M for machine-to-machine): Here, two systems act as user and agent, interacting with each other to generate conversation templates. These templates are later paraphrased by humans. The main advantage of this method is the automatic generation of annotations. Nonetheless, the dialogues produced often lack complexity and linguistic diversity due to their simulated nature.",NO,True,1060,True,True
251196750-s8,"""Do you follow me?"": A Survey of Recent Approaches in Dialogue State Tracking",2022-07-29,Adapting PLMs to Dialogues,"Though now commonly used for DST, existing PLMs are pretrained on free-form text using language modeling objectives. Their ability to model dialogue context and multi-turn dynamics is therefore limited. It has been shown that adapting a PLM to the target domain or task by continuing self-supervised learning can lead to performance gains (Gururangan et al., 2020). This method has been applied to TOD systems and DST.

There are two underlying questions with this approach: the selection of adaptation data and the formulation of self-supervised training objectives to learn better dialogue representations for the downstream task.  gather nine TOD corpora and continue BERT's pretraining with masked language modeling and next response selection. The obtained model TOD-BERT provides an improvement over a standard BERT model on several TOD tasks including DST. With a similar setup, Zhu et al. (2021) contrast these results and find that such adaptation is most beneficial when little annotated data is available. Based on TOD-BERT, Hung et al. (2022) show that it is advantageous not only to adapt a PLM to dialogues but also to the target domain. To do so, they use conversational data from Reddit filtered to contain terms specific to the target domain. Finally,  introduce two objective functions designed to inject inductive biases into a PLM in order to jointly represent dynamic dialogue utterances and ontology structure. They evaluate their method on conversational semantic parsing tasks including DST.",How can adapting PLMs to dialogues and target domains enhance performance in dialogue state tracking tasks?,"Though now commonly used for DST, existing PLMs are pretrained on free-form text using language modeling objectives. Their ability to model dialogue context and multi-turn dynamics is therefore limited. It has been shown that adapting a PLM to the target domain or task by continuing self-supervised learning can lead to performance gains (Gururangan et al., 2020). This method has been applied to TOD systems and DST. There are two underlying questions with this approach: the selection of adaptation data and the formulation of self-supervised training objectives to learn better dialogue representations for the downstream task. The obtained model TOD-BERT provides an improvement over a standard BERT model on several TOD tasks including DST. With a similar setup, Zhu et al. (2021) contrast these results and find that such adaptation is most beneficial when little annotated data is available. Based on TOD-BERT, Hung et al. (2022) show that it is advantageous not only to adapt a PLM to dialogues but also to the target domain. To do so, they use conversational data from Reddit filtered to contain terms specific to the target domain. Finally, introduce two objective functions designed to inject inductive biases into a PLM in order to jointly represent dynamic dialogue utterances and ontology structure. They evaluate their method on conversational semantic parsing tasks including DST.",How does adapting pre-trained language models (PLMs) to dialogues and specific target domains improve performance in dialogue state tracking (DST) tasks?,"Pre-trained language models (PLMs) are typically trained on free-form text using language modeling objectives, which limits their ability to effectively model dialogue context and multi-turn dynamics. To enhance their performance in dialogue state tracking (DST) tasks, adapting PLMs to the target domain or task through continued self-supervised learning has been shown to yield performance improvements (Gururangan et al., 2020). This adaptation approach has been applied to task-oriented dialogue (TOD) systems and DST, focusing on two key aspects: selecting appropriate adaptation data and formulating self-supervised training objectives to better capture dialogue representations for downstream tasks.

For instance, the TOD-BERT model demonstrates improved performance over a standard BERT model on several TOD tasks, including DST. Zhu et al. (2021) further highlight that such adaptation is particularly beneficial when there is limited annotated data available. Building on TOD-BERT, Hung et al. (2022) emphasize the importance of adapting PLMs not only to dialogues but also to the specific target domain. They achieve this by using conversational data from Reddit, filtered to include terms specific to the target domain. Additionally, they introduce two objective functions designed to inject inductive biases into a PLM, enabling it to jointly represent dynamic dialogue utterances and ontology structure. Their method is evaluated on conversational semantic parsing tasks, including DST, demonstrating the advantages of this adaptation approach.",7,6,7,4,"Question:

How can adapting PLMs to dialogues and target domains enhance performance in dialogue state tracking tasks?

Answer:

Though now commonly used for DST, existing PLMs are pretrained on free-form text using language modeling objectives. Their ability to model dialogue context and multi-turn dynamics is therefore limited. It has been shown that adapting a PLM to the target domain or task by continuing self-supervised learning can lead to performance gains (Gururangan et al., 2020). This method has been applied to TOD systems and DST. There are two underlying questions with this approach: the selection of adaptation data and the formulation of self-supervised training objectives to learn better dialogue representations for the downstream task. The obtained model TOD-BERT provides an improvement over a standard BERT model on several TOD tasks including DST. With a similar setup, Zhu et al. (2021) contrast these results and find that such adaptation is most beneficial when little annotated data is available. Based on TOD-BERT, Hung et al. (2022) show that it is advantageous not only to adapt a PLM to dialogues but also to the target domain. To do so, they use conversational data from Reddit filtered to contain terms specific to the target domain. Finally, introduce two objective functions designed to inject inductive biases into a PLM in order to jointly represent dynamic dialogue utterances and ontology structure. They evaluate their method on conversational semantic parsing tasks including DST.","Question:

How does adapting pre-trained language models (PLMs) to dialogues and specific target domains improve performance in dialogue state tracking (DST) tasks?

Answer:

Pre-trained language models (PLMs) are typically trained on free-form text using language modeling objectives, which limits their ability to effectively model dialogue context and multi-turn dynamics. To enhance their performance in dialogue state tracking (DST) tasks, adapting PLMs to the target domain or task through continued self-supervised learning has been shown to yield performance improvements (Gururangan et al., 2020). This adaptation approach has been applied to task-oriented dialogue (TOD) systems and DST, focusing on two key aspects: selecting appropriate adaptation data and formulating self-supervised training objectives to better capture dialogue representations for downstream tasks.

For instance, the TOD-BERT model demonstrates improved performance over a standard BERT model on several TOD tasks, including DST. Zhu et al. (2021) further highlight that such adaptation is particularly beneficial when there is limited annotated data available. Building on TOD-BERT, Hung et al. (2022) emphasize the importance of adapting PLMs not only to dialogues but also to the specific target domain. They achieve this by using conversational data from Reddit, filtered to include terms specific to the target domain. Additionally, they introduce two objective functions designed to inject inductive biases into a PLM, enabling it to jointly represent dynamic dialogue utterances and ontology structure. Their method is evaluated on conversational semantic parsing tasks, including DST, demonstrating the advantages of this adaptation approach.",NO,True,1559,True,True
251196750-s10,"""Do you follow me?"": A Survey of Recent Approaches in Dialogue State Tracking",2022-07-29,Prompting Generative Models to Address Unseen Domains,"A recent paradigm tackles all text-based language tasks with a single model by converting them into a text-to-text format (Raffel et al., 2020). This approach relies on textual instructions called prompts that are prepended to the input to condition the model to perform a task. It has been successfully applied to DST, not only closing the performance gap between classification and generation methods but also opening opportunities to address unseen domains using schemas (cfr. Section 2). In addition to slot and domain names, Lee et al. (2021a) in-clude slot descriptions in a prompt to independently generate values for each slot. They also add possible values for categorical slots. Zhao et al. ( , 2022 expand on this approach by generating the entire dialogue state sequentially, as illustrated in Figure 2. In an analysis of prompt formulation, Cao and Zhang (2021) find that a question format yields better results for prompt-based DST.

Hand-crafting textual prompts may lead to suboptimal conditioning of a PLM. Instead of using discrete tokens, we can optimize these instructions as continuous embeddings, a method called prompt tuning (Lester et al., 2021), which has been applied to DST for continual learning, adding new domains through time (Zhu et al., 2022). Alternatively, Yang et al. (2022) reverse description-based prompts and formulate a prompt based on values extracted from the utterance to generate their respective slot. They argue that slots that appear in a small corpus do not represent all potential requirements whereas values are often explicitly mentioned. Rather than using descriptions that indirectly convey the semantics of a schema, others have sought to prompt a model with instructions, i.e. in-context learning, in which the prompt consists of a few example input-output pairs (Hu et al., 2022;Gupta et al., 2022). ",How has the recent paradigm of converting tasks into a text-to-text format been applied to Dialogue State Tracking (DST)?,"A recent paradigm tackles all text-based language tasks with a single model by converting them into a text-to-text format (Raffel et al., 2020). This approach relies on textual instructions called prompts that are prepended to the input to condition the model to perform a task. It has been successfully applied to DST, not only closing the performance gap between classification and generation methods but also opening opportunities to address unseen domains using schemas. In addition to slot and domain names, Lee et al. (2021a) include slot descriptions in a prompt to independently generate values for each slot. They also add possible values for categorical slots. Zhao et al. (2022) expand on this approach by generating the entire dialogue state sequentially. In an analysis of prompt formulation, Cao and Zhang (2021) find that a question format yields better results for prompt-based DST. Hand-crafting textual prompts may lead to suboptimal conditioning of a PLM. Instead of using discrete tokens, continuous embeddings can be optimized, a method called prompt tuning (Lester et al., 2021), which has been applied to DST for continual learning, adding new domains through time (Zhu et al., 2022). Alternatively, Yang et al. (2022) reverse description-based prompts and formulate a prompt based on values extracted from the utterance to generate their respective slot. They argue that slots that appear in a small corpus do not represent all potential requirements whereas values are often explicitly mentioned. Rather than using descriptions that indirectly convey the semantics of a schema, others have sought to prompt a model with instructions, i.e., in-context learning, in which the prompt consists of a few example input-output pairs (Hu et al., 2022; Gupta et al., 2022).",How is the text-to-text paradigm being utilized in Dialogue State Tracking (DST)?,"The recent paradigm of converting all text-based language tasks into a text-to-text format, as proposed by Raffel et al. (2020), has been effectively applied to Dialogue State Tracking (DST). This approach uses textual instructions, known as prompts, which are added to the input to guide the model in performing specific tasks. It has successfully bridged the performance gap between classification and generation methods in DST and has also enabled the handling of unseen domains using schemas. For instance, Lee et al. (2021a) enhance this method by including slot descriptions in prompts to independently generate values for each slot, and they also incorporate possible values for categorical slots. Zhao et al. (2022) further develop this by generating the entire dialogue state sequentially. 

Cao and Zhang (2021) found that using a question format for prompts yields better results in DST. However, crafting textual prompts manually can lead to suboptimal conditioning of a pre-trained language model (PLM). To address this, prompt tuning, as described by Lester et al. (2021), optimizes continuous embeddings instead of discrete tokens and has been applied to DST for continual learning, allowing the addition of new domains over time (Zhu et al., 2022). Alternatively, Yang et al. (2022) propose reversing description-based prompts by formulating prompts based on values extracted from the utterance to generate their respective slots, arguing that values are often explicitly mentioned, unlike slots that may not represent all potential requirements. Instead of using descriptions that indirectly convey schema semantics, some researchers have explored prompting models with instructions through in-context learning, where the prompt includes a few example input-output pairs (Hu et al., 2022; Gupta et al., 2022).",7,4,7,4,"Question:

How has the recent paradigm of converting tasks into a text-to-text format been applied to Dialogue State Tracking (DST)?

Answer:

A recent paradigm tackles all text-based language tasks with a single model by converting them into a text-to-text format (Raffel et al., 2020). This approach relies on textual instructions called prompts that are prepended to the input to condition the model to perform a task. It has been successfully applied to DST, not only closing the performance gap between classification and generation methods but also opening opportunities to address unseen domains using schemas. In addition to slot and domain names, Lee et al. (2021a) include slot descriptions in a prompt to independently generate values for each slot. They also add possible values for categorical slots. Zhao et al. (2022) expand on this approach by generating the entire dialogue state sequentially. In an analysis of prompt formulation, Cao and Zhang (2021) find that a question format yields better results for prompt-based DST. Hand-crafting textual prompts may lead to suboptimal conditioning of a PLM. Instead of using discrete tokens, continuous embeddings can be optimized, a method called prompt tuning (Lester et al., 2021), which has been applied to DST for continual learning, adding new domains through time (Zhu et al., 2022). Alternatively, Yang et al. (2022) reverse description-based prompts and formulate a prompt based on values extracted from the utterance to generate their respective slot. They argue that slots that appear in a small corpus do not represent all potential requirements whereas values are often explicitly mentioned. Rather than using descriptions that indirectly convey the semantics of a schema, others have sought to prompt a model with instructions, i.e., in-context learning, in which the prompt consists of a few example input-output pairs (Hu et al., 2022; Gupta et al., 2022).","Question:

How is the text-to-text paradigm being utilized in Dialogue State Tracking (DST)?

Answer:

The recent paradigm of converting all text-based language tasks into a text-to-text format, as proposed by Raffel et al. (2020), has been effectively applied to Dialogue State Tracking (DST). This approach uses textual instructions, known as prompts, which are added to the input to guide the model in performing specific tasks. It has successfully bridged the performance gap between classification and generation methods in DST and has also enabled the handling of unseen domains using schemas. For instance, Lee et al. (2021a) enhance this method by including slot descriptions in prompts to independently generate values for each slot, and they also incorporate possible values for categorical slots. Zhao et al. (2022) further develop this by generating the entire dialogue state sequentially. 

Cao and Zhang (2021) found that using a question format for prompts yields better results in DST. However, crafting textual prompts manually can lead to suboptimal conditioning of a pre-trained language model (PLM). To address this, prompt tuning, as described by Lester et al. (2021), optimizes continuous embeddings instead of discrete tokens and has been applied to DST for continual learning, allowing the addition of new domains over time (Zhu et al., 2022). Alternatively, Yang et al. (2022) propose reversing description-based prompts by formulating prompts based on values extracted from the utterance to generate their respective slots, arguing that values are often explicitly mentioned, unlike slots that may not represent all potential requirements. Instead of using descriptions that indirectly convey schema semantics, some researchers have explored prompting models with instructions through in-context learning, where the prompt includes a few example input-output pairs (Hu et al., 2022; Gupta et al., 2022).",NO,True,1826,True,True
251196750-s14,"""Do you follow me?"": A Survey of Recent Approaches in Dialogue State Tracking",2022-07-29,Robustness,"Since users may express similar requirements in different ways, a DST model must be able to interpret different formulations of a request in a consistent manner. In other words, it must be robust to variations in the input. Analytical work has shown that models' performance drops when they are confronted with realistic examples that deviate from the test set distribution Li et al., 2021a). Recent work has tried to address this issue through regularization techniques (Heck et al., 2022). Related to this, an understudied aspect is dealing with utterances that deviate from the norm in the case of a written dialogue system. A DST model must be able to take into account all the history and adjust its predictions of the dialogue state using all available information. Many works have found that performance degrades rapidly as the dialogue length increases. Another critical aspect is therefore efficiently processing long dialogues . The dialogue state condenses important information, but correcting an error made in an earlier turn may be difficult. To overcome this error propagation issue, Tian et al. (2021a) use a two-pass dialogue state generation to correct potential errors, while Manotumruksa et al. (2021) propose a turn-based objective function to penalize the model for incorrect prediction in early turns. Despite this need, Jakobovits et al. (2022) have shown that popular DST datasets are not conversational: most utterances can be parsed in isolation. Some works simulate longer and more realistic dialogues by inserting chit-chat into TODs (Kottur et al., 2021b;Sun et al., 2021).

Another point that has not been studied much in recent approaches is robustness to speech inputs (Faruqui and Hakkani-Tür, 2021). For spoken dialogue systems, new challenges arise such as ASR errors or verbal disfluencies. Early editions of DSTC provided spoken corpora that included a transcript and ASR hypotheses. Since then, DST datasets have been primarily text-based. A DSTC10 track considered this aspect again and proposed a DST task with validation and test sets containing ASR hypotheses (Kim et al., 2021).

Learning robust models requires diverse datasets that represent real-world challenges. In this sense, several evaluation benchmarks have been published to study TOD systems' robustness (Lee et al., 2021b;Peng et al., 2021b;. Data augmentation is a potential solution to the lack of variety in datasets (Campagna et al., 2020;Li et al., 2021a;Aksu et al., 2022), especially for simulating ASR errors (Wang et al., 2020;Tian et al., 2021b).",How do recent approaches address the issue of performance drop in DST models due to input variations?,"Since users may express similar requirements in different ways, a DST model must be able to interpret different formulations of a request in a consistent manner. Recent work has tried to address this issue through regularization techniques (Heck et al., 2022). A DST model must be able to take into account all the history and adjust its predictions of the dialogue state using all available information. To overcome this error propagation issue, Tian et al. (2021a) use a two-pass dialogue state generation to correct potential errors, while Manotumruksa et al. (2021) propose a turn-based objective function to penalize the model for incorrect prediction in early turns. Learning robust models requires diverse datasets that represent real-world challenges. In this sense, several evaluation benchmarks have been published to study TOD systems' robustness (Lee et al., 2021b; Peng et al., 2021b). Data augmentation is a potential solution to the lack of variety in datasets (Campagna et al., 2020; Li et al., 2021a; Aksu et al., 2022), especially for simulating ASR errors (Wang et al., 2020; Tian et al., 2021b).",How are recent approaches tackling the performance drop in Dialogue State Tracking (DST) models caused by input variations?,"Recent approaches have focused on several strategies to address the performance drop in DST models due to input variations. Since users can express similar needs in various ways, DST models must consistently interpret different formulations of a request. Regularization techniques have been employed to enhance this capability (Heck et al., 2022). Additionally, DST models need to consider the entire dialogue history to adjust their predictions accurately. To mitigate error propagation, Tian et al. (2021a) introduced a two-pass dialogue state generation method to correct potential errors, while Manotumruksa et al. (2021) developed a turn-based objective function that penalizes incorrect predictions in early dialogue turns. Building robust models also requires diverse datasets that reflect real-world challenges. In this regard, several evaluation benchmarks have been published to assess the robustness of Task-Oriented Dialogue (TOD) systems (Lee et al., 2021b; Peng et al., 2021b). Data augmentation is another promising solution to address the lack of variety in datasets, particularly for simulating Automatic Speech Recognition (ASR) errors (Campagna et al., 2020; Li et al., 2021a; Aksu et al., 2022; Wang et al., 2020; Tian et al., 2021b).",7,4,7,4,"Question:

How do recent approaches address the issue of performance drop in DST models due to input variations?

Answer:

Since users may express similar requirements in different ways, a DST model must be able to interpret different formulations of a request in a consistent manner. Recent work has tried to address this issue through regularization techniques (Heck et al., 2022). A DST model must be able to take into account all the history and adjust its predictions of the dialogue state using all available information. To overcome this error propagation issue, Tian et al. (2021a) use a two-pass dialogue state generation to correct potential errors, while Manotumruksa et al. (2021) propose a turn-based objective function to penalize the model for incorrect prediction in early turns. Learning robust models requires diverse datasets that represent real-world challenges. In this sense, several evaluation benchmarks have been published to study TOD systems' robustness (Lee et al., 2021b; Peng et al., 2021b). Data augmentation is a potential solution to the lack of variety in datasets (Campagna et al., 2020; Li et al., 2021a; Aksu et al., 2022), especially for simulating ASR errors (Wang et al., 2020; Tian et al., 2021b).","Question:

How are recent approaches tackling the performance drop in Dialogue State Tracking (DST) models caused by input variations?

Answer:

Recent approaches have focused on several strategies to address the performance drop in DST models due to input variations. Since users can express similar needs in various ways, DST models must consistently interpret different formulations of a request. Regularization techniques have been employed to enhance this capability (Heck et al., 2022). Additionally, DST models need to consider the entire dialogue history to adjust their predictions accurately. To mitigate error propagation, Tian et al. (2021a) introduced a two-pass dialogue state generation method to correct potential errors, while Manotumruksa et al. (2021) developed a turn-based objective function that penalizes incorrect predictions in early dialogue turns. Building robust models also requires diverse datasets that reflect real-world challenges. In this regard, several evaluation benchmarks have been published to assess the robustness of Task-Oriented Dialogue (TOD) systems (Lee et al., 2021b; Peng et al., 2021b). Data augmentation is another promising solution to address the lack of variety in datasets, particularly for simulating Automatic Speech Recognition (ASR) errors (Campagna et al., 2020; Li et al., 2021a; Aksu et al., 2022; Wang et al., 2020; Tian et al., 2021b).",NO,True,1254,True,True
53593090-s3,A Review of Standard Text Classification Practices for Multi-label Toxicity Identification of Online Content,2018,Neural Network Classification Models,"While word embeddings are a semantic representation of words, bidirectional neural networks are the technology known for generating a semantic representation for a given sequence of words. Bidirectional recurrent neural networks learn the meaning of a sentence not only from the individual words but by processing the dependencies of the surrounding words through forward and backward connections. Both bi-LSTM (Chen et al., 2016) and bi-GRU (Chung et al., 2015) architectures are shown to perform well in sentence representation. LSTM and GRU layers have a proficient learning ability for long text, because they can control how much information should be received in the current step, how much should be forgotten, and how much information should be passed back. Attention layers (Parikh et al., 2016;Felbo et al., 2017) are mechanisms suitable for converting sequence representations, which are usually in the form of matrices, to a vector representation that is tailored for the desired classification tasks. We investigated the impact of leveraging these technologies by training and testing of two neural network structures shown in Figures 2a and b. Pre-trained fasttext embeddings are used and stop words are not removed, since we want the LSTM and attention layer learn the complete sequences. The neural network shown in Figure 2a which contains two layers of biLSTM to encode the information of sequences achieves 0.9842 and the one shown in Figure 2b which uses attention mechanism to combine the context information from embedding layer and the sequence information from each biL-STM layer to get a summary vector of the sentence, reaches 0.9844 in AUC.",How do bidirectional recurrent neural networks and attention layers contribute to sentence representation in neural network models?,"Bidirectional recurrent neural networks learn the meaning of a sentence not only from the individual words but by processing the dependencies of the surrounding words through forward and backward connections. Both bi-LSTM (Chen et al., 2016) and bi-GRU (Chung et al., 2015) architectures are shown to perform well in sentence representation. LSTM and GRU layers have a proficient learning ability for long text, because they can control how much information should be received in the current step, how much should be forgotten, and how much information should be passed back. Attention layers (Parikh et al., 2016; Felbo et al., 2017) are mechanisms suitable for converting sequence representations, which are usually in the form of matrices, to a vector representation that is tailored for the desired classification tasks.",How do bidirectional recurrent neural networks and attention layers enhance sentence representation in neural network models?,"Bidirectional recurrent neural networks (RNNs) enhance sentence representation by capturing the context of words in both forward and backward directions. This dual processing allows the model to understand the meaning of a sentence not just from individual words, but also by considering the dependencies between surrounding words. Architectures like bidirectional LSTM (bi-LSTM) (Chen et al., 2016) and bidirectional GRU (bi-GRU) (Chung et al., 2015) have demonstrated strong performance in this area. LSTM and GRU layers are particularly adept at handling long text sequences because they can effectively manage the flow of information—deciding how much to retain, forget, or pass on at each step.

Attention layers further contribute to sentence representation by transforming sequence representations, typically in matrix form, into vector representations that are optimized for specific classification tasks. This mechanism, as shown in works by Parikh et al. (2016) and Felbo et al. (2017), allows the model to focus on the most relevant parts of the input sequence, thereby improving the quality of the sentence representation.",8,3,8,2,"Question:

How do bidirectional recurrent neural networks and attention layers contribute to sentence representation in neural network models?

Answer:

Bidirectional recurrent neural networks learn the meaning of a sentence not only from the individual words but by processing the dependencies of the surrounding words through forward and backward connections. Both bi-LSTM (Chen et al., 2016) and bi-GRU (Chung et al., 2015) architectures are shown to perform well in sentence representation. LSTM and GRU layers have a proficient learning ability for long text, because they can control how much information should be received in the current step, how much should be forgotten, and how much information should be passed back. Attention layers (Parikh et al., 2016; Felbo et al., 2017) are mechanisms suitable for converting sequence representations, which are usually in the form of matrices, to a vector representation that is tailored for the desired classification tasks.","Question:

How do bidirectional recurrent neural networks and attention layers enhance sentence representation in neural network models?

Answer:

Bidirectional recurrent neural networks (RNNs) enhance sentence representation by capturing the context of words in both forward and backward directions. This dual processing allows the model to understand the meaning of a sentence not just from individual words, but also by considering the dependencies between surrounding words. Architectures like bidirectional LSTM (bi-LSTM) (Chen et al., 2016) and bidirectional GRU (bi-GRU) (Chung et al., 2015) have demonstrated strong performance in this area. LSTM and GRU layers are particularly adept at handling long text sequences because they can effectively manage the flow of information—deciding how much to retain, forget, or pass on at each step.

Attention layers further contribute to sentence representation by transforming sequence representations, typically in matrix form, into vector representations that are optimized for specific classification tasks. This mechanism, as shown in works by Parikh et al. (2016) and Felbo et al. (2017), allows the model to focus on the most relevant parts of the input sequence, thereby improving the quality of the sentence representation.",NO,True,1134,True,True
252624550-s3,IMTVault: Extracting and Enriching Low-resource Language Interlinear Glossed Text from Grammatical Descriptions and Typological Survey Articles,2022,Data Modelling,"The interlinear sentence has received quite some theoretical treatment. The first technical approach was the implementation in the program Shoebox, which would later become Toolbox. 10 The representation used therein was actually never intended to be used in a productive environment, but turned out to become the mainstay for language documenters for more than two decades. Shoebox/Toolbox was developed by SIL, who discontinued development in favour of FLEx, an XML based tool. 11 In parallel, ELAN 12 ( (Wittenburg et al., 2006)) is another XML-based tool for the representation of correpondences and part-whole relations 10 https://software.sil.org/toolbox 11 https://software.sil.org/fieldworks 12 https://archive.mpi.nl/tla/elan in glossed texts ( (Nordhoff, 2020a)). While XML suggest a good perspectives for programmatic extraction of data, (Nordhoff, 2020a) reports that while syntactically valid XML, the ELAN files retrieved from language archives are semantically wildly heterogeneous, making a principled approach very difficult (also compare (Cimiano et al., 2020, 4)). On a more theoretical level, (Drude, 2002) proposed a very elaborate model with a multiplicity of tiers. The XML Interlinear Glossed Text (XIGT, (Goodman et al., 2015)) format has a recursive structure instead, allowing for an arbitrary number of tiers ( (Xia et al., 2014)). (Chiarcos and Ionov, 2019) and (Ionov, 2021) developed a Linked Data version of XIGT, called LIGT, also used in (Nordhoff, 2020a;Nordhoff, 2020b). For the purposes of this paper, a very simple data model distinguishing the tiers of ""utterance"" and ""word"", with respective translations, is sufficient; the level of ""morpheme"" is disregarded. Basic storage is done in JSON, while transformations into JSON-LD, RDF, and CLDF are also made available. An additional morpheme tier could also have been made available, but it was determined that data consumers could easily create such more granular structures easily themselves should the need arise and that it was not necessary to provide an artificially inflated dataset.",What are the challenges and solutions in using XML for interlinear glossed text data extraction?,"While XML suggests good perspectives for programmatic extraction of data, the ELAN files retrieved from language archives are semantically wildly heterogeneous, making a principled approach very difficult (Nordhoff, 2020a). On a more theoretical level, Drude (2002) proposed a very elaborate model with a multiplicity of tiers. The XML Interlinear Glossed Text (XIGT) format has a recursive structure instead, allowing for an arbitrary number of tiers (Xia et al., 2014). Chiarcos and Ionov (2019) and Ionov (2021) developed a Linked Data version of XIGT, called LIGT, also used in Nordhoff (2020a; Nordhoff, 2020b).",What are the challenges and solutions in using XML for extracting interlinear glossed text data?,"Using XML for extracting interlinear glossed text data presents several challenges. One major issue is the semantic heterogeneity of ELAN files retrieved from language archives, which makes a principled approach to data extraction very difficult (Nordhoff, 2020a). On a theoretical level, Drude (2002) proposed a complex model with multiple tiers, while the XML Interlinear Glossed Text (XIGT) format offers a recursive structure that allows for an arbitrary number of tiers (Xia et al., 2014). To address these challenges, Chiarcos and Ionov (2019) and Ionov (2021) developed a Linked Data version of XIGT, known as LIGT, which has also been utilized in Nordhoff's work (2020a; 2020b).",7,4,7,4,"Question:

What are the challenges and solutions in using XML for interlinear glossed text data extraction?

Answer:

While XML suggests good perspectives for programmatic extraction of data, the ELAN files retrieved from language archives are semantically wildly heterogeneous, making a principled approach very difficult (Nordhoff, 2020a). On a more theoretical level, Drude (2002) proposed a very elaborate model with a multiplicity of tiers. The XML Interlinear Glossed Text (XIGT) format has a recursive structure instead, allowing for an arbitrary number of tiers (Xia et al., 2014). Chiarcos and Ionov (2019) and Ionov (2021) developed a Linked Data version of XIGT, called LIGT, also used in Nordhoff (2020a; Nordhoff, 2020b).","Question:

What are the challenges and solutions in using XML for extracting interlinear glossed text data?

Answer:

Using XML for extracting interlinear glossed text data presents several challenges. One major issue is the semantic heterogeneity of ELAN files retrieved from language archives, which makes a principled approach to data extraction very difficult (Nordhoff, 2020a). On a theoretical level, Drude (2002) proposed a complex model with multiple tiers, while the XML Interlinear Glossed Text (XIGT) format offers a recursive structure that allows for an arbitrary number of tiers (Xia et al., 2014). To address these challenges, Chiarcos and Ionov (2019) and Ionov (2021) developed a Linked Data version of XIGT, known as LIGT, which has also been utilized in Nordhoff's work (2020a; 2020b).",NO,True,686,True,True
251719280-s3,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,2022-08-22,Single-Domain Datasets,"Single-domain text-to-SQL datasets typically collect question-SQL pairs for a single database in some real-world tasks, including early ones such as Academic (Li and Jagadish, 2014), Advising (Finegan-Dollak et al., 2018), ATIS (Price, 1990;Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), Yelp and IMDB (Yaghmazadeh et al., 2017), Scholar (Iyer et al., 2017) and Restaurants (Tang and Mooney, 2000;Popescu et al., 2003), as well as recent ones such as SEDE (Hazoom et al., 2021), ESQL (Chen et al., 2021a) and MIMICSQL (Wang et al., 2020d). These single-domain datasets, particularly the early ones, are usually limited in size, containing only a few hundred to a few thousand examples. Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models that are trained on these single-domain datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other unless otherwise specified.

domains (Finegan-Dollak et al., 2018;Yu et al., 2018c). However, since these datasets are adapted from real-life applications, most of them contain domain knowledge (Gan et al., 2021b) and dataset conventions (Suhr et al., 2020). Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.

Appendix B gives a detailed discussion on domain knowledge and dataset convention, and concrete text-to-SQL examples.

Large Scale Cross-domain Datasets Large cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) are proposed to better evaluate deep neural models. Wik-iSQL uses tables extracted from Wikipedia and lets annotators paraphrase questions generated for the tables. Compared to other datasets, WikiSQL is an order of magnitude larger, containing 80,654 natural utterances in total (Zhong et al., 2017). However, WikiSQL contains only simple SQL queries, and only a single table is queried within each SQL query (Yu et al., 2018c). Yu et al. (2018c) propose Spider, which contains 200 databases with an average of 5 tables for each database, to test models' performance on complicated unseen SQL queries and their ability to generalize to new domains. Furthermore, researchers expand Spider to study various issues of their inter-est (Lei et al., 2020;Zeng et al., 2020;Gan et al., 2021b;Taniguchi et al., 2021;Gan et al., 2021a).

Besides, researchers build several large-scale text-to-SQL datasets in different languages such as CSpider (Min et al., 2019a), TableQA (Sun et al., 2020), DuSQL (Wang et al., 2020c) in Chinese, ViText2SQL (Tuan Nguyen et al., 2020) in Vietnamese, and PortugueseSpider (José and Cozman, 2021) in Portuguese. Given that human translation has shown to be more accurate than machine translation (Min et al., 2019a), these datasets are annotated mainly by human experts based on the English Spider dataset. These Spider-based datasets can serve as potential resources for multi-lingual text-to-SQL research.

Other Datasets Several context-dependent textto-SQL datasets have been proposed, which involve user interactions with the text-to-SQL system in English (Price, 1990;Dahl et al., 1994;Yu et al., 2019a,b) and Chinese (Guo et al., 2021). In addition, researchers collect datasets to study questions in text-to-SQL being answerable or not (Zhang et al., 2020), lexicon-level mapping (Shi et al., 2020b) and cross-domain evaluation for real Web databases (Lee et al., 2021).

Appendix C.1 discusses more details about datasets mentioned in § 2.",Why are single-domain text-to-SQL datasets still considered valuable despite their limitations?,"Single-domain text-to-SQL datasets are usually limited in size, containing only a few hundred to a few thousand examples. Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models trained on these datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other domains. However, since these datasets are adapted from real-life applications, most of them contain domain knowledge and dataset conventions. Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.",Why do single-domain text-to-SQL datasets remain valuable despite their inherent limitations?,"Single-domain text-to-SQL datasets, although limited in size with only a few hundred to a few thousand examples, remain valuable for several reasons. These datasets often feature similar SQL patterns during both training and testing phases, which can lead models to achieve decent performance by memorizing these patterns rather than truly understanding them. This limitation can hinder a model's ability to generalize to unseen SQL queries or those from different domains. However, the value of these datasets lies in their adaptation from real-life applications, which means they often contain essential domain knowledge and adhere to specific dataset conventions. As a result, they provide a useful benchmark for evaluating a model's ability to generalize to new domains and offer insights into how domain knowledge and dataset conventions can be incorporated into model predictions.",7,4,8,4,"Question:

Why are single-domain text-to-SQL datasets still considered valuable despite their limitations?

Answer:

Single-domain text-to-SQL datasets are usually limited in size, containing only a few hundred to a few thousand examples. Because of the limited size and similar SQL patterns in training and testing phases, text-to-SQL models trained on these datasets can achieve decent performance by simply memorizing the SQL patterns and fail to generalize to unseen SQL queries or SQL queries from other domains. However, since these datasets are adapted from real-life applications, most of them contain domain knowledge and dataset conventions. Thus, they are still valuable to evaluate models' ability to generalize to new domains and explore how to incorporate domain knowledge and dataset convention to model predictions.","Question:

Why do single-domain text-to-SQL datasets remain valuable despite their inherent limitations?

Answer:

Single-domain text-to-SQL datasets, although limited in size with only a few hundred to a few thousand examples, remain valuable for several reasons. These datasets often feature similar SQL patterns during both training and testing phases, which can lead models to achieve decent performance by memorizing these patterns rather than truly understanding them. This limitation can hinder a model's ability to generalize to unseen SQL queries or those from different domains. However, the value of these datasets lies in their adaptation from real-life applications, which means they often contain essential domain knowledge and adhere to specific dataset conventions. As a result, they provide a useful benchmark for evaluating a model's ability to generalize to new domains and offer insights into how domain knowledge and dataset conventions can be incorporated into model predictions.",NO,True,886,True,True
251719280-s7,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,2022-08-22,Graph-based Methods,"Since DB schemas contain rich structural information, graph-based methods are used to better encode such structures. As summarized in § 2, datasets prior to Spider typically involve simple DBs that contain only one table or a single DB in both training and testing. As a result, modeling DB schema receives little attention. Because Spider contains complex and different DB in training and testing, Bogin et al. (2019a) propose to use graphs to represent the structure of the DB schemas. Specifically, Bogin et al. (2019a) use nodes to represent tables and columns, edges to represent relationships between tables and columns, such as tables containing columns, primary key, and foreign key constraints, and then use graph neural networks (GNNs) (Li et al., 2016) to encode the graph structure. In their subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL  encodes more relationships for DB schemas such as ""both columns are from the same table"" in their graph.

Graphs have also been used to encode questions together with DB schema. Researchers have been using different types of graphs to capture the semantics in NL and facilitate linking between NL and table schema. Cao et al. (2021) adopt line graph (Gross et al., 2018) to capture multi-hop semantics by meta-path (e.g., an exact match for a question token and column, together with the column belonging to a table can form a 2-hop meta-path) and distinguish between local and nonlocal neighbors so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. Apart from the relations between entities in both questions and DB schema, the structure for DB schemas, S 2 SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShawdowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas.

Finally, graph-based techniques are also exploited in context-dependent text-to-SQL. For instance, IGSQL (Cai and Wan, 2020) uses a graph encoder to utilize historical information of DB schemas in the previous turns.

Self-attention Models using transformer-based encoder (He et al., 2019;Xie et al., 2022) incorporate the original self-attention mechanism by default because it is the building block of the transformer structure.

RAT-SQL  applies relationaware self-attention, a modified version of selfattention (Vaswani et al., 2017), to leverage relations of tables and columns. DuoRAT (Scholak et al., 2021a) also adopts such a relation-aware self-attention in their encoder.

Adapt PLM Various methods have been proposed to leverage the knowledge in pre-trained language models (PLMs) and better align PLM with the text-to-SQL task. PLMs such as BERT (Devlin et al., 2019) are used to encode questions and DB schemas. The modus operandi is to input the concatenation of question words and schema words to the BERT encoder Choi et al., 2021). Other methods adjust the embeddings by PLMs. On WikiSQL, for instance, X-SQL (He et al., 2019) replaces segment embeddings from the pre-trained encoder by column type embeddings. Guo and Gao (2019) encode two additional feature vectors for matching between question tokens and table cells as well as column names and concatenate them with BERT embeddings of questions and DB schemas.

HydraNet  uses BERT to encode the question and an individual column, aligning with the tasks BERT is pre-trained on. After obtaining the BERT representations of all columns, Lyu et al. (2020) select top-ranked columns for SQL prediction. Liu et al. (2021b) train an auxiliary concept prediction module to predict which tables and columns correspond to the question. They detect important question tokens by detecting the largest drop in the confidence score caused by erasing that token in the question. Lastly, they train the PLM with a grounding module using the question tokens and the corresponding tables as well as columns. By empirical studies, Liu et al. (2021b) claim that their approach can awaken the latent grounding from PLM via this erase-andpredict technique.

Pre-training There have been various works proposing different pre-training objectives and using different pre-training data to better align the transformer-based encoder with the text-to-SQL task. For instance, TaBERT  uses tabular data for pre-training with objectives of masked column prediction and cell value recovery to pre-train BERT. Grappa  synthesizes question-SQL pairs over tables and pre-trains BERT with the objectives of masked language modeling (MLM) and predicting whether a column appears in the SQL query as well as what SQL operations are triggered. GAP (Shi et al., 2020a) pre-trains BART (Lewis et al., 2020) on synthesized text-to-SQL and tabular data with the objectives of MLM, column prediction, column recovery, and SQL generation.",How do graph-based methods enhance the representation and encoding of DB schema structures in text-to-SQL tasks?,"Graph-based methods are used to better encode the structures of DB schemas, which contain rich structural information. Bogin et al. (2019a) propose using graphs to represent the structure of DB schemas, with nodes representing tables and columns, and edges representing relationships such as tables containing columns, primary key, and foreign key constraints. Graph neural networks (GNNs) (Li et al., 2016) are then used to encode the graph structure. In subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL encodes more relationships for DB schemas, such as ""both columns are from the same table,"" in their graph. Graphs have also been used to encode questions together with DB schema, facilitating linking between natural language and table schema. Cao et al. (2021) adopt line graphs to capture multi-hop semantics by meta-path and distinguish between local and nonlocal neighbors, so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. S2SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShadowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas. Graph-based techniques are also exploited in context-dependent text-to-SQL, such as IGSQL (Cai and Wan, 2020), which uses a graph encoder to utilize historical information of DB schemas in previous turns.",How do graph-based methods improve the representation and encoding of database schema structures in text-to-SQL tasks?,"Graph-based methods significantly enhance the representation and encoding of database (DB) schema structures in text-to-SQL tasks by leveraging the rich structural information inherent in these schemas. Bogin et al. (2019a) introduced the concept of using graphs to represent DB schemas, where nodes symbolize tables and columns, and edges denote relationships such as tables containing columns, as well as primary key and foreign key constraints. To encode this graph structure, Graph Neural Networks (GNNs) (Li et al., 2016) are employed. In further research, Bogin et al. (2019b) utilized a Graph Convolutional Network (GCN) to capture DB structures and a gated GCN to select relevant DB information for SQL generation. RAT-SQL extends this approach by encoding additional relationships within DB schemas, such as ""both columns are from the same table,"" in their graph representation.

Moreover, graphs have been used to encode questions alongside DB schemas, facilitating the linking between natural language and table schema. For instance, Cao et al. (2021) adopted line graphs to capture multi-hop semantics through meta-paths and to distinguish between local and nonlocal neighbors, ensuring that different tables and columns receive appropriate attention. Similarly, SADGA (Cai et al., 2021) employs a graph structure to provide unified encoding for both natural utterances and DB schemas, aiding in question-schema linking. S2SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to enhance model performance.

To improve the generalization of graph methods for unseen domains, ShadowGNN (Chen et al., 2021b) disregards the names of tables or columns in the database and uses abstract schemas in the graph projection neural network to achieve delexicalized representations of questions and DB schemas. Graph-based techniques are also utilized in context-dependent text-to-SQL tasks. For example, IGSQL (Cai and Wan, 2020) employs a graph encoder to incorporate historical information of DB schemas from previous turns, thereby enhancing the model's ability to handle context-dependent queries.",4,4,7,4,"Question:

How do graph-based methods enhance the representation and encoding of DB schema structures in text-to-SQL tasks?

Answer:

Graph-based methods are used to better encode the structures of DB schemas, which contain rich structural information. Bogin et al. (2019a) propose using graphs to represent the structure of DB schemas, with nodes representing tables and columns, and edges representing relationships such as tables containing columns, primary key, and foreign key constraints. Graph neural networks (GNNs) (Li et al., 2016) are then used to encode the graph structure. In subsequent work, Bogin et al. (2019b) use a graph convolutional network (GCN) to capture DB structures and a gated GCN to select the relevant DB information for SQL generation. RAT-SQL encodes more relationships for DB schemas, such as ""both columns are from the same table,"" in their graph. Graphs have also been used to encode questions together with DB schema, facilitating linking between natural language and table schema. Cao et al. (2021) adopt line graphs to capture multi-hop semantics by meta-path and distinguish between local and nonlocal neighbors, so that different tables and columns will be attended differently. SADGA (Cai et al., 2021) adopts the graph structure to provide a unified encoding for both natural utterances and DB schemas to help question-schema linking. S2SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to improve model performance. To improve the generalization of the graph method for unseen domains, ShadowGNN (Chen et al., 2021b) ignores names of tables or columns in the database and uses abstract schemas in the graph projection neural network to obtain delexicalized representations of questions and DB schemas. Graph-based techniques are also exploited in context-dependent text-to-SQL, such as IGSQL (Cai and Wan, 2020), which uses a graph encoder to utilize historical information of DB schemas in previous turns.","Question:

How do graph-based methods improve the representation and encoding of database schema structures in text-to-SQL tasks?

Answer:

Graph-based methods significantly enhance the representation and encoding of database (DB) schema structures in text-to-SQL tasks by leveraging the rich structural information inherent in these schemas. Bogin et al. (2019a) introduced the concept of using graphs to represent DB schemas, where nodes symbolize tables and columns, and edges denote relationships such as tables containing columns, as well as primary key and foreign key constraints. To encode this graph structure, Graph Neural Networks (GNNs) (Li et al., 2016) are employed. In further research, Bogin et al. (2019b) utilized a Graph Convolutional Network (GCN) to capture DB structures and a gated GCN to select relevant DB information for SQL generation. RAT-SQL extends this approach by encoding additional relationships within DB schemas, such as ""both columns are from the same table,"" in their graph representation.

Moreover, graphs have been used to encode questions alongside DB schemas, facilitating the linking between natural language and table schema. For instance, Cao et al. (2021) adopted line graphs to capture multi-hop semantics through meta-paths and to distinguish between local and nonlocal neighbors, ensuring that different tables and columns receive appropriate attention. Similarly, SADGA (Cai et al., 2021) employs a graph structure to provide unified encoding for both natural utterances and DB schemas, aiding in question-schema linking. S2SQL (Hui et al., 2022) integrates syntax dependency among question tokens into the graph to enhance model performance.

To improve the generalization of graph methods for unseen domains, ShadowGNN (Chen et al., 2021b) disregards the names of tables or columns in the database and uses abstract schemas in the graph projection neural network to achieve delexicalized representations of questions and DB schemas. Graph-based techniques are also utilized in context-dependent text-to-SQL tasks. For example, IGSQL (Cai and Wan, 2020) employs a graph encoder to incorporate historical information of DB schemas from previous turns, thereby enhancing the model's ability to handle context-dependent queries.",NO,True,2137,True,True
251719280-s8,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,2022-08-22,Decoding,"Various methods have been proposed for decoding to achieve a fine-grained and easier process for SQL generation and bridge the gap between natural language and SQL queries. As shown in Table 3 Table 9 in Appendix D. IR: Intermediate Representation. and other technologies.

Tree-based Seq2Tree (Dong and Lapata, 2016) employs a decoder that generates logical forms in a top-down manner. The components in the sub-tree are generated conditioned on their parents apart from the input question. Note that the syntax of the logical forms is implicitly learned from data for Seq2Tree. Similarly, Seq2AST (Yin and Neubig, 2017) uses an abstract syntax tree (AST) for decoding the target programming language, where the syntax is explicitly integrated with AST. Although both Seq2Tree (Dong and Lapata, 2016) and Seq2AST (Yin and Neubig, 2017) do not study text-to-SQL datasets, their uses of trees inspire tree-based decoding in text-to-SQL. SyntaxSQL-Net (Yu et al., 2018b) employs a tree-based decoding method specific to SQL syntax and recursively calls modules to predict different SQL components.

Sketch-based SQLNet (Xu et al., 2017) designs a sketch aligned with the SQL grammar, and SQL-Net only needs to fill in the slots in the sketch rather than predict both the output grammar and the content. Besides, the sketch captures the dependency of the predictions. Thus, the prediction of one slot is only conditioned on the slots it depends on, which avoids issues of the same SQL query with varied equivalent serializations. Dong and Lapata (2018) decompose the decoding into two stages, where the first decoder predicts a rough sketch, and the second decoder fills in the lowlevel details conditioned on the question and the sketch. Such coarse-to-fine decoding has also been adopted in other works such as IRNet (Guo et al., 2019). To address the complex SQL queries with nested structures, RYANSQL (Choi et al., 2021) recursively yields SELECT statements and uses a sketch-based slot filling for each of the SELECT statements.

Bottom-up Both the tree-based and the sketchbased decoding mechanisms can be viewed as top-down decoding mechanisms. Rubin and Berant (2021) use a bottom-up decoding mechanism. Given K trees of height t, the decoder scores trees with height t + 1 constructed by SQL grammar from the current beam, and K trees with the highest scores are kept. Then, a representation of the new K trees is generated and placed in the new beam.

Attention Mechanism To integrate the encoderside information at decoding, an attention score is computed and multiplied with hidden vectors from the encoder to get the context vector, which is then used to generate an output token (Dong and Lapata, 2016;Zhong et al., 2017). Variants of the attention mechanism have been used to better propagate the information encoded from questions and DB schemas to the decoder. SQLNet (Xu et al., 2017) designs column attention, where it uses hidden states from columns multiplied by embeddings for the question to calculate attention scores for a column given the question. Guo and Gao (2018)  Copy Mechanism Seq2AST (Yin and Neubig, 2017) and Seq2SQL (Zhong et al., 2017) employ the pointer network (Vinyals et al., 2015) to compute the probability of copying words from the input. Wang et al. (2018a) use types (e.g., columns, SQL operators, constant from questions) to explicitly restrict locations in the query to copy from and develop a new training objective to only copy from the first occurrence in the input. In addition, the copy mechanism is also adopted in context-dependent text-to-SQL task (Wang et al., 2020b).

Intermediate Representations Researchers use intermediate representations to bridge the gap between natural language and SQL queries. Inc-SQL (Shi et al., 2018)  However, the intermediate representations are usually designed for a specific dataset and cannot be easily adapted to others (Suhr et al., 2020). To construct a more generalized intermediate representation, Herzig et al. (2021) propose to omit tokens in the SQL query that do not align to any phrase in the utterance.

Inspired by the success of text-to-SQL task, intermediate representations are also studied for SPARQL, another executable language for database systems (Saparina and Osokin, 2021;Herzig et al., 2021).

Others PICARD (Scholak et al., 2021b) and UniSAr (Dou et al., 2022) set constraints to the decoder to prevent generating invalid tokens. Several methods adopt an execution-guided decoding mechanism to exclude non-executable partial SQL queries from the output candidates (Wang et al., 2018b;. Global-GNN (Bogin et al., 2019b) employs a separately trained discriminative model to rerank the top-K SQL queries in the decoder's output beam, which is to reason about the complete SQL queries instead of considering each word and DB schemas in isolation.  (2018); Lee (2019) use separate submodules to predict different SQL components, easing the difficulty of generating a complete SQL query. Chen et al. (2020b) employ a gate to select between the output sequence encoded for the question and the output sequence from the previous decoding steps at each step for SQL generation. Inspired by machine translation, Müller and Vlachos (2019) apply byte-pair encoding (BPE) (Sennrich et al., 2016) to compress SQL queries to shorter sequences guided by AST, reducing the difficulties in SQL generation.",How do various decoding methods improve the process of SQL generation in text-to-SQL tasks?,"Various methods have been proposed for decoding to achieve a fine-grained and easier process for SQL generation and bridge the gap between natural language and SQL queries. Tree-based Seq2Tree (Dong and Lapata, 2016) employs a decoder that generates logical forms in a top-down manner, where components in the sub-tree are generated conditioned on their parents apart from the input question. Seq2AST (Yin and Neubig, 2017) uses an abstract syntax tree (AST) for decoding the target programming language, where the syntax is explicitly integrated with AST. SyntaxSQL-Net (Yu et al., 2018b) employs a tree-based decoding method specific to SQL syntax and recursively calls modules to predict different SQL components. Sketch-based SQLNet (Xu et al., 2017) designs a sketch aligned with the SQL grammar, and SQL-Net only needs to fill in the slots in the sketch rather than predict both the output grammar and the content. Dong and Lapata (2018) decompose the decoding into two stages, where the first decoder predicts a rough sketch, and the second decoder fills in the low-level details conditioned on the question and the sketch. RYANSQL (Choi et al., 2021) recursively yields SELECT statements and uses a sketch-based slot filling for each of the SELECT statements. Rubin and Berant (2021) use a bottom-up decoding mechanism, where the decoder scores trees with height t + 1 constructed by SQL grammar from the current beam, and K trees with the highest scores are kept. Attention mechanisms are used to integrate encoder-side information at decoding, where an attention score is computed and multiplied with hidden vectors from the encoder to get the context vector, which is then used to generate an output token (Dong and Lapata, 2016; Zhong et al., 2017). SQLNet (Xu et al., 2017) designs column attention, where it uses hidden states from columns multiplied by embeddings for the question to calculate attention scores for a column given the question. Seq2AST (Yin and Neubig, 2017) and Seq2SQL (Zhong et al., 2017) employ the pointer network to compute the probability of copying words from the input. Intermediate representations are used to bridge the gap between natural language and SQL queries, although they are usually designed for a specific dataset and cannot be easily adapted to others (Suhr et al., 2020). Herzig et al. (2021) propose to omit tokens in the SQL query that do not align to any phrase in the utterance to construct a more generalized intermediate representation. Execution-guided decoding mechanisms are adopted to exclude non-executable partial SQL queries from the output candidates (Wang et al., 2018b). Global-GNN (Bogin et al., 2019b) employs a separately trained discriminative model to rerank the top-K SQL queries in the decoder's output beam, reasoning about the complete SQL queries instead of considering each word and DB schemas in isolation.",How do different decoding methods enhance SQL generation in text-to-SQL tasks?,"Decoding methods play a crucial role in improving SQL generation by bridging the gap between natural language and SQL queries. Various approaches have been developed to make this process more efficient and accurate. For instance, the Tree-based Seq2Tree model (Dong and Lapata, 2016) uses a top-down approach where logical forms are generated with sub-tree components conditioned on their parent nodes and the input question. Similarly, Seq2AST (Yin and Neubig, 2017) employs an abstract syntax tree (AST) to explicitly integrate syntax into the decoding process.

SyntaxSQL-Net (Yu et al., 2018b) takes a tree-based approach specific to SQL syntax, using recursive modules to predict different SQL components. Sketch-based SQLNet (Xu et al., 2017) simplifies the task by aligning with SQL grammar, requiring only the filling of slots in a predefined sketch. Dong and Lapata (2018) further refine this by decomposing decoding into two stages: first predicting a rough sketch, then filling in details based on the question and sketch.

RYANSQL (Choi et al., 2021) uses a recursive method to generate SELECT statements, employing sketch-based slot filling for each statement. Rubin and Berant (2021) introduce a bottom-up decoding mechanism, scoring trees constructed by SQL grammar and retaining the highest-scoring ones. Attention mechanisms are also utilized to incorporate encoder-side information during decoding, as seen in works by Dong and Lapata (2016) and Zhong et al. (2017).

SQLNet (Xu et al., 2017) introduces column attention, calculating attention scores for columns based on the question. Seq2AST (Yin and Neubig, 2017) and Seq2SQL (Zhong et al., 2017) use pointer networks to compute the probability of copying words from the input. Intermediate representations, although often dataset-specific, help bridge the gap between natural language and SQL (Suhr et al., 2020). Herzig et al. (2021) propose omitting non-aligned tokens to create a more generalized intermediate representation.

Execution-guided decoding mechanisms, such as those by Wang et al. (2018b), exclude non-executable partial SQL queries from output candidates. Global-GNN (Bogin et al., 2019b) employs a discriminative model to rerank top-K SQL queries, considering complete SQL queries and database schemas holistically rather than in isolation. These diverse methods collectively enhance the accuracy and efficiency of SQL generation in text-to-SQL tasks.",7,4,7,4,"Question:

How do various decoding methods improve the process of SQL generation in text-to-SQL tasks?

Answer:

Various methods have been proposed for decoding to achieve a fine-grained and easier process for SQL generation and bridge the gap between natural language and SQL queries. Tree-based Seq2Tree (Dong and Lapata, 2016) employs a decoder that generates logical forms in a top-down manner, where components in the sub-tree are generated conditioned on their parents apart from the input question. Seq2AST (Yin and Neubig, 2017) uses an abstract syntax tree (AST) for decoding the target programming language, where the syntax is explicitly integrated with AST. SyntaxSQL-Net (Yu et al., 2018b) employs a tree-based decoding method specific to SQL syntax and recursively calls modules to predict different SQL components. Sketch-based SQLNet (Xu et al., 2017) designs a sketch aligned with the SQL grammar, and SQL-Net only needs to fill in the slots in the sketch rather than predict both the output grammar and the content. Dong and Lapata (2018) decompose the decoding into two stages, where the first decoder predicts a rough sketch, and the second decoder fills in the low-level details conditioned on the question and the sketch. RYANSQL (Choi et al., 2021) recursively yields SELECT statements and uses a sketch-based slot filling for each of the SELECT statements. Rubin and Berant (2021) use a bottom-up decoding mechanism, where the decoder scores trees with height t + 1 constructed by SQL grammar from the current beam, and K trees with the highest scores are kept. Attention mechanisms are used to integrate encoder-side information at decoding, where an attention score is computed and multiplied with hidden vectors from the encoder to get the context vector, which is then used to generate an output token (Dong and Lapata, 2016; Zhong et al., 2017). SQLNet (Xu et al., 2017) designs column attention, where it uses hidden states from columns multiplied by embeddings for the question to calculate attention scores for a column given the question. Seq2AST (Yin and Neubig, 2017) and Seq2SQL (Zhong et al., 2017) employ the pointer network to compute the probability of copying words from the input. Intermediate representations are used to bridge the gap between natural language and SQL queries, although they are usually designed for a specific dataset and cannot be easily adapted to others (Suhr et al., 2020). Herzig et al. (2021) propose to omit tokens in the SQL query that do not align to any phrase in the utterance to construct a more generalized intermediate representation. Execution-guided decoding mechanisms are adopted to exclude non-executable partial SQL queries from the output candidates (Wang et al., 2018b). Global-GNN (Bogin et al., 2019b) employs a separately trained discriminative model to rerank the top-K SQL queries in the decoder's output beam, reasoning about the complete SQL queries instead of considering each word and DB schemas in isolation.","Question:

How do different decoding methods enhance SQL generation in text-to-SQL tasks?

Answer:

Decoding methods play a crucial role in improving SQL generation by bridging the gap between natural language and SQL queries. Various approaches have been developed to make this process more efficient and accurate. For instance, the Tree-based Seq2Tree model (Dong and Lapata, 2016) uses a top-down approach where logical forms are generated with sub-tree components conditioned on their parent nodes and the input question. Similarly, Seq2AST (Yin and Neubig, 2017) employs an abstract syntax tree (AST) to explicitly integrate syntax into the decoding process.

SyntaxSQL-Net (Yu et al., 2018b) takes a tree-based approach specific to SQL syntax, using recursive modules to predict different SQL components. Sketch-based SQLNet (Xu et al., 2017) simplifies the task by aligning with SQL grammar, requiring only the filling of slots in a predefined sketch. Dong and Lapata (2018) further refine this by decomposing decoding into two stages: first predicting a rough sketch, then filling in details based on the question and sketch.

RYANSQL (Choi et al., 2021) uses a recursive method to generate SELECT statements, employing sketch-based slot filling for each statement. Rubin and Berant (2021) introduce a bottom-up decoding mechanism, scoring trees constructed by SQL grammar and retaining the highest-scoring ones. Attention mechanisms are also utilized to incorporate encoder-side information during decoding, as seen in works by Dong and Lapata (2016) and Zhong et al. (2017).

SQLNet (Xu et al., 2017) introduces column attention, calculating attention scores for columns based on the question. Seq2AST (Yin and Neubig, 2017) and Seq2SQL (Zhong et al., 2017) use pointer networks to compute the probability of copying words from the input. Intermediate representations, although often dataset-specific, help bridge the gap between natural language and SQL (Suhr et al., 2020). Herzig et al. (2021) propose omitting non-aligned tokens to create a more generalized intermediate representation.

Execution-guided decoding mechanisms, such as those by Wang et al. (2018b), exclude non-executable partial SQL queries from output candidates. Global-GNN (Bogin et al., 2019b) employs a discriminative model to rerank top-K SQL queries, considering complete SQL queries and database schemas holistically rather than in isolation. These diverse methods collectively enhance the accuracy and efficiency of SQL generation in text-to-SQL tasks.",NO,True,2441,True,True
251719280-s9,Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect,2022-08-22,Learning Techniques,"Apart from end-to-end supervised learning, different learning techniques have been proposed to help text-to-SQL research. Here we summarize these learning techniques, each addressing a specific issue for the task. In SQL generation, IncSQL (Shi et al., 2018) allows parsers to explore alternative correct action sequences to generate different SQL queries. Brunner and Stockinger (2021) search values in DB to insert values into SQL query.

For context-dependent text-to-SQL, researchers adopt techniques such as turn-level encoder and copy mechanism (Suhr et al., 2018;Zhang et al., 2019;Wang et al., 2020b), constrained decoding (Wang et al., 2020b), dynamic memory decay mechanism (Hui et al., 2021a), treating questions and SQL queries as two modalities, and using bimodal pre-trained models (Zheng et al., 2022).",What techniques are used to address context-dependency in text-to-SQL tasks?,"For context-dependent text-to-SQL, researchers adopt techniques such as turn-level encoder and copy mechanism (Suhr et al., 2018; Zhang et al., 2019; Wang et al., 2020b). Constrained decoding is another technique used (Wang et al., 2020b). A dynamic memory decay mechanism is also employed (Hui et al., 2021a). Additionally, treating questions and SQL queries as two modalities and using bimodal pre-trained models are techniques used to address context-dependency (Zheng et al., 2022).",What are some techniques used to handle context-dependency in text-to-SQL tasks?,"To address context-dependency in text-to-SQL tasks, researchers have developed several techniques. One approach involves using a turn-level encoder combined with a copy mechanism, as demonstrated by Suhr et al. (2018), Zhang et al. (2019), and Wang et al. (2020b). Another technique is constrained decoding, which is also highlighted by Wang et al. (2020b). Additionally, a dynamic memory decay mechanism is employed, as discussed by Hui et al. (2021a). Furthermore, treating questions and SQL queries as two distinct modalities and utilizing bimodal pre-trained models is another effective strategy, as explored by Zheng et al. (2022).",8,4,7,4,"Question:

What techniques are used to address context-dependency in text-to-SQL tasks?

Answer:

For context-dependent text-to-SQL, researchers adopt techniques such as turn-level encoder and copy mechanism (Suhr et al., 2018; Zhang et al., 2019; Wang et al., 2020b). Constrained decoding is another technique used (Wang et al., 2020b). A dynamic memory decay mechanism is also employed (Hui et al., 2021a). Additionally, treating questions and SQL queries as two modalities and using bimodal pre-trained models are techniques used to address context-dependency (Zheng et al., 2022).","Question:

What are some techniques used to handle context-dependency in text-to-SQL tasks?

Answer:

To address context-dependency in text-to-SQL tasks, researchers have developed several techniques. One approach involves using a turn-level encoder combined with a copy mechanism, as demonstrated by Suhr et al. (2018), Zhang et al. (2019), and Wang et al. (2020b). Another technique is constrained decoding, which is also highlighted by Wang et al. (2020b). Additionally, a dynamic memory decay mechanism is employed, as discussed by Hui et al. (2021a). Furthermore, treating questions and SQL queries as two distinct modalities and utilizing bimodal pre-trained models is another effective strategy, as explored by Zheng et al. (2022).",NO,True,636,True,True
252571112-s3,SHAP-Based Explanation Methods: A Review for NLP Interpretability,2022,Shapley Values Approximation and SHAP,"The idea of utilizing Shapley values to compute feature attribution scores precedes the SHAP framework (Lipovetsky and Conklin, 2001;Song et al., 2016). In this case, the outcome val of the game is the prediction of a machine learning model f and Shapley values ϕ f (i) measure the influence that each feature i has based on its current value. The early literature also worked on approximation strategies, as the exponential number of coalitions renders the exact estimation of Shapley values unfeasible (Štrumbelj and Kononenko, 2014;Datta et al., 2016). The main idea from these works is to compute ϕ f (i) only for a smaller selection of subsets S ⊆ F and to estimate the effect of removing a feature by integrating over training samples. This eliminates the need to retrain the model for each choice of S.

The work from Lundberg and Lee (2017) introduces a new perspective that unifies Shapley value estimation with popular explainability methods such as LIME (Ribeiro et al., 2016), LRP (Binder et al., 2016), and DeepLIFT (Shrikumar et al., 2017). Furthermore, they propose SHAP values as a unified measure of feature importance and prove them to be the unique solution respecting the criteria of local accuracy, missingness, and consistency. The authors contribute a library of methods to efficiently approximate SHAP values in a variety of settings:

KernelSHAP: Adaptation of LIME-hence model-agnostic-to approximate SHAP values. As it works for any model f , it cannot make any assumption on its structure and is thus the slowest within the framework.

LinearSHAP: Specific to linear models, uses the model's weight coefficients and optionally accounts for inter-feature correlations.

DeepSHAP: Adaptation of DeepLIFT-hence specific to neural networks-to approximate SHAP values. Considerably faster than its model-agnostic counterpart as it makes assumptions about the model's compositional nature.

While not initially presented in Lundberg and Lee (2017), the following algorithms were later  GradientSHAP: An extension of the Integrated Gradients (IG) method (Sundararajan et al., 2017)again specific to neural networks-that aggregates gradients over the difference between the expected model output and the current output.

TreeSHAP: A fast method for computing exact SHAP values for both trees and ensembles (Lundberg et al., 2020a). In comparison to KernelSHAP, it also accounts for interactions among features.

Other minor approaches-PermutationSHAP, SamplingSHAP, ExactSHAP, and MimicSHAPare also available in the official library 1 . To avoid confusion, we point out that the implementations have slightly different names: they use ""Explainer"" instead of ""SHAP"". For instance, KernelSHAP and DeepSHAP are implemented with the names of KernelExplainer and DeepExplainer respectively. Figure 2 sketches an explanation generated with SHAP.","What methods and algorithms are available for efficiently approximating SHAP values, and how do they differ?","The work from Lundberg and Lee (2017) introduces a new perspective that unifies Shapley value estimation with popular explainability methods such as LIME (Ribeiro et al., 2016), LRP (Binder et al., 2016), and DeepLIFT (Shrikumar et al., 2017). They propose SHAP values as a unified measure of feature importance and prove them to be the unique solution respecting the criteria of local accuracy, missingness, and consistency. The authors contribute a library of methods to efficiently approximate SHAP values in a variety of settings:  KernelSHAP: Adaptation of LIME—hence model-agnostic—to approximate SHAP values. As it works for any model \( f \), it cannot make any assumption on its structure and is thus the slowest within the framework. LinearSHAP: Specific to linear models, uses the model's weight coefficients and optionally accounts for inter-feature correlations.  DeepSHAP: Adaptation of DeepLIFT—hence specific to neural networks—to approximate SHAP values. Considerably faster than its model-agnostic counterpart as it makes assumptions about the model's compositional nature. While not initially presented in Lundberg and Lee (2017), the following algorithms were later introduced: GradientSHAP: An extension of the Integrated Gradients (IG) method (Sundararajan et al., 2017)—again specific to neural networks—that aggregates gradients over the difference between the expected model output and the current output. TreeSHAP: A fast method for computing exact SHAP values for both trees and ensembles (Lundberg et al., 2020a). In comparison to KernelSHAP, it also accounts for interactions among features. Other minor approaches—PermutationSHAP, SamplingSHAP, ExactSHAP, and MimicSHAP—are also available in the official library. To avoid confusion, the implementations have slightly different names: they use ""Explainer"" instead of ""SHAP"". For instance, KernelSHAP and DeepSHAP are implemented with the names of KernelExplainer and DeepExplainer respectively.","What are the available methods and algorithms for efficiently approximating SHAP values, and how do they differ from each other?","Lundberg and Lee (2017) introduced a novel approach that unifies Shapley value estimation with popular explainability methods such as LIME (Ribeiro et al., 2016), LRP (Binder et al., 2016), and DeepLIFT (Shrikumar et al., 2017). They proposed SHAP values as a unified measure of feature importance, proving them to be the unique solution that respects the criteria of local accuracy, missingness, and consistency. To efficiently approximate SHAP values in various settings, they contributed a library of methods:

1. **KernelSHAP**: This is an adaptation of LIME, making it model-agnostic and capable of approximating SHAP values for any model \( f \). However, because it cannot make assumptions about the model's structure, it is the slowest method within the framework.

2. **LinearSHAP**: Specific to linear models, this method uses the model's weight coefficients and can optionally account for inter-feature correlations.

3. **DeepSHAP**: An adaptation of DeepLIFT, this method is specific to neural networks and is considerably faster than its model-agnostic counterpart due to assumptions about the model's compositional nature.

While not initially presented in Lundberg and Lee (2017), the following algorithms were later introduced:

4. **GradientSHAP**: An extension of the Integrated Gradients (IG) method (Sundararajan et al., 2017), this method is also specific to neural networks and aggregates gradients over the difference between the expected model output and the current output.

5. **TreeSHAP**: A fast method for computing exact SHAP values for both trees and ensembles (Lundberg et al., 2020a). Unlike KernelSHAP, it accounts for interactions among features.

Other minor approaches, such as PermutationSHAP, SamplingSHAP, ExactSHAP, and MimicSHAP, are also available in the official library. To avoid confusion, the implementations have slightly different names, using ""Explainer"" instead of ""SHAP"". For instance, KernelSHAP and DeepSHAP are implemented as KernelExplainer and DeepExplainer, respectively.",7,4,7,4,"Question:

What methods and algorithms are available for efficiently approximating SHAP values, and how do they differ?

Answer:

The work from Lundberg and Lee (2017) introduces a new perspective that unifies Shapley value estimation with popular explainability methods such as LIME (Ribeiro et al., 2016), LRP (Binder et al., 2016), and DeepLIFT (Shrikumar et al., 2017). They propose SHAP values as a unified measure of feature importance and prove them to be the unique solution respecting the criteria of local accuracy, missingness, and consistency. The authors contribute a library of methods to efficiently approximate SHAP values in a variety of settings:  KernelSHAP: Adaptation of LIME—hence model-agnostic—to approximate SHAP values. As it works for any model \( f \), it cannot make any assumption on its structure and is thus the slowest within the framework. LinearSHAP: Specific to linear models, uses the model's weight coefficients and optionally accounts for inter-feature correlations.  DeepSHAP: Adaptation of DeepLIFT—hence specific to neural networks—to approximate SHAP values. Considerably faster than its model-agnostic counterpart as it makes assumptions about the model's compositional nature. While not initially presented in Lundberg and Lee (2017), the following algorithms were later introduced: GradientSHAP: An extension of the Integrated Gradients (IG) method (Sundararajan et al., 2017)—again specific to neural networks—that aggregates gradients over the difference between the expected model output and the current output. TreeSHAP: A fast method for computing exact SHAP values for both trees and ensembles (Lundberg et al., 2020a). In comparison to KernelSHAP, it also accounts for interactions among features. Other minor approaches—PermutationSHAP, SamplingSHAP, ExactSHAP, and MimicSHAP—are also available in the official library. To avoid confusion, the implementations have slightly different names: they use ""Explainer"" instead of ""SHAP"". For instance, KernelSHAP and DeepSHAP are implemented with the names of KernelExplainer and DeepExplainer respectively.","Question:

What are the available methods and algorithms for efficiently approximating SHAP values, and how do they differ from each other?

Answer:

Lundberg and Lee (2017) introduced a novel approach that unifies Shapley value estimation with popular explainability methods such as LIME (Ribeiro et al., 2016), LRP (Binder et al., 2016), and DeepLIFT (Shrikumar et al., 2017). They proposed SHAP values as a unified measure of feature importance, proving them to be the unique solution that respects the criteria of local accuracy, missingness, and consistency. To efficiently approximate SHAP values in various settings, they contributed a library of methods:

1. **KernelSHAP**: This is an adaptation of LIME, making it model-agnostic and capable of approximating SHAP values for any model \( f \). However, because it cannot make assumptions about the model's structure, it is the slowest method within the framework.

2. **LinearSHAP**: Specific to linear models, this method uses the model's weight coefficients and can optionally account for inter-feature correlations.

3. **DeepSHAP**: An adaptation of DeepLIFT, this method is specific to neural networks and is considerably faster than its model-agnostic counterpart due to assumptions about the model's compositional nature.

While not initially presented in Lundberg and Lee (2017), the following algorithms were later introduced:

4. **GradientSHAP**: An extension of the Integrated Gradients (IG) method (Sundararajan et al., 2017), this method is also specific to neural networks and aggregates gradients over the difference between the expected model output and the current output.

5. **TreeSHAP**: A fast method for computing exact SHAP values for both trees and ensembles (Lundberg et al., 2020a). Unlike KernelSHAP, it accounts for interactions among features.

Other minor approaches, such as PermutationSHAP, SamplingSHAP, ExactSHAP, and MimicSHAP, are also available in the official library. To avoid confusion, the implementations have slightly different names, using ""Explainer"" instead of ""SHAP"". For instance, KernelSHAP and DeepSHAP are implemented as KernelExplainer and DeepExplainer, respectively.",NO,True,2030,True,True
252571112-s16,SHAP-Based Explanation Methods: A Review for NLP Interpretability,2022,Criticisms,"The usage of Shapley values for generating model explanations has also been criticized. For instance, Kumar et al. (2020) shows that using Shapley values for feature importance leads to mathematical inconsistencies which can only be mitigated by introducing further complexity like causality assumptions. Moreover, the authors argue that Shapley values do not represent an intuitive solution to the human-centric goals of model explanations and thus are only suitable in a limited range of settings. Sundararajan and Najmi (2020), on the other hand, criticize some Shapley-value-based methods. In fact, while a strong case for utilizing Shapley values can be made thanks to their uniqueness result in satisfying certain properties (see 2.1), often methods employing them operate under different assumptions and hence the uniqueness results loses validity in their context. Merrick and Taly (2020) argues that existing SHAP-based literature focuses on the axiomatic foundation of Shapley values and their efficient estimation but neglects the uncertainty of the explanations produced. The authors illustrate how small differences in the underlying game formulation can lead to sudden leaps in Shapley values and can attribute a positive contribution to features that do not play any role in the machine learning model.",What are the main criticisms of using Shapley values for feature importance in model explanations?,"The usage of Shapley values for generating model explanations has been criticized for several reasons. Kumar et al. (2020) shows that using Shapley values for feature importance leads to mathematical inconsistencies which can only be mitigated by introducing further complexity like causality assumptions. Moreover, the authors argue that Shapley values do not represent an intuitive solution to the human-centric goals of model explanations and thus are only suitable in a limited range of settings. Sundararajan and Najmi (2020) criticize some Shapley-value-based methods, noting that while a strong case for utilizing Shapley values can be made thanks to their uniqueness result in satisfying certain properties, often methods employing them operate under different assumptions and hence the uniqueness results lose validity in their context. Merrick and Taly (2020) argue that existing SHAP-based literature focuses on the axiomatic foundation of Shapley values and their efficient estimation but neglects the uncertainty of the explanations produced. The authors illustrate how small differences in the underlying game formulation can lead to sudden leaps in Shapley values and can attribute a positive contribution to features that do not play any role in the machine learning model.",What are the main criticisms of using Shapley values for feature importance in model explanations?,"The use of Shapley values for feature importance in model explanations has faced several criticisms. Kumar et al. (2020) highlight that Shapley values can lead to mathematical inconsistencies unless additional complexities, such as causality assumptions, are introduced. They also argue that Shapley values may not align well with the human-centric goals of model explanations, making them suitable only in limited contexts. Sundararajan and Najmi (2020) point out that while Shapley values have a strong theoretical foundation due to their uniqueness in satisfying certain properties, many methods that employ them operate under different assumptions, which can undermine the validity of these uniqueness results. Merrick and Taly (2020) emphasize that the existing SHAP-based literature often focuses on the axiomatic foundation and efficient estimation of Shapley values but overlooks the uncertainty in the explanations they produce. They demonstrate that small changes in the underlying game formulation can cause significant shifts in Shapley values, potentially attributing positive contributions to features that do not influence the machine learning model.",8,4,7,4,"Question:

What are the main criticisms of using Shapley values for feature importance in model explanations?

Answer:

The usage of Shapley values for generating model explanations has been criticized for several reasons. Kumar et al. (2020) shows that using Shapley values for feature importance leads to mathematical inconsistencies which can only be mitigated by introducing further complexity like causality assumptions. Moreover, the authors argue that Shapley values do not represent an intuitive solution to the human-centric goals of model explanations and thus are only suitable in a limited range of settings. Sundararajan and Najmi (2020) criticize some Shapley-value-based methods, noting that while a strong case for utilizing Shapley values can be made thanks to their uniqueness result in satisfying certain properties, often methods employing them operate under different assumptions and hence the uniqueness results lose validity in their context. Merrick and Taly (2020) argue that existing SHAP-based literature focuses on the axiomatic foundation of Shapley values and their efficient estimation but neglects the uncertainty of the explanations produced. The authors illustrate how small differences in the underlying game formulation can lead to sudden leaps in Shapley values and can attribute a positive contribution to features that do not play any role in the machine learning model.","Question:

What are the main criticisms of using Shapley values for feature importance in model explanations?

Answer:

The use of Shapley values for feature importance in model explanations has faced several criticisms. Kumar et al. (2020) highlight that Shapley values can lead to mathematical inconsistencies unless additional complexities, such as causality assumptions, are introduced. They also argue that Shapley values may not align well with the human-centric goals of model explanations, making them suitable only in limited contexts. Sundararajan and Najmi (2020) point out that while Shapley values have a strong theoretical foundation due to their uniqueness in satisfying certain properties, many methods that employ them operate under different assumptions, which can undermine the validity of these uniqueness results. Merrick and Taly (2020) emphasize that the existing SHAP-based literature often focuses on the axiomatic foundation and efficient estimation of Shapley values but overlooks the uncertainty in the explanations they produce. They demonstrate that small changes in the underlying game formulation can cause significant shifts in Shapley values, potentially attributing positive contributions to features that do not influence the machine learning model.",NO,True,1165,True,True
9840922-s10,Word Meaning Inducing via Character Ontology: A Survey on the Semantic Prediction of Chinese Two-Character Words,2005,"Circumventing ""Head-oriented"" Presupposition","As remarked in Chen (2004), the previous research concerning the automatic semantic classification of Chinese compounds (Lua 1997;Chen and Chen 2000) presupposes the endocentric feature of compounds. That is, by supposing that compounds are composed of a head and a modifier, determining the semantic category of the target therefore boils down to determine the semantic category of the head compound. In order to circumventing the strict ""headdetermination"" presumption, which might suffer problems in some borderline cases of V-V compounds, the weight value (β and 1 − β) is proposed. The idea of weighting comes from the discussion of morphological productivity in Baayen (2001). We presume that, within a given two-character words, the more productive, that is, the more numbers of characters a character can combine with, the more possible it is a head, and the more weight should be given to it. The weight is defined as β = C(n,1) N , viz, the number of candidate morphemic components divided by the total number of N. For instance, in the above-mentioned example, N S 1 should gain more weights than N S 2 , forˆcan combine with more characters (5 near-synonyms candidates) in

N S 1 than \ does in N S 2 (3 near-synonyms candidates). In this case, β = 5 8 = 0.625. It is noted that the weight assignment should be character and position independent.",How is the weight value for characters in Chinese two-character words determined and calculated?,"The weight value for characters in Chinese two-character words is determined by the productivity of the characters, which refers to the number of other characters a given character can combine with. The more productive a character is, the more likely it is to be a head, and thus, more weight should be assigned to it. The weight is calculated as β = C(n,1) / N, where C(n,1) is the number of candidate morphemic components, and N is the total number of components. For example, if a character can combine with more characters (e.g., 5 near-synonyms candidates) compared to another character (e.g., 3 near-synonyms candidates), it should receive more weight. In this case, the weight β is calculated as 5/8 = 0.625. The weight assignment is character and position independent.",How is the weight value for characters in Chinese two-character words determined and calculated?,"The weight value for characters in Chinese two-character words is determined by the productivity of the characters. This productivity refers to the number of other characters a given character can combine with. A character that can combine with more characters is considered more productive and is more likely to be a head, thus warranting a higher weight. The weight is calculated using the formula β = C(n,1) / N, where C(n,1) represents the number of candidate morphemic components, and N is the total number of components. For instance, if a character can combine with 5 near-synonyms candidates, compared to another character that can combine with only 3, the more productive character should receive more weight. In this example, the weight β is calculated as 5/8 = 0.625. Importantly, this weight assignment is independent of the character's position.",7,2,4,2,"Question:

How is the weight value for characters in Chinese two-character words determined and calculated?

Answer:

The weight value for characters in Chinese two-character words is determined by the productivity of the characters, which refers to the number of other characters a given character can combine with. The more productive a character is, the more likely it is to be a head, and thus, more weight should be assigned to it. The weight is calculated as β = C(n,1) / N, where C(n,1) is the number of candidate morphemic components, and N is the total number of components. For example, if a character can combine with more characters (e.g., 5 near-synonyms candidates) compared to another character (e.g., 3 near-synonyms candidates), it should receive more weight. In this case, the weight β is calculated as 5/8 = 0.625. The weight assignment is character and position independent.","Question:

How is the weight value for characters in Chinese two-character words determined and calculated?

Answer:

The weight value for characters in Chinese two-character words is determined by the productivity of the characters. This productivity refers to the number of other characters a given character can combine with. A character that can combine with more characters is considered more productive and is more likely to be a head, thus warranting a higher weight. The weight is calculated using the formula β = C(n,1) / N, where C(n,1) represents the number of candidate morphemic components, and N is the total number of components. For instance, if a character can combine with 5 near-synonyms candidates, compared to another character that can combine with only 3, the more productive character should receive more weight. In this example, the weight β is calculated as 5/8 = 0.625. Importantly, this weight assignment is independent of the character's position.",NO,True,858,True,True
229376920-s14,Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2020-11-01,Automatic Evaluation,"Automatic evaluation provides an economic, reproducible, and scalable way to assess the quality of generation results. However, due to the complexities of natural language, each metric introduced below can address certain aspects, but also has intrinsic blind spots.

BLEU with Gold References. Similar to many text generation tasks, TST also has humanwritten references on several datasets (Yelp, Captions, etc.), so it is common to use the BLEU score (Papineni et al. 2002) between the gold references and model outputs. Using BLEU to evaluate TST models has been seen across pre-deep learning works (Xu et al. 2012;Jhamtani et al. 2017) and deep learning approaches (Rao and Tetreault 2018;Jin et al. 2019).

There are three problems with using BLEU between the gold references and model outputs:

Problem 1. It mainly evaluates content and simply copying the input can result in high BLEU scores. Problem 2. BLEU is shown to have low correlation with human evaluation. Problem 3. Some datasets do not have human-written references.

Problem 1: Different from machine translation, where using BLEU only is sufficient, TST has to consider the caveat that simply copying the input sentence can achieve high BLEU scores with the gold references on many datasets (e.g., ∼40 on Yelp, ∼20 on Humor & Romance, ∼50 for informal-to-formal style transfer, and ∼30 for formal-toinformal style transfer). This is because most text rewrites have a large extent of n-gram overlap with the source sentence. In contrast, machine translation does not have this concern, because the vocabulary of its input and output are different, and copying the input sequence does not give high BLEU scores. A possible fix to consider is to combine BLEU with PINC (Chen and Dolan 2011) as in paraphrasing (Xu et al. 2012;Jhamtani et al. 2017). By using PINC and BLEU as a 2-dimensional metric, we can minimize the n-gram overlap with the source sentence but maximize the n-gram overlap with the reference sentences.

Problems 2 & 3: Other problems include insufficient correlation of BLEU with human evaluations (e.g., ≤0.30 with respect to human-rated grammaticality shown in  and ≤0.45 with respect to human evaluations shown in Mir et al. [2019]), and the unavailability of human-written references for some datasets (e.g., gender and political datasets [Prabhumoye et al. 2018], and the politeness dataset [Madaan et al. 2020]). A commonly used fix is to make the evaluation more fine-grained using three different independent aspects, namely, transferred style strength, semantic preservation, and fluency, which will be detailed below.

Transferred Style Strength. To automatically evaluate the transferred style strength, most works separately train a style classifier to distinguish the attributes (Hu et al. 2017;Shen et al. 2017;Fu et al. 2018;Prabhumoye et al. 2018 .  shows that the attribute classifier correlates well with human evaluation on some datasets (e.g., Yelp and Captions), but has almost no correlation with others (e.g., Amazon). The reason is that some product genres has a dominant number of positive or negative reviews.

Semantic Preservation. Many metrics can be applied to measure the similarity between the input and output sentence pairs, including BLEU (Papineni et al. 2002), ROUGE (Lin and Och 2004), METEOR (Banerjee and Lavie 2005), chrF (Popović 2015), and Word Mover Distance (WMD) (Kusner et al. 2015). Recently, some additional deep-learningbased metrics have been proposed, such as cosine similarity based on sentence embeddings (Fu et al. 2018), and BERTScore ). There are also evaluation metrics that are specific for TST such as the Part-of-Speech distance (Tian, Hu, and Yu 2018). Another newly proposed metric is to first delete all attribute-related expressions in the text, and then apply the above similarity evaluations (Mir et al. 2019). Among all the metrics, Mir et al. (2019) and Yamshchikov et al. (2021) showed that METEOR and WMD have better correlation with human evaluation than BLEU, although, in practice, BLEU is the most widely used metric to evaluate the semantic similarity between the source sentence and style-transferred output Madaan et al. 2020).

Fluency. Fluency is a basic requirement for natural language outputs. To automate this evaluation, perplexity is calculated via a language model (LM) pretrained on the training data of all attributes ). However, the effectiveness of perplexity remains debatable, as Pang and Gimpel (2019) showed its high correlation with human ratings of fluency, whereas Mir et al. (2019) suggested no significant correlation between perplexity and human scores. We note that perplexity by LM can suffer from the following undesired properties:","How is BLEU used in TST evaluation, and what are its limitations?","Automatic evaluation provides an economic, reproducible, and scalable way to assess the quality of generation results. However, due to the complexities of natural language, each metric introduced can address certain aspects, but also has intrinsic blind spots. BLEU with Gold References is used in TST evaluation, similar to many text generation tasks, as TST also has human-written references on several datasets (Yelp, Captions, etc.). It is common to use the BLEU score (Papineni et al. 2002) between the gold references and model outputs. Using BLEU to evaluate TST models has been seen across pre-deep learning works (Xu et al. 2012; Jhamtani et al. 2017) and deep learning approaches (Rao and Tetreault 2018; Jin et al. 2019). There are three problems with using BLEU between the gold references and model outputs. Problem 1: It mainly evaluates content and simply copying the input can result in high BLEU scores. Problem 2: BLEU is shown to have low correlation with human evaluation. Problem 3: Some datasets do not have human-written references. Different from machine translation, where using BLEU only is sufficient, TST has to consider the caveat that simply copying the input sentence can achieve high BLEU scores with the gold references on many datasets. This is because most text rewrites have a large extent of n-gram overlap with the source sentence. In contrast, machine translation does not have this concern, because the vocabulary of its input and output are different, and copying the input sequence does not give high BLEU scores. A possible fix to consider is to combine BLEU with PINC (Chen and Dolan 2011) as in paraphrasing (Xu et al. 2012; Jhamtani et al. 2017). By using PINC and BLEU as a 2-dimensional metric, it is possible to minimize the n-gram overlap with the source sentence but maximize the n-gram overlap with the reference sentences. Other problems include insufficient correlation of BLEU with human evaluations and the unavailability of human-written references for some datasets.","How is BLEU utilized in the evaluation of Text Style Transfer (TST), and what are its limitations?","BLEU, or the Bilingual Evaluation Understudy, is commonly used in the evaluation of Text Style Transfer (TST) models, much like in other text generation tasks. This is because TST often involves datasets with human-written references, such as those from Yelp or image captions. The BLEU score, introduced by Papineni et al. in 2002, measures the similarity between the model outputs and these gold references. Its application in TST evaluation spans both pre-deep learning works (Xu et al. 2012; Jhamtani et al. 2017) and deep learning approaches (Rao and Tetreault 2018; Jin et al. 2019).

However, using BLEU in TST evaluation presents several challenges. Firstly, BLEU primarily evaluates content overlap, which means that simply copying the input text can result in high BLEU scores. This is a significant issue in TST, as text rewrites often have substantial n-gram overlap with the source sentence. Unlike machine translation, where the input and output vocabularies differ, TST can achieve high BLEU scores by merely replicating the input, which is not desirable.

Secondly, BLEU has been shown to have a low correlation with human evaluations, which questions its reliability as a sole metric for assessing TST quality. Lastly, some datasets used in TST do not have human-written references, making BLEU less applicable.

To address these limitations, a potential solution is to combine BLEU with the PINC metric (Chen and Dolan 2011), as suggested in paraphrasing studies (Xu et al. 2012; Jhamtani et al. 2017). By using BLEU and PINC as a two-dimensional metric, it is possible to minimize n-gram overlap with the source sentence while maximizing overlap with reference sentences, thus providing a more balanced evaluation.",8,2,7,2,"Question:

How is BLEU used in TST evaluation, and what are its limitations?

Answer:

Automatic evaluation provides an economic, reproducible, and scalable way to assess the quality of generation results. However, due to the complexities of natural language, each metric introduced can address certain aspects, but also has intrinsic blind spots. BLEU with Gold References is used in TST evaluation, similar to many text generation tasks, as TST also has human-written references on several datasets (Yelp, Captions, etc.). It is common to use the BLEU score (Papineni et al. 2002) between the gold references and model outputs. Using BLEU to evaluate TST models has been seen across pre-deep learning works (Xu et al. 2012; Jhamtani et al. 2017) and deep learning approaches (Rao and Tetreault 2018; Jin et al. 2019). There are three problems with using BLEU between the gold references and model outputs. Problem 1: It mainly evaluates content and simply copying the input can result in high BLEU scores. Problem 2: BLEU is shown to have low correlation with human evaluation. Problem 3: Some datasets do not have human-written references. Different from machine translation, where using BLEU only is sufficient, TST has to consider the caveat that simply copying the input sentence can achieve high BLEU scores with the gold references on many datasets. This is because most text rewrites have a large extent of n-gram overlap with the source sentence. In contrast, machine translation does not have this concern, because the vocabulary of its input and output are different, and copying the input sequence does not give high BLEU scores. A possible fix to consider is to combine BLEU with PINC (Chen and Dolan 2011) as in paraphrasing (Xu et al. 2012; Jhamtani et al. 2017). By using PINC and BLEU as a 2-dimensional metric, it is possible to minimize the n-gram overlap with the source sentence but maximize the n-gram overlap with the reference sentences. Other problems include insufficient correlation of BLEU with human evaluations and the unavailability of human-written references for some datasets.","Question:

How is BLEU utilized in the evaluation of Text Style Transfer (TST), and what are its limitations?

Answer:

BLEU, or the Bilingual Evaluation Understudy, is commonly used in the evaluation of Text Style Transfer (TST) models, much like in other text generation tasks. This is because TST often involves datasets with human-written references, such as those from Yelp or image captions. The BLEU score, introduced by Papineni et al. in 2002, measures the similarity between the model outputs and these gold references. Its application in TST evaluation spans both pre-deep learning works (Xu et al. 2012; Jhamtani et al. 2017) and deep learning approaches (Rao and Tetreault 2018; Jin et al. 2019).

However, using BLEU in TST evaluation presents several challenges. Firstly, BLEU primarily evaluates content overlap, which means that simply copying the input text can result in high BLEU scores. This is a significant issue in TST, as text rewrites often have substantial n-gram overlap with the source sentence. Unlike machine translation, where the input and output vocabularies differ, TST can achieve high BLEU scores by merely replicating the input, which is not desirable.

Secondly, BLEU has been shown to have a low correlation with human evaluations, which questions its reliability as a sole metric for assessing TST quality. Lastly, some datasets used in TST do not have human-written references, making BLEU less applicable.

To address these limitations, a potential solution is to combine BLEU with the PINC metric (Chen and Dolan 2011), as suggested in paraphrasing studies (Xu et al. 2012; Jhamtani et al. 2017). By using BLEU and PINC as a two-dimensional metric, it is possible to minimize n-gram overlap with the source sentence while maximizing overlap with reference sentences, thus providing a more balanced evaluation.",NO,True,1733,True,True
229376920-s21,Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2020-11-01,3.,"The pretrained models are imperfect in the sense that they will favor toward a certain type of methods.

For the first point, it is important to not use the same style classifier or LM in the proposed TST approach, otherwise it can overfit or hack the metrics. For the second point, we need to understand what the false positives and false negatives of the generated outputs can be. An illustrative example is that if the style classifier only reports 80+% performance (e.g., on the gender dataset [Prabhumoye et al. 2018] and Amazon dataset ), even perfect style rewrites can only score 80+%, but maybe an imperfect model can score 90% because it can resemble the imperfect style classification model more and takes advantage of the false positives. Other reasons for false positives can be adversarial attacks. Jin et al. (2020b) showed that merely paraphrasing using synonyms can drop the performance of high-accuracy classification models from TextCNN (Kim 2014) to BERT (Devlin et al. 2019) by 90+%. Therefore, higher scores by the style classifier do not necessarily indicate more successful transfer. Moreover, the style classifier can produce false negatives if there is a distribution shift between the training data and style-transferred outputs. For example, in the training corpus, a product may appear often with the positive attribute, and in the style-transferred outputs, this product co-occurs with the opposite, negative attribute. Such false negatives are observed on the Amazon product review dataset ). On the other hand, the biases of the LM correlate with sentence length, synonym replacement, and prior context.

The third point is a direct result implied by the second point, so in practice, we need to keep in mind and check whether the proposed model takes advantage of the evaluation metrics or makes improvements that are generalizable.",How do false positives and false negatives impact the evaluation of text style transfer models?,"The pretrained models are imperfect in the sense that they will favor toward a certain type of methods. If the style classifier only reports 80+% performance, even perfect style rewrites can only score 80+%, but maybe an imperfect model can score 90% because it can resemble the imperfect style classification model more and takes advantage of the false positives. Other reasons for false positives can be adversarial attacks. Higher scores by the style classifier do not necessarily indicate more successful transfer. The style classifier can produce false negatives if there is a distribution shift between the training data and style-transferred outputs. For example, in the training corpus, a product may appear often with the positive attribute, and in the style-transferred outputs, this product co-occurs with the opposite, negative attribute. Such false negatives are observed on the Amazon product review dataset.",How do false positives and false negatives affect the evaluation of text style transfer models?,"The evaluation of text style transfer models is significantly impacted by false positives and false negatives due to the imperfections in pretrained models. These models often have a bias towards certain methods, which can skew results. For instance, if a style classifier reports an accuracy of only 80+%, even perfectly styled rewrites can only achieve a score of 80+%. Conversely, an imperfect model might score 90% by exploiting these biases and taking advantage of false positives. Adversarial attacks can also contribute to false positives, leading to higher scores that do not necessarily reflect successful style transfer.

On the other hand, false negatives can occur when there is a distribution shift between the training data and the style-transferred outputs. For example, in a training corpus, a product might frequently be associated with a positive attribute, but in the style-transferred outputs, it might co-occur with a negative attribute. Such false negatives have been observed in datasets like the Amazon product review dataset. Therefore, higher scores from a style classifier do not always indicate a more successful transfer, as they may be influenced by these inaccuracies.",7,4,7,2,"Question:

How do false positives and false negatives impact the evaluation of text style transfer models?

Answer:

The pretrained models are imperfect in the sense that they will favor toward a certain type of methods. If the style classifier only reports 80+% performance, even perfect style rewrites can only score 80+%, but maybe an imperfect model can score 90% because it can resemble the imperfect style classification model more and takes advantage of the false positives. Other reasons for false positives can be adversarial attacks. Higher scores by the style classifier do not necessarily indicate more successful transfer. The style classifier can produce false negatives if there is a distribution shift between the training data and style-transferred outputs. For example, in the training corpus, a product may appear often with the positive attribute, and in the style-transferred outputs, this product co-occurs with the opposite, negative attribute. Such false negatives are observed on the Amazon product review dataset.","Question:

How do false positives and false negatives affect the evaluation of text style transfer models?

Answer:

The evaluation of text style transfer models is significantly impacted by false positives and false negatives due to the imperfections in pretrained models. These models often have a bias towards certain methods, which can skew results. For instance, if a style classifier reports an accuracy of only 80+%, even perfectly styled rewrites can only achieve a score of 80+%. Conversely, an imperfect model might score 90% by exploiting these biases and taking advantage of false positives. Adversarial attacks can also contribute to false positives, leading to higher scores that do not necessarily reflect successful style transfer.

On the other hand, false negatives can occur when there is a distribution shift between the training data and the style-transferred outputs. For example, in a training corpus, a product might frequently be associated with a positive attribute, but in the style-transferred outputs, it might co-occur with a negative attribute. Such false negatives have been observed in datasets like the Amazon product review dataset. Therefore, higher scores from a style classifier do not always indicate a more successful transfer, as they may be influenced by these inaccuracies.",NO,True,1199,True,True
229376920-s41,Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2020-11-01,Method Strengths & Weaknesses,"Disentanglement + More profound in theoretical analysis, e.g., disentangled representation learning − Difficulties of training deep generative models (VAEs, GANs) for text − Hard to represent all styles as latent code − Computational cost rises with the number of styles to model Prototype Editing + High BLEU scores due to large word preservation − Attribute marker detection step can fail if the style and semantics are confounded − The step target attribute retrieval by templates can fail if there are large rewrites for styles, e.g., Shakespearean English vs. modern English − Target attribute retrieval step has large complexity (quadratic to the number of sentences) − Large computational cost if there are many styles, each of which needs a pre-trained LM for the generation step ? Future work can enable matchings for syntactic variation ? Future work can use grammatical error correction to post-edit the output Pseudo-Parallel Corpus Construction + Performance can approximate supervised model performance, if the pseudo-parallel data are of good quality − May fail for small corpora − May fail if the mono-style corpora do not have many samples with similar contents − For IBT, divergence is possible, and sometimes needs special designs to prevent it − ForIBT,time complexityishigh(duetoiterativepseudo data generation) ? Improve the convergence of the IBT Challenges for Disentanglement. Theoretically, although disentanglement is impossible without inductive biases or other forms of supervision (Locatello et al. 2019), disentanglement is achievable with some weak signals, such as only knowing how many factors have changed, but not which ones (Locatello et al. 2020).

In practice, some big challenges for disentanglement-based methods include, for example, the difficulty to train deep text generative models such as VAEs and GANs. Also, it is not easy to represent all styles as latent code. Moreover, if targeting multiple styles, the computational complexity linearly increases with the number of styles to model.

Challenges for Prototype Editing. Prototype-editing approaches usually result in relatively high BLEU scores, partly because the output text largely overlaps with the input text. This line of methods is likely to perform well on tasks such as sentiment modification, for which it is easy to identify ""attribute markers,"" and the input and output sentences share an attribute-independent template.

However, prototype editing cannot be applied to all types of style transfer tasks. The first step, attribute marker retrieval, might not work if the datasets have confounded style and contents, because they may lead to wrong extraction of attribute markers, such as some content words or artifacts which can also be used to distinguish the stylespecific data.

The second step, target attribute retrieval by templates, will fail if there is too little word overlap between a sentence and its counterpart carrying another style. An example is the TST task to ""Shakespearize"" modern English. There is little lexical overlap between a Shakespearean sentence written in early modern English and its corresponding modern English expression. In such cases, the retrieval step is likely to fail, because there is a large number of rewrites between the two styles, and the template might be almost hollow. Moreover, this step is also computationally expensive, if there are a large number of sentences in the data (e.g., all Wikipedia text), since this step needs to calculate the pair-wise similarity among all available sentences across style-specific corpora.

The third step, generation from prototype, requires a separate pretrained LM for each style corpus. When there are multiple styles of interest (e.g., multiple persona), this will induce a large computational cost.

The last limitation of prototype editing is that it amplifies the intrinsic problem of using BLEU to evaluate TST (Problem 1, namely, the fact that simply copying the input can result in a high BLEU score, as introduced in Section 3.1). For the retrieval-based method, some can argue that there is some performance gain because this method in practice copies more expressions in the input sentence than other lines of methods.

As future study, there can be many interesting directions to explore, for example, investigating the performance of existing prototype editing models under a challenging dataset that reveals the above shortcomings, proposing new models to improve this line of approaches, and better evaluation methods for prototype editing models.

Challenges for Pseudo-Parallel Corpus Construction. The method to construct pseudoparallel data can be effective, especially when the pseudo-parallel corpora resemble supervised data. The challenge is that this approach may not work if the non-parallel corpora do not have enough samples that can be matched to create the pseudo-parallel corpora, or when the IBT cannot bootstrap well or fails to converge. The time complexity for training IBT is also very high because it needs to iteratively generate pseudo-parallel corpus and re-train models. Interesting future directions can be reducing the computational cost, designing more effective bootstrapping, and improving the convergence of IBT.

6.2.2 Understanding the Evolution from Traditional NLG to Deep Learning Methods. Despite the exciting methodological revolution led by deep learning recently, we are also interested in the merging point of traditional computational linguistics and the deep learning techniques (Henderson 2020). Specific to the context of TST, we will introduce the traditional NLG framework, and its impact on the current TST approaches, especially the prototype editing method.

Traditional NLG Framework. The traditional NLG framework stages sentence generation into the following steps (Reiter and Dale 1997 ",What are the challenges and limitations of prototype editing methods in text style transfer?,"Prototype-editing approaches usually result in relatively high BLEU scores, partly because the output text largely overlaps with the input text. This line of methods is likely to perform well on tasks such as sentiment modification, for which it is easy to identify ""attribute markers,"" and the input and output sentences share an attribute-independent template. However, prototype editing cannot be applied to all types of style transfer tasks. The first step, attribute marker retrieval, might not work if the datasets have confounded style and contents, because they may lead to wrong extraction of attribute markers, such as some content words or artifacts which can also be used to distinguish the style-specific data. The second step, target attribute retrieval by templates, will fail if there is too little word overlap between a sentence and its counterpart carrying another style. An example is the TST task to ""Shakespearize"" modern English. There is little lexical overlap between a Shakespearean sentence written in early modern English and its corresponding modern English expression. In such cases, the retrieval step is likely to fail, because there is a large number of rewrites between the two styles, and the template might be almost hollow. Moreover, this step is also computationally expensive, if there are a large number of sentences in the data (e.g., all Wikipedia text), since this step needs to calculate the pair-wise similarity among all available sentences across style-specific corpora. The third step, generation from prototype, requires a separate pretrained LM for each style corpus. When there are multiple styles of interest (e.g., multiple persona), this will induce a large computational cost. The last limitation of prototype editing is that it amplifies the intrinsic problem of using BLEU to evaluate TST (Problem 1, namely, the fact that simply copying the input can result in a high BLEU score).",What are the challenges and limitations associated with prototype editing methods in text style transfer?,"Prototype editing methods in text style transfer face several challenges and limitations. One of the main advantages of these approaches is their ability to achieve relatively high BLEU scores, primarily because the output text often closely resembles the input text. This makes them particularly effective for tasks like sentiment modification, where ""attribute markers"" are easily identifiable, and the input and output sentences share a template that is independent of the attribute being modified.

However, prototype editing is not universally applicable to all style transfer tasks. The initial step of attribute marker retrieval can be problematic if the datasets have intertwined style and content. This can lead to incorrect extraction of attribute markers, as some content words or artifacts might also serve to distinguish style-specific data. The subsequent step, which involves retrieving the target attribute using templates, may fail if there is minimal word overlap between a sentence and its counterpart in another style. For instance, in the task of ""Shakespearizing"" modern English, there is little lexical overlap between a Shakespearean sentence in early modern English and its modern English equivalent. In such scenarios, the retrieval step is likely to falter due to the extensive rewrites required between the two styles, rendering the template nearly empty.

Additionally, this retrieval step can be computationally expensive, especially when dealing with large datasets, such as the entirety of Wikipedia text, as it necessitates calculating pair-wise similarity among all available sentences across style-specific corpora. The final step, generating text from the prototype, demands a separate pretrained language model for each style corpus. When multiple styles are of interest, such as various personas, this results in significant computational costs.

Lastly, prototype editing exacerbates the inherent issue of using BLEU scores to evaluate text style transfer. The problem lies in the fact that simply copying the input text can yield a high BLEU score, which does not necessarily reflect successful style transfer.",7,6,7,6,"Question:

What are the challenges and limitations of prototype editing methods in text style transfer?

Answer:

Prototype-editing approaches usually result in relatively high BLEU scores, partly because the output text largely overlaps with the input text. This line of methods is likely to perform well on tasks such as sentiment modification, for which it is easy to identify ""attribute markers,"" and the input and output sentences share an attribute-independent template. However, prototype editing cannot be applied to all types of style transfer tasks. The first step, attribute marker retrieval, might not work if the datasets have confounded style and contents, because they may lead to wrong extraction of attribute markers, such as some content words or artifacts which can also be used to distinguish the style-specific data. The second step, target attribute retrieval by templates, will fail if there is too little word overlap between a sentence and its counterpart carrying another style. An example is the TST task to ""Shakespearize"" modern English. There is little lexical overlap between a Shakespearean sentence written in early modern English and its corresponding modern English expression. In such cases, the retrieval step is likely to fail, because there is a large number of rewrites between the two styles, and the template might be almost hollow. Moreover, this step is also computationally expensive, if there are a large number of sentences in the data (e.g., all Wikipedia text), since this step needs to calculate the pair-wise similarity among all available sentences across style-specific corpora. The third step, generation from prototype, requires a separate pretrained LM for each style corpus. When there are multiple styles of interest (e.g., multiple persona), this will induce a large computational cost. The last limitation of prototype editing is that it amplifies the intrinsic problem of using BLEU to evaluate TST (Problem 1, namely, the fact that simply copying the input can result in a high BLEU score).","Question:

What are the challenges and limitations associated with prototype editing methods in text style transfer?

Answer:

Prototype editing methods in text style transfer face several challenges and limitations. One of the main advantages of these approaches is their ability to achieve relatively high BLEU scores, primarily because the output text often closely resembles the input text. This makes them particularly effective for tasks like sentiment modification, where ""attribute markers"" are easily identifiable, and the input and output sentences share a template that is independent of the attribute being modified.

However, prototype editing is not universally applicable to all style transfer tasks. The initial step of attribute marker retrieval can be problematic if the datasets have intertwined style and content. This can lead to incorrect extraction of attribute markers, as some content words or artifacts might also serve to distinguish style-specific data. The subsequent step, which involves retrieving the target attribute using templates, may fail if there is minimal word overlap between a sentence and its counterpart in another style. For instance, in the task of ""Shakespearizing"" modern English, there is little lexical overlap between a Shakespearean sentence in early modern English and its modern English equivalent. In such scenarios, the retrieval step is likely to falter due to the extensive rewrites required between the two styles, rendering the template nearly empty.

Additionally, this retrieval step can be computationally expensive, especially when dealing with large datasets, such as the entirety of Wikipedia text, as it necessitates calculating pair-wise similarity among all available sentences across style-specific corpora. The final step, generating text from the prototype, demands a separate pretrained language model for each style corpus. When multiple styles are of interest, such as various personas, this results in significant computational costs.

Lastly, prototype editing exacerbates the inherent issue of using BLEU scores to evaluate text style transfer. The problem lies in the fact that simply copying the input text can yield a high BLEU score, which does not necessarily reflect successful style transfer.",NO,True,2150,True,True
229376920-s44,Deep Learning for Text Style Transfer: A Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2020-11-01,Improving Evaluation Metrics,"There has been a lot of attention to the problems of evaluation metrics of TST and potential improvements (Pang and Gimpel 2019;Tikhonov and Yamshchikov 2018;Mir et al. 2019;Fu et al. 2019;Yamshchikov et al. 2021;Jafaritazehjani et al. 2020). Recently, Gehrmann et al. (2021) have proposed a new framework that is a live environment to evaluate NLG in a principled and reproducible manner. Apart from the existing scoring methods, future work can also make use of linguistic rules such as a checklist to evaluate what capabilities the TST model has achieved. For example, there can be a checklist for formality transfer according to existing style guidelines, such as the APA style guide (American Psychological Association 2020). Such a checklistbased evaluation can make the performance of black-box deep learning models more interpretable, and also allow for more insightful error analysis.",How can linguistic rules enhance the evaluation of Text Style Transfer models?,"Apart from the existing scoring methods, future work can also make use of linguistic rules such as a checklist to evaluate what capabilities the TST model has achieved. For example, there can be a checklist for formality transfer according to existing style guidelines, such as the APA style guide (American Psychological Association 2020). Such a checklist-based evaluation can make the performance of black-box deep learning models more interpretable, and also allow for more insightful error analysis.",In what ways can linguistic rules improve the assessment of Text Style Transfer (TST) models?,"In addition to the current scoring methods, future research can incorporate linguistic rules, such as a checklist, to evaluate the capabilities achieved by TST models. For instance, a checklist for formality transfer could be developed based on existing style guidelines, like the APA style guide (American Psychological Association 2020). This checklist-based evaluation approach can enhance the interpretability of black-box deep learning models and provide more insightful error analysis.",7,6,7,6,"Question:

How can linguistic rules enhance the evaluation of Text Style Transfer models?

Answer:

Apart from the existing scoring methods, future work can also make use of linguistic rules such as a checklist to evaluate what capabilities the TST model has achieved. For example, there can be a checklist for formality transfer according to existing style guidelines, such as the APA style guide (American Psychological Association 2020). Such a checklist-based evaluation can make the performance of black-box deep learning models more interpretable, and also allow for more insightful error analysis.","Question:

In what ways can linguistic rules improve the assessment of Text Style Transfer (TST) models?

Answer:

In addition to the current scoring methods, future research can incorporate linguistic rules, such as a checklist, to evaluate the capabilities achieved by TST models. For instance, a checklist for formality transfer could be developed based on existing style guidelines, like the APA style guide (American Psychological Association 2020). This checklist-based evaluation approach can enhance the interpretability of black-box deep learning models and provide more insightful error analysis.",NO,True,491,True,True
237371890-s6,Survey of Low-Resource Machine Translation,2021-09-01,Test Sets,"Obtaining more training data is important, but we should not forget the role of standardised and reliable test sets in improving performance on low-resource translation. Important contributions have come from shared tasks, such as those organised by the WMT Conference on Machine Translation Barrault et al. 2019Barrault et al. , 2020 (Nakazawa et al. 2018(Nakazawa et al. , 2019(Nakazawa et al. , 2020 and the Workshop on Technologies for MT of Low Resource Languages (LoResMT) (Karakanta et al. 2019;Ojha et al. 2020). The test sets in these shared tasks are very useful, but inevitably only cover a small selection of low-resource languages, and usually English is one of the languages in the pair; non-English language pairs are poorly covered. A recent initiative towards rectifying this situation is the FLORES-101 benchmark , which covers a large number of low-resource languages with multi-parallel test sets, vastly expanding on the original FLORES release . Since FLORES-101 consists of the same set of English sentences, translated into 100 other languages, it can also be used for testing translation between non-English pairs. It has the limitation that, for non-English pairs, the two sides are ""translationese"", and not mutual translations of each other, but there is currently no other data set with the coverage of FLORES-101.

We summarise the data sets described in this section in Table 2.",How does the FLORES-101 benchmark address the limitations of previous test sets for low-resource languages?,"The FLORES-101 benchmark covers a large number of low-resource languages with multi-parallel test sets, vastly expanding on the original FLORES release. Since FLORES-101 consists of the same set of English sentences, translated into 100 other languages, it can also be used for testing translation between non-English pairs. It has the limitation that, for non-English pairs, the two sides are ""translationese"", and not mutual translations of each other, but there is currently no other data set with the coverage of FLORES-101.",How does the FLORES-101 benchmark improve upon previous test sets for low-resource languages?,"The FLORES-101 benchmark significantly enhances the evaluation of low-resource languages by providing multi-parallel test sets for a vast array of languages, greatly expanding upon the original FLORES release. It includes the same set of English sentences translated into 100 other languages, allowing for the testing of translation between non-English language pairs. However, it is important to note that for non-English pairs, both sides are ""translationese"" rather than direct translations of each other. Despite this limitation, FLORES-101 remains unmatched in its coverage of low-resource languages.",8,2,7,2,"Question:

How does the FLORES-101 benchmark address the limitations of previous test sets for low-resource languages?

Answer:

The FLORES-101 benchmark covers a large number of low-resource languages with multi-parallel test sets, vastly expanding on the original FLORES release. Since FLORES-101 consists of the same set of English sentences, translated into 100 other languages, it can also be used for testing translation between non-English pairs. It has the limitation that, for non-English pairs, the two sides are ""translationese"", and not mutual translations of each other, but there is currently no other data set with the coverage of FLORES-101.","Question:

How does the FLORES-101 benchmark improve upon previous test sets for low-resource languages?

Answer:

The FLORES-101 benchmark significantly enhances the evaluation of low-resource languages by providing multi-parallel test sets for a vast array of languages, greatly expanding upon the original FLORES release. It includes the same set of English sentences translated into 100 other languages, allowing for the testing of translation between non-English language pairs. However, it is important to note that for non-English pairs, both sides are ""translationese"" rather than direct translations of each other. Despite this limitation, FLORES-101 remains unmatched in its coverage of low-resource languages.",NO,True,605,True,True
237371890-s8,Survey of Low-Resource Machine Translation,2021-09-01,Integration of external language models,"For SMT, monolingual data was normally incorporated into the system using a language model, in a formulation that can be traced back to the noisy channel model (Brown et al. 1993). In early work on NMT, researchers drew inspiration from SMT, and several works have focused on integrating external language models into NMT models.

The first approach, proposed by Gülçehre et al. (2015), was to modify the scoring function of the MT model by either interpolating the probabilities from a language model with the translation probabilities (they call this shallow fusion) or integrating the hidden states from the language model within the decoder (they call this deep fusion). Importantly, they see improved scores for a range of scenarios, including a (simulated) low-resource language direction (Turkish→English), with best results achieved using deep fusion. Building on this, Stahlberg, Cross, and Stoyanov (2018) proposed simple fusion as an alternative method for including a pre-trained LM. In this case, the NMT model is trained from scratch with the fixed LM, offering it a chance to learn to be complementary to the LM. The result is improved translation performance as well as training efficiency, with experiments again on low-resource Turkish-English, as well as on larger data sets for Xhosa→English and Estonian→English.

The addition of a language model to the scoring function as in the works described above has the disadvantage of increasing the time necessary for decoding (as well as training). An alternative approach was proposed by Baziotis, Haddow, and Birch (2020), who aim to overcome this by using the language model as a regulariser during training, pushing the source-conditional NMT probabilities to be closer to the unconditional LM prior. They see considerable gains in very low-resource settings (albeit simulated), using small data sets for Turkish-English and German-English.",How have external language models been integrated into NMT models to improve performance in low-resource settings?,"In early work on NMT, researchers drew inspiration from SMT, and several works have focused on integrating external language models into NMT models. The first approach, proposed by Gülçehre et al. (2015), was to modify the scoring function of the MT model by either interpolating the probabilities from a language model with the translation probabilities (they call this shallow fusion) or integrating the hidden states from the language model within the decoder (they call this deep fusion). Importantly, they see improved scores for a range of scenarios, including a (simulated) low-resource language direction (Turkish→English), with best results achieved using deep fusion. Building on this, Stahlberg, Cross, and Stoyanov (2018) proposed simple fusion as an alternative method for including a pre-trained LM. In this case, the NMT model is trained from scratch with the fixed LM, offering it a chance to learn to be complementary to the LM. The result is improved translation performance as well as training efficiency, with experiments again on low-resource Turkish-English, as well as on larger data sets for Xhosa→English and Estonian→English. An alternative approach was proposed by Baziotis, Haddow, and Birch (2020), who aim to overcome this by using the language model as a regulariser during training, pushing the source-conditional NMT probabilities to be closer to the unconditional LM prior. They see considerable gains in very low-resource settings (albeit simulated), using small data sets for Turkish-English and German-English.",How have external language models been integrated into neural machine translation (NMT) models to enhance performance in low-resource settings?,"In the early development of neural machine translation (NMT), researchers took inspiration from statistical machine translation (SMT) to integrate external language models into NMT systems, particularly to improve performance in low-resource settings. One of the pioneering approaches was introduced by Gülçehre et al. (2015), who proposed modifying the scoring function of the machine translation model. They introduced two methods: shallow fusion, which involves interpolating probabilities from a language model with translation probabilities, and deep fusion, which integrates the hidden states from the language model within the decoder. Their experiments showed improved translation scores across various scenarios, including a simulated low-resource language pair, Turkish to English, with deep fusion yielding the best results.

Building on this foundation, Stahlberg, Cross, and Stoyanov (2018) introduced simple fusion as an alternative method for incorporating a pre-trained language model. In this approach, the NMT model is trained from scratch alongside a fixed language model, allowing the NMT system to learn to complement the language model. This method resulted in enhanced translation performance and training efficiency, demonstrated in experiments with low-resource Turkish-English as well as larger datasets for Xhosa-English and Estonian-English.

Another approach was proposed by Baziotis, Haddow, and Birch (2020), who used the language model as a regularizer during training. This technique encourages the source-conditional NMT probabilities to align more closely with the unconditional language model prior. Their method showed significant gains in very low-resource settings, using small datasets for Turkish-English and German-English, albeit in simulated environments.",7,4,7,4,"Question:

How have external language models been integrated into NMT models to improve performance in low-resource settings?

Answer:

In early work on NMT, researchers drew inspiration from SMT, and several works have focused on integrating external language models into NMT models. The first approach, proposed by Gülçehre et al. (2015), was to modify the scoring function of the MT model by either interpolating the probabilities from a language model with the translation probabilities (they call this shallow fusion) or integrating the hidden states from the language model within the decoder (they call this deep fusion). Importantly, they see improved scores for a range of scenarios, including a (simulated) low-resource language direction (Turkish→English), with best results achieved using deep fusion. Building on this, Stahlberg, Cross, and Stoyanov (2018) proposed simple fusion as an alternative method for including a pre-trained LM. In this case, the NMT model is trained from scratch with the fixed LM, offering it a chance to learn to be complementary to the LM. The result is improved translation performance as well as training efficiency, with experiments again on low-resource Turkish-English, as well as on larger data sets for Xhosa→English and Estonian→English. An alternative approach was proposed by Baziotis, Haddow, and Birch (2020), who aim to overcome this by using the language model as a regulariser during training, pushing the source-conditional NMT probabilities to be closer to the unconditional LM prior. They see considerable gains in very low-resource settings (albeit simulated), using small data sets for Turkish-English and German-English.","Question:

How have external language models been integrated into neural machine translation (NMT) models to enhance performance in low-resource settings?

Answer:

In the early development of neural machine translation (NMT), researchers took inspiration from statistical machine translation (SMT) to integrate external language models into NMT systems, particularly to improve performance in low-resource settings. One of the pioneering approaches was introduced by Gülçehre et al. (2015), who proposed modifying the scoring function of the machine translation model. They introduced two methods: shallow fusion, which involves interpolating probabilities from a language model with translation probabilities, and deep fusion, which integrates the hidden states from the language model within the decoder. Their experiments showed improved translation scores across various scenarios, including a simulated low-resource language pair, Turkish to English, with deep fusion yielding the best results.

Building on this foundation, Stahlberg, Cross, and Stoyanov (2018) introduced simple fusion as an alternative method for incorporating a pre-trained language model. In this approach, the NMT model is trained from scratch alongside a fixed language model, allowing the NMT system to learn to complement the language model. This method resulted in enhanced translation performance and training efficiency, demonstrated in experiments with low-resource Turkish-English as well as larger datasets for Xhosa-English and Estonian-English.

Another approach was proposed by Baziotis, Haddow, and Birch (2020), who used the language model as a regularizer during training. This technique encourages the source-conditional NMT probabilities to align more closely with the unconditional language model prior. Their method showed significant gains in very low-resource settings, using small datasets for Turkish-English and German-English, albeit in simulated environments.",NO,True,1799,True,True
237371890-s10,Survey of Low-Resource Machine Translation,2021-09-01,Self-learning: backtranslation and its variants.,"One of the most successful strategies for leveraging monolingual data has been the creation of synthetic parallel data through translating monolingual texts either using a heuristic strategy or an intermediately trained MT model. This results in parallel data where one side is human generated, and the other is automatically produced. We focus here on backtranslation and its variants, before exploring in the next section how unsupervised MT can be seen as an extension of this idea.

Backtranslation. Backtranslation corresponds to the scenario where target-side monolingual data is translated using an MT system to give corresponding synthetic source sentences, the idea being that it is particularly beneficial for the MT decoder to see wellformed sentences. Backtranslation was first introduced in SMT (Bertoldi and Federico 2009; Bojar and Tamchyna 2011), but since monolingual data could already be incorporated easily into SMT systems using language models, and because inference in SMT was quite slow, backtranslation was not widely used. For NMT however, it was discovered that backtranslation was a remarkably effective way of exploiting monolingual data (Sennrich, Haddow, and Birch 2016a), and it remains an important technique, for both low-resource MT and MT in general.

There has since been considerable interest in understanding and improving backtranslation. For example, Edunov et al. (2018a) showed that backtranslation improved performance even at a very large scale, but also that it provided improvements in (simulated) low-resource settings, where it was important to use beam-search rather than sampling to create backtranslations (the opposite situation to high-resource pairs). Caswell, Chelba, and Grangier (2019) showed that simply adding a tag to the backtranslated data during training, to let the model know which was back-translated data and which was natural data, could improve performance.

Variants of backtranslation. Forward translation, where monolingual source data is translated into the target language (Zhang and Zong 2016), is also possible but has received considerably less interest than backtranslation for low-resource MT, presumably because of the noise it introduces for the decoder. However, He et al. (2019) actually find it more effective than backtranslation in their experiments for low-resource English→Nepali when coupled with noising (as a kind of dropout), which they identify as an important factor in self-supervised learning. A related (and even simpler) tech-nique related to forward and backtranslation, that of copying from target to source to create synthetic data, was introduced by Currey, Miceli Barone, and Heafield (2017). They showed that this was particularly useful in low-resource settings (tested on Turkish-English and Romanian-English) and hypothesise that it helped particularly with the translation of named entities.

Iterative backtranslation. For low-resource NMT, back-translation can be a particularly effective way of improving quality . However one possible issue is that the initial model used for translation (trained on available parallel data) is often of poor quality when parallel data is scarce, which inevitable leads to poor quality backtranslations. The logical way to address this is to perform iterative backtranslation, whereby intermediate models of increasing quality in both language directions are successively used to create synthetic parallel data for the next step. This has been successfully applied to low-resource settings by several authors (Hoang et al. 2018;Dandapat and Federmann. 2018;Bawden et al. 2019;, although successive iterations offer diminishing returns, and often two iterations are sufficient, as has been shown experimentally .

Other authors have sought to improve on iterative backtranslation by introducing a round-trip (i.e. autoencoder) objective for monolingual data, in other words performing backtranslation and forward translation implicitly during training. This was simultaneously proposed by Cheng et al. (2016) and He et al. (2016) and also by Zhang and Zong (2016) who also added forward translation. However, none of these authors applied their techniques to low-resource settings. In contrast, Niu, Xu, and Carpuat (2019) developed a method using Gumbel softmax to enable back-propagation through backtranslation: they tested in low-resource settings but achieved limited success.

Despite the large body of literature on applying back-translation and related techniques, and evidence that it works in low-resource NMT, there are few systematic experimental study of back-translation specifically for low-resource NMT, apart from ), which appears to confirm the findings of (Edunov et al. 2018a) that sampling is best when there are reasonable amounts of data and beam search is better when data is very scarce.",How has backtranslation been adapted and improved for low-resource neural machine translation?,"One of the most successful strategies for leveraging monolingual data has been the creation of synthetic parallel data through translating monolingual texts either using a heuristic strategy or an intermediately trained MT model. Backtranslation corresponds to the scenario where target-side monolingual data is translated using an MT system to give corresponding synthetic source sentences, the idea being that it is particularly beneficial for the MT decoder to see well-formed sentences. For NMT, it was discovered that backtranslation was a remarkably effective way of exploiting monolingual data (Sennrich, Haddow, and Birch 2016a), and it remains an important technique, for both low-resource MT and MT in general. Edunov et al. (2018a) showed that backtranslation improved performance even at a very large scale, but also that it provided improvements in (simulated) low-resource settings, where it was important to use beam-search rather than sampling to create backtranslations. Caswell, Chelba, and Grangier (2019) showed that simply adding a tag to the backtranslated data during training, to let the model know which was back-translated data and which was natural data, could improve performance. Iterative backtranslation can be a particularly effective way of improving quality in low-resource NMT. The logical way to address the issue of poor quality initial models is to perform iterative backtranslation, whereby intermediate models of increasing quality in both language directions are successively used to create synthetic parallel data for the next step. This has been successfully applied to low-resource settings by several authors, although successive iterations offer diminishing returns, and often two iterations are sufficient, as has been shown experimentally. Other authors have sought to improve on iterative backtranslation by introducing a round-trip (i.e., autoencoder) objective for monolingual data, in other words performing backtranslation and forward translation implicitly during training. Despite the large body of literature on applying back-translation and related techniques, and evidence that it works in low-resource NMT, there are few systematic experimental studies of back-translation specifically for low-resource NMT.",How has backtranslation been adapted and improved for low-resource neural machine translation?,"Backtranslation has become a pivotal technique in enhancing low-resource neural machine translation (NMT) by leveraging monolingual data to create synthetic parallel datasets. This process involves translating target-side monolingual data into synthetic source sentences using a machine translation (MT) system, which helps the MT decoder encounter well-formed sentences. Sennrich, Haddow, and Birch (2016a) highlighted the effectiveness of backtranslation in utilizing monolingual data for NMT, making it a crucial strategy for both low-resource and general MT scenarios.

Edunov et al. (2018a) demonstrated that backtranslation significantly boosts performance even on a large scale and in simulated low-resource settings. They emphasized the importance of using beam-search rather than sampling to generate backtranslations in these contexts. Furthermore, Caswell, Chelba, and Grangier (2019) found that tagging backtranslated data during training, to distinguish it from natural data, can enhance model performance.

Iterative backtranslation is another effective method for improving low-resource NMT quality. This approach involves using intermediate models of increasing quality in both language directions to successively create synthetic parallel data for subsequent steps. Although successive iterations yield diminishing returns, experimental evidence suggests that two iterations are often sufficient. Some researchers have further refined iterative backtranslation by incorporating a round-trip (autoencoder) objective for monolingual data, implicitly performing backtranslation and forward translation during training.

Despite the extensive literature on backtranslation and related techniques, and their proven efficacy in low-resource NMT, there are few systematic experimental studies specifically focusing on backtranslation for low-resource scenarios.",7,4,7,4,"Question:

How has backtranslation been adapted and improved for low-resource neural machine translation?

Answer:

One of the most successful strategies for leveraging monolingual data has been the creation of synthetic parallel data through translating monolingual texts either using a heuristic strategy or an intermediately trained MT model. Backtranslation corresponds to the scenario where target-side monolingual data is translated using an MT system to give corresponding synthetic source sentences, the idea being that it is particularly beneficial for the MT decoder to see well-formed sentences. For NMT, it was discovered that backtranslation was a remarkably effective way of exploiting monolingual data (Sennrich, Haddow, and Birch 2016a), and it remains an important technique, for both low-resource MT and MT in general. Edunov et al. (2018a) showed that backtranslation improved performance even at a very large scale, but also that it provided improvements in (simulated) low-resource settings, where it was important to use beam-search rather than sampling to create backtranslations. Caswell, Chelba, and Grangier (2019) showed that simply adding a tag to the backtranslated data during training, to let the model know which was back-translated data and which was natural data, could improve performance. Iterative backtranslation can be a particularly effective way of improving quality in low-resource NMT. The logical way to address the issue of poor quality initial models is to perform iterative backtranslation, whereby intermediate models of increasing quality in both language directions are successively used to create synthetic parallel data for the next step. This has been successfully applied to low-resource settings by several authors, although successive iterations offer diminishing returns, and often two iterations are sufficient, as has been shown experimentally. Other authors have sought to improve on iterative backtranslation by introducing a round-trip (i.e., autoencoder) objective for monolingual data, in other words performing backtranslation and forward translation implicitly during training. Despite the large body of literature on applying back-translation and related techniques, and evidence that it works in low-resource NMT, there are few systematic experimental studies of back-translation specifically for low-resource NMT.","Question:

How has backtranslation been adapted and improved for low-resource neural machine translation?

Answer:

Backtranslation has become a pivotal technique in enhancing low-resource neural machine translation (NMT) by leveraging monolingual data to create synthetic parallel datasets. This process involves translating target-side monolingual data into synthetic source sentences using a machine translation (MT) system, which helps the MT decoder encounter well-formed sentences. Sennrich, Haddow, and Birch (2016a) highlighted the effectiveness of backtranslation in utilizing monolingual data for NMT, making it a crucial strategy for both low-resource and general MT scenarios.

Edunov et al. (2018a) demonstrated that backtranslation significantly boosts performance even on a large scale and in simulated low-resource settings. They emphasized the importance of using beam-search rather than sampling to generate backtranslations in these contexts. Furthermore, Caswell, Chelba, and Grangier (2019) found that tagging backtranslated data during training, to distinguish it from natural data, can enhance model performance.

Iterative backtranslation is another effective method for improving low-resource NMT quality. This approach involves using intermediate models of increasing quality in both language directions to successively create synthetic parallel data for subsequent steps. Although successive iterations yield diminishing returns, experimental evidence suggests that two iterations are often sufficient. Some researchers have further refined iterative backtranslation by incorporating a round-trip (autoencoder) objective for monolingual data, implicitly performing backtranslation and forward translation during training.

Despite the extensive literature on backtranslation and related techniques, and their proven efficacy in low-resource NMT, there are few systematic experimental studies specifically focusing on backtranslation for low-resource scenarios.",NO,True,1871,True,True
237371890-s11,Survey of Low-Resource Machine Translation,2021-09-01,Unsupervised MT.,"The goal of unsupervised MT is to learn a translation model without any parallel data, so this can be considered an extreme form of low-resource MT. The first unsupervised NMT models (Lample et al. 2018a;Artetxe et al. 2018) were typically trained in a two-phase process: a rough translation system is first created by aligning word embeddings across the two languages (e.g. using bilingual seed lexicons), and then several rounds of iterative backtranslation and denoising autoencoding are used to further train the system. Whilst this approach has been successfully applied to high-resource language pairs (by ignoring available parallel data) it has been shown to perform poorly on genuine low-resource language pairs Marchisio, Duh, and Koehn 2020;Kim, Graça, and Ney 2020), mainly because the initial quality of the word embeddings and their cross-lingual alignments is poor (Edman, Toral, and van Noord 2020). The situation is somewhat improved using transfer learning from models trained on large amounts of monolingual data (Section 3.3), and some further gains can be achieved by adding a supervised training step with the limited parallel data (i.e. semi-supervised rather than unsupervised) (Bawden et al. 2019). However the performance remains limited, especially compared to high-resource language pairs. These negative results have focused researchers' attention on making unsupervised MT work better for low-resource languages. Chronopoulou, Stojanovski, and Fraser (2021) improved the cross-lingual alignment of word embeddings in order to get better results on unsupervised Macedonian-English and Albanian-English. A separate line of work is concerned with using corpora from other languages to improve unsupervised NMT (see Section 4.2.2) 3.2.3 Modification of existing parallel data. Another way in which language models have been used to generate synthetic parallel data is to synthesise parallel examples from new ones by replacing certain words. 15 In translation, it is important to maintain the relation of translation between the two sentences in the parallel pair when modification of the pair occurs. There are to our knowledge few works so far in this area. Fadaee, Bisazza, and Monz (2017) explore data augmentation for MT for a simulated low-resource setting (using English-German). They rely on bi-LSTM language models to predict plausible but rare equivalents of words in sentences. They then substitute in the rare words and replace the aligned word in the corresponding parallel sentence with its translation (obtained through a look-up in an SMT phrase table). They see improved BLEU scores (Papineni et al. 2002) and find that it is a complementary technique to backtranslation. More recently, Arthaud, Bawden, and Birch (2021) apply a similar technique to improve the adaptation of a model to new vocabulary for the low-resource translation direction Gujarati→English. They use a BERT language model to select training sentences that provide the appropriate context to substitute new and unseen words in order to create new synthetic parallel training sentences. While their work explores the trade-off between specialising to the new vocabulary and maintaining overall translation quality, they show that the approach can improve the translation of new words more effectively following data augmentation.",How have language models been utilized to generate synthetic parallel data for low-resource machine translation?,"Language models have been used to generate synthetic parallel data by synthesizing parallel examples from new ones through word replacement. Fadaee, Bisazza, and Monz (2017) explore data augmentation for machine translation in a simulated low-resource setting using English-German. They rely on bi-LSTM language models to predict plausible but rare equivalents of words in sentences. These rare words are then substituted, and the aligned word in the corresponding parallel sentence is replaced with its translation obtained through a look-up in an SMT phrase table. This approach has shown improved BLEU scores (Papineni et al. 2002) and is found to be complementary to backtranslation. More recently, Arthaud, Bawden, and Birch (2021) applied a similar technique to improve the adaptation of a model to new vocabulary for the low-resource translation direction Gujarati→English. They use a BERT language model to select training sentences that provide the appropriate context to substitute new and unseen words, creating new synthetic parallel training sentences. Their work explores the trade-off between specializing in the new vocabulary and maintaining overall translation quality, showing that the approach can improve the translation of new words more effectively following data augmentation.",In what ways have language models been employed to create synthetic parallel data for enhancing low-resource machine translation?,"Language models have been effectively utilized to generate synthetic parallel data by creating new parallel examples through word replacement techniques. Fadaee, Bisazza, and Monz (2017) investigated data augmentation for machine translation in a simulated low-resource environment, specifically focusing on English-German translation. They employed bi-LSTM language models to predict plausible yet rare word equivalents within sentences. These rare words were then substituted, and the corresponding word in the parallel sentence was replaced with its translation, obtained from a look-up in an SMT phrase table. This method demonstrated improved BLEU scores (Papineni et al. 2002) and was found to complement backtranslation strategies.

Building on this approach, Arthaud, Bawden, and Birch (2021) applied a similar technique to enhance model adaptation to new vocabulary in the low-resource translation direction of Gujarati→English. They utilized a BERT language model to select training sentences that offer the appropriate context for substituting new and unseen words, thereby creating new synthetic parallel training sentences. Their research delved into the balance between specializing in new vocabulary and maintaining overall translation quality, revealing that this approach can more effectively improve the translation of new words following data augmentation.",7,4,8,4,"Question:

How have language models been utilized to generate synthetic parallel data for low-resource machine translation?

Answer:

Language models have been used to generate synthetic parallel data by synthesizing parallel examples from new ones through word replacement. Fadaee, Bisazza, and Monz (2017) explore data augmentation for machine translation in a simulated low-resource setting using English-German. They rely on bi-LSTM language models to predict plausible but rare equivalents of words in sentences. These rare words are then substituted, and the aligned word in the corresponding parallel sentence is replaced with its translation obtained through a look-up in an SMT phrase table. This approach has shown improved BLEU scores (Papineni et al. 2002) and is found to be complementary to backtranslation. More recently, Arthaud, Bawden, and Birch (2021) applied a similar technique to improve the adaptation of a model to new vocabulary for the low-resource translation direction Gujarati→English. They use a BERT language model to select training sentences that provide the appropriate context to substitute new and unseen words, creating new synthetic parallel training sentences. Their work explores the trade-off between specializing in the new vocabulary and maintaining overall translation quality, showing that the approach can improve the translation of new words more effectively following data augmentation.","Question:

In what ways have language models been employed to create synthetic parallel data for enhancing low-resource machine translation?

Answer:

Language models have been effectively utilized to generate synthetic parallel data by creating new parallel examples through word replacement techniques. Fadaee, Bisazza, and Monz (2017) investigated data augmentation for machine translation in a simulated low-resource environment, specifically focusing on English-German translation. They employed bi-LSTM language models to predict plausible yet rare word equivalents within sentences. These rare words were then substituted, and the corresponding word in the parallel sentence was replaced with its translation, obtained from a look-up in an SMT phrase table. This method demonstrated improved BLEU scores (Papineni et al. 2002) and was found to complement backtranslation strategies.

Building on this approach, Arthaud, Bawden, and Birch (2021) applied a similar technique to enhance model adaptation to new vocabulary in the low-resource translation direction of Gujarati→English. They utilized a BERT language model to select training sentences that offer the appropriate context for substituting new and unseen words, thereby creating new synthetic parallel training sentences. Their research delved into the balance between specializing in new vocabulary and maintaining overall translation quality, revealing that this approach can more effectively improve the translation of new words following data augmentation.",NO,True,1375,True,True
237371890-s14,Survey of Low-Resource Machine Translation,2021-09-01,Transfer Learning,"In the earliest form of multilingual transfer learning for NMT, a parent model is trained on one language pair, and then the trained parameters are used to initialise a child model, which is then trained on the desired low-resource language pair.

This idea was first explored by Zoph et al. (2016), who considered a French-English parent model, and child models translating from 4 low-resource languages (Hausa, Turkish, Uzbek and Urdu) into English. They showed that transfer learning could indeed improve over random initialisation, and the best performance for this scenario was obtained when the values of the target embeddings were fixed after training the parent, but the training continued for all the other parameters. Zoph et al. (2016) suggest that the choice of the parent language could be important, but did not explore this further for their low-resource languages.

Whilst Zoph et al. (2016) treat the parent and child vocabularies as independent, Nguyen and Chiang (2017) showed that when transferring between related languages (in this case, within the Turkic family), it is beneficial to share the vocabularies between the parent and child models. To boost this effect, subword segmentation such as BPE (Sennrich, Haddow, and Birch 2016b) can help to further increase the vocabulary overlap. In cases where there is little vocabulary overlap (for example, because the languages are distantly related), mapping the bilingual embeddings between parent and child can help (Kim, Gao, and Ney 2019). In cases where the languages are highly related but are written in different scripts, transliteration may be used to increase the overlap in terms of the surface forms (Dabre et al. 2018;Goyal, Kumar, and Sharma 2020). Interestingly, in the case of transferring from a high-resource language pair into a low-resource one where the target language is a variant of the initial parent language Kumar et al. (2021) found it useful to pretrain embeddings externally and then fix them during the training of the parent, before initialising the embeddings of the low-resource language using those of the high-resource variant. Tested from English into Russian (transferring to Ukranian and Belarusian), Norwegian Bokmål (transferring to Nynorsk) and Arabic (transferring to four Arabic dialects), they hypothesise that decoupling the embeddings from training helps to avoid mismatch when transferring from the parent to the child target language.

The question of how to choose the parent language for transfer learning, as posed by Zoph et al. (2016), has been taken up by later authors. One study suggests that language relatedness is important (Dabre, Nakagawa, and Kazawa 2017). However Kocmi and Bojar (2018) showed that the main consideration in transfer learning is to have a strong parent model, and it can work well for unrelated language pairs. Still, if languages are unrelated and the scripts are different, for example transferring from an Arabic-Russian parent to Estonian-English, transfer learning is less useful. Lin et al. (2019) perform an extensive study on choosing the parent language for transfer learning, showing that data-related features of the parent models and lexical overlap are often more important than language similarity. Further insight into transfer learning for low-resource settings was provided by Aji et al. (2020), who analysed the training dynamics and concluded that the parent language is not important. The effectiveness from transfer learning with strong (but linguistically unrelated) parent models has been confirmed in shared task submissions such as Bawden et al. (2020) -see Section 8.2.4.

Multi-stage transfer learning methods have also been explored. Dabre, Fujita, and Chu (2019) propose a two-step transfer with English on the source side for both parent and child models. First, a one-to-one parent model is used to initialise weights in a multilingual one-to-many model, using a multi-way parallel corpora that includes the child target language. Second, the intermediate multilingual model is fine-tuned on parallel data between English and the child target language.  use a two-parent model and a pivot language. One parent model is between the child source language and the pivot language (e.g. German-English), and the other translates between the pivot and the child target language (e.g. English-Czech). Then, the encoder parameters from the first model and the decoder parameters of the second models are used to initialise the parameters of the child model (e.g. German-Czech).",How has the choice of parent language in transfer learning for NMT been explored in recent studies?,"In the earliest form of multilingual transfer learning for NMT, a parent model is trained on one language pair, and then the trained parameters are used to initialize a child model, which is then trained on the desired low-resource language pair. Zoph et al. (2016) suggest that the choice of the parent language could be important, but did not explore this further for their low-resource languages. Whilst Zoph et al. (2016) treat the parent and child vocabularies as independent, Nguyen and Chiang (2017) showed that when transferring between related languages (in this case, within the Turkic family), it is beneficial to share the vocabularies between the parent and child models. The question of how to choose the parent language for transfer learning, as posed by Zoph et al. (2016), has been taken up by later authors. One study suggests that language relatedness is important (Dabre, Nakagawa, and Kazawa 2017). However, Kocmi and Bojar (2018) showed that the main consideration in transfer learning is to have a strong parent model, and it can work well for unrelated language pairs. Lin et al. (2019) perform an extensive study on choosing the parent language for transfer learning, showing that data-related features of the parent models and lexical overlap are often more important than language similarity. Further insight into transfer learning for low-resource settings was provided by Aji et al. (2020), who analyzed the training dynamics and concluded that the parent language is not important. The effectiveness from transfer learning with strong (but linguistically unrelated) parent models has been confirmed in shared task submissions such as Bawden et al. (2020).",How have recent studies explored the choice of parent language in transfer learning for neural machine translation (NMT)?,"In the realm of multilingual transfer learning for NMT, the choice of parent language has been a subject of investigation in recent studies. Initially, Zoph et al. (2016) introduced a method where a parent model is trained on one language pair, and its parameters are used to initialize a child model for a low-resource language pair. They suggested that the choice of the parent language might be significant, although they did not delve deeper into this aspect for low-resource languages. While Zoph et al. treated the parent and child vocabularies as independent, Nguyen and Chiang (2017) demonstrated that sharing vocabularies between parent and child models is advantageous when transferring between related languages, such as those within the Turkic family.

Subsequent research has further explored the question posed by Zoph et al. regarding the selection of the parent language. Dabre, Nakagawa, and Kazawa (2017) proposed that language relatedness plays a crucial role. However, Kocmi and Bojar (2018) argued that the primary factor in transfer learning is having a robust parent model, which can be effective even for unrelated language pairs. Lin et al. (2019) conducted an extensive study on this topic, revealing that data-related features of the parent models and lexical overlap often outweigh language similarity in importance.

Further insights into transfer learning for low-resource settings were provided by Aji et al. (2020), who analyzed training dynamics and concluded that the parent language is not a critical factor. The effectiveness of transfer learning with strong, linguistically unrelated parent models has been corroborated in shared task submissions, such as those by Bawden et al. (2020).",7,4,7,7,"Question:

How has the choice of parent language in transfer learning for NMT been explored in recent studies?

Answer:

In the earliest form of multilingual transfer learning for NMT, a parent model is trained on one language pair, and then the trained parameters are used to initialize a child model, which is then trained on the desired low-resource language pair. Zoph et al. (2016) suggest that the choice of the parent language could be important, but did not explore this further for their low-resource languages. Whilst Zoph et al. (2016) treat the parent and child vocabularies as independent, Nguyen and Chiang (2017) showed that when transferring between related languages (in this case, within the Turkic family), it is beneficial to share the vocabularies between the parent and child models. The question of how to choose the parent language for transfer learning, as posed by Zoph et al. (2016), has been taken up by later authors. One study suggests that language relatedness is important (Dabre, Nakagawa, and Kazawa 2017). However, Kocmi and Bojar (2018) showed that the main consideration in transfer learning is to have a strong parent model, and it can work well for unrelated language pairs. Lin et al. (2019) perform an extensive study on choosing the parent language for transfer learning, showing that data-related features of the parent models and lexical overlap are often more important than language similarity. Further insight into transfer learning for low-resource settings was provided by Aji et al. (2020), who analyzed the training dynamics and concluded that the parent language is not important. The effectiveness from transfer learning with strong (but linguistically unrelated) parent models has been confirmed in shared task submissions such as Bawden et al. (2020).","Question:

How have recent studies explored the choice of parent language in transfer learning for neural machine translation (NMT)?

Answer:

In the realm of multilingual transfer learning for NMT, the choice of parent language has been a subject of investigation in recent studies. Initially, Zoph et al. (2016) introduced a method where a parent model is trained on one language pair, and its parameters are used to initialize a child model for a low-resource language pair. They suggested that the choice of the parent language might be significant, although they did not delve deeper into this aspect for low-resource languages. While Zoph et al. treated the parent and child vocabularies as independent, Nguyen and Chiang (2017) demonstrated that sharing vocabularies between parent and child models is advantageous when transferring between related languages, such as those within the Turkic family.

Subsequent research has further explored the question posed by Zoph et al. regarding the selection of the parent language. Dabre, Nakagawa, and Kazawa (2017) proposed that language relatedness plays a crucial role. However, Kocmi and Bojar (2018) argued that the primary factor in transfer learning is having a robust parent model, which can be effective even for unrelated language pairs. Lin et al. (2019) conducted an extensive study on this topic, revealing that data-related features of the parent models and lexical overlap often outweigh language similarity in importance.

Further insights into transfer learning for low-resource settings were provided by Aji et al. (2020), who analyzed training dynamics and concluded that the parent language is not a critical factor. The effectiveness of transfer learning with strong, linguistically unrelated parent models has been corroborated in shared task submissions, such as those by Bawden et al. (2020).",NO,True,1723,True,True
237371890-s15,Survey of Low-Resource Machine Translation,2021-09-01,Multilingual Models,"The goal of multilingual MT is to have a universal model capable of translation between any two languages. Including low-resource language pairs in multilingual models can be seen as means of exploiting additional data from other, possibly related, languages. Having more languages in the training data helps developing a universal representation space, which in turn allows for some level of parameter sharing among the languagespecific model components.

The degree to which parameters are shared across multiple language directions varies considerably in the literature, with early models showing little sharing across languages (Dong et al. 2015) and some later models exploring the sharing of most or all parameters (Johnson et al. 2017). The amount of parameter sharing can be seen as a trade-off between ensuring that each language is sufficiently represented (has enough parameters allocated) and that low-resource languages can benefit from the joint training of parameters with other (higher-resource) language pairs (which also importantly reduces the complexity of the model by reducing the number of parameters required). Dong et al. (2015) present one of the earliest studies in multilingual NMT, focused on translation from a single language into multiple languages simultaneously. The central idea of this approach is to have a shared encoder and many language-specific decoders, including language-specific weights in the attention modules. By training on multiple target languages (presenting as a multi-task setup), the motivation is that the representation of the source language will not only be trained on more data (thanks to the multiple language pairs), but the representation may be more universal, since it is being used to decode several languages. They find that the multi-decoder setup provides systematic gains over the bilingual counterparts, although the model was only tested in simulated low-resource settings.

As an extension to this method, Firat, Cho, and Bengio (2016) experiment with multilingual models in the many-to-many scenario. They too use separate encoders and decoders for each language, but the attention mechanism is shared across all directions, which means adding languages increases the number of model parameters linearly (as opposed to quadratic increase when attention is language-direction-specific). In all cases, the multi-task model performed better than the bilingual models according to BLEU scores, although it was again only tested in simulated low-resource scenarios.

More recent work has looked into the benefits of sharing only certain parts of multilingual models, ensuring language-dependent components. For example, Platanios et al. (2018) present a contextual parameter generator component, which allows finer control of the parameter sharing across different languages. Fan et al. (2021) also include language-specific components by sharing certain parameters across pre-defined language groups in order to efficiently and effectively upscale the number of languages included (see Section 4.2.1).

In a bid to both simplify the model (also reducing the number of parameters) and to maximally encourage sharing between languages, Ha, Niehues, and Waibel (2016) and Johnson et al. (2017) proposed to use a single encoder and decoder to train all language directions (known as the universal encoder-decoder). Whereas Ha, Niehues, and Waibel (2016) propose language-specific embeddings, Johnson et al. (2017) use a joint vocabulary over all languages included, which has the advantage of allowing shared lexical representations (and ultimately this second strategy is the one that has been retained by the community). The control over the target language was ensured in both cases by including pseudo-tokens indicating the target language in the source sentence. Although not trained or evaluated on low-resource language pairs, the model by Johnson et al. (2017) showed promise in terms of the ability to model multilingual translation with a universal model, and zero-shot translation (between language directions for which no parallel training data was provided) was also shown to be possible. The model was later shown to bring improvements when dealing with translation into several low-resource language varieties (Lakew, Erofeeva, and Federico 2018), a particular type of multilingual MT where the several target languages are very similar. We shall see in the next section (Section 4.2.1) how scaling up the number of languages used for training can be beneficial in the low-resource setting.

Combining multilingual models, with the transfer learning approaches of the previous section, Neubig and Hu (2018) present a number of approaches for adaptation of multilingual models to new languages. The authors consider cold-and warm-start scenarios, depending on whether the training data for the new language was available for training the original multilingual model. They find that multilingual models finetuned with the low-resource language training data mixed in with data from a similar high-resource language (i.e. similar-language regularisation) give the best translation performance.",How does parameter sharing impact the performance and complexity of multilingual machine translation models?,"The degree to which parameters are shared across multiple language directions varies considerably in the literature, with early models showing little sharing across languages (Dong et al. 2015) and some later models exploring the sharing of most or all parameters (Johnson et al. 2017). The amount of parameter sharing can be seen as a trade-off between ensuring that each language is sufficiently represented (has enough parameters allocated) and that low-resource languages can benefit from the joint training of parameters with other (higher-resource) language pairs (which also importantly reduces the complexity of the model by reducing the number of parameters required). In a bid to both simplify the model (also reducing the number of parameters) and to maximally encourage sharing between languages, Ha, Niehues, and Waibel (2016) and Johnson et al. (2017) proposed to use a single encoder and decoder to train all language directions (known as the universal encoder-decoder). Whereas Ha, Niehues, and Waibel (2016) propose language-specific embeddings, Johnson et al. (2017) use a joint vocabulary over all languages included, which has the advantage of allowing shared lexical representations. The control over the target language was ensured in both cases by including pseudo-tokens indicating the target language in the source sentence. Although not trained or evaluated on low-resource language pairs, the model by Johnson et al. (2017) showed promise in terms of the ability to model multilingual translation with a universal model, and zero-shot translation (between language directions for which no parallel training data was provided) was also shown to be possible.",What is the impact of parameter sharing on the performance and complexity of multilingual machine translation models?,"Parameter sharing in multilingual machine translation models significantly influences both their performance and complexity. The extent of parameter sharing across different language directions has evolved over time. Early models, such as those by Dong et al. (2015), exhibited minimal parameter sharing between languages. In contrast, later models, like those by Johnson et al. (2017), explored extensive sharing, even proposing the sharing of most or all parameters.

The degree of parameter sharing presents a trade-off. On one hand, it is crucial to allocate enough parameters to adequately represent each language. On the other hand, sharing parameters can benefit low-resource languages by leveraging joint training with higher-resource language pairs. This approach also reduces model complexity by decreasing the total number of parameters required.

To address these challenges, Ha, Niehues, and Waibel (2016), along with Johnson et al. (2017), proposed the use of a universal encoder-decoder, employing a single encoder and decoder for all language directions. Ha, Niehues, and Waibel (2016) suggested using language-specific embeddings, while Johnson et al. (2017) advocated for a joint vocabulary across all languages, facilitating shared lexical representations. Both approaches controlled the target language by incorporating pseudo-tokens in the source sentence to indicate the desired target language.

Although Johnson et al. (2017) did not specifically train or evaluate their model on low-resource language pairs, their approach demonstrated potential for modeling multilingual translation with a universal model. Additionally, it enabled zero-shot translation, allowing translation between language directions without parallel training data.",8,4,7,4,"Question:

How does parameter sharing impact the performance and complexity of multilingual machine translation models?

Answer:

The degree to which parameters are shared across multiple language directions varies considerably in the literature, with early models showing little sharing across languages (Dong et al. 2015) and some later models exploring the sharing of most or all parameters (Johnson et al. 2017). The amount of parameter sharing can be seen as a trade-off between ensuring that each language is sufficiently represented (has enough parameters allocated) and that low-resource languages can benefit from the joint training of parameters with other (higher-resource) language pairs (which also importantly reduces the complexity of the model by reducing the number of parameters required). In a bid to both simplify the model (also reducing the number of parameters) and to maximally encourage sharing between languages, Ha, Niehues, and Waibel (2016) and Johnson et al. (2017) proposed to use a single encoder and decoder to train all language directions (known as the universal encoder-decoder). Whereas Ha, Niehues, and Waibel (2016) propose language-specific embeddings, Johnson et al. (2017) use a joint vocabulary over all languages included, which has the advantage of allowing shared lexical representations. The control over the target language was ensured in both cases by including pseudo-tokens indicating the target language in the source sentence. Although not trained or evaluated on low-resource language pairs, the model by Johnson et al. (2017) showed promise in terms of the ability to model multilingual translation with a universal model, and zero-shot translation (between language directions for which no parallel training data was provided) was also shown to be possible.","Question:

What is the impact of parameter sharing on the performance and complexity of multilingual machine translation models?

Answer:

Parameter sharing in multilingual machine translation models significantly influences both their performance and complexity. The extent of parameter sharing across different language directions has evolved over time. Early models, such as those by Dong et al. (2015), exhibited minimal parameter sharing between languages. In contrast, later models, like those by Johnson et al. (2017), explored extensive sharing, even proposing the sharing of most or all parameters.

The degree of parameter sharing presents a trade-off. On one hand, it is crucial to allocate enough parameters to adequately represent each language. On the other hand, sharing parameters can benefit low-resource languages by leveraging joint training with higher-resource language pairs. This approach also reduces model complexity by decreasing the total number of parameters required.

To address these challenges, Ha, Niehues, and Waibel (2016), along with Johnson et al. (2017), proposed the use of a universal encoder-decoder, employing a single encoder and decoder for all language directions. Ha, Niehues, and Waibel (2016) suggested using language-specific embeddings, while Johnson et al. (2017) advocated for a joint vocabulary across all languages, facilitating shared lexical representations. Both approaches controlled the target language by incorporating pseudo-tokens in the source sentence to indicate the desired target language.

Although Johnson et al. (2017) did not specifically train or evaluate their model on low-resource language pairs, their approach demonstrated potential for modeling multilingual translation with a universal model. Additionally, it enabled zero-shot translation, allowing translation between language directions without parallel training data.",NO,True,1761,True,True
237371890-s21,Survey of Low-Resource Machine Translation,2021-09-01,Morphological segmentation.,"A crucial part of training NMT system is the choice of subword segmentation, a pre-processing technique providing the ability to represent an infinite vocabulary with a fixed number of units and to better generalise over shorter units. For low-resource languages, it is even more important because there is a greater chance of coming across words that were not seen at training time. The most commonly used strategies are statistics-based, such as BPE (Sennrich, Haddow, and Birch 2016b) and sentencepiece (Kudo and Richardson 2018). Not only might these strategies not be optimal from a point of view of linguistic generalisation, but for low-resource languages especially they have also been shown to give highly variable results, depending on what degree of segmentation is selected; this degree is a parameter which therefore must be chosen wisely (Ding, Renduchintala, and Duh 2019;Sennrich and Zhang 2019).

Works exploring linguistic subword segmentation go back to statistical MT (Oflazer, Durgar El-Kahlout, and Durgar El-Kahlout 2007;Goldwater and McClosky 2005). Much of the focus has been on morphologically rich languages, with high degrees of inflection and/or compounding, for example for German, where minor gains can be seen over standard BPE (Huck, Riess, and Fraser 2017). Specifically for low-resource languages, several works have tested the use of morphological analysers to assist the segmentation of texts into more meaningful units. In their submission to the WMT19 shared task for English→Kazakh, Sánchez-Cartagena, Pérez-Ortiz, and Sánchez-Martínez (2019) use the morphological analyser from Apertium (Forcada and Tyers 2016) to segment Kazakh words into stem (often corresponding to the lemma in Kazakh) and the remainder of the word. They then learnt BPE over the morphological segmented data. Ortega, Castro Mamani, and Cho (2020) also use a BPE approach, guided by a list of suffixes, which are provided to the algorithm and are not segmented. They see better performance than using Morfessor or standard BPE. Saleva and Lignos (2021) also test morphologically aware subword segmentation for three low-resource language pairs: Nepali, Sinhala and Kazakh to and from English. They test segmentations using the LMVR (Ataman et al. 2017) and MORSEL (Lignos 2010) analysers, but found no gain over BPE and no consistent pattern in the results. These results go against previous results from Grönroos et al. (2014) that showed that an LMVR segmentation can outperform BPE when handling low-resource Turkish, but they are in accordance with more recent ones for Kazakh-English (Toral et al. 2019) and Tamil-English (Dhar, Bisazza, and van Noord 2020), where it does not seem to improve over BPE. (Garcia-Martinez, Barrault, and Bougares 2016;Sennrich and Haddow 2016;Burlot et al. 2017) were designed as a way of decomposing word units into component parts, which can help to provide some level of composite abstraction from the original wordform. For example, a wordform may be represented by its lemma and its part-of-speech, which together can be used to recover the original surface form. This type of modelling can be particularly useful for morphologically rich languages (many of which are already low-resource), for which the large number of surface forms can result in greater data sparsity and normally necessitate greater quantities of data.","How have morphological analyzers been utilized in subword segmentation for low-resource languages, and what are the outcomes?","Several works have tested the use of morphological analyzers to assist the segmentation of texts into more meaningful units for low-resource languages. In their submission to the WMT19 shared task for English→Kazakh, Sánchez-Cartagena, Pérez-Ortiz, and Sánchez-Martínez (2019) used the morphological analyzer from Apertium (Forcada and Tyers 2016) to segment Kazakh words into stem (often corresponding to the lemma in Kazakh) and the remainder of the word. They then learned BPE over the morphologically segmented data. Ortega, Castro Mamani, and Cho (2020) also used a BPE approach, guided by a list of suffixes, which are provided to the algorithm and are not segmented, and they saw better performance than using Morfessor or standard BPE. Saleva and Lignos (2021) tested morphologically aware subword segmentation for three low-resource language pairs: Nepali, Sinhala, and Kazakh to and from English. They tested segmentations using the LMVR (Ataman et al. 2017) and MORSEL (Lignos 2010) analyzers but found no gain over BPE and no consistent pattern in the results. These results go against previous results from Grönroos et al. (2014) that showed that an LMVR segmentation can outperform BPE when handling low-resource Turkish, but they are in accordance with more recent ones for Kazakh-English (Toral et al. 2019) and Tamil-English (Dhar, Bisazza, and van Noord 2020), where it does not seem to improve over BPE.","How have morphological analyzers been used in subword segmentation for low-resource languages, and what have been the results?","Morphological analyzers have been explored as tools to enhance subword segmentation in low-resource languages, with varying outcomes. For instance, in the WMT19 shared task for English→Kazakh, Sánchez-Cartagena, Pérez-Ortiz, and Sánchez-Martínez (2019) employed the Apertium morphological analyzer (Forcada and Tyers 2016) to segment Kazakh words into stems and the remaining parts. They then applied Byte Pair Encoding (BPE) on this morphologically segmented data. Similarly, Ortega, Castro Mamani, and Cho (2020) utilized a BPE approach guided by a list of suffixes, which were preserved during segmentation, achieving better performance compared to using Morfessor or standard BPE.

However, the results are not always consistent. Saleva and Lignos (2021) investigated morphologically aware subword segmentation for language pairs such as Nepali, Sinhala, and Kazakh with English. They used the LMVR (Ataman et al. 2017) and MORSEL (Lignos 2010) analyzers but found no improvement over BPE and no consistent pattern in the results. This contrasts with earlier findings by Grönroos et al. (2014), which indicated that LMVR segmentation could outperform BPE for low-resource Turkish. Nonetheless, the findings align with more recent studies for Kazakh-English (Toral et al. 2019) and Tamil-English (Dhar, Bisazza, and van Noord 2020), where morphological segmentation did not show advantages over BPE.",7,4,7,4,"Question:

How have morphological analyzers been utilized in subword segmentation for low-resource languages, and what are the outcomes?

Answer:

Several works have tested the use of morphological analyzers to assist the segmentation of texts into more meaningful units for low-resource languages. In their submission to the WMT19 shared task for English→Kazakh, Sánchez-Cartagena, Pérez-Ortiz, and Sánchez-Martínez (2019) used the morphological analyzer from Apertium (Forcada and Tyers 2016) to segment Kazakh words into stem (often corresponding to the lemma in Kazakh) and the remainder of the word. They then learned BPE over the morphologically segmented data. Ortega, Castro Mamani, and Cho (2020) also used a BPE approach, guided by a list of suffixes, which are provided to the algorithm and are not segmented, and they saw better performance than using Morfessor or standard BPE. Saleva and Lignos (2021) tested morphologically aware subword segmentation for three low-resource language pairs: Nepali, Sinhala, and Kazakh to and from English. They tested segmentations using the LMVR (Ataman et al. 2017) and MORSEL (Lignos 2010) analyzers but found no gain over BPE and no consistent pattern in the results. These results go against previous results from Grönroos et al. (2014) that showed that an LMVR segmentation can outperform BPE when handling low-resource Turkish, but they are in accordance with more recent ones for Kazakh-English (Toral et al. 2019) and Tamil-English (Dhar, Bisazza, and van Noord 2020), where it does not seem to improve over BPE.","Question:

How have morphological analyzers been used in subword segmentation for low-resource languages, and what have been the results?

Answer:

Morphological analyzers have been explored as tools to enhance subword segmentation in low-resource languages, with varying outcomes. For instance, in the WMT19 shared task for English→Kazakh, Sánchez-Cartagena, Pérez-Ortiz, and Sánchez-Martínez (2019) employed the Apertium morphological analyzer (Forcada and Tyers 2016) to segment Kazakh words into stems and the remaining parts. They then applied Byte Pair Encoding (BPE) on this morphologically segmented data. Similarly, Ortega, Castro Mamani, and Cho (2020) utilized a BPE approach guided by a list of suffixes, which were preserved during segmentation, achieving better performance compared to using Morfessor or standard BPE.

However, the results are not always consistent. Saleva and Lignos (2021) investigated morphologically aware subword segmentation for language pairs such as Nepali, Sinhala, and Kazakh with English. They used the LMVR (Ataman et al. 2017) and MORSEL (Lignos 2010) analyzers but found no improvement over BPE and no consistent pattern in the results. This contrasts with earlier findings by Grönroos et al. (2014), which indicated that LMVR segmentation could outperform BPE for low-resource Turkish. Nonetheless, the findings align with more recent studies for Kazakh-English (Toral et al. 2019) and Tamil-English (Dhar, Bisazza, and van Noord 2020), where morphological segmentation did not show advantages over BPE.",NO,True,1402,True,True
237371890-s22,Survey of Low-Resource Machine Translation,2021-09-01,Factored models. Factored source and target representations,"Factored models originated in SMT , but were notably not easy to scale. The advantage of factored representations in NMT is that the factors are represented in continuous space and therefore may be combined more easily, without resulting in an explosion in the number of calculations necessary. Garcia-Martinez, Barrault, and Bougares (2016), Sennrich and Haddow (2016) and Burlot et al. (2017) evaluate on language pairs involving at least one morphologically rich language and show that improvements in translation quality can be seen, but this is dependent on the language pair and the type of linguistic information included in the factors. Nȃdejde et al. (2017) use factors to integrate source-side syntactic information in the form of CCG tags (Steedman 2000;Clark and Curran 2007), which they combine with an interleaving approach on the target side (see Section 5.1.4) to significantly improve MT performance, for high-resource (German→English) and mid-low-resource (Romanian→English) language directions.","How do factored models improve translation quality in NMT, and what factors influence their effectiveness?","Factored models originated in SMT, but were notably not easy to scale. The advantage of factored representations in NMT is that the factors are represented in continuous space and therefore may be combined more easily, without resulting in an explosion in the number of calculations necessary. Garcia-Martinez, Barrault, and Bougares (2016), Sennrich and Haddow (2016), and Burlot et al. (2017) evaluate on language pairs involving at least one morphologically rich language and show that improvements in translation quality can be seen, but this is dependent on the language pair and the type of linguistic information included in the factors. Nȃdejde et al. (2017) use factors to integrate source-side syntactic information in the form of CCG tags (Steedman 2000; Clark and Curran 2007), which they combine with an interleaving approach on the target side to significantly improve MT performance, for high-resource (German→English) and mid-low-resource (Romanian→English) language directions.","How do factored models enhance translation quality in neural machine translation (NMT), and what factors affect their effectiveness?","Factored models, which originated in statistical machine translation (SMT), were initially challenging to scale. However, in the context of neural machine translation (NMT), factored representations offer a significant advantage. They are represented in continuous space, allowing for easier combination without leading to a computational explosion. Studies by Garcia-Martinez, Barrault, and Bougares (2016), Sennrich and Haddow (2016), and Burlot et al. (2017) have demonstrated that factored models can improve translation quality, particularly for language pairs involving at least one morphologically rich language. The effectiveness of these improvements depends on the specific language pair and the type of linguistic information included in the factors. Additionally, Nȃdejde et al. (2017) have shown that integrating source-side syntactic information using factors, such as CCG tags (Steedman 2000; Clark and Curran 2007), and combining them with an interleaving approach on the target side, can significantly enhance machine translation performance for both high-resource (German→English) and mid-low-resource (Romanian→English) language directions.",8,4,7,4,"Question:

How do factored models improve translation quality in NMT, and what factors influence their effectiveness?

Answer:

Factored models originated in SMT, but were notably not easy to scale. The advantage of factored representations in NMT is that the factors are represented in continuous space and therefore may be combined more easily, without resulting in an explosion in the number of calculations necessary. Garcia-Martinez, Barrault, and Bougares (2016), Sennrich and Haddow (2016), and Burlot et al. (2017) evaluate on language pairs involving at least one morphologically rich language and show that improvements in translation quality can be seen, but this is dependent on the language pair and the type of linguistic information included in the factors. Nȃdejde et al. (2017) use factors to integrate source-side syntactic information in the form of CCG tags (Steedman 2000; Clark and Curran 2007), which they combine with an interleaving approach on the target side to significantly improve MT performance, for high-resource (German→English) and mid-low-resource (Romanian→English) language directions.","Question:

How do factored models enhance translation quality in neural machine translation (NMT), and what factors affect their effectiveness?

Answer:

Factored models, which originated in statistical machine translation (SMT), were initially challenging to scale. However, in the context of neural machine translation (NMT), factored representations offer a significant advantage. They are represented in continuous space, allowing for easier combination without leading to a computational explosion. Studies by Garcia-Martinez, Barrault, and Bougares (2016), Sennrich and Haddow (2016), and Burlot et al. (2017) have demonstrated that factored models can improve translation quality, particularly for language pairs involving at least one morphologically rich language. The effectiveness of these improvements depends on the specific language pair and the type of linguistic information included in the factors. Additionally, Nȃdejde et al. (2017) have shown that integrating source-side syntactic information using factors, such as CCG tags (Steedman 2000; Clark and Curran 2007), and combining them with an interleaving approach on the target side, can significantly enhance machine translation performance for both high-resource (German→English) and mid-low-resource (Romanian→English) language directions.",NO,True,1159,True,True
237371890-s26,Survey of Low-Resource Machine Translation,2021-09-01,Bilingual lexicons,"Bilingual lexicons are lists of terms (words or phrases) in one language associated with their translations in a second language. The advantage of bilingual lexicons is that they may well provide specialist or infrequent terms that do not appear in available parallel data, with the downside that they do not give information about the translation of terms in context, notably when there are several possible translations of a same term. However, they may be important resources to exploit, since they provide complementary information to parallel data and may be more readily available and cheaper to produce. 16 The approaches developed so far to exploit bilingual lexicons to directly improve NMT can be summarised as follows: (i) as seed lexicons to initialise unsupervised MT (Lample et al. 2018b;Duan et al. 2020) (as described in Section 3.2.2), (ii) as an additional scoring component, particularly to provide coverage for rare of otherwise unseen vocabulary (Arthur, Neubig, and Nakamura 2016; Feng et al. 2017) and (iii) as annotations in the source sentence by adding translations from lexicons just after their corresponding source words (Dinu et al. 2019) 17 or by replacing them in a codeswitching-style setup (Song et al. 2019a).

The most recent work on using lexicons in pretrained multilingual models (Lin et al. 2020, mRASP) shows the most promise. Here translations of words are substituted into the source sentence in pretraining, with the goal of bringing words with similar meanings across multiple languages closer in the representation space. Please see Section 4.3 for more details.",How have bilingual lexicons been utilized to enhance neural machine translation models?,"Bilingual lexicons are lists of terms (words or phrases) in one language associated with their translations in a second language. They may provide specialist or infrequent terms that do not appear in available parallel data, offering complementary information to parallel data and potentially being more readily available and cheaper to produce. The approaches developed to exploit bilingual lexicons to directly improve NMT include: (i) using them as seed lexicons to initialize unsupervised MT (Lample et al. 2018b; Duan et al. 2020), (ii) employing them as an additional scoring component to provide coverage for rare or otherwise unseen vocabulary (Arthur, Neubig, and Nakamura 2016; Feng et al. 2017), and (iii) using them as annotations in the source sentence by adding translations from lexicons just after their corresponding source words (Dinu et al. 2019) or by replacing them in a code-switching-style setup (Song et al. 2019a). The most recent work on using lexicons in pretrained multilingual models (Lin et al. 2020, mRASP) shows promise by substituting translations of words into the source sentence in pretraining, aiming to bring words with similar meanings across multiple languages closer in the representation space.",In what ways have bilingual lexicons been used to improve neural machine translation models?,"Bilingual lexicons, which are lists of terms in one language paired with their translations in another, have been effectively utilized to enhance neural machine translation (NMT) models. These lexicons are particularly valuable for providing translations of specialist or infrequent terms that might not be present in available parallel data. They offer complementary information to parallel data and are often more accessible and cost-effective to produce. Several approaches have been developed to leverage bilingual lexicons for improving NMT. One method involves using them as seed lexicons to initialize unsupervised machine translation (Lample et al. 2018b; Duan et al. 2020). Another approach incorporates them as an additional scoring component to ensure coverage for rare or unseen vocabulary (Arthur, Neubig, and Nakamura 2016; Feng et al. 2017). Additionally, lexicons can be used as annotations in the source sentence by adding translations directly after their corresponding source words (Dinu et al. 2019) or by replacing them in a code-switching-style setup (Song et al. 2019a). Recent advancements in using lexicons within pretrained multilingual models, such as mRASP (Lin et al. 2020), show promise by substituting translations of words into the source sentence during pretraining. This approach aims to bring words with similar meanings across multiple languages closer in the representation space, thereby enhancing translation quality.",7,4,7,4,"Question:

How have bilingual lexicons been utilized to enhance neural machine translation models?

Answer:

Bilingual lexicons are lists of terms (words or phrases) in one language associated with their translations in a second language. They may provide specialist or infrequent terms that do not appear in available parallel data, offering complementary information to parallel data and potentially being more readily available and cheaper to produce. The approaches developed to exploit bilingual lexicons to directly improve NMT include: (i) using them as seed lexicons to initialize unsupervised MT (Lample et al. 2018b; Duan et al. 2020), (ii) employing them as an additional scoring component to provide coverage for rare or otherwise unseen vocabulary (Arthur, Neubig, and Nakamura 2016; Feng et al. 2017), and (iii) using them as annotations in the source sentence by adding translations from lexicons just after their corresponding source words (Dinu et al. 2019) or by replacing them in a code-switching-style setup (Song et al. 2019a). The most recent work on using lexicons in pretrained multilingual models (Lin et al. 2020, mRASP) shows promise by substituting translations of words into the source sentence in pretraining, aiming to bring words with similar meanings across multiple languages closer in the representation space.","Question:

In what ways have bilingual lexicons been used to improve neural machine translation models?

Answer:

Bilingual lexicons, which are lists of terms in one language paired with their translations in another, have been effectively utilized to enhance neural machine translation (NMT) models. These lexicons are particularly valuable for providing translations of specialist or infrequent terms that might not be present in available parallel data. They offer complementary information to parallel data and are often more accessible and cost-effective to produce. Several approaches have been developed to leverage bilingual lexicons for improving NMT. One method involves using them as seed lexicons to initialize unsupervised machine translation (Lample et al. 2018b; Duan et al. 2020). Another approach incorporates them as an additional scoring component to ensure coverage for rare or unseen vocabulary (Arthur, Neubig, and Nakamura 2016; Feng et al. 2017). Additionally, lexicons can be used as annotations in the source sentence by adding translations directly after their corresponding source words (Dinu et al. 2019) or by replacing them in a code-switching-style setup (Song et al. 2019a). Recent advancements in using lexicons within pretrained multilingual models, such as mRASP (Lin et al. 2020), show promise by substituting translations of words into the source sentence during pretraining. This approach aims to bring words with similar meanings across multiple languages closer in the representation space, thereby enhancing translation quality.",NO,True,1456,True,True
237371890-s32,Survey of Low-Resource Machine Translation,2021-09-01,Rule-based approaches,"Rule-based machine translation (RBMT) consists in analysing and transforming a source text into a translation by applying a set of hand-coded linguistically motivated rules. This was the oldest and the most common paradigm for machine translation before being largely supplanted by corpus-based approaches such as phrase-based statistical machine translation and neural machine translation, which usually outperform it on both accuracy and fluency, especially when translating between language pairs where at least one language is high-resource, such as English. However, rule-based techniques can still be successfully applied to the task of translation between closely related languages.

Modern implementations, such as the Apertium system (Forcada et al. 2011;Forcada and Tyers 2016;Khanna et al. 2021), use lexical translation and shallow transfer rules that avoid full parsing and instead exploit the similarities between the source and target language to restructure a sentence into its translation. This approach has been applied to various language pairs, especially in the Western Romance and the South Slavic sub-families. NMT approaches tend to have better fluency than RBMT but they can produce hallucinations in low-resource settings where RBMT can instead benefit from lexical translation with explicit bilingual dictionaries, thus a line of research has developed that attempts to combine both approaches. For instance Sánchez-Cartagena,  used multi-source Transformer and deep-GRU (Miceli Barone et al. 2017) models to post-edit translations produced by a RBMT system for the Breton-French language pair.

One of the main drawbacks of RBMT is that it requires substantial language-specific resources and expertise which might not be available for all low-resource languages. See Section 5 for a discussion of other methods to use various linguistic resources that might be more readily available.","How do modern implementations of rule-based machine translation, like Apertium, handle translation between closely related languages?","Modern implementations of rule-based machine translation, such as the Apertium system, use lexical translation and shallow transfer rules that avoid full parsing. Instead, they exploit the similarities between the source and target language to restructure a sentence into its translation. This approach has been applied to various language pairs, especially in the Western Romance and the South Slavic sub-families.","How do modern rule-based machine translation systems, like Apertium, manage translations between languages that are closely related?","Modern rule-based machine translation systems, such as Apertium, handle translations between closely related languages by utilizing lexical translation and shallow transfer rules. These systems avoid full parsing and instead capitalize on the similarities between the source and target languages to restructure sentences into their translations. This method has been effectively applied to various language pairs, particularly within the Western Romance and South Slavic language families.",7,2,7,2,"Question:

How do modern implementations of rule-based machine translation, like Apertium, handle translation between closely related languages?

Answer:

Modern implementations of rule-based machine translation, such as the Apertium system, use lexical translation and shallow transfer rules that avoid full parsing. Instead, they exploit the similarities between the source and target language to restructure a sentence into its translation. This approach has been applied to various language pairs, especially in the Western Romance and the South Slavic sub-families.","Question:

How do modern rule-based machine translation systems, like Apertium, manage translations between languages that are closely related?

Answer:

Modern rule-based machine translation systems, such as Apertium, handle translations between closely related languages by utilizing lexical translation and shallow transfer rules. These systems avoid full parsing and instead capitalize on the similarities between the source and target languages to restructure sentences into their translations. This method has been effectively applied to various language pairs, particularly within the Western Romance and South Slavic language families.",NO,True,489,True,True
237371890-s33,Survey of Low-Resource Machine Translation,2021-09-01,Evaluation,"As researchers build different MT systems in order to try out new ideas, how do they know whether one is better than another? If a system developer wants to deploy an MT system, how do they know which is the best? Answering these questions is the goal of MT evaluation -to provide a quantitative estimate of the quality of an MT system's output. MT evaluation is a difficult problem, since there can be many possible correct translations of a given source sentence. The intended use is an important consideration in evaluation; if translation is mainly for assimilation (gisting) then adequacy is of primary importance and errors in fluency can be tolerated; but if the translation is for dissemination (with post-editing) then errors in meaning can be corrected, but the translation should be as close to a publishable form as possible. Evaluation is not specific to low-resource MT; it is a problem for all types of MT research, but low-resource language pairs can present specific difficulties for evaluation.

Evaluation can either be manual (using human judgements) or automatic (using software). Human judgements are generally considered the ""gold standard"" for MT evaluation because, ultimately, the translation is intended to be consumed by humans. The annual WMT shared tasks have employed human evaluation every year since they started in 2006, and the organisers argue that (Callison-Burch et al. 2007):

While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are an imperfect substitute for human assessment of translation quality.

Human evaluation is of course much more time-consuming (and therefore expensive) than automatic evaluation, and so can only be used to compare a small number of variants; with most of the system selection performed by automatic evaluation. For low-resource MT, the potential difficulty with human evaluation is connecting the researchers with the evaluators. Some low-resource languages have very small language communities, so the pool of potential evaluators is small, whilst in other cases the researchers may not be well connected with the language community -in that case the answer should be for the researchers to engage more with the community (Nekoto et al. 2020).

Most of the evaluation in MT is performed using automatic metrics, but when these metrics are developed they need to be validated against human judgements as the gold standard. An important source of gold standard data for validation of metrics is the series of WMT metrics shared tasks (Freitag et al. 2021b). However this data covers the language pairs used in the WMT news tasks, whose coverage of low-resource languages is limited to those listed in Table 3. It is unclear how well conclusions about the utility of metrics will transfer from high-resource languages to low-resource languages.

Automatic metrics are nearly always reference-based, in other words they work by comparing the MT hypothesis with a human-produced reference. This means that references need to be available for the chosen language pair, they should be of good quality, and ideally should be established benchmarks used by the research community. Such references are in short supply for low-resource language pairs (Section 2), and when available may be in the wrong domain, small, or of poor quality.

Looking at the automatic metrics in common use in current MT research, we see two main types of metrics being used in current research: string-based metrics (e.g. BLEU (Papineni et al. 2002;Post 2018), ChrF (Popović 2015) etc.) and embedding-based metrics (e.g. BERTscore , COMET (Rei et al. 2020), BLEURT (Sellam, Das, and Parikh 2020) etc.). The string-based metrics are ""low-resource metrics"", since they do not require any resources beyond tokenisation/segmentation, whereas the embeddingbased metrics require more significant resources such as pre-trained sentence embeddings, and often need human judgements for fine-tuning. The embedding-based metrics could be considered successors to metrics like METEOR (Denkowski and Lavie 2014), which uses synonym list to improve matching between hypothesis and reference.

Recent comparisons (Freitag et al. 2021b;Kocmi et al. 2021) have suggested that embedding-based metrics have superior performance, in other words that they correlate better with human judgements than string-based metrics. However, since embeddingbased metrics generally rely on sentence embeddings, they only work when such embeddings are available, and if they are fine-tuned on human evaluation data, they may not perform as well when such data is not available. For instance COMET is based on the XML-R embeddings (Conneau et al. 2020), so can support the 100 languages supported by XML-R, but will not give reliable results for unsupported languages.

String-based metrics (such as BLEU and ChrF) will generally support any language for reference-based automatic evaluation. BLEU has been in use in MT evaluation for many years, and its benefits and limitations are well-studied; ChrF has a much shorter history, but recent comparisons suggest that it performs better than BLEU (see references above) and its use of character n-grams probably make it more suited to the morphological complexity found in many low-resource languages. A recent debate in the MT evaluation literature has been about the ability for automatic metrics to discern differences in high-quality MT systems , Figure 6). However in lowresource MT, we may be faced with the opposite problem, i.e. metrics may be less reliable when faced with several low-quality systems (Fomicheva and Specia 2019).

In conclusion, for evaluation of low-resource MT, we recommend human evaluation as the gold standard, but where automatic evaluation is used, to be especially wary of the lack of calibration of metrics and the potential unreliability of test sets for lowresource language pairs.","What are the two main types of automatic metrics used in MT research, and how do they differ?","The two main types of automatic metrics used in MT research are string-based metrics and embedding-based metrics. String-based metrics, such as BLEU (Papineni et al. 2002; Post 2018) and ChrF (Popović 2015), are considered ""low-resource metrics"" because they do not require any resources beyond tokenization/segmentation. Embedding-based metrics, such as BERTscore, COMET (Rei et al. 2020), and BLEURT (Sellam, Das, and Parikh 2020), require more significant resources, such as pre-trained sentence embeddings, and often need human judgments for fine-tuning. Embedding-based metrics are considered successors to metrics like METEOR (Denkowski and Lavie 2014), which uses a synonym list to improve matching between hypothesis and reference. Recent comparisons have suggested that embedding-based metrics have superior performance, meaning they correlate better with human judgments than string-based metrics. However, embedding-based metrics generally rely on sentence embeddings, so they only work when such embeddings are available, and if they are fine-tuned on human evaluation data, they may not perform as well when such data is not available. String-based metrics, such as BLEU and ChrF, will generally support any language for reference-based automatic evaluation. BLEU has been in use in MT evaluation for many years, and its benefits and limitations are well-studied. ChrF has a much shorter history, but recent comparisons suggest that it performs better than BLEU, and its use of character n-grams probably makes it more suited to the morphological complexity found in many low-resource languages.","What are the two main types of automatic metrics used in machine translation (MT) research, and how do they differ?","In machine translation (MT) research, the two primary types of automatic metrics are string-based metrics and embedding-based metrics. String-based metrics, such as BLEU (Papineni et al. 2002; Post 2018) and ChrF (Popović 2015), are often referred to as ""low-resource metrics"" because they require minimal resources beyond tokenization or segmentation. These metrics are widely used for reference-based automatic evaluation and are generally applicable to any language. BLEU, in particular, has been a staple in MT evaluation for many years, with its strengths and limitations well-documented. ChrF, although newer, has shown promising results, especially in handling the morphological complexity of low-resource languages due to its use of character n-grams.

On the other hand, embedding-based metrics, such as BERTscore, COMET (Rei et al. 2020), and BLEURT (Sellam, Das, and Parikh 2020), require more substantial resources, including pre-trained sentence embeddings. These metrics often need human judgments for fine-tuning and are considered successors to metrics like METEOR (Denkowski and Lavie 2014), which utilized a synonym list to enhance matching between hypothesis and reference. Recent studies suggest that embedding-based metrics generally outperform string-based metrics in terms of correlation with human judgments. However, they are dependent on the availability of sentence embeddings and may not perform optimally without fine-tuning on human evaluation data. Despite these limitations, embedding-based metrics are gaining traction due to their superior performance in capturing semantic nuances.",9,2,9,2,"Question:

What are the two main types of automatic metrics used in MT research, and how do they differ?

Answer:

The two main types of automatic metrics used in MT research are string-based metrics and embedding-based metrics. String-based metrics, such as BLEU (Papineni et al. 2002; Post 2018) and ChrF (Popović 2015), are considered ""low-resource metrics"" because they do not require any resources beyond tokenization/segmentation. Embedding-based metrics, such as BERTscore, COMET (Rei et al. 2020), and BLEURT (Sellam, Das, and Parikh 2020), require more significant resources, such as pre-trained sentence embeddings, and often need human judgments for fine-tuning. Embedding-based metrics are considered successors to metrics like METEOR (Denkowski and Lavie 2014), which uses a synonym list to improve matching between hypothesis and reference. Recent comparisons have suggested that embedding-based metrics have superior performance, meaning they correlate better with human judgments than string-based metrics. However, embedding-based metrics generally rely on sentence embeddings, so they only work when such embeddings are available, and if they are fine-tuned on human evaluation data, they may not perform as well when such data is not available. String-based metrics, such as BLEU and ChrF, will generally support any language for reference-based automatic evaluation. BLEU has been in use in MT evaluation for many years, and its benefits and limitations are well-studied. ChrF has a much shorter history, but recent comparisons suggest that it performs better than BLEU, and its use of character n-grams probably makes it more suited to the morphological complexity found in many low-resource languages.","Question:

What are the two main types of automatic metrics used in machine translation (MT) research, and how do they differ?

Answer:

In machine translation (MT) research, the two primary types of automatic metrics are string-based metrics and embedding-based metrics. String-based metrics, such as BLEU (Papineni et al. 2002; Post 2018) and ChrF (Popović 2015), are often referred to as ""low-resource metrics"" because they require minimal resources beyond tokenization or segmentation. These metrics are widely used for reference-based automatic evaluation and are generally applicable to any language. BLEU, in particular, has been a staple in MT evaluation for many years, with its strengths and limitations well-documented. ChrF, although newer, has shown promising results, especially in handling the morphological complexity of low-resource languages due to its use of character n-grams.

On the other hand, embedding-based metrics, such as BERTscore, COMET (Rei et al. 2020), and BLEURT (Sellam, Das, and Parikh 2020), require more substantial resources, including pre-trained sentence embeddings. These metrics often need human judgments for fine-tuning and are considered successors to metrics like METEOR (Denkowski and Lavie 2014), which utilized a synonym list to enhance matching between hypothesis and reference. Recent studies suggest that embedding-based metrics generally outperform string-based metrics in terms of correlation with human judgments. However, they are dependent on the availability of sentence embeddings and may not perform optimally without fine-tuning on human evaluation data. Despite these limitations, embedding-based metrics are gaining traction due to their superior performance in capturing semantic nuances.",NO,True,1616,True,True
237371890-s37,Survey of Low-Resource Machine Translation,2021-09-01,Data preparation.,"An important initial step to training an NMT model is to identify available data (See Section 2) and to potentially filter it depending on the noisiness of the dataset and how out-of-domain it is or to use an alternative strategy to indicate domain or data quality (i.e. tagging). So what choices do participants tend to make in terms of using (or excluding) data sources, filtering and cleaning of data and using meta-information such as domain tags?

Choice of data. We focus on constrained submissions only (i.e. where participants can only use the data provided by the organisers), so most participants use all available data. Hesitancy can sometimes be seen with regards to web-crawled data (other than WMT newscrawl, which is generally more homogeneous and therefore of better quality), some choosing to omit the data (Singh 2020) and others to filter it for quality . It is very unusual to see teams do their own crawling (Hernandez and Nguyen (2020) is a counter-example); teams doing so run the risk of crawling data that overlaps with the development set or one side of the test set. Tran et al. (2021) successfully mined an extra million sentences pairs of Hausa-English data from the allowed monolingual data, helping them win the 2021 task.

Data cleaning and filtering. Although not exhaustively reported, many of the submissions apply some degree of data cleaning and filtering to the parallel and monolingual data. In its simplest form, this means excluding sentences based on their length (if too long) and the ratio between the lengths of parallel sentences (if too different). Some teams also remove duplicates (e.g. Li et al. (2019a)). More rigorous cleaning includes eliminating sentences containing fewer than a specified percentage of alpha-numeric characters in sentences (depending on the language's script), those identified as belonging to another language (e.g. using language identification) or those less likely to belong to the same distribution as the training data (e.g. using filtering techniques such as Moore-Lewis (Moore and Lewis 2010)). Data filtering is also a commonly used technique for backtranslation data (see the paragraph on data augmentation below), often using similar filtering techniques such as dual conditional cross-entropy filtering (Junczys-Dowmunt 2018) to retain only the cleanest and most relevant synthetic parallel sentences. Unfortunately the effect of data filtering is rarely evaluated, probably because it would involve expensive re-training.

Data tagging. Some teams choose to include meta-information in their models through the addition of pseudo-tokens. For example, Dutta et al. (2020) choose to tag sentences according to their quality for the Upper Sorbian-German task, this information being provided by the organisers. Domain tagging (i.e. indicating the type of data), which can be useful to indicate whether data is in-domain or out-of-domain was used by Chen et al. (2020), one of the top-scoring systems for Tamil-English. For the Basque-English task, Scherrer (2018) find that using domain tags gives systematic improvements over not using them, and Knowles et al. (2020a) come to the same conclusion when translating into Inuktitut.",What are the methods and benefits of using domain tagging in low-resource machine translation tasks?,"Some teams choose to include meta-information in their models through the addition of pseudo-tokens. Domain tagging, which indicates the type of data, can be useful to show whether data is in-domain or out-of-domain. Chen et al. (2020) used domain tagging in one of the top-scoring systems for Tamil-English. For the Basque-English task, Scherrer (2018) found that using domain tags gives systematic improvements over not using them. Knowles et al. (2020a) reached the same conclusion when translating into Inuktitut.",What are the methods and benefits of implementing domain tagging in low-resource machine translation tasks?,"In low-resource machine translation tasks, one effective method for enhancing model performance is the use of domain tagging. This involves adding pseudo-tokens to the data to indicate whether it is in-domain or out-of-domain. This approach allows models to better understand the context and type of data they are processing. For instance, Chen et al. (2020) successfully applied domain tagging in their top-performing system for Tamil-English translation. Similarly, Scherrer (2018) demonstrated that using domain tags in the Basque-English task consistently improved translation quality compared to not using them. Knowles et al. (2020a) also found that domain tagging led to better results when translating into Inuktitut. These examples highlight the systematic improvements that domain tagging can bring to low-resource machine translation tasks.",7,4,7,4,"Question:

What are the methods and benefits of using domain tagging in low-resource machine translation tasks?

Answer:

Some teams choose to include meta-information in their models through the addition of pseudo-tokens. Domain tagging, which indicates the type of data, can be useful to show whether data is in-domain or out-of-domain. Chen et al. (2020) used domain tagging in one of the top-scoring systems for Tamil-English. For the Basque-English task, Scherrer (2018) found that using domain tags gives systematic improvements over not using them. Knowles et al. (2020a) reached the same conclusion when translating into Inuktitut.","Question:

What are the methods and benefits of implementing domain tagging in low-resource machine translation tasks?

Answer:

In low-resource machine translation tasks, one effective method for enhancing model performance is the use of domain tagging. This involves adding pseudo-tokens to the data to indicate whether it is in-domain or out-of-domain. This approach allows models to better understand the context and type of data they are processing. For instance, Chen et al. (2020) successfully applied domain tagging in their top-performing system for Tamil-English translation. Similarly, Scherrer (2018) demonstrated that using domain tags in the Basque-English task consistently improved translation quality compared to not using them. Knowles et al. (2020a) also found that domain tagging led to better results when translating into Inuktitut. These examples highlight the systematic improvements that domain tagging can bring to low-resource machine translation tasks.",NO,True,851,True,True
237371890-s38,Survey of Low-Resource Machine Translation,2021-09-01,Data pre-processing.,"There is some variation in which data pre-processing steps are used. For example, it has been shown that for high-resource language pairs such as Czech-English, it is not always necessary to applying tokenisation and truecasing steps (Bawden et al. 2019) before apply subword segmentation. We do not observe a clear pattern, with many systems applying all steps, and some excluding tokenisation  for Tamil) and truecasing. Among the different possible pre-processing steps, we review participants choices concerning tokenisation, subword segmentation and transliteration/alphabet mapping (relevant when translating between languages that use different scripts).

Tokenisation. If a tokeniser is used before subword segmentation, it is common for it to be language-specific, particularly for the low-resource language in question. For example IndicNLP 22 (Kunchukuttan 2020) is widely used for Indian languages (e.g. for the shared tasks involving Gujarati and Tamil), and many of the Khmer-English submissions also used Khmer-specific tokenisers. For European languages, the Moses tokeniser ) remains the most commonly used option.

Subword segmentation. All participants perform some sort of subword segmentation, with most participants using either sentencepiece (Kudo and Richardson 2018) 23 or subword_nmt toolkits (Sennrich, Haddow, and Birch 2016b). 24 Even though the BPE toolkit is not compatible with the Abugida scripts used for Gujarati, Tamil and Khmer (in these scripts, two unicode codepoints can be used to represent one glyph), we only found one group who modified BPE to take this into account (Shi et al. 2020). BPE-dropout (Provilkov, Emelianenko, and Voita 2020), a regularisation method, was found to be useful by a number of teams (Knowles et al. 2020b;Libovický et al. 2020;Chronopoulou et al. 2020). The size of the subword vocabulary is often a tuned parameter, although the range of different values tested is not always reported. Surprisingly, there is significant variation in the subword vocabulary sizes used, and there is not always a clear pattern. Despite the low-resource settings, many of the systems use quite large subword vocabularies (30k-60k merge operations). There are exceptions: a large number of the systems for Tamil-English use small vocabularies (6k-30k merge operations), which may be attributed to the morphologically rich nature of Tamil coupled with the scarcity of data.

Joint subword segmentation is fairly common. Its use is particularly well motivated when the source and target languages are similar where we may expect to see a high amount of lexical overlap (e.g. for the similar language shared tasks such as Upper Sorbian-German) and when 'helper languages' are used to compensate for the lowresource scenario (e.g. addition of Czech and English data). However, it is also used in some cases even where there is little lexical overlap, for example for Tamil-English, where the languages do not share the same script, including by some of the top-scoring systems (Shi et al. 2020;Wu et al. 2018). Although few systematic studies are reported, one hypothesis could be that even if different scripts are used there is no disadvantage to sharing segmentation; it could help with named entities and therefore reducing the overall vocabulary size of the model (Ding, Renduchintala, and Duh 2019).

A few works explore alternative morphology-driven segmentation schemes, but without seeing any clear advantage: Scherrer, Grönroos, and Virpioja (2020) find that, for Upper-Sorbian-German, Morfessor can equal the performance of BPE when tuned correctly (but without surpassing it), whereas Sánchez-Cartagena (2018) find gains for Morfessor over BPE. Dhar, Bisazza, and van Noord (2020) have mixed results for Tamil-English when comparing linguistically motivated subword units compared to the use of statistics-based sentencepiece (Kudo and Richardson 2018).

Transliteration and alphabet mapping. Transliteration and alphabet mapping has been principally used in the context of exploiting data from related languages that are written in different scripts. This was particularly present for translation involving Indian languages, which often have their own script. For the Gujarati-English task, many of the top systems used Hindi-English data (see below the paragraph on using other language data) and performed alphabet mapping into the Gujarati script (Li et al. 2019b;Bawden et al. 2019;Dabre et al. 2019). For Tamil-English,  found that when using Hindi in a multilingual setup, it helped for Hindi to be mapped into the Tamil script for the Tamil→English direction, but did not bring improvements for English→Tamil. Transliteration was also used in the Kazakh-English task, particularly with the addition of Turkish as higher-resourced language. Toral et al. (2019), a topscoring system, chose to cyrillise Turkish to increase overlap with Kazakh, whereas Briakou and Carpuat (2019) chose to romanise Kazakh to increase the overlap with Turkish, but only for the Kazakh→English direction.",How is transliteration and alphabet mapping applied in machine translation for languages with different scripts?,"Transliteration and alphabet mapping have been principally used in the context of exploiting data from related languages that are written in different scripts. This was particularly present for translation involving Indian languages, which often have their own script. For the Gujarati-English task, many of the top systems used Hindi-English data and performed alphabet mapping into the Gujarati script (Li et al. 2019b; Bawden et al. 2019; Dabre et al. 2019). For Tamil-English, it was found that when using Hindi in a multilingual setup, it helped for Hindi to be mapped into the Tamil script for the Tamil→English direction, but did not bring improvements for English→Tamil. Transliteration was also used in the Kazakh-English task, particularly with the addition of Turkish as a higher-resourced language. Toral et al. (2019), a top-scoring system, chose to cyrillise Turkish to increase overlap with Kazakh, whereas Briakou and Carpuat (2019) chose to romanise Kazakh to increase the overlap with Turkish, but only for the Kazakh→English direction.",How are transliteration and alphabet mapping utilized in machine translation for languages with different scripts?,"Transliteration and alphabet mapping play a crucial role in machine translation, especially when dealing with languages that use different scripts. This technique is particularly useful for leveraging data from related languages. For instance, in the context of Indian languages, which often have distinct scripts, transliteration has been effectively applied. In the Gujarati-English translation task, many top-performing systems utilized Hindi-English data by mapping the Hindi script into the Gujarati script (Li et al. 2019b; Bawden et al. 2019; Dabre et al. 2019). Similarly, for Tamil-English translation, it was observed that incorporating Hindi in a multilingual setup was beneficial when Hindi was mapped into the Tamil script for the Tamil→English direction, although it did not yield improvements for the English→Tamil direction.

Transliteration has also been employed in the Kazakh-English translation task, particularly with the inclusion of Turkish as a higher-resourced language. In this scenario, Toral et al. (2019), a top-scoring system, opted to cyrillise Turkish to enhance overlap with Kazakh. Conversely, Briakou and Carpuat (2019) chose to romanise Kazakh to increase overlap with Turkish, but this approach was only applied for the Kazakh→English direction.",8,4,8,4,"Question:

How is transliteration and alphabet mapping applied in machine translation for languages with different scripts?

Answer:

Transliteration and alphabet mapping have been principally used in the context of exploiting data from related languages that are written in different scripts. This was particularly present for translation involving Indian languages, which often have their own script. For the Gujarati-English task, many of the top systems used Hindi-English data and performed alphabet mapping into the Gujarati script (Li et al. 2019b; Bawden et al. 2019; Dabre et al. 2019). For Tamil-English, it was found that when using Hindi in a multilingual setup, it helped for Hindi to be mapped into the Tamil script for the Tamil→English direction, but did not bring improvements for English→Tamil. Transliteration was also used in the Kazakh-English task, particularly with the addition of Turkish as a higher-resourced language. Toral et al. (2019), a top-scoring system, chose to cyrillise Turkish to increase overlap with Kazakh, whereas Briakou and Carpuat (2019) chose to romanise Kazakh to increase the overlap with Turkish, but only for the Kazakh→English direction.","Question:

How are transliteration and alphabet mapping utilized in machine translation for languages with different scripts?

Answer:

Transliteration and alphabet mapping play a crucial role in machine translation, especially when dealing with languages that use different scripts. This technique is particularly useful for leveraging data from related languages. For instance, in the context of Indian languages, which often have distinct scripts, transliteration has been effectively applied. In the Gujarati-English translation task, many top-performing systems utilized Hindi-English data by mapping the Hindi script into the Gujarati script (Li et al. 2019b; Bawden et al. 2019; Dabre et al. 2019). Similarly, for Tamil-English translation, it was observed that incorporating Hindi in a multilingual setup was beneficial when Hindi was mapped into the Tamil script for the Tamil→English direction, although it did not yield improvements for the English→Tamil direction.

Transliteration has also been employed in the Kazakh-English translation task, particularly with the inclusion of Turkish as a higher-resourced language. In this scenario, Toral et al. (2019), a top-scoring system, opted to cyrillise Turkish to enhance overlap with Kazakh. Conversely, Briakou and Carpuat (2019) chose to romanise Kazakh to increase overlap with Turkish, but this approach was only applied for the Kazakh→English direction.",NO,True,1282,True,True
237371890-s41,Survey of Low-Resource Machine Translation,2021-09-01,Using additional data.,"Much of this survey has been dedicated to approaches for the exploitation of additional resources to compensate for the lack of data for lowresource language pairs: monolingual data (Section 3), multilingual data (Section 4) or other linguistic resources (Section 5). In shared tasks, the following approaches have been shown to be highly effective to boosting performance in low-resource scenarios.

Backtranslation. The majority of high-performing systems carry out some sort of data augmentation, the most common being backtranslation, often used iteratively, although forward translation is also used (Shi et al. 2020;Chen et al. 2020;Zhang et al. 2020c;Wei et al. 2020). For particularly challenging language pairs (e.g. for very low-resource between languages that are not very close), it is important for the initial model that is used to produce the backtranslations to be of sufficiently high quality. For example, some of the top Gujarati-English systems employed pretraining before backtranslation to boost the quality of the initial model (Bawden et al. 2019;Bei et al. 2019). Participants do not always report the number of iterations of backtranslations performed, however those that do often cite the fact that few improvements are seen beyond two iterations . Tagged backtranslation, whereby a pseudo-token is added to sentences that are backtranslated to distinguish then from genuine parallel data have previously shown to provide improvements (Caswell, Chelba, and Grangier 2019). Several participants report gains thanks to the addition of backtranslation tags Chen et al. 2020;Knowles et al. 2020a), although  find that tagged backtranslation under-performs normal backtranslation in a multilingual setup for Tamil-English.

Synthetic data from other languages. A number of top-performing systems successfully exploit parallel corpora from related languages. The two top-performing systems for Gujarati-English use a Hindi-English parallel corpus to create synthetic Gujarati-English data (Li et al. 2019b;Bawden et al. 2019). Both exploit the fact that there is a high degree of lexical overlap between Hindi and Gujarati once Hindi has been transliterated into Gujarati script. Li et al. (2019b) choose to transliterate the Hindi side and then to select the best sentences using cross-entropy filtering, whereas Bawden et al. (2019) choose to train a Hindi→Gujarati model, which they use to translate the Hindi side of the corpus. Pivoting through a higher-resource related language was also found to be useful for other language pairs: for Kazakh-English, Russian was the language of choice (Li et al. 2019b;Toral et al. 2019;Dabre et al. 2019;Budiwati et al. 2019), for Basque-English, Spanish was used as a pivot (Scherrer 2018;, which was found to be more effective than backtranslation by Scherrer (2018), and was found to benefit from additional filtering by .

Transfer-learning using language modelling objectives. The top choices of language modelling objectives are mBART ) (used by Chen et al. (2020) andBawden et al. (2020) for Tamil Kvapilíková, Kocmi, and Bojar (2020) and Dutta et al. (2020) for Upper-Sorbian-German), and MASS (Song et al. 2019b) (used by  and Singh, Singh, and Bandyopadhyay (2020) for Upper Sorbian-German). Some of the top systems used these language modelling objectives, but their use was not across the board, and pretraining using translation objectives was arguably more common. Given the success of pretrained models in NLP, this could be surprising. A possible explanation for these techniques not being used systematically is that they can be computationally expensive to train from scratch and the constrained nature of the shared tasks means that the participants are discouraged from using pretrained language models.

Transfer learning from other MT systems. Another commonly used technique used by participants was transfer learning involving other language pairs. Many of the teams exploited a high-resource related language pair. For example, for Kazakh-English, pretraining was done using Turkish-English (Briakou and Carpuat 2019) and Russian-English (Kocmi and Bojar 2019), Dabre et al. (2019) pretrained for Gujarati-English using Hindi-English, and Czech-German was used to pretrain for Upper-Sorbian-German (Knowles et al. 2020b).

An alternative but successful approach was to use a high-resource but not necessarily related language pair. For example, the CUNI systems use Czech-English to pretrain Inuktitut (Kocmi 2020) and Gujarati (Kocmi andBawden et al. (2020) found pretraining on English-German to be as effective as mBART training for Tamil-English. Finally, a number of teams opted for multilingual pretraining, involving the language pair in question and a higher-resource language or several higher-resource languages. Wu et al. (2020) use the mRASP approach: a universal multilingual model involving language data for English to and from Pashto, Khmer, Tamil, Inuktitut, German and Polish, which is then fine-tuned to the individual low-resource language pairs. Multilingual models. Other than the pretraining strategies mentioned just above, multilingual models feature heavily in shared task submissions. The overwhelmingly most common framework used was the universal encoder-decoder models as proposed by Johnson et al. (2017). Some participants chose to include select (related) languages. Williams et al. (2018) use Spanish to boost Basque-English translation and find that the addition of French data degrades results. Goyal and Sharma (2019) add Hindi as an additional encoder language for Gujarati-English and for Tamil-English, they test adding Hindi to either the source or target side depending on whether Tamil is the source or target language . Other participants choose to use a larger number of languages. Zhang et al. (2020c) train a multilingual system on six Indian languages for Tamil-English and Hokamp, Glover, and Gholipour Ghalandari (2019) choose to train a multilingual model on all WMT languages for Gujarati-English (coming middle in the results table). Upsampling the lower-resourced languages in the multilingual systems is an important factor, whether the multilingual system is used as the main model or for pretraining (Zhang et al. 2020c;Wu et al. 2020). Recent approaches has seen success using more diverse and even larger numbers of languages. Tran et al. (2021) train a model for 14 diverse language directions, winning 10 of them (although their system is unconstrained). Their two models, many to English and English to many, used a Sparsely Gated Mixture-of-Expert (MoE) models (Lepikhin et al. 2020). The MoE strike a balance between allowing high-resource directions to benefit from increased model capacity, while also allowing transfer to low-resource directions via shared capacity. Microsoft's winning submission to the 2021 large scale multilingual task (Yang et al. 2021) covered 10k language pairs across the FLORES-101 data set. They use the public available DeltaLM-Large model ), a multilingual pre-trained encoderdecoder model, and apply progressive learning (Zhang et al. 2020b) (starting training with 24 encoder layers and adding 12 more) and iterative back-translation.",How is synthetic data from related languages utilized in low-resource machine translation systems?,"A number of top-performing systems successfully exploit parallel corpora from related languages. The two top-performing systems for Gujarati-English use a Hindi-English parallel corpus to create synthetic Gujarati-English data (Li et al. 2019b; Bawden et al. 2019). Both exploit the fact that there is a high degree of lexical overlap between Hindi and Gujarati once Hindi has been transliterated into Gujarati script. Li et al. (2019b) choose to transliterate the Hindi side and then to select the best sentences using cross-entropy filtering, whereas Bawden et al. (2019) choose to train a Hindi→Gujarati model, which they use to translate the Hindi side of the corpus. Pivoting through a higher-resource related language was also found to be useful for other language pairs: for Kazakh-English, Russian was the language of choice (Li et al. 2019b; Toral et al. 2019; Dabre et al. 2019; Budiwati et al. 2019), for Basque-English, Spanish was used as a pivot (Scherrer 2018), which was found to be more effective than backtranslation by Scherrer (2018), and was found to benefit from additional filtering.",How do low-resource machine translation systems leverage synthetic data from related languages?,"Low-resource machine translation systems often utilize parallel corpora from related languages to enhance their performance. For instance, in the case of Gujarati-English translation, top-performing systems have successfully used a Hindi-English parallel corpus to generate synthetic Gujarati-English data (Li et al. 2019b; Bawden et al. 2019). This approach takes advantage of the significant lexical overlap between Hindi and Gujarati, especially after Hindi is transliterated into the Gujarati script. Li et al. (2019b) opted to transliterate the Hindi side and then apply cross-entropy filtering to select the best sentences. In contrast, Bawden et al. (2019) trained a Hindi→Gujarati model to translate the Hindi side of the corpus. This strategy of pivoting through a higher-resource related language has also proven beneficial for other language pairs. For Kazakh-English translation, Russian served as the pivot language (Li et al. 2019b; Toral et al. 2019; Dabre et al. 2019; Budiwati et al. 2019). Similarly, for Basque-English translation, Spanish was used as a pivot, which Scherrer (2018) found to be more effective than backtranslation, especially when combined with additional filtering.",7,4,7,4,"Question:

How is synthetic data from related languages utilized in low-resource machine translation systems?

Answer:

A number of top-performing systems successfully exploit parallel corpora from related languages. The two top-performing systems for Gujarati-English use a Hindi-English parallel corpus to create synthetic Gujarati-English data (Li et al. 2019b; Bawden et al. 2019). Both exploit the fact that there is a high degree of lexical overlap between Hindi and Gujarati once Hindi has been transliterated into Gujarati script. Li et al. (2019b) choose to transliterate the Hindi side and then to select the best sentences using cross-entropy filtering, whereas Bawden et al. (2019) choose to train a Hindi→Gujarati model, which they use to translate the Hindi side of the corpus. Pivoting through a higher-resource related language was also found to be useful for other language pairs: for Kazakh-English, Russian was the language of choice (Li et al. 2019b; Toral et al. 2019; Dabre et al. 2019; Budiwati et al. 2019), for Basque-English, Spanish was used as a pivot (Scherrer 2018), which was found to be more effective than backtranslation by Scherrer (2018), and was found to benefit from additional filtering.","Question:

How do low-resource machine translation systems leverage synthetic data from related languages?

Answer:

Low-resource machine translation systems often utilize parallel corpora from related languages to enhance their performance. For instance, in the case of Gujarati-English translation, top-performing systems have successfully used a Hindi-English parallel corpus to generate synthetic Gujarati-English data (Li et al. 2019b; Bawden et al. 2019). This approach takes advantage of the significant lexical overlap between Hindi and Gujarati, especially after Hindi is transliterated into the Gujarati script. Li et al. (2019b) opted to transliterate the Hindi side and then apply cross-entropy filtering to select the best sentences. In contrast, Bawden et al. (2019) trained a Hindi→Gujarati model to translate the Hindi side of the corpus. This strategy of pivoting through a higher-resource related language has also proven beneficial for other language pairs. For Kazakh-English translation, Russian served as the pivot language (Li et al. 2019b; Toral et al. 2019; Dabre et al. 2019; Budiwati et al. 2019). Similarly, for Basque-English translation, Spanish was used as a pivot, which Scherrer (2018) found to be more effective than backtranslation, especially when combined with additional filtering.",NO,True,1202,True,True
21715311-s1,A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches,2018-05-01,Background -WordNet Construction,"Generally speaking, WordNets are constructed using one of two approaches (Vossen, 1998):

• The merge approach -whereby an exhaustive repository of senses (meanings) of each word is compiled, with synsets then created that contain all of the applicable words for a given sense.

• The expansion approach -whereby existing synsets from a reference WordNet are used as a guide to create corresponding synsets in a new WordNet, by gathering applicable words that represent the meaning of the synset and ordering them by frequency.

Since the introduction of the PWN and the success of early projects such as EuroWordNet that were built around its principles, many projects have focused on building new WordNets in diverse languages using these methods. These endeavours have highlighted various advantages and disadvantages of both the merge and the expansion approaches. Bhattacharyya (2010) describes how the merge approach results in WordNets of high quality, on account of expert lexicographers working in detail on only one language; however, the process is typically very slow. Conversely, the expansion approach can allow the construction of the Word-Net to take place much more quickly, with construction guided by synsets and semantic relationships in the source (or reference) WordNet; however, lexicographers still need to dedicate time to constructing language-specific synsets (meanings or concepts which may not be represented or have a place in the source WordNet), and there is a danger of specific concepts only applicable to the target language being overlooked altogether (Bhattacharyya, 2010). Years of work on constructing new WordNets have also contributed to the development of guidelines and principles for creating them, largely based on leveraging existing knowledge using the expansion approach. The GWA outlines the importance of 'base concepts' 4 -those concepts that occupy a high position in a semantic hierarchy and have many relations to other concepts -as playing a vital role in constructing WordNets. Base concepts are defined by their universality -common (to at least two languages), local (to only one language), or global (across all languages) -with an initial set of 1024 common base concepts being released as part of the EuroWordNet project 5 . As a starting point, the GWA proposes that WordNets be constructed in two steps:

• A core WordNet of between 5,000 and 10,000 synsets is constructed around the common base concepts,

• An extended WordNet is built (semi-automatically, given the semantic basis of the core WordNet) to increase the total number of synsets to 20,000 and beyond.

Given that for many languages these core synsets are readily-constructed, it makes sense to leverage them when constructing new WordNets, and to 'borrow' the semantic relationships that have already been created (Bhattacharyya, 2010). Given the amount of time that can be saved by re-using existing work, there is a tendency to see the expansion approach favoured over the merge approachit also lends itself extremely well to the automatic construction of synsets, where input from lexicographers is minimal to zero. Thus, the research described in this paper (and particularly in section 3.) largely follows the expansion approach, with synsets being constructed by automatically extracting lexical data from a range of resources in order to build a skeleton framework of meanings based on a reference WordNet.","Why is the expansion approach often preferred for constructing WordNets, and how does it facilitate automatic synset construction?","The expansion approach can allow the construction of the WordNet to take place much more quickly, with construction guided by synsets and semantic relationships in the source (or reference) WordNet. Lexicographers still need to dedicate time to constructing language-specific synsets, but there is a danger of specific concepts only applicable to the target language being overlooked altogether (Bhattacharyya, 2010). Years of work on constructing new WordNets have contributed to the development of guidelines and principles for creating them, largely based on leveraging existing knowledge using the expansion approach. Given the amount of time that can be saved by re-using existing work, there is a tendency to see the expansion approach favored over the merge approach. It also lends itself extremely well to the automatic construction of synsets, where input from lexicographers is minimal to zero. Thus, the research described in this paper largely follows the expansion approach, with synsets being constructed by automatically extracting lexical data from a range of resources in order to build a skeleton framework of meanings based on a reference WordNet.","Why is the expansion approach often preferred for constructing WordNets, and how does it facilitate automatic synset construction?","The expansion approach is often favored for constructing WordNets because it significantly accelerates the process by leveraging synsets and semantic relationships from an existing reference WordNet. While lexicographers still need to invest time in creating language-specific synsets, there is a risk that unique concepts relevant only to the target language might be overlooked (Bhattacharyya, 2010). Over the years, the development of new WordNets has led to the establishment of guidelines and principles that heavily rely on the expansion approach, which utilizes existing knowledge. This method is generally preferred over the merge approach due to the substantial time savings it offers by re-using existing work. Additionally, the expansion approach is highly conducive to the automatic construction of synsets, requiring minimal to no input from lexicographers. Consequently, the research discussed in this paper predominantly employs the expansion approach, where synsets are automatically constructed by extracting lexical data from various resources to create a foundational framework of meanings based on a reference WordNet.",7,2,7,2,"Question:

Why is the expansion approach often preferred for constructing WordNets, and how does it facilitate automatic synset construction?

Answer:

The expansion approach can allow the construction of the WordNet to take place much more quickly, with construction guided by synsets and semantic relationships in the source (or reference) WordNet. Lexicographers still need to dedicate time to constructing language-specific synsets, but there is a danger of specific concepts only applicable to the target language being overlooked altogether (Bhattacharyya, 2010). Years of work on constructing new WordNets have contributed to the development of guidelines and principles for creating them, largely based on leveraging existing knowledge using the expansion approach. Given the amount of time that can be saved by re-using existing work, there is a tendency to see the expansion approach favored over the merge approach. It also lends itself extremely well to the automatic construction of synsets, where input from lexicographers is minimal to zero. Thus, the research described in this paper largely follows the expansion approach, with synsets being constructed by automatically extracting lexical data from a range of resources in order to build a skeleton framework of meanings based on a reference WordNet.","Question:

Why is the expansion approach often preferred for constructing WordNets, and how does it facilitate automatic synset construction?

Answer:

The expansion approach is often favored for constructing WordNets because it significantly accelerates the process by leveraging synsets and semantic relationships from an existing reference WordNet. While lexicographers still need to invest time in creating language-specific synsets, there is a risk that unique concepts relevant only to the target language might be overlooked (Bhattacharyya, 2010). Over the years, the development of new WordNets has led to the establishment of guidelines and principles that heavily rely on the expansion approach, which utilizes existing knowledge. This method is generally preferred over the merge approach due to the substantial time savings it offers by re-using existing work. Additionally, the expansion approach is highly conducive to the automatic construction of synsets, requiring minimal to no input from lexicographers. Consequently, the research discussed in this paper predominantly employs the expansion approach, where synsets are automatically constructed by extracting lexical data from various resources to create a foundational framework of meanings based on a reference WordNet.",NO,True,1138,True,True
21715311-s4,A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches,2018-05-01,Bilingual Dictionaries,"The most common technique for populating new WordNets automatically has been to leverage the information in bilingual dictionaries in the source and target languages. Use of bilingual dictionaries for this purpose goes back to very early work on building Catalan and Spanish WordNets as part of the EuroWordNet project. In this work, translations of English words in the source WordNet were found, and these translations classified by features such as polysemy (number of translations for each word), structure (the semantic relationships between translations in the source WordNet), and 'conceptual distance' (length of the path between two words in a graph-based representation of the source WordNet) to create a skeleton WordNet in the target language, which could be extended later using bilingual taxonomies (Farreres et al., 1998). Since then, bilingual dictionaries have continued to be a popular resource for the automatic construction of Word-Nets. A Romanian WordNet was built by using a range of heuristics to:

• Analyse the relationships between synsets in the source (English) WordNet,

• Identify semantic relationships in various target language resources,

• Map these relationships to each other in the target (Romanian) WordNet using a bilingual dictionary (Barbu and Barbu Mititelu, 2005).

The method was evaluated using 9716 synsets from a preexisting Romanian WordNet that also had entries in PWN, from which these 9716 synsets were extracted and used as the source (English) WordNet -the synsets used were limited to hypernymy and meronymy relations, and all 19,624 literal words within the synsets had an entry in the bilingual dictionary. The resulting automatically-constructed Romanian WordNet contained 9610 synsets connected by approximately 11,969 semantic relationships, which were reported to be 91% accurate when compared to the 9716 synsets from the pre-existing Romanian WordNet (Barbu and Barbu Mititelu, 2005). In more recent work on building a Persian WordNet, a bilingual dictionary was used to extract a group of 'candidate' synsets containing English translations of a given Persian word from a source WordNet (PWN). These candidate synsets were then ranked by calculating the Mutual Information of the given Persian word and its English translations in both source and target language corpora, and based on this ranking the most appropriate candidate synset to use for the target (Persian) WordNet was selected (Montazery and Faili, 2010). An extension of this work specifically aimed at lesser-resourced languages was also described, in which a Persian WordNet is constructed by finding the English translations of Persian words in small corpora using a bilingual dictionary. These translations are then used to perform word sense disambiguation (WSD) on a Persian sentence using a source (English) WordNet, and the English synsets returned by the WSD algorithm are mapped to the target (Persian) WordNet (Taghizadeh and Faili, 2016). Again, these techniques have been shown to be able to automatically construct WordNets with a good degree of accuracy. Montazery and Faili (2010) report that a manual evaluation of 500 synsets from their automatically-constructed target WordNet (which in total covered 29,716 synsets from PWN) resulted in an accuracy of 82.6% (95.8% for synsets whose mapping from source to target WordNet was unambiguous and 76.4% for synsets whose correct mapping had to be decided by ranking multiple candidates). Taghizadeh and Faili (2016) manually evaluated 1,750 word/synset pairs from their target WordNet, and describe how a threshold value (between 0 and 1) used by their WSD algorithm to remove low-scoring candidate synsets had a significant impact on their results. Higher threshold values resulted in the WordNet being more precise (90% with a threshold value of 0.1) but with low recall (fewer synsets in the target WordNet), while lower threshold values resulted in a Word-Net with higher recall (more synsets) but with low precision (74% with the threshold value set to 0).",How do threshold values affect precision and recall in the evaluation of automatically-constructed Persian WordNets?,"Higher threshold values used by the WSD algorithm result in the WordNet being more precise, achieving 90% precision with a threshold value of 0.1. However, this comes with low recall, meaning fewer synsets are included in the target WordNet. Conversely, lower threshold values lead to a WordNet with higher recall, incorporating more synsets. This increase in recall is accompanied by lower precision, with precision dropping to 74% when the threshold value is set to 0.",How do threshold values influence precision and recall in evaluating automatically-constructed Persian WordNets?,"Threshold values play a crucial role in balancing precision and recall when evaluating automatically-constructed Persian WordNets. When higher threshold values are applied by the Word Sense Disambiguation (WSD) algorithm, the resulting WordNet becomes more precise, achieving up to 90% precision with a threshold value of 0.1. However, this increased precision comes at the cost of recall, as fewer synsets are included in the target WordNet. On the other hand, using lower threshold values results in a WordNet with higher recall, as more synsets are incorporated. This improvement in recall, however, leads to a decrease in precision, with precision dropping to 74% when the threshold value is set to 0.",7,2,4,2,"Question:

How do threshold values affect precision and recall in the evaluation of automatically-constructed Persian WordNets?

Answer:

Higher threshold values used by the WSD algorithm result in the WordNet being more precise, achieving 90% precision with a threshold value of 0.1. However, this comes with low recall, meaning fewer synsets are included in the target WordNet. Conversely, lower threshold values lead to a WordNet with higher recall, incorporating more synsets. This increase in recall is accompanied by lower precision, with precision dropping to 74% when the threshold value is set to 0.","Question:

How do threshold values influence precision and recall in evaluating automatically-constructed Persian WordNets?

Answer:

Threshold values play a crucial role in balancing precision and recall when evaluating automatically-constructed Persian WordNets. When higher threshold values are applied by the Word Sense Disambiguation (WSD) algorithm, the resulting WordNet becomes more precise, achieving up to 90% precision with a threshold value of 0.1. However, this increased precision comes at the cost of recall, as fewer synsets are included in the target WordNet. On the other hand, using lower threshold values results in a WordNet with higher recall, as more synsets are incorporated. This improvement in recall, however, leads to a decrease in precision, with precision dropping to 74% when the threshold value is set to 0.",NO,True,705,True,True
21715311-s9,A Survey on Automatically-Constructed WordNets and their Evaluation: Lexical and Word Embedding-based Approaches,2018-05-01,Issues for Evaluating Automatically-Constructed WordNets,"One of the biggest issues for the automatic construction of WordNets is how to properly and effectively evaluate their accuracy and/or precision. Across all of the different lexical resource and word embedding-based approaches to automatic synset construction described in the previous two sections, evaluation methods can be split across two types:

• Comparison against a reference WordNet,

• Manual evaluations against fixed samples of automatically constructed synsets.

Focusing first on comparisons with reference WordNets, much of the research referenced in the preceding sections reports on problems with this kind of evaluation. Khodak et al. (2017) describe an attempted comparison with their automatically-constructed WordNets and reference Word-Nets being difficult, with the ELRA French WordNet 2 being only around half the size of their new French WordNet and most Russian WordNets being a) even smaller, and b) not easily linked to (or compared with) PWN. Similarly, Taghizadeh and Faili (2016) cite 'the lack of correct links' in the pre-existing FarsNet (an ontology of Persian words mapped to PWN synsets) as being troublesome when attempting to compare their automatically-constructed Persian WordNet to it -they reported after comparing their automatically-constructed Persian WordNet to Farsnet that that the precision of their new WordNet was just 19% and its recall 49%, too low 'to be considered as a reliable resource'. The discrepancies between size and coverage of reference WordNets and the original PWN can be viewed as an issue of granularity: PWN is large enough that its senses are fine-grained, and so several PWN synsets can generally mapped onto one synset in a reference WordNet (such as FarsNet) while other PWN synsets will not be present at all (Khodak et al., 2017). This makes it difficult, when comparing automatically-constructed WordNets to reference WordNets in a target language, to decide whether newly-created synsets are correct or not. For example, Taghizadeh and Faili (2016) use the following criteria to decide whether word and synset pairs in an automaticallyconstructed Persian WordNet are correct, using FarsNet as their reference WordNet:

• If a Persian word does not exist in FarsNet, it IS NOT correct,

• If a Persian word exists in FarsNet but is not linked to a PWN synset, it IS NOT correct,

• If a Persian word exists in Farsnet and and at least one PWN synset is linked to it:

-If the automatically-constructed synset is not one of the linked PWN synsets in FarsNet, it IS NOT correct,

-IF the automatically-constructed synset is one of the linked PWN sysnets in FarsNet, it IS correct.

Out of three options here, two of them lead to the word in the automatically constructed Persian WordNet being classed as incorrect -and even if the word is in both FarsNet and PWN, that word still has to be linked between those resources to be accepted as correct. This approach is therefore totally dependent on the quality of FarsNet, and any words in the automatically constructed WordNet that are in PWN and that should be in FarsNet will, unfortunately, be classed as incorrect. As Oliver and Climent (2014) -who considered an automatically extracted synset correct only if it was also present in a reference WordNethighlight, automatic comparisons with reference WordNets inevitably mean that if the reference WordNets are not complete, then correctly extracted synsets in the automatically constructed WordNet can be evaluated as incorrect -and this is a major problem when reporting on their accuracy and legitimacy as a lexical resource. Sand et al. (2017) also touch on potential discrepancies between automatically extracted synsets and their equivalent synsets in reference WordNets or in PWN, noting that hypernymy relations 'can be right or wrong by varying degrees'. They describe a 'soft accuracy' measure whereby the accuracy of an automatically extracted synset is weighted according to the number of links (or edges) between words in different synsets that separate a given word from what would be its correct position in a graph-based representation of the WordNet. Weighting the accuracy of automatically extracted synsets according to how comparable they are with their fully-formed PWN equivalents is certainly more logical than evaluating strictly on 'correct insertions' -an automatically extracted synset containing 8 of the 10 links to other words present in the same sysnet in PWN, for example, is surely more correct than an automatically extracted synset containing only 2 or 3 of the 10 links.

The alternative to automatic evaluations of synset correctness is of course manual evaluation, which is widely used both in isolation and in conjunction with automated evaluations in the works cited in Section 3. (Ruiz-Casado et al., 2005;Montazery and Faili, 2010;Lam et al., 2014;Taghizadeh and Faili, 2016). However, as Ruiz-Casado et al. (2005) point out, it is 'difficult to know how accurate manually-evaluated synsets are without some common guidelines. Some works simply describe having manual annotators decide if an automatically extracted is or is not semantically similar to a reference synset (Taghizadeh and Faili, 2016), while others -much more in line with the idea of weighting accuracy according to a degree of correctness (Sand et al., 2017) -have used a Likert scale for conducting manual evaluations (Lam et al., 2014).",What are the challenges and methods associated with manual evaluation of synsets in automatically-constructed WordNets?,"The alternative to automatic evaluations of synset correctness is manual evaluation, which is widely used both in isolation and in conjunction with automated evaluations. However, it is 'difficult to know how accurate manually-evaluated synsets are without some common guidelines. Some works simply describe having manual annotators decide if an automatically extracted synset is or is not semantically similar to a reference synset. Others, much more in line with the idea of weighting accuracy according to a degree of correctness, have used a Likert scale for conducting manual evaluations.",What are the challenges and methods involved in the manual evaluation of synsets in automatically-constructed WordNets?,"Manual evaluation of synsets, as an alternative to automatic evaluations, is a common practice used either independently or alongside automated methods. However, one of the main challenges is determining the accuracy of these manually-evaluated synsets without established guidelines. Some approaches involve having manual annotators assess whether an automatically extracted synset is semantically similar to a reference synset. Other methods, which align more closely with the concept of weighting accuracy based on the degree of correctness, employ a Likert scale for conducting these evaluations.",7,4,7,4,"Question:

What are the challenges and methods associated with manual evaluation of synsets in automatically-constructed WordNets?

Answer:

The alternative to automatic evaluations of synset correctness is manual evaluation, which is widely used both in isolation and in conjunction with automated evaluations. However, it is 'difficult to know how accurate manually-evaluated synsets are without some common guidelines. Some works simply describe having manual annotators decide if an automatically extracted synset is or is not semantically similar to a reference synset. Others, much more in line with the idea of weighting accuracy according to a degree of correctness, have used a Likert scale for conducting manual evaluations.","Question:

What are the challenges and methods involved in the manual evaluation of synsets in automatically-constructed WordNets?

Answer:

Manual evaluation of synsets, as an alternative to automatic evaluations, is a common practice used either independently or alongside automated methods. However, one of the main challenges is determining the accuracy of these manually-evaluated synsets without established guidelines. Some approaches involve having manual annotators assess whether an automatically extracted synset is semantically similar to a reference synset. Other methods, which align more closely with the concept of weighting accuracy based on the degree of correctness, employ a Likert scale for conducting these evaluations.",NO,True,600,True,True
258378266-s5,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,2023,Choices of Filter,"Filtering is a crucial part of QG since a significant portion of generated questions could be of low quality and would provide misleading signals when used to train the NR model (Alberti et al., 2019). A typical choice is filtering based on round-trip consistency (Alberti et al., 2019;Dong et al., 2019), where a pre-trained QA system is applied to produce an answer based on the generated question. A question is kept only when the produced answer is consistent with the answer from which the question is generated. We can also relax this strict consistency requirement and manually adjust an acceptance threshold based on the probability from the pre-trained QA system (Zhang and Bansal, 2019;Lewis et al., 2021), LM score from the generator itself (Shakeri et al., 2020;Liang et al., 2020), or an entailment score from a model trained on question-context-answer pairs . Influence functions (Cook and Weisberg, 1982) can be used to estimate the effect on the validation loss of including a synthetic example (Yang et al., 2020), but this does not achieve satisfying performances on QA tasks (Bartolo et al., 2021). Bartolo et al. (2021) propose filtering questions based on ensemble consistency, where an ensemble of QA models are trained with different random seeds and only questions agreed by most QA models are selected. When minimal target-domain annotation is available, we can also learn to reweight pseudo samples based on the validation loss , or use RL to select samples that lead to validation performance gains (value estimation) (Yue et al., 2022b).",How can filtering methods improve the quality of generated questions in neural ranking models for open-domain QA?,"Filtering is a crucial part of question generation since a significant portion of generated questions could be of low quality and would provide misleading signals when used to train the neural ranking model (Alberti et al., 2019). A typical choice is filtering based on round-trip consistency (Alberti et al., 2019; Dong et al., 2019), where a pre-trained QA system is applied to produce an answer based on the generated question. A question is kept only when the produced answer is consistent with the answer from which the question is generated. The strict consistency requirement can be relaxed by manually adjusting an acceptance threshold based on the probability from the pre-trained QA system (Zhang and Bansal, 2019; Lewis et al., 2021), the language model score from the generator itself (Shakeri et al., 2020; Liang et al., 2020), or an entailment score from a model trained on question-context-answer pairs. Bartolo et al. (2021) propose filtering questions based on ensemble consistency, where an ensemble of QA models are trained with different random seeds and only questions agreed by most QA models are selected. When minimal target-domain annotation is available, pseudo samples can be reweighted based on the validation loss, or reinforcement learning can be used to select samples that lead to validation performance gains (Yue et al., 2022b).",How do filtering methods enhance the quality of questions generated by neural ranking models in open-domain question answering systems?,"Filtering methods play a vital role in improving the quality of questions generated for neural ranking models in open-domain question answering (QA) systems. This is because a significant portion of generated questions can be of low quality, potentially providing misleading signals when used to train these models (Alberti et al., 2019). One common approach is filtering based on round-trip consistency (Alberti et al., 2019; Dong et al., 2019). In this method, a pre-trained QA system generates an answer from the produced question, and the question is retained only if the generated answer aligns with the original answer from which the question was derived.

To enhance flexibility, the strict consistency requirement can be relaxed by adjusting an acceptance threshold. This can be done using the probability from the pre-trained QA system (Zhang and Bansal, 2019; Lewis et al., 2021), the language model score from the generator itself (Shakeri et al., 2020; Liang et al., 2020), or an entailment score from a model trained on question-context-answer pairs. Additionally, Bartolo et al. (2021) suggest filtering questions based on ensemble consistency, where an ensemble of QA models, trained with different random seeds, is used, and only questions agreed upon by most models are selected.

In scenarios with minimal target-domain annotation, pseudo samples can be reweighted based on the validation loss, or reinforcement learning can be employed to select samples that lead to improvements in validation performance (Yue et al., 2022b). These filtering strategies collectively ensure that only high-quality questions are used, thereby enhancing the overall performance of neural ranking models in open-domain QA systems.",7,4,7,4,"Question:

How can filtering methods improve the quality of generated questions in neural ranking models for open-domain QA?

Answer:

Filtering is a crucial part of question generation since a significant portion of generated questions could be of low quality and would provide misleading signals when used to train the neural ranking model (Alberti et al., 2019). A typical choice is filtering based on round-trip consistency (Alberti et al., 2019; Dong et al., 2019), where a pre-trained QA system is applied to produce an answer based on the generated question. A question is kept only when the produced answer is consistent with the answer from which the question is generated. The strict consistency requirement can be relaxed by manually adjusting an acceptance threshold based on the probability from the pre-trained QA system (Zhang and Bansal, 2019; Lewis et al., 2021), the language model score from the generator itself (Shakeri et al., 2020; Liang et al., 2020), or an entailment score from a model trained on question-context-answer pairs. Bartolo et al. (2021) propose filtering questions based on ensemble consistency, where an ensemble of QA models are trained with different random seeds and only questions agreed by most QA models are selected. When minimal target-domain annotation is available, pseudo samples can be reweighted based on the validation loss, or reinforcement learning can be used to select samples that lead to validation performance gains (Yue et al., 2022b).","Question:

How do filtering methods enhance the quality of questions generated by neural ranking models in open-domain question answering systems?

Answer:

Filtering methods play a vital role in improving the quality of questions generated for neural ranking models in open-domain question answering (QA) systems. This is because a significant portion of generated questions can be of low quality, potentially providing misleading signals when used to train these models (Alberti et al., 2019). One common approach is filtering based on round-trip consistency (Alberti et al., 2019; Dong et al., 2019). In this method, a pre-trained QA system generates an answer from the produced question, and the question is retained only if the generated answer aligns with the original answer from which the question was derived.

To enhance flexibility, the strict consistency requirement can be relaxed by adjusting an acceptance threshold. This can be done using the probability from the pre-trained QA system (Zhang and Bansal, 2019; Lewis et al., 2021), the language model score from the generator itself (Shakeri et al., 2020; Liang et al., 2020), or an entailment score from a model trained on question-context-answer pairs. Additionally, Bartolo et al. (2021) suggest filtering questions based on ensemble consistency, where an ensemble of QA models, trained with different random seeds, is used, and only questions agreed upon by most models are selected.

In scenarios with minimal target-domain annotation, pseudo samples can be reweighted based on the validation loss, or reinforcement learning can be employed to select samples that lead to improvements in validation performance (Yue et al., 2022b). These filtering strategies collectively ensure that only high-quality questions are used, thereby enhancing the overall performance of neural ranking models in open-domain QA systems.",NO,True,1729,True,True
258378266-s10,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,2023,Answer as a Document,"As a straightforward way to leverage QA pairs, this method directly treats QA pairs as positive samples and does not distinguish between documents and answers (Lai et al., 2018). These QA pairs can provide direct WS signals to train the NR model:

where (q, a + ) ∈ Q × A are question-answer pairs in the target domain, a − 1∼n are sampled n negative answers and L is the standard ranking loss.

Though simple, this has been a common practice to ""warm up"" the NR model when no sufficient relevance annotations are available. For largesized models, this can be crucial to fully leverage the model capacity since we often have orders of magnitude more QA pairs than relevance annotations (Ni et al., 2021;Oguz et al., 2021). However, the style, structure and format differ between the document and the answer. The answer is a direct response to the question, and so it is easier to predict due to its strong semantic correlation with the question. Whereas the document can be implicit and may contain fewer obvious clues that can imply an answer; deep text understanding is required to predict the relevance between questions and documents Shen et al., 2022b). Therefore, this approach may be insufficient to reach satisfying results as a standalone method.",What challenges arise when using QA pairs as positive samples for neural ranking models?,"This method directly treats QA pairs as positive samples and does not distinguish between documents and answers (Lai et al., 2018). The style, structure, and format differ between the document and the answer. The answer is a direct response to the question, and so it is easier to predict due to its strong semantic correlation with the question. Whereas the document can be implicit and may contain fewer obvious clues that can imply an answer; deep text understanding is required to predict the relevance between questions and documents (Shen et al., 2022b). Therefore, this approach may be insufficient to reach satisfying results as a standalone method.",What are the challenges of using QA pairs as positive samples in neural ranking models?,"Using QA pairs as positive samples in neural ranking models presents several challenges. One major issue is that this method does not differentiate between documents and answers, as noted by Lai et al. (2018). The style, structure, and format of a document differ significantly from those of an answer. An answer is typically a direct response to a question, making it easier to predict due to its strong semantic correlation with the question. In contrast, a document may be more implicit and contain fewer obvious clues that suggest an answer, requiring deep text understanding to assess the relevance between questions and documents (Shen et al., 2022b). Consequently, relying solely on QA pairs as positive samples may not yield satisfactory results as a standalone approach.",7,4,7,4,"Question:

What challenges arise when using QA pairs as positive samples for neural ranking models?

Answer:

This method directly treats QA pairs as positive samples and does not distinguish between documents and answers (Lai et al., 2018). The style, structure, and format differ between the document and the answer. The answer is a direct response to the question, and so it is easier to predict due to its strong semantic correlation with the question. Whereas the document can be implicit and may contain fewer obvious clues that can imply an answer; deep text understanding is required to predict the relevance between questions and documents (Shen et al., 2022b). Therefore, this approach may be insufficient to reach satisfying results as a standalone method.","Question:

What are the challenges of using QA pairs as positive samples in neural ranking models?

Answer:

Using QA pairs as positive samples in neural ranking models presents several challenges. One major issue is that this method does not differentiate between documents and answers, as noted by Lai et al. (2018). The style, structure, and format of a document differ significantly from those of an answer. An answer is typically a direct response to a question, making it easier to predict due to its strong semantic correlation with the question. In contrast, a document may be more implicit and contain fewer obvious clues that suggest an answer, requiring deep text understanding to assess the relevance between questions and documents (Shen et al., 2022b). Consequently, relying solely on QA pairs as positive samples may not yield satisfactory results as a standalone approach.",NO,True,779,True,True
258378266-s12,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,2023,Latent-Variable Model,"We can still train the NR model on questiondocument pairs as in Answer-Document Mapping. However, instead of relying on a heuristic-based mapping function, we can treat this mapping as a ""latent variable"" within a probabilistic generative process (Lee et al., 2019;Shen, 2022). By this means, the NR model R gets WS signals from the QA reader G by maximizing the marginal likelihood:

where Z indicates all possible document combinations. Directly optimizing over Eq 6 is infeasible as it requires enumerating over all documents. A closed-form solution does not exist due to the deep neural network parameterization of R and G. The following section explains popular optimization options. An overview can be seen in Table 4.

Top-k approximation A popular approach is to assume a categorical distribution for R(Z|q); that is, to assume for each question only a single document is selected and the answer is generated from that one document. Eq 6 can be approximated by enumerating over only the top-k documents, assuming the remaining documents having negligibly small contributions to the likelihood:

This has been a popular choice in end-to-end training of text generation models (Lee et al., 2019;Shen et al., 2019b;Guu et al., 2020;Lewis et al., 2020;Shuster et al., 2021;Ferguson et al., 2022). Despite its simplicity, the top-k approximation has two main drawbacks. (1) The approximation is performed on the top-k documents obtained from the NR model. If the NR model is very weak at the beginning of training, these top-k documents can be a bad approximation to the real joint likelihood and the model might struggle to converge. (2) The assumption that document follow a categorical distribution might be problematic especially if the answer requires evidence from multiple documents (Wang and Pan, 2022).",What are the main drawbacks of using the top-k approximation in neural ranking models?,"The top-k approximation has two main drawbacks. The approximation is performed on the top-k documents obtained from the NR model. If the NR model is very weak at the beginning of training, these top-k documents can be a bad approximation to the real joint likelihood and the model might struggle to converge. The assumption that documents follow a categorical distribution might be problematic especially if the answer requires evidence from multiple documents (Wang and Pan, 2022).",What are the primary limitations of employing the top-k approximation in neural ranking models?,"The top-k approximation in neural ranking models presents two primary limitations. Firstly, this approximation is applied to the top-k documents retrieved by the neural ranking model. If the model is initially weak during the early stages of training, these top-k documents may not accurately represent the true joint likelihood, potentially hindering the model's ability to converge effectively. Secondly, the assumption that documents adhere to a categorical distribution can be problematic, particularly when the answer necessitates evidence from multiple documents (Wang and Pan, 2022).",7,2,7,2,"Question:

What are the main drawbacks of using the top-k approximation in neural ranking models?

Answer:

The top-k approximation has two main drawbacks. The approximation is performed on the top-k documents obtained from the NR model. If the NR model is very weak at the beginning of training, these top-k documents can be a bad approximation to the real joint likelihood and the model might struggle to converge. The assumption that documents follow a categorical distribution might be problematic especially if the answer requires evidence from multiple documents (Wang and Pan, 2022).","Question:

What are the primary limitations of employing the top-k approximation in neural ranking models?

Answer:

The top-k approximation in neural ranking models presents two primary limitations. Firstly, this approximation is applied to the top-k documents retrieved by the neural ranking model. If the model is initially weak during the early stages of training, these top-k documents may not accurately represent the true joint likelihood, potentially hindering the model's ability to converge effectively. Secondly, the assumption that documents adhere to a categorical distribution can be problematic, particularly when the answer necessitates evidence from multiple documents (Wang and Pan, 2022).",NO,True,590,True,True
258378191-s8,A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,2023,Two-Stage Supervised Keyphrase Extraction Models,"Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE 9  also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE 10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019;, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP 11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch 12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.  ",How do recent supervised models approach keyphrase extraction and what techniques do they employ?,"Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Recent supervised models, such as the one proposed by Xiong et al. (2019), formulate keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, incorporating pre-trained embeddings like ELMo (Peters et al., 2018) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Ainslie et al. (2020) replace the full self-attention of Transformers with local-global attention, significantly boosting the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pre-trained language models, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","How do recent supervised models approach keyphrase extraction, and what techniques do they employ?","Recent supervised models for keyphrase extraction have evolved to integrate candidate extraction and importance estimation into a cohesive end-to-end learning framework. This approach allows the model to rank and extract keyphrases using annotated data, optimizing both stages simultaneously. For instance, Xiong et al. (2019) introduced a model that treats keyphrase extraction as an n-gram level chunking task, determining whether a candidate is a keyphrase by using pre-trained embeddings like ELMo (Peters et al., 2018) within a convolutional transformer network to model n-gram representations. This model, known as BLING-KPE, has shown significant improvements over previous models.

To further enhance keyphrase extraction, SMART-KPE demonstrates that incorporating multimodal information from web pages, such as font, size, and DOM features, can improve open-domain web keyphrase extraction. Ainslie et al. (2020) improved performance for long documents by replacing the full self-attention of Transformers with local-global attention. Another model, SKE-BASE-RANK (Mu et al., 2020), uses a span-based approach to model the relationships between candidates and the document context.

JointKPE (Sun et al., 2020a) offers an open-domain keyphrase extraction method built on pre-trained language models, capturing both local phraseness and global informativeness. It ranks keyphrases by estimating their informativeness across the document and is trained on a keyphrase chunking task to ensure candidate phraseness. KIEMP (Song et al., 2021) enhances relevance by estimating the importance score of each candidate from multiple perspectives and introducing a matching module to align high-level concepts between the document and candidates.

To extract more relevant keyphrases, HyperMatch (Song et al., 2022a) explores keyphrase extraction in hyperbolic space. It maps phrase and document representations into the same hyperbolic space and models the relevance between candidate phrases and the document using the Poincaré distance, thereby improving the extraction of keyphrases.",7,4,8,4,"Question:

How do recent supervised models approach keyphrase extraction and what techniques do they employ?

Answer:

Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Recent supervised models, such as the one proposed by Xiong et al. (2019), formulate keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, incorporating pre-trained embeddings like ELMo (Peters et al., 2018) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Ainslie et al. (2020) replace the full self-attention of Transformers with local-global attention, significantly boosting the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context. JointKPE (Sun et al., 2020a) proposes an open-domain keyphrase extraction approach built on pre-trained language models, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincaré distance to extract keyphrases.","Question:

How do recent supervised models approach keyphrase extraction, and what techniques do they employ?

Answer:

Recent supervised models for keyphrase extraction have evolved to integrate candidate extraction and importance estimation into a cohesive end-to-end learning framework. This approach allows the model to rank and extract keyphrases using annotated data, optimizing both stages simultaneously. For instance, Xiong et al. (2019) introduced a model that treats keyphrase extraction as an n-gram level chunking task, determining whether a candidate is a keyphrase by using pre-trained embeddings like ELMo (Peters et al., 2018) within a convolutional transformer network to model n-gram representations. This model, known as BLING-KPE, has shown significant improvements over previous models.

To further enhance keyphrase extraction, SMART-KPE demonstrates that incorporating multimodal information from web pages, such as font, size, and DOM features, can improve open-domain web keyphrase extraction. Ainslie et al. (2020) improved performance for long documents by replacing the full self-attention of Transformers with local-global attention. Another model, SKE-BASE-RANK (Mu et al., 2020), uses a span-based approach to model the relationships between candidates and the document context.

JointKPE (Sun et al., 2020a) offers an open-domain keyphrase extraction method built on pre-trained language models, capturing both local phraseness and global informativeness. It ranks keyphrases by estimating their informativeness across the document and is trained on a keyphrase chunking task to ensure candidate phraseness. KIEMP (Song et al., 2021) enhances relevance by estimating the importance score of each candidate from multiple perspectives and introducing a matching module to align high-level concepts between the document and candidates.

To extract more relevant keyphrases, HyperMatch (Song et al., 2022a) explores keyphrase extraction in hyperbolic space. It maps phrase and document representations into the same hyperbolic space and models the relevance between candidate phrases and the document using the Poincaré distance, thereby improving the extraction of keyphrases.",NO,True,2086,True,True
258378191-s9,A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models,2023,One-Stage Supervised Keyphrase Extraction Models,"A major limitation of the above two-stage supervised approaches is classifying the labels of each candidate phrase independently while ignoring the dependencies that could potentially exist between candidates. Therefore, recent studies (Gollapalli et al., 2017;Basaldella et al., 2018;Wang et al., 2018;Alzaidy et al., 2019;Sun et al., 2019;Mu et al., 2020;Sahrawat et al., 2020) formulated keyphrase extraction as sequence labeling and showed that using linear-chain Conditional Random Fields improved the performance over baseline models for this task. Then, Mu et al. (2020) proposes SKE-BASE-CLS and -RANK, which directly extracts span-based phrase representations from all the document tokens via pre-trained language models and further learn to capture the interaction between them and their corresponding document to get better ranking results. Furthermore, this kind of model can extract overlapped keyphrases (Mu et al., 2020).",How do recent one-stage supervised models improve keyphrase extraction over two-stage approaches?,"Recent studies formulated keyphrase extraction as sequence labeling and showed that using linear-chain Conditional Random Fields improved the performance over baseline models for this task. Mu et al. (2020) propose SKE-BASE-CLS and -RANK, which directly extract span-based phrase representations from all the document tokens via pre-trained language models and further learn to capture the interaction between them and their corresponding document to get better ranking results. Furthermore, this kind of model can extract overlapped keyphrases (Mu et al., 2020).",How do recent one-stage supervised models enhance keyphrase extraction compared to traditional two-stage approaches?,"Recent advancements in keyphrase extraction have seen the formulation of the task as sequence labeling, where linear-chain Conditional Random Fields have been employed to enhance performance over traditional baseline models. Notably, Mu et al. (2020) introduced models like SKE-BASE-CLS and SKE-BASE-RANK, which leverage pre-trained language models to directly extract span-based phrase representations from all document tokens. These models further refine the extraction process by learning to capture interactions between the extracted phrases and their corresponding documents, leading to improved ranking results. Additionally, this approach allows for the extraction of overlapping keyphrases, as demonstrated by Mu et al. (2020).",4,2,7,2,"Question:

How do recent one-stage supervised models improve keyphrase extraction over two-stage approaches?

Answer:

Recent studies formulated keyphrase extraction as sequence labeling and showed that using linear-chain Conditional Random Fields improved the performance over baseline models for this task. Mu et al. (2020) propose SKE-BASE-CLS and -RANK, which directly extract span-based phrase representations from all the document tokens via pre-trained language models and further learn to capture the interaction between them and their corresponding document to get better ranking results. Furthermore, this kind of model can extract overlapped keyphrases (Mu et al., 2020).","Question:

How do recent one-stage supervised models enhance keyphrase extraction compared to traditional two-stage approaches?

Answer:

Recent advancements in keyphrase extraction have seen the formulation of the task as sequence labeling, where linear-chain Conditional Random Fields have been employed to enhance performance over traditional baseline models. Notably, Mu et al. (2020) introduced models like SKE-BASE-CLS and SKE-BASE-RANK, which leverage pre-trained language models to directly extract span-based phrase representations from all document tokens. These models further refine the extraction process by learning to capture interactions between the extracted phrases and their corresponding documents, leading to improved ranking results. Additionally, this approach allows for the extraction of overlapping keyphrases, as demonstrated by Mu et al. (2020).",NO,True,735,True,True
246863418-s1,A Survey on Dynamic Neural Networks for Natural Language Processing,2022-02-15,Skimming,"Skimming techniques, as summarized in Table 1, skip some time steps or allocate different computation on them. Intuitively, skimming matches how human beings efficiently read text and extract information from it (Li et al., 2019). By em-phasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies. The three categories of skimming are skipping and early stopping, computation reduction, and dynamic hierarchical RNN, corresponding with three motivations: to skip unimportant input, to allocate less computation to unimportant input, and to increase computation to important input only.

Skipping and Early Stopping Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early. LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU). At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading. LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions. The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up. Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update. If the gate decides to skip a time step, the hidden states will be directly copied without any update.

To stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level. Jumper  first splits a paragraph to several sub-sentences and encodes them into sentence embeddings. They then apply early stopping at a sentence when the policy network decides to stop reading. Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text. They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early. Yu et al. (2018) add a rereading operation to LSTM-Jump (Yu et al., 2017) which allows the model to stay on the current time step, allocating more computation to important information.

The aforementioned techniques can only go forward, which makes it impossible to regret if hav-",How do skipping and early stopping techniques improve efficiency in dynamic neural networks for NLP?,"Skimming techniques skip some time steps or allocate different computation on them. By emphasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies. Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early. LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU). At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading. LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions. The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up. Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update. If the gate decides to skip a time step, the hidden states will be directly copied without any update. To stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level. Jumper first splits a paragraph into several sub-sentences and encodes them into sentence embeddings. They then apply early stopping at a sentence when the policy network decides to stop reading. Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text. They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early.",How do skipping and early stopping techniques enhance the efficiency of dynamic neural networks in natural language processing?,"Skipping and early stopping techniques significantly enhance the efficiency of dynamic neural networks in natural language processing (NLP) by optimizing the way models process sequences. Skimming techniques, for instance, allow models to skip certain time steps or allocate varying levels of computation to them. This approach emphasizes important information within a sequence while ignoring less significant parts, leading to faster inference speeds and improved capture of long-term dependencies.

Skipping mechanisms, such as LSTM-Jump (Yu et al., 2017), are designed to ignore irrelevant information in natural language understanding (NLU). At each step, LSTM-Jump uses the current states to compute a ""jumping softmax,"" which determines how many steps to jump forward and whether to stop reading. This model employs policy gradient to train itself to make non-differentiable discrete jumping decisions, with a binary reward function that rewards correct predictions and penalizes incorrect ones. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to a 6× speed-up.

Similarly, Skip RNN (Campos et al., 2018) introduces a binary gate to decide whether to skip a state update. If the gate opts to skip a time step, the hidden states are directly copied without any update. For early stopping, Rea-soNet (Shen et al., 2017) introduces a terminal state that decides whether to terminate early during machine reading comprehension at each time step at the token level. Jumper splits a paragraph into sub-sentences, encodes them into sentence embeddings, and applies early stopping when the policy network decides to stop reading.

Research by Li et al. (2019) using eye-tracking devices confirms that skipping and early stopping are common behaviors when humans read text. They propose a Reading Inspired Model to mimic these human behaviors, allowing the model to decide whether to skip a single time step or stop reading early. These techniques collectively contribute to more efficient processing in dynamic neural networks for NLP tasks.",7,2,7,2,"Question:

How do skipping and early stopping techniques improve efficiency in dynamic neural networks for NLP?

Answer:

Skimming techniques skip some time steps or allocate different computation on them. By emphasizing the important information within a sequence and ignoring parts with little importance, skimming helps the model achieve faster inference speed and better capture long-term dependencies. Skipping and early stopping aim to improve efficiency for a long sequence by skipping some tokens or stopping reading early. LSTM-Jump (Yu et al., 2017) is a skipping mechanism to ignore irrelevant information for natural language understanding (NLU). At each step, the current states are used to compute a ""jumping softmax"", which decides how many steps to jump forward and whether to stop reading. LSTM-Jump employs policy gradient to train the model to make non-differentiable discrete jumping decisions. The reward is a binary function which rewards a correct prediction and penalizes an incorrect prediction of the label. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to 6× speed-up. Skip RNN (Campos et al., 2018) introduces a binary gate to learn whether to skip a state update. If the gate decides to skip a time step, the hidden states will be directly copied without any update. To stop reading early as needed, Rea-soNet (Shen et al., 2017) introduces a terminal state which decides whether to terminate early for machine reading comprehension on each time step at the token level. Jumper first splits a paragraph into several sub-sentences and encodes them into sentence embeddings. They then apply early stopping at a sentence when the policy network decides to stop reading. Li et al. (2019) use eye-tracking devices and confirm that skipping and early stopping are common when humans read text. They propose Reading Inspired Model to mimic the behaviors of humans, which allows the model to decide whether to skip a single time step or stop reading early.","Question:

How do skipping and early stopping techniques enhance the efficiency of dynamic neural networks in natural language processing?

Answer:

Skipping and early stopping techniques significantly enhance the efficiency of dynamic neural networks in natural language processing (NLP) by optimizing the way models process sequences. Skimming techniques, for instance, allow models to skip certain time steps or allocate varying levels of computation to them. This approach emphasizes important information within a sequence while ignoring less significant parts, leading to faster inference speeds and improved capture of long-term dependencies.

Skipping mechanisms, such as LSTM-Jump (Yu et al., 2017), are designed to ignore irrelevant information in natural language understanding (NLU). At each step, LSTM-Jump uses the current states to compute a ""jumping softmax,"" which determines how many steps to jump forward and whether to stop reading. This model employs policy gradient to train itself to make non-differentiable discrete jumping decisions, with a binary reward function that rewards correct predictions and penalizes incorrect ones. Compared to a standard LSTM, LSTM-Jump achieves better accuracy with up to a 6× speed-up.

Similarly, Skip RNN (Campos et al., 2018) introduces a binary gate to decide whether to skip a state update. If the gate opts to skip a time step, the hidden states are directly copied without any update. For early stopping, Rea-soNet (Shen et al., 2017) introduces a terminal state that decides whether to terminate early during machine reading comprehension at each time step at the token level. Jumper splits a paragraph into sub-sentences, encodes them into sentence embeddings, and applies early stopping when the policy network decides to stop reading.

Research by Li et al. (2019) using eye-tracking devices confirms that skipping and early stopping are common behaviors when humans read text. They propose a Reading Inspired Model to mimic these human behaviors, allowing the model to decide whether to skip a single time step or stop reading early. These techniques collectively contribute to more efficient processing in dynamic neural networks for NLP tasks.",NO,True,2064,True,True
246863418-s7,A Survey on Dynamic Neural Networks for Natural Language Processing,2022-02-15,Confidence-based Early Exit,"Early works for early exit in computer vision (Park et al., 2015;Teerapittayanon et al., 2016;Kaya et al., 2019) often fall into this category. They define a metric as the proxy for confidence of a model prediction. The model exits early when the confidence hits a predefined threshold. DeeBERT (Xin et al., 2020b) applies BranchyNet (Teerapittayanon et al., 2016) to BERT inference. The training for DeeBERT is two-stage: they first train BERT on downstream tasks following standard fine-tuning. Then, they freeze the parameters of the Transformer and insert a linear classifier (i.e., internal classifier) after each Transformer layer. They train the classifiers by minimizing the sum of their cross-entropy loss. For inference, the model exits early when an internal classifier outputs a prediction probability distribution that has an entropy lower than a predefined threshold. RightTool (Schwartz et al., 2020) jointly finetunes BERT with internal classifiers. They use the temperature-calibrated maximum class probability as confidence. FastBERT  first trains the BERT backbone and the final classifier. Then, they distill the final classifier layer to the internal classifiers (Hinton et al., 2015). For inference, the model exits when the entropy of a prediction is below the threshold. Rome-BERT (Geng et al., 2021) provides a simple fix for learning internal classifiers efficiently. Besides self-distillation as in FastBERT, they propose gradient regularization (GR) to facilitate distillation. SkipBERT (Wang et al., 2022) caches pre-computed representation of text chunks to re-",How do various models implement confidence-based early exit strategies in BERT inference?,"Early works for early exit in computer vision often define a metric as the proxy for confidence of a model prediction, and the model exits early when the confidence hits a predefined threshold. DeeBERT applies BranchyNet to BERT inference by first training BERT on downstream tasks following standard fine-tuning. Then, it freezes the parameters of the Transformer and inserts a linear classifier (i.e., internal classifier) after each Transformer layer. The classifiers are trained by minimizing the sum of their cross-entropy loss. For inference, the model exits early when an internal classifier outputs a prediction probability distribution that has an entropy lower than a predefined threshold. RightTool jointly finetunes BERT with internal classifiers and uses the temperature-calibrated maximum class probability as confidence. FastBERT first trains the BERT backbone and the final classifier, then distills the final classifier layer to the internal classifiers. For inference, the model exits when the entropy of a prediction is below the threshold. Rome-BERT provides a simple fix for learning internal classifiers efficiently by proposing gradient regularization (GR) to facilitate distillation, in addition to self-distillation as in FastBERT.",How are confidence-based early exit strategies implemented in BERT inference models?,"In the realm of computer vision, early exit strategies often rely on a confidence metric to determine when a model should terminate its processing early. This is typically achieved by setting a predefined confidence threshold. In the context of BERT inference, several models have adopted similar strategies. DeeBERT, for instance, applies the BranchyNet approach by initially fine-tuning BERT on downstream tasks. After this, the parameters of the Transformer are frozen, and a linear classifier, known as an internal classifier, is inserted after each Transformer layer. These classifiers are trained by minimizing the sum of their cross-entropy loss. During inference, the model exits early if an internal classifier produces a prediction probability distribution with entropy below a certain threshold.

RightTool takes a slightly different approach by jointly fine-tuning BERT with internal classifiers and using the temperature-calibrated maximum class probability as a measure of confidence. FastBERT, on the other hand, first trains the BERT backbone along with the final classifier and then distills the final classifier layer to the internal classifiers. The model exits early during inference when the entropy of a prediction falls below the threshold.

Rome-BERT introduces a straightforward enhancement for efficiently learning internal classifiers by proposing gradient regularization (GR) to aid in distillation, complementing the self-distillation process used in FastBERT. These strategies collectively illustrate the diverse methods employed to implement confidence-based early exit strategies in BERT inference models.",7,4,7,2,"Question:

How do various models implement confidence-based early exit strategies in BERT inference?

Answer:

Early works for early exit in computer vision often define a metric as the proxy for confidence of a model prediction, and the model exits early when the confidence hits a predefined threshold. DeeBERT applies BranchyNet to BERT inference by first training BERT on downstream tasks following standard fine-tuning. Then, it freezes the parameters of the Transformer and inserts a linear classifier (i.e., internal classifier) after each Transformer layer. The classifiers are trained by minimizing the sum of their cross-entropy loss. For inference, the model exits early when an internal classifier outputs a prediction probability distribution that has an entropy lower than a predefined threshold. RightTool jointly finetunes BERT with internal classifiers and uses the temperature-calibrated maximum class probability as confidence. FastBERT first trains the BERT backbone and the final classifier, then distills the final classifier layer to the internal classifiers. For inference, the model exits when the entropy of a prediction is below the threshold. Rome-BERT provides a simple fix for learning internal classifiers efficiently by proposing gradient regularization (GR) to facilitate distillation, in addition to self-distillation as in FastBERT.","Question:

How are confidence-based early exit strategies implemented in BERT inference models?

Answer:

In the realm of computer vision, early exit strategies often rely on a confidence metric to determine when a model should terminate its processing early. This is typically achieved by setting a predefined confidence threshold. In the context of BERT inference, several models have adopted similar strategies. DeeBERT, for instance, applies the BranchyNet approach by initially fine-tuning BERT on downstream tasks. After this, the parameters of the Transformer are frozen, and a linear classifier, known as an internal classifier, is inserted after each Transformer layer. These classifiers are trained by minimizing the sum of their cross-entropy loss. During inference, the model exits early if an internal classifier produces a prediction probability distribution with entropy below a certain threshold.

RightTool takes a slightly different approach by jointly fine-tuning BERT with internal classifiers and using the temperature-calibrated maximum class probability as a measure of confidence. FastBERT, on the other hand, first trains the BERT backbone along with the final classifier and then distills the final classifier layer to the internal classifiers. The model exits early during inference when the entropy of a prediction falls below the threshold.

Rome-BERT introduces a straightforward enhancement for efficiently learning internal classifiers by proposing gradient regularization (GR) to aid in distillation, complementing the self-distillation process used in FastBERT. These strategies collectively illustrate the diverse methods employed to implement confidence-based early exit strategies in BERT inference models.",NO,True,1637,True,True
246863418-s9,A Survey on Dynamic Neural Networks for Natural Language Processing,2022-02-15,Internal classifier training Exit criterion,"DeeBERT (Xin et al., 2020b) two-stage; sum of CE loss entropy < θ RightTool (Schwartz et al., 2020) joint; sum of CE loss calibrated max class probability > θ FastBERT  two-stage; self-distillation entropy < θ RomeBERT (Geng et al., 2021) joint; self-distillation + GR entropy < θ SkipBERT (2022) joint; weighted sum of CE + KD max class probability > θ PABEE (Zhou et al., 2020a) joint; weighted sum of CE loss patience (#consistent prediction > θ ) Voting  joint; sum of CE + diversity loss accumulated votes > θ LeeBERT (Zhu, 2021) joint; auto-weighted sum of CE + KD loss patience (#consistent prediction > θ ) Past-Future (Liao et al., 2021) joint; weighted sum of CE + imitation learning entropy < θ PCEE-BERT (2022a) joint; weighted sum of CE patience (#consistent IC confidence > θ)

BERxiT (Xin et al., 2021) alternate; sum of CE loss estimated confidence > θ CAT (Schuster et al., 2021) joint; avg. of CE loss estimated conformity > θ

CascadeBERT (Li et al., 2021a) standard model FT with confidence calibration calibrated max class probability > θ place lower BERT layers and uses confidencebased early exit for higher layers to achieve maximum acceleration.

Ensemble-based Early Exit One drawback in confidence-based early exit is wasted computation. That is to say, if the confidence of an internal classifier does not satisfy the exit criterion, it will be disregarded. Ensemble-based early exit recycles these predictions and considers output from multiple internal classifiers to make better predictions. Based on the similarity between overfitting and overthinking, PABEE (Zhou et al., 2020a) borrows early stopping from model training. They first jointly train the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. Other than improvement on performance and efficiency, they find that PABEE can improve adversarial robustness, which they attribute to the ensemble effect.  further introduce a diversity loss that encourages internal classifiers to have a diverse predicted probability distribution. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. Interestingly, LeeBERT (Zhu, 2021) adopts the opposite strategy: they promote consistency across internal classifiers by distilling them to each other. However, they introduce a learnable weight for the cross-entropy loss of each classifier and the distillation loss between each pair. They optimize these weights by a cross-level optimization algorithm.

They adopt PABEE's patience-based strategy for exiting. Liao et al. (2021) train linear transformation layers called ""imitation learners"", to approximate the hidden states of future layers based on current hidden states. For inference, the prediction after each layer is calculated by mixing the past predictions and the future predictions of the imitation learners. Entropy is used as the exit criterion. PCEE-BERT (Zhang et al., 2022a) borrows from both ensemble-based exit and confidencebased methods. The inference is terminated when multiple layers are confident.

Learning-based Early Exit Another stream of research is to learn a criterion for early exiting.

BERxiT (Xin et al., 2021) alternates between joint fine-tuning and two-stage fine-tuning by freezing parameters of Transformer and the final classifier for even-numbered iterations and unfreezing them for odd-numbered iterations. They also train a linear layer called a learning-to-exit (LTE) module to predict whether the current internal classifier makes the correct prediction. It takes the hidden states as input and outputs a confidence score, which is used to decide whether to exit. CAT (Schuster et al., 2021) introduces a ""meta consistency classifier"" to predict whether the output of an internal classifier conforms to the final classifier and exits when the consistency classifier predicts a certain level of conformity.

Cascading Cascading can be seen as a special form of early exit, performed at the model level. Li et al. (2021a) find that shallow features and internal classifiers in the first few layers of BERT utilized by early exit methods like DeeBERT (Xin et al., 2020b) are not sufficient and reliable, underperforming a fine-tuned BERT with the same number of layers. Therefore, they propose to use a suite of complete models with different numbers of layers for cascading. CascadeBERT executes models one by one, from the smallest to the largest. It stops when a model outputs a confidence score (calibrated maximum class probability) that reaches the threshold.

Applications Although early exit is originally developed for classification, there have been works extending it to more tasks and settings. Li et al. (2021b) propose Token-Level Early-Exit that targets early exiting for sequence labeling. They use the maximum class probability as confidence on a per-token basis. Once the confidence hits the threshold, the hidden states of the corresponding tokens will be frozen and directly copied to upper layers. These exited tokens will not attend to other tokens at upper layers but can still be attended by other tokens. The model completely exits when every token exits. A similar idea is also presented in Elbayad et al. (2020) and Liu et al. (2021b) where hidden states of some positions can be frozen and directly copied to upper layers, although the former is focused on generation and the latter is for classification. Xin et al. (2020a) apply DeeBERT (Xin et al., 2020b) to document ranking and set different thresholds to the negative and positive classes for early exiting, to accommodate the imbalanced class distribution in document ranking. ELUE (Liu et al., 2021a) is a benchmark which evaluates the Pareto Front of early exit models on the FLOPs-performance plane. They provide a BERT-like baseline with jointly pretrained internal classifiers, to mitigate the gap between pretraining and fine-tuning.",How does the voting mechanism enhance ensemble-based early exit in dynamic neural networks?,"Ensemble-based early exit recycles predictions and considers output from multiple internal classifiers to make better predictions. PABEE (Zhou et al., 2020a) borrows early stopping from model training and jointly trains the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. This mechanism not only improves performance and efficiency but also enhances adversarial robustness, attributed to the ensemble effect.",How does the voting mechanism improve the performance of ensemble-based early exit in dynamic neural networks?,"Ensemble-based early exit in dynamic neural networks enhances predictions by utilizing outputs from multiple internal classifiers. This approach, as demonstrated by PABEE (Zhou et al., 2020a), incorporates early stopping techniques from model training and involves jointly training internal classifiers with BERT. This is achieved through a weighted sum of cross-entropy losses, where greater emphasis is placed on the upper classifiers. During inference, the model is designed to exit when k consecutive internal classifiers produce the same prediction. The voting mechanism plays a crucial role by allowing the model to exit early once a class accumulates more votes than a predetermined threshold. This not only boosts performance and efficiency but also enhances adversarial robustness, thanks to the ensemble effect.",7,2,4,2,"Question:

How does the voting mechanism enhance ensemble-based early exit in dynamic neural networks?

Answer:

Ensemble-based early exit recycles predictions and considers output from multiple internal classifiers to make better predictions. PABEE (Zhou et al., 2020a) borrows early stopping from model training and jointly trains the internal classifiers with BERT by a weighted sum of cross-entropy losses that assigns larger weights for upper classifiers. For inference, the model exits when k consecutive internal classifiers make the same prediction. They propose a voting mechanism to ensemble the internal classifiers by exiting early when a class has accumulated more votes than the threshold. This mechanism not only improves performance and efficiency but also enhances adversarial robustness, attributed to the ensemble effect.","Question:

How does the voting mechanism improve the performance of ensemble-based early exit in dynamic neural networks?

Answer:

Ensemble-based early exit in dynamic neural networks enhances predictions by utilizing outputs from multiple internal classifiers. This approach, as demonstrated by PABEE (Zhou et al., 2020a), incorporates early stopping techniques from model training and involves jointly training internal classifiers with BERT. This is achieved through a weighted sum of cross-entropy losses, where greater emphasis is placed on the upper classifiers. During inference, the model is designed to exit when k consecutive internal classifiers produce the same prediction. The voting mechanism plays a crucial role by allowing the model to exit early once a class accumulates more votes than a predetermined threshold. This not only boosts performance and efficiency but also enhances adversarial robustness, thanks to the ensemble effect.",NO,True,821,True,True
254877753-s3,Towards Reasoning in Large Language Models: A Survey,2022-12-20,Fully Supervised Finetuning,"Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets. For example, Rajani et al.

(2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019). Talmor et al. (2020) train RoBERTa (Liu et al., 2019) to perform reasoning/inference based on both implicit pre-trained knowledge and explicit free-text statements. Hendrycks et al. (2021) finetune pretrained 2 It is important to note that the term ""reasoning"" in this paper does not necessarily imply that LLMs are truly capable of reasoning or that they are able to reason in the same way that humans do. We will discuss this issue in more detail in §6. language models to solve competition mathematics problems by generating full step-by-step solutions, though the accuracy is relatively low. Nye et al. (2022) train language models to do multi-step reasoning for program synthesis/execution by generating ""scratchpads"", i.e., intermediate computations, before producing the final answers. We refer the reader to Helwe et al. (2021); Bhargava and Ng (2022)'s survey for more studies in this line.

There are two major limitations of fully supervised finetuning. First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create. Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.",What are the methods and challenges of improving reasoning in small language models through fully supervised finetuning?,"Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets. For example, Rajani et al. (2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019). Talmor et al. (2020) train RoBERTa (Liu et al., 2019) to perform reasoning/inference based on both implicit pre-trained knowledge and explicit free-text statements. Hendrycks et al. (2021) finetune pretrained language models to solve competition mathematics problems by generating full step-by-step solutions, though the accuracy is relatively low. Nye et al. (2022) train language models to do multi-step reasoning for program synthesis/execution by generating ""scratchpads"", i.e., intermediate computations, before producing the final answers. There are two major limitations of fully supervised finetuning. First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create. Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.",What are the methods and challenges associated with enhancing reasoning capabilities in small language models through fully supervised finetuning?,"Improving reasoning in small language models through fully supervised finetuning involves several methods and faces distinct challenges. Research has shown that finetuning on specific datasets can elicit or enhance reasoning abilities in these models. For instance, Rajani et al. (2019) finetuned a pretrained GPT model (Radford et al., 2018) using the CoS-E dataset to generate rationales that explain model predictions. This approach led to improved performance on commonsense question answering tasks (Talmor et al., 2019). Similarly, Talmor et al. (2020) trained RoBERTa (Liu et al., 2019) to perform reasoning and inference by leveraging both implicit pre-trained knowledge and explicit free-text statements. Hendrycks et al. (2021) focused on finetuning pretrained language models to solve competition mathematics problems by generating full step-by-step solutions, although the accuracy remained relatively low. Additionally, Nye et al. (2022) trained language models for multi-step reasoning in program synthesis and execution by generating ""scratchpads,"" or intermediate computations, before arriving at final answers.

However, fully supervised finetuning presents two major limitations. First, it necessitates a dataset containing explicit reasoning, which can be challenging and time-consuming to create. Second, the model is typically trained on a specific dataset, restricting its application to a particular domain. This limitation may lead the model to rely on artifacts in the training data rather than genuine reasoning to make predictions.",7,6,7,6,"Question:

What are the methods and challenges of improving reasoning in small language models through fully supervised finetuning?

Answer:

Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets. For example, Rajani et al. (2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019). Talmor et al. (2020) train RoBERTa (Liu et al., 2019) to perform reasoning/inference based on both implicit pre-trained knowledge and explicit free-text statements. Hendrycks et al. (2021) finetune pretrained language models to solve competition mathematics problems by generating full step-by-step solutions, though the accuracy is relatively low. Nye et al. (2022) train language models to do multi-step reasoning for program synthesis/execution by generating ""scratchpads"", i.e., intermediate computations, before producing the final answers. There are two major limitations of fully supervised finetuning. First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create. Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.","Question:

What are the methods and challenges associated with enhancing reasoning capabilities in small language models through fully supervised finetuning?

Answer:

Improving reasoning in small language models through fully supervised finetuning involves several methods and faces distinct challenges. Research has shown that finetuning on specific datasets can elicit or enhance reasoning abilities in these models. For instance, Rajani et al. (2019) finetuned a pretrained GPT model (Radford et al., 2018) using the CoS-E dataset to generate rationales that explain model predictions. This approach led to improved performance on commonsense question answering tasks (Talmor et al., 2019). Similarly, Talmor et al. (2020) trained RoBERTa (Liu et al., 2019) to perform reasoning and inference by leveraging both implicit pre-trained knowledge and explicit free-text statements. Hendrycks et al. (2021) focused on finetuning pretrained language models to solve competition mathematics problems by generating full step-by-step solutions, although the accuracy remained relatively low. Additionally, Nye et al. (2022) trained language models for multi-step reasoning in program synthesis and execution by generating ""scratchpads,"" or intermediate computations, before arriving at final answers.

However, fully supervised finetuning presents two major limitations. First, it necessitates a dataset containing explicit reasoning, which can be challenging and time-consuming to create. Second, the model is typically trained on a specific dataset, restricting its application to a particular domain. This limitation may lead the model to rely on artifacts in the training data rather than genuine reasoning to make predictions.",NO,True,1558,True,True
254854317-s13,The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges,2022-12-19,The Era of Statistical Methods,"The research on CSW is also influenced by the progress and development of machine learning. According to Figure 5, starting in 2006, statistical methods have been adapted to CSW research, while before that year, the approaches were mainly rule-based. There are common statistical methods for text classification used in the literature, such as Naive Bayes (Solorio and Liu, 2008a) and Support Vector Machine (SVM) (Solorio and Liu, 2008b). Conditional Random Field (CRF) (Sutton et al., 2012) is also widely seen in the literature for sequence labeling, such as Part-of-Speech (POS) tagging (Vyas et al., 2014), Named Entity Recognition (NER), and word-level language identification (Lin et al., 2014;Chittaranjan et al., 2014;Jain and Bhat, 2014). HMM-based models have been used in speech-related tasks, such as speech recognition (Weiner et al., 2012a;Li and Fung, 2013) and text synthesis (Qian et al., 2008;Shuang et al., 2010;He et al., 2012).",What are the common statistical methods used for text classification and sequence labeling in code-switching research?,"There are common statistical methods for text classification used in the literature, such as Naive Bayes (Solorio and Liu, 2008a) and Support Vector Machine (SVM) (Solorio and Liu, 2008b). Conditional Random Field (CRF) (Sutton et al., 2012) is also widely seen in the literature for sequence labeling, such as Part-of-Speech (POS) tagging (Vyas et al., 2014), Named Entity Recognition (NER), and word-level language identification (Lin et al., 2014; Chittaranjan et al., 2014; Jain and Bhat, 2014).",What statistical methods are commonly employed for text classification and sequence labeling in code-switching research?,"In the realm of code-switching research, several statistical methods are frequently utilized for text classification and sequence labeling tasks. For text classification, Naive Bayes (Solorio and Liu, 2008a) and Support Vector Machine (SVM) (Solorio and Liu, 2008b) are among the most commonly used techniques. When it comes to sequence labeling, Conditional Random Field (CRF) (Sutton et al., 2012) is a popular choice. This method is widely applied in tasks such as Part-of-Speech (POS) tagging (Vyas et al., 2014), Named Entity Recognition (NER), and word-level language identification (Lin et al., 2014; Chittaranjan et al., 2014; Jain and Bhat, 2014).",8,4,7,4,"Question:

What are the common statistical methods used for text classification and sequence labeling in code-switching research?

Answer:

There are common statistical methods for text classification used in the literature, such as Naive Bayes (Solorio and Liu, 2008a) and Support Vector Machine (SVM) (Solorio and Liu, 2008b). Conditional Random Field (CRF) (Sutton et al., 2012) is also widely seen in the literature for sequence labeling, such as Part-of-Speech (POS) tagging (Vyas et al., 2014), Named Entity Recognition (NER), and word-level language identification (Lin et al., 2014; Chittaranjan et al., 2014; Jain and Bhat, 2014).","Question:

What statistical methods are commonly employed for text classification and sequence labeling in code-switching research?

Answer:

In the realm of code-switching research, several statistical methods are frequently utilized for text classification and sequence labeling tasks. For text classification, Naive Bayes (Solorio and Liu, 2008a) and Support Vector Machine (SVM) (Solorio and Liu, 2008b) are among the most commonly used techniques. When it comes to sequence labeling, Conditional Random Field (CRF) (Sutton et al., 2012) is a popular choice. This method is widely applied in tasks such as Part-of-Speech (POS) tagging (Vyas et al., 2014), Named Entity Recognition (NER), and word-level language identification (Lin et al., 2014; Chittaranjan et al., 2014; Jain and Bhat, 2014).",NO,True,656,True,True
264426545-s3,Automatic Pronunciation Assessment -A Review,2023-10-21,Prosodic Errors,"Prosodic features encompass elements that influence the pronunciation of an entire word or sentence, including stress, rhythm, and intonation.Errors related to prosodic features involve the production of larger sound units.For intelligibility, prosodic features particularly play a significant role (Raux and Kawahara, 2002).This is especially true for tonal languages (Dahmen et al., 2023) where variation in the pitch can lead to words with different meanings.Prosodic errors are often languagedependent and categorized by: stress (lexical and sentence), rhythm, and intonation.Accent PCC: 68% (Rasipuram et al., 2015) ERJ (Minematsu et al., 2004) * English Japanese /68,000 200 # Utterance PCC (Luan et al., 2012).Word Intelligibility (Minematsu et al., 2011).Phoneme Errors (Ito et al., 2005) CU-CHLOE (Meng et al., 2007a Stress is the emphasis placed on certain syllables in a word or sentence.It is articulated by increasing the loudness, duration, and pitch of the stressed syllable.It can be categorized as lexical stress, if the stress is placed on syllables within the word, or sentence stress if the stress is placed on words within sentences.Mandarin learners of English have contrastive stress at the word-level that is absent in Korean, Mandarin speakers can have an advantage over Korean speakers in stress processing of English words (Wang, 2022).

Rythm is the pattern of stressed and unstressed syllables in a word or sentence.A language can be classified as either stress-timed or syllable-timed (Ohata, 2004;Matthews, 2014).In stress-timed languages, the duration of stressed syllables tends to dominate the overall time required to complete a sentence.Conversely, in syllable-timed languages, each syllable receives an equal amount of time during production.

Intonation refers to the melodic pattern and pitch variations in speech.L2 learners of Vietnamese and Mandarin Chinese encounter significant difficulty in acquiring distinct tones, particularly if their native language lacks tonality.Such tonal languages rely on different pitch patterns to convey distinct meanings, making it challenging for learners to accurately grasp and reproduce these tonal variations (Nguyen et al., 2014;Chen et al., 2015).","How are prosodic errors categorized, and what factors influence their categorization in pronunciation assessment?","Prosodic errors are often language-dependent and categorized by stress (lexical and sentence), rhythm, and intonation. Stress is the emphasis placed on certain syllables in a word or sentence and can be categorized as lexical stress if the stress is placed on syllables within the word, or sentence stress if the stress is placed on words within sentences. Rhythm is the pattern of stressed and unstressed syllables in a word or sentence, and a language can be classified as either stress-timed or syllable-timed. Intonation refers to the melodic pattern and pitch variations in speech. Factors influencing their categorization include the language being assessed, as prosodic features are language-dependent, and the specific prosodic elements such as stress, rhythm, and intonation that are being evaluated.","How are prosodic errors categorized in pronunciation assessment, and what factors influence their categorization?","Prosodic errors in pronunciation assessment are typically categorized based on stress, rhythm, and intonation, and these categories are often language-dependent. Stress errors can be further divided into lexical stress, which involves the emphasis on certain syllables within a word, and sentence stress, which pertains to the emphasis on words within a sentence. Rhythm errors relate to the pattern of stressed and unstressed syllables in speech, with languages being classified as either stress-timed or syllable-timed. Intonation errors involve the melodic pattern and pitch variations in speech. The categorization of these prosodic errors is influenced by the specific language being assessed, as prosodic features vary across languages, and by the particular prosodic elements—such as stress, rhythm, and intonation—that are being evaluated.",7,4,7,4,"Question:

How are prosodic errors categorized, and what factors influence their categorization in pronunciation assessment?

Answer:

Prosodic errors are often language-dependent and categorized by stress (lexical and sentence), rhythm, and intonation. Stress is the emphasis placed on certain syllables in a word or sentence and can be categorized as lexical stress if the stress is placed on syllables within the word, or sentence stress if the stress is placed on words within sentences. Rhythm is the pattern of stressed and unstressed syllables in a word or sentence, and a language can be classified as either stress-timed or syllable-timed. Intonation refers to the melodic pattern and pitch variations in speech. Factors influencing their categorization include the language being assessed, as prosodic features are language-dependent, and the specific prosodic elements such as stress, rhythm, and intonation that are being evaluated.","Question:

How are prosodic errors categorized in pronunciation assessment, and what factors influence their categorization?

Answer:

Prosodic errors in pronunciation assessment are typically categorized based on stress, rhythm, and intonation, and these categories are often language-dependent. Stress errors can be further divided into lexical stress, which involves the emphasis on certain syllables within a word, and sentence stress, which pertains to the emphasis on words within a sentence. Rhythm errors relate to the pattern of stressed and unstressed syllables in speech, with languages being classified as either stress-timed or syllable-timed. Intonation errors involve the melodic pattern and pitch variations in speech. The categorization of these prosodic errors is influenced by the specific language being assessed, as prosodic features vary across languages, and by the particular prosodic elements—such as stress, rhythm, and intonation—that are being evaluated.",NO,True,847,True,True
264426545-s7,Automatic Pronunciation Assessment -A Review,2023-10-21,Classification based on Acoustic Phonetics,"Classifier-based approaches explored both segmental and prosodic aspects of pronunciation.Segmental approaches involve the use of classifiers targeting specific phoneme pair errors, utilizing different acoustic features such as Mel-frequency cepstral coefficients (MFCCs) along with its first and second derivative, energy, zero-cross, and spectral features (Van Doremalen et al., 2009;Huang et al., 2020), with different techniques such as Linear Discriminant Analysis (LDA) (Truong et al., 2004;Strik et al., 2009), decision trees (Strik et al., 2009).Prosodic approaches focus on detecting lexical stress and tones, utilizing features such as energy, pitch, duration, and spectral characteristics, with classifiers like Gaussian mixture models (GMMs) (Ferrer et al., 2015), support vector machines (SVMs) (Chen and Wang, 2010;Shahin et al., 2016), and deep neural network (DNNs) (Shahin et al., 2016), and multi-distribution DNNs (Li et al., 2018a).",How do classifier-based approaches utilize segmental and prosodic features for pronunciation assessment?  ,"Classifier-based approaches explored both segmental and prosodic aspects of pronunciation. Segmental approaches involve the use of classifiers targeting specific phoneme pair errors, utilizing different acoustic features such as Mel-frequency cepstral coefficients (MFCCs) along with its first and second derivative, energy, zero-cross, and spectral features (Van Doremalen et al., 2009; Huang et al., 2020), with different techniques such as Linear Discriminant Analysis (LDA) (Truong et al., 2004; Strik et al., 2009), decision trees (Strik et al., 2009). Prosodic approaches focus on detecting lexical stress and tones, utilizing features such as energy, pitch, duration, and spectral characteristics, with classifiers like Gaussian mixture models (GMMs) (Ferrer et al., 2015), support vector machines (SVMs) (Chen and Wang, 2010; Shahin et al., 2016), and deep neural network (DNNs) (Shahin et al., 2016), and multi-distribution DNNs (Li et al., 2018a).",How do classifier-based approaches use segmental and prosodic features in pronunciation assessment?,"Classifier-based approaches in pronunciation assessment leverage both segmental and prosodic features to evaluate pronunciation quality. Segmental features focus on specific phoneme pair errors and utilize various acoustic features such as Mel-frequency cepstral coefficients (MFCCs), along with their first and second derivatives, energy, zero-crossing rate, and spectral features (Van Doremalen et al., 2009; Huang et al., 2020). Techniques like Linear Discriminant Analysis (LDA) (Truong et al., 2004; Strik et al., 2009) and decision trees (Strik et al., 2009) are employed to analyze these features. On the other hand, prosodic features are concerned with detecting lexical stress and tones, using attributes such as energy, pitch, duration, and spectral characteristics. Classifiers such as Gaussian mixture models (GMMs) (Ferrer et al., 2015), support vector machines (SVMs) (Chen and Wang, 2010; Shahin et al., 2016), deep neural networks (DNNs) (Shahin et al., 2016), and multi-distribution DNNs (Li et al., 2018a) are utilized to process these prosodic features. Together, these approaches provide a comprehensive assessment of pronunciation by analyzing both the segmental and prosodic aspects.",7,2,7,2,"Question:

How do classifier-based approaches utilize segmental and prosodic features for pronunciation assessment?  

Answer:

Classifier-based approaches explored both segmental and prosodic aspects of pronunciation. Segmental approaches involve the use of classifiers targeting specific phoneme pair errors, utilizing different acoustic features such as Mel-frequency cepstral coefficients (MFCCs) along with its first and second derivative, energy, zero-cross, and spectral features (Van Doremalen et al., 2009; Huang et al., 2020), with different techniques such as Linear Discriminant Analysis (LDA) (Truong et al., 2004; Strik et al., 2009), decision trees (Strik et al., 2009). Prosodic approaches focus on detecting lexical stress and tones, utilizing features such as energy, pitch, duration, and spectral characteristics, with classifiers like Gaussian mixture models (GMMs) (Ferrer et al., 2015), support vector machines (SVMs) (Chen and Wang, 2010; Shahin et al., 2016), and deep neural network (DNNs) (Shahin et al., 2016), and multi-distribution DNNs (Li et al., 2018a).","Question:

How do classifier-based approaches use segmental and prosodic features in pronunciation assessment?

Answer:

Classifier-based approaches in pronunciation assessment leverage both segmental and prosodic features to evaluate pronunciation quality. Segmental features focus on specific phoneme pair errors and utilize various acoustic features such as Mel-frequency cepstral coefficients (MFCCs), along with their first and second derivatives, energy, zero-crossing rate, and spectral features (Van Doremalen et al., 2009; Huang et al., 2020). Techniques like Linear Discriminant Analysis (LDA) (Truong et al., 2004; Strik et al., 2009) and decision trees (Strik et al., 2009) are employed to analyze these features. On the other hand, prosodic features are concerned with detecting lexical stress and tones, using attributes such as energy, pitch, duration, and spectral characteristics. Classifiers such as Gaussian mixture models (GMMs) (Ferrer et al., 2015), support vector machines (SVMs) (Chen and Wang, 2010; Shahin et al., 2016), deep neural networks (DNNs) (Shahin et al., 2016), and multi-distribution DNNs (Li et al., 2018a) are utilized to process these prosodic features. Together, these approaches provide a comprehensive assessment of pronunciation by analyzing both the segmental and prosodic aspects.",NO,True,1205,True,True
264426545-s12,Automatic Pronunciation Assessment -A Review,2023-10-21,Self-Supervised Models,"Motivated by the recent success of self-supervised learning methods (Baevski et al., 2020;Hsu et al., 2021;Chen et al., 2022;Mohamed et al., 2022) in speech recognition and related downstream tasks such as emotion recognition, speaker verification, and language identification (Chen and Rudnicky, 2023;Fan et al., 2020), self-supervised approaches is employed also in this field.Xu et al. (2021) explored finetuning wav2vec 2.0 on frame-level L2 phoneme prediction.A pretrained HMM-DNN ASR is used to extract time force-alignment.To overcome the dependency on time alignment, Peng et al. (2021) propose a CTC-based wav2vec 2.0 to predict L2 phonemes sequences.Building upon this work, Yang et al. (2022) propose an approach that leverages unlabeled L2 speech using momentum pseudo-labeling.In a contrasting approach, (Lin and Wang, 2022b) combined wav2vec 2.0 features and phoneme text embeddings in a jointly learning framework to predict frame-level phoneme sequence and detect boundaries.Recently, EL Kheir et al. (2023a) explored the multi-view representation utilizing mono-and multilingual wav2vec 2.0 encoders to capture different aspects of speech production and leveraging articulatory features as auxiliary tasks to phoneme sequence prediction.Furthermore, Kheir et al. (2023b) introduces a novel L1-aware multilingual, L1-MultiMDD, architecture for addressing mispronunciation in multilingual settings encompassing Arabic, English, and Mandarin using wav2vec-large pre-trained model as the acoustic encoder.L1-MultiMDD is enriched with L1-aware speech representation, allowing it to understand the nuances of each speaker's native language.

SSL models have proven to be effective in predicting fluency and prosodic scores assigned by human annotators.Kim et al. (2022); Lin and Wang (2023a); Yang et al. (2022) fine-tuned wav2vec 2.0 and Hubert to predict prosodic and fluency scores.

Similarly, another research conducted in (Lin and Wang, 2023a) jointly predicts L2 phoneme sequence using CTC loss, and predicts prosodic scores using fused acoustic representations with phoneme embeddings.Subsequently Lin and Wang (2023b) introduced a fusion of language embedding, representation features and build a unified framework for multi-lingual prosodic scoring.Recently, Chao et al. ( 2022",How do self-supervised models predict fluency and prosodic scores in automatic pronunciation assessment?,"Self-supervised models have proven to be effective in predicting fluency and prosodic scores assigned by human annotators. Kim et al. (2022), Lin and Wang (2023a), and Yang et al. (2022) fine-tuned wav2vec 2.0 and Hubert to predict prosodic and fluency scores. Similarly, another research conducted by Lin and Wang (2023a) jointly predicts L2 phoneme sequence using CTC loss and predicts prosodic scores using fused acoustic representations with phoneme embeddings. Subsequently, Lin and Wang (2023b) introduced a fusion of language embedding and representation features to build a unified framework for multi-lingual prosodic scoring.",How do self-supervised models contribute to predicting fluency and prosodic scores in automatic pronunciation assessment?,"Self-supervised models have shown significant effectiveness in predicting fluency and prosodic scores, which are typically assigned by human annotators. Researchers like Kim et al. (2022), Lin and Wang (2023a), and Yang et al. (2022) have fine-tuned models such as wav2vec 2.0 and Hubert to predict these scores. In particular, Lin and Wang (2023a) developed a method that jointly predicts L2 phoneme sequences using CTC loss while also predicting prosodic scores by integrating acoustic representations with phoneme embeddings. Building on this, Lin and Wang (2023b) introduced a unified framework for multi-lingual prosodic scoring by fusing language embedding and representation features.",7,4,7,4,"Question:

How do self-supervised models predict fluency and prosodic scores in automatic pronunciation assessment?

Answer:

Self-supervised models have proven to be effective in predicting fluency and prosodic scores assigned by human annotators. Kim et al. (2022), Lin and Wang (2023a), and Yang et al. (2022) fine-tuned wav2vec 2.0 and Hubert to predict prosodic and fluency scores. Similarly, another research conducted by Lin and Wang (2023a) jointly predicts L2 phoneme sequence using CTC loss and predicts prosodic scores using fused acoustic representations with phoneme embeddings. Subsequently, Lin and Wang (2023b) introduced a fusion of language embedding and representation features to build a unified framework for multi-lingual prosodic scoring.","Question:

How do self-supervised models contribute to predicting fluency and prosodic scores in automatic pronunciation assessment?

Answer:

Self-supervised models have shown significant effectiveness in predicting fluency and prosodic scores, which are typically assigned by human annotators. Researchers like Kim et al. (2022), Lin and Wang (2023a), and Yang et al. (2022) have fine-tuned models such as wav2vec 2.0 and Hubert to predict these scores. In particular, Lin and Wang (2023a) developed a method that jointly predicts L2 phoneme sequences using CTC loss while also predicting prosodic scores by integrating acoustic representations with phoneme embeddings. Building on this, Lin and Wang (2023b) introduced a unified framework for multi-lingual prosodic scoring by fusing language embedding and representation features.",NO,True,691,True,True
264426545-s13,Automatic Pronunciation Assessment -A Review,2023-10-21,Unsupervised Approaches,"It is important to note that the aforementioned approaches for studying mispronunciation detection typically involve the need for expert knowledge, laborious manual labeling, or dependable ASR results, all of which come with significant costs.In contrast, recent years have witnessed considerable endeavors in unsupervised acoustic pattern discovery, yielding sub-optimal outcomes.Lee and Glass (2012) initially investigated a comparison-based approach that analyzes the extent of misalignment between a student's speech and a teacher's speech.In subsequent studies Lee and Glass (2015); Lee et al. (2016), explored the discovery of mispronunciation errors by analyzing the acoustic similarities across individual learners' utterances, with a proposed nbest filtering method to resolve ambiguous error candidate hypotheses derived from acoustic similarity clustering.Furthermore, Mao et al. (2018) proposed k-means clustering on phoneme-based phonemic posterior-grams (PPGs) to expand the phoneme set in L2 speech.More recently, Sini et al.

(2023) introduced a weighted DTW alignment as an alternative to the GOP algorithm for predicting probabilities and the sequence of target phonemes.Their proposed method achieves comparable results to the GOP scoring algorithm, likewise Anand et al. (2023) explored alignment distance between wav2vec 2.0 representations of teacher and learner speech using DTW, to distinguish between intelligible and unintelligible speech.",How have recent unsupervised methods advanced the detection of mispronunciation errors in speech assessment?  ,"Recent unsupervised methods have advanced the detection of mispronunciation errors in speech assessment through various approaches. Lee and Glass (2012) initially investigated a comparison-based approach that analyzes the extent of misalignment between a student's speech and a teacher's speech. In subsequent studies, Lee and Glass (2015) and Lee et al. (2016) explored the discovery of mispronunciation errors by analyzing the acoustic similarities across individual learners' utterances, with a proposed n-best filtering method to resolve ambiguous error candidate hypotheses derived from acoustic similarity clustering. Mao et al. (2018) proposed k-means clustering on phoneme-based phonemic posterior-grams (PPGs) to expand the phoneme set in L2 speech. More recently, Sini et al. (2023) introduced a weighted DTW alignment as an alternative to the GOP algorithm for predicting probabilities and the sequence of target phonemes, achieving comparable results to the GOP scoring algorithm. Similarly, Anand et al. (2023) explored alignment distance between wav2vec 2.0 representations of teacher and learner speech using DTW to distinguish between intelligible and unintelligible speech.",How have recent unsupervised methods improved the detection of mispronunciation errors in speech assessment?,"Recent advancements in unsupervised methods have significantly enhanced the detection of mispronunciation errors in speech assessment through a variety of innovative approaches. Initially, Lee and Glass (2012) explored a comparison-based method that focused on analyzing the degree of misalignment between a student's speech and a teacher's speech. Building on this, Lee and Glass (2015) and Lee et al. (2016) investigated the identification of mispronunciation errors by examining acoustic similarities across individual learners' utterances. They proposed an n-best filtering method to address ambiguous error candidate hypotheses that emerged from acoustic similarity clustering. In 2018, Mao et al. introduced the use of k-means clustering on phoneme-based phonemic posterior-grams (PPGs) to expand the phoneme set in second language (L2) speech. More recently, Sini et al. (2023) proposed a weighted Dynamic Time Warping (DTW) alignment as an alternative to the Goodness of Pronunciation (GOP) algorithm for predicting probabilities and the sequence of target phonemes, achieving results comparable to the GOP scoring algorithm. Similarly, Anand et al. (2023) examined the alignment distance between wav2vec 2.0 representations of teacher and learner speech using DTW to differentiate between intelligible and unintelligible speech.",7,4,7,6,"Question:

How have recent unsupervised methods advanced the detection of mispronunciation errors in speech assessment?  

Answer:

Recent unsupervised methods have advanced the detection of mispronunciation errors in speech assessment through various approaches. Lee and Glass (2012) initially investigated a comparison-based approach that analyzes the extent of misalignment between a student's speech and a teacher's speech. In subsequent studies, Lee and Glass (2015) and Lee et al. (2016) explored the discovery of mispronunciation errors by analyzing the acoustic similarities across individual learners' utterances, with a proposed n-best filtering method to resolve ambiguous error candidate hypotheses derived from acoustic similarity clustering. Mao et al. (2018) proposed k-means clustering on phoneme-based phonemic posterior-grams (PPGs) to expand the phoneme set in L2 speech. More recently, Sini et al. (2023) introduced a weighted DTW alignment as an alternative to the GOP algorithm for predicting probabilities and the sequence of target phonemes, achieving comparable results to the GOP scoring algorithm. Similarly, Anand et al. (2023) explored alignment distance between wav2vec 2.0 representations of teacher and learner speech using DTW to distinguish between intelligible and unintelligible speech.","Question:

How have recent unsupervised methods improved the detection of mispronunciation errors in speech assessment?

Answer:

Recent advancements in unsupervised methods have significantly enhanced the detection of mispronunciation errors in speech assessment through a variety of innovative approaches. Initially, Lee and Glass (2012) explored a comparison-based method that focused on analyzing the degree of misalignment between a student's speech and a teacher's speech. Building on this, Lee and Glass (2015) and Lee et al. (2016) investigated the identification of mispronunciation errors by examining acoustic similarities across individual learners' utterances. They proposed an n-best filtering method to address ambiguous error candidate hypotheses that emerged from acoustic similarity clustering. In 2018, Mao et al. introduced the use of k-means clustering on phoneme-based phonemic posterior-grams (PPGs) to expand the phoneme set in second language (L2) speech. More recently, Sini et al. (2023) proposed a weighted Dynamic Time Warping (DTW) alignment as an alternative to the Goodness of Pronunciation (GOP) algorithm for predicting probabilities and the sequence of target phonemes, achieving results comparable to the GOP scoring algorithm. Similarly, Anand et al. (2023) examined the alignment distance between wav2vec 2.0 representations of teacher and learner speech using DTW to differentiate between intelligible and unintelligible speech.",NO,True,1337,True,True
264426545-s14,Automatic Pronunciation Assessment -A Review,2023-10-21,Data Augmentation,"Two major challenges in this field are L2 data scarcity and the imbalanced distribution of negative classes (mispronunciation).To address these challenges, researchers have opted for data augmentation techniques that are proven to be quite effective in pronunciation assessment.Such methods employed strategies like altering the canonical text by introducing mismatched phoneme pairs while preserving the original word-level speech (Fu et al., 2021).Additionally, a mixup technique is utilized in the feature space, leveraging phone-level GOP pooling to construct word-level training data (Fu et al., 2022).Furthermore, the error distance of the clustered SSL model embeddings are employed to substitute the phoneme sound with a similar sound (Zhang et al., 2022b).These latter approaches depend on the reuse of existing information rather than generating novel instances of mispronunciations.In (Fernandez et al., 2017), voice transformations in pitch, vocal-tract, vocal-source characteristics to generate new samples.Furthermore, L2-GEN can synthesize realistic L2 phoneme sequences by building a novel Seq2Seq phoneme paraphrasing model (Zhang et al., 2022a).Korzekwa et al. (2020b) proposed an augmentation technique by generating incorrectly stressed words using Neural TTS.Furthermore, Korzekwa et al. (2022) provided an overview of mispronunciation error generation using three methods, phoneme-2phoneme P2P relies on perturbing phonetic transcription for the corresponding speech audio, text-2-speech create speech signals that match the synthetic mispronunciations, and speech-2-speech S2S to simulate a different aspect of prosodic nature of speech.Recently, SpeechBlender (EL Kheir et al., 2023b) framework is introduced as a fine-grained data augmentation pipeline that linearly interpolates raw good speech pronunciations to generate mispronunciations at the phoneme level.",What are the various data augmentation strategies used to address challenges in automatic pronunciation assessment?,"Two major challenges in this field are L2 data scarcity and the imbalanced distribution of negative classes (mispronunciation). To address these challenges, researchers have opted for data augmentation techniques that are proven to be quite effective in pronunciation assessment. Such methods employed strategies like altering the canonical text by introducing mismatched phoneme pairs while preserving the original word-level speech (Fu et al., 2021). Additionally, a mixup technique is utilized in the feature space, leveraging phone-level GOP pooling to construct word-level training data (Fu et al., 2022). Furthermore, the error distance of the clustered SSL model embeddings are employed to substitute the phoneme sound with a similar sound (Zhang et al., 2022b). These latter approaches depend on the reuse of existing information rather than generating novel instances of mispronunciations. In (Fernandez et al., 2017), voice transformations in pitch, vocal-tract, vocal-source characteristics to generate new samples. Furthermore, L2-GEN can synthesize realistic L2 phoneme sequences by building a novel Seq2Seq phoneme paraphrasing model (Zhang et al., 2022a). Korzekwa et al. (2020b) proposed an augmentation technique by generating incorrectly stressed words using Neural TTS. Furthermore, Korzekwa et al. (2022) provided an overview of mispronunciation error generation using three methods, phoneme-2phoneme P2P relies on perturbing phonetic transcription for the corresponding speech audio, text-2-speech create speech signals that match the synthetic mispronunciations, and speech-2-speech S2S to simulate a different aspect of prosodic nature of speech. Recently, SpeechBlender (EL Kheir et al., 2023b) framework is introduced as a fine-grained data augmentation pipeline that linearly interpolates raw good speech pronunciations to generate mispronunciations at the phoneme level.",What data augmentation strategies are used to overcome challenges in automatic pronunciation assessment?,"In the field of automatic pronunciation assessment, two significant challenges are the scarcity of L2 (second language) data and the imbalanced distribution of negative classes, such as mispronunciations. To tackle these issues, researchers have developed several effective data augmentation techniques. One approach involves altering the canonical text by introducing mismatched phoneme pairs while maintaining the original word-level speech (Fu et al., 2021). Another strategy is the mixup technique, which operates in the feature space by using phone-level GOP pooling to create word-level training data (Fu et al., 2022). Additionally, the error distance of clustered SSL model embeddings is used to replace phoneme sounds with similar ones (Zhang et al., 2022b). These methods focus on reusing existing information rather than generating entirely new instances of mispronunciations.

Further techniques include voice transformations that modify pitch, vocal-tract, and vocal-source characteristics to produce new samples (Fernandez et al., 2017). The L2-GEN model can synthesize realistic L2 phoneme sequences by employing a novel Seq2Seq phoneme paraphrasing model (Zhang et al., 2022a). Korzekwa et al. (2020b) proposed generating incorrectly stressed words using Neural TTS. Moreover, Korzekwa et al. (2022) reviewed mispronunciation error generation using three methods: phoneme-to-phoneme (P2P), which perturbs phonetic transcription; text-to-speech (T2S), which creates speech signals that mimic synthetic mispronunciations; and speech-to-speech (S2S), which simulates different prosodic aspects of speech. Recently, the SpeechBlender framework was introduced as a fine-grained data augmentation pipeline that linearly interpolates raw, well-pronounced speech to generate mispronunciations at the phoneme level (EL Kheir et al., 2023b).",7,4,7,4,"Question:

What are the various data augmentation strategies used to address challenges in automatic pronunciation assessment?

Answer:

Two major challenges in this field are L2 data scarcity and the imbalanced distribution of negative classes (mispronunciation). To address these challenges, researchers have opted for data augmentation techniques that are proven to be quite effective in pronunciation assessment. Such methods employed strategies like altering the canonical text by introducing mismatched phoneme pairs while preserving the original word-level speech (Fu et al., 2021). Additionally, a mixup technique is utilized in the feature space, leveraging phone-level GOP pooling to construct word-level training data (Fu et al., 2022). Furthermore, the error distance of the clustered SSL model embeddings are employed to substitute the phoneme sound with a similar sound (Zhang et al., 2022b). These latter approaches depend on the reuse of existing information rather than generating novel instances of mispronunciations. In (Fernandez et al., 2017), voice transformations in pitch, vocal-tract, vocal-source characteristics to generate new samples. Furthermore, L2-GEN can synthesize realistic L2 phoneme sequences by building a novel Seq2Seq phoneme paraphrasing model (Zhang et al., 2022a). Korzekwa et al. (2020b) proposed an augmentation technique by generating incorrectly stressed words using Neural TTS. Furthermore, Korzekwa et al. (2022) provided an overview of mispronunciation error generation using three methods, phoneme-2phoneme P2P relies on perturbing phonetic transcription for the corresponding speech audio, text-2-speech create speech signals that match the synthetic mispronunciations, and speech-2-speech S2S to simulate a different aspect of prosodic nature of speech. Recently, SpeechBlender (EL Kheir et al., 2023b) framework is introduced as a fine-grained data augmentation pipeline that linearly interpolates raw good speech pronunciations to generate mispronunciations at the phoneme level.","Question:

What data augmentation strategies are used to overcome challenges in automatic pronunciation assessment?

Answer:

In the field of automatic pronunciation assessment, two significant challenges are the scarcity of L2 (second language) data and the imbalanced distribution of negative classes, such as mispronunciations. To tackle these issues, researchers have developed several effective data augmentation techniques. One approach involves altering the canonical text by introducing mismatched phoneme pairs while maintaining the original word-level speech (Fu et al., 2021). Another strategy is the mixup technique, which operates in the feature space by using phone-level GOP pooling to create word-level training data (Fu et al., 2022). Additionally, the error distance of clustered SSL model embeddings is used to replace phoneme sounds with similar ones (Zhang et al., 2022b). These methods focus on reusing existing information rather than generating entirely new instances of mispronunciations.

Further techniques include voice transformations that modify pitch, vocal-tract, and vocal-source characteristics to produce new samples (Fernandez et al., 2017). The L2-GEN model can synthesize realistic L2 phoneme sequences by employing a novel Seq2Seq phoneme paraphrasing model (Zhang et al., 2022a). Korzekwa et al. (2020b) proposed generating incorrectly stressed words using Neural TTS. Moreover, Korzekwa et al. (2022) reviewed mispronunciation error generation using three methods: phoneme-to-phoneme (P2P), which perturbs phonetic transcription; text-to-speech (T2S), which creates speech signals that mimic synthetic mispronunciations; and speech-to-speech (S2S), which simulates different prosodic aspects of speech. Recently, the SpeechBlender framework was introduced as a fine-grained data augmentation pipeline that linearly interpolates raw, well-pronounced speech to generate mispronunciations at the phoneme level (EL Kheir et al., 2023b).",NO,True,1847,True,True
264426545-s20,Automatic Pronunciation Assessment -A Review,2023-10-21,A,"A.8 L2-ARCTIC (Zhao et al., 2018a) The L2-ARCTIC 2 corpus is a specialized speech corpus designed for research in voice conversion, accent conversion, and mispronunciation detection in non-native English.It encompasses a substantial collection of 26867 utterances from 24 non-native speakers (12 males and 12 females) whose L1 languages include Hindi, Korean, Mandarin, Spanish, Arabic, and Vietnamese.The recordings were sourced from a total of 4 speakers per L1 language, consisting of 2 males and 2 females ensuring a balanced distribution in terms of gender and native 2 version 5 released in 2020 avalaible: https://psi.engr.tamu.edu/l2-arctic-corpuslanguages (L1s).Yet, only 150 utterances is manually per speaker to identify three types of segmental mispronunciation errors: substitutions, deletions, and insertions resulting in 3.66 hours.

A.9 VoisTUTOR corpus (Yarra et al., 2019) VoisTUTOR is a pronunciation assessment corpus of Indian second language (L2) learners learning English.The corpus consists of audio recordings of 16 Indian L2 learners reading a set of 1676 sentences.The recordings are accompanied by phonetic transcriptions, human ratings of pronunciation accuracy on a scale of 0 to 10 for each utterance, and binary decisions for seven factors that affect pronunciation quality: intelligibility, phoneme quality, phoneme mispronunciation, syllable stress quality, intonation quality, correctness of pauses, and mother tongue influence.

A.10 SELL-CORPUS (Chen et al., 2019) SELL-CORPUS is a multiple accented speech corpus for L2 English learning in China.The corpus consists of audio recordings of 389 volunteer speakers, including 186 males and 203 females.The speakers are from seven major regional dialects of China, including Mandarin, Cantonese, Wu, Min, Hakka, and Southwestern Mandarin.The corpus contains 31.6 hours of speech recordings.Each recording in the corpus contains a word-level orthographic transcription manually inspected and cleaned by inserting, substituting, or deleting mismatching characters.",What are the characteristics and contents of the SELL-CORPUS for L2 English learning in China?,"SELL-CORPUS is a multiple accented speech corpus for L2 English learning in China. The corpus consists of audio recordings of 389 volunteer speakers, including 186 males and 203 females. The speakers are from seven major regional dialects of China, including Mandarin, Cantonese, Wu, Min, Hakka, and Southwestern Mandarin. The corpus contains 31.6 hours of speech recordings. Each recording in the corpus contains a word-level orthographic transcription manually inspected and cleaned by inserting, substituting, or deleting mismatching characters.",What are the features and components of the SELL-CORPUS designed for L2 English learners in China?,"The SELL-CORPUS is a diverse speech corpus specifically created for L2 English learners in China, featuring multiple accents. It comprises audio recordings from 389 volunteer speakers, with a gender distribution of 186 males and 203 females. These speakers represent seven major regional dialects of China, including Mandarin, Cantonese, Wu, Min, Hakka, and Southwestern Mandarin. The corpus offers a total of 31.6 hours of speech recordings. Each recording is accompanied by a word-level orthographic transcription, which has been meticulously inspected and refined by inserting, substituting, or deleting mismatched characters to ensure accuracy.",7,2,7,2,"Question:

What are the characteristics and contents of the SELL-CORPUS for L2 English learning in China?

Answer:

SELL-CORPUS is a multiple accented speech corpus for L2 English learning in China. The corpus consists of audio recordings of 389 volunteer speakers, including 186 males and 203 females. The speakers are from seven major regional dialects of China, including Mandarin, Cantonese, Wu, Min, Hakka, and Southwestern Mandarin. The corpus contains 31.6 hours of speech recordings. Each recording in the corpus contains a word-level orthographic transcription manually inspected and cleaned by inserting, substituting, or deleting mismatching characters.","Question:

What are the features and components of the SELL-CORPUS designed for L2 English learners in China?

Answer:

The SELL-CORPUS is a diverse speech corpus specifically created for L2 English learners in China, featuring multiple accents. It comprises audio recordings from 389 volunteer speakers, with a gender distribution of 186 males and 203 females. These speakers represent seven major regional dialects of China, including Mandarin, Cantonese, Wu, Min, Hakka, and Southwestern Mandarin. The corpus offers a total of 31.6 hours of speech recordings. Each recording is accompanied by a word-level orthographic transcription, which has been meticulously inspected and refined by inserting, substituting, or deleting mismatched characters to ensure accuracy.",NO,True,648,True,True
264451714-s4,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,2023-10-25,Optimized Inputs,"Probing inputs contribute substantially to the probing procedure.PLMs are sensitive to the inputs (Petroni et al., 2019;Jiang et al., 2020b;Elazar et al., 2021), and even syntactical variations or distractors, that do not alter the meaning, cause the PLM's predictions to change (Heinzerling and Inui, 2021;Longpre et al., 2021;Pandia and Ettinger, 2021;Podkorytov et al., 2021;Li et al., 2022a).Therefore, depending on the probing inputs, the estimate on factual knowledge we obtain may vary significantly.Optimized inputs represent variations of the inputs, where the inputs are changed to account for the sensitivity of the probed PLMs.

Diversification and mining methods aim to diversify and optimize prompts by mining Wikipedia or other resources, and selecting the best performing prompts or a combination of them.For example, Jiang et al. (2020b) propose a mining-based and a paraphrasing-based approach to create alternative prompts that outperform manual ones.The final prompts are selected based on their performance on a training set, and can also be combined in an ensemble.Bouraoui et al. (2020) mine for prompts that contain the entities of interest, and filter these based on the ability of the probed PLMs to predict the masked objects.After the filtering step, the remaining prompts are utilized to create a dataset that consists of positive inputs, i.e., containing true subject-object pairs, and negative inputs, which contain false pairs.This dataset is then used for the final evaluation.

Direct optimization methods aim to directly optimize existing prompts.This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary.Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task.Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradientbased search to sequentially replace the trigger tokens with concrete tokens.These tokens are chosen to increase the probability of predicting the correct object.OPTIPROMPT (Zhong et al., 2021) is sim-ilar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space.In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation.Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors.The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs.Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts.Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings.These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity.PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs.Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts.(Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts.

Generation with PLM methods re-write prompts with the help of a secondary PLM.Haviv et al. (2021) re-write manual prompts using another version of the probed model.The re-writing model is trained to produce prompts that help extract more knowledge from the probed one, which is kept unchanged.Zhang et al. (2022) leverage a generative PLM to produce optimized prompts.",How do direct optimization methods enhance prompt effectiveness in probing pre-trained language models?,"Direct optimization methods aim to directly optimize existing prompts. This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary. Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task. Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradient-based search to sequentially replace the trigger tokens with concrete tokens. These tokens are chosen to increase the probability of predicting the correct object. OPTIPROMPT (Zhong et al., 2021) is similar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space. In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation. Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors. The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs. Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts. Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings. These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity. PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs. Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts. (Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts.",How do direct optimization methods improve the effectiveness of prompts when probing pre-trained language models?,"Direct optimization methods enhance prompt effectiveness by optimizing existing prompts either in a discrete or continuous space. In a discrete space, prompts remain in natural language, while in a continuous space, they do not need to correspond to specific vocabulary tokens. Optimization can focus on the masked token or the order of examples in the prompt, especially when examples are provided to clarify the task. For instance, AUTOPROMPT by Shin et al. (2020) extends manually created prompts with a set number of trigger tokens, using gradient-based search to replace these tokens with ones that increase the likelihood of predicting the correct object. Similarly, OPTIPROMPT (Zhong et al., 2021) allows trigger tokens to be replaced with vectors from a continuous embedding space. Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation and perturbing prompt representations in each layer of the probed PLMs with small learnable vectors, aiming to mimic activation patterns from pre-training to facilitate knowledge elicitation. Newman et al. (2022) use adapters (Houlsby et al., 2019) to map embedding vectors to continuous prompts, reducing sensitivity to different phrasings. Saeed and Papotti (2022) introduce Type Embeddings, derived from entities sharing the same type, to align PLM predictions with expected entity types. PERO (Kumar and Talukdar, 2021) employs a genetic algorithm to optimize prompts and separator tokens, considering the sensitivity of PLMs to the order and quality of training examples. Li et al. (2022c) leverage task symmetry, optimizing prompts in a continuous space to maximize the probability of predicting both the subject and object.",7,4,7,4,"Question:

How do direct optimization methods enhance prompt effectiveness in probing pre-trained language models?

Answer:

Direct optimization methods aim to directly optimize existing prompts. This optimization happens either in a discrete space, to keep the prompts in natural language, or in a continuous space where the prompts do not have to correspond to specific tokens from the vocabulary. Optimization could also target only the masked token or the order of the examples in the prompt, in case a few examples are provided in the prompt to better indicate the task. Shin et al. (2020)'s AUTOPROMPT extends manually created prompts by prompts with a pre-defined number of trigger tokens, and employs gradient-based search to sequentially replace the trigger tokens with concrete tokens. These tokens are chosen to increase the probability of predicting the correct object. OPTIPROMPT (Zhong et al., 2021) is similar to AUTOPROMPT, but allows for the trigger tokens to be replaced with vectors from a continuous embedding space. In a similar fashion, Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation. Additionally, they perturb the representations of the prompts in each layer in the probed PLMs using small learnable vectors. The intuition is to have activation patterns that are similar to the ones encountered during pre-training, which would make it easier to elicit knowledge from PLMs. Newman et al. (2022) utilize adapters (Houlsby et al., 2019) to map the embedding vectors to continuous prompts in order to make the probed PLMs less sensitive to different phrasings of the same prompts. Saeed and Papotti (2022) augment the masked tokens with a special type of embeddings, called Type Embeddings. These embeddings are derived from several entities that share the same type, and are shown to help tie the probed PLM's predictions to the expected type of the masked entity. PERO (Kumar and Talukdar, 2021) depends on querying PLMs with prompts containing few training examples (or shots), which demonstrate the task to the queried PLMs. Since PLMs are quite sensitive to the order and the quality of the provided training examples in the prompt, PERO leverages a genetic algorithm to find an optimized prompt and a separator token to concatenate the examples in the prompts. (Li et al., 2022c) exploit the symmetry of the task, and optimize prompts in a continuous space so that the probability of predicting both the subject and the object is maximized using the resulting prompts.","Question:

How do direct optimization methods improve the effectiveness of prompts when probing pre-trained language models?

Answer:

Direct optimization methods enhance prompt effectiveness by optimizing existing prompts either in a discrete or continuous space. In a discrete space, prompts remain in natural language, while in a continuous space, they do not need to correspond to specific vocabulary tokens. Optimization can focus on the masked token or the order of examples in the prompt, especially when examples are provided to clarify the task. For instance, AUTOPROMPT by Shin et al. (2020) extends manually created prompts with a set number of trigger tokens, using gradient-based search to replace these tokens with ones that increase the likelihood of predicting the correct object. Similarly, OPTIPROMPT (Zhong et al., 2021) allows trigger tokens to be replaced with vectors from a continuous embedding space. Qin and Eisner (2021) propose learning an ensemble of continuous prompts per relation and perturbing prompt representations in each layer of the probed PLMs with small learnable vectors, aiming to mimic activation patterns from pre-training to facilitate knowledge elicitation. Newman et al. (2022) use adapters (Houlsby et al., 2019) to map embedding vectors to continuous prompts, reducing sensitivity to different phrasings. Saeed and Papotti (2022) introduce Type Embeddings, derived from entities sharing the same type, to align PLM predictions with expected entity types. PERO (Kumar and Talukdar, 2021) employs a genetic algorithm to optimize prompts and separator tokens, considering the sensitivity of PLMs to the order and quality of training examples. Li et al. (2022c) leverage task symmetry, optimizing prompts in a continuous space to maximize the probability of predicting both the subject and object.",NO,True,1706,True,True
264451714-s7,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,2023-10-25,Adapted PLMs,"Some works adapt the PLMs under evaluation to enable evaluation tasks, that do not correspond to any pre-training objective.The adaptation, however, is also coupled with risks such as train-test overlap (Lewis et al., 2021;Wang et al., 2021a).Supervised adaptation.Most methods finetune the probed PLMs in a supervised manner to adapt them to the probing task.Roberts et al. (2020) finetune T5 models for closed-book question answering, where models have only questions as inputs, while leaving out any context or external knowledge sources that might contain the answer.Similarly, Wang et al. (2021a) finetune BART to output a related passage, and then the answer.Bouraoui et al. (2020) finetune BERT to classify prompts based on whether the relation between the subject and object entities truly holds or not.Fichtel et al. (2021) finetune a BERT model with its masked language modeling head to predict the masked tokens in the provided prompts.Abaho et al. (2022) propose an additional position-attention layer on top of transformer models, where the position of the masked token is kept constant, and the remaining tokens are given positions relative to the masked token.This approach is considered to put more focus on the masked tokens and its interaction with the remaining tokens in the prompt.Chen et al. (2022) leverage a task description that depends on the relation between the subject and object entity, alongside a few labeled examples to train the probed PLMs.At inference time, the PLMs are kept frozen and are provided with unseen task descriptions and labeled examples to adapt to the task.Elazar et al. (2021) further train BERT with a consistency loss to increase its robustness to paraphrases that describe the same relation.Shi et al. (2021) finetune generative PLMs to generate entity descriptions depending only on their knoweldge from pre-training.Qin and Eisner (2021) do not directly change any parameters in PLMs, but rather introduce additional trainable parameters in each layer that change the hidden representations of the prompts to help make them more suitable for knowledge extraction.

Self-supervised adaptation.Adaptations in a self-supervised manner can introduce changes to the model without explicitly finetuning the model to the probing task.For example, Meng et al. (2022b) propose to re-wire the probed PLM in a self-supervised manner.Their method depends on using data from the pre-training phase, splitting each sentence into a head part and a tail part, and using a contrastive learning objective to push the representations of the matching head and tail pairs (positives) closer to one another, and that of the non-matching pairs (negatives) to be further apart.The evaluation is based on the similarity between the representations of the prompt and a predefined set of entities that represent potential answers.",How are pre-trained language models adapted for evaluation tasks without corresponding pre-training objectives?,"Some works adapt the pre-trained language models (PLMs) under evaluation to enable evaluation tasks that do not correspond to any pre-training objective. The adaptation, however, is also coupled with risks such as train-test overlap (Lewis et al., 2021; Wang et al., 2021a). Most methods finetune the probed PLMs in a supervised manner to adapt them to the probing task. Roberts et al. (2020) finetune T5 models for closed-book question answering, where models have only questions as inputs, while leaving out any context or external knowledge sources that might contain the answer. Similarly, Wang et al. (2021a) finetune BART to output a related passage, and then the answer. Bouraoui et al. (2020) finetune BERT to classify prompts based on whether the relation between the subject and object entities truly holds or not. Fichtel et al. (2021) finetune a BERT model with its masked language modeling head to predict the masked tokens in the provided prompts. Abaho et al. (2022) propose an additional position-attention layer on top of transformer models, where the position of the masked token is kept constant, and the remaining tokens are given positions relative to the masked token. This approach is considered to put more focus on the masked tokens and its interaction with the remaining tokens in the prompt. Chen et al. (2022) leverage a task description that depends on the relation between the subject and object entity, alongside a few labeled examples to train the probed PLMs. At inference time, the PLMs are kept frozen and are provided with unseen task descriptions and labeled examples to adapt to the task. Elazar et al. (2021) further train BERT with a consistency loss to increase its robustness to paraphrases that describe the same relation. Shi et al. (2021) finetune generative PLMs to generate entity descriptions depending only on their knowledge from pre-training. Qin and Eisner (2021) do not directly change any parameters in PLMs, but rather introduce additional trainable parameters in each layer that change the hidden representations of the prompts to help make them more suitable for knowledge extraction. Adaptations in a self-supervised manner can introduce changes to the model without explicitly finetuning the model to the probing task. For example, Meng et al. (2022b) propose to re-wire the probed PLM in a self-supervised manner. Their method depends on using data from the pre-training phase, splitting each sentence into a head part and a tail part, and using a contrastive learning objective to push the representations of the matching head and tail pairs (positives) closer to one another, and that of the non-matching pairs (negatives) to be further apart. The evaluation is based on the similarity between the representations of the prompt and a predefined set of entities that represent potential answers.",How can pre-trained language models be adapted for evaluation tasks that lack corresponding pre-training objectives?,"Adapting pre-trained language models (PLMs) for evaluation tasks that do not align with their pre-training objectives involves several strategies, though it comes with risks such as train-test overlap (Lewis et al., 2021; Wang et al., 2021a). A common approach is to finetune the PLMs in a supervised manner to tailor them to specific probing tasks. For instance, Roberts et al. (2020) finetune T5 models for closed-book question answering, where the models receive only questions as inputs, excluding any context or external knowledge sources. Similarly, Wang et al. (2021a) finetune BART to first output a related passage and then the answer. Bouraoui et al. (2020) adapt BERT to classify prompts based on the validity of the relationship between subject and object entities. Fichtel et al. (2021) finetune a BERT model with its masked language modeling head to predict masked tokens in given prompts. 

In another approach, Abaho et al. (2022) introduce an additional position-attention layer on top of transformer models, maintaining a constant position for the masked token while assigning relative positions to the remaining tokens. This method emphasizes the interaction between masked tokens and other tokens in the prompt. Chen et al. (2022) use a task description based on the relationship between subject and object entities, along with a few labeled examples, to train the PLMs. During inference, the PLMs remain frozen and are provided with new task descriptions and labeled examples for task adaptation. Elazar et al. (2021) enhance BERT with a consistency loss to improve its robustness against paraphrases describing the same relation. Shi et al. (2021) finetune generative PLMs to generate entity descriptions relying solely on pre-training knowledge. 

Qin and Eisner (2021) take a different route by introducing additional trainable parameters in each layer to modify the hidden representations of prompts, aiding in knowledge extraction without directly altering PLM parameters. Self-supervised adaptations can modify the model without explicit finetuning for the probing task. For example, Meng et al. (2022b) propose a self-supervised re-wiring of the probed PLM. Their method uses pre-training data, splitting sentences into head and tail parts, and employs a contrastive learning objective to bring representations of matching head-tail pairs closer while distancing non-matching pairs. Evaluation is based on the similarity between prompt representations and a predefined set of entities representing potential answers.",7,6,7,6,"Question:

How are pre-trained language models adapted for evaluation tasks without corresponding pre-training objectives?

Answer:

Some works adapt the pre-trained language models (PLMs) under evaluation to enable evaluation tasks that do not correspond to any pre-training objective. The adaptation, however, is also coupled with risks such as train-test overlap (Lewis et al., 2021; Wang et al., 2021a). Most methods finetune the probed PLMs in a supervised manner to adapt them to the probing task. Roberts et al. (2020) finetune T5 models for closed-book question answering, where models have only questions as inputs, while leaving out any context or external knowledge sources that might contain the answer. Similarly, Wang et al. (2021a) finetune BART to output a related passage, and then the answer. Bouraoui et al. (2020) finetune BERT to classify prompts based on whether the relation between the subject and object entities truly holds or not. Fichtel et al. (2021) finetune a BERT model with its masked language modeling head to predict the masked tokens in the provided prompts. Abaho et al. (2022) propose an additional position-attention layer on top of transformer models, where the position of the masked token is kept constant, and the remaining tokens are given positions relative to the masked token. This approach is considered to put more focus on the masked tokens and its interaction with the remaining tokens in the prompt. Chen et al. (2022) leverage a task description that depends on the relation between the subject and object entity, alongside a few labeled examples to train the probed PLMs. At inference time, the PLMs are kept frozen and are provided with unseen task descriptions and labeled examples to adapt to the task. Elazar et al. (2021) further train BERT with a consistency loss to increase its robustness to paraphrases that describe the same relation. Shi et al. (2021) finetune generative PLMs to generate entity descriptions depending only on their knowledge from pre-training. Qin and Eisner (2021) do not directly change any parameters in PLMs, but rather introduce additional trainable parameters in each layer that change the hidden representations of the prompts to help make them more suitable for knowledge extraction. Adaptations in a self-supervised manner can introduce changes to the model without explicitly finetuning the model to the probing task. For example, Meng et al. (2022b) propose to re-wire the probed PLM in a self-supervised manner. Their method depends on using data from the pre-training phase, splitting each sentence into a head part and a tail part, and using a contrastive learning objective to push the representations of the matching head and tail pairs (positives) closer to one another, and that of the non-matching pairs (negatives) to be further apart. The evaluation is based on the similarity between the representations of the prompt and a predefined set of entities that represent potential answers.","Question:

How can pre-trained language models be adapted for evaluation tasks that lack corresponding pre-training objectives?

Answer:

Adapting pre-trained language models (PLMs) for evaluation tasks that do not align with their pre-training objectives involves several strategies, though it comes with risks such as train-test overlap (Lewis et al., 2021; Wang et al., 2021a). A common approach is to finetune the PLMs in a supervised manner to tailor them to specific probing tasks. For instance, Roberts et al. (2020) finetune T5 models for closed-book question answering, where the models receive only questions as inputs, excluding any context or external knowledge sources. Similarly, Wang et al. (2021a) finetune BART to first output a related passage and then the answer. Bouraoui et al. (2020) adapt BERT to classify prompts based on the validity of the relationship between subject and object entities. Fichtel et al. (2021) finetune a BERT model with its masked language modeling head to predict masked tokens in given prompts. 

In another approach, Abaho et al. (2022) introduce an additional position-attention layer on top of transformer models, maintaining a constant position for the masked token while assigning relative positions to the remaining tokens. This method emphasizes the interaction between masked tokens and other tokens in the prompt. Chen et al. (2022) use a task description based on the relationship between subject and object entities, along with a few labeled examples, to train the PLMs. During inference, the PLMs remain frozen and are provided with new task descriptions and labeled examples for task adaptation. Elazar et al. (2021) enhance BERT with a consistency loss to improve its robustness against paraphrases describing the same relation. Shi et al. (2021) finetune generative PLMs to generate entity descriptions relying solely on pre-training knowledge. 

Qin and Eisner (2021) take a different route by introducing additional trainable parameters in each layer to modify the hidden representations of prompts, aiding in knowledge extraction without directly altering PLM parameters. Self-supervised adaptations can modify the model without explicit finetuning for the probing task. For example, Meng et al. (2022b) propose a self-supervised re-wiring of the probed PLM. Their method uses pre-training data, splitting sentences into head and tail parts, and employs a contrastive learning objective to bring representations of matching head-tail pairs closer while distancing non-matching pairs. Evaluation is based on the similarity between prompt representations and a predefined set of entities representing potential answers.",NO,True,2544,True,True
264451714-s8,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,2023-10-25,Outputs,"Methods focusing on the outputs of PLMs address restricting the output space of PLMs, debiasing their outputs, and handling multi-token entities.

Typed querying.Kassner et al. (2021) propose to restrict the space of possible values for replacing the masked token (object) from the whole vocabulary to a specific set of tokens whose type matches the type of the ground truth object.For example, if the PLM is queried with the prompt: ""The smallest country in the world is [MASK]"", only entities of type country are considered to replace the [MASK] token.This method has two advantages: it reduces the number of objects under consideration and allows for a better comparison across PLMs with different vocabularies (Kassner et al., 2021).

Debiasing.Zhao et al. (2021) identify biases in the predictions of PLMs towards common and recent tokens, and propose a method that adapts the output probabilities by first estimating these biases using neutral examples and then correcting them.This debiasing method is shown to reduce the variance across prompts and has a positive effect on fact retrieval.Malkin et al. (2022) propose a method to increase the effect of distant tokens on the predictions of PLMs.The method depends on combining two output distributions over the vocabulary.One distribution is based on the full-length input, whereas the other is based on a shortened version of the same input.Wang et al. (2023) identify the problem of object bias in optimized prompts and propose to make all potential objects equally probable when no subject is provided, and increasing the probability of the correct object, when the subject is available.Yoshikawa and Okazaki (2023) output predictions only above a sufficient confidence threshold.This results in a less biased evaluation, and reflects the ability of PLMs in excluding uncertain predictions.To address the problems of multiple valid answers and frequency bias, i.e., the co-occurence of some subject and object entities despite not being in a factual relation to one another, Dong et al. (2022) use two templates, one contains the correct relation while the other contains an erroneous relation between the two entities, and compare the probability for the correct object under both relations.

Multi-token entities.To handle multi-token entities, Jiang et al. (2020a) propose using a predefined number of masked tokens and filling these using different strategies: 1) independent from each other, 2) sequentially (left-to-right for English), 3) starting with the most confident predictions.(Kalinsky et al., 2023) leverage the masked token representation to generate multiple tokens using a small generative model.",How do methods address the challenges of handling multi-token entities in pre-trained language models?,"To handle multi-token entities, Jiang et al. (2020a) propose using a predefined number of masked tokens and filling these using different strategies: 1) independent from each other, 2) sequentially (left-to-right for English), 3) starting with the most confident predictions. Kalinsky et al. (2023) leverage the masked token representation to generate multiple tokens using a small generative model.",What strategies are used to address the challenges of handling multi-token entities in pre-trained language models?,"To effectively manage multi-token entities in pre-trained language models, Jiang et al. (2020a) suggest using a predefined number of masked tokens and employing various strategies to fill them. These strategies include: 1) treating each token independently, 2) filling tokens sequentially, such as left-to-right for English, and 3) starting with the most confident predictions. Additionally, Kalinsky et al. (2023) utilize the masked token representation to generate multiple tokens by employing a small generative model.",7,4,8,4,"Question:

How do methods address the challenges of handling multi-token entities in pre-trained language models?

Answer:

To handle multi-token entities, Jiang et al. (2020a) propose using a predefined number of masked tokens and filling these using different strategies: 1) independent from each other, 2) sequentially (left-to-right for English), 3) starting with the most confident predictions. Kalinsky et al. (2023) leverage the masked token representation to generate multiple tokens using a small generative model.","Question:

What strategies are used to address the challenges of handling multi-token entities in pre-trained language models?

Answer:

To effectively manage multi-token entities in pre-trained language models, Jiang et al. (2020a) suggest using a predefined number of masked tokens and employing various strategies to fill them. These strategies include: 1) treating each token independently, 2) filling tokens sequentially, such as left-to-right for English, and 3) starting with the most confident predictions. Additionally, Kalinsky et al. (2023) utilize the masked token representation to generate multiple tokens by employing a small generative model.",NO,True,521,True,True
264451714-s12,Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models,2023-10-25,Should Prompts be Optimized?,"Prompt Optimizing leads to better probing performance (Jiang et al., 2020b;Shin et al., 2020;Kumar and Talukdar, 2021;Newman et al., 2022;Zhang et al., 2022) .However, it remains unclear whether this improvement is due to optimized prompts leaking new knowledge into the probed PLMs.

Optimized prompts can be mere paraphrases of manually created prompts (Bouraoui et al., 2020;Jiang et al., 2020b).These paraphrases might be better fact retrievers because of their similarity to the pre-training corpus (Cao et al., 2022).Other prompt optimization methods find better prompts in discrete or continuous spaces (Shin et al., 2020;Zhong et al., 2021).These prompts are largely uninterpretable, and can even retrieve facts from randomly initialized PLMs (Zhong et al., 2021;Ishibashi et al., 2023).

Performance improvements for optimized prompts can be attributed either to prompts becoming more similar to the pre-training data or overfitting the facts distribution.Evaluation should take the pre-training corpora and the facts distribution in the probing dataset into account (Cao et al., 2021(Cao et al., , 2022)).Future work should consider adapting prompt optimization methods to produce more interpretable prompts.This would keep the performance gains, and increase the trustworthiness of optimized prompts.",What factors should be considered when evaluating the effectiveness of optimized prompts in probing tasks?,"Prompt Optimizing leads to better probing performance (Jiang et al., 2020b; Shin et al., 2020; Kumar and Talukdar, 2021; Newman et al., 2022; Zhang et al., 2022). However, it remains unclear whether this improvement is due to optimized prompts leaking new knowledge into the probed PLMs. Optimized prompts can be mere paraphrases of manually created prompts (Bouraoui et al., 2020; Jiang et al., 2020b). These paraphrases might be better fact retrievers because of their similarity to the pre-training corpus (Cao et al., 2022). Other prompt optimization methods find better prompts in discrete or continuous spaces (Shin et al., 2020; Zhong et al., 2021). These prompts are largely uninterpretable, and can even retrieve facts from randomly initialized PLMs (Zhong et al., 2021; Ishibashi et al., 2023). Performance improvements for optimized prompts can be attributed either to prompts becoming more similar to the pre-training data or overfitting the facts distribution. Evaluation should take the pre-training corpora and the facts distribution in the probing dataset into account (Cao et al., 2021; Cao et al., 2022). Future work should consider adapting prompt optimization methods to produce more interpretable prompts. This would keep the performance gains, and increase the trustworthiness of optimized prompts.",What should be considered when assessing the effectiveness of optimized prompts in probing tasks?,"When evaluating the effectiveness of optimized prompts in probing tasks, several factors should be considered. While prompt optimization has been shown to enhance probing performance (Jiang et al., 2020b; Shin et al., 2020; Kumar and Talukdar, 2021; Newman et al., 2022; Zhang et al., 2022), it is not entirely clear if this improvement is due to the optimized prompts introducing new knowledge into the probed pre-trained language models (PLMs). Optimized prompts can sometimes be paraphrases of manually created prompts (Bouraoui et al., 2020; Jiang et al., 2020b), which might perform better as fact retrievers because they resemble the pre-training corpus more closely (Cao et al., 2022). Additionally, some prompt optimization methods identify better prompts in either discrete or continuous spaces (Shin et al., 2020; Zhong et al., 2021). These prompts are often difficult to interpret and can even retrieve facts from randomly initialized PLMs (Zhong et al., 2021; Ishibashi et al., 2023). The performance gains from optimized prompts may result from their increased similarity to the pre-training data or from overfitting the facts distribution. Therefore, it is important to consider the pre-training corpora and the facts distribution in the probing dataset during evaluation (Cao et al., 2021; Cao et al., 2022). Future research should focus on adapting prompt optimization methods to generate more interpretable prompts, which would maintain performance improvements while enhancing the trustworthiness of optimized prompts.",7,4,7,4,"Question:

What factors should be considered when evaluating the effectiveness of optimized prompts in probing tasks?

Answer:

Prompt Optimizing leads to better probing performance (Jiang et al., 2020b; Shin et al., 2020; Kumar and Talukdar, 2021; Newman et al., 2022; Zhang et al., 2022). However, it remains unclear whether this improvement is due to optimized prompts leaking new knowledge into the probed PLMs. Optimized prompts can be mere paraphrases of manually created prompts (Bouraoui et al., 2020; Jiang et al., 2020b). These paraphrases might be better fact retrievers because of their similarity to the pre-training corpus (Cao et al., 2022). Other prompt optimization methods find better prompts in discrete or continuous spaces (Shin et al., 2020; Zhong et al., 2021). These prompts are largely uninterpretable, and can even retrieve facts from randomly initialized PLMs (Zhong et al., 2021; Ishibashi et al., 2023). Performance improvements for optimized prompts can be attributed either to prompts becoming more similar to the pre-training data or overfitting the facts distribution. Evaluation should take the pre-training corpora and the facts distribution in the probing dataset into account (Cao et al., 2021; Cao et al., 2022). Future work should consider adapting prompt optimization methods to produce more interpretable prompts. This would keep the performance gains, and increase the trustworthiness of optimized prompts.","Question:

What should be considered when assessing the effectiveness of optimized prompts in probing tasks?

Answer:

When evaluating the effectiveness of optimized prompts in probing tasks, several factors should be considered. While prompt optimization has been shown to enhance probing performance (Jiang et al., 2020b; Shin et al., 2020; Kumar and Talukdar, 2021; Newman et al., 2022; Zhang et al., 2022), it is not entirely clear if this improvement is due to the optimized prompts introducing new knowledge into the probed pre-trained language models (PLMs). Optimized prompts can sometimes be paraphrases of manually created prompts (Bouraoui et al., 2020; Jiang et al., 2020b), which might perform better as fact retrievers because they resemble the pre-training corpus more closely (Cao et al., 2022). Additionally, some prompt optimization methods identify better prompts in either discrete or continuous spaces (Shin et al., 2020; Zhong et al., 2021). These prompts are often difficult to interpret and can even retrieve facts from randomly initialized PLMs (Zhong et al., 2021; Ishibashi et al., 2023). The performance gains from optimized prompts may result from their increased similarity to the pre-training data or from overfitting the facts distribution. Therefore, it is important to consider the pre-training corpora and the facts distribution in the probing dataset during evaluation (Cao et al., 2021; Cao et al., 2022). Future research should focus on adapting prompt optimization methods to generate more interpretable prompts, which would maintain performance improvements while enhancing the trustworthiness of optimized prompts.",NO,True,1536,True,True
249204437-s1,How well do real-time machine translation apps perform in practice? Insights from a literature review,2022,MT quality assessment,"The quality of MT output has been a hotly debated topic for decades, and a wide variety of methods for its assessment have been proposed (cf. Castilho et al., 2018). When classifying these methods, authors commonly distinguish between automated metrics and human metrics (e.g., Rivera-Trigueros, 2021;Chatzikoumi, 2020). Automated metrics include Word Error Rates (WERs), precision, recall, and BLEU scores, all of which are calculated on the basis of a comparison between MT output and a reference translation created by a professional human translator.

Human metrics are further subdivided by Chatzikoumi (2020) into metrics in which human experts express a direct judgement concerning the translation quality and metrics in which no direct judgement is expressed. When experts are asked to indicate the adequacy or fluency of a machine translated text on a 5-point scale, for example, they make an explicit quality judgement. When, on the other hand, they classify the translation errors occurring in the MT output, they provide useful information for improving the application without explicitly judging the quality of the output. Measuring the post-editing effort required to reach an acceptable quality level for the target text (e.g. Lacruz et al., 2014) also provides an indirect indication of MT quality.

There are several reasons why most of the metrics discussed above can be considered less suitable for assessing real-time MT that is used to support synchronous dialogues. First of all, postediting does not occur in such situations, so postediting effort cannot be used as a quality indicator. In the absence of a human-generated reference translation, automated metrics can also not be calculated. Technically speaking, human experts could judge the quality of the output after the dialogue has taken place, but they would be at a disadvantage due to the limited length and disfluent nature of the source texts, particularly when speech input is used (Przybocki et al., 2011).

Moreover, it is important to acknowledge that MT quality assessment can have different purposes. Many of the metrics above were primarily developed to identify areas of improvement for MT applications that are 'under construction' (Dorr et al., 2011). For professionals contemplating the use of real-time MT in their daily professional routines, however, improving the application is not the main priority. They want to know whether using MT will enhance the quality of their interactions with patients, students or business partners who speak a different language. In some cases, they might even wonder whether the use of MT is ethically responsible given the prevalence of errors in MT output and the potentially damaging consequences of such errors in certain contexts (Vieira et al., 2020).

Taken together, these considerations suggest that the evaluation of real-time MT might best be approached from the perspective of 'fitness for purpose', which is achieved when the quality of a translation is 'good enough' for the end user to understand the information content and pragmatic intent of a translated message Directorate General for Translation, 2016). Although this concept has featured prominently in both practical and academic discourse about translation quality for quite some time (Jiménez-Crespo, 2018), it is not yet standard practice to ask end users to assess the quality of (post-edited) MT output (cf. Van Egdom & Pluymaekers, 2019).

This raises the question to what extent existing studies into the performance of real-time MT apps are guided by the concept of fitness for purpose, and how fitness for purpose is operationalized in evaluation methods used in these studies. For the current paper, we are specifically interested in the answers to the following questions:

RQ1: To what extent are real-time MT applications tested in authentic professional situations?

RQ2: Which quality indicators are most commonly used and how are they operationalized? RQ3: Who judges the performance of real-time MT apps? RQ4: Which overall picture concerning the performance of real-time MT apps emerges from the research conducted so far?

We hope to find these answers by conducting a systematic literature review of prior studies (N = 34) which report an evaluation of a real-time MT app that was or could be used to facilitate a synchronous dialogue between interlocutors who did not speak the same language. More information about our methodology is provided in the next chapter.",Why are traditional MT quality metrics considered less suitable for real-time MT in synchronous dialogues?,"There are several reasons why most of the traditional MT quality metrics can be considered less suitable for assessing real-time MT that is used to support synchronous dialogues. First of all, postediting does not occur in such situations, so postediting effort cannot be used as a quality indicator. In the absence of a human-generated reference translation, automated metrics can also not be calculated. Technically speaking, human experts could judge the quality of the output after the dialogue has taken place, but they would be at a disadvantage due to the limited length and disfluent nature of the source texts, particularly when speech input is used (Przybocki et al., 2011). Moreover, it is important to acknowledge that MT quality assessment can have different purposes. Many of the metrics were primarily developed to identify areas of improvement for MT applications that are 'under construction' (Dorr et al., 2011). For professionals contemplating the use of real-time MT in their daily professional routines, however, improving the application is not the main priority. They want to know whether using MT will enhance the quality of their interactions with patients, students, or business partners who speak a different language. In some cases, they might even wonder whether the use of MT is ethically responsible given the prevalence of errors in MT output and the potentially damaging consequences of such errors in certain contexts (Vieira et al., 2020).",Why are traditional machine translation (MT) quality metrics often considered inadequate for evaluating real-time MT in synchronous dialogues?,"Traditional MT quality metrics are often seen as inadequate for evaluating real-time MT in synchronous dialogues for several reasons. Firstly, these situations do not involve post-editing, so post-editing effort cannot serve as a quality indicator. Additionally, without a human-generated reference translation, automated metrics cannot be calculated. While human experts could theoretically assess the quality of the output after the dialogue, they face challenges due to the limited length and disfluent nature of the source texts, especially when speech input is involved (Przybocki et al., 2011). It's also crucial to recognize that MT quality assessment serves different purposes. Many traditional metrics were developed to identify areas for improvement in MT applications that are still being refined (Dorr et al., 2011). However, for professionals considering the use of real-time MT in their daily routines, the primary concern is not improving the application but determining whether MT will enhance their interactions with patients, students, or business partners who speak different languages. In some cases, they may even question the ethical responsibility of using MT, given the prevalence of errors in MT output and the potentially harmful consequences of such errors in certain contexts (Vieira et al., 2020).",7,2,7,4,"Question:

Why are traditional MT quality metrics considered less suitable for real-time MT in synchronous dialogues?

Answer:

There are several reasons why most of the traditional MT quality metrics can be considered less suitable for assessing real-time MT that is used to support synchronous dialogues. First of all, postediting does not occur in such situations, so postediting effort cannot be used as a quality indicator. In the absence of a human-generated reference translation, automated metrics can also not be calculated. Technically speaking, human experts could judge the quality of the output after the dialogue has taken place, but they would be at a disadvantage due to the limited length and disfluent nature of the source texts, particularly when speech input is used (Przybocki et al., 2011). Moreover, it is important to acknowledge that MT quality assessment can have different purposes. Many of the metrics were primarily developed to identify areas of improvement for MT applications that are 'under construction' (Dorr et al., 2011). For professionals contemplating the use of real-time MT in their daily professional routines, however, improving the application is not the main priority. They want to know whether using MT will enhance the quality of their interactions with patients, students, or business partners who speak a different language. In some cases, they might even wonder whether the use of MT is ethically responsible given the prevalence of errors in MT output and the potentially damaging consequences of such errors in certain contexts (Vieira et al., 2020).","Question:

Why are traditional machine translation (MT) quality metrics often considered inadequate for evaluating real-time MT in synchronous dialogues?

Answer:

Traditional MT quality metrics are often seen as inadequate for evaluating real-time MT in synchronous dialogues for several reasons. Firstly, these situations do not involve post-editing, so post-editing effort cannot serve as a quality indicator. Additionally, without a human-generated reference translation, automated metrics cannot be calculated. While human experts could theoretically assess the quality of the output after the dialogue, they face challenges due to the limited length and disfluent nature of the source texts, especially when speech input is involved (Przybocki et al., 2011). It's also crucial to recognize that MT quality assessment serves different purposes. Many traditional metrics were developed to identify areas for improvement in MT applications that are still being refined (Dorr et al., 2011). However, for professionals considering the use of real-time MT in their daily routines, the primary concern is not improving the application but determining whether MT will enhance their interactions with patients, students, or business partners who speak different languages. In some cases, they may even question the ethical responsibility of using MT, given the prevalence of errors in MT output and the potentially harmful consequences of such errors in certain contexts (Vieira et al., 2020).",NO,True,1326,True,True
256461385-s2,Narrative Why-Question Answering: A Review of Challenges and Datasets,2022,Causality,"Causality is a semantic relationship between events showing that an event occurs or holds due to another event (Mostafazadeh et al., 2016b). Mostafazadeh et al. (2016b) distinguish four types of lexical causality relations: cause, enable, prevent, and cause-to-end based on the works by Wolff and Song (2003), Wolff (2007), and Khemlani et al. (2014). Moreover, causality has temporal implications such that if an event A causes/enables/prevents an event B, then A should start before B, or if an event A causes an event B to end, then B should start before A. Causality relations can hold one of the three temporal implications: before, overlaps, and during (Mostafazadeh et al., 2016b). Thus, while answering a whyquestion, the temporal relation between the events should also be taken into account in addition to the causality relation.

A causal relation is constructed from two components: cause and effect. Based on how the cause and the effect are conveyed in a text, causation can be distinguished into the following categories: explicit vs implicit, marked vs unmarked, and ambiguous vs unambiguous.

Explicit vs Implicit. Causation is explicit if both the cause and the effect are present in the text. Causation is implicit if either the cause or the effect of both are missing from the text (Blanco et al., 2008). For instance, ""She was accepted to a top university after receiving a high score in the state examination"" is explicit, while ""I did not attend the mandatory final exam."" is implicit because the effect of ""failing the course"" is not explicitly stated.

Marked vs Unmarked. Causation is marked if the text contains the causal signal words that indicate the causal relation (Blanco et al., 2008). For example, ""I was late because of traffic"" is marked, but ""Do not buy any bread. We have already got two at home"" is unmarked.

Ambiguous vs Unambiguous. If the causal relation is presented in the text with causal keywords (e.g., cause, effect, consequence) or with causal signals (e.g., because of, due to, as a result of ), it is considered unambiguous (Girju, 2003). On the other hand, if a causal relation is constructed in the form of an expression containing affect verbs (e.g., affect, change, influence) or link verbs (e.g., link, lead, depend), it is considered ambiguous. Furthermore, if a marked signal always refers to causation (e.g., because), it is unambiguous, while if a marked word occasionally signals causation (e.g., since), it is ambiguous (Blanco et al., 2008).",How is causation categorized based on the presence of causal signals and keywords in text?,"Causation can be categorized into explicit vs implicit, marked vs unmarked, and ambiguous vs unambiguous based on the presence of causal signals and keywords in text. Causation is explicit if both the cause and the effect are present in the text, while it is implicit if either the cause or the effect or both are missing from the text (Blanco et al., 2008). Causation is marked if the text contains the causal signal words that indicate the causal relation (Blanco et al., 2008). If the causal relation is presented in the text with causal keywords (e.g., cause, effect, consequence) or with causal signals (e.g., because of, due to, as a result of), it is considered unambiguous (Girju, 2003). On the other hand, if a causal relation is constructed in the form of an expression containing affect verbs (e.g., affect, change, influence) or link verbs (e.g., link, lead, depend), it is considered ambiguous. Furthermore, if a marked signal always refers to causation (e.g., because), it is unambiguous, while if a marked word occasionally signals causation (e.g., since), it is ambiguous (Blanco et al., 2008).",How is causation categorized in text based on causal signals and keywords?,"Causation in text can be categorized into several types based on the presence of causal signals and keywords. It is considered explicit when both the cause and the effect are clearly present in the text, whereas it is implicit if either the cause, the effect, or both are missing (Blanco et al., 2008). Additionally, causation is marked if the text includes specific causal signal words that indicate a causal relationship (Blanco et al., 2008). When a causal relationship is presented with clear causal keywords such as ""cause,"" ""effect,"" or ""consequence,"" or with causal signals like ""because of,"" ""due to,"" or ""as a result of,"" it is deemed unambiguous (Girju, 2003). Conversely, if a causal relationship is expressed using affect verbs like ""affect,"" ""change,"" or ""influence,"" or link verbs such as ""link,"" ""lead,"" or ""depend,"" it is considered ambiguous. Furthermore, if a marked signal consistently indicates causation, such as ""because,"" it is unambiguous. However, if a marked word only occasionally signals causation, like ""since,"" it is ambiguous (Blanco et al., 2008).",7,4,8,4,"Question:

How is causation categorized based on the presence of causal signals and keywords in text?

Answer:

Causation can be categorized into explicit vs implicit, marked vs unmarked, and ambiguous vs unambiguous based on the presence of causal signals and keywords in text. Causation is explicit if both the cause and the effect are present in the text, while it is implicit if either the cause or the effect or both are missing from the text (Blanco et al., 2008). Causation is marked if the text contains the causal signal words that indicate the causal relation (Blanco et al., 2008). If the causal relation is presented in the text with causal keywords (e.g., cause, effect, consequence) or with causal signals (e.g., because of, due to, as a result of), it is considered unambiguous (Girju, 2003). On the other hand, if a causal relation is constructed in the form of an expression containing affect verbs (e.g., affect, change, influence) or link verbs (e.g., link, lead, depend), it is considered ambiguous. Furthermore, if a marked signal always refers to causation (e.g., because), it is unambiguous, while if a marked word occasionally signals causation (e.g., since), it is ambiguous (Blanco et al., 2008).","Question:

How is causation categorized in text based on causal signals and keywords?

Answer:

Causation in text can be categorized into several types based on the presence of causal signals and keywords. It is considered explicit when both the cause and the effect are clearly present in the text, whereas it is implicit if either the cause, the effect, or both are missing (Blanco et al., 2008). Additionally, causation is marked if the text includes specific causal signal words that indicate a causal relationship (Blanco et al., 2008). When a causal relationship is presented with clear causal keywords such as ""cause,"" ""effect,"" or ""consequence,"" or with causal signals like ""because of,"" ""due to,"" or ""as a result of,"" it is deemed unambiguous (Girju, 2003). Conversely, if a causal relationship is expressed using affect verbs like ""affect,"" ""change,"" or ""influence,"" or link verbs such as ""link,"" ""lead,"" or ""depend,"" it is considered ambiguous. Furthermore, if a marked signal consistently indicates causation, such as ""because,"" it is unambiguous. However, if a marked word only occasionally signals causation, like ""since,"" it is ambiguous (Blanco et al., 2008).",NO,True,1079,True,True
256461385-s11,Narrative Why-Question Answering: A Review of Challenges and Datasets,2022,Evaluation measures,"For multiple-choice QA datasets, accuracy is a commonly used metric to measure the performance of a model. For free-form QA datasets, both automatic and human evaluation measures are utilized to evaluate the capabilities of the QA model. Most commonly, ROUGE-L (Lin, 2004), Meteor (Denkowski and Lavie, 2011), BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020) and

BertScore (Zhang et al., 2020) have been used to automatically evaluate the performance of the freeform QA models in narrative setting. Overall, F1 score of the ROUGE-L is the most commonly reported automatic evaluation measure.

In terms of human evaluation, Lal et al. (2021) proposed to assess the grammaticality and validity of the answers based on a 5-point Likert scale. The scale of the grammaticality ranges from strongly ungrammatical (1) to strongly grammatical (5), where a strongly grammatical answer must follow all the rules of the English grammar and a neutral score (3) is indicated when the meaning of the answer can be still inferred despite clear grammatical mistakes. The validity scale assesses whether the answer is valid and makes sense in the given context.",How are free-form QA models evaluated using both automatic and human measures in narrative settings?,"For free-form QA datasets, both automatic and human evaluation measures are utilized to evaluate the capabilities of the QA model. Most commonly, ROUGE-L (Lin, 2004), Meteor (Denkowski and Lavie, 2011), BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), and BertScore (Zhang et al., 2020) have been used to automatically evaluate the performance of the free-form QA models in narrative settings. Overall, the F1 score of the ROUGE-L is the most commonly reported automatic evaluation measure. In terms of human evaluation, Lal et al. (2021) proposed to assess the grammaticality and validity of the answers based on a 5-point Likert scale. The scale of grammaticality ranges from strongly ungrammatical (1) to strongly grammatical (5), where a strongly grammatical answer must follow all the rules of English grammar, and a neutral score (3) is indicated when the meaning of the answer can still be inferred despite clear grammatical mistakes. The validity scale assesses whether the answer is valid and makes sense in the given context.",How is the performance of free-form QA models assessed in narrative contexts using both automatic and human evaluation methods?,"The evaluation of free-form QA models in narrative settings involves a combination of automatic and human measures. For automatic evaluation, several metrics are commonly employed, including ROUGE-L (Lin, 2004), Meteor (Denkowski and Lavie, 2011), BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), and BertScore (Zhang et al., 2020). Among these, the F1 score of ROUGE-L is the most frequently reported metric for assessing model performance. On the human evaluation side, Lal et al. (2021) introduced a method to evaluate the grammaticality and validity of the model's answers using a 5-point Likert scale. This scale ranges from strongly ungrammatical (1) to strongly grammatical (5), with a neutral score (3) indicating that the answer's meaning can still be inferred despite grammatical errors. The validity scale measures whether the answer is appropriate and makes sense within the given context.",7,4,7,4,"Question:

How are free-form QA models evaluated using both automatic and human measures in narrative settings?

Answer:

For free-form QA datasets, both automatic and human evaluation measures are utilized to evaluate the capabilities of the QA model. Most commonly, ROUGE-L (Lin, 2004), Meteor (Denkowski and Lavie, 2011), BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), and BertScore (Zhang et al., 2020) have been used to automatically evaluate the performance of the free-form QA models in narrative settings. Overall, the F1 score of the ROUGE-L is the most commonly reported automatic evaluation measure. In terms of human evaluation, Lal et al. (2021) proposed to assess the grammaticality and validity of the answers based on a 5-point Likert scale. The scale of grammaticality ranges from strongly ungrammatical (1) to strongly grammatical (5), where a strongly grammatical answer must follow all the rules of English grammar, and a neutral score (3) is indicated when the meaning of the answer can still be inferred despite clear grammatical mistakes. The validity scale assesses whether the answer is valid and makes sense in the given context.","Question:

How is the performance of free-form QA models assessed in narrative contexts using both automatic and human evaluation methods?

Answer:

The evaluation of free-form QA models in narrative settings involves a combination of automatic and human measures. For automatic evaluation, several metrics are commonly employed, including ROUGE-L (Lin, 2004), Meteor (Denkowski and Lavie, 2011), BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), and BertScore (Zhang et al., 2020). Among these, the F1 score of ROUGE-L is the most frequently reported metric for assessing model performance. On the human evaluation side, Lal et al. (2021) introduced a method to evaluate the grammaticality and validity of the model's answers using a 5-point Likert scale. This scale ranges from strongly ungrammatical (1) to strongly grammatical (5), with a neutral score (3) indicating that the answer's meaning can still be inferred despite grammatical errors. The validity scale measures whether the answer is appropriate and makes sense within the given context.",NO,True,912,True,True
32461868-s2,"A Survey on Intelligent Poetry Generation: Languages, Features, Techniques, Reutilisation and Evaluation",2017-09-01,Form features,"Despite all the levels of language involved in poetry, form is a key feature for, at the first glance, recognis-ing the resulting text as poetic. Most common formrelated features are, without a doubt, a regular metre and rhymes. When alone, both of them are quite straightforward to handle by computer programs, especially when compared with content features.

Metre is generally modelled with the number of syllables each line has, sometimes also considering the stress patterns (e.g. Manurung (2003), Gervás (2001), Tobing and Manurung (2015)), which indicate the position of the stressed syllables. Rhyme results from the repetition of certain sounds (e.g. in great and mate). End-rhymes, the most typical, occur when two lines end in the same sound. But some systems consider other kinds of rhyme, such as assonance or alliteration, which respectively involve the repetition of the same vowel or of a consonant sound throughout the poem.

For less phonetic languages, such as Portuguese (Gonçalo Oliveira et al., 2007) or Spanish (Gervás, 2001), it is often enough to design a set of orthography-based rules to handle metre and rhyme. For English, poetry generators (e.g. Manurung (2003), , Tobing and Manurung (2015)) typically resort to a pronunciation dictionary for this purpose (e.g. CMU's 1 ). Yet, automatic methods for the automatic scansion of poetry have also been developed (Agirrezabal et al., 2016).

Metre and rhymes are often organised according to a well-known poetry form and some systems are designed to produce only poems of specific forms. Haikus traditionally have 3 lines, respectively with 5, 7 and 5 syllables (Manurung, 2003;Netzer et al., 2009), but there are modern haikus with a different number (Wong and Chun, 2008). Limericks have five lines, with lines 1, 2 and 5 generally longer, and rhyme of the kind AABBA (Levy, 2001;Manurung, 2003). The sonnet is a classic form of poem with 14 lines, typically with 10-syllables each. Depending on the tradition, it might have different groupings, stress patterns and rhyming schemes, such as ABAB CDCD EFEF GG (Ghazvininejad et al., 2016). Spanish traditional forms (Gervás, 2000;Gervás, 2001) include the romance, lines of 8 syllables, where all even-numbered rhyme together; the cuarteto, a stanza with four 11-syllable lines, where the two outer lines rhyme together; and tercetos encadenados, stanzas of three 11-syllable lines with the pattern ABA BCB CDC... Bertsolaritza is a Basque traditional verse with metre and rhyme constraints, typically sung (Agirrezabal et al., 2013). The generation of classic Chinese poetry has focused mostly on quartrains, four lines of 5 or 7 characters with a rigid tonal pattern where two kinds of tones are interleaved, and a rhyme scheme where the majority of the lines in the same poem end with the same vowel, but not the same character (Yan et al., 2013;Zhang and Lapata, 2014;Yan, 2016).

The poetry form can be decided from the initial data (Gervás, 2000), while other systems generate poetry in more or less any form, depending on a user-provided template, which might be strictly structural (Gonçalo Oliveira, 2012) or a poem, possibly with some words stripped (Toivanen et al., 2014). There are also systems focused on generating song lyrics, which have less traditional forms, but where metre is key for matching the rhythm, while other features should still be present. These include melodies where stressed and weak beats are identified (Gonçalo Oliveira et al., 2007;Ramakrishnan A et al., 2009;Gonçalo Oliveira, 2015), pop songs (Barbieri et al., 2012), or rap (Malmi et al., 2016;Potash et al., 2015) where, besides rhyme, assonance is modelled as the repetition of vowel phonemes (e.g. in raps and tax). ered and is often only softly satisfied, for instance, by using words that belong to the same semantic domain. This section describes how different poetry generators select their content in order to transmit a meaningful message or, at least, to be, as much as possible, semantically coherent.

Intelligent poetry generation systems often exploit a model of semantics, either a semantic knowledge base, or a statistical model of distributional semantics. The former is usually a more theoretical view on linguistic knowledge, where words are connected according to labelled relations, with different meanings. Poetry generators have used knowledge bases with verbs and their restrictions and ontological categories (Ramakrishnan A and Devi, 2010); semantic networks extracted from dictionaries, that go beyond synonymy and hypernymy, and cover other relations such as causation, property and others (Gonçalo Oliveira, 2012); WordNet, a lexical knowledge base Agirrezabal et al., 2013;Tobing and Manurung, 2015); and Con-ceptNet, a common sense knowledge base (Das and Gambäck, 2014). Those have been used not only to restrict the generated words to a common semantic domain, but also for increasing the paraphrasing power, towards higher variation and better covering of different metres.

Distributional models of semantics target how language is actually used, in a collection of documents, and consider that words that occur in similar contexts have similar meanings. These include vector space models, either based on words (Wong and Chun, 2008;McGregor et al., 2016), also including word embeddings learned from collections of poems (Yan, 2016) or from Wikipedia (Ghazvininejad et al., 2016), or based on sentences (Malmi et al., 2016), both used to compute the semantic relatedness with the cosine similarity; or word associations (Netzer et al., 2009;Toivanen et al., 2012) which, according to some authors, capture relations in poetic text better than WordNet-like lexical knowledge bases.

In some systems, text is generated according to a grammar for handling syntax, possibly also considering semantic features (Manurung, 2003). In Gonçalo Oliveira (2012)'s system, the grammar is tightly related to the semantics, as each rule transmits a known semantic relation and can be instan-tiated with any pair of words sharing relations of that kind (e.g. vehicle-car or fruit-mango, for hypernymy).

Yet, in order to enable some kind of interpretation, the poem must actually be about something or, at least, be different for different stimuli, reflected in its content. Stimuli can be given in different forms, with different degrees of precision, namely: a list of semantic predicates (e.g. love(John, Mary)) (Manurung, 2003); one (Netzer et al., 2009;Toivanen et al., 2013;Ghazvininejad et al., 2016) or more (Wong and Chun, 2008;Gonçalo Oliveira, 2012;Zhang and Lapata, 2014;Yan, 2016) keywords that will, somehow, set a semantic domain and constraint the generation space; a line of text (Das and Gambäck, 2014) or a sequence of lines (Malmi et al., 2016) to be followed; a textual document, which can either be a single sentence with a message (Gervás, 2001), or a longer text from a blog (Misztal and Indurkhya, 2014) or newspaper (Díaz-Agudo et al., 2002;Rashel and Manurung, 2014;Toivanen et al., 2014;Tobing and Manurung, 2015;Gonçalo Oliveira and Alves, 2016).

In order to extract meaningful information to be used in the poem, different systems process the input document differently. For instance, Toivanen et al. (2014) acquire novel associations from the document (e.g. bieber and alcohol, in opposition to pop and star), identified by contrast with well-known associations. Tobing and Manurung (2015) extract dependency relations from the document and use them to constrain the generated poem. They argue that, though not a genuine semantic representation, dependency relations are a useful abstraction of the text and end up conveying its semantics. In fact, some dependency relations include semantic relations (e.g. agent-of, subject-of, object-of ). A final example (Gonçalo Oliveira and Alves, 2016) extracts concept maps from the input document, and uses them as a semantic network.

Towards an improved interpretation, 's system produces natural language commentaries for each generated poem, providing some generation context. A similar feature is presented by Gonçalo Oliveira and Alves (2016) or Gonçalo Oliveira et al. (2017). In this case, semantic relation instances explaining the connection between the input keywords and the words used can be provided either in raw format or, if a grammar exists for this purpose, in natural language.

Additional semantic features captured by poetry generators include sentiment (Gervás, 2000;Gonçalo Oliveira et al., 2017), which typically involves exploiting a polarity lexicon; or emotion (Misztal and Indurkhya, 2014), in this case achieved with the help of WordNet Affect.

Figurative language is often implicitly present as a consequence of reusing material from humanproduced poetry, but its presence can also be explicitly handled, for instance, by exploiting similes mined from Google n-grams . Veale (2013) points out the importance of contentfeatures and presents a system more relaxed on form but heavily influenced by figurative language. More precisely, similes (e.g. politicians are crooks) are exploited for generating metaphors (e.g. he is a crook) and conceptual blends (e.g. sweet silence).

Poetry generation systems handle a broad range of features both at the formal and at the content level. Dealing with so many constraints may actually turn out to be computationally impractical (see e.g. Tobing and Manurung (2015)). Yet, this also depends on the techniques adopted for handling all the constrains, surveyed in the following section.",How are metre and rhyme typically handled in English poetry generation systems?,"Metre is generally modeled with the number of syllables each line has, sometimes also considering the stress patterns, which indicate the position of the stressed syllables. Rhyme results from the repetition of certain sounds, such as in great and mate. End-rhymes, the most typical, occur when two lines end in the same sound. Some systems consider other kinds of rhyme, such as assonance or alliteration, which respectively involve the repetition of the same vowel or of a consonant sound throughout the poem. For English, poetry generators typically resort to a pronunciation dictionary for handling metre and rhyme. Automatic methods for the automatic scansion of poetry have also been developed.",How do English poetry generation systems typically handle metre and rhyme?,"In English poetry generation systems, metre is typically managed by counting the number of syllables in each line and sometimes considering the stress patterns to identify the position of stressed syllables. Rhyme is achieved through the repetition of certain sounds, such as in the words ""great"" and ""mate."" The most common form is end-rhyme, where two lines conclude with the same sound. Some systems also incorporate other types of rhyme, like assonance, which involves the repetition of vowel sounds, or alliteration, which focuses on the repetition of consonant sounds throughout the poem. To handle metre and rhyme, poetry generators often use a pronunciation dictionary. Additionally, automatic methods for the scansion of poetry have been developed to further aid in this process.",9,4,9,4,"Question:

How are metre and rhyme typically handled in English poetry generation systems?

Answer:

Metre is generally modeled with the number of syllables each line has, sometimes also considering the stress patterns, which indicate the position of the stressed syllables. Rhyme results from the repetition of certain sounds, such as in great and mate. End-rhymes, the most typical, occur when two lines end in the same sound. Some systems consider other kinds of rhyme, such as assonance or alliteration, which respectively involve the repetition of the same vowel or of a consonant sound throughout the poem. For English, poetry generators typically resort to a pronunciation dictionary for handling metre and rhyme. Automatic methods for the automatic scansion of poetry have also been developed.","Question:

How do English poetry generation systems typically handle metre and rhyme?

Answer:

In English poetry generation systems, metre is typically managed by counting the number of syllables in each line and sometimes considering the stress patterns to identify the position of stressed syllables. Rhyme is achieved through the repetition of certain sounds, such as in the words ""great"" and ""mate."" The most common form is end-rhyme, where two lines conclude with the same sound. Some systems also incorporate other types of rhyme, like assonance, which involves the repetition of vowel sounds, or alliteration, which focuses on the repetition of consonant sounds throughout the poem. To handle metre and rhyme, poetry generators often use a pronunciation dictionary. Additionally, automatic methods for the scansion of poetry have been developed to further aid in this process.",NO,True,788,True,True
2395785-s4,To what extent is Immediate Constituency Analysis dependency-based? A survey of foundational texts,2017-09-13,Connection-basedness.,"Words combine pairwise, they are in a syntagmatic relationship in the sense of de Saussure (2013, 170):

Words as used in discourse, strung together one after another, enter into relations based on the linear character of languages. Linearity precludes the possibility of uttering two words simultaneously. They must be arranged consecutively in spoken sequence. Combinations based on sequentiality may be called syntagmas. The syntagma invariably comprises two or more consecutive units: for example, re-lire ('re-read'), contre tous ('against all'), la vie humaine ('the life of man'), Dieu est bon ('God is good'), s'il fait beau temps, nous sortirons ('if it's fine, we'll go out').

Since the term syntagma has been led astray -this is especially the case in French linguistic: Fr. syntagme has been used to translate phrase (Chomsky, 1969) -, we suggest to use the term connection introduced by Tesnière (2015, ch. 1, § 3-5):

Each word in a sentence is not isolated as it is in the dictionary. The mind perceives connections between a word and its neighbors. The totality of these connections forms the scaffold of the sentence. [. . . ] [A] sentence of the type Alfred speaks is not composed of just the two elements, Alfred and speaks, but rather of three elements, the first being Alfred, the second speaks, and the third the connection that unites them -without which there would be no sentence.

Elaborating from this quotation, we call connection the undirected relation underlying any dependency. 3 Hence, in a dependency tree, syntagmatic relations are encoded by edges. By contrast, in a PST, edges represent constituency relations -see also (Mel'čuk, 1988, 13-14). Analyses and diagrams that make use of connections to describe the syntactic structure of constructions are connection-based.

Binarity. In a dependency tree, a connection always involves exactly two words. In a PST, a phrase can have more than two immediate constituents. Binarity is a central property of ICA until the 60's and still remains preeminent. 4 It seems that binarity is the consequence of the connectionbasedness of these ICAs. Non-binary structures appear later, cf. fig. 6 (Chomsky, 1965, 65). 5 Figure 6: First PST in (Chomsky, 1965) Headedness. Connections are directed, as explained by Tesnière (2015, ch. 2, § 1-3):

Structural connections establish dependency relations between words. In principle, each connection unites a superior term and an inferior term. The superior term is called the governor, and the inferior term the subordinate. Thus in the sentence Alfred speaks (Stemma 1), speaks is the governor and Alfred is the subordinate. We say that the subordinate depends on the governor and that the governor governs the subordinate. Thus in the sentence Alfred speaks (Stemma 1), Alfred depends on speaks, and speaks governs Alfred.

We call this property headedness.

It is noteworthy to mention that although the notion of head is absent from , headedness is considered as a central notion in many early ICA-based presentations, and especially in (Bloomfield, 1933). Bloomfield's work emphasizes constituency relations, but connections are also considered: ""Every syntactic constructions shows us two (or sometimes more free forms combined in a phrase, which may call the resultant phrase."" ( § 12.10) This last definition allows Bloomfield to oppose endocentric vs. exocentric constructions, according to the fact that the resultant phrase may belong or not to the ""formclass"" (i.e. distributional class) of one of the constituents (called the head). In a dependency tree, every construction is endocentric, i.e. connections are directed from a governor to a dependent. In a PST, endocentric constructions can be encoded by marking one of their constituents as the head.

Flatness (i.e. absence of stratification). In a dependency tree, dependents that have the same governor are not hierarchized. In a PST, phrases are embedded: if a head word has several complements (or specifiers, or adjuncts), each of them can belong to a different stratum (Kahane, 1997;Kahane and Mazziotta, 2015). E.g., the dependency tree of a sentence such as Mary gives Peter a book represents Mary, Peter and a book as co-dependents of gives that belong to the same level, whereas a PST of the same sentence can attach Mary, Peter and a book at different levels. Stratification remains the main difference between dependency syntax and ICA-based syntax. This point will be developed in Section 4.

Node-to-word mapping. Dependency trees do not encode connections by the means of nodes: these are used exclusively to encode words. 6 As a result, one can state:

A dependency structure for a sentence is a one-to-one mapping between the nodes of a tree (the dependency tree) and the words of the sentence. (Kahane, 1996, 45) By contrast, classical PST use nodes to encode words as well as constituents. Thus the mapping between nodes and words is not one-to-one. As it will appear in the next section, node-to-word mapping does not imply flatness.

As soon as additional nodes are introduced, labels on these nodes can be used to reify other information. E.g., X-bar syntax (Chomsky, 1970) uses XP vs. X labels to express headedness.  ",How are syntagmatic relations and connections represented differently in dependency trees and phrase structure trees (PSTs)?,"Words combine pairwise, they are in a syntagmatic relationship in the sense of de Saussure (2013, 170). In a dependency tree, syntagmatic relations are encoded by edges. By contrast, in a PST, edges represent constituency relations (Mel'čuk, 1988, 13-14). Analyses and diagrams that make use of connections to describe the syntactic structure of constructions are connection-based. In a dependency tree, a connection always involves exactly two words. In a PST, a phrase can have more than two immediate constituents. Connections are directed, establishing dependency relations between words, with a superior term called the governor and an inferior term called the subordinate. In a dependency tree, every construction is endocentric, meaning connections are directed from a governor to a dependent. In a PST, endocentric constructions can be encoded by marking one of their constituents as the head. In a dependency tree, dependents that have the same governor are not hierarchized. In a PST, phrases are embedded, allowing for different levels of stratification.",How do dependency trees and phrase structure trees (PSTs) represent syntagmatic relations and connections differently?,"In linguistic analysis, words combine in syntagmatic relationships, as described by de Saussure (2013, 170). In dependency trees, these syntagmatic relations are represented by edges, which establish connections between pairs of words. Each connection involves exactly two words, with a directed relationship that identifies one word as the governor and the other as the subordinate. This creates endocentric constructions, where connections are directed from the governor to the dependent, and dependents sharing the same governor are not hierarchized.

In contrast, phrase structure trees (PSTs) use edges to represent constituency relations, as noted by Mel'čuk (1988, 13-14). In PSTs, a phrase can have more than two immediate constituents, allowing for a more complex hierarchical structure. Phrases are embedded within each other, creating different levels of stratification. Endocentric constructions in PSTs can be indicated by marking one of the constituents as the head, providing a different approach to representing syntactic structure compared to dependency trees.",8,2,8,2,"Question:

How are syntagmatic relations and connections represented differently in dependency trees and phrase structure trees (PSTs)?

Answer:

Words combine pairwise, they are in a syntagmatic relationship in the sense of de Saussure (2013, 170). In a dependency tree, syntagmatic relations are encoded by edges. By contrast, in a PST, edges represent constituency relations (Mel'čuk, 1988, 13-14). Analyses and diagrams that make use of connections to describe the syntactic structure of constructions are connection-based. In a dependency tree, a connection always involves exactly two words. In a PST, a phrase can have more than two immediate constituents. Connections are directed, establishing dependency relations between words, with a superior term called the governor and an inferior term called the subordinate. In a dependency tree, every construction is endocentric, meaning connections are directed from a governor to a dependent. In a PST, endocentric constructions can be encoded by marking one of their constituents as the head. In a dependency tree, dependents that have the same governor are not hierarchized. In a PST, phrases are embedded, allowing for different levels of stratification.","Question:

How do dependency trees and phrase structure trees (PSTs) represent syntagmatic relations and connections differently?

Answer:

In linguistic analysis, words combine in syntagmatic relationships, as described by de Saussure (2013, 170). In dependency trees, these syntagmatic relations are represented by edges, which establish connections between pairs of words. Each connection involves exactly two words, with a directed relationship that identifies one word as the governor and the other as the subordinate. This creates endocentric constructions, where connections are directed from the governor to the dependent, and dependents sharing the same governor are not hierarchized.

In contrast, phrase structure trees (PSTs) use edges to represent constituency relations, as noted by Mel'čuk (1988, 13-14). In PSTs, a phrase can have more than two immediate constituents, allowing for a more complex hierarchical structure. Phrases are embedded within each other, creating different levels of stratification. Endocentric constructions in PSTs can be indicated by marking one of the constituents as the head, providing a different approach to representing syntactic structure compared to dependency trees.",NO,True,1077,True,True
12719479-s2,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,"Agreement, Reliability, and Validity","We begin with a quick recap of the goals of agreement studies, inspired by Krippendorff (2004a, Section 11.1). Researchers who wish to use hand-coded data-that is, data in which items are labeled with categories, whether to support an empirical claim or to develop and test a computational model-need to show that such data are reliable.

The fundamental assumption behind the methodologies discussed in this article is that data are reliable if coders can be shown to agree on the categories assigned to units to an extent determined by the purposes of the study (Krippendorff 2004a;Craggs and McGee Wood 2005). If different coders produce consistently similar results, then we can infer that they have internalized a similar understanding of the annotation guidelines, and we can expect them to perform consistently under this understanding.

Reliability is thus a prerequisite for demonstrating the validity of the coding scheme-that is, to show that the coding scheme captures the ""truth"" of the phenomenon being studied, in case this matters: If the annotators are not consistent then either some of them are wrong or else the annotation scheme is inappropriate for the data. (Just as in real life, the fact that witnesses to an event disagree with each other makes it difficult for third parties to know what actually happened.) However, it is important to keep in mind that achieving good agreement cannot ensure validity: Two observers of the same event may well share the same prejudice while still being objectively wrong.",Why is reliability crucial for validating a coding scheme in computational linguistics?,"Reliability is crucial for validating a coding scheme in computational linguistics because it demonstrates that coders can agree on the categories assigned to units to an extent determined by the purposes of the study (Krippendorff 2004a; Craggs and McGee Wood 2005). If different coders produce consistently similar results, it can be inferred that they have internalized a similar understanding of the annotation guidelines, and they are expected to perform consistently under this understanding. Reliability is thus a prerequisite for demonstrating the validity of the coding scheme, which means showing that the coding scheme captures the ""truth"" of the phenomenon being studied, in case this matters. If the annotators are not consistent, then either some of them are wrong or the annotation scheme is inappropriate for the data.",Why is reliability important for validating a coding scheme in computational linguistics?,"Reliability is important for validating a coding scheme in computational linguistics because it ensures that coders consistently agree on the categories assigned to units, as determined by the study's objectives (Krippendorff 2004a; Craggs and McGee Wood 2005). When different coders produce consistently similar results, it indicates that they have internalized a similar understanding of the annotation guidelines and are expected to perform consistently under this shared understanding. Therefore, reliability is a prerequisite for demonstrating the validity of the coding scheme, which involves showing that the scheme accurately captures the ""truth"" of the phenomenon being studied, if this is relevant. If annotators are not consistent, it suggests that either some of them are incorrect or the annotation scheme is unsuitable for the data.",9,2,9,2,"Question:

Why is reliability crucial for validating a coding scheme in computational linguistics?

Answer:

Reliability is crucial for validating a coding scheme in computational linguistics because it demonstrates that coders can agree on the categories assigned to units to an extent determined by the purposes of the study (Krippendorff 2004a; Craggs and McGee Wood 2005). If different coders produce consistently similar results, it can be inferred that they have internalized a similar understanding of the annotation guidelines, and they are expected to perform consistently under this understanding. Reliability is thus a prerequisite for demonstrating the validity of the coding scheme, which means showing that the coding scheme captures the ""truth"" of the phenomenon being studied, in case this matters. If the annotators are not consistent, then either some of them are wrong or the annotation scheme is inappropriate for the data.","Question:

Why is reliability important for validating a coding scheme in computational linguistics?

Answer:

Reliability is important for validating a coding scheme in computational linguistics because it ensures that coders consistently agree on the categories assigned to units, as determined by the study's objectives (Krippendorff 2004a; Craggs and McGee Wood 2005). When different coders produce consistently similar results, it indicates that they have internalized a similar understanding of the annotation guidelines and are expected to perform consistently under this shared understanding. Therefore, reliability is a prerequisite for demonstrating the validity of the coding scheme, which involves showing that the scheme accurately captures the ""truth"" of the phenomenon being studied, if this is relevant. If annotators are not consistent, it suggests that either some of them are incorrect or the annotation scheme is unsuitable for the data.",NO,True,846,True,True
12719479-s5,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,Agreement Without Chance Correction,"The simplest measure of agreement between two coders is percentage of agreement or observed agreement, defined for example by Scott (1955, page 323) as ""the percentage of judgments on which the two analysts agree when coding the same data independently."" This is the number of items on which the coders agree divided by the total number of items. More precisely, and looking ahead to the following discussion, observed agreement is the arithmetic mean of the agreement value agr i for all items i ∈ I, defined as follows:

agr i = 1 if the two coders assign i to the same category 0 if the two coders assign i to different categories Observed agreement over the values agr i for all items i ∈ I is then:

For example, let us assume a very simple annotation scheme for dialogue acts in information-seeking dialogues which makes a binary distinction between the categories statement and info-request, as in the DAMSL dialogue act scheme . Two coders classify 100 utterances according to this scheme as shown in Table 1. Percentage agreement for this data set is obtained by summing up the cells on the diagonal and dividing by the total number of items: A o = (20 + 50)/100 = 0.7. Observed agreement enters in the computation of all the measures of agreement we consider, but on its own it does not yield values that can be compared across studies, because some agreement is due to chance, and the amount of chance agreement is affected by two factors that vary from one study to the other. First of all, as Scott (1955, page 322) points out, ""[percentage agreement] is biased in favor of dimensions with a small number of categories."" In other words, given two coding schemes for the same phenomenon, the one with fewer categories will result in higher percentage agreement just by chance. If two coders randomly classify utterances in a uniform manner using the scheme of Table 1, we would expect an equal number of items to fall in each of the four cells in the table, and therefore pure chance will cause the coders to agree on half of the items (the two cells on the diagonal: 1 4 + 1 4 ). But suppose we want to refine the simple binary coding scheme by introducing a new category, check, as in the MapTask coding scheme (Carletta et al. 1997). If two coders randomly classify utterances in a uniform manner using the three categories in the second scheme, they would only agree on a third of the items ( 1 9 + 1 9 + 1 9 ). The second reason percentage agreement cannot be trusted is that it does not correct for the distribution of items among categories: We expect a higher percentage agreement when one category is much more common than the other. This problem, already raised by Hsu and Field (2003, page 207) among others, can be illustrated using the following example (Di Eugenio and Glass 2004, example 3, pages 98-99). Suppose 95% of utterances in a particular domain are statement, and only 5% are inforequest. We would then expect by chance that 0.95 × 0.95 = 0.9025 of the utterances would be classified as statement by both coders, and 0.05 × 0.05 = 0.0025 as inforequest, so the coders would agree on 90.5% of the utterances. Under such circumstances, a seemingly high observed agreement of 90% is actually worse than expected by chance.

The conclusion reached in the literature is that in order to get figures that are comparable across studies, observed agreement has to be adjusted for chance agreement. These are the measures we will review in the remainder of this article. We will not look at the variants of percentage agreement used in CL work on discourse before the introduction of kappa, such as percentage agreement with an expert and percentage agreement with the majority; see Carletta (1996) for discussion and criticism. 3",Why is observed agreement considered insufficient for comparing inter-coder agreement across different studies?,"Observed agreement is considered insufficient for comparing inter-coder agreement across different studies because it does not account for chance agreement. Some agreement is due to chance, and the amount of chance agreement is affected by factors that vary from one study to another. Firstly, observed agreement is biased in favor of dimensions with a small number of categories, meaning that a coding scheme with fewer categories will result in higher percentage agreement just by chance. Secondly, observed agreement does not correct for the distribution of items among categories, leading to higher percentage agreement when one category is much more common than the other. For example, if 95% of utterances in a domain are classified as one category, coders would agree on a high percentage of utterances by chance, making a seemingly high observed agreement misleading. Therefore, to obtain figures that are comparable across studies, observed agreement must be adjusted for chance agreement.",Why is observed agreement not sufficient for comparing inter-coder agreement across different studies?,"Observed agreement is not sufficient for comparing inter-coder agreement across different studies because it fails to account for chance agreement. Some level of agreement between coders can occur purely by chance, and this chance agreement is influenced by factors that vary from one study to another. Firstly, observed agreement tends to be biased towards coding schemes with fewer categories, as these naturally result in higher percentage agreement by chance. Secondly, it does not adjust for the distribution of items among categories, which can lead to misleadingly high agreement percentages when one category is overwhelmingly more common than others. For instance, if 95% of utterances in a study are classified under a single category, coders might appear to agree on a high percentage of utterances simply by chance. Therefore, to ensure comparability across studies, observed agreement must be adjusted to account for chance agreement.",9,2,9,2,"Question:

Why is observed agreement considered insufficient for comparing inter-coder agreement across different studies?

Answer:

Observed agreement is considered insufficient for comparing inter-coder agreement across different studies because it does not account for chance agreement. Some agreement is due to chance, and the amount of chance agreement is affected by factors that vary from one study to another. Firstly, observed agreement is biased in favor of dimensions with a small number of categories, meaning that a coding scheme with fewer categories will result in higher percentage agreement just by chance. Secondly, observed agreement does not correct for the distribution of items among categories, leading to higher percentage agreement when one category is much more common than the other. For example, if 95% of utterances in a domain are classified as one category, coders would agree on a high percentage of utterances by chance, making a seemingly high observed agreement misleading. Therefore, to obtain figures that are comparable across studies, observed agreement must be adjusted for chance agreement.","Question:

Why is observed agreement not sufficient for comparing inter-coder agreement across different studies?

Answer:

Observed agreement is not sufficient for comparing inter-coder agreement across different studies because it fails to account for chance agreement. Some level of agreement between coders can occur purely by chance, and this chance agreement is influenced by factors that vary from one study to another. Firstly, observed agreement tends to be biased towards coding schemes with fewer categories, as these naturally result in higher percentage agreement by chance. Secondly, it does not adjust for the distribution of items among categories, which can lead to misleadingly high agreement percentages when one category is overwhelmingly more common than others. For instance, if 95% of utterances in a study are classified under a single category, coders might appear to agree on a high percentage of utterances simply by chance. Therefore, to ensure comparability across studies, observed agreement must be adjusted to account for chance agreement.",NO,True,947,True,True
12719479-s6,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,Chance-Corrected Coefficients for Measuring Agreement between Two Coders,"All of the coefficients of agreement discussed in this article correct for chance on the basis of the same idea. First we find how much agreement is expected by chance: Let us call this value A e . The value 1 − A e will then measure how much agreement over and above chance is attainable; the value A o − A e will tell us how much agreement beyond chance was actually found. The ratio between A o − A e and 1 − A e will then tell us which proportion of the possible agreement beyond chance was actually observed. This idea is expressed by the following formula.

The three best-known coefficients, S (Bennett, Alpert, and Goldstein 1954), π (Scott 1955), and κ (Cohen 1960), and their generalizations, all use this formula; whereas Krippendorff's α is based on a related formula expressed in terms of disagreement (see Section 2.6). All three coefficients therefore yield values of agreement between −A e /1 − A e (no observed agreement) and 1 (observed agreement = 1), with the value 0 signifying chance agreement (observed agreement = expected agreement). Note also that whenever agreement is less than perfect (A o < 1), chance-corrected agreement will be strictly lower than observed agreement, because some amount of agreement is always expected by chance. Observed agreement A o is easy to compute, and is the same for all three coefficients-the proportion of items on which the two coders agree. But the notion of chance agreement, or the probability that two coders will classify an arbitrary item as belonging to the same category by chance, requires a model of what would happen if coders' behavior was only by chance. All three coefficients assume independence of the two coders-that is, that the chance of c 1 and c 2 agreeing on any given category k Table 2 The value of different coefficients applied to the data from Table 1.",How do chance-corrected coefficients measure agreement beyond chance between two coders?,"All of the coefficients of agreement discussed in this article correct for chance on the basis of the same idea. First, the expected agreement by chance is determined, denoted as A_e. The value 1 − A_e measures how much agreement over and above chance is attainable, while A_o − A_e indicates how much agreement beyond chance was actually found. The ratio between A_o − A_e and 1 − A_e reveals the proportion of the possible agreement beyond chance that was actually observed. This concept is expressed by a specific formula. The three best-known coefficients, S (Bennett, Alpert, and Goldstein 1954), π (Scott 1955), and κ (Cohen 1960), and their generalizations, all use this formula, whereas Krippendorff's α is based on a related formula expressed in terms of disagreement. All three coefficients yield values of agreement between −A_e /1 − A_e (no observed agreement) and 1 (observed agreement = 1), with the value 0 signifying chance agreement (observed agreement = expected agreement). Whenever agreement is less than perfect (A_o < 1), chance-corrected agreement will be strictly lower than observed agreement, because some amount of agreement is always expected by chance.",How do chance-corrected coefficients assess the level of agreement between two coders beyond what would be expected by chance?,"Chance-corrected coefficients of agreement are designed to measure the extent of agreement between two coders, accounting for the agreement that could occur purely by chance. The process begins by determining the expected agreement by chance, denoted as A_e. The potential agreement beyond chance is represented by the value 1 − A_e, while the actual agreement beyond chance is given by A_o − A_e. The ratio of A_o − A_e to 1 − A_e indicates the proportion of possible agreement beyond chance that was actually observed. This concept is encapsulated in a specific formula used by several well-known coefficients, including S (Bennett, Alpert, and Goldstein 1954), π (Scott 1955), and κ (Cohen 1960), along with their generalizations. Krippendorff's α, while based on a related formula, is expressed in terms of disagreement. These coefficients yield values ranging from −A_e / (1 − A_e) (indicating no observed agreement) to 1 (indicating perfect observed agreement), with a value of 0 signifying that the observed agreement equals the expected agreement by chance. When the agreement is less than perfect (A_o < 1), the chance-corrected agreement will always be lower than the observed agreement, as some level of agreement is always anticipated by chance.",9,2,9,2,"Question:

How do chance-corrected coefficients measure agreement beyond chance between two coders?

Answer:

All of the coefficients of agreement discussed in this article correct for chance on the basis of the same idea. First, the expected agreement by chance is determined, denoted as A_e. The value 1 − A_e measures how much agreement over and above chance is attainable, while A_o − A_e indicates how much agreement beyond chance was actually found. The ratio between A_o − A_e and 1 − A_e reveals the proportion of the possible agreement beyond chance that was actually observed. This concept is expressed by a specific formula. The three best-known coefficients, S (Bennett, Alpert, and Goldstein 1954), π (Scott 1955), and κ (Cohen 1960), and their generalizations, all use this formula, whereas Krippendorff's α is based on a related formula expressed in terms of disagreement. All three coefficients yield values of agreement between −A_e /1 − A_e (no observed agreement) and 1 (observed agreement = 1), with the value 0 signifying chance agreement (observed agreement = expected agreement). Whenever agreement is less than perfect (A_o < 1), chance-corrected agreement will be strictly lower than observed agreement, because some amount of agreement is always expected by chance.","Question:

How do chance-corrected coefficients assess the level of agreement between two coders beyond what would be expected by chance?

Answer:

Chance-corrected coefficients of agreement are designed to measure the extent of agreement between two coders, accounting for the agreement that could occur purely by chance. The process begins by determining the expected agreement by chance, denoted as A_e. The potential agreement beyond chance is represented by the value 1 − A_e, while the actual agreement beyond chance is given by A_o − A_e. The ratio of A_o − A_e to 1 − A_e indicates the proportion of possible agreement beyond chance that was actually observed. This concept is encapsulated in a specific formula used by several well-known coefficients, including S (Bennett, Alpert, and Goldstein 1954), π (Scott 1955), and κ (Cohen 1960), along with their generalizations. Krippendorff's α, while based on a related formula, is expressed in terms of disagreement. These coefficients yield values ranging from −A_e / (1 − A_e) (indicating no observed agreement) to 1 (indicating perfect observed agreement), with a value of 0 signifying that the observed agreement equals the expected agreement by chance. When the agreement is less than perfect (A_o < 1), the chance-corrected agreement will always be lower than the observed agreement, as some level of agreement is always anticipated by chance.",NO,True,1257,True,True
12719479-s11,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,All Categories Are Equally,"Likely: S. The simplest way of discounting for chance is the one adopted to compute the coefficient S (Bennett, Alpert, and Goldstein 1954), also known in the literature as C, κ n , G, and RE (see Zwick 1988;Hsu and Field 2003). As noted previously, the computation of S is based on an interpretation of chance as a random choice of category from a uniform distribution-that is, all categories are equally likely. If coders classify the items into k categories, then the chance P(k|c i ) of any coder assigning an item to category k under the uniformity assumption is 1 k ; hence the total agreement expected by chance is

The calculation of the value of S for the figures in Table 1 is shown in Table 2. The coefficient S is problematic in many respects. The value of the coefficient can be artificially increased simply by adding spurious categories which the coders would never use (Scott 1955, pages 322-323). In the case of CL, for example, S would reward designing extremely fine-grained tagsets, provided that most tags are never actually encountered in real data. Additional limitations are noted by Hsu and Field (2003). It has been argued that uniformity is the best model for a chance distribution of items among categories if we have no independent prior knowledge of the distribution (Brennan and Prediger 1981). However, a lack of prior knowledge does not mean that the distribution cannot be estimated post hoc, and this is what the other coefficients do.",What are the problems and limitations associated with using the coefficient S for inter-coder agreement?  ,"The coefficient S is problematic in many respects. The value of the coefficient can be artificially increased simply by adding spurious categories which the coders would never use (Scott 1955, pages 322-323). In the case of computational linguistics, for example, S would reward designing extremely fine-grained tagsets, provided that most tags are never actually encountered in real data. Additional limitations are noted by Hsu and Field (2003).",What are the issues and limitations of using the coefficient S for measuring inter-coder agreement?,"The coefficient S has several notable issues. One major problem is that its value can be artificially inflated by adding unnecessary categories that coders are unlikely to use (Scott 1955, pages 322-323). For instance, in computational linguistics, S might favor the creation of highly detailed tagsets, as long as most of these tags do not appear in actual data. Furthermore, additional limitations have been highlighted by Hsu and Field (2003).",7,2,4,2,"Question:

What are the problems and limitations associated with using the coefficient S for inter-coder agreement?  

Answer:

The coefficient S is problematic in many respects. The value of the coefficient can be artificially increased simply by adding spurious categories which the coders would never use (Scott 1955, pages 322-323). In the case of computational linguistics, for example, S would reward designing extremely fine-grained tagsets, provided that most tags are never actually encountered in real data. Additional limitations are noted by Hsu and Field (2003).","Question:

What are the issues and limitations of using the coefficient S for measuring inter-coder agreement?

Answer:

The coefficient S has several notable issues. One major problem is that its value can be artificially inflated by adding unnecessary categories that coders are unlikely to use (Scott 1955, pages 322-323). For instance, in computational linguistics, S might favor the creation of highly detailed tagsets, as long as most of these tags do not appear in actual data. Furthermore, additional limitations have been highlighted by Hsu and Field (2003).",NO,True,446,True,True
12719479-s14,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,More Than Two Coders,"In corpus annotation practice, measuring reliability with only two coders is seldom considered enough, except for small-scale studies. Sometimes researchers run reliability studies with more than two coders, measure agreement separately for each pair of coders, and report the average. However, a better practice is to use generalized versions of the coefficients. A generalization of Scott's π is proposed in Fleiss (1971), and a generalization of Cohen's κ is given in Davies and Fleiss (1982). We will call these coefficients multi-π and multi-κ, respectively, dropping the multi-prefixes when no confusion is expected to arise. 5 2.5.1 Fleiss's Multi-π. With more than two coders, the observed agreement A o can no longer be defined as the percentage of items on which there is agreement, because inevitably there will be items on which some coders agree and others disagree. The solution proposed in the literature is to measure pairwise agreement (Fleiss 1971): Define the amount of agreement on a particular item as the proportion of agreeing judgment pairs out of the total number of judgment pairs for that item.

Multiple coders also pose a problem for the visualization of the data. When the number of coders c is greater than two, judgments cannot be shown in a contingency table like Table 1, because each coder has to be represented in a separate dimension. 5 Due to historical accident, the terminology in the literature is confusing. Fleiss (1971) proposed a coefficient of agreement for multiple coders and called it κ, even though it calculates expected agreement based on the cumulative distribution of judgments by all coders and is thus better thought of as a generalization of Scott's π. This unfortunate choice of name was the cause of much confusion in subsequent literature: Often, studies which claim to give a generalization of κ to more than two coders actually report Fleiss's coefficient (e.g., Bartko and Carpenter 1976;Siegel and Castellan 1988;Di Eugenio and Glass 2004). Since Carletta (1996) introduced reliability to the CL community based on the definitions of Siegel and Castellan (1988), the term ""kappa"" has been usually associated in this community with Siegel and Castellan's K, which is in effect Fleiss's coefficient, that is, a generalization of Scott's π. Fleiss (1971) Table 3 lose information because they do not say which coder gave each judgment. This information is not used in the calculation of π, but is necessary for determining the individual coders' distributions in the calculation of κ. (Agreement tables also add information compared to contingency tables, namely, the identity of the items that make up each contingency class, but this information is not used in the calculation of either κ or π.) Let n ik stand for the number of times an item i is classified in category k (i.e., the number of coders that make such a judgment): For example, given the distribution in Table 3, n Utt 1 Stat = 2 and n Utt 1 IReq = 1. Each category k contributes ( n ik 2 ) pairs of agreeing judgments for item i; the amount of agreement agr i for item i is the sum of ( n ik 2 ) over all categories k ∈ K, divided by ( c 2 ), the total number of judgment pairs per item.

For example, given the results in Table 3, we find the agreement value for Utterance 1 as follows.

The overall observed agreement is the mean of agr i for all items i ∈ I.

(Notice that this definition of observed agreement is equivalent to the mean of the two-coder observed agreement values from Section 2.4 for all coder pairs.) If observed agreement is measured on the basis of pairwise agreement (the proportion of agreeing judgment pairs), it makes sense to measure expected agreement in terms of pairwise comparisons as well, that is, as the probability that any pair of judgments for an item would be in agreement-or, said otherwise, the probability that two arbitrary coders would make the same judgment for a particular item by chance. This is the approach taken by Fleiss (1971). Like Scott, Fleiss interprets ""chance agreement"" as the agreement expected on the basis of a single distribution which reflects the combined judgments of all coders, meaning that expected agreement is calculated usingP(k), the overall proportion of items assigned to category k, which is the total number of such assignments by all coders n k divided by the overall number of assignments. The latter, in turn, is the number of items i multiplied by the number of coders c.

As in the two-coder case, the probability that two arbitrary coders assign an item to a particular category k ∈ K is assumed to be the joint probability of each coder making this assignment independently, that is (P(k)) 2 . The expected agreement is the sum of this joint probability over all the categories k ∈ K.

Multi-π is the coefficient that Siegel and Castellan (1988) call K.",How is the expected agreement calculated for multiple coders using the concept of pairwise comparisons?,"The expected agreement for multiple coders is calculated using the concept of pairwise comparisons by measuring the probability that any pair of judgments for an item would be in agreement. This approach assumes that two arbitrary coders would make the same judgment for a particular item by chance. Fleiss (1971) interprets ""chance agreement"" as the agreement expected on the basis of a single distribution, which reflects the combined judgments of all coders. Expected agreement is calculated using P(k), the overall proportion of items assigned to category k, which is the total number of such assignments by all coders n_k divided by the overall number of assignments. The probability that two arbitrary coders assign an item to a particular category k is assumed to be the joint probability of each coder making this assignment independently, that is (P(k))^2. The expected agreement is the sum of this joint probability over all the categories k ∈ K.",How do you calculate the expected agreement for multiple coders using pairwise comparisons?,"To calculate the expected agreement for multiple coders using pairwise comparisons, we assess the probability that any pair of judgments for an item will agree. This method assumes that two random coders would make the same judgment for a particular item purely by chance. According to Fleiss (1971), ""chance agreement"" is interpreted as the agreement expected based on a single distribution, which represents the combined judgments of all coders. The expected agreement is determined using P(k), which is the overall proportion of items assigned to category k. This is calculated by dividing the total number of such assignments by all coders, denoted as n_k, by the overall number of assignments. The probability that two random coders assign an item to a specific category k is considered the joint probability of each coder independently making this assignment, expressed as (P(k))^2. The expected agreement is then the sum of these joint probabilities across all categories k ∈ K.",9,2,7,2,"Question:

How is the expected agreement calculated for multiple coders using the concept of pairwise comparisons?

Answer:

The expected agreement for multiple coders is calculated using the concept of pairwise comparisons by measuring the probability that any pair of judgments for an item would be in agreement. This approach assumes that two arbitrary coders would make the same judgment for a particular item by chance. Fleiss (1971) interprets ""chance agreement"" as the agreement expected on the basis of a single distribution, which reflects the combined judgments of all coders. Expected agreement is calculated using P(k), the overall proportion of items assigned to category k, which is the total number of such assignments by all coders n_k divided by the overall number of assignments. The probability that two arbitrary coders assign an item to a particular category k is assumed to be the joint probability of each coder making this assignment independently, that is (P(k))^2. The expected agreement is the sum of this joint probability over all the categories k ∈ K.","Question:

How do you calculate the expected agreement for multiple coders using pairwise comparisons?

Answer:

To calculate the expected agreement for multiple coders using pairwise comparisons, we assess the probability that any pair of judgments for an item will agree. This method assumes that two random coders would make the same judgment for a particular item purely by chance. According to Fleiss (1971), ""chance agreement"" is interpreted as the agreement expected based on a single distribution, which represents the combined judgments of all coders. The expected agreement is determined using P(k), which is the overall proportion of items assigned to category k. This is calculated by dividing the total number of such assignments by all coders, denoted as n_k, by the overall number of assignments. The probability that two random coders assign an item to a specific category k is considered the joint probability of each coder independently making this assignment, expressed as (P(k))^2. The expected agreement is then the sum of these joint probabilities across all categories k ∈ K.",NO,True,985,True,True
12719479-s16,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,Krippendorff's α and Other Weighted Agreement Coefficients,"A serious limitation of both π and κ is that all disagreements are treated equally. But especially for semantic and pragmatic features, disagreements are not all alike. Even for the relatively simple case of dialogue act tagging, a disagreement between an accept and a reject interpretation of an utterance is clearly more serious than a disagreement between an info-request and a check. For tasks such as anaphora resolution, where reliability is determined by measuring agreement on sets (coreference chains), allowing for degrees of disagreement becomes essential (see Section 4.4). Under such circumstances, π and κ are not very useful. In this section we discuss two coefficients that make it possible to differentiate between types of disagreements: α (Krippendorff 1980(Krippendorff , 2004a, which is a coefficient defined in a general way that is appropriate for use with multiple coders, different magnitudes of disagreement, and missing values, and is based on assumptions similar to those of π; and weighted kappa κ w (Cohen 1968), a generalization of κ.

2.6.1 Krippendorff's α. The coefficient α (Krippendorff 1980(Krippendorff , 2004a is an extremely versatile agreement coefficient based on assumptions similar to π, namely, that expected agreement is calculated by looking at the overall distribution of judgments without regard to which coders produced these judgments. It applies to multiple coders, and it allows for different magnitudes of disagreement. When all disagreements are considered equal it is nearly identical to multi-π, correcting for small sample sizes by using an unbiased estimator for expected agreement. In this section we will present Krippendorff's α and relate it to the other coefficients discussed in this article, but we will start with α's origins as a measure of variance, following a long tradition of using variance to measure reliability (see citations in Rajaratnam 1960;Krippendorff 1970).

A sample's variance s 2 is defined as the sum of square differences from the mean SS = ∑(x −x) 2 divided by the degrees of freedom df . Variance is a useful way of looking at agreement if coders assign numerical values to the items, as in magnitude estimation tasks. Each item in a reliability study can be considered a separate level in a single-factor analysis of variance: The smaller the variance around each level, the higher the reliability. When agreement is perfect, the variance within the levels (s 2 within ) is zero; when agreement is at chance, the variance within the levels is equal to the variance between the levels, in which case it is also equal to the overall variance of the data: s 2 within = s 2 between = s 2 total . The ratios s 2 within /s 2 between (that is, 1/F) and s 2 within /s 2 total are therefore 0 when agreement is perfect and 1 when agreement is at chance. Additionally, the latter ratio is bounded at 2: SS within ≤ SS total by definition, and df total < 2 df within because each item has at least two judgments. Subtracting the ratio s 2 within /s 2 total from 1 yields a coefficient which ranges between −1 and 1, where 1 signifies perfect agreement and 0 signifies chance agreement.

We can unpack the formula for α to bring it to a form which is similar to the other coefficients we have looked at, and which will allow generalizing α beyond simple numerical values. The first step is to get rid of the notion of arithmetic mean which lies at the heart of the measure of variance. We observe that for any set of numbers x 1 , . . . , x N with a meanx = 1 N ∑ N n=1 x n , the sum of square differences from the mean SS can be expressed as the sum of square of differences between all the (ordered) pairs of numbers, scaled by a factor of 1/2N.

For calculating α we considered each item to be a separate level in an analysis of variance; the number of levels is thus the number of items i, and because each coder marks each item, the number of observations for each item is the number of coders c.

Within-level variance is the sum of the square differences from the mean of each item,

. We can express this as the sum of the squares of the differences between all of the judgment pairs for each item, summed over all items and scaled by the appropriate factor. We use the notation x ic for the value given by coder c to item i, andx i for the mean of all the values given to item i.

The total variance is the sum of the square differences of all judgments from the grand mean, SS total = ∑ i ∑ c (x ic −x) 2 , divided by the degrees of freedom df total = ic − 1. This can be expressed as the sum of the squares of the differences between all of the judgments pairs without regard to items, again scaled by the appropriate factor. The notation x is the overall mean of all the judgments in the data.

Now that we have removed references to means from our formulas, we can abstract over the measure of variance. We define a distance function d which takes two numbers and returns the square of their difference.

We also simplify the computation by counting all the identical value assignments together. Each unique value used by the coders will be considered a category k ∈ K. We use n ik for the number of times item i is given the value k, that is, the number of coders that make such a judgment. For every (ordered) pair of distinct values k a , k b ∈ K there are n ik a n ik b pairs of judgments of item i, whereas for non-distinct values there are n ik a (n ik a − 1) pairs. We use this notation to rewrite the formula for the within-level variance. D α o , the observed disagreement for α, is defined as twice the variance within the levels in order to get rid of the factor 2 in the denominator; we also simplify the formula by using the multiplier n ik a n ik a for identical categories-this is allowed because

We perform the same simplification for the total variance, where n k stands for the total number of times the value k is assigned to any item by any coder. The expected disagreement for α, D α e , is twice the total variance.

Because both expected and observed disagreement are twice the respective variances, the coefficient α retains the same form when expressed with the disagreement values.

Now that α has been expressed without explicit reference to means, differences, and squares, it can be generalized to a variety of coding schemes in which the labels cannot be interpreted as numerical values: All one has to do is to replace the square difference function d with a different distance function. Krippendorff (1980Krippendorff ( , 2004a offers distance metrics suitable for nominal, interval, ordinal, and ratio scales. Of particular interest is the function for nominal categories, that is, a function which considers all distinct labels equally distant from one another.

It turns out that with this distance function, the observed disagreement D α o is exactly the complement of the observed agreement of Fleiss's multi-π, 1 − A π o , and the expected disagreement D α e differs from 1 − A π e by a factor of (ic − 1)/ic; the difference is due to the fact that π uses a biased estimator of the expected agreement in the population whereas α uses an unbiased estimator. The following equation shows that given the correspondence between observed and expected agreement and disagreement, the coefficients themselves are nearly equivalent.

For nominal data, the coefficients π and α approach each other as either the number of items or the number of coders approaches infinity. Krippendorff's α will work with any distance metric, provided that identical categories always have a distance of zero (d kk = 0 for all k). Another useful constraint is symmetry (d ab = d ba for all a, b). This flexibility affords new possibilities for analysis, which we will illustrate in Section 4. We should also note, however, that the flexibility also creates new pitfalls, especially in cases where it is not clear what the natural distance metric is. For example, there are different ways to measure dissimilarity between sets, and any of these measures can be justifiably used when the category labels are sets of items (as in the annotation of anaphoric relations). The different distance metrics yield different values of α for the same annotation data, making it difficult to interpret the resulting values. We will return to this problem in Section 4.4. Cohen (1968). The implementation of weights is similar to that of Krippendorff's α-each pair of categories k a , k b ∈ K is associated with a weight d k a k b , where a larger weight indicates more disagreement (Cohen uses the notation v; he does not place any general constraints on the weights-not even a requirement that a pair of identical categories have a weight of zero, or that the weights be symmetric across the diagonal). The coefficient is defined for two coders: The disagreement for a particular item i is the weight of the pair of categories assigned to it by the two coders, and the overall observed disagreement is the (normalized) mean disagreement of all the items. Let k(c n , i) denote the category assigned by coder c n to item i; then the disagreement for item i is disagr i = d k(c 1 ,i)k(c 2 ,i) . The observed disagreement D o is the mean of disagr i for all items i, normalized to the interval [0, 1] through division by the maximal weight d max .",How does Krippendorff's α accommodate different types of disagreements in computational linguistics?,"Krippendorff's α is a versatile agreement coefficient that allows for different magnitudes of disagreement. It is based on assumptions similar to π, where expected agreement is calculated by looking at the overall distribution of judgments without regard to which coders produced these judgments. Krippendorff's α applies to multiple coders and can accommodate missing values. It allows for differentiating between types of disagreements by using a distance function that takes two numbers and returns the square of their difference. This flexibility enables Krippendorff's α to be generalized to a variety of coding schemes in which the labels cannot be interpreted as numerical values. Krippendorff offers distance metrics suitable for nominal, interval, ordinal, and ratio scales. For nominal categories, the function considers all distinct labels equally distant from one another. The observed disagreement for α is defined as twice the variance within the levels, and the expected disagreement is twice the total variance. This formulation allows Krippendorff's α to retain the same form when expressed with the disagreement values. The flexibility of Krippendorff's α affords new possibilities for analysis, but it also creates new pitfalls, especially in cases where it is not clear what the natural distance metric is.",How does Krippendorff's α handle different types of disagreements in computational linguistics?,"Krippendorff's α is a versatile agreement coefficient designed to handle various magnitudes of disagreement in computational linguistics. It operates on principles similar to those of π, where expected agreement is determined by examining the overall distribution of judgments, independent of which coders made these judgments. This coefficient is applicable to multiple coders and can accommodate missing values, making it highly adaptable.

One of the key features of Krippendorff's α is its ability to differentiate between types of disagreements through a distance function. This function takes two numbers and returns the square of their difference, allowing the coefficient to be generalized across different coding schemes where labels may not be numerical. Krippendorff provides distance metrics suitable for nominal, interval, ordinal, and ratio scales. For nominal categories, for instance, the function treats all distinct labels as equally distant from one another.

The observed disagreement in Krippendorff's α is defined as twice the variance within the levels, while the expected disagreement is twice the total variance. This formulation ensures that Krippendorff's α maintains a consistent form when expressed with disagreement values. While this flexibility opens up new possibilities for analysis, it also introduces potential pitfalls, particularly in situations where the natural distance metric is not clearly defined.",7,2,7,2,"Question:

How does Krippendorff's α accommodate different types of disagreements in computational linguistics?

Answer:

Krippendorff's α is a versatile agreement coefficient that allows for different magnitudes of disagreement. It is based on assumptions similar to π, where expected agreement is calculated by looking at the overall distribution of judgments without regard to which coders produced these judgments. Krippendorff's α applies to multiple coders and can accommodate missing values. It allows for differentiating between types of disagreements by using a distance function that takes two numbers and returns the square of their difference. This flexibility enables Krippendorff's α to be generalized to a variety of coding schemes in which the labels cannot be interpreted as numerical values. Krippendorff offers distance metrics suitable for nominal, interval, ordinal, and ratio scales. For nominal categories, the function considers all distinct labels equally distant from one another. The observed disagreement for α is defined as twice the variance within the levels, and the expected disagreement is twice the total variance. This formulation allows Krippendorff's α to retain the same form when expressed with the disagreement values. The flexibility of Krippendorff's α affords new possibilities for analysis, but it also creates new pitfalls, especially in cases where it is not clear what the natural distance metric is.","Question:

How does Krippendorff's α handle different types of disagreements in computational linguistics?

Answer:

Krippendorff's α is a versatile agreement coefficient designed to handle various magnitudes of disagreement in computational linguistics. It operates on principles similar to those of π, where expected agreement is determined by examining the overall distribution of judgments, independent of which coders made these judgments. This coefficient is applicable to multiple coders and can accommodate missing values, making it highly adaptable.

One of the key features of Krippendorff's α is its ability to differentiate between types of disagreements through a distance function. This function takes two numbers and returns the square of their difference, allowing the coefficient to be generalized across different coding schemes where labels may not be numerical. Krippendorff provides distance metrics suitable for nominal, interval, ordinal, and ratio scales. For nominal categories, for instance, the function treats all distinct labels as equally distant from one another.

The observed disagreement in Krippendorff's α is defined as twice the variance within the levels, while the expected disagreement is twice the total variance. This formulation ensures that Krippendorff's α maintains a consistent form when expressed with disagreement values. While this flexibility opens up new possibilities for analysis, it also introduces potential pitfalls, particularly in situations where the natural distance metric is not clearly defined.",NO,True,1441,True,True
12719479-s23,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,Annotator Bias,"The difference between π and α on the one hand and κ on the other hand lies in the interpretation of the notion of chance agreement, whether it is the amount expected from the the actual distribution of items among categories (π) or from individual coder priors (κ). As mentioned in Section 2.4, this difference has been the subject of much debate (Fleiss 1975;Krippendorff 1978Krippendorff , 2004bByrt, Bishop, and Carlin 1993;Zwick 1988;Hsu and Field 2003;Di Eugenio and Glass 2004;Craggs and McGee Wood 2005). A claim often repeated in the literature is that single-distribution coefficients like π and α assume that different coders produce similar distributions of items among categories, with the implication that these coefficients are inapplicable when the annotators show substantially different distributions. Recommendations vary: Zwick (1988) suggests testing the individual coders' distributions using the modified χ 2 test of Stuart (1955), and discarding the annotation as unreliable if significant systematic discrepancies are observed. In contrast, Hsu and Field (2003, page 214) recommend reporting the value of κ even when the coders produce different distributions, because it is ""the only [index] . . . that could legitimately be applied in the presence of marginal heterogeneity""; likewise, Di Eugenio and Glass (2004, page 96) recommend using κ in ""the vast majority . . . of discourse-and dialogue-tagging efforts"" where the individual coders' distributions tend to vary. All of these proposals are based on a misconception: that single-distribution coefficients require similar distributions by the individual annotators in order to work properly. This is not the case. The difference between the coefficients is only in the interpretation of ""chance agreement"": π-style coefficients calculate the chance of agreement among arbitrary coders, whereas κ-style coefficients calculate the chance of agreement among the coders who produced the reliability data. Therefore, the choice of coefficient should not depend on the magnitude of the divergence between the coders, but rather on the desired interpretation of chance agreement.

Another common claim is that individual-distribution coefficients like κ ""reward"" annotators for disagreeing on the marginal distributions. For example, Di Eugenio and Glass (2004, page 99) say that κ suffers from what they call the bias problem, described as ""the paradox that κ Co [our κ] increases as the coders become less similar."" Similar reservations about the use of κ have been noted by Brennan and Prediger (1981) and Zwick (1988). However, the bias problem is less paradoxical than it sounds. Although it is true that for a fixed observed agreement, a higher difference in coder marginals implies a lower expected agreement and therefore a higher κ value, the conclusion that κ penalizes coders for having similar distributions is unwarranted. This is because A o and A e are not independent: Both are drawn from the same set of observations. What κ does is discount some of the disagreement resulting from different coder marginals by incorporating it into A e . Whether this is desirable depends on the application for which the coefficient is used.

The most common application of agreement measures in CL is to infer the reliability of a large-scale annotation, where typically each piece of data will be marked by just one coder, by measuring agreement on a small subset of the data which is annotated by multiple coders. In order to make this generalization, the measure must reflect the reliability of the annotation procedure, which is independent of the actual annotators used. Reliability, or reproducibility of the coding, is reduced by all disagreements-both random and systematic. The most appropriate measures of reliability for this purpose are therefore single-distribution coefficients like π and α, which generalize over the individual coders and exclude marginal disagreements from the expected agreement. This argument has been presented recently in much detail by Krippendorff (2004b) and reiterated by Craggs and McGee Wood (2005).

At the same time, individual-distribution coefficients like κ provide important information regarding the trustworthiness (validity) of the data on which the annotators agree. As an intuitive example, think of a person who consults two analysts when deciding whether to buy or sell certain stocks. If one analyst is an optimist and tends to recommend buying whereas the other is a pessimist and tends to recommend selling, they are likely to agree with each other less than two more neutral analysts, so overall their recommendations are likely to be less reliable-less reproducible-than those that come from a population of like-minded analysts. This reproducibility is measured by π. But whenever the optimistic and pessimistic analysts agree on a recommendation for a particular stock, whether it is ""buy"" or ""sell,"" the confidence that this is indeed the right decision is higher than the same advice from two like-minded analysts. This is why κ ""rewards"" biased annotators: it is not a matter of reproducibility (reliability) but rather of trustworthiness (validity).

Having said this, we should point out that, first, in practice the difference between π and κ doesn't often amount to much (see discussion in Section 4). Moreover, the difference becomes smaller as agreement increases, because all the points of agreement contribute toward making the coder marginals similar (it took a lot of experimentation to create data for Table 4 so that the values of π and κ would straddle the conventional cutoff point of 0.80, and even so the difference is very small). Finally, one would expect the difference between π and κ to diminish as the number of coders grows; this is shown subsequently. 6 We define B, the overall annotator bias in a particular set of coding data, as the difference between the expected agreement according to (multi)-π and the expected agreement according to (multi)-κ. Annotator bias is a measure of variance: If we take c to be a random variable with equal probabilities for all coders, then the annotator bias B is the sum of the variances of P(k|c) for all categories k ∈ K, divided by the number of coders c less one (see Artstein and Poesio [2005] for a proof).

Annotator bias can be used to express the difference between κ and π.

This allows us to make the following observations about the relationship between π and κ.

Observation 1. The difference between κ and π grows as the annotator bias grows: For a constant A o and A π e , a greater B implies a greater value for κ − π.

Observation 2. The greater the number of coders, the lower the annotator bias B, and hence the lower the difference between κ and π, because the variance ofP(k|c) does not increase in proportion to the number of coders.

In other words, provided enough coders are used, it should not matter whether a single-distribution or individual-distribution coefficient is used. This is not to imply that multiple coders increase reliability: The variance of the individual coders' distributions can be just as large with many coders as with few coders, but its effect on the value of κ decreases as the number of coders grows, and becomes more similar to random noise. The same holds for weighted measures too; see the extended version of this article for definitions and proof. In an annotation study with 18 subjects, we compared α with a variant which uses individual coder distributions to calculate expected agreement, and found that the values never differed beyond the third decimal point (Poesio and Artstein 2005).

We conclude with a summary of our views concerning the difference between πstyle and κ-style coefficients. First of all, keep in mind that empirically the difference is small, and gets smaller as the number of annotators increases. Then instead of reporting two coefficients, as suggested by Di Eugenio and Glass (2004), the appropriate coefficient should be chosen based on the task (not on the observed differences between coder marginals). When the coefficient is used to assess reliability, a single-distribution coefficient like π or α should be used; this is indeed already the practice in CL, because Siegel and Castellan's K is identical with (multi-)π. It is also good practice to test reliability with more than two coders, in order to reduce the likelihood of coders sharing a deviant reading of the annotation guidelines.",How does annotator bias influence the interpretation of agreement coefficients like π and κ in computational linguistics?,"The difference between π and α on the one hand and κ on the other hand lies in the interpretation of the notion of chance agreement, whether it is the amount expected from the actual distribution of items among categories (π) or from individual coder priors (κ). A claim often repeated in the literature is that single-distribution coefficients like π and α assume that different coders produce similar distributions of items among categories, with the implication that these coefficients are inapplicable when the annotators show substantially different distributions. This is not the case. The difference between the coefficients is only in the interpretation of ""chance agreement"": π-style coefficients calculate the chance of agreement among arbitrary coders, whereas κ-style coefficients calculate the chance of agreement among the coders who produced the reliability data. Therefore, the choice of coefficient should not depend on the magnitude of the divergence between the coders, but rather on the desired interpretation of chance agreement. Another common claim is that individual-distribution coefficients like κ ""reward"" annotators for disagreeing on the marginal distributions. Although it is true that for a fixed observed agreement, a higher difference in coder marginals implies a lower expected agreement and therefore a higher κ value, the conclusion that κ penalizes coders for having similar distributions is unwarranted. What κ does is discount some of the disagreement resulting from different coder marginals by incorporating it into A e. Whether this is desirable depends on the application for which the coefficient is used. The most appropriate measures of reliability for this purpose are therefore single-distribution coefficients like π and α, which generalize over the individual coders and exclude marginal disagreements from the expected agreement. At the same time, individual-distribution coefficients like κ provide important information regarding the trustworthiness (validity) of the data on which the annotators agree. This is why κ ""rewards"" biased annotators: it is not a matter of reproducibility (reliability) but rather of trustworthiness (validity). The difference between π and κ grows as the annotator bias grows: For a constant A o and A π e, a greater B implies a greater value for κ − π. The greater the number of coders, the lower the annotator bias B, and hence the lower the difference between κ and π, because the variance of P(k|c) does not increase in proportion to the number of coders. In other words, provided enough coders are used, it should not matter whether a single-distribution or individual-distribution coefficient is used.",How does annotator bias affect the interpretation of agreement coefficients like π and κ in computational linguistics?,"The influence of annotator bias on agreement coefficients such as π and κ primarily revolves around the interpretation of ""chance agreement."" The key distinction between these coefficients lies in how they define this notion. π and α coefficients consider chance agreement as the amount expected from the actual distribution of items among categories, while κ considers it based on individual coder priors. A common misconception is that single-distribution coefficients like π and α assume that different coders produce similar distributions of items among categories, rendering them inapplicable when annotators exhibit significantly different distributions. However, this is not accurate. The real difference between these coefficients is in their interpretation of chance agreement: π-style coefficients calculate the chance of agreement among arbitrary coders, whereas κ-style coefficients focus on the coders who produced the reliability data.

The choice of coefficient should not be based on the degree of divergence between coders but rather on the desired interpretation of chance agreement. Another frequent claim is that individual-distribution coefficients like κ ""reward"" annotators for disagreeing on the marginal distributions. While it is true that for a fixed observed agreement, a higher difference in coder marginals results in a lower expected agreement and thus a higher κ value, the conclusion that κ penalizes coders for having similar distributions is unfounded. Instead, κ discounts some of the disagreement arising from different coder marginals by incorporating it into the expected agreement (A e). Whether this is desirable depends on the application for which the coefficient is used.

Single-distribution coefficients like π and α are most appropriate for measuring reliability, as they generalize over individual coders and exclude marginal disagreements from the expected agreement. Conversely, individual-distribution coefficients like κ provide valuable insights into the trustworthiness (validity) of the data on which annotators agree. This is why κ appears to ""reward"" biased annotators: it is not about reproducibility (reliability) but rather about trustworthiness (validity). The difference between π and κ increases with annotator bias: for a constant observed agreement (A o) and expected agreement (A π e), a greater bias implies a greater value for κ − π. As the number of coders increases, annotator bias decreases, and consequently, the difference between κ and π diminishes, because the variance of P(k|c) does not increase proportionally with the number of coders. Therefore, with enough coders, it should not matter whether a single-distribution or individual-distribution coefficient is used.",7,4,8,2,"Question:

How does annotator bias influence the interpretation of agreement coefficients like π and κ in computational linguistics?

Answer:

The difference between π and α on the one hand and κ on the other hand lies in the interpretation of the notion of chance agreement, whether it is the amount expected from the actual distribution of items among categories (π) or from individual coder priors (κ). A claim often repeated in the literature is that single-distribution coefficients like π and α assume that different coders produce similar distributions of items among categories, with the implication that these coefficients are inapplicable when the annotators show substantially different distributions. This is not the case. The difference between the coefficients is only in the interpretation of ""chance agreement"": π-style coefficients calculate the chance of agreement among arbitrary coders, whereas κ-style coefficients calculate the chance of agreement among the coders who produced the reliability data. Therefore, the choice of coefficient should not depend on the magnitude of the divergence between the coders, but rather on the desired interpretation of chance agreement. Another common claim is that individual-distribution coefficients like κ ""reward"" annotators for disagreeing on the marginal distributions. Although it is true that for a fixed observed agreement, a higher difference in coder marginals implies a lower expected agreement and therefore a higher κ value, the conclusion that κ penalizes coders for having similar distributions is unwarranted. What κ does is discount some of the disagreement resulting from different coder marginals by incorporating it into A e. Whether this is desirable depends on the application for which the coefficient is used. The most appropriate measures of reliability for this purpose are therefore single-distribution coefficients like π and α, which generalize over the individual coders and exclude marginal disagreements from the expected agreement. At the same time, individual-distribution coefficients like κ provide important information regarding the trustworthiness (validity) of the data on which the annotators agree. This is why κ ""rewards"" biased annotators: it is not a matter of reproducibility (reliability) but rather of trustworthiness (validity). The difference between π and κ grows as the annotator bias grows: For a constant A o and A π e, a greater B implies a greater value for κ − π. The greater the number of coders, the lower the annotator bias B, and hence the lower the difference between κ and π, because the variance of P(k|c) does not increase in proportion to the number of coders. In other words, provided enough coders are used, it should not matter whether a single-distribution or individual-distribution coefficient is used.","Question:

How does annotator bias affect the interpretation of agreement coefficients like π and κ in computational linguistics?

Answer:

The influence of annotator bias on agreement coefficients such as π and κ primarily revolves around the interpretation of ""chance agreement."" The key distinction between these coefficients lies in how they define this notion. π and α coefficients consider chance agreement as the amount expected from the actual distribution of items among categories, while κ considers it based on individual coder priors. A common misconception is that single-distribution coefficients like π and α assume that different coders produce similar distributions of items among categories, rendering them inapplicable when annotators exhibit significantly different distributions. However, this is not accurate. The real difference between these coefficients is in their interpretation of chance agreement: π-style coefficients calculate the chance of agreement among arbitrary coders, whereas κ-style coefficients focus on the coders who produced the reliability data.

The choice of coefficient should not be based on the degree of divergence between coders but rather on the desired interpretation of chance agreement. Another frequent claim is that individual-distribution coefficients like κ ""reward"" annotators for disagreeing on the marginal distributions. While it is true that for a fixed observed agreement, a higher difference in coder marginals results in a lower expected agreement and thus a higher κ value, the conclusion that κ penalizes coders for having similar distributions is unfounded. Instead, κ discounts some of the disagreement arising from different coder marginals by incorporating it into the expected agreement (A e). Whether this is desirable depends on the application for which the coefficient is used.

Single-distribution coefficients like π and α are most appropriate for measuring reliability, as they generalize over individual coders and exclude marginal disagreements from the expected agreement. Conversely, individual-distribution coefficients like κ provide valuable insights into the trustworthiness (validity) of the data on which annotators agree. This is why κ appears to ""reward"" biased annotators: it is not about reproducibility (reliability) but rather about trustworthiness (validity). The difference between π and κ increases with annotator bias: for a constant observed agreement (A o) and expected agreement (A π e), a greater bias implies a greater value for κ − π. As the number of coders increases, annotator bias decreases, and consequently, the difference between κ and π diminishes, because the variance of P(k|c) does not increase proportionally with the number of coders. Therefore, with enough coders, it should not matter whether a single-distribution or individual-distribution coefficient is used.",NO,True,2744,True,True
12719479-s28,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,Establishing Significance.,"In hypothesis testing, it is common to test for the significance of a result against a null hypothesis of chance behavior; for an agreement coefficient this would mean rejecting the possibility that a positive value of agreement is nevertheless due to random coding. We can rely on the statement by Siegel and Castellan (1988, Section 9.8.2) that when sample sizes are large, the sampling distribution of K (Fleiss's multi-π) is approximately normal and centered around zero-this allows testing the obtained value of K against the null hypothesis of chance agreement by using the z statistic. It is also easy to test Krippendorff's α with the interval distance metric against the null hypothesis of chance agreement, because the hypothesis α = 0 is identical to the hypothesis F = 1 in an analysis of variance.

However, a null hypothesis of chance agreement is not very interesting, and demonstrating that agreement is significantly better than chance is not enough to establish reliability. This has already been pointed out by Cohen (1960, page 44): ""to know merely that κ is beyond chance is trivial since one usually expects much more than this in the way of reliability in psychological measurement."" The same point has been repeated and stressed in many subsequent works (e.g., Posner et al. 1990;Di Eugenio 2000;Krippendorff 2004a): The reason for measuring reliability is not to test whether coders perform better than chance, but to ensure that the coders do not deviate too much from perfect agreement (Krippendorff 2004a, page 237).

The relevant notion of significance for agreement coefficients is therefore a confidence interval. Cohen (1960, pages 43-44) implies that when sample sizes are large, the sampling distribution of κ is approximately normal for any true population value of κ, and therefore confidence intervals for the observed value of κ can be determined using the usual multiples of the standard error. Donner and Eliasziw (1987) propose a more general form of significance test for arbitrary levels of agreement. In contrast, Krippendorff (2004a, Section 11.4.2) states that the distribution of α is unknown, so confidence intervals must be obtained by bootstrapping; a software package for doing this is described in Hayes and Krippendorff (2007).",How is the significance of agreement coefficients established beyond testing against chance agreement?,"In hypothesis testing, it is common to test for the significance of a result against a null hypothesis of chance behavior; for an agreement coefficient, this would mean rejecting the possibility that a positive value of agreement is nevertheless due to random coding. However, a null hypothesis of chance agreement is not very interesting, and demonstrating that agreement is significantly better than chance is not enough to establish reliability. The reason for measuring reliability is not to test whether coders perform better than chance, but to ensure that the coders do not deviate too much from perfect agreement (Krippendorff 2004a, page 237). The relevant notion of significance for agreement coefficients is therefore a confidence interval. Cohen (1960, pages 43-44) implies that when sample sizes are large, the sampling distribution of κ is approximately normal for any true population value of κ, and therefore confidence intervals for the observed value of κ can be determined using the usual multiples of the standard error. Donner and Eliasziw (1987) propose a more general form of significance test for arbitrary levels of agreement. In contrast, Krippendorff (2004a, Section 11.4.2) states that the distribution of α is unknown, so confidence intervals must be obtained by bootstrapping; a software package for doing this is described in Hayes and Krippendorff (2007).",How is the significance of agreement coefficients established beyond testing against chance agreement?,"In hypothesis testing, the significance of a result is often evaluated against a null hypothesis of chance behavior. For agreement coefficients, this involves rejecting the possibility that a positive value of agreement is merely due to random coding. However, testing against a null hypothesis of chance agreement is not particularly insightful, as demonstrating that agreement is significantly better than chance does not necessarily establish reliability. The primary goal of measuring reliability is to ensure that coders do not deviate significantly from perfect agreement (Krippendorff 2004a, page 237).

The relevant notion of significance for agreement coefficients is therefore a confidence interval. Cohen (1960, pages 43-44) suggests that when sample sizes are large, the sampling distribution of κ (kappa) is approximately normal for any true population value of κ. Consequently, confidence intervals for the observed value of κ can be determined using the usual multiples of the standard error. Donner and Eliasziw (1987) propose a more general form of significance test for arbitrary levels of agreement.

In contrast, Krippendorff (2004a, Section 11.4.2) notes that the distribution of α (alpha) is unknown, necessitating the use of bootstrapping to obtain confidence intervals. A software package for this purpose is described in Hayes and Krippendorff (2007).",7,4,7,4,"Question:

How is the significance of agreement coefficients established beyond testing against chance agreement?

Answer:

In hypothesis testing, it is common to test for the significance of a result against a null hypothesis of chance behavior; for an agreement coefficient, this would mean rejecting the possibility that a positive value of agreement is nevertheless due to random coding. However, a null hypothesis of chance agreement is not very interesting, and demonstrating that agreement is significantly better than chance is not enough to establish reliability. The reason for measuring reliability is not to test whether coders perform better than chance, but to ensure that the coders do not deviate too much from perfect agreement (Krippendorff 2004a, page 237). The relevant notion of significance for agreement coefficients is therefore a confidence interval. Cohen (1960, pages 43-44) implies that when sample sizes are large, the sampling distribution of κ is approximately normal for any true population value of κ, and therefore confidence intervals for the observed value of κ can be determined using the usual multiples of the standard error. Donner and Eliasziw (1987) propose a more general form of significance test for arbitrary levels of agreement. In contrast, Krippendorff (2004a, Section 11.4.2) states that the distribution of α is unknown, so confidence intervals must be obtained by bootstrapping; a software package for doing this is described in Hayes and Krippendorff (2007).","Question:

How is the significance of agreement coefficients established beyond testing against chance agreement?

Answer:

In hypothesis testing, the significance of a result is often evaluated against a null hypothesis of chance behavior. For agreement coefficients, this involves rejecting the possibility that a positive value of agreement is merely due to random coding. However, testing against a null hypothesis of chance agreement is not particularly insightful, as demonstrating that agreement is significantly better than chance does not necessarily establish reliability. The primary goal of measuring reliability is to ensure that coders do not deviate significantly from perfect agreement (Krippendorff 2004a, page 237).

The relevant notion of significance for agreement coefficients is therefore a confidence interval. Cohen (1960, pages 43-44) suggests that when sample sizes are large, the sampling distribution of κ (kappa) is approximately normal for any true population value of κ. Consequently, confidence intervals for the observed value of κ can be determined using the usual multiples of the standard error. Donner and Eliasziw (1987) propose a more general form of significance test for arbitrary levels of agreement.

In contrast, Krippendorff (2004a, Section 11.4.2) notes that the distribution of α (alpha) is unknown, necessitating the use of bootstrapping to obtain confidence intervals. A software package for this purpose is described in Hayes and Krippendorff (2007).",NO,True,1376,True,True
12719479-s29,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,Interpreting the Value of Kappa-Like Coefficients.,"Even after testing significance and establishing confidence intervals for agreement coefficients, we are still faced with the problem of interpreting the meaning of the resulting values. Suppose, for example, we establish that for a particular task, K = 0.78 ± 0.05. Is this good or bad? Unfortunately, deciding what counts as an adequate level of agreement for a specific purpose is still little more than a black art: As we will see, different levels of agreement may be appropriate for resource building and for more linguistic purposes.

The problem is not unlike that of interpreting the values of correlation coefficients, and in the area of medical diagnosis, the best known conventions concerning the value of kappa-like coefficients, those proposed by Landis and Koch (1977) and reported in Figure 1, are indeed similar to those used for correlation coefficients, where values above 0.4 are also generally considered adequate (Marion 2004). Many medical researchers feel that these conventions are appropriate, and in language studies, a similar interpretation of the values has been proposed by Rietveld and van Hout (1993). In CL, however, most researchers follow the more stringent conventions from content analysis proposed by Krippendorff (1980, page 147), as reported by Carletta (1996, page 252): ""content analysis researchers generally think of K > .8 as good reliability, with .67 < K < .8 allowing tentative conclusions to be drawn"" (Krippendorff was discussing values of α rather than K, but the coefficients are nearly equivalent for categorical labels). As a result, ever since Carletta's influential paper, CL researchers have attempted to achieve a value of K (more seldom, of α) above the 0.8 threshold, or, failing that, the 0.67 level allowing for ""tentative conclusions."" However, the description of the 0.67 boundary in Krippendorff (1980) was actually ""highly tentative and cautious,"" and in later work Krippendorff clearly considers 0.8 the absolute minimum value of α to accept for any serious purpose: ""Even a cutoff point of α = .800 . . . is a pretty low standard"" (Krippendorff 2004a, page 242). Recent content analysis practice seems to have settled for even more stringent requirements: A recent textbook, Neuendorf (2002, page 3), analyzing several proposals concerning ""acceptable"" reliability, concludes that ""reliability coefficients of .90 or greater would be acceptable to all, .80 or greater would be acceptable in most situations, and below that, there exists great disagreement."" This is clearly a fundamental issue. Ideally we would want to establish thresholds which are appropriate for the field of CL, but as we will see in the rest of this section, a decade of practical experience hasn't helped in settling the matter. In fact, weighted coefficients, while arguably more appropriate for many annotation tasks, make the issue of deciding when the value of a coefficient indicates sufficient agreement even Kappa values and strength of agreement according to Landis and Koch (1977). more complicated because of the problem of determining appropriate weights (see Section 4.4). We will return to the issue of interpreting the value of the coefficients at the end of this article.",How do conventions for interpreting kappa-like coefficients differ between medical research and computational linguistics?,"Even after testing significance and establishing confidence intervals for agreement coefficients, interpreting the meaning of the resulting values remains challenging. In the area of medical diagnosis, the best-known conventions concerning the value of kappa-like coefficients, proposed by Landis and Koch (1977), are similar to those used for correlation coefficients, where values above 0.4 are generally considered adequate (Marion 2004). Many medical researchers feel that these conventions are appropriate. In language studies, a similar interpretation of the values has been proposed by Rietveld and van Hout (1993). In computational linguistics (CL), however, most researchers follow the more stringent conventions from content analysis proposed by Krippendorff (1980, page 147), as reported by Carletta (1996, page 252). Content analysis researchers generally think of K > .8 as good reliability, with .67 < K < .8 allowing tentative conclusions to be drawn. Krippendorff was discussing values of α rather than K, but the coefficients are nearly equivalent for categorical labels. As a result, ever since Carletta's influential paper, CL researchers have attempted to achieve a value of K (more seldom, of α) above the 0.8 threshold, or, failing that, the 0.67 level allowing for ""tentative conclusions."" However, the description of the 0.67 boundary in Krippendorff (1980) was actually ""highly tentative and cautious,"" and in later work, Krippendorff clearly considers 0.8 the absolute minimum value of α to accept for any serious purpose. Recent content analysis practice seems to have settled for even more stringent requirements. A recent textbook, Neuendorf (2002, page 3), concludes that ""reliability coefficients of .90 or greater would be acceptable to all, .80 or greater would be acceptable in most situations, and below that, there exists great disagreement.""",How do the conventions for interpreting kappa-like coefficients differ between medical research and computational linguistics?,"Interpreting kappa-like coefficients can be challenging, even after testing significance and establishing confidence intervals. In medical diagnosis, the conventions for these coefficients, as proposed by Landis and Koch (1977), are similar to those used for correlation coefficients. Generally, values above 0.4 are considered adequate (Marion 2004), and many medical researchers find these conventions appropriate. In language studies, Rietveld and van Hout (1993) proposed a similar interpretation.

However, in computational linguistics (CL), researchers often adhere to more stringent standards from content analysis, as proposed by Krippendorff (1980, page 147) and reported by Carletta (1996, page 252). In content analysis, a kappa-like coefficient (K) greater than 0.8 is considered indicative of good reliability, while values between 0.67 and 0.8 allow for tentative conclusions. Although Krippendorff originally discussed values of α rather than K, the coefficients are nearly equivalent for categorical labels. Since Carletta's influential paper, CL researchers have aimed for a K value above 0.8, or at least above 0.67 for tentative conclusions. However, Krippendorff (1980) described the 0.67 boundary as ""highly tentative and cautious,"" and in later work, he considered 0.8 the minimum acceptable value of α for serious purposes.

Recent content analysis practices have become even more stringent. According to Neuendorf (2002, page 3), ""reliability coefficients of .90 or greater would be acceptable to all, .80 or greater would be acceptable in most situations, and below that, there exists great disagreement.""",7,4,7,4,"Question:

How do conventions for interpreting kappa-like coefficients differ between medical research and computational linguistics?

Answer:

Even after testing significance and establishing confidence intervals for agreement coefficients, interpreting the meaning of the resulting values remains challenging. In the area of medical diagnosis, the best-known conventions concerning the value of kappa-like coefficients, proposed by Landis and Koch (1977), are similar to those used for correlation coefficients, where values above 0.4 are generally considered adequate (Marion 2004). Many medical researchers feel that these conventions are appropriate. In language studies, a similar interpretation of the values has been proposed by Rietveld and van Hout (1993). In computational linguistics (CL), however, most researchers follow the more stringent conventions from content analysis proposed by Krippendorff (1980, page 147), as reported by Carletta (1996, page 252). Content analysis researchers generally think of K > .8 as good reliability, with .67 < K < .8 allowing tentative conclusions to be drawn. Krippendorff was discussing values of α rather than K, but the coefficients are nearly equivalent for categorical labels. As a result, ever since Carletta's influential paper, CL researchers have attempted to achieve a value of K (more seldom, of α) above the 0.8 threshold, or, failing that, the 0.67 level allowing for ""tentative conclusions."" However, the description of the 0.67 boundary in Krippendorff (1980) was actually ""highly tentative and cautious,"" and in later work, Krippendorff clearly considers 0.8 the absolute minimum value of α to accept for any serious purpose. Recent content analysis practice seems to have settled for even more stringent requirements. A recent textbook, Neuendorf (2002, page 3), concludes that ""reliability coefficients of .90 or greater would be acceptable to all, .80 or greater would be acceptable in most situations, and below that, there exists great disagreement.""","Question:

How do the conventions for interpreting kappa-like coefficients differ between medical research and computational linguistics?

Answer:

Interpreting kappa-like coefficients can be challenging, even after testing significance and establishing confidence intervals. In medical diagnosis, the conventions for these coefficients, as proposed by Landis and Koch (1977), are similar to those used for correlation coefficients. Generally, values above 0.4 are considered adequate (Marion 2004), and many medical researchers find these conventions appropriate. In language studies, Rietveld and van Hout (1993) proposed a similar interpretation.

However, in computational linguistics (CL), researchers often adhere to more stringent standards from content analysis, as proposed by Krippendorff (1980, page 147) and reported by Carletta (1996, page 252). In content analysis, a kappa-like coefficient (K) greater than 0.8 is considered indicative of good reliability, while values between 0.67 and 0.8 allow for tentative conclusions. Although Krippendorff originally discussed values of α rather than K, the coefficients are nearly equivalent for categorical labels. Since Carletta's influential paper, CL researchers have aimed for a K value above 0.8, or at least above 0.67 for tentative conclusions. However, Krippendorff (1980) described the 0.67 boundary as ""highly tentative and cautious,"" and in later work, he considered 0.8 the minimum acceptable value of α for serious purposes.

Recent content analysis practices have become even more stringent. According to Neuendorf (2002, page 3), ""reliability coefficients of .90 or greater would be acceptable to all, .80 or greater would be acceptable in most situations, and below that, there exists great disagreement.""",NO,True,1630,True,True
12719479-s31,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,Labeling Units with a Common and Predefined Set of Categories: The Case of Dialogue Act Tagging,"The simplest and most common coding in CL involves labeling segments of text with a limited number of linguistic categories: Examples include part-of-speech tagging, dialogue act tagging, and named entity tagging. The practices used to test reliability for this type of annotation tend to be based on the assumption that the categories used in the annotation are mutually exclusive and equally distinct from one another; this assumption seems to have worked out well in practice, but questions about it have been raised even for the annotation of parts of speech (Babarczy, Carroll, and Sampson 2006), let alone for discourse coding tasks such as dialogue act coding. We concentrate here on this latter type of coding, but a discussion of issues raised for POS, named entity, and prosodic coding can be found in the extended version of the article. Dialogue act tagging is a type of linguistic annotation with which by now the CL community has had extensive experience: Several dialogue-act-annotated spoken language corpora now exist, such as MapTask (Carletta et al. 1997), Switchboard (Stolcke et al. 2000), Verbmobil (Jekat et al. 1995), and Communicator (e.g., Doran et al. 2001), among others. Historically, dialogue act annotation was also one of the types of annotation that motivated the introduction in CL of chance-corrected coefficients of agreement (Carletta et al. 1997) and, as we will see, it has been the type of annotation that has generated the most discussion concerning annotation methodology and measuring agreement.

A number of coding schemes for dialogue acts have achieved values of K over 0.8 and have therefore been assumed to be reliable: For example, K = 0.83 for the 13-tag MapTask coding scheme (Carletta et al. 1997), K = 0.8 for the 42-tag Switchboard-DAMSL scheme (Stolcke et al. 2000), K = 0.90 for the smaller 20-tag subset of the CSTAR scheme used by Doran et al. (2001). All of these tests were based on the same two assumptions: that every unit (utterance) is assigned to exactly one category (dialogue act), and that these categories are distinct. Therefore, again, unweighted measures, and in particular K, tend to be used for measuring inter-coder agreement.

However, these assumptions have been challenged based on the observation that utterances tend to have more than one function at the dialogue act level (Traum and Hinkelman 1992;Allen and Core 1997;Bunt 2000); for a useful survey, see Popescu-Belis (2005). An assertion performed in answer to a question, for instance, typically performs at least two functions at different levels: asserting some information-the dialogue act that we called Statement in Section 2.3, operating at what Traum and Hinkelman called the ""core speech act"" level-and confirming that the question has been understood, a dialogue act operating at the ""grounding"" level and usually known as Acknowledgment (Ack). In older dialogue act tagsets, acknowledgments and statements were treated as alternative labels at the same ""level"", forcing coders to choose one or the other when an utterance performed a dual function, according to a well-specified set of instructions. By contrast, in the annotation schemes inspired from these newer theories such as DAMSL , coders are allowed to assign tags along distinct ""dimensions"" or ""levels"".

Two annotation experiments testing this solution to the ""multi-tag"" problem with the DAMSL scheme were reported in Core and Allen (1997) and Di Eugenio et al. (1998). In both studies, coders were allowed to mark each communicative function independently: That is, they were allowed to choose for each utterance one of the Statement tags (or possibly none), one of the Influencing-Addressee-Future-Action tags, and so forth-and agreement was evaluated separately for each dimension using (unweighted) K. Core and Allen found values of K ranging from 0.76 for answer to 0.42 for agreement to 0.15 for Committing-Speaker-Future-Action. Using different coding instructions and on a different corpus, Di Eugenio et al. observed higher agreement, ranging from K = 0.93 (for other-forward-function) to 0.54 (for the tag agreement).

These relatively low levels of agreement led many researchers to return to ""flat"" tagsets for dialogue acts, incorporating however in their schemes some of the insights motivating the work on schemes such as DAMSL. The best known example of this type of approach is the development of the SWITCHBOARD-DAMSL tagset by Jurafsky, Shriberg, and Biasca (1997), which incorporates many ideas from the ""multi-dimensional"" theories of dialogue acts, but does not allow marking an utterance as both an acknowledgment and a statement; a choice has to be made. This tagset results in overall agreement of K = 0.80. Interestingly, subsequent developments of SWITCHBOARD-DAMSL backtracked on some of these decisions. For instance, the ICSI-MRDA tagset developed for the annotation of the ICSI Meeting Recorder corpus reintroduces some of the DAMSL ideas, in that annotators are allowed to assign multiple SWITCHBOARD-DAMSL labels to utterances (Shriberg et al. 2004). Shriberg et al. achieved a comparable reliability to that obtained with SWITCHBOARD-DAMSL, but only when using a tagset of just five ""class-maps"". Shriberg et al. (2004) also introduced a hierarchical organization of tags to improve reliability. The dimensions of the DAMSL scheme can be viewed as ""superclasses"" of dialogue acts which share some aspect of their meaning. For instance, the dimension of Influencing-Addressee-Future-Action (IAFA) includes the two dialogue acts Open-option (used to mark suggestions) and Directive, both of which bring into consideration a future action to be performed by the addressee. At least in principle, an organization of this type opens up the possibility for coders to mark an utterance with the superclass (IAFA) in case they do not feel confident that the utterance satisfies the additional requirements for Open-option or Directive. This, in turn, would do away with the need to make a choice between these two options. This possibility wasn't pursued in the studies using the original DAMSL that we are aware of Di Eugenio 2000;Stent 2001), but was tested by Shriberg et al. (2004) and subsequent work, in particular Geertzen and Bunt (2006), who were specifically interested in the idea of using hierarchical schemes to measure partial agreement, and in addition experimented with weighted coefficients of agreement for their hierarchical tagging scheme, specifically κ w .

Geertzen and Bunt tested intercoder agreement with Bunt's DIT++ (Bunt 2005), a scheme with 11 dimensions that builds on ideas from DAMSL and from Dynamic Interpretation Theory (Bunt 2000). In DIT++, tags can be hierarchically related: For example, the class information-seeking is viewed as consisting of two classes, yesno question (ynq) and wh-question (whq). The hierarchy is explicitly introduced in order to allow coders to leave some aspects of the coding undecided. For example, check is treated as a subclass of ynq in which, in addition, the speaker has a weak belief that the proposition that forms the belief is true. A coder who is not certain about the dialogue act performed using an utterance may simply choose to tag it as ynq.

The distance metric d proposed by Geertzen and Bunt is based on the criterion that two communicative functions are related (d(c 1 , c 2 ) < 1) if they stand in an ancestor-offspring relation within a hierarchy. Furthermore, they argue, the magnitude of d(c 1 , c 2 ) should be proportional to the distance between the functions in the hierarchy. A level-dependent correction factor is also proposed so as to leave open the option to make disagreements at higher levels of the hierarchy matter more than disagreements at the deeper level (for example, the distance between information-seeking and ynq might be considered greater than the distance between check and positive-check).

The results of an agreement test with two annotators run by Geertzen and Bunt show that taking into account partial agreement leads to values of κ w that are higher than the values of κ for the same categories, particularly for feedback, a class for which Core and Allen (1997) got low agreement. Of course, even assuming that the values of κ w and κ were directly comparable-we remark on the difficulty of interpreting the values of weighted coefficients of agreement in Section 4.4-it remains to be seen whether these higher values are a better indication of the extent of agreement between coders than the values of unweighted κ.

This discussion of coding schemes for dialogue acts introduced issues to which we will return for other CL annotation tasks as well. There are a number of wellestablished schemes for large-scale dialogue act annotation based on the assumption of mutual exclusivity between dialogue act tags, whose reliability is also well known; if one of these schemes is appropriate for modeling the communicative intentions found in a task, we recommend to our readers to use it. They should also realize, however, that the mutual exclusivity assumption is somewhat dubious. If a multi-dimensional or hierarchical tagset is used, readers should also be aware that weighted coefficients do capture partial agreement, and need not automatically result in lower reliability or in an explosion in the number of labels. However, a hierarchical scheme may not reflect genuine annotation difficulties: For example, in the case of DIT++, one might argue that it is more difficult to confuse yes-no questions with wh-questions than with statements. We will also see in a moment that interpreting the results with weighted coefficients is difficult. We will return to both of these problems in what follows.",How do newer annotation schemes like DAMSL address the challenges of multi-tag problems in dialogue act tagging?,"Newer annotation schemes like DAMSL address the challenges of multi-tag problems in dialogue act tagging by allowing coders to assign tags along distinct ""dimensions"" or ""levels."" In these schemes, coders are permitted to mark each communicative function independently, choosing for each utterance one of the Statement tags, one of the Influencing-Addressee-Future-Action tags, and so forth. Agreement is then evaluated separately for each dimension using unweighted measures like K. This approach contrasts with older dialogue act tagsets, where acknowledgments and statements were treated as alternative labels at the same ""level,"" forcing coders to choose one or the other when an utterance performed a dual function. The DAMSL scheme's multi-dimensional approach allows for more flexibility and can capture the multiple functions an utterance may perform at different levels.",How do annotation schemes like DAMSL tackle the challenges of multi-tag issues in dialogue act tagging?,"Annotation schemes like DAMSL effectively tackle the challenges of multi-tag issues in dialogue act tagging by allowing coders to assign tags across distinct ""dimensions"" or ""levels."" This means that for each utterance, coders can independently mark each communicative function, selecting one of the Statement tags, one of the Influencing-Addressee-Future-Action tags, and so on. This approach enables the evaluation of agreement separately for each dimension using unweighted measures like K. Unlike older dialogue act tagsets, where acknowledgments and statements were treated as alternative labels at the same ""level,"" forcing coders to choose one or the other when an utterance served a dual function, the DAMSL scheme's multi-dimensional approach provides more flexibility. It captures the multiple functions an utterance may perform at different levels, offering a more comprehensive understanding of dialogue acts.",7,2,7,2,"Question:

How do newer annotation schemes like DAMSL address the challenges of multi-tag problems in dialogue act tagging?

Answer:

Newer annotation schemes like DAMSL address the challenges of multi-tag problems in dialogue act tagging by allowing coders to assign tags along distinct ""dimensions"" or ""levels."" In these schemes, coders are permitted to mark each communicative function independently, choosing for each utterance one of the Statement tags, one of the Influencing-Addressee-Future-Action tags, and so forth. Agreement is then evaluated separately for each dimension using unweighted measures like K. This approach contrasts with older dialogue act tagsets, where acknowledgments and statements were treated as alternative labels at the same ""level,"" forcing coders to choose one or the other when an utterance performed a dual function. The DAMSL scheme's multi-dimensional approach allows for more flexibility and can capture the multiple functions an utterance may perform at different levels.","Question:

How do annotation schemes like DAMSL tackle the challenges of multi-tag issues in dialogue act tagging?

Answer:

Annotation schemes like DAMSL effectively tackle the challenges of multi-tag issues in dialogue act tagging by allowing coders to assign tags across distinct ""dimensions"" or ""levels."" This means that for each utterance, coders can independently mark each communicative function, selecting one of the Statement tags, one of the Influencing-Addressee-Future-Action tags, and so on. This approach enables the evaluation of agreement separately for each dimension using unweighted measures like K. Unlike older dialogue act tagsets, where acknowledgments and statements were treated as alternative labels at the same ""level,"" forcing coders to choose one or the other when an utterance served a dual function, the DAMSL scheme's multi-dimensional approach provides more flexibility. It captures the multiple functions an utterance may perform at different levels, offering a more comprehensive understanding of dialogue acts.",NO,True,921,True,True
12719479-s32,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,Marking Boundaries and Unitizing,"Before labeling can take place, the units of annotation, or markables, need to be identified-a process Krippendorff (1995Krippendorff ( , 2004a calls unitizing. The practice in CL for the forms of annotation discussed in the previous section is to assume that the units are linguistic constituents which can be easily identified, such as words, utterances, or noun phrases, and therefore there is no need to check the reliability of this process. We are aware of few exceptions to this assumption, such as Carletta et al. (1997) on unitization for move coding and our own work on the GNOME corpus (Poesio 2004b). In cases such as text segmentation, however, the identification of units is as important as their labeling, if not more important, and therefore checking agreement on unit identification is essential. In this section we discuss current CL practice with reliability testing of these types of annotation, before briefly summarizing Krippendorff's proposals concerning measuring reliability for unitizing.",Why is unit identification considered crucial in text segmentation within computational linguistics?,"In cases such as text segmentation, the identification of units is as important as their labeling, if not more important, and therefore checking agreement on unit identification is essential.",Why is unit identification considered crucial in text segmentation within computational linguistics?,"In the context of text segmentation, identifying the units is as crucial, if not more so, than labeling them. Therefore, ensuring agreement on unit identification is essential.",9,2,7,2,"Question:

Why is unit identification considered crucial in text segmentation within computational linguistics?

Answer:

In cases such as text segmentation, the identification of units is as important as their labeling, if not more important, and therefore checking agreement on unit identification is essential.","Question:

Why is unit identification considered crucial in text segmentation within computational linguistics?

Answer:

In the context of text segmentation, identifying the units is as crucial, if not more so, than labeling them. Therefore, ensuring agreement on unit identification is essential.",NO,True,176,False,False
12719479-s39,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,Word Senses,"Word sense tagging is one of the hardest annotation tasks. Whereas in the case of partof-speech and dialogue act tagging the same categories are used to classify all units, in the case of word sense tagging different categories must be used for each word, which makes writing a single coding manual specifying examples for all categories impossible: The only option is to rely on a dictionary. Unfortunately, different dictionaries make different distinctions, and often coders can't make the fine-grained distinctions that trained lexicographers can make. The problem is particularly serious for verbs, which tend to be polysemous rather than homonymous (Palmer, Dang, and Fellbaum 2007).

These difficulties, and in particular the difficulty of tagging senses with a finegrained repertoire of senses such as that provided by dictionaries or by WordNet (Fellbaum 1998), have been highlighted by the three SENSEVAL initiatives. Already during the first SENSEVAL, Véronis (1998) carried out two studies of intercoder agreement on word sense tagging in the so-called ROMANSEVAL task. One study was concerned with agreement on polysemy-that is, the extent to which coders agreed that a word was polysemous in a given context. Six naive coders were asked to make this judgment about 600 French words (200 nouns, 200 verbs, 200 adjectives) using the repertoire of senses in the Petit Larousse. On this task, a (pairwise) percentage agreement of 0.68 for nouns, 0.74 for verbs, and 0.78 for adjectives was observed, corresponding to K values of 0.36, 0.37, and 0.67, respectively. The 20 words from each category perceived by the coders in this first experiment to be most polysemous were then used in a second study, of intercoder agreement on the sense tagging task, which involved six different naive coders. Interestingly, the coders in this second experiment were allowed to assign multiple tags to words, although they did not make much use of this possibility; so κ w was used to measure agreement. In this experiment, Véronis observed (weighted) pairwise agreement of 0.63 for verbs, 0.71 for adjectives, and 0.73 for nouns, corresponding to κ w values of 0.41, 0.41, and 0.46, but with a wide variety of values when measured per word-ranging from 0.007 for the adjective correct to 0.92 for the noun détention. Similarly mediocre results for intercoder agreement between naive coders were reported in the subsequent editions of SENSEVAL. Agreement studies for SENSEVAL-2, where WordNet senses were used as tags, reported a percentage agreement for verb senses of around 70%, whereas for SENSEVAL-3 (English Lexical Sample Task), Mihalcea, Chklovski, and Kilgarriff (2004) report a percentage agreement of 67.3% and average K of 0.58.

Two types of solutions have been proposed for the problem of low agreement on sense tagging. The solution proposed by Kilgarriff (1999) is to use professional lexicographers and arbitration. The study carried out by Kilgarriff does not therefore qualify as a true study of replicability in the sense of the terms used by Krippendorff, but it did show that this approach makes it possible to achieve percentage agreement of around 95.5%. An alternative approach has been to address the problem of the inability of naive coders to make fine-grained distinctions by introducing coarser-grained classification schemes which group together dictionary senses (Bruce and Wiebe, 1998;Buitelaar 1998;Véronis 1998;Palmer, Dang, and Fellbaum 2007). Hierarchical tagsets were also developed, such as HECTOR (Atkins 1992) or, indeed, WordNet itself (where senses are related by hyponymy links). In the case of Buitelaar and Palmer, Dang, and Fellbaum, the ""supersenses"" were identified by hand, whereas Bruce and Wiebe and Véronis used clustering methods such as those from Bruce and Wiebe (1999) to collapse some of the initial sense distinctions. 9 Palmer, Dang, and Fellbaum (2007) illustrate this practice with the example of the verb call, which has 28 fine-grained senses in WordNet 1.7: They conflate these senses into a small number of groups using various criteria-for example, four senses can be grouped in a group they call Group 1 on the basis of subcategorization frame similarities (Table 9). Palmer, Dang, and Fellbaum (2007) achieved for the English Verb Lexical Sense task of SENSEVAL-2 a percentage agreement among coders of 82% with grouped senses, as opposed to 71% with the original WordNet senses. Bruce and Wiebe (1998) found that collapsing the senses of their test word (interest) on the basis of their use by coders and merging the two classes found to be harder to distinguish resulted in an increase of Table 9 Group 1 of senses of call in Palmer, Dang, and Fellbaum (2007, page 149 the value of K from 0.874 to 0.898. Using a related technique, Véronis (1998) found that agreement on noun word sense tagging went up from a K of around 0.45 to a K of 0.86. We should note, however, that the post hoc merging of categories is not equivalent to running a study with fewer categories to begin with. Attempts were also made to develop techniques to measure partial agreement with hierarchical tagsets. A first proposal in this direction was advanced by Melamed and Resnik (2000), who developed a coefficient for hierarchical tagsets that could be used in SENSEVAL for measuring agreement with tagsets such as HECTOR. Melamed and Resnik proposed to ""normalize"" the computation of observed and expected agreement by taking each label which is not a leaf in the tag hierarchy and distributing it down to the leaves in a uniform way, and then only computing agreement on the leaves. For example, with a tagset like the one in Table 9, the cases in which the coders used the label 'Group 1' would be uniformly ""distributed down"" and added in equal measure to the number of cases in which the coders assigned each of the four WordNet labels. The method proposed in the paper has, however, problematic properties when used to measure intercoder agreement. For example, suppose tag A dominates two sub-tags A1 and A2, and that two coders mark a particular item as A. Intuitively, we would want to consider this a case of perfect agreement, but this is not what the method proposed by Melamed and Resnik yields. The annotators' marks are distributed over the two sub-tags, each with probability 0.5, and then the agreement is computed by summing the joint probabilities over the two subtags (Equation (4) of Melamed and Resnik 2000), with the result that the agreement over the item turns out to be 0.5 2 + 0.5 2 = 0.5 instead of 1. To correct this, Dan Melamed (personal communication) suggested replacing the product in Equation (4) with a minimum operator. However, the calculation of expected agreement (Equation (5) of Melamed and Resnik 2000) still gives the amount of agreement which is expected if coders are forced to choose among leaf nodes, which makes this method inappropriate for coding schemes that do not force coders to do this.

One way to use Melamed and Resnik's proposal while avoiding the discrepancy between observed and expected agreement is to treat the proposal not as a new coefficient, but rather as a distance metric to be plugged into a weighted coefficient like α. Let A and B be two nodes in a hierarchical tagset, let L be the set of all leaf nodes in the tagset, and let P(l|T) be the probability of selecting a leaf node l given an arbitrary node T when the probability mass of T is distributed uniformly to all the nodes dominated by T. We can reinterpret Melamed's modification of Equation (4) in Melamed and Resnik (2000) as a metric measuring the distance between nodes A and B. d M+R = 1 − ∑ l∈L min(P(l|A), P(l|B)) This metric has the desirable properties-it is 0 when tags A and B are identical, 1 when the tags do not overlap, and somewhere in between in all other cases. If we use this metric for Krippendorff's α we find that observed agreement is exactly the same as in Melamed and Resnik (2000) with the product operator replaced by minimum (Melamed's modification).

We can also use other distance metrics with α. For example, we could associate with each sense an extended sense-a set es(s) including the sense itself and its grouped sense-and then use set-based distance metrics from Section 4.4, for example Passonneau's d P . To illustrate how this approach could be used to measure (dis)agreement on word sense annotation, suppose that two coders have to annotate the use of call in the following sentence (from the WSJ part of the Penn Treebank, section 02, text w0209):

This gene, called ""gametocide,"" is carried into the plant by a virus that remains active for a few days.

The standard guidelines (in SENSEVAL, say) require coders to assign a WN sense to words. Under such guidelines, if coder A classifies the use of called in the above example as an instance of WN1, whereas coder B annotates it as an instance of WN3, we would find total disagreement (d k a k b = 1) which seems excessively harsh as the two senses are clearly related. However, by using the broader senses proposed by Palmer, Dang, and Fellbaum (2007) in combination with a distance metric such as the one just proposed, it is possible to get more flexible and, we believe, more realistic assessments of the degree of agreement in situations such as this. For instance, in case the reliability study had already been carried out under the standard SENSEVAL guidelines, the distance metric proposed above could be used to identify post hoc cases of partial agreement by adding to each WN sense its hypernyms according to the groupings proposed by Palmer, Dang, and Fellbaum. For example, A's annotation could be turned into a new set label {WN1,LABEL} and B's mark into the set table {WN3,LABEL}, which would give a distance d = 2/3, indicating a degree of overlap. The method for computing agreement proposed here could could also be used to allow coders to choose either a more specific label or one of Palmer, Dang, and Fellbaum's superlabels. For example, suppose A sticks to WN1, but B decides to mark the use above using Palmer, Dang, and Fellbaum's LABEL category, then we would still find a distance d = 1/3. An alternative way of using α for word sense annotation was developed and tested by Passonneau, Habash, and Rambow (2006). Their approach is to allow coders to assign multiple labels (WordNet synsets) for wordsenses, as done by Véronis (1998) and more recently by Rosenberg and Binkowski (2004) for text classification labels and by Poesio and Artstein (2005) for anaphora. These multi-label sets can then be compared using the MASI distance metric for α (Passonneau 2006).","What solutions have been proposed to improve intercoder agreement in word sense tagging, and how effective are they?","Two types of solutions have been proposed for the problem of low agreement on sense tagging. The solution proposed by Kilgarriff (1999) is to use professional lexicographers and arbitration. This approach makes it possible to achieve percentage agreement of around 95.5%. An alternative approach has been to address the problem of the inability of naive coders to make fine-grained distinctions by introducing coarser-grained classification schemes which group together dictionary senses (Bruce and Wiebe, 1998; Buitelaar 1998; Véronis 1998; Palmer, Dang, and Fellbaum 2007). Hierarchical tagsets were also developed, such as HECTOR (Atkins 1992) or WordNet itself (where senses are related by hyponymy links). Palmer, Dang, and Fellbaum (2007) illustrate this practice with the example of the verb call, which has 28 fine-grained senses in WordNet 1.7: They conflate these senses into a small number of groups using various criteria. Palmer, Dang, and Fellbaum (2007) achieved for the English Verb Lexical Sense task of SENSEVAL-2 a percentage agreement among coders of 82% with grouped senses, as opposed to 71% with the original WordNet senses. Bruce and Wiebe (1998) found that collapsing the senses of their test word (interest) on the basis of their use by coders and merging the two classes found to be harder to distinguish resulted in an increase of the value of K from 0.874 to 0.898. Using a related technique, Véronis (1998) found that agreement on noun word sense tagging went up from a K of around 0.45 to a K of 0.86. The post hoc merging of categories is not equivalent to running a study with fewer categories to begin with. Attempts were also made to develop techniques to measure partial agreement with hierarchical tagsets. A first proposal in this direction was advanced by Melamed and Resnik (2000), who developed a coefficient for hierarchical tagsets that could be used in SENSEVAL for measuring agreement with tagsets such as HECTOR. Melamed and Resnik proposed to ""normalize"" the computation of observed and expected agreement by taking each label which is not a leaf in the tag hierarchy and distributing it down to the leaves in a uniform way, and then only computing agreement on the leaves. The method proposed in the paper has, however, problematic properties when used to measure intercoder agreement. One way to use Melamed and Resnik's proposal while avoiding the discrepancy between observed and expected agreement is to treat the proposal not as a new coefficient, but rather as a distance metric to be plugged into a weighted coefficient like α. This metric has the desirable properties—it is 0 when tags A and B are identical, 1 when the tags do not overlap, and somewhere in between in all other cases. If this metric is used for Krippendorff's α, observed agreement is exactly the same as in Melamed and Resnik (2000) with the product operator replaced by minimum (Melamed's modification). An alternative way of using α for word sense annotation was developed and tested by Passonneau, Habash, and Rambow (2006). Their approach is to allow coders to assign multiple labels (WordNet synsets) for word senses, as done by Véronis (1998) and more recently by Rosenberg and Binkowski (2004) for text classification labels and by Poesio and Artstein (2005) for anaphora. These multi-label sets can then be compared using the MASI distance metric for α (Passonneau 2006).","What are the proposed solutions to improve intercoder agreement in word sense tagging, and how effective are they?","To address the challenge of low intercoder agreement in word sense tagging, several solutions have been proposed. One approach, suggested by Kilgarriff (1999), involves using professional lexicographers and arbitration, which can achieve a high percentage agreement of around 95.5%. Alternatively, researchers have explored coarser-grained classification schemes to help naive coders make more accurate distinctions. This involves grouping dictionary senses together, as demonstrated by Bruce and Wiebe (1998), Buitelaar (1998), Véronis (1998), and Palmer, Dang, and Fellbaum (2007). For instance, Palmer, Dang, and Fellbaum (2007) showed that by conflating the 28 fine-grained senses of the verb ""call"" in WordNet 1.7 into fewer groups, they increased coder agreement from 71% to 82% in the English Verb Lexical Sense task of SENSEVAL-2.

Further, Bruce and Wiebe (1998) found that merging indistinguishable classes for the word ""interest"" improved the K value from 0.874 to 0.898. Similarly, Véronis (1998) reported an increase in agreement on noun word sense tagging from a K of around 0.45 to 0.86 by collapsing categories post hoc. However, this method is not equivalent to initially using fewer categories.

Efforts have also been made to measure partial agreement with hierarchical tagsets. Melamed and Resnik (2000) proposed a coefficient for hierarchical tagsets, which normalizes observed and expected agreement by distributing non-leaf labels down to the leaves. Although this method has some problematic properties, it can be used as a distance metric in a weighted coefficient like Krippendorff's α, ensuring desirable properties such as a metric value of 0 for identical tags and 1 for non-overlapping tags.

Additionally, Passonneau, Habash, and Rambow (2006) developed an approach allowing coders to assign multiple labels (WordNet synsets) for word senses, similar to the methods used by Véronis (1998), Rosenberg and Binkowski (2004) for text classification, and Poesio and Artstein (2005) for anaphora. These multi-label sets can be compared using the MASI distance metric for α, as demonstrated by Passonneau (2006).",7,4,7,4,"Question:

What solutions have been proposed to improve intercoder agreement in word sense tagging, and how effective are they?

Answer:

Two types of solutions have been proposed for the problem of low agreement on sense tagging. The solution proposed by Kilgarriff (1999) is to use professional lexicographers and arbitration. This approach makes it possible to achieve percentage agreement of around 95.5%. An alternative approach has been to address the problem of the inability of naive coders to make fine-grained distinctions by introducing coarser-grained classification schemes which group together dictionary senses (Bruce and Wiebe, 1998; Buitelaar 1998; Véronis 1998; Palmer, Dang, and Fellbaum 2007). Hierarchical tagsets were also developed, such as HECTOR (Atkins 1992) or WordNet itself (where senses are related by hyponymy links). Palmer, Dang, and Fellbaum (2007) illustrate this practice with the example of the verb call, which has 28 fine-grained senses in WordNet 1.7: They conflate these senses into a small number of groups using various criteria. Palmer, Dang, and Fellbaum (2007) achieved for the English Verb Lexical Sense task of SENSEVAL-2 a percentage agreement among coders of 82% with grouped senses, as opposed to 71% with the original WordNet senses. Bruce and Wiebe (1998) found that collapsing the senses of their test word (interest) on the basis of their use by coders and merging the two classes found to be harder to distinguish resulted in an increase of the value of K from 0.874 to 0.898. Using a related technique, Véronis (1998) found that agreement on noun word sense tagging went up from a K of around 0.45 to a K of 0.86. The post hoc merging of categories is not equivalent to running a study with fewer categories to begin with. Attempts were also made to develop techniques to measure partial agreement with hierarchical tagsets. A first proposal in this direction was advanced by Melamed and Resnik (2000), who developed a coefficient for hierarchical tagsets that could be used in SENSEVAL for measuring agreement with tagsets such as HECTOR. Melamed and Resnik proposed to ""normalize"" the computation of observed and expected agreement by taking each label which is not a leaf in the tag hierarchy and distributing it down to the leaves in a uniform way, and then only computing agreement on the leaves. The method proposed in the paper has, however, problematic properties when used to measure intercoder agreement. One way to use Melamed and Resnik's proposal while avoiding the discrepancy between observed and expected agreement is to treat the proposal not as a new coefficient, but rather as a distance metric to be plugged into a weighted coefficient like α. This metric has the desirable properties—it is 0 when tags A and B are identical, 1 when the tags do not overlap, and somewhere in between in all other cases. If this metric is used for Krippendorff's α, observed agreement is exactly the same as in Melamed and Resnik (2000) with the product operator replaced by minimum (Melamed's modification). An alternative way of using α for word sense annotation was developed and tested by Passonneau, Habash, and Rambow (2006). Their approach is to allow coders to assign multiple labels (WordNet synsets) for word senses, as done by Véronis (1998) and more recently by Rosenberg and Binkowski (2004) for text classification labels and by Poesio and Artstein (2005) for anaphora. These multi-label sets can then be compared using the MASI distance metric for α (Passonneau 2006).","Question:

What are the proposed solutions to improve intercoder agreement in word sense tagging, and how effective are they?

Answer:

To address the challenge of low intercoder agreement in word sense tagging, several solutions have been proposed. One approach, suggested by Kilgarriff (1999), involves using professional lexicographers and arbitration, which can achieve a high percentage agreement of around 95.5%. Alternatively, researchers have explored coarser-grained classification schemes to help naive coders make more accurate distinctions. This involves grouping dictionary senses together, as demonstrated by Bruce and Wiebe (1998), Buitelaar (1998), Véronis (1998), and Palmer, Dang, and Fellbaum (2007). For instance, Palmer, Dang, and Fellbaum (2007) showed that by conflating the 28 fine-grained senses of the verb ""call"" in WordNet 1.7 into fewer groups, they increased coder agreement from 71% to 82% in the English Verb Lexical Sense task of SENSEVAL-2.

Further, Bruce and Wiebe (1998) found that merging indistinguishable classes for the word ""interest"" improved the K value from 0.874 to 0.898. Similarly, Véronis (1998) reported an increase in agreement on noun word sense tagging from a K of around 0.45 to 0.86 by collapsing categories post hoc. However, this method is not equivalent to initially using fewer categories.

Efforts have also been made to measure partial agreement with hierarchical tagsets. Melamed and Resnik (2000) proposed a coefficient for hierarchical tagsets, which normalizes observed and expected agreement by distributing non-leaf labels down to the leaves. Although this method has some problematic properties, it can be used as a distance metric in a weighted coefficient like Krippendorff's α, ensuring desirable properties such as a metric value of 0 for identical tags and 1 for non-overlapping tags.

Additionally, Passonneau, Habash, and Rambow (2006) developed an approach allowing coders to assign multiple labels (WordNet synsets) for word senses, similar to the methods used by Véronis (1998), Rosenberg and Binkowski (2004) for text classification, and Poesio and Artstein (2005) for anaphora. These multi-label sets can be compared using the MASI distance metric for α, as demonstrated by Passonneau (2006).",NO,True,2136,True,True
12719479-s41,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,Methodology,"Our first recommendation is that annotation efforts should perform and report rigorous reliability testing. The last decade has already seen considerable improvement, from the absence of any tests for the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993) or the British National Corpus (Leech, Garside, and Bryant 1994) to the central role played by reliability testing in the Penn Discourse Treebank (Miltsakaki et al. 2004) and OntoNotes (Hovy et al. 2006). But even the latter efforts only measure and report percent agreement. We believe that part of the reluctance to report chance-corrected measures is the difficulty in interpreting them. However, our experience is that chancecorrected coefficients of agreement do provide a better indication of the quality of the resulting annotation than simple percent agreement, and moreover, the detailed calculations leading to the coefficients can be very revealing as to where the disagreements are located and what their sources may be.

A rigorous methodology for reliability testing does not, in our opinion, exclude the use of expert coders, and here we feel there may be a motivated difference between the fields of content analysis and CL. There is a clear tradeoff between the complexity of the judgments that coders are required to make and the reliability of such judgments, and we should strive to devise annotation schemes that are not only reliable enough to be replicated, but also sophisticated enough to be useful (cf. Krippendorff 2004a, pages 213-214). In content analysis, conclusions are drawn directly from annotated corpora, so the emphasis is more on replicability; whereas in CL, corpora constitute a resource which is used by other processes, so the emphasis is more towards usefulness. There is also a tradeoff between the sophistication of judgments and the availability of coders who can make such judgments. Consequently, annotation by experts is often the only practical way to get useful corpora for CL. Current practice achieves high reliability either by using professionals (Kilgarriff 1999) or through intensive training (Hovy et al. 2006;Carlson, Marcu, and Okurowski 2003); this means that results are not replicable across sites, and are therefore less reliable than annotation by naive coders adhering to written instructions. We feel that inter-annotator agreement studies should still be carried out, as they serve as an assurance that the results are replicable when the annotators are chosen from the same population as the original annotators. An important additional assurance should be provided in the form of an independent evaluation of the task for which the corpus is used (cf. Passonneau 2006).","How is high reliability achieved in annotation practices, and what are the associated drawbacks?","High reliability in annotation practices is achieved either by using professionals (Kilgarriff 1999) or through intensive training (Hovy et al. 2006; Carlson, Marcu, and Okurowski 2003). This approach means that results are not replicable across sites, and are therefore less reliable than annotation by naive coders adhering to written instructions. There is a tradeoff between the sophistication of judgments and the availability of coders who can make such judgments. Consequently, annotation by experts is often the only practical way to get useful corpora for computational linguistics. Current practice achieves high reliability by using professionals or through intensive training, but this means that results are not replicable across sites. Inter-annotator agreement studies should still be carried out, as they serve as an assurance that the results are replicable when the annotators are chosen from the same population as the original annotators. An important additional assurance should be provided in the form of an independent evaluation of the task for which the corpus is used (cf. Passonneau 2006).","How can high reliability be achieved in annotation practices, and what are the potential drawbacks?","High reliability in annotation practices is typically achieved by employing professionals (Kilgarriff 1999) or through intensive training programs (Hovy et al. 2006; Carlson, Marcu, and Okurowski 2003). While this approach ensures sophisticated judgments, it also means that results are not easily replicable across different sites, making them less reliable than those produced by naive coders following written instructions. There is a tradeoff between the sophistication of judgments and the availability of coders capable of making such judgments. As a result, using experts for annotation is often the only practical way to develop useful corpora for computational linguistics. Although current practices achieve high reliability through professional or intensively trained annotators, it is crucial to conduct inter-annotator agreement studies. These studies provide assurance that results are replicable when annotators are selected from the same population as the original ones. Additionally, an independent evaluation of the task for which the corpus is used should be conducted to further ensure reliability (cf. Passonneau 2006).",9,4,9,7,"Question:

How is high reliability achieved in annotation practices, and what are the associated drawbacks?

Answer:

High reliability in annotation practices is achieved either by using professionals (Kilgarriff 1999) or through intensive training (Hovy et al. 2006; Carlson, Marcu, and Okurowski 2003). This approach means that results are not replicable across sites, and are therefore less reliable than annotation by naive coders adhering to written instructions. There is a tradeoff between the sophistication of judgments and the availability of coders who can make such judgments. Consequently, annotation by experts is often the only practical way to get useful corpora for computational linguistics. Current practice achieves high reliability by using professionals or through intensive training, but this means that results are not replicable across sites. Inter-annotator agreement studies should still be carried out, as they serve as an assurance that the results are replicable when the annotators are chosen from the same population as the original annotators. An important additional assurance should be provided in the form of an independent evaluation of the task for which the corpus is used (cf. Passonneau 2006).","Question:

How can high reliability be achieved in annotation practices, and what are the potential drawbacks?

Answer:

High reliability in annotation practices is typically achieved by employing professionals (Kilgarriff 1999) or through intensive training programs (Hovy et al. 2006; Carlson, Marcu, and Okurowski 2003). While this approach ensures sophisticated judgments, it also means that results are not easily replicable across different sites, making them less reliable than those produced by naive coders following written instructions. There is a tradeoff between the sophistication of judgments and the availability of coders capable of making such judgments. As a result, using experts for annotation is often the only practical way to develop useful corpora for computational linguistics. Although current practices achieve high reliability through professional or intensively trained annotators, it is crucial to conduct inter-annotator agreement studies. These studies provide assurance that results are replicable when annotators are selected from the same population as the original ones. Additionally, an independent evaluation of the task for which the corpus is used should be conducted to further ensure reliability (cf. Passonneau 2006).",NO,True,1140,True,True
12719479-s43,Survey Article Inter-Coder Agreement for Computational Linguistics,2008-12-01,Interpreting the Values,"We view the lack of consensus on how to interpret the values of agreement coefficients as a serious problem with current practice in reliability testing, and as one of the main reasons for the reluctance of many in CL to embark on reliability studies. Unlike significance values which report a probability (that an observed effect is due to chance), agreement coefficients report a magnitude, and it is less clear how to interpret such magnitudes. Our own experience is consistent with that of Krippendorff: Both in our earlier work (Poesio and Vieira 1998;Poesio 2004a) and in the more recent efforts (Poesio and Artstein 2005) we found that only values above 0.8 ensured an annotation of reasonable quality (Poesio 2004a). We therefore feel that if a threshold needs to be set, 0.8 is a good value.

That said, we doubt that a single cutoff point is appropriate for all purposes. For some CL studies, particularly on discourse, useful corpora have been obtained while attaining reliability only at the 0.7 level. We agree therefore with Craggs and McGee Wood (2005) that setting a specific agreement threshold should not be a prerequisite for publication. Instead, as recommended by Di Eugenio and Glass (2004) and others, researchers should report in detail on the methodology that was followed in collecting the reliability data (number of coders, whether they coded independently, whether they relied exclusively on an annotation manual), whether agreement was statistically significant, and provide a confusion matrix or agreement table so that readers can find out whether overall figures of agreement hide disagreements on less common categories. For an example of good practice in this respect, see Teufel and Moens (2002). The decision whether a corpus is good enough for publication should be based on more than the agreement score-specifically, an important consideration is an independent evaluation of the results that are based on the corpus.",What do the authors recommend researchers report in detail when collecting reliability data for computational linguistics studies?,"Researchers should report in detail on the methodology that was followed in collecting the reliability data. This includes the number of coders, whether they coded independently, and whether they relied exclusively on an annotation manual. Researchers should also report whether agreement was statistically significant. Additionally, they should provide a confusion matrix or agreement table so that readers can find out whether overall figures of agreement hide disagreements on less common categories.",What detailed information should researchers provide when reporting reliability data in computational linguistics studies?,"Researchers should thoroughly detail the methodology used in collecting reliability data. This includes specifying the number of coders involved, whether these coders worked independently, and if they relied solely on an annotation manual. Additionally, it is important to report whether the agreement among coders was statistically significant. Providing a confusion matrix or an agreement table is also recommended, as it allows readers to discern whether overall agreement figures obscure disagreements in less common categories.",9,2,9,2,"Question:

What do the authors recommend researchers report in detail when collecting reliability data for computational linguistics studies?

Answer:

Researchers should report in detail on the methodology that was followed in collecting the reliability data. This includes the number of coders, whether they coded independently, and whether they relied exclusively on an annotation manual. Researchers should also report whether agreement was statistically significant. Additionally, they should provide a confusion matrix or agreement table so that readers can find out whether overall figures of agreement hide disagreements on less common categories.","Question:

What detailed information should researchers provide when reporting reliability data in computational linguistics studies?

Answer:

Researchers should thoroughly detail the methodology used in collecting reliability data. This includes specifying the number of coders involved, whether these coders worked independently, and if they relied solely on an annotation manual. Additionally, it is important to report whether the agreement among coders was statistically significant. Providing a confusion matrix or an agreement table is also recommended, as it allows readers to discern whether overall agreement figures obscure disagreements in less common categories.",NO,True,532,True,True
8132278-s1,A Survey of Idiomatic Preposition-Noun-Verb Triples on Token Level,2010-05-01,Background,"Most of the research on the extraction of idiomatic MWEs focused on the acquisition of MWE types. The procedures made use of several corpus-observable idiosyncratic properties of MWEs: they were identified either based on their co-occurrence frequency (Evert, 2004), their morphosyntactic fixedness -e.g. (Fazly and Stevenson, 2006), (Bannard, 2007) -or their semantics -e.g. (Lin, 1999), (Baldwin et al., 2003), to name only a few examples. However, most of these approaches operate on lexical type level, stating, e.g. that spill+beans is idiomatic, but not on token level. Contrary to this, we intend to take into account whether a text instance of a potentially idiomatic MWE is actually used idiomatically in a given context or not. In fact, there are a number of idiomatic MWEs that can also have a straightforward literal meaning. It is possible to automatically distinguish the idiomatic from the literal use in the way (Katz and Giesbrecht, 2006) did by using latent semantic analysis. In one of their case-studies they found that two thirds of the occurrences of the German idiom ins Wasser fallen (lit.: ""to fall into the water"", idiom.: ""to be cancelled"") were idiomatic uses, as opposed to one third literal uses. In the case of ins Wasser fallen, the two meanings exhibit the same morpho-syntactic surface form. However, sometimes the surface form may help to distinguish the different idiomatic vs. literal uses. Quite often, morpho-syntactic features also support a separation of ""homonymous"" idioms, which have the same lexical items as components, or of different (idiomatic) readings of a ""polysemous"" idiom (see Section 5.2. below) . An example of homography is the German idiom in Gang kommen which means ""to be set in motion"" when it appears in singular form without determiner, while the same used in plural form with definite article in die Gänge kommen, bear the meaning ""to get organised"". A literal meaning is also thinkable, e.g. in singular with definite article in den Gang kommen, where it would mean something like ""to reach the hallway"". These examples show that it is not sufficient to handle MWEs solely on the basis of the lemmas of their components, but that their context and surface form has also to be taken into account. To our knowledge, (Katz and Giesbrecht, 2006) were so far the only authors who investigated the automatic identification of idiomatic vs. literal uses of German MWEs. For English, however, there has been some more work in this field recently: this includes unsupervised methods like e.g. (Sporleder and Li, 2009) who make use of lexical cohesion in order to recognise different uses of idiomatic MWEs or (Fazly et al., 2009) who use combined knowledge of canonical forms and context information; there have also been supervised methods like (Diab and Bhutada, 2009), who used the MWEs' context and surface form features in a classification approach based on machine learning.","How can idiomatic and literal uses of MWEs be automatically distinguished, and what are some methods used for English?","It is possible to automatically distinguish the idiomatic from the literal use by using latent semantic analysis, as demonstrated by Katz and Giesbrecht (2006). For English, there has been some more work in this field recently. This includes unsupervised methods like Sporleder and Li (2009), who make use of lexical cohesion in order to recognize different uses of idiomatic MWEs, or Fazly et al. (2009), who use combined knowledge of canonical forms and context information. There have also been supervised methods like Diab and Bhutada (2009), who used the MWEs' context and surface form features in a classification approach based on machine learning.","How can idiomatic and literal uses of multiword expressions (MWEs) be automatically distinguished, and what are some methods used for English?","Distinguishing between idiomatic and literal uses of multiword expressions (MWEs) can be achieved through various computational methods. One approach is latent semantic analysis, as demonstrated by Katz and Giesbrecht (2006). For English, recent advancements have introduced both unsupervised and supervised methods. Unsupervised methods include the work of Sporleder and Li (2009), who utilize lexical cohesion to identify different uses of idiomatic MWEs, and Fazly et al. (2009), who combine knowledge of canonical forms with context information. On the other hand, supervised methods, such as those by Diab and Bhutada (2009), employ machine learning techniques that leverage the context and surface form features of MWEs for classification purposes.",7,4,7,4,"Question:

How can idiomatic and literal uses of MWEs be automatically distinguished, and what are some methods used for English?

Answer:

It is possible to automatically distinguish the idiomatic from the literal use by using latent semantic analysis, as demonstrated by Katz and Giesbrecht (2006). For English, there has been some more work in this field recently. This includes unsupervised methods like Sporleder and Li (2009), who make use of lexical cohesion in order to recognize different uses of idiomatic MWEs, or Fazly et al. (2009), who use combined knowledge of canonical forms and context information. There have also been supervised methods like Diab and Bhutada (2009), who used the MWEs' context and surface form features in a classification approach based on machine learning.","Question:

How can idiomatic and literal uses of multiword expressions (MWEs) be automatically distinguished, and what are some methods used for English?

Answer:

Distinguishing between idiomatic and literal uses of multiword expressions (MWEs) can be achieved through various computational methods. One approach is latent semantic analysis, as demonstrated by Katz and Giesbrecht (2006). For English, recent advancements have introduced both unsupervised and supervised methods. Unsupervised methods include the work of Sporleder and Li (2009), who utilize lexical cohesion to identify different uses of idiomatic MWEs, and Fazly et al. (2009), who combine knowledge of canonical forms with context information. On the other hand, supervised methods, such as those by Diab and Bhutada (2009), employ machine learning techniques that leverage the context and surface form features of MWEs for classification purposes.",NO,True,754,True,True
216552915-s4,When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?,2020-04-25,Dimension Selection,"To measure beliefs, one first must select a dimension along which the belief is assumed to be held. Much of the literature has focused on dimensions related to gender or race. Others, however, have seen value in moving beyond these dimensions (Agarwal et al., 2019;Sweeney and Najafian, 2019). Most relevant is the work of Kozlowski et al. (2019), who study the association of 59 concepts across 20 different dimensions of sociocultural meaning, and that of An et al. (2018), who induce 732 different dimensions using WordNet to study contextual effects of linguistic meaning. While neither work focuses heavily on identities, these efforts compliment our goal of studying a broad range of dimensions of social meaning.

Scholars then identify a direction within the embedding that represents this dimension. To do so, an approach similar to the semantic differential idea is used. Terms are selected to represent the two ends of the dimension. For example, to identify the gender direction, words at one end might be he and him, and words at the other end, she and her. Scholarship varies on how these dimensioninducing word sets are selected. For example, several scholars have used demographically gendered and/or racialized names (Bolukbasi et al., 2016;Caliskan et al., 2017), while others have relied on careful extraction of concepts from dictionaries and thesauri (Kozlowski et al., 2019). Kozlowski et al. (2019) find that having more words at each end generally provides better measurements, and others have found a need to use frequently occurring terms (Ethayarajh et al., 2019;Brunet et al., 2019). Beyond these observations, however, scholars have generally found stable results as long as reasonable word sets are selected.

Word Position Measurement Finally, the position of each identity along this direction must be identified. Doing so entails two major deci-sions. First, how should one quantify the direction, given the dimension-inducing words? For example, Bolukbasi et al. (2016) identify the direction by taking the first dimension of a PCA on the full set of direction words. Second, how should one define the position of points along this line? For example, several works use the cosine similarity between the identified ""bias direction"" and the embedding of each identity. Scholars have also recently proposed supervised methods for word position measurement (Sweeney and Najafian, 2019;Agarwal et al., 2019). Such approaches are important, but assume the existence of some training data, which may or may not be available in certain measurement contexts. We therefore do not explore these methods further in the present work.

In sum, using embeddings to measure beliefs requires a series of decisions, the impacts of which are still debated. Below, we provide the most comprehensive study to date on the importance of these decisions on measurement quality.",What are the major steps and considerations in using word embeddings to measure beliefs about social dimensions?,"To measure beliefs, one first must select a dimension along which the belief is assumed to be held. Much of the literature has focused on dimensions related to gender or race. Others, however, have seen value in moving beyond these dimensions (Agarwal et al., 2019; Sweeney and Najafian, 2019). Most relevant is the work of Kozlowski et al. (2019), who study the association of 59 concepts across 20 different dimensions of sociocultural meaning, and that of An et al. (2018), who induce 732 different dimensions using WordNet to study contextual effects of linguistic meaning. While neither work focuses heavily on identities, these efforts complement the goal of studying a broad range of dimensions of social meaning. Scholars then identify a direction within the embedding that represents this dimension. To do so, an approach similar to the semantic differential idea is used. Terms are selected to represent the two ends of the dimension. For example, to identify the gender direction, words at one end might be he and him, and words at the other end, she and her. Scholarship varies on how these dimension-inducing word sets are selected. For example, several scholars have used demographically gendered and/or racialized names (Bolukbasi et al., 2016; Caliskan et al., 2017), while others have relied on careful extraction of concepts from dictionaries and thesauri (Kozlowski et al., 2019). Kozlowski et al. (2019) find that having more words at each end generally provides better measurements, and others have found a need to use frequently occurring terms (Ethayarajh et al., 2019; Brunet et al., 2019). Beyond these observations, however, scholars have generally found stable results as long as reasonable word sets are selected. Finally, the position of each identity along this direction must be identified. Doing so entails two major decisions. First, how should one quantify the direction, given the dimension-inducing words? For example, Bolukbasi et al. (2016) identify the direction by taking the first dimension of a PCA on the full set of direction words. Second, how should one define the position of points along this line? For example, several works use the cosine similarity between the identified ""bias direction"" and the embedding of each identity. Scholars have also recently proposed supervised methods for word position measurement (Sweeney and Najafian, 2019; Agarwal et al., 2019). Such approaches are important, but assume the existence of some training data, which may or may not be available in certain measurement contexts. In sum, using embeddings to measure beliefs requires a series of decisions, the impacts of which are still debated.",What are the key steps and considerations when using word embeddings to analyze beliefs about social dimensions?,"To analyze beliefs using word embeddings, the first step is to select a social dimension along which the belief is assumed to be held. Much of the existing literature has focused on dimensions related to gender or race, but there is value in exploring beyond these dimensions (Agarwal et al., 2019; Sweeney and Najafian, 2019). Notably, Kozlowski et al. (2019) examined the association of 59 concepts across 20 different dimensions of sociocultural meaning, while An et al. (2018) used WordNet to induce 732 different dimensions to study contextual effects of linguistic meaning. Although these works do not focus heavily on identities, they complement the goal of studying a broad range of social meanings.

Once a dimension is selected, scholars identify a direction within the embedding that represents this dimension. This process is similar to the semantic differential approach, where terms are chosen to represent the two ends of the dimension. For instance, to identify the gender direction, words like ""he"" and ""him"" might be used at one end, and ""she"" and ""her"" at the other. The selection of these dimension-inducing word sets varies among scholars. Some have used demographically gendered and/or racialized names (Bolukbasi et al., 2016; Caliskan et al., 2017), while others have carefully extracted concepts from dictionaries and thesauri (Kozlowski et al., 2019). Kozlowski et al. (2019) found that having more words at each end generally provides better measurements, and others have emphasized the need to use frequently occurring terms (Ethayarajh et al., 2019; Brunet et al., 2019). Despite these variations, scholars have generally found stable results as long as reasonable word sets are selected.

Finally, the position of each identity along this direction must be determined. This involves two major decisions. First, how should the direction be quantified, given the dimension-inducing words? For example, Bolukbasi et al. (2016) identified the direction by taking the first dimension of a PCA on the full set of direction words. Second, how should the position of points along this line be defined? Several works use the cosine similarity between the identified ""bias direction"" and the embedding of each identity. Recently, supervised methods for word position measurement have been proposed (Sweeney and Najafian, 2019; Agarwal et al., 2019). These approaches are important but assume the existence of some training data, which may not always be available in certain measurement contexts.

In summary, using embeddings to measure beliefs involves a series of decisions, the impacts of which are still debated in the field.",7,7,7,6,"Question:

What are the major steps and considerations in using word embeddings to measure beliefs about social dimensions?

Answer:

To measure beliefs, one first must select a dimension along which the belief is assumed to be held. Much of the literature has focused on dimensions related to gender or race. Others, however, have seen value in moving beyond these dimensions (Agarwal et al., 2019; Sweeney and Najafian, 2019). Most relevant is the work of Kozlowski et al. (2019), who study the association of 59 concepts across 20 different dimensions of sociocultural meaning, and that of An et al. (2018), who induce 732 different dimensions using WordNet to study contextual effects of linguistic meaning. While neither work focuses heavily on identities, these efforts complement the goal of studying a broad range of dimensions of social meaning. Scholars then identify a direction within the embedding that represents this dimension. To do so, an approach similar to the semantic differential idea is used. Terms are selected to represent the two ends of the dimension. For example, to identify the gender direction, words at one end might be he and him, and words at the other end, she and her. Scholarship varies on how these dimension-inducing word sets are selected. For example, several scholars have used demographically gendered and/or racialized names (Bolukbasi et al., 2016; Caliskan et al., 2017), while others have relied on careful extraction of concepts from dictionaries and thesauri (Kozlowski et al., 2019). Kozlowski et al. (2019) find that having more words at each end generally provides better measurements, and others have found a need to use frequently occurring terms (Ethayarajh et al., 2019; Brunet et al., 2019). Beyond these observations, however, scholars have generally found stable results as long as reasonable word sets are selected. Finally, the position of each identity along this direction must be identified. Doing so entails two major decisions. First, how should one quantify the direction, given the dimension-inducing words? For example, Bolukbasi et al. (2016) identify the direction by taking the first dimension of a PCA on the full set of direction words. Second, how should one define the position of points along this line? For example, several works use the cosine similarity between the identified ""bias direction"" and the embedding of each identity. Scholars have also recently proposed supervised methods for word position measurement (Sweeney and Najafian, 2019; Agarwal et al., 2019). Such approaches are important, but assume the existence of some training data, which may or may not be available in certain measurement contexts. In sum, using embeddings to measure beliefs requires a series of decisions, the impacts of which are still debated.","Question:

What are the key steps and considerations when using word embeddings to analyze beliefs about social dimensions?

Answer:

To analyze beliefs using word embeddings, the first step is to select a social dimension along which the belief is assumed to be held. Much of the existing literature has focused on dimensions related to gender or race, but there is value in exploring beyond these dimensions (Agarwal et al., 2019; Sweeney and Najafian, 2019). Notably, Kozlowski et al. (2019) examined the association of 59 concepts across 20 different dimensions of sociocultural meaning, while An et al. (2018) used WordNet to induce 732 different dimensions to study contextual effects of linguistic meaning. Although these works do not focus heavily on identities, they complement the goal of studying a broad range of social meanings.

Once a dimension is selected, scholars identify a direction within the embedding that represents this dimension. This process is similar to the semantic differential approach, where terms are chosen to represent the two ends of the dimension. For instance, to identify the gender direction, words like ""he"" and ""him"" might be used at one end, and ""she"" and ""her"" at the other. The selection of these dimension-inducing word sets varies among scholars. Some have used demographically gendered and/or racialized names (Bolukbasi et al., 2016; Caliskan et al., 2017), while others have carefully extracted concepts from dictionaries and thesauri (Kozlowski et al., 2019). Kozlowski et al. (2019) found that having more words at each end generally provides better measurements, and others have emphasized the need to use frequently occurring terms (Ethayarajh et al., 2019; Brunet et al., 2019). Despite these variations, scholars have generally found stable results as long as reasonable word sets are selected.

Finally, the position of each identity along this direction must be determined. This involves two major decisions. First, how should the direction be quantified, given the dimension-inducing words? For example, Bolukbasi et al. (2016) identified the direction by taking the first dimension of a PCA on the full set of direction words. Second, how should the position of points along this line be defined? Several works use the cosine similarity between the identified ""bias direction"" and the embedding of each identity. Recently, supervised methods for word position measurement have been proposed (Sweeney and Najafian, 2019; Agarwal et al., 2019). These approaches are important but assume the existence of some training data, which may not always be available in certain measurement contexts.

In summary, using embeddings to measure beliefs involves a series of decisions, the impacts of which are still debated in the field.",NO,True,2649,True,True
218487374-s5,Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates,2020-05-01,Measuring confounders via text,"After drawing the causal graph, the next step is to use available text data to recover latent confounders. Some approaches pre-specify the confounders of interest and measure them from text, P (z | x). Others learn confounders inductively and use a low-dimensional representation of text as the confounding variable z in subsequent causal adjustments.

Pre-specified confounders. When a practitioner can specify confounders they want to measure from text (e.g., extracting ""occupation"" from text in our smoking example), they can use either (1) lexicons or (2) 2019) also build machine learning classifiers for users' mental states (e.g., depression and anxiety) and apply these classifiers on Twitter posts that are temporally prior to treatment. If these classifiers accurately recover mental states and there are no additional latent confounders, then conditioning on the measured mental states renders treatment independent of potential outcomes.

Open problems: Since NLP methods are still far from perfectly accurate, how can one mitigate error that arises from approximating confounding variables? Closely related to this question is effect restoration which addresses error from using proxy variables (e.g., a father's occupation) in place of true confounders (e.g, socioeconomic status) (Kuroki and Pearl, 2014;Oktay et al., 2019). Wood-Doughty et al. (2018) build upon effect restoration for causal inference with text classifiers, but there are still open problems in accounting for error arising from other text representations and issues of calibration (Nguyen and O'Connor, 2015) and prevalence estimation (Card and Smith, 2018; Keith and O'Connor, 2018) in conjunction with NLP. Ideas from the large literature on measurement error models may also be helpful (Fuller, 1987;Carroll et al., 2006;Buonaccorsi, 2010).

Inductively derived confounders. Other researchers inductively learn confounders in order to condition on all aspects of text, known and unknown. For example, some applications condition on the entirety of news (Johansson et al., 2016) or scientific articles (Veitch et al., 2019;Roberts et al., 2020). This approach typically summarizes textual information with text representations common in NLP. Ideally, this would encode all aspects of language (meaning, topic, style, affect, etc.), though this is an extremely difficult, open NLP problem. Typical approaches include the following. (1) Bag-of-words representations discard word order and use word counts as representations. (2) Topic models are generative probabilistic models that learn latent topics in document collections and represent documents as distributions over topics (Blei et al., 2003;Boyd-Graber et al., 2014;Roberts et al., 2014). (3) Embeddings are continuous, vector-based representations of text. To create vector representations of longer texts, off-the-shelf word embeddings such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) or combined via variants of weighted averaging (Arora et al., 2017) or neural models (Iyyer et al., 2015;Bojanowski et al., 2017;Yang et al., 2016). (4) Recently, fine-tuned, large-scale neural language models such as BERT (Devlin et al., 2019) have achieved state-of-the-art performance on semantic benchmarks, and are now used as text representations. Each of these text representations is a real-valued vector that is used in place of the confounder, z, in a causal adjustment method ( §5)

Open problems: Estimates of causal effects are contingent on the ""garden of forking paths"" of data analysis, meaning any ""paths"" an analyst did not take could have resulted in different conclusions (Gelman and Loken, 2013). For settings with causal confounders from text, the first fork is the choice of representation (e.g., topic models or embeddings) and the second fork is the pre-processing and hyperparameter decisions for the chosen representations.

We highlight that these decisions have been shown to alter results in predictive tasks. For instance, studies have shown that pre-processing decisions dramatically change topic models (Denny and Spirling, 2018;Schofield et al., 2017); embeddings are sensitive to hyperparameter tuning (Levy et al., 2015) and the construction of the training corpus (Antoniak and Mimno, 2018); and fine-tuned language model performance is sensitive to random restarts (Phang et al., 2018). Thus, reporting sensitivity analysis of the causal effects from these decisions seems crucial: how robust are the results to variations in modeling specifications?","What are the typical approaches to text representation in NLP, and how do they differ in handling textual information?","Typical approaches to text representation in NLP include bag-of-words representations, topic models, embeddings, and fine-tuned, large-scale neural language models such as BERT (Devlin et al., 2019). Bag-of-words representations discard word order and use word counts as representations. Topic models are generative probabilistic models that learn latent topics in document collections and represent documents as distributions over topics (Blei et al., 2003; Boyd-Graber et al., 2014; Roberts et al., 2014). Embeddings are continuous, vector-based representations of text. To create vector representations of longer texts, off-the-shelf word embeddings such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) are used, or they are combined via variants of weighted averaging (Arora et al., 2017) or neural models (Iyyer et al., 2015; Bojanowski et al., 2017; Yang et al., 2016). Recently, fine-tuned, large-scale neural language models such as BERT have achieved state-of-the-art performance on semantic benchmarks and are now used as text representations. Each of these text representations is a real-valued vector that is used in place of the confounder, z, in a causal adjustment method.","What are the common methods for text representation in NLP, and how do they differ in processing textual information?","In natural language processing (NLP), several common methods are used for text representation, each handling textual information differently. One of the simplest approaches is the bag-of-words model, which represents text by counting word occurrences, disregarding word order. Another method is topic modeling, which uses generative probabilistic models to identify latent topics within document collections, representing documents as distributions over these topics (Blei et al., 2003; Boyd-Graber et al., 2014; Roberts et al., 2014).

Embeddings offer a more sophisticated approach, providing continuous, vector-based representations of text. Word embeddings like word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) are popular for creating vector representations of words, which can be combined for longer texts using techniques like weighted averaging (Arora et al., 2017) or neural models (Iyyer et al., 2015; Bojanowski et al., 2017; Yang et al., 2016).

Recently, fine-tuned, large-scale neural language models such as BERT (Devlin et al., 2019) have set new benchmarks in semantic understanding, offering state-of-the-art performance. These models are now widely used for text representation. Each of these methods ultimately produces a real-valued vector that can be utilized in various applications, such as causal adjustment methods, replacing traditional confounders.",9,7,9,7,"Question:

What are the typical approaches to text representation in NLP, and how do they differ in handling textual information?

Answer:

Typical approaches to text representation in NLP include bag-of-words representations, topic models, embeddings, and fine-tuned, large-scale neural language models such as BERT (Devlin et al., 2019). Bag-of-words representations discard word order and use word counts as representations. Topic models are generative probabilistic models that learn latent topics in document collections and represent documents as distributions over topics (Blei et al., 2003; Boyd-Graber et al., 2014; Roberts et al., 2014). Embeddings are continuous, vector-based representations of text. To create vector representations of longer texts, off-the-shelf word embeddings such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) are used, or they are combined via variants of weighted averaging (Arora et al., 2017) or neural models (Iyyer et al., 2015; Bojanowski et al., 2017; Yang et al., 2016). Recently, fine-tuned, large-scale neural language models such as BERT have achieved state-of-the-art performance on semantic benchmarks and are now used as text representations. Each of these text representations is a real-valued vector that is used in place of the confounder, z, in a causal adjustment method.","Question:

What are the common methods for text representation in NLP, and how do they differ in processing textual information?

Answer:

In natural language processing (NLP), several common methods are used for text representation, each handling textual information differently. One of the simplest approaches is the bag-of-words model, which represents text by counting word occurrences, disregarding word order. Another method is topic modeling, which uses generative probabilistic models to identify latent topics within document collections, representing documents as distributions over these topics (Blei et al., 2003; Boyd-Graber et al., 2014; Roberts et al., 2014).

Embeddings offer a more sophisticated approach, providing continuous, vector-based representations of text. Word embeddings like word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) are popular for creating vector representations of words, which can be combined for longer texts using techniques like weighted averaging (Arora et al., 2017) or neural models (Iyyer et al., 2015; Bojanowski et al., 2017; Yang et al., 2016).

Recently, fine-tuned, large-scale neural language models such as BERT (Devlin et al., 2019) have set new benchmarks in semantic understanding, offering state-of-the-art performance. These models are now widely used for text representation. Each of these methods ultimately produces a real-valued vector that can be utilized in various applications, such as causal adjustment methods, replacing traditional confounders.",NO,True,1395,True,True
218971825-s10,"Language (Technology) is Power: A Critical Survey of ""Bias"" in NLP",2020-05-28,Case study,"To illustrate our recommendations, we present a case study covering work on African-American English (AAE). 5 Work analyzing ""bias"" in the context of AAE has shown that part-of-speech taggers, language identifcation systems, and dependency parsers all work less well on text containing features associated with AAE than on text without these features (Jørgensen et al., , 2016, and that toxicity detection systems score tweets containing features associated with AAE as more offensive than tweets without them .

These papers have been critical for highlighting AAE as a language variety for which existing NLP systems may not work, illustrating their limitations. However, they do not conceptualize ""racial bias"" in the same way. The frst four of these papers simply focus on system performance differences between text containing features associated with AAE and text without these features. In contrast, the last two papers also focus on such system performance differences, but motivate this focus with the following additional reasoning: If tweets containing features associated with AAE are scored as more offensive than tweets without these features, then this might (a) yield negative perceptions of AAE; (b) result in disproportionate removal of tweets containing these features, impeding participation in online platforms and reducing the space available online in which speakers can use AAE freely; and (c) cause AAE speakers to incur additional costs if they have to change their language practices to avoid negative perceptions or tweet removal.

More importantly, none of these papers engage with the literature on AAE, racial hierarchies in the U.S., and raciolinguistic ideologies. By failing to engage with this literature-thereby treating AAE simply as one of many non-Penn Treebank varieties of English or perhaps as another challenging domain-work analyzing ""bias"" in NLP systems in the context of AAE fails to situate these systems in the world. Who are the speakers of AAE? How are they viewed? We argue that AAE as a language variety cannot be separated from its speakersprimarily Black people in the U.S., who experience systemic anti-Black racism-and the language ideologies that reinforce and justify racial hierarchies.

Even after decades of sociolinguistic efforts to legitimize AAE, it continues to be viewed as ""bad"" English and its speakers continue to be viewed as linguistically inadequate-a view called the defcit perspective (Alim et al., 2016;Rosa and Flores, 2017). This perspective persists despite demonstrations that AAE is rule-bound and grammatical (Mufwene et al., 1998;Green, 2002), in addition to ample evidence of its speakers' linguistic adroitness (e.g., Alim, 2004;Rickford and King, 2016). This perspective belongs to a broader set of raciolinguistic ideologies (Rosa and Flores, 2017), which also produce allocational harms; speakers of AAE are frequently penalized for not adhering to dominant language practices, including in the education system (Alim, 2004;Terry et al., 2010), when seeking housing (Baugh, 2018), and in the judicial system, where their testimony is misunderstood or, worse yet, disbelieved (Rickford and King, 2016;Jones et al., 2019). These raciolinguistic ideologies position racialized communities as needing linguistic intervention, such as language education programs, in which these and other harms can be reduced if communities accommodate to dominant language practices (Rosa and Flores, 2017).

In the technology industry, speakers of AAE are often not considered consumers who matter. For example, Benjamin (2019) recounts an Apple employee who worked on speech recognition for Siri: ""As they worked on different English dialects -Australian, Singaporean, and Indian English -[the employee] asked his boss: 'What about African American English?' To this his boss responded: 'Well, Apple products are for the premium market.""'

The reality, of course, is that speakers of AAE tend not to represent the ""premium market"" precisely because of institutions and policies that help to maintain racial hierarchies by systematically denying them the opportunities to develop wealth that are available to white Americans (Rothstein, 2017)an exclusion that is reproduced in technology by countless decisions like the one described above.

Engaging with the literature outlined above situates the system behaviors that are described as ""bias,"" providing a foundation for normative reasoning. Researchers and practitioners should be concerned about ""racial bias"" in toxicity detection systems not only because performance differences impair system performance, but because they reproduce longstanding injustices of stigmatization and disenfranchisement for speakers of AAE. In re-stigmatizing AAE, they reproduce language ideologies in which AAE is viewed as ungrammatical, uneducated, and offensive. These ideologies, in turn, enable linguistic discrimination and justify enduring racial hierarchies (Rosa and Flores, 2017). Our perspective, which understands racial hierarchies and raciolinguistic ideologies as structural conditions that govern the development and deployment of technology, implies that techniques for measuring or mitigating ""bias"" in NLP systems will necessarily be incomplete unless they interrogate and dismantle these structural conditions, including the power relations between technologists and racialized communities.

We emphasize that engaging with the literature on AAE, racial hierarchies in the U.S., and raciolinguistic ideologies can generate new lines of engagement. These lines include work on the ways that the decisions made during the development and deployment of NLP systems produce stigmatization and disenfranchisement, and work on AAE use in practice, such as the ways that speakers of AAE interact with NLP systems that were not designed for them. This literature can also help researchers and practitioners address the allocational harms that may be produced by NLP systems, and ensure that even well-intentioned NLP systems do not position racialized communities as needing linguistic intervention or accommodation to dominant language practices. Finally, researchers and practitioners wishing to design better systems can also draw on a growing body of work on anti-racist language pedagogy that challenges the defcit perspective of AAE and other racialized language practices (e.g. Flores and Chaparro, 2018; Baker-Bell, 2019; Martínez and Mejía, 2019), as well as the work that we described in section 4.3 on reimagining the power relations between technologists and communities affected by technology.",How do the authors suggest addressing the limitations of current NLP systems regarding African-American English (AAE)?,"The authors suggest that engaging with the literature on AAE, racial hierarchies in the U.S., and raciolinguistic ideologies can generate new lines of engagement. These lines include work on the ways that the decisions made during the development and deployment of NLP systems produce stigmatization and disenfranchisement, and work on AAE use in practice, such as the ways that speakers of AAE interact with NLP systems that were not designed for them. This literature can also help researchers and practitioners address the allocational harms that may be produced by NLP systems, and ensure that even well-intentioned NLP systems do not position racialized communities as needing linguistic intervention or accommodation to dominant language practices. Finally, researchers and practitioners wishing to design better systems can also draw on a growing body of work on anti-racist language pedagogy that challenges the deficit perspective of AAE and other racialized language practices (e.g. Flores and Chaparro, 2018; Baker-Bell, 2019; Martínez and Mejía, 2019), as well as the work that is described in section 4.3 on reimagining the power relations between technologists and communities affected by technology.",What strategies do the authors propose to overcome the limitations of current NLP systems in handling African-American English (AAE)?,"The authors propose several strategies to address the limitations of current NLP systems concerning African-American English (AAE). They emphasize the importance of engaging with existing literature on AAE, racial hierarchies in the U.S., and raciolinguistic ideologies. This engagement can lead to new approaches, such as examining how decisions made during the development and deployment of NLP systems can result in stigmatization and disenfranchisement. Additionally, they suggest studying how AAE speakers interact with NLP systems that were not designed with them in mind. This literature can also guide researchers and practitioners in addressing the allocational harms that NLP systems might produce, ensuring that these systems do not inadvertently position racialized communities as needing linguistic intervention or accommodation to dominant language practices. Furthermore, researchers and practitioners aiming to design better systems can draw on a growing body of work on anti-racist language pedagogy, which challenges the deficit perspective of AAE and other racialized language practices, as highlighted by Flores and Chaparro (2018), Baker-Bell (2019), and Martínez and Mejía (2019). Finally, the authors point to the work described in section 4.3, which focuses on reimagining the power relations between technologists and communities affected by technology.",7,4,7,7,"Question:

How do the authors suggest addressing the limitations of current NLP systems regarding African-American English (AAE)?

Answer:

The authors suggest that engaging with the literature on AAE, racial hierarchies in the U.S., and raciolinguistic ideologies can generate new lines of engagement. These lines include work on the ways that the decisions made during the development and deployment of NLP systems produce stigmatization and disenfranchisement, and work on AAE use in practice, such as the ways that speakers of AAE interact with NLP systems that were not designed for them. This literature can also help researchers and practitioners address the allocational harms that may be produced by NLP systems, and ensure that even well-intentioned NLP systems do not position racialized communities as needing linguistic intervention or accommodation to dominant language practices. Finally, researchers and practitioners wishing to design better systems can also draw on a growing body of work on anti-racist language pedagogy that challenges the deficit perspective of AAE and other racialized language practices (e.g. Flores and Chaparro, 2018; Baker-Bell, 2019; Martínez and Mejía, 2019), as well as the work that is described in section 4.3 on reimagining the power relations between technologists and communities affected by technology.","Question:

What strategies do the authors propose to overcome the limitations of current NLP systems in handling African-American English (AAE)?

Answer:

The authors propose several strategies to address the limitations of current NLP systems concerning African-American English (AAE). They emphasize the importance of engaging with existing literature on AAE, racial hierarchies in the U.S., and raciolinguistic ideologies. This engagement can lead to new approaches, such as examining how decisions made during the development and deployment of NLP systems can result in stigmatization and disenfranchisement. Additionally, they suggest studying how AAE speakers interact with NLP systems that were not designed with them in mind. This literature can also guide researchers and practitioners in addressing the allocational harms that NLP systems might produce, ensuring that these systems do not inadvertently position racialized communities as needing linguistic intervention or accommodation to dominant language practices. Furthermore, researchers and practitioners aiming to design better systems can draw on a growing body of work on anti-racist language pedagogy, which challenges the deficit perspective of AAE and other racialized language practices, as highlighted by Flores and Chaparro (2018), Baker-Bell (2019), and Martínez and Mejía (2019). Finally, the authors point to the work described in section 4.3, which focuses on reimagining the power relations between technologists and communities affected by technology.",NO,True,1378,True,True
253447259-s1,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,The Task,"The definition of a grammatical error is surprisingly difficult. Some types of spelling errors (such as accomodation with a single m) are about equally distributed between native and non-native writers and have no grammatical reflexes, so could be reasonably excluded. Others, such as he eated, are boundary cases as they result from over-regularisation of morphology, whilst he would eated is clearly ungrammatical in the context of a modal auxiliary verb. At the interpretative boundary, infelicitous discourse organisation, such as Kim fell. Sandy pushed him. where the intention is to explain why Kim fell, is not obviously a grammatical error per se but nevertheless can be 'corrected' via a tense change (Sandy had pushed him.) as opposed to a reordering of the sentences. Other tense changes which can span sentences appear more grammatical, such as Kim will We met they talked and left We met, they talked and left Unidiomatic

We had a big conversation We had a long conversation Multiple I sea the see from the seasoar I saw the sea from the seesaw Table 1 Example error types make Sandy a sandwich. Sandy ate it., as the discourse is incoherent and correction will require a tense change in one or other sentence. In practice, the task has increasingly been defined in terms of what corrections are annotated in corpora used for the shared tasks. These use a variety of annotation schemes but all tend to adopt minimal modifications of errorful texts to create errorfree text with the same perceived meaning. Other sources of annotated data, such as that sourced from the online language learning platform Lang-8 (Mizumoto et al. 2012;Tajiri, Komachi, and Matsumoto 2012), often contain much more extensive rewrites of entire paragraphs of text. Given this resource-derived definition of the task, systems are evaluated on their ability to correct all kinds of mistakes in text, including spelling and discourse level errors that have no or little grammatical reflex. The term 'Grammatical' Error Correction is thus something of a misnomer, but is nevertheless now commonly understood to encompass errors that are not always strictly grammatical in nature. A more descriptive term is Language Error Correction. Table 1 provides a small sample of (constructed) examples that illustrate the range of errors to be corrected and some of the issues that arise with the precise definition and evaluation of the task. Errors can be classified into three broad categories: replacement errors, such as dreamed for dreamt in the second example; omission errors, such as on in the first example; and insertion errors, such as the in the third example. Some errors are complex in the sense that their correction requires a sequence of replacement, omission or insertion steps to correct, as with the syntax example. Sentences may also contain multiple distinct errors that require a sequence of corrections, as in the multiple example. Both the classification and specification of correction steps for errors can be and has been achieved using different schemes and approaches. For instance, correction of the syntax example involves transposing two adjacent words so we could introduce a fourth broad class and correction step of transposition (word order). All extant annotation schemes break these broad classes down into further subclasses based on the part-ofspeech of the words involved, and perceived morphological, lexical, syntactic, semantic or pragmatic source of the error. The schemes vary in the number of such distinctions, ranging from just over two dozen (NUCLE: (Dahlmeier, Ng, and Wu 2013)) to almost a hundred (CLC: (Nicholls 2003)). The schemes also identify different error spans in source sentences and thus suggest different sets of edit operations to obtain the suggested corrections. For instance, the agreement error example might be annotated as She likes him and [kiss → kisses] him at the token level or simply [ǫ → es] at the character level. These differing annotation decisions affected the evaluation of system performance in artefactual ways, so a two-stage automatic standardisation process was developed, ERRANT (Felice, Bryant, and Briscoe 2016;Bryant, Felice, and Briscoe 2017), which maps parallel errorful and corrected sentence pairs to a single annotation scheme using a linguistically-enhanced alignment algorithm and series of error type classification rules. This scheme uses 25 main error type categories, based primarily on part-of-speech and morphology, which are further subdivided into missing (omission), unnecessary (insertion) and replacement errors. This approach allows consistent automated training and evaluation of systems on any or all parallel corpora as well as supporting a more fined-grained analysis of the strengths and weaknesses of systems in terms of different error types.

Ultimately however, the correction of errors requires an understanding of the communicative intention of the writer. For instance, the determiner example in Table 1 implicitly assumes a 'neutral' context where the intent is to make a statement about generic icecream rather than a specific instance. In a context where, say, a specific ice-cream dessert is being compared to an alternative dessert, then the determiner is felicitous. Similarly the preposition omission error might not be an error if the writer is describing a context in which a talk was oversubscribed and many attendees had to stand because of a lack of seats. Though annotators will most likely take both the context and perceived writer's intention into account when identifying errors, GEC itself is instead often framed as an isolated sentence-based task that ignores the wider context. This can introduce noise in the task in that errorful sequences in context may appear correct in isolation out of context. A related issue is that correction may not only depend on communicative intent, but also factors such as dialect and genre. For example, correcting dreamed to dreamt may be appropriate if the target is British English, but incorrect for American English.

A larger issue arises with differing possibilities for correction. For example, correcting the tense/aspect example to kissing or to kiss in the context of likes seems equally correct. However, few existing corpora provide more than one possibility which means the true performance of systems is often underestimated. However, the same two corrections are not equally correct as complements of a verb such as try depending on whether the context implies that a kissing event occurred or not. The issue of multiple possible corrections arises with many if not most examples: for instance I haven't the book, We met them, talked and left, We had an important conversation, The sea I see from the seesaw (is calm) are all plausible alternative corrections for some of the examples in Table 1. For this reason, several of the shared tasks have also evaluated performance on grammatical error detection, as this is valuable in some applications. Recently, some work has explored treating the GEC task as one of document-level correction (e.g. Chollampatt, Wang, and Ng (2019); ) which, in principle, could ameliorate some of these issues but is currently hampered by a lack of appropriately structured corpora.","How are errors classified and corrected in grammatical error correction, and what challenges arise with annotation schemes?","Errors in grammatical error correction are classified into three broad categories: replacement errors, omission errors, and insertion errors. Some errors are complex and require a sequence of replacement, omission, or insertion steps to correct. Sentences may also contain multiple distinct errors that require a sequence of corrections. Both the classification and specification of correction steps for errors can be achieved using different schemes and approaches. All extant annotation schemes break these broad classes down into further subclasses based on the part-of-speech of the words involved, and perceived morphological, lexical, syntactic, semantic, or pragmatic source of the error. The schemes vary in the number of such distinctions, ranging from just over two dozen to almost a hundred. The schemes also identify different error spans in source sentences and suggest different sets of edit operations to obtain the suggested corrections. These differing annotation decisions affect the evaluation of system performance in artefactual ways. A two-stage automatic standardisation process, ERRANT, was developed to map parallel errorful and corrected sentence pairs to a single annotation scheme using a linguistically-enhanced alignment algorithm and a series of error type classification rules. This approach allows consistent automated training and evaluation of systems on any or all parallel corpora and supports a more fine-grained analysis of the strengths and weaknesses of systems in terms of different error types. Ultimately, the correction of errors requires an understanding of the communicative intention of the writer. Though annotators will most likely take both the context and perceived writer's intention into account when identifying errors, grammatical error correction is often framed as an isolated sentence-based task that ignores the wider context. This can introduce noise in the task, as errorful sequences in context may appear correct in isolation out of context. A related issue is that correction may not only depend on communicative intent but also factors such as dialect and genre. A larger issue arises with differing possibilities for correction, as few existing corpora provide more than one possibility, which means the true performance of systems is often underestimated.","How are errors classified and corrected in grammatical error correction, and what challenges arise with annotation schemes?","In grammatical error correction, errors are typically classified into three main categories: replacement errors, omission errors, and insertion errors. Some errors are complex and require a sequence of these steps to be corrected, while sentences may contain multiple distinct errors needing a series of corrections. Classification and correction of these errors are achieved through various annotation schemes, which break down these broad categories into further subclasses. These subclasses are based on factors such as the part-of-speech of the words involved and the perceived morphological, lexical, syntactic, semantic, or pragmatic source of the error. The number of distinctions in these schemes can range from just over two dozen to nearly a hundred. They also identify different error spans in source sentences and suggest different sets of edit operations for corrections. These varying annotation decisions can affect the evaluation of system performance in artefactual ways.

To address these challenges, a two-stage automatic standardization process called ERRANT was developed. ERRANT maps parallel errorful and corrected sentence pairs to a single annotation scheme using a linguistically-enhanced alignment algorithm and a series of error type classification rules. This approach enables consistent automated training and evaluation of systems across any or all parallel corpora and supports a more detailed analysis of system strengths and weaknesses concerning different error types.

Ultimately, correcting errors requires understanding the writer's communicative intention. Annotators often consider both context and perceived writer's intention when identifying errors. However, grammatical error correction is frequently framed as an isolated sentence-based task, ignoring the wider context. This can introduce noise, as sequences that appear errorful in context may seem correct in isolation. Additionally, correction may depend on factors such as dialect and genre. A significant issue is the limited possibilities for correction, as few existing corpora offer more than one correction option, leading to an underestimation of systems' true performance.",8,4,7,4,"Question:

How are errors classified and corrected in grammatical error correction, and what challenges arise with annotation schemes?

Answer:

Errors in grammatical error correction are classified into three broad categories: replacement errors, omission errors, and insertion errors. Some errors are complex and require a sequence of replacement, omission, or insertion steps to correct. Sentences may also contain multiple distinct errors that require a sequence of corrections. Both the classification and specification of correction steps for errors can be achieved using different schemes and approaches. All extant annotation schemes break these broad classes down into further subclasses based on the part-of-speech of the words involved, and perceived morphological, lexical, syntactic, semantic, or pragmatic source of the error. The schemes vary in the number of such distinctions, ranging from just over two dozen to almost a hundred. The schemes also identify different error spans in source sentences and suggest different sets of edit operations to obtain the suggested corrections. These differing annotation decisions affect the evaluation of system performance in artefactual ways. A two-stage automatic standardisation process, ERRANT, was developed to map parallel errorful and corrected sentence pairs to a single annotation scheme using a linguistically-enhanced alignment algorithm and a series of error type classification rules. This approach allows consistent automated training and evaluation of systems on any or all parallel corpora and supports a more fine-grained analysis of the strengths and weaknesses of systems in terms of different error types. Ultimately, the correction of errors requires an understanding of the communicative intention of the writer. Though annotators will most likely take both the context and perceived writer's intention into account when identifying errors, grammatical error correction is often framed as an isolated sentence-based task that ignores the wider context. This can introduce noise in the task, as errorful sequences in context may appear correct in isolation out of context. A related issue is that correction may not only depend on communicative intent but also factors such as dialect and genre. A larger issue arises with differing possibilities for correction, as few existing corpora provide more than one possibility, which means the true performance of systems is often underestimated.","Question:

How are errors classified and corrected in grammatical error correction, and what challenges arise with annotation schemes?

Answer:

In grammatical error correction, errors are typically classified into three main categories: replacement errors, omission errors, and insertion errors. Some errors are complex and require a sequence of these steps to be corrected, while sentences may contain multiple distinct errors needing a series of corrections. Classification and correction of these errors are achieved through various annotation schemes, which break down these broad categories into further subclasses. These subclasses are based on factors such as the part-of-speech of the words involved and the perceived morphological, lexical, syntactic, semantic, or pragmatic source of the error. The number of distinctions in these schemes can range from just over two dozen to nearly a hundred. They also identify different error spans in source sentences and suggest different sets of edit operations for corrections. These varying annotation decisions can affect the evaluation of system performance in artefactual ways.

To address these challenges, a two-stage automatic standardization process called ERRANT was developed. ERRANT maps parallel errorful and corrected sentence pairs to a single annotation scheme using a linguistically-enhanced alignment algorithm and a series of error type classification rules. This approach enables consistent automated training and evaluation of systems across any or all parallel corpora and supports a more detailed analysis of system strengths and weaknesses concerning different error types.

Ultimately, correcting errors requires understanding the writer's communicative intention. Annotators often consider both context and perceived writer's intention when identifying errors. However, grammatical error correction is frequently framed as an isolated sentence-based task, ignoring the wider context. This can introduce noise, as sequences that appear errorful in context may seem correct in isolation. Additionally, correction may depend on factors such as dialect and genre. A significant issue is the limited possibilities for correction, as few existing corpora offer more than one correction option, leading to an underestimation of systems' true performance.",NO,True,2179,True,True
253447259-s4,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Annotation Challenges,"As mentioned in Section 1.1, the notion of a grammatical error is hard to define as different errors may have different scope (e.g. local vs. contextual), complexity (e.g. orthographic vs. semantic) and corrections (e.g. [this books → this book] vs. [this books → these books]. Human annotation is thus an extremely cognitively demanding task and so clear annotation guidelines are a crucial component of dataset quality. This section briefly outlines three important aspects of data collection: Minimal vs. Fluent Corrections, Annotation Consistency, and Preprocessing Challenges.

Minimal vs. Fluent Corrections. Most GEC corpora have been annotated on the principle of minimal corrections, i.e. annotators should make the minimum number of changes to make a text grammatical. Sakaguchi et al. (2016) argue, however, that this can often lead to corrections that sound unnatural, and so it would be better to annotate corpora on the principle of fluent corrections instead. Consider the following example:

Original I want explain to you some interesting part from my experience.

Minimal I want to explain to you some interesting parts of my experience. Fluent I want to tell you about some interesting parts of my experience.

While the minimal correction primarily inserts a missing infinitival to before explain to make the sentence grammatical, the fluent correction also changes explain to tell you about because it is more idiomatic to tell someone about an experience rather than explain an experience. One of the main challenges of this distinction, however, is that it is very difficult to draw a line between what constitutes a minimal correction and what constitutes a fluent correction. This is because minimal corrections (e.g. missing determiners) are a subset of fluent corrections, and so there cannot be fluent corrections without minimal corrections. It is also the case that minimal corrections are typically easier to make than fluent corrections (for both humans and machines), although it is undeniable that fluent corrections are the more desirable outcome. Ultimately, although it is very difficult to precisely define a fluent correction, annotation guidelines should nevertheless attempt to make clear the extent to which annotators are expected to edit.

Annotation Consistency. A significant challenge of human annotation is that corrections are subjective and there is often more than one way to correct a sentence (Bryant and Ng 2015;Choshen and Abend 2018b). It is nevertheless important that annotators attempt to be consistent in their judgements, especially if they are explicitly annotating edit spans. For example the edit [has eating → was eaten] can also be represented as [has → was] and [eating → eaten], and this choice not only affects data exploration and analysis, but can also have an impact on edit-based evaluation. Similarly, the edit [the informations → information] can also be represented as [the → ǫ] and [informations → information], but the latter may be more intuitive because it represents two independent edits of clearly distinct types. Explicit error type classification is thus another important aspect of annotator consistency, as an error type framework (if any) not only increases the cognitive burden on the annotator, but also might influence an annotator towards a particular correction given the error types that are available . Ultimately, if annotators are tasked with explicitly defining the edits they make to correct a sentence, annotator guidelines must clearly define the notion of an edit.

Preprocessing Challenges. While human annotators are trained to correct natural text, GEC systems are typically trained to correct word tokenised sentences (mainly for evaluation purposes). This mismatch means human annotations typically undergo several preprocessing steps in order to produce the desired output format (Bryant and Felice 2016). The first of these transformations involves converting character-level edits to token-level edits. While this is often straightforward, it can sometimes be the case that a human-annotated character span does not map to a complete token; e.g. [ing → ed] to denote the edit [dancing → danced]. Although such cases can often (but not always) be resolved automatically, e.g., by expanding the character spans of the edit or calculating token alignment, they can also be reduced by training annotators to explicitly annotate longer spans rather than sub-words.

The second transformation involves sentence tokenisation, which is potentially more complex given human edits may change sentence boundaries; e.g. [A. B, C. → A, B. C.]. Sentences are nevertheless typically tokenised based solely on the original text, with the acknowledgement that some may be sentence fragments (to be joined with the following sentence) and that edits which cross sentence boundaries are ignored (e.g. [. Because → ,  because]. It is worth noting that this issue only affects sentence-based GEC systems (the vast majority) but paragraph or document-based systems are unaffected.",What are the challenges and transformations involved in preprocessing human annotations for GEC systems?,"While human annotators are trained to correct natural text, GEC systems are typically trained to correct word tokenised sentences (mainly for evaluation purposes). This mismatch means human annotations typically undergo several preprocessing steps in order to produce the desired output format (Bryant and Felice 2016). The first of these transformations involves converting character-level edits to token-level edits. While this is often straightforward, it can sometimes be the case that a human-annotated character span does not map to a complete token; e.g. [ing → ed] to denote the edit [dancing → danced]. Although such cases can often (but not always) be resolved automatically, e.g., by expanding the character spans of the edit or calculating token alignment, they can also be reduced by training annotators to explicitly annotate longer spans rather than sub-words. The second transformation involves sentence tokenisation, which is potentially more complex given human edits may change sentence boundaries; e.g. [A. B, C. → A, B. C.]. Sentences are nevertheless typically tokenised based solely on the original text, with the acknowledgement that some may be sentence fragments (to be joined with the following sentence) and that edits which cross sentence boundaries are ignored (e.g. [. Because → , because]. It is worth noting that this issue only affects sentence-based GEC systems (the vast majority) but paragraph or document-based systems are unaffected.",What challenges and transformations are involved in preprocessing human annotations for Grammatical Error Correction (GEC) systems?,"Preprocessing human annotations for GEC systems involves several challenges and transformations due to the inherent differences between human and system processing. Human annotators are trained to correct natural text, whereas GEC systems are typically trained to correct word-tokenized sentences, primarily for evaluation purposes. This discrepancy necessitates multiple preprocessing steps to align human annotations with the desired output format (Bryant and Felice 2016).

The first transformation involves converting character-level edits to token-level edits. While this process is often straightforward, complications arise when a human-annotated character span does not correspond to a complete token. For example, the edit [ing → ed] to change [dancing → danced] may not map directly to token-level edits. Although such cases can often be resolved automatically by expanding the character spans of the edit or calculating token alignment, they can also be mitigated by training annotators to explicitly annotate longer spans rather than sub-words.

The second transformation involves sentence tokenization, which can be more complex because human edits may alter sentence boundaries. For instance, an edit like [A. B, C. → A, B. C.] changes the sentence structure. Typically, sentences are tokenized based solely on the original text, acknowledging that some may be sentence fragments that need to be joined with the following sentence. Edits that cross sentence boundaries, such as [. Because → , because], are generally ignored. This issue primarily affects sentence-based GEC systems, which are the majority, while paragraph or document-based systems remain unaffected.",7,4,7,4,"Question:

What are the challenges and transformations involved in preprocessing human annotations for GEC systems?

Answer:

While human annotators are trained to correct natural text, GEC systems are typically trained to correct word tokenised sentences (mainly for evaluation purposes). This mismatch means human annotations typically undergo several preprocessing steps in order to produce the desired output format (Bryant and Felice 2016). The first of these transformations involves converting character-level edits to token-level edits. While this is often straightforward, it can sometimes be the case that a human-annotated character span does not map to a complete token; e.g. [ing → ed] to denote the edit [dancing → danced]. Although such cases can often (but not always) be resolved automatically, e.g., by expanding the character spans of the edit or calculating token alignment, they can also be reduced by training annotators to explicitly annotate longer spans rather than sub-words. The second transformation involves sentence tokenisation, which is potentially more complex given human edits may change sentence boundaries; e.g. [A. B, C. → A, B. C.]. Sentences are nevertheless typically tokenised based solely on the original text, with the acknowledgement that some may be sentence fragments (to be joined with the following sentence) and that edits which cross sentence boundaries are ignored (e.g. [. Because → , because]. It is worth noting that this issue only affects sentence-based GEC systems (the vast majority) but paragraph or document-based systems are unaffected.","Question:

What challenges and transformations are involved in preprocessing human annotations for Grammatical Error Correction (GEC) systems?

Answer:

Preprocessing human annotations for GEC systems involves several challenges and transformations due to the inherent differences between human and system processing. Human annotators are trained to correct natural text, whereas GEC systems are typically trained to correct word-tokenized sentences, primarily for evaluation purposes. This discrepancy necessitates multiple preprocessing steps to align human annotations with the desired output format (Bryant and Felice 2016).

The first transformation involves converting character-level edits to token-level edits. While this process is often straightforward, complications arise when a human-annotated character span does not correspond to a complete token. For example, the edit [ing → ed] to change [dancing → danced] may not map directly to token-level edits. Although such cases can often be resolved automatically by expanding the character spans of the edit or calculating token alignment, they can also be mitigated by training annotators to explicitly annotate longer spans rather than sub-words.

The second transformation involves sentence tokenization, which can be more complex because human edits may alter sentence boundaries. For instance, an edit like [A. B, C. → A, B. C.] changes the sentence structure. Typically, sentences are tokenized based solely on the original text, acknowledging that some may be sentence fragments that need to be joined with the following sentence. Edits that cross sentence boundaries, such as [. Because → , because], are generally ignored. This issue primarily affects sentence-based GEC systems, which are the majority, while paragraph or document-based systems remain unaffected.",NO,True,1681,True,True
253447259-s7,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,FCE.,"The First Certificate in English (FCE) corpus (Yannakoudakis, Briscoe, and Medlock 2011) is a public subset of the Cambridge Learner Corpus (CLC) (Nicholls 2003) that consists of 1,244 scripts (∼531k words) written by international learners of English as a second language (L2 learners). Each script typically contains two answers to a prompt in the style of a short essay, letter, or description, and each answer has been corrected by a single annotator who has identified and classified each edit according to a framework of 88 error types (Nicholls 2003 2001)) and the data is split into a standard training, development and test set. The FCE was used as the official dataset of the HOO-2012 shared task (Dale, Anisimoff, and Narroway 2012), one of the official training datasets of the BEA-2019 shared task (Bryant et al. 2019), and has otherwise commonly been used for grammatical error detection (Rei and Yannakoudakis 2016;Bell, Yannakoudakis, and Rei 2019;). It also contains essay level scores, as well as other limited metadata about the learner, and has been used for automatic essay scoring (AES) (e.g. Ke and Ng (2019)).

NUCLE/CoNLL. The National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier, Ng, and Wu 2013) consists of 1,397 argumentative essays (∼1.16m words) written by NUS undergraduate students who needed L2 English language support. The essays, which are approximately C1 level, are written on a diverse range of topics including technology, healthcare, and finance, and were each corrected by a single annotator who identified and classified each edit according to a framework of 28 error types. NUCLE was used as the official training corpus of the CoNLL-2013 and CoNLL-2014 shared tasks  as well as one of the official training datasets of the BEA-2019 shared task (Bryant et al. 2019). The CoNLL-2013 and CoNLL-2014 test sets were annotated under similar conditions to NUCLE and respectively consist of 50 essays each (∼30k words) on the topics of i) surveillance technology and population aging, and ii) genetic testing and social media. The CoNLL-2014 test set was also doubly annotated by 2 independent annotators, resulting in 2 sets of official reference annotations; Bryant and Ng (2015) and Sakaguchi et al. (2016) subsequently collected another 8 sets of annotations each for a total of 18 sets of reference annotations. The CoNLL-2013 dataset is now occasionally used as a development set, while the CoNLL-2014 dataset is one of the most commonly used benchmark test sets. One limitation of the CoNLL-2014 test set is that it is not very diverse given that it consists entirely of essays written by a narrow range of learners on only two different topics.

Lang-8. The Lang-8 Corpus of Learner English (Mizumoto et al. 2012;Tajiri, Komachi, and Matsumoto 2012) is a preprocessed subset of the multilingual Lang-8 Learner Corpus (Mizumoto et al. 2011), which consists of 100,000 submissions (∼11.8m words) to the language learning social network service,  The texts are wholly unconstrained by topic, and hence include the full range of ability levels (A1-C2), and were written by international L2 English language learners with a bias towards Japanese L1 speakers. Although Lang-8 is one of the largest publicly available corpora, it is also one of the noisiest as corrections are provided by other users rather than professional annotators. A small number of submissions also contain multiple sets of corrections, but all annotations are provided as parallel text and so do not contain explicit edits or error types. Lang-8 was also one of the official training datasets of the BEA-2019 shared task (Bryant et al. 2019).

JFLEG. The Johns Hopkins Fluency-Extended GUG corpus (JFLEG) (Napoles, Sakaguchi, and Tetreault 2017) is a collection of 1,501 sentences (∼28.1k words) split roughly equally into a development and test set. The sentences were randomly sampled from essays written by L2 learners of English of an unspecified ability level (Heilman et al. 2014) and corrected by crowdsourced annotators on Amazon Mechanical Turk (Crowston 2012). Each sentence was annotated a total of 4 times, resulting in 4 sets of parallel reference annotations, but edits were not explicitly defined or classified. The main innovation of JFLEG is that sentences were corrected to be fluent rather than minimally grammatical (Section 2.1). The main criticisms of JFLEG are that it is much smaller than other test sets, the sentences are presented out of context, and it was not corrected by professional annotators (Napoles, Nădejde, and Tetreault 2019).

W&I+LOCNESS. The Write & Improve (W&I) and LOCNESS corpus (Bryant et al. 2019) respectively consist of 3,600 essays (∼755k words) written by international learners of all ability levels (A1-C2) and 100 essays (∼46.2k words) written by native British/American English undergraduates. It was released as the official training, development and test corpus of the BEA-2019 shared task and was designed to be more balanced than other corpora such that there are roughly an equal number of sentences at each ability level: Beginner, Intermediate, Advanced, Native. The W&I essays come from submissions to the Write & Improve online essay-writing platform 3 (Yannakoudakis et al. 2018) and the LOCNESS essays, which only comprise part of the development and test sets, come from the LOCNESS corpus (Granger 1998). The training and development set essays were each corrected by a single annotator, while the test set essays were corrected by 5 annotators resulting in 5 sets of parallel reference annotations. Edits were explicitly defined, but not manually classified, so error types were added automatically using the ERRANT framework (Bryant, Felice, and Briscoe 2017). The test set references are not currently publicly available, so all evaluation on this dataset is done via the BEA-2019 Codalab competition platform, 4 which ensures all systems are evaluated in the same conditions.",How are the essays in the W&I+LOCNESS corpus annotated and evaluated for grammatical errors?,"The Write & Improve (W&I) and LOCNESS corpus consists of essays written by international learners of all ability levels and native British/American English undergraduates. The training and development set essays were each corrected by a single annotator. The test set essays were corrected by 5 annotators, resulting in 5 sets of parallel reference annotations. Edits were explicitly defined, but not manually classified, so error types were added automatically using the ERRANT framework (Bryant, Felice, and Briscoe 2017). The test set references are not currently publicly available, so all evaluation on this dataset is done via the BEA-2019 Codalab competition platform, which ensures all systems are evaluated in the same conditions.",How is the W&I+LOCNESS corpus used to annotate and evaluate grammatical errors in essays?,"The Write & Improve (W&I) and LOCNESS corpus is a collection of essays written by international learners of varying proficiency levels, as well as native British and American English undergraduates. For the training and development sets, each essay was corrected by a single annotator. In contrast, the test set essays were reviewed by five annotators, resulting in five sets of parallel reference annotations. While edits were explicitly defined, they were not manually classified. Instead, error types were automatically added using the ERRANT framework (Bryant, Felice, and Briscoe 2017). The test set references are not publicly available, so all evaluations on this dataset are conducted through the BEA-2019 Codalab competition platform, ensuring that all systems are assessed under the same conditions.",7,2,8,2,"Question:

How are the essays in the W&I+LOCNESS corpus annotated and evaluated for grammatical errors?

Answer:

The Write & Improve (W&I) and LOCNESS corpus consists of essays written by international learners of all ability levels and native British/American English undergraduates. The training and development set essays were each corrected by a single annotator. The test set essays were corrected by 5 annotators, resulting in 5 sets of parallel reference annotations. Edits were explicitly defined, but not manually classified, so error types were added automatically using the ERRANT framework (Bryant, Felice, and Briscoe 2017). The test set references are not currently publicly available, so all evaluation on this dataset is done via the BEA-2019 Codalab competition platform, which ensures all systems are evaluated in the same conditions.","Question:

How is the W&I+LOCNESS corpus used to annotate and evaluate grammatical errors in essays?

Answer:

The Write & Improve (W&I) and LOCNESS corpus is a collection of essays written by international learners of varying proficiency levels, as well as native British and American English undergraduates. For the training and development sets, each essay was corrected by a single annotator. In contrast, the test set essays were reviewed by five annotators, resulting in five sets of parallel reference annotations. While edits were explicitly defined, they were not manually classified. Instead, error types were automatically added using the ERRANT framework (Bryant, Felice, and Briscoe 2017). The test set references are not publicly available, so all evaluations on this dataset are conducted through the BEA-2019 Codalab competition platform, ensuring that all systems are assessed under the same conditions.",NO,True,809,True,True
253447259-s15,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Statistical Machine Translation,"In contrast with statistical classifiers, one of the main advantages treating GEC as a statistical machine translation (SMT) problem is that SMT can theoretically correct all error types simultaneously without expert knowledge or feature engineering. This also includes interacting errors, which are problematic for rule-based systems and classifiers. Despite originally being developed for translation between different languages, SMT has been successfully applied to GEC, which can be seen as a translation problem from errorful to correct sentences. More specifically, although both the source and target sentences are in the same language, i.e. monolingual translation, the source may contain grammatical errors which should be 'translated' to appropriate corrections. SMT is inspired by the noisy channel model (Shannon 1948) and is mathematically formulated using Bayes' rule:

where a correct sentence C is said to have passed through a noisy channel to produce an erroneous sentence E, and the goal is to reconstruct the correct sentenceĈ using a language model (LM) P (C) and a translation model (TM) P (E|C) -see Figure 1. Candidate sentences are generated by means of a decoder, which normally uses a beam search strategy. The denominator P (E) in Equation 1 is ignored since it is constant across all Cs. The use of SMT for GEC was pioneered by Brockett, Dolan, and Gamon (2006), who built a system to correct errors involving 14 countable and uncountable nouns. Their training data comprised a large corpus of sentences extracted from news articles which were deliberately modified to include artificial mass noun errors. Mizumoto et al. (2011) applied the same techniques to Japanese error correction but improved on them by not only considering a wider set of error types, but also training on real learner examples extracted from the language learning social network website Lang-8. Yuan and Felice (2013) subsequently trained a POS-factored SMT system to correct five types of errors in learner text for the CoNLL-2013 shared task, and revealed the potential of using SMT as a general approach for correcting multiple error types and interacting errors simultaneously. In the following year, the two top-performing systems in the CoNLL-2014 shared task demonstrated that SMT yielded state-of-the-art performance on general error correction in contrast with other methods (Felice et al. 2014;Junczys-Dowmunt and Grundkiewicz 2014). This success led to SMT becoming a dominant approach in the field and inspired other researchers to adapt SMT technology for GEC, including:

• Adding GEC-specific features to the model to allow for the fact that most words translate into themselves and errors are often similar to their correct forms. Two types of these features include the Levenshtein distance ( • Introducing neural network components, such as a neural network global lexicon model (NNGLM) and neural network joint model (NNJM) (Chollampatt, Taghipour, and Ng 2016;Chollampatt and Ng 2017).

Despite their success in GEC, SMT-based approaches suffer from a few shortcomings. In particular, they i) tend to produce locally well-formed phrases with poor overall grammar, ii) exhibit a predilection for changing phrases to more frequent versions even when the original is correct, resulting in unnecessary corrections, iii) are unable to process long-range dependencies and iv) are hard to constrain to particular error types (Felice 2016;Yuan 2017). Last but not least, the performance of SMT systems depends heavily on the amount and quality of parallel data available for training, which is very limited in GEC. A common solution to this problem is to generate artificial datasets, where errors are injected into well-formed text to produce pseudo-incorrect sentences, as described in Section 5.","What are the limitations of SMT-based approaches in grammatical error correction, and how do they impact performance?","Despite their success in GEC, SMT-based approaches suffer from a few shortcomings. In particular, they tend to produce locally well-formed phrases with poor overall grammar. They exhibit a predilection for changing phrases to more frequent versions even when the original is correct, resulting in unnecessary corrections. They are unable to process long-range dependencies. They are hard to constrain to particular error types (Felice 2016; Yuan 2017). Last but not least, the performance of SMT systems depends heavily on the amount and quality of parallel data available for training, which is very limited in GEC.","What are the limitations of SMT-based approaches in grammatical error correction, and how do these limitations affect their performance?","SMT-based approaches in grammatical error correction, while successful in some areas, have several notable limitations. Firstly, they often generate phrases that are locally well-formed but fail to maintain overall grammatical coherence. This can lead to unnecessary corrections, as these systems tend to replace phrases with more frequent alternatives, even when the original phrasing is correct. Additionally, SMT-based methods struggle with processing long-range dependencies, which are crucial for understanding context in language. They also face challenges in being constrained to specific error types, as noted by Felice (2016) and Yuan (2017). Lastly, the performance of SMT systems is heavily reliant on the availability and quality of parallel data for training, which is particularly scarce in the field of grammatical error correction.",7,2,7,2,"Question:

What are the limitations of SMT-based approaches in grammatical error correction, and how do they impact performance?

Answer:

Despite their success in GEC, SMT-based approaches suffer from a few shortcomings. In particular, they tend to produce locally well-formed phrases with poor overall grammar. They exhibit a predilection for changing phrases to more frequent versions even when the original is correct, resulting in unnecessary corrections. They are unable to process long-range dependencies. They are hard to constrain to particular error types (Felice 2016; Yuan 2017). Last but not least, the performance of SMT systems depends heavily on the amount and quality of parallel data available for training, which is very limited in GEC.","Question:

What are the limitations of SMT-based approaches in grammatical error correction, and how do these limitations affect their performance?

Answer:

SMT-based approaches in grammatical error correction, while successful in some areas, have several notable limitations. Firstly, they often generate phrases that are locally well-formed but fail to maintain overall grammatical coherence. This can lead to unnecessary corrections, as these systems tend to replace phrases with more frequent alternatives, even when the original phrasing is correct. Additionally, SMT-based methods struggle with processing long-range dependencies, which are crucial for understanding context in language. They also face challenges in being constrained to specific error types, as noted by Felice (2016) and Yuan (2017). Lastly, the performance of SMT systems is heavily reliant on the availability and quality of parallel data for training, which is particularly scarce in the field of grammatical error correction.",NO,True,847,True,True
253447259-s16,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Neural Machine Translation,"With the advent of deep learning and promising results reported in machine translation and other sequence-to-sequence tasks, neural machine translation (NMT) was naturally extended to GEC. Compared to SMT, NMT uses a single large neural network to model the entire correction process, eliminating the need for complex GEC-specific feature engineering. Training an NMT system is furthermore an end-to-end process and so does not require separately trained and tuned components as in SMT. Despite its simplicity, NMT has achieved state-of-the-art performance on various GEC tasks (Flachs, Stahlberg, and Kumar 2021;Rothe et al. 2021).

NMT employs the encoder-decoder framework (Cho et al. 2014). An encoder first reads and encodes an entire input sequence x = (x 1 , x 2 , ..., x T ) into hidden state representations, and a decoder then generates an output sequence y = (y 1 , y 2 , ..., y T ′ ) by predicting the next word y t based on the input sequence x and all the previously generated words {y 1 , y 2 , ..., y t−1 }:

Different network architectures have been proposed for building the encoders and decoders; three commonly used sequence-to-sequence models are RNNs (Bahdanau, Cho, and Bengio 2015), CNNs (Gehring et al. 2017), and Transformers (Vaswani et al. 2017).",How does the encoder-decoder framework function in neural machine translation for grammatical error correction?,"The encoder-decoder framework in neural machine translation for grammatical error correction functions by employing an encoder to first read and encode an entire input sequence \( x = (x_1, x_2, ..., x_T) \) into hidden state representations. A decoder then generates an output sequence \( y = (y_1, y_2, ..., y_{T'}) \) by predicting the next word \( y_t \) based on the input sequence \( x \) and all the previously generated words \(\{y_1, y_2, ..., y_{t-1}\}\). Different network architectures have been proposed for building the encoders and decoders, with three commonly used sequence-to-sequence models being RNNs (Bahdanau, Cho, and Bengio 2015), CNNs (Gehring et al. 2017), and Transformers (Vaswani et al. 2017).",How does the encoder-decoder framework operate in neural machine translation for correcting grammatical errors?,"In neural machine translation for grammatical error correction, the encoder-decoder framework operates by utilizing an encoder to read and encode the entire input sequence \( x = (x_1, x_2, ..., x_T) \) into hidden state representations. Subsequently, a decoder generates the output sequence \( y = (y_1, y_2, ..., y_{T'}) \) by predicting each next word \( y_t \) based on the input sequence \( x \) and all previously generated words \(\{y_1, y_2, ..., y_{t-1}\}\). Various network architectures have been proposed for constructing the encoders and decoders, with three commonly used sequence-to-sequence models being Recurrent Neural Networks (RNNs) as described by Bahdanau, Cho, and Bengio in 2015, Convolutional Neural Networks (CNNs) as detailed by Gehring et al. in 2017, and Transformers as introduced by Vaswani et al. in 2017.",8,2,8,2,"Question:

How does the encoder-decoder framework function in neural machine translation for grammatical error correction?

Answer:

The encoder-decoder framework in neural machine translation for grammatical error correction functions by employing an encoder to first read and encode an entire input sequence \( x = (x_1, x_2, ..., x_T) \) into hidden state representations. A decoder then generates an output sequence \( y = (y_1, y_2, ..., y_{T'}) \) by predicting the next word \( y_t \) based on the input sequence \( x \) and all the previously generated words \(\{y_1, y_2, ..., y_{t-1}\}\). Different network architectures have been proposed for building the encoders and decoders, with three commonly used sequence-to-sequence models being RNNs (Bahdanau, Cho, and Bengio 2015), CNNs (Gehring et al. 2017), and Transformers (Vaswani et al. 2017).","Question:

How does the encoder-decoder framework operate in neural machine translation for correcting grammatical errors?

Answer:

In neural machine translation for grammatical error correction, the encoder-decoder framework operates by utilizing an encoder to read and encode the entire input sequence \( x = (x_1, x_2, ..., x_T) \) into hidden state representations. Subsequently, a decoder generates the output sequence \( y = (y_1, y_2, ..., y_{T'}) \) by predicting each next word \( y_t \) based on the input sequence \( x \) and all previously generated words \(\{y_1, y_2, ..., y_{t-1}\}\). Various network architectures have been proposed for constructing the encoders and decoders, with three commonly used sequence-to-sequence models being Recurrent Neural Networks (RNNs) as described by Bahdanau, Cho, and Bengio in 2015, Convolutional Neural Networks (CNNs) as detailed by Gehring et al. in 2017, and Transformers as introduced by Vaswani et al. in 2017.",NO,True,837,True,True
253447259-s17,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Recurrent Neural Networks,"Recurrent Neural Networks (RNN) are a type of neural network that is specifically designed to process sequential data. RNNs are used to transform a variablelength input sequence to another variable-length output sequence (Cho et al. 2014;Sutskever, Vinyals, and Le 2014). To handle long-term dependencies, gated units are usually used in RNNs (Goodfellow, Bengio, and Courville 2016). The two most effective RNN gates are Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber 1997) and Gated Recurrent Units (GRU) (Cho et al. 2014). Bahdanau, Cho, and Bengio (2015) introduced an attention mechanism to implement variable-length representations, which eased optimisation difficulty and resulted in improved performance.  presented the first work on NMT-based approach for GEC. Their model consists of a bidirectional RNN encoder and an attention-based RNN decoder. Xie et al. (2016) proposed the use of a character-level RNN sequence-to-sequence model for GEC. Following their work, a hybrid model with nested attention at both the word and character level was later introduced by Ji et al. (2017).",How have RNNs been adapted to improve performance in Grammatical Error Correction tasks?,"Recurrent Neural Networks (RNN) are a type of neural network that is specifically designed to process sequential data. RNNs are used to transform a variable-length input sequence to another variable-length output sequence (Cho et al. 2014; Sutskever, Vinyals, and Le 2014). To handle long-term dependencies, gated units are usually used in RNNs (Goodfellow, Bengio, and Courville 2016). The two most effective RNN gates are Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber 1997) and Gated Recurrent Units (GRU) (Cho et al. 2014).  Bahdanau, Cho, and Bengio (2015) introduced an attention mechanism to implement variable-length representations, which eased optimization difficulty and resulted in improved performance. Their model consists of a bidirectional RNN encoder and an attention-based RNN decoder. Xie et al. (2016) proposed the use of a character-level RNN sequence-to-sequence model for GEC. Following their work, a hybrid model with nested attention at both the word and character level was later introduced by Ji et al. (2017).",How have Recurrent Neural Networks (RNNs) been adapted to enhance performance in Grammatical Error Correction (GEC) tasks?,"Recurrent Neural Networks (RNNs) are specifically designed to process sequential data, making them suitable for tasks like Grammatical Error Correction (GEC). RNNs are adept at transforming variable-length input sequences into variable-length output sequences, as demonstrated by Cho et al. (2014) and Sutskever, Vinyals, and Le (2014). To effectively manage long-term dependencies within sequences, RNNs often employ gated units. The most effective of these are Long-Short Term Memory (LSTM) units, introduced by Hochreiter and Schmidhuber (1997), and Gated Recurrent Units (GRU), developed by Cho et al. (2014).

An important advancement in RNNs for GEC tasks was the introduction of an attention mechanism by Bahdanau, Cho, and Bengio (2015). This mechanism allows for variable-length representations, easing optimization challenges and improving performance. Their model features a bidirectional RNN encoder paired with an attention-based RNN decoder. Building on these developments, Xie et al. (2016) proposed a character-level RNN sequence-to-sequence model specifically for GEC. Further innovation came from Ji et al. (2017), who introduced a hybrid model incorporating nested attention at both the word and character levels, enhancing the model's ability to correct grammatical errors.",7,4,7,4,"Question:

How have RNNs been adapted to improve performance in Grammatical Error Correction tasks?

Answer:

Recurrent Neural Networks (RNN) are a type of neural network that is specifically designed to process sequential data. RNNs are used to transform a variable-length input sequence to another variable-length output sequence (Cho et al. 2014; Sutskever, Vinyals, and Le 2014). To handle long-term dependencies, gated units are usually used in RNNs (Goodfellow, Bengio, and Courville 2016). The two most effective RNN gates are Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber 1997) and Gated Recurrent Units (GRU) (Cho et al. 2014).  Bahdanau, Cho, and Bengio (2015) introduced an attention mechanism to implement variable-length representations, which eased optimization difficulty and resulted in improved performance. Their model consists of a bidirectional RNN encoder and an attention-based RNN decoder. Xie et al. (2016) proposed the use of a character-level RNN sequence-to-sequence model for GEC. Following their work, a hybrid model with nested attention at both the word and character level was later introduced by Ji et al. (2017).","Question:

How have Recurrent Neural Networks (RNNs) been adapted to enhance performance in Grammatical Error Correction (GEC) tasks?

Answer:

Recurrent Neural Networks (RNNs) are specifically designed to process sequential data, making them suitable for tasks like Grammatical Error Correction (GEC). RNNs are adept at transforming variable-length input sequences into variable-length output sequences, as demonstrated by Cho et al. (2014) and Sutskever, Vinyals, and Le (2014). To effectively manage long-term dependencies within sequences, RNNs often employ gated units. The most effective of these are Long-Short Term Memory (LSTM) units, introduced by Hochreiter and Schmidhuber (1997), and Gated Recurrent Units (GRU), developed by Cho et al. (2014).

An important advancement in RNNs for GEC tasks was the introduction of an attention mechanism by Bahdanau, Cho, and Bengio (2015). This mechanism allows for variable-length representations, easing optimization challenges and improving performance. Their model features a bidirectional RNN encoder paired with an attention-based RNN decoder. Building on these developments, Xie et al. (2016) proposed a character-level RNN sequence-to-sequence model specifically for GEC. Further innovation came from Ji et al. (2017), who introduced a hybrid model incorporating nested attention at both the word and character levels, enhancing the model's ability to correct grammatical errors.",NO,True,1293,True,True
253447259-s20,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Edit-based approaches,"While most GEC approaches generate a corrected sentence from an input sentence, the edit generation approach generates a sequence of edits to be applied to the input sentence instead. As GEC has a high degree of token copying from the input to the output, Stahlberg and Kumar (2020) argued that generating the full sequence is wasteful. By generating edit operations instead of all tokens in a sentence, the edit generation approach typically has a faster inference speed, reported to be five to ten times faster than GEC systems that generate the whole sentence. One limitation of this approach, however, is that edit operations tend to be token-based, and so sometimes fail to capture more complex, multi-token fluency edits (Lai et al. 2022). Edit generation has been cast as a sequence tagging task (Malmi et al. 2019;Awasthi et al. 2019;Omelianchuk et al. 2020;Tarnavskyi, Chernodub, and Omelianchuk 2022) or a sequence-to-sequence task (Stahlberg and Kumar 2020).

In the sequence tagging approach, for each token of an input sentence, the system predicts an edit operation to be applied to that token (Table 5). This approach requires the user to define a set of tags representing the edit operations to be modelled by the system. Some edits can be universally modelled, such as conversion of verb forms or conversion of nouns from singular to plural form. Some others such as word insertion and word replacement are token-dependent. Token-dependent edits need a different tag for each possible word in the vocabulary, resulting in the number of tags growing linearly with the number of unique words in the training data. Thus, the number of token-dependent tags to be modelled in the system becomes a trade-off between coverage and model size.  Table 5 Example task formulation of edit generation in the sequence tagging approach from (Omelianchuk et al. 2020). APP_x denotes an operation of appending token x, and REP_x denotes replacing the current token with x.

On the other hand, the sequence-to-sequence approach is more flexible as it does not limit the output to pre-defined edit operation tags. It produces a sequence of edits, each consisting of a span position, a replacement string, and an optional tag for edit type (Table 6). These tags add interpretability to the process and have been shown to improve model performance. As generation in the sequence-to-sequence approach has a left-to-right dependency, the inference procedure is slower than that in the sequence tagging approach. It is still five times faster than that in the whole sentence generation approach as the edit sequence generated is much shorter than the sequence of all tokens in the sentence (Stahlberg and Kumar 2020).

Source After many years he still dream to become a super hero . Target After many years , he still dreams of becoming a super hero . Edits (SELF,3,SELF), (PUNCT,3,','), (SELF,5,SELF), (SVA,6,'dreams'), (PART,7,'of'), (FORM,8,'becoming'), (SELF,12,SELF) Table 6 Example task formulation of edit generation in the sequence-to-sequence approach from (Stahlberg and Kumar 2020). Each tuple represents a tag, a span's ending position, and a replacement string.

The main advantages of edit-based approaches to GEC are thus that they not only add much needed transparency and explainability to the correction process, but they are also much faster at inference time than NMT. Their main disadvantages, however, are that they generally require human engineering to define the size and scope of the edit label set, and that it is more difficult to represent interacting and complex multi-token edits with token-based labels. Like all neural approaches, they also depend on as much training data as possible, but when data is available, edit-based approaches are very competitive with state-of-the-art NMT models.",How is the edit generation approach in GEC advantageous and what limitations does it face?,"The edit generation approach in GEC is advantageous because it generates a sequence of edits to be applied to the input sentence instead of generating the full sequence, which is considered wasteful due to the high degree of token copying from the input to the output. By generating edit operations instead of all tokens in a sentence, the edit generation approach typically has a faster inference speed, reported to be five to ten times faster than GEC systems that generate the whole sentence. The main advantages of edit-based approaches to GEC are that they not only add much-needed transparency and explainability to the correction process, but they are also much faster at inference time than NMT. However, the approach faces limitations as edit operations tend to be token-based, and so sometimes fail to capture more complex, multi-token fluency edits (Lai et al. 2022). Their main disadvantages are that they generally require human engineering to define the size and scope of the edit label set, and that it is more difficult to represent interacting and complex multi-token edits with token-based labels. Like all neural approaches, they also depend on as much training data as possible, but when data is available, edit-based approaches are very competitive with state-of-the-art NMT models.",What are the advantages and limitations of the edit generation approach in Grammatical Error Correction (GEC)?,"The edit generation approach in Grammatical Error Correction (GEC) offers several advantages. Instead of generating the entire corrected sentence, which can be inefficient due to the high degree of token copying from the input to the output, this approach focuses on generating a sequence of edits to be applied to the input sentence. This method significantly enhances inference speed, making it five to ten times faster than GEC systems that generate the whole sentence. Additionally, edit-based approaches provide greater transparency and explainability in the correction process compared to Neural Machine Translation (NMT) systems.

However, the edit generation approach also faces certain limitations. Since edit operations are typically token-based, they sometimes struggle to capture more complex, multi-token fluency edits, as noted by Lai et al. (2022). Another drawback is the need for human engineering to define the size and scope of the edit label set, which can be challenging when representing interacting and complex multi-token edits with token-based labels. Like all neural approaches, edit-based methods rely heavily on the availability of extensive training data. Nevertheless, when sufficient data is available, these approaches are highly competitive with state-of-the-art NMT models.",7,4,8,4,"Question:

How is the edit generation approach in GEC advantageous and what limitations does it face?

Answer:

The edit generation approach in GEC is advantageous because it generates a sequence of edits to be applied to the input sentence instead of generating the full sequence, which is considered wasteful due to the high degree of token copying from the input to the output. By generating edit operations instead of all tokens in a sentence, the edit generation approach typically has a faster inference speed, reported to be five to ten times faster than GEC systems that generate the whole sentence. The main advantages of edit-based approaches to GEC are that they not only add much-needed transparency and explainability to the correction process, but they are also much faster at inference time than NMT. However, the approach faces limitations as edit operations tend to be token-based, and so sometimes fail to capture more complex, multi-token fluency edits (Lai et al. 2022). Their main disadvantages are that they generally require human engineering to define the size and scope of the edit label set, and that it is more difficult to represent interacting and complex multi-token edits with token-based labels. Like all neural approaches, they also depend on as much training data as possible, but when data is available, edit-based approaches are very competitive with state-of-the-art NMT models.","Question:

What are the advantages and limitations of the edit generation approach in Grammatical Error Correction (GEC)?

Answer:

The edit generation approach in Grammatical Error Correction (GEC) offers several advantages. Instead of generating the entire corrected sentence, which can be inefficient due to the high degree of token copying from the input to the output, this approach focuses on generating a sequence of edits to be applied to the input sentence. This method significantly enhances inference speed, making it five to ten times faster than GEC systems that generate the whole sentence. Additionally, edit-based approaches provide greater transparency and explainability in the correction process compared to Neural Machine Translation (NMT) systems.

However, the edit generation approach also faces certain limitations. Since edit operations are typically token-based, they sometimes struggle to capture more complex, multi-token fluency edits, as noted by Lai et al. (2022). Another drawback is the need for human engineering to define the size and scope of the edit label set, which can be challenging when representing interacting and complex multi-token edits with token-based labels. Like all neural approaches, edit-based methods rely heavily on the availability of extensive training data. Nevertheless, when sufficient data is available, these approaches are highly competitive with state-of-the-art NMT models.",NO,True,1307,True,True
253447259-s22,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Language models as Discriminators,"The traditional LM-based approach to GEC makes the assumption that low probability sentences are more likely to contain grammatical errors than high probability sentences, and so a GEC system must determine how to transform the former into the latter based on language model probabilities (Bryant and Briscoe 2018). Correction candidates can be generated from confusion sets (Dahlmeier and Ng 2011a;Bryant and Briscoe 2018), classification-based GEC models (Dahlmeier and Ng 2012a), or finite-state transducers (Stahlberg, Bryant, and Byrne 2019). Yasunaga, Leskovec, and Liang (2021) proposed an alternative method using the break-it-fix-it (BIFI) approach , with a language model as the critic (LM-critic). Specifically, BIFI trains a breaker (noising channel) and a fixer (GEC model) on multiple rounds of feedback loops. An initial fixer is used to correct erroneous text, then the sentence pairs are filtered using LM-critic. Using this filtered data, the breaker is trained and used to generate new synthetic data from a clean corpus. These new sentence pairs are then also filtered using LM-critic and subsequently used to train the fixer again. The BIFI approach can be used for unsupervised GEC by training the fixer on synthetic data.",How does the break-it-fix-it (BIFI) approach function in grammatical error correction?,"The break-it-fix-it (BIFI) approach functions in grammatical error correction by using a language model as the critic (LM-critic). Specifically, BIFI trains a breaker (noising channel) and a fixer (GEC model) on multiple rounds of feedback loops. An initial fixer is used to correct erroneous text, then the sentence pairs are filtered using LM-critic. Using this filtered data, the breaker is trained and used to generate new synthetic data from a clean corpus. These new sentence pairs are then also filtered using LM-critic and subsequently used to train the fixer again. The BIFI approach can be used for unsupervised GEC by training the fixer on synthetic data.","What is the break-it-fix-it (BIFI) approach in grammatical error correction, and how does it work?","The break-it-fix-it (BIFI) approach in grammatical error correction involves a process where a language model acts as a critic (LM-critic) to enhance the correction process. This approach consists of training two components: a breaker (noising channel) and a fixer (GEC model), through multiple rounds of feedback loops. Initially, a fixer is employed to correct erroneous text, and the resulting sentence pairs are filtered using the LM-critic. The filtered data is then used to train the breaker, which generates new synthetic data from a clean corpus. These newly created sentence pairs undergo another round of filtering with the LM-critic and are subsequently used to retrain the fixer. The BIFI approach is particularly useful for unsupervised grammatical error correction, as it allows the fixer to be trained on synthetic data.",7,2,7,2,"Question:

How does the break-it-fix-it (BIFI) approach function in grammatical error correction?

Answer:

The break-it-fix-it (BIFI) approach functions in grammatical error correction by using a language model as the critic (LM-critic). Specifically, BIFI trains a breaker (noising channel) and a fixer (GEC model) on multiple rounds of feedback loops. An initial fixer is used to correct erroneous text, then the sentence pairs are filtered using LM-critic. Using this filtered data, the breaker is trained and used to generate new synthetic data from a clean corpus. These new sentence pairs are then also filtered using LM-critic and subsequently used to train the fixer again. The BIFI approach can be used for unsupervised GEC by training the fixer on synthetic data.","Question:

What is the break-it-fix-it (BIFI) approach in grammatical error correction, and how does it work?

Answer:

The break-it-fix-it (BIFI) approach in grammatical error correction involves a process where a language model acts as a critic (LM-critic) to enhance the correction process. This approach consists of training two components: a breaker (noising channel) and a fixer (GEC model), through multiple rounds of feedback loops. Initially, a fixer is employed to correct erroneous text, and the resulting sentence pairs are filtered using the LM-critic. The filtered data is then used to train the breaker, which generates new synthetic data from a clean corpus. These newly created sentence pairs undergo another round of filtering with the LM-critic and are subsequently used to retrain the fixer. The BIFI approach is particularly useful for unsupervised grammatical error correction, as it allows the fixer to be trained on synthetic data.",NO,True,835,True,True
253447259-s25,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Re-ranking,"Machine translation based (both SMT and NMT) systems can produce an n-best list of alternative corrections for a single sentence. This has led to much work on n-best list re-ranking, which aims to determine whether the best correction for a sentence is not the most likely candidate produced by the system (i.e. n = 1), but is rather somewhere further down the top n most likely candidates (Yuan, Briscoe, and Felice 2016;Mizumoto and Matsumoto 2016;Hoang, Chollampatt, and Ng 2016). As a separate postprocessing step, candidates produced by an SMT-based or NMT-based GEC system can be re-ranked using a rich set of features that have not been explored by the decoder before, so that better candidates can be selected as 'optimal' corrections. During re-ranking, GEC-specific features can then be easily adapted without worrying about fine-grained model smoothing issues. In addition to the original model scores of the candidates, useful features include: • error detection information which has been used in a binary setting (Yannakoudakis et al. 2017;Yuan et al. 2019), as well as a multi-class setting .

N -best list reranking has traditionally been one of the simplest and most popular methods of boosting system performance. An alternative form of reranking is to collect all the edits from the N -best corrections and filter them using an edit-scorer (Sorokin 2022).",How does re-ranking improve grammatical error correction by utilizing additional features and candidate selection methods?,"Machine translation-based systems can produce an n-best list of alternative corrections for a single sentence. Re-ranking aims to determine whether the best correction for a sentence is not the most likely candidate produced by the system, but is rather somewhere further down the top n most likely candidates (Yuan, Briscoe, and Felice 2016; Mizumoto and Matsumoto 2016; Hoang, Chollampatt, and Ng 2016). As a separate postprocessing step, candidates produced by an SMT-based or NMT-based GEC system can be re-ranked using a rich set of features that have not been explored by the decoder before, so that better candidates can be selected as 'optimal' corrections. During re-ranking, GEC-specific features can then be easily adapted without worrying about fine-grained model smoothing issues. In addition to the original model scores of the candidates, useful features include error detection information which has been used in a binary setting (Yannakoudakis et al. 2017; Yuan et al. 2019), as well as a multi-class setting. N-best list reranking has traditionally been one of the simplest and most popular methods of boosting system performance. An alternative form of reranking is to collect all the edits from the N-best corrections and filter them using an edit-scorer (Sorokin 2022).",How does re-ranking enhance grammatical error correction by leveraging additional features and candidate selection methods?,"Re-ranking plays a crucial role in improving grammatical error correction (GEC) by refining the selection of candidate corrections generated by machine translation-based systems. These systems typically produce an n-best list of alternative corrections for a given sentence. The primary goal of re-ranking is to identify the best correction, which may not always be the top candidate initially suggested by the system. Instead, the optimal correction might be found further down the list of the top n candidates (Yuan, Briscoe, and Felice 2016; Mizumoto and Matsumoto 2016; Hoang, Chollampatt, and Ng 2016).

As a separate post-processing step, re-ranking involves evaluating these candidates using a rich set of features that the initial decoder may not have considered. This process allows for the selection of better candidates as 'optimal' corrections. During re-ranking, GEC-specific features can be easily adapted without the need to address fine-grained model smoothing issues. In addition to the original model scores, useful features for re-ranking include error detection information, which can be applied in both binary and multi-class settings (Yannakoudakis et al. 2017; Yuan et al. 2019).

Traditionally, n-best list re-ranking has been one of the simplest and most popular methods for enhancing system performance. An alternative approach involves collecting all the edits from the n-best corrections and filtering them using an edit-scorer (Sorokin 2022). This method further refines the selection process, ensuring that the most accurate corrections are chosen.",7,3,7,4,"Question:

How does re-ranking improve grammatical error correction by utilizing additional features and candidate selection methods?

Answer:

Machine translation-based systems can produce an n-best list of alternative corrections for a single sentence. Re-ranking aims to determine whether the best correction for a sentence is not the most likely candidate produced by the system, but is rather somewhere further down the top n most likely candidates (Yuan, Briscoe, and Felice 2016; Mizumoto and Matsumoto 2016; Hoang, Chollampatt, and Ng 2016). As a separate postprocessing step, candidates produced by an SMT-based or NMT-based GEC system can be re-ranked using a rich set of features that have not been explored by the decoder before, so that better candidates can be selected as 'optimal' corrections. During re-ranking, GEC-specific features can then be easily adapted without worrying about fine-grained model smoothing issues. In addition to the original model scores of the candidates, useful features include error detection information which has been used in a binary setting (Yannakoudakis et al. 2017; Yuan et al. 2019), as well as a multi-class setting. N-best list reranking has traditionally been one of the simplest and most popular methods of boosting system performance. An alternative form of reranking is to collect all the edits from the N-best corrections and filter them using an edit-scorer (Sorokin 2022).","Question:

How does re-ranking enhance grammatical error correction by leveraging additional features and candidate selection methods?

Answer:

Re-ranking plays a crucial role in improving grammatical error correction (GEC) by refining the selection of candidate corrections generated by machine translation-based systems. These systems typically produce an n-best list of alternative corrections for a given sentence. The primary goal of re-ranking is to identify the best correction, which may not always be the top candidate initially suggested by the system. Instead, the optimal correction might be found further down the list of the top n candidates (Yuan, Briscoe, and Felice 2016; Mizumoto and Matsumoto 2016; Hoang, Chollampatt, and Ng 2016).

As a separate post-processing step, re-ranking involves evaluating these candidates using a rich set of features that the initial decoder may not have considered. This process allows for the selection of better candidates as 'optimal' corrections. During re-ranking, GEC-specific features can be easily adapted without the need to address fine-grained model smoothing issues. In addition to the original model scores, useful features for re-ranking include error detection information, which can be applied in both binary and multi-class settings (Yannakoudakis et al. 2017; Yuan et al. 2019).

Traditionally, n-best list re-ranking has been one of the simplest and most popular methods for enhancing system performance. An alternative approach involves collecting all the edits from the n-best corrections and filtering them using an edit-scorer (Sorokin 2022). This method further refines the selection process, ensuring that the most accurate corrections are chosen.",NO,True,1578,True,True
253447259-s27,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Multi-task learning,"Multi-task learning allows systems to use information from related tasks and learn from multiple objectives via shared representations, leading to performance gains on individual tasks. Rei and Yannakoudakis (2017) was the first to investigate the use of different auxiliary objectives for the task of error detection in learner writing through a neural sequence-labelling model. In addition to predicting the binary error labels (i.e. correct or incorrect), they experimented with also predicting specific error type information, including the learner's L1, token frequency, POS tags and dependency relations. Asano et al. (2019) employed a similar approach in which their error correction model additionally estimated the learner's language proficiency level and performed sentence-level error detection simultaneously. Token-level and sentence-level error detection have also both been explored as auxiliary objectives in NMT-based GEC (Yuan et al. 2019;Zhao et al. 2019), where systems have been trained to jointly generate a correction and predict whether the source sentence (or any token in it) is correct or incorrect. Labels for these auxiliary error detection tasks can be extracted automatically from existing datasets using automatic alignment tools like ERRANT (Bryant, Felice, and Briscoe 2017).",How has multi-task learning been applied to enhance grammatical error correction in learner writing?,"Multi-task learning allows systems to use information from related tasks and learn from multiple objectives via shared representations, leading to performance gains on individual tasks. Rei and Yannakoudakis (2017) was the first to investigate the use of different auxiliary objectives for the task of error detection in learner writing through a neural sequence-labelling model. In addition to predicting the binary error labels (i.e., correct or incorrect), they experimented with also predicting specific error type information, including the learner's L1, token frequency, POS tags, and dependency relations. Asano et al. (2019) employed a similar approach in which their error correction model additionally estimated the learner's language proficiency level and performed sentence-level error detection simultaneously. Token-level and sentence-level error detection have also both been explored as auxiliary objectives in NMT-based GEC (Yuan et al. 2019; Zhao et al. 2019), where systems have been trained to jointly generate a correction and predict whether the source sentence (or any token in it) is correct or incorrect. Labels for these auxiliary error detection tasks can be extracted automatically from existing datasets using automatic alignment tools like ERRANT (Bryant, Felice, and Briscoe 2017).",In what ways has multi-task learning been utilized to improve grammatical error correction in learner writing?,"Multi-task learning enhances grammatical error correction by leveraging information from related tasks and learning from multiple objectives through shared representations, which results in improved performance on individual tasks. The pioneering work by Rei and Yannakoudakis (2017) explored the use of various auxiliary objectives for error detection in learner writing using a neural sequence-labelling model. They extended beyond predicting binary error labels (correct or incorrect) by also predicting specific error type information, such as the learner's first language (L1), token frequency, part-of-speech (POS) tags, and dependency relations. Similarly, Asano et al. (2019) applied this approach by having their error correction model estimate the learner's language proficiency level and perform sentence-level error detection simultaneously. Additionally, both token-level and sentence-level error detection have been investigated as auxiliary objectives in neural machine translation (NMT)-based grammatical error correction (GEC) models (Yuan et al. 2019; Zhao et al. 2019). These models are trained to jointly generate corrections and predict the correctness of the source sentence or any token within it. The labels for these auxiliary error detection tasks can be automatically extracted from existing datasets using alignment tools like ERRANT (Bryant, Felice, and Briscoe 2017).",8,4,9,4,"Question:

How has multi-task learning been applied to enhance grammatical error correction in learner writing?

Answer:

Multi-task learning allows systems to use information from related tasks and learn from multiple objectives via shared representations, leading to performance gains on individual tasks. Rei and Yannakoudakis (2017) was the first to investigate the use of different auxiliary objectives for the task of error detection in learner writing through a neural sequence-labelling model. In addition to predicting the binary error labels (i.e., correct or incorrect), they experimented with also predicting specific error type information, including the learner's L1, token frequency, POS tags, and dependency relations. Asano et al. (2019) employed a similar approach in which their error correction model additionally estimated the learner's language proficiency level and performed sentence-level error detection simultaneously. Token-level and sentence-level error detection have also both been explored as auxiliary objectives in NMT-based GEC (Yuan et al. 2019; Zhao et al. 2019), where systems have been trained to jointly generate a correction and predict whether the source sentence (or any token in it) is correct or incorrect. Labels for these auxiliary error detection tasks can be extracted automatically from existing datasets using automatic alignment tools like ERRANT (Bryant, Felice, and Briscoe 2017).","Question:

In what ways has multi-task learning been utilized to improve grammatical error correction in learner writing?

Answer:

Multi-task learning enhances grammatical error correction by leveraging information from related tasks and learning from multiple objectives through shared representations, which results in improved performance on individual tasks. The pioneering work by Rei and Yannakoudakis (2017) explored the use of various auxiliary objectives for error detection in learner writing using a neural sequence-labelling model. They extended beyond predicting binary error labels (correct or incorrect) by also predicting specific error type information, such as the learner's first language (L1), token frequency, part-of-speech (POS) tags, and dependency relations. Similarly, Asano et al. (2019) applied this approach by having their error correction model estimate the learner's language proficiency level and perform sentence-level error detection simultaneously. Additionally, both token-level and sentence-level error detection have been investigated as auxiliary objectives in neural machine translation (NMT)-based grammatical error correction (GEC) models (Yuan et al. 2019; Zhao et al. 2019). These models are trained to jointly generate corrections and predict the correctness of the source sentence or any token within it. The labels for these auxiliary error detection tasks can be automatically extracted from existing datasets using alignment tools like ERRANT (Bryant, Felice, and Briscoe 2017).",NO,True,1397,True,True
253447259-s31,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Data Augmentation,"A common problem in GEC is that the largest publicly-available high-quality parallel corpora only contain roughly 50k sentence pairs, and larger corpora, such as Lang-8, are noisy Rothe et al. 2021). This data sparsity problem has motivated a lot of research into synthetic data generation, especially in the context of resourceheavy NMT approaches, because synthetic data primarily requires a native monolingual source corpus rather than a labour-intensive manual annotation process. In this section, we introduce several different types of data augmentation methods, including rule-based noise injection and back-translation, but also noise reduction which aims to improve the quality of existing datasets by removing/down-weighting noisy examples. It is an open question as to how to best evaluate the quality of synthetic data (Htut and Tetreault 2019;White and Rozovskaya 2020). An effort has been made by (Kiyono et al. 2019) to compare the noise injection method and back-translation, but it is hard to comprehensively compare synthetic data generation methods directly, so most research evaluates it indirectly in terms of its impact on the performance of previous experiments. Data augmentation has nevertheless contributed greatly to GEC system improvement and has become a staple component of recent models.",How has data augmentation influenced the development and performance of Grammatical Error Correction systems?,"A common problem in GEC is that the largest publicly-available high-quality parallel corpora only contain roughly 50k sentence pairs, and larger corpora, such as Lang-8, are noisy (Rothe et al. 2021). This data sparsity problem has motivated a lot of research into synthetic data generation, especially in the context of resource-heavy NMT approaches, because synthetic data primarily requires a native monolingual source corpus rather than a labor-intensive manual annotation process. Data augmentation has nevertheless contributed greatly to GEC system improvement and has become a staple component of recent models.",What impact has data augmentation had on the development and performance of Grammatical Error Correction (GEC) systems?,"Data augmentation has significantly influenced the development and performance of Grammatical Error Correction (GEC) systems, primarily by addressing the challenge of data sparsity. The largest publicly available high-quality parallel corpora for GEC contain only about 50,000 sentence pairs, while larger datasets, like Lang-8, tend to be noisy (Rothe et al. 2021). This scarcity of high-quality data has driven extensive research into synthetic data generation, particularly within resource-intensive Neural Machine Translation (NMT) approaches. Synthetic data generation is advantageous because it relies on a native monolingual source corpus, eliminating the need for labor-intensive manual annotation. As a result, data augmentation has become a crucial component in enhancing GEC systems and is now a staple in recent models.",7,4,8,4,"Question:

How has data augmentation influenced the development and performance of Grammatical Error Correction systems?

Answer:

A common problem in GEC is that the largest publicly-available high-quality parallel corpora only contain roughly 50k sentence pairs, and larger corpora, such as Lang-8, are noisy (Rothe et al. 2021). This data sparsity problem has motivated a lot of research into synthetic data generation, especially in the context of resource-heavy NMT approaches, because synthetic data primarily requires a native monolingual source corpus rather than a labor-intensive manual annotation process. Data augmentation has nevertheless contributed greatly to GEC system improvement and has become a staple component of recent models.","Question:

What impact has data augmentation had on the development and performance of Grammatical Error Correction (GEC) systems?

Answer:

Data augmentation has significantly influenced the development and performance of Grammatical Error Correction (GEC) systems, primarily by addressing the challenge of data sparsity. The largest publicly available high-quality parallel corpora for GEC contain only about 50,000 sentence pairs, while larger datasets, like Lang-8, tend to be noisy (Rothe et al. 2021). This scarcity of high-quality data has driven extensive research into synthetic data generation, particularly within resource-intensive Neural Machine Translation (NMT) approaches. Synthetic data generation is advantageous because it relies on a native monolingual source corpus, eliminating the need for labor-intensive manual annotation. As a result, data augmentation has become a crucial component in enhancing GEC systems and is now a staple in recent models.",NO,True,831,True,True
253447259-s33,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Noise Injection,"One way to artificially generate grammatical errors to clean monolingual corpora is by perturbing a clean text to make it grammatically incorrect. The perturbations can be in the form of rule-based noising operations or error patterns that usually appear in GEC parallel corpora.

Rule-based. The most intuitive way of adding noise to a clean corpus is by applying a series of perturbation operations based on some pre-defined rules. The rules are applied based on a probability, which can be decided arbitrarily, empirically, or through some observations of available data. Ehsan and Faili (2013) apply one error to each sentence from predefined error templates that include omitting prepositions, repeating words, and so on. Lichtarge et al. (2019) introduce spelling errors to Wikipedia edit history by performing deletion, insertion, replacement, and transposition of characters. Zhao et al. (2019) also apply a similar noising strategy but at the word level, that is deleting, adding, shuffling, and replacing words in a sentence. Grundkiewicz, Junczys-Dowmunt, and Heafield (2019) combine both approaches, character-level and word-level noising, but word substitution is limited to pairs from a confusion set made from an inverted spellchecker. Similarly, Xu et al. (2019) also combine both approaches but with a more complex word substitution strategy by making use of part-of-speech (POS) tags. The rule-based injection technique can also be applied dynamically during training to increase the error rate in a parallel corpus instead of creating additional training data (Zhao and Wang 2020).

Error patterns. Another way of generating synthetic data is through injecting errors that frequently occur in GEC parallel corpora. In this way, the errors are more similar to the ones that humans usually make. Rozovskaya and Roth (2010b) proposed three different methods of injecting article errors, based on the error distribution in English as a Second Language (ESL) data. They proposed adding article errors based on the distribution of articles in a text before correction, the distribution of articles in the corrected text, and the distribution of article corrections themselves. Felice and Yuan (2014a) later improved the method by taking into consideration the morphology, POS tag, semantic concept, and word sense information of a text when generating the artificial errors. Rei et al. (2017) further extended it to all types of errors. Another direction of emulating human errors is by extracting the correction patterns from GEC parallel corpora and applying the inverse of those corrections on grammatically correct sentences, as done by Yuan and Felice (2013) using the corrections from the NUCLE corpus and by Choe et al. (2019) using the corrections from the W&I training data. The correction patterns are extracted both in lexical form (an → the) and POS (NN → NNS).","How are error patterns used to generate synthetic data for grammatical error correction, and why are they effective?","Error patterns are used to generate synthetic data for grammatical error correction by injecting errors that frequently occur in GEC parallel corpora. In this way, the errors are more similar to the ones that humans usually make. Rozovskaya and Roth (2010b) proposed three different methods of injecting article errors, based on the error distribution in English as a Second Language (ESL) data. They proposed adding article errors based on the distribution of articles in a text before correction, the distribution of articles in the corrected text, and the distribution of article corrections themselves. Felice and Yuan (2014a) later improved the method by taking into consideration the morphology, POS tag, semantic concept, and word sense information of a text when generating the artificial errors. Rei et al. (2017) further extended it to all types of errors. Another direction of emulating human errors is by extracting the correction patterns from GEC parallel corpora and applying the inverse of those corrections on grammatically correct sentences, as done by Yuan and Felice (2013) using the corrections from the NUCLE corpus and by Choe et al. (2019) using the corrections from the W&I training data. The correction patterns are extracted both in lexical form (an → the) and POS (NN → NNS).","How are error patterns utilized in generating synthetic data for grammatical error correction, and what makes them effective?","Error patterns are instrumental in generating synthetic data for grammatical error correction by introducing errors that commonly appear in GEC parallel corpora, thereby making them more akin to the mistakes typically made by humans. Rozovskaya and Roth (2010b) introduced three methods for injecting article errors, which are based on the error distribution found in English as a Second Language (ESL) data. These methods involve adding article errors according to the distribution of articles in a text before correction, the distribution in the corrected text, and the distribution of the article corrections themselves. Felice and Yuan (2014a) enhanced this approach by considering additional factors such as morphology, POS tags, semantic concepts, and word sense information when generating artificial errors. Rei et al. (2017) expanded this method to encompass all types of errors. Another approach to simulating human errors involves extracting correction patterns from GEC parallel corpora and applying the inverse of these corrections to grammatically correct sentences. This technique was employed by Yuan and Felice (2013) using corrections from the NUCLE corpus and by Choe et al. (2019) using corrections from the W&I training data. The correction patterns are extracted in both lexical form (e.g., ""an"" to ""the"") and POS (e.g., ""NN"" to ""NNS"").",7,2,7,4,"Question:

How are error patterns used to generate synthetic data for grammatical error correction, and why are they effective?

Answer:

Error patterns are used to generate synthetic data for grammatical error correction by injecting errors that frequently occur in GEC parallel corpora. In this way, the errors are more similar to the ones that humans usually make. Rozovskaya and Roth (2010b) proposed three different methods of injecting article errors, based on the error distribution in English as a Second Language (ESL) data. They proposed adding article errors based on the distribution of articles in a text before correction, the distribution of articles in the corrected text, and the distribution of article corrections themselves. Felice and Yuan (2014a) later improved the method by taking into consideration the morphology, POS tag, semantic concept, and word sense information of a text when generating the artificial errors. Rei et al. (2017) further extended it to all types of errors. Another direction of emulating human errors is by extracting the correction patterns from GEC parallel corpora and applying the inverse of those corrections on grammatically correct sentences, as done by Yuan and Felice (2013) using the corrections from the NUCLE corpus and by Choe et al. (2019) using the corrections from the W&I training data. The correction patterns are extracted both in lexical form (an → the) and POS (NN → NNS).","Question:

How are error patterns utilized in generating synthetic data for grammatical error correction, and what makes them effective?

Answer:

Error patterns are instrumental in generating synthetic data for grammatical error correction by introducing errors that commonly appear in GEC parallel corpora, thereby making them more akin to the mistakes typically made by humans. Rozovskaya and Roth (2010b) introduced three methods for injecting article errors, which are based on the error distribution found in English as a Second Language (ESL) data. These methods involve adding article errors according to the distribution of articles in a text before correction, the distribution in the corrected text, and the distribution of the article corrections themselves. Felice and Yuan (2014a) enhanced this approach by considering additional factors such as morphology, POS tags, semantic concepts, and word sense information when generating artificial errors. Rei et al. (2017) expanded this method to encompass all types of errors. Another approach to simulating human errors involves extracting correction patterns from GEC parallel corpora and applying the inverse of these corrections to grammatically correct sentences. This technique was employed by Yuan and Felice (2013) using corrections from the NUCLE corpus and by Choe et al. (2019) using corrections from the W&I training data. The correction patterns are extracted in both lexical form (e.g., ""an"" to ""the"") and POS (e.g., ""NN"" to ""NNS"").",NO,True,1358,True,True
253447259-s34,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Back-translation,"Emulating human errors can be made in a more automated and dynamic way via a noisy channel model. The noisy channel model is trained with the inverse of a GEC parallel corpus, treating the learner's sentence as the target and the reference sentence as the source. This technique is commonly called back-translation. The technique was originally proposed for generating additional data in machine translation (Sennrich, Haddow, and Birch 2016), but it is also directly applicable to GEC. Rei et al. (2017) were the first to apply back-translation to grammatical error detection (GED) and Xie et al. (2018) were the first to apply it to GEC. Yuan et al. (2019) add a form of quality control to Rei et al. (2017) based on language model probabilities in an effort to make sure that the generated synthetic sentences are less probable (and hence hopefully less grammatical) than the original input sentences. Between the rulebased and back-translation strategy, Kiyono et al. (2019) report that the back-translation strategy has better empirical performance. They also compare back-translation with a noisy beam-search strategy (Xie et al. 2018) and back-translation with sampling strategy (Edunov et al. 2018), and report that both achieve competitive performance. Koyama et al. (2021) furthermore compare the effect of using different architectures (e.g. CNN, LSTM, Transformer) for back-translation, and find that interpolating multiple generation systems tends to produce better synthetic data to train a GEC system on. Another variant of back-translation was proposed by Stahlberg and Kumar (2021) to generate more complex edits. They found that generating a sequence of edits using Seq2Edit (Stahlberg and Kumar 2020) works better than generating the corrupted sentences directly. They also reported that back-translation with sampling worked better than beam search in their experiments.",How has back-translation been adapted and evaluated for grammatical error correction tasks?,"Emulating human errors can be made in a more automated and dynamic way via a noisy channel model. The noisy channel model is trained with the inverse of a GEC parallel corpus, treating the learner's sentence as the target and the reference sentence as the source. This technique is commonly called back-translation. Rei et al. (2017) were the first to apply back-translation to grammatical error detection (GED) and Xie et al. (2018) were the first to apply it to GEC. Yuan et al. (2019) add a form of quality control to Rei et al. (2017) based on language model probabilities in an effort to make sure that the generated synthetic sentences are less probable (and hence hopefully less grammatical) than the original input sentences. Between the rule-based and back-translation strategy, Kiyono et al. (2019) report that the back-translation strategy has better empirical performance. They also compare back-translation with a noisy beam-search strategy (Xie et al. 2018) and back-translation with sampling strategy (Edunov et al. 2018), and report that both achieve competitive performance. Koyama et al. (2021) furthermore compare the effect of using different architectures (e.g. CNN, LSTM, Transformer) for back-translation, and find that interpolating multiple generation systems tends to produce better synthetic data to train a GEC system on. Another variant of back-translation was proposed by Stahlberg and Kumar (2021) to generate more complex edits. They found that generating a sequence of edits using Seq2Edit (Stahlberg and Kumar 2020) works better than generating the corrupted sentences directly. They also reported that back-translation with sampling worked better than beam search in their experiments.",How has back-translation been adapted and evaluated for grammatical error correction tasks?,"Back-translation has been effectively adapted for grammatical error correction (GEC) tasks by emulating human errors through a noisy channel model. This model is trained using the inverse of a GEC parallel corpus, where the learner's sentence is treated as the target and the reference sentence as the source. This approach, known as back-translation, was first applied to grammatical error detection (GED) by Rei et al. (2017) and later to GEC by Xie et al. (2018).

Yuan et al. (2019) introduced a quality control mechanism to the method proposed by Rei et al. (2017), utilizing language model probabilities to ensure that the generated synthetic sentences are less probable, and thus hopefully less grammatical, than the original input sentences. Kiyono et al. (2019) found that the back-translation strategy outperformed rule-based methods and also compared favorably against a noisy beam-search strategy (Xie et al. 2018) and a sampling strategy (Edunov et al. 2018), both of which achieved competitive performance.

Further exploration by Koyama et al. (2021) examined the impact of different architectures, such as CNN, LSTM, and Transformer, on back-translation. They discovered that interpolating multiple generation systems tends to produce better synthetic data for training a GEC system. Additionally, Stahlberg and Kumar (2021) proposed a variant of back-translation to generate more complex edits. They found that using Seq2Edit (Stahlberg and Kumar 2020) to generate a sequence of edits was more effective than generating corrupted sentences directly. Their experiments also indicated that back-translation with sampling performed better than beam search.",8,2,8,2,"Question:

How has back-translation been adapted and evaluated for grammatical error correction tasks?

Answer:

Emulating human errors can be made in a more automated and dynamic way via a noisy channel model. The noisy channel model is trained with the inverse of a GEC parallel corpus, treating the learner's sentence as the target and the reference sentence as the source. This technique is commonly called back-translation. Rei et al. (2017) were the first to apply back-translation to grammatical error detection (GED) and Xie et al. (2018) were the first to apply it to GEC. Yuan et al. (2019) add a form of quality control to Rei et al. (2017) based on language model probabilities in an effort to make sure that the generated synthetic sentences are less probable (and hence hopefully less grammatical) than the original input sentences. Between the rule-based and back-translation strategy, Kiyono et al. (2019) report that the back-translation strategy has better empirical performance. They also compare back-translation with a noisy beam-search strategy (Xie et al. 2018) and back-translation with sampling strategy (Edunov et al. 2018), and report that both achieve competitive performance. Koyama et al. (2021) furthermore compare the effect of using different architectures (e.g. CNN, LSTM, Transformer) for back-translation, and find that interpolating multiple generation systems tends to produce better synthetic data to train a GEC system on. Another variant of back-translation was proposed by Stahlberg and Kumar (2021) to generate more complex edits. They found that generating a sequence of edits using Seq2Edit (Stahlberg and Kumar 2020) works better than generating the corrupted sentences directly. They also reported that back-translation with sampling worked better than beam search in their experiments.","Question:

How has back-translation been adapted and evaluated for grammatical error correction tasks?

Answer:

Back-translation has been effectively adapted for grammatical error correction (GEC) tasks by emulating human errors through a noisy channel model. This model is trained using the inverse of a GEC parallel corpus, where the learner's sentence is treated as the target and the reference sentence as the source. This approach, known as back-translation, was first applied to grammatical error detection (GED) by Rei et al. (2017) and later to GEC by Xie et al. (2018).

Yuan et al. (2019) introduced a quality control mechanism to the method proposed by Rei et al. (2017), utilizing language model probabilities to ensure that the generated synthetic sentences are less probable, and thus hopefully less grammatical, than the original input sentences. Kiyono et al. (2019) found that the back-translation strategy outperformed rule-based methods and also compared favorably against a noisy beam-search strategy (Xie et al. 2018) and a sampling strategy (Edunov et al. 2018), both of which achieved competitive performance.

Further exploration by Koyama et al. (2021) examined the impact of different architectures, such as CNN, LSTM, and Transformer, on back-translation. They discovered that interpolating multiple generation systems tends to produce better synthetic data for training a GEC system. Additionally, Stahlberg and Kumar (2021) proposed a variant of back-translation to generate more complex edits. They found that using Seq2Edit (Stahlberg and Kumar 2020) to generate a sequence of edits was more effective than generating corrupted sentences directly. Their experiments also indicated that back-translation with sampling performed better than beam search.",NO,True,1670,True,True
253447259-s36,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Augmenting Official Datasets,"Besides generating synthetic data to address the data sparsity problem in GEC, other works focus on augmenting official datasets, via noise reduction or model enhancement.

Noise reduction aims to reduce the impact of wrong corrections in the official GEC datasets. One direction focuses on correcting noisy sentences. Mita et al. (2020) and Rothe et al. (2021) achieve this by incorporating a well-trained GEC model to reduce wrong corrections. The other direction attempts to down-weight noisy sentences. Lichtarge, Alberti, and Kumar (2020) introduce an offline re-weighting method to score each training sentence based on delta-log perplexity, ∆ppl, which measures the model's log perplexity difference between checkpoints for a single sentence. Sentences with lower ∆ppl are preferred and assigned a higher weight during training.

Model enhancement augments official datasets to address the model's weakness. Parnow, Li, and Zhao (2021) aim to enhance performance by reducing the error density mismatch between training and inference. They use a generative adversarial network (GAN) (Goodfellow et al. 2014) to produce an ungrammatical sentence that could better represent the error density at inference time. Lai et al. (2022) also address the mismatch between training and inference, but specific to multi-round inference. They propose additional training stages that make the model consider edit type interdependence when predicting the corrections. Cao, Yang, and Ng (2021) aim to enhance model performance in low-error density domains. The augmented sentences are generated by beam search to capture wrong corrections that the model tends to make. Supervised contrastive learning (Chen et al. 2020b) is then applied to enhance model performance. Cao, Yang, and Ng (2023) use augmented sentences generated during beam search to address the exposure bias problem in seq2seq GEC models. A dynamic data reweighting method through reinforcement learning is used to select an optimal sampling strategy for different beam search candidates.",How do different methods aim to enhance model performance in grammatical error correction by augmenting official datasets?,"Besides generating synthetic data to address the data sparsity problem in GEC, other works focus on augmenting official datasets, via noise reduction or model enhancement. Noise reduction aims to reduce the impact of wrong corrections in the official GEC datasets. One direction focuses on correcting noisy sentences. Mita et al. (2020) and Rothe et al. (2021) achieve this by incorporating a well-trained GEC model to reduce wrong corrections. The other direction attempts to down-weight noisy sentences. Lichtarge, Alberti, and Kumar (2020) introduce an offline re-weighting method to score each training sentence based on delta-log perplexity, ∆ppl, which measures the model's log perplexity difference between checkpoints for a single sentence. Sentences with lower ∆ppl are preferred and assigned a higher weight during training. Model enhancement augments official datasets to address the model's weakness. Parnow, Li, and Zhao (2021) aim to enhance performance by reducing the error density mismatch between training and inference. They use a generative adversarial network (GAN) (Goodfellow et al. 2014) to produce an ungrammatical sentence that could better represent the error density at inference time. Lai et al. (2022) also address the mismatch between training and inference, but specific to multi-round inference. They propose additional training stages that make the model consider edit type interdependence when predicting the corrections. Cao, Yang, and Ng (2021) aim to enhance model performance in low-error density domains. The augmented sentences are generated by beam search to capture wrong corrections that the model tends to make. Supervised contrastive learning (Chen et al. 2020b) is then applied to enhance model performance. Cao, Yang, and Ng (2023) use augmented sentences generated during beam search to address the exposure bias problem in seq2seq GEC models. A dynamic data reweighting method through reinforcement learning is used to select an optimal sampling strategy for different beam search candidates.",How do various methods improve model performance in grammatical error correction by augmenting official datasets?,"To enhance model performance in grammatical error correction (GEC), several methods focus on augmenting official datasets through noise reduction and model enhancement. Noise reduction aims to mitigate the impact of incorrect corrections in GEC datasets. One approach involves correcting noisy sentences, as demonstrated by Mita et al. (2020) and Rothe et al. (2021), who use a well-trained GEC model to minimize wrong corrections. Another approach is to down-weight noisy sentences, as proposed by Lichtarge, Alberti, and Kumar (2020). They introduce an offline re-weighting method that scores each training sentence based on delta-log perplexity (∆ppl), which measures the model's log perplexity difference between checkpoints for a single sentence. Sentences with lower ∆ppl are preferred and assigned a higher weight during training.

Model enhancement strategies aim to address the model's weaknesses by augmenting official datasets. Parnow, Li, and Zhao (2021) focus on reducing the error density mismatch between training and inference using a generative adversarial network (GAN) (Goodfellow et al. 2014) to generate ungrammatical sentences that better represent the error density at inference time. Lai et al. (2022) also tackle the mismatch between training and inference, specifically for multi-round inference, by proposing additional training stages that consider edit type interdependence when predicting corrections. Cao, Yang, and Ng (2021) aim to improve model performance in low-error density domains by generating augmented sentences through beam search to capture common wrong corrections. They apply supervised contrastive learning (Chen et al. 2020b) to further enhance model performance. Additionally, Cao, Yang, and Ng (2023) address the exposure bias problem in seq2seq GEC models by using augmented sentences generated during beam search. They employ a dynamic data reweighting method through reinforcement learning to select an optimal sampling strategy for different beam search candidates.",7,4,7,4,"Question:

How do different methods aim to enhance model performance in grammatical error correction by augmenting official datasets?

Answer:

Besides generating synthetic data to address the data sparsity problem in GEC, other works focus on augmenting official datasets, via noise reduction or model enhancement. Noise reduction aims to reduce the impact of wrong corrections in the official GEC datasets. One direction focuses on correcting noisy sentences. Mita et al. (2020) and Rothe et al. (2021) achieve this by incorporating a well-trained GEC model to reduce wrong corrections. The other direction attempts to down-weight noisy sentences. Lichtarge, Alberti, and Kumar (2020) introduce an offline re-weighting method to score each training sentence based on delta-log perplexity, ∆ppl, which measures the model's log perplexity difference between checkpoints for a single sentence. Sentences with lower ∆ppl are preferred and assigned a higher weight during training. Model enhancement augments official datasets to address the model's weakness. Parnow, Li, and Zhao (2021) aim to enhance performance by reducing the error density mismatch between training and inference. They use a generative adversarial network (GAN) (Goodfellow et al. 2014) to produce an ungrammatical sentence that could better represent the error density at inference time. Lai et al. (2022) also address the mismatch between training and inference, but specific to multi-round inference. They propose additional training stages that make the model consider edit type interdependence when predicting the corrections. Cao, Yang, and Ng (2021) aim to enhance model performance in low-error density domains. The augmented sentences are generated by beam search to capture wrong corrections that the model tends to make. Supervised contrastive learning (Chen et al. 2020b) is then applied to enhance model performance. Cao, Yang, and Ng (2023) use augmented sentences generated during beam search to address the exposure bias problem in seq2seq GEC models. A dynamic data reweighting method through reinforcement learning is used to select an optimal sampling strategy for different beam search candidates.","Question:

How do various methods improve model performance in grammatical error correction by augmenting official datasets?

Answer:

To enhance model performance in grammatical error correction (GEC), several methods focus on augmenting official datasets through noise reduction and model enhancement. Noise reduction aims to mitigate the impact of incorrect corrections in GEC datasets. One approach involves correcting noisy sentences, as demonstrated by Mita et al. (2020) and Rothe et al. (2021), who use a well-trained GEC model to minimize wrong corrections. Another approach is to down-weight noisy sentences, as proposed by Lichtarge, Alberti, and Kumar (2020). They introduce an offline re-weighting method that scores each training sentence based on delta-log perplexity (∆ppl), which measures the model's log perplexity difference between checkpoints for a single sentence. Sentences with lower ∆ppl are preferred and assigned a higher weight during training.

Model enhancement strategies aim to address the model's weaknesses by augmenting official datasets. Parnow, Li, and Zhao (2021) focus on reducing the error density mismatch between training and inference using a generative adversarial network (GAN) (Goodfellow et al. 2014) to generate ungrammatical sentences that better represent the error density at inference time. Lai et al. (2022) also tackle the mismatch between training and inference, specifically for multi-round inference, by proposing additional training stages that consider edit type interdependence when predicting corrections. Cao, Yang, and Ng (2021) aim to improve model performance in low-error density domains by generating augmented sentences through beam search to capture common wrong corrections. They apply supervised contrastive learning (Chen et al. 2020b) to further enhance model performance. Additionally, Cao, Yang, and Ng (2023) address the exposure bias problem in seq2seq GEC models by using augmented sentences generated during beam search. They employ a dynamic data reweighting method through reinforcement learning to select an optimal sampling strategy for different beam search candidates.",NO,True,2018,True,True
253447259-s43,Grammatical Error Correction: A Survey of the State of the Art,2022-11-09,Reference-based Metrics,"I-measure. The I -measure (Felice and Briscoe 2015) was designed to overcome certain shortcomings of the M 2 scorer, e.g. the M 2 scorer is unable to differentiate between a bad system (TP=0, FP>0) and a do-nothing system (TP=0, FP=0) which both result in F=0, and instead measure system performance in terms of relative textual Improvement. The I -measure is calculated by carrying out a 3-way alignment between the original, hypothesis and reference texts and classifying each token according to an extended version of the Writer-Annotator-System (WAS) evaluation scheme . This ultimately enables the calculation of accuracy, which Felice and Briscoe (2015) modify to weight TPs and FPs differently to more intuitively reward or punish a system. Having calculated a weighted accuracy score for a system, a baseline weighted accuracy score is computed in the same manner using a copy of the original text as the hypothesis. The difference between these scores is then normalised to fall between -1 and 1, where I < 0 indicates text degradation and I > 0 indicates text improvement.

GMEG. The GMEG metric (Napoles, Nădejde, and Tetreault 2019) is an ensemble metric that was designed to correlate with human judgements on three different datasets. It was motivated by the observation that different metrics correlate very differently with human judgements in different domains, and so a better metric would be more consistent. As an ensemble metric, GMEG depends on features (e.g. precision and recall) from several other metrics, including M 2 , ERRANT, GLEU, and the I -measure (73 features in total). The authors then use these features to train a ridge regression model that was optimised to predict the human scores for different systems.

GoToScorer. The GoToScorer (Gotou et al. 2020) was motivated by the observation that some errors are more difficult to correct than others yet all metrics treat them equally. The GoToScorer hence models error difficulty by weighting edits according to how many different systems were able to correct them; e.g., edits that were successfully corrected by all systems would yield a smaller reward than those successfully corrected by fewer systems. Although this methodology confirmed the intuition that some errors types were easier to correct than others, e.g. spelling errors (easy) vs. synonym errors (hard), one disadvantage of this approach is that the difficulty weights depend entirely on the type and number of systems involved. Consequently, results do not generalise well and error difficulty (or gravity) remains an unsolved problem.

SErCl/SERRANT. SErCl (Choshen et al. 2020) is not a metric per se, but rather a method of automatically classifying grammatical errors by their syntactic properties using the Universal Dependencies formalism (Nivre et al. 2020). It is hence similar to ERRANT except it can more easily support other languages. The main disadvantage of SErCl is that it is not always meaningful to classify errors entirely based on their syntactic properties, e.g. spelling and orthography errors, and some error types are not very informative, e.g. ""VERB→ADJ"". SERRANT (Choshen et al. 2021) is hence a compromise that attempts to combine the advantages of both SErCl and ERRANT.

PT-M 2 . The pretraining-based MaxMatch (PT-M 2 ) metric (Gong et al. 2022) is a hybrid metric that combines traditional edit-based metrics, such as M 2 , with recent pretraining-based metrics, such as BERTScore ). The main advantage of pretraining-based metrics over edit-based metrics is that they are more capable of measuring the semantic similarity between pairs of sentences, rather than just comparing edits. Since Gong et al. (2022) found that off-the-shelf pretraining metrics correlated poorly with human judgements on GEC at the sentence level, they instead proposed measuring performance at the edit level. This approach ultimately produced the highest correlation with human judgements on the CoNLL-2014 test set to date, but should be considered with caution, as Hanna and Bojar (2021) also highlight some of the limitations of pretraining metrics and cite sources that claim correlation with human judgements may not be the best way to evaluate a metric (see Section 6.5).",How do various reference-based metrics address the limitations of traditional grammatical error correction evaluation methods?,"The I-measure (Felice and Briscoe 2015) was designed to overcome certain shortcomings of the M2 scorer, such as its inability to differentiate between a bad system and a do-nothing system, by measuring system performance in terms of relative textual improvement. The I-measure calculates a weighted accuracy score for a system and compares it to a baseline score, normalizing the difference to indicate text degradation or improvement. The GMEG metric (Napoles, Nădejde, and Tetreault 2019) is an ensemble metric that correlates with human judgments by using features from several other metrics to train a ridge regression model optimized to predict human scores. The GoToScorer (Gotou et al. 2020) models error difficulty by weighting edits based on how many systems corrected them, acknowledging that some errors are more difficult to correct than others. SErCl (Choshen et al. 2020) classifies grammatical errors by their syntactic properties, similar to ERRANT, but can support other languages more easily. SERRANT (Choshen et al. 2021) combines the advantages of SErCl and ERRANT. The PT-M2 metric (Gong et al. 2022) combines traditional edit-based metrics with pretraining-based metrics to measure semantic similarity at the edit level, achieving high correlation with human judgments.",How do reference-based metrics improve upon traditional methods for evaluating grammatical error correction?,"Reference-based metrics have been developed to address the limitations of traditional grammatical error correction evaluation methods. The I-measure (Felice and Briscoe 2015) was introduced to tackle issues with the M2 scorer, particularly its inability to distinguish between a poor system and a do-nothing system. It does this by assessing system performance in terms of relative textual improvement, calculating a weighted accuracy score, and comparing it to a baseline to indicate text degradation or improvement. The GMEG metric (Napoles, Nădejde, and Tetreault 2019) enhances evaluation by correlating with human judgments. It achieves this by using features from several other metrics to train a ridge regression model optimized to predict human scores. The GoToScorer (Gotou et al. 2020) acknowledges the varying difficulty of errors by weighting edits based on how many systems corrected them, recognizing that some errors are more challenging to correct. SErCl (Choshen et al. 2020) classifies grammatical errors by their syntactic properties, similar to ERRANT, but with better support for other languages. SERRANT (Choshen et al. 2021) combines the strengths of SErCl and ERRANT. Lastly, the PT-M2 metric (Gong et al. 2022) merges traditional edit-based metrics with pretraining-based metrics to measure semantic similarity at the edit level, achieving a high correlation with human judgments.",7,4,7,2,"Question:

How do various reference-based metrics address the limitations of traditional grammatical error correction evaluation methods?

Answer:

The I-measure (Felice and Briscoe 2015) was designed to overcome certain shortcomings of the M2 scorer, such as its inability to differentiate between a bad system and a do-nothing system, by measuring system performance in terms of relative textual improvement. The I-measure calculates a weighted accuracy score for a system and compares it to a baseline score, normalizing the difference to indicate text degradation or improvement. The GMEG metric (Napoles, Nădejde, and Tetreault 2019) is an ensemble metric that correlates with human judgments by using features from several other metrics to train a ridge regression model optimized to predict human scores. The GoToScorer (Gotou et al. 2020) models error difficulty by weighting edits based on how many systems corrected them, acknowledging that some errors are more difficult to correct than others. SErCl (Choshen et al. 2020) classifies grammatical errors by their syntactic properties, similar to ERRANT, but can support other languages more easily. SERRANT (Choshen et al. 2021) combines the advantages of SErCl and ERRANT. The PT-M2 metric (Gong et al. 2022) combines traditional edit-based metrics with pretraining-based metrics to measure semantic similarity at the edit level, achieving high correlation with human judgments.","Question:

How do reference-based metrics improve upon traditional methods for evaluating grammatical error correction?

Answer:

Reference-based metrics have been developed to address the limitations of traditional grammatical error correction evaluation methods. The I-measure (Felice and Briscoe 2015) was introduced to tackle issues with the M2 scorer, particularly its inability to distinguish between a poor system and a do-nothing system. It does this by assessing system performance in terms of relative textual improvement, calculating a weighted accuracy score, and comparing it to a baseline to indicate text degradation or improvement. The GMEG metric (Napoles, Nădejde, and Tetreault 2019) enhances evaluation by correlating with human judgments. It achieves this by using features from several other metrics to train a ridge regression model optimized to predict human scores. The GoToScorer (Gotou et al. 2020) acknowledges the varying difficulty of errors by weighting edits based on how many systems corrected them, recognizing that some errors are more challenging to correct. SErCl (Choshen et al. 2020) classifies grammatical errors by their syntactic properties, similar to ERRANT, but with better support for other languages. SERRANT (Choshen et al. 2021) combines the strengths of SErCl and ERRANT. Lastly, the PT-M2 metric (Gong et al. 2022) merges traditional edit-based metrics with pretraining-based metrics to measure semantic similarity at the edit level, achieving a high correlation with human judgments.",NO,True,1405,True,True
221970053-s4,A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English,2020-09-27,Rule and Feature Based Methods,"Before the deep learning era, human-designed rules (Hobbs, 1978;Raghunathan et al., 2010), knowledge (Ponzetto and Strube, 2006;Versley et al., 2016), and features (Ng, 2005;Wiseman et al., 2016) dominated the general coreference resolution and PCR tasks. Some rules and features are crucial for correctly resolving pronouns (Lee et al., 2013). For example, 'he' typically refers to males and 'she' typically refers to females; 'it' typically refers to singular objects and 'them' typically refers to plural objects. The performances of these methods heavily rely on the coverage and quality of the manually defined rules and features. Based on these designed features (Bengtson and Roth, 2008), a few more advanced machine learning models were applied to the coreference resolution task. For example, instead of identifying coreference relation pair-wisely, (Clark and Manning, 2015) proposes an entity-centric coreference system that can learn an effective policy for building coreference chains incrementally. Besides that, a novel model was also proposed to predict coreference relations with a deep reinforcement learning framework (Clark and Manning, 2016). Moreover, heuristic rules based on linguistic knowledge can also be incorporated into constraints for machine learning models .",How were advanced machine learning models applied to coreference resolution tasks before deep learning?  ,"Before the deep learning era, human-designed rules, knowledge, and features dominated the general coreference resolution and PCR tasks. Based on these designed features, a few more advanced machine learning models were applied to the coreference resolution task. For example, instead of identifying coreference relation pair-wisely, Clark and Manning (2015) proposed an entity-centric coreference system that can learn an effective policy for building coreference chains incrementally. Additionally, a novel model was proposed to predict coreference relations with a deep reinforcement learning framework (Clark and Manning, 2016). Moreover, heuristic rules based on linguistic knowledge can also be incorporated into constraints for machine learning models.",How were advanced machine learning models utilized for coreference resolution tasks prior to the advent of deep learning?,"Before the deep learning era, coreference resolution tasks primarily relied on human-designed rules, knowledge, and features. These elements formed the foundation for more advanced machine learning models applied to the task. For instance, instead of identifying coreference relations on a pair-wise basis, Clark and Manning (2015) introduced an entity-centric coreference system capable of learning an effective policy for incrementally building coreference chains. Furthermore, a novel approach was developed to predict coreference relations using a deep reinforcement learning framework (Clark and Manning, 2016). Additionally, heuristic rules based on linguistic knowledge were often incorporated as constraints within these machine learning models.",7,2,7,4,"Question:

How were advanced machine learning models applied to coreference resolution tasks before deep learning?  

Answer:

Before the deep learning era, human-designed rules, knowledge, and features dominated the general coreference resolution and PCR tasks. Based on these designed features, a few more advanced machine learning models were applied to the coreference resolution task. For example, instead of identifying coreference relation pair-wisely, Clark and Manning (2015) proposed an entity-centric coreference system that can learn an effective policy for building coreference chains incrementally. Additionally, a novel model was proposed to predict coreference relations with a deep reinforcement learning framework (Clark and Manning, 2016). Moreover, heuristic rules based on linguistic knowledge can also be incorporated into constraints for machine learning models.","Question:

How were advanced machine learning models utilized for coreference resolution tasks prior to the advent of deep learning?

Answer:

Before the deep learning era, coreference resolution tasks primarily relied on human-designed rules, knowledge, and features. These elements formed the foundation for more advanced machine learning models applied to the task. For instance, instead of identifying coreference relations on a pair-wise basis, Clark and Manning (2015) introduced an entity-centric coreference system capable of learning an effective policy for incrementally building coreference chains. Furthermore, a novel approach was developed to predict coreference relations using a deep reinforcement learning framework (Clark and Manning, 2016). Additionally, heuristic rules based on linguistic knowledge were often incorporated as constraints within these machine learning models.",NO,True,753,True,True
221970053-s6,A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English,2020-09-27,Further Improvements,"Recently, on top of the end-to-end model, a few improved works were proposed to address different limitations of the original end-to-end model 5 :

1. Higher-order Information: One limitation of the original end-to-end model is that all predictions are based on pairs, which is not sufficient for capturing higher-order coreference relations. To fix this issue, a differentiable approximation module was proposed in  to provide the higher-order coreference resolution inference ability (i.e., leveraging the coreference cluster to better predict the coreference relations). Moreover, this work first incorporates ELMo (Peters et al., 2018), a kind of deep contextualized word representations, as part of the word representation, which is proven very effective.

2. Structured Knowledge: Another limitation of the end-to-end model is that its success heavily relies on the quality and coverage of the training data. However, in real applications, it is labor-intensive and almost impossible to annotate a large-scale dataset to contain all scenar-  (Clark and Manning, 2015) 25.8 62.1 36.5 28.9 64.9 40.0 9.8 6.3 7.6 25.4 59.3 36.5 Deep-RL (Clark and Manning, 2016) 78.6 63.9 70.5 73.3 68.9 71.0 3.7 2.9 5.5 76.4 61.2 68.0

End-to-end (Lee et al., 2017) (Zhang et al., 2019c) 80.0 75.6 77.7 81.7 72.2 76.7 50.8 64.6 56.9 77.9 74.0 75.9 + SpanBERT (Joshi et al., 2020) 82   ios. To solve this problem, two works (Zhang et al., 2019b,c) were proposed to inject external structured knowledge into the end-to-end model. Among these two, (Zhang et al., 2019b) requires converting external knowledge into features while (Zhang et al., 2019c) directly uses external knowledge in the format of triplets.",How have recent works addressed the limitations of the end-to-end model in pronoun coreference resolution?,"Recently, on top of the end-to-end model, a few improved works were proposed to address different limitations of the original end-to-end model. One limitation of the original end-to-end model is that all predictions are based on pairs, which is not sufficient for capturing higher-order coreference relations. To fix this issue, a differentiable approximation module was proposed to provide the higher-order coreference resolution inference ability, leveraging the coreference cluster to better predict the coreference relations. Moreover, this work first incorporates ELMo (Peters et al., 2018), a kind of deep contextualized word representations, as part of the word representation, which is proven very effective. Another limitation of the end-to-end model is that its success heavily relies on the quality and coverage of the training data. To solve this problem, two works (Zhang et al., 2019b,c) were proposed to inject external structured knowledge into the end-to-end model. Among these two, Zhang et al. (2019b) requires converting external knowledge into features while Zhang et al. (2019c) directly uses external knowledge in the format of triplets.",What recent advancements have been made to overcome the limitations of the end-to-end model in pronoun coreference resolution?,"Recent advancements have been made to enhance the end-to-end model for pronoun coreference resolution by addressing its limitations. One significant limitation of the original end-to-end model is its reliance on pairwise predictions, which are inadequate for capturing higher-order coreference relations. To address this, a differentiable approximation module was introduced, enabling higher-order coreference resolution by leveraging coreference clusters to improve the prediction of coreference relations. Additionally, this approach was the first to incorporate ELMo (Peters et al., 2018), a type of deep contextualized word representation, into the word representation, proving to be highly effective. Another limitation of the end-to-end model is its dependency on the quality and coverage of the training data. To mitigate this issue, two works by Zhang et al. (2019b, c) proposed integrating external structured knowledge into the end-to-end model. Zhang et al. (2019b) focused on converting external knowledge into features, while Zhang et al. (2019c) utilized external knowledge directly in the form of triplets.",7,4,7,4,"Question:

How have recent works addressed the limitations of the end-to-end model in pronoun coreference resolution?

Answer:

Recently, on top of the end-to-end model, a few improved works were proposed to address different limitations of the original end-to-end model. One limitation of the original end-to-end model is that all predictions are based on pairs, which is not sufficient for capturing higher-order coreference relations. To fix this issue, a differentiable approximation module was proposed to provide the higher-order coreference resolution inference ability, leveraging the coreference cluster to better predict the coreference relations. Moreover, this work first incorporates ELMo (Peters et al., 2018), a kind of deep contextualized word representations, as part of the word representation, which is proven very effective. Another limitation of the end-to-end model is that its success heavily relies on the quality and coverage of the training data. To solve this problem, two works (Zhang et al., 2019b,c) were proposed to inject external structured knowledge into the end-to-end model. Among these two, Zhang et al. (2019b) requires converting external knowledge into features while Zhang et al. (2019c) directly uses external knowledge in the format of triplets.","Question:

What recent advancements have been made to overcome the limitations of the end-to-end model in pronoun coreference resolution?

Answer:

Recent advancements have been made to enhance the end-to-end model for pronoun coreference resolution by addressing its limitations. One significant limitation of the original end-to-end model is its reliance on pairwise predictions, which are inadequate for capturing higher-order coreference relations. To address this, a differentiable approximation module was introduced, enabling higher-order coreference resolution by leveraging coreference clusters to improve the prediction of coreference relations. Additionally, this approach was the first to incorporate ELMo (Peters et al., 2018), a type of deep contextualized word representation, into the word representation, proving to be highly effective. Another limitation of the end-to-end model is its dependency on the quality and coverage of the training data. To mitigate this issue, two works by Zhang et al. (2019b, c) proposed integrating external structured knowledge into the end-to-end model. Zhang et al. (2019b) focused on converting external knowledge into features, while Zhang et al. (2019c) utilized external knowledge directly in the form of triplets.",NO,True,1121,True,True
221970053-s17,A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English,2020-09-27,Language Representation Models,"Another approach is leveraging language models to solve WSC questions (Trinh and Le, 2018), where each WSC question is first converted into two sentences by replacing the target pronoun with the two candidates respectively and then the language models can be employed to compute the probability of both sentences. The sentence with a higher probability will be selected as the final prediction. As this method does not require any string match, it can make prediction for all WSC questions and achieve better overall performance. Recently, a more advanced transformer-based language model GPT-2 (Radford et al., 2019) achieved better performance due to its stronger language representation ability. The success of language models demonstrates that rich commonsense knowledge can be indeed encoded within language models implicitly.

Another interesting finding about these language model based approaches is that they proposed two settings to predict the probability: (1) Full: use the probability of the whole sentence as the final prediction;

(2) Partial: only consider the probability of the partial sentence after the target pronoun. Experiments show that the partial model always outperforms the full model. One explanation is that the influence of the imbalanced distribution of candidate words is relieved by only considering the sentence probability after them. Such observation also explains why GPT-2 can outperform unsuper-  (Emami et al., 2018) 119 79 75 60.1% 57.3% SP (Human) (Zhang et al., 2019a) 15 0 258 100% 52.7% SP (PP) (Zhang et al., 2019a) 50 26 197 65.8% 54.4% ASER (String Match) (Zhang et al., 2020) 63 27 183 70.0% 56.6% LM (Single) (Trinh and Le, 2018) 149 124 0 54.5% 54.5% LM (Ensemble) (Trinh and Le, 2018) 168 105 0 61.5% 61.5% GPT-2 (Radford et al., 2019) 193 80 0 70.7% 70.7%

Finetuning BERT (Devlin et al., 2019) +ASER (Zhang et al., 2020) 177 96 0 64.5% 64.5% BERT (Devlin et al., 2019) +DPR (Rahman and Ng, 2012) 195 78 0 71.4% 71.4% BERT (Devlin et al., 2019) +WinoGrande (Sakaguchi et al., 2020) 210 63 0 76.9% 76.9% RoBERTa (Liu et al., 2019) +DRP (Rahman and Ng, 2012) 227 46 0 83.1% 83.1% RoBERTa (Liu et al., 2019) +WinoGrande (Sakaguchi et al., 2020) 246 27 0 90.1% 90.1%","What are the two settings for predicting probability in language model-based approaches, and which setting performs better?",Another interesting finding about these language model-based approaches is that they proposed two settings to predict the probability: (1) Full: use the probability of the whole sentence as the final prediction; (2) Partial: only consider the probability of the partial sentence after the target pronoun. Experiments show that the partial model always outperforms the full model.,"What are the two approaches for predicting probability in language model-based methods, and which approach yields better performance?","In language model-based methods, there are two primary approaches for predicting probability: (1) the Full approach, which uses the probability of the entire sentence as the final prediction, and (2) the Partial approach, which considers only the probability of the sentence portion following the target pronoun. Experiments have consistently shown that the Partial approach outperforms the Full approach.",7,2,8,2,"Question:

What are the two settings for predicting probability in language model-based approaches, and which setting performs better?

Answer:

Another interesting finding about these language model-based approaches is that they proposed two settings to predict the probability: (1) Full: use the probability of the whole sentence as the final prediction; (2) Partial: only consider the probability of the partial sentence after the target pronoun. Experiments show that the partial model always outperforms the full model.","Question:

What are the two approaches for predicting probability in language model-based methods, and which approach yields better performance?

Answer:

In language model-based methods, there are two primary approaches for predicting probability: (1) the Full approach, which uses the probability of the entire sentence as the final prediction, and (2) the Partial approach, which considers only the probability of the sentence portion following the target pronoun. Experiments have consistently shown that the Partial approach outperforms the Full approach.",NO,True,405,True,True
221970053-s21,A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution in English,2020-09-27,Other PCR Tasks,"Besides the ordinary and hard PCR tasks, PCR is also an important research topic for many special purposes (e.g., gender bias) or in some special settings (e.g., Visual-aware PCR). In this section, we briefly introduce these tasks:

1. PCR in the Medical Domain: I2b2 (Uzuner et al., 2012) is a dataset that focuses on identify-10 The original batch size is 16 and our batch size is 4 due to the GPU memory limitation, so the experimental result is slightly different from the one reported in the original paper.

ing coreference relations in electronic medical records. As reported in (Zhang et al., 2019c), the training set of I2b2 contains 2,024 third personal pronouns, 685 possessive pronouns, and 270 demonstrative pronouns. Its test set contains 1,244 third personal pronouns, 367 possessive pronouns, and 166 demonstrative pronouns. As a dataset in a relatively narrow domain, the usage of domain knowledge becomes important. As shown in (Zhang et al., 2019c), i2b2 can be used as an additional dataset to evaluate models' cross-domain abilities. 3. PCR for Chatbots: CIC (Chen and Choi, 2016) is a dataset focusing on identifying coreference relations in multi-party conversations. Compared with the ordinary PCR tasks, which are mostly annotated on formal textual data (e.g., newswire), identifying coreference relation in conversation is more challenging.

4. PCR for Studying Gender Bias: Nowadays, gender bias has been a hot research topic in the NLP community (Rudinger et al., 2018;Zhao et al., 2018). WinoGender (Rudinger et al., 2018) is among the most popular works. The setting of WinoGender is similar to the setting of WSC (Levesque et al., 2012), where each sentence contains one target pronoun and two candidate noun phrases and the models are required to select the correct antecedent from the two candidates. But the purpose is different. WSC aims at evaluating models' abilities to understand commonsense knowledge, while Wino-Gender aims at evaluating how well models can predict without the influence of gender bias. The experiments show that some gender bias (e.g., 'he' is more likely to be predicted to be the doctor rather than the nurse by the machine) indeed exists in pre-trained language representation models. Such observation is astonishing and motivates the community to think about how to minimize the influence of such gender bias.",How does the WinoGender dataset aim to evaluate gender bias in pre-trained language models?,"WinoGender (Rudinger et al., 2018) is among the most popular works for studying gender bias. The setting of WinoGender is similar to the setting of WSC (Levesque et al., 2012), where each sentence contains one target pronoun and two candidate noun phrases, and the models are required to select the correct antecedent from the two candidates. However, the purpose of WinoGender is different from WSC. WinoGender aims at evaluating how well models can predict without the influence of gender bias. The experiments show that some gender bias (e.g., 'he' is more likely to be predicted to be the doctor rather than the nurse by the machine) indeed exists in pre-trained language representation models. Such observation is astonishing and motivates the community to think about how to minimize the influence of such gender bias.",How is the WinoGender dataset used to assess gender bias in pre-trained language models?,"The WinoGender dataset, introduced by Rudinger et al. in 2018, is a prominent tool for examining gender bias in language models. It is designed similarly to the Winograd Schema Challenge (WSC) by Levesque et al. in 2012, where each sentence includes a target pronoun and two candidate noun phrases. The task for the models is to identify the correct antecedent from these two options. However, unlike the WSC, the primary goal of WinoGender is to evaluate the extent to which models can make predictions without being influenced by gender bias. Experiments using WinoGender have revealed that pre-trained language models often exhibit gender bias, such as a tendency to associate 'he' more frequently with professions like doctors rather than nurses. This finding is significant and has prompted the research community to explore ways to reduce such biases in language models.",9,2,8,2,"Question:

How does the WinoGender dataset aim to evaluate gender bias in pre-trained language models?

Answer:

WinoGender (Rudinger et al., 2018) is among the most popular works for studying gender bias. The setting of WinoGender is similar to the setting of WSC (Levesque et al., 2012), where each sentence contains one target pronoun and two candidate noun phrases, and the models are required to select the correct antecedent from the two candidates. However, the purpose of WinoGender is different from WSC. WinoGender aims at evaluating how well models can predict without the influence of gender bias. The experiments show that some gender bias (e.g., 'he' is more likely to be predicted to be the doctor rather than the nurse by the machine) indeed exists in pre-trained language representation models. Such observation is astonishing and motivates the community to think about how to minimize the influence of such gender bias.","Question:

How is the WinoGender dataset used to assess gender bias in pre-trained language models?

Answer:

The WinoGender dataset, introduced by Rudinger et al. in 2018, is a prominent tool for examining gender bias in language models. It is designed similarly to the Winograd Schema Challenge (WSC) by Levesque et al. in 2012, where each sentence includes a target pronoun and two candidate noun phrases. The task for the models is to identify the correct antecedent from these two options. However, unlike the WSC, the primary goal of WinoGender is to evaluate the extent to which models can make predictions without being influenced by gender bias. Experiments using WinoGender have revealed that pre-trained language models often exhibit gender bias, such as a tendency to associate 'he' more frequently with professions like doctors rather than nurses. This finding is significant and has prompted the research community to explore ways to reduce such biases in language models.",NO,True,876,True,True
237941087-s1,Coreference Resolution for the Biomedical Domain: A Survey,2021-09-25,Background,"Coreference resolution in the general domain has a long history of being studied from early heuristicbased and rule-based approaches to recent learningbased approaches. Lee et al. (2017) proposed the first end-to-end neural coreference resolution model which uses LSTM encoder. Based on the end-to-end model, many extensions to the model have been proposed. BERT and SpanBERT were proposed to replace the LSTM encoder and achieved better performance on OntoNotes dataset (Joshi et al. 2019, Joshi et al. 2020. Wu et al. (2020) adapted questionanswering framework on coreference resolution, and achieved the state-of-the-art result with 83.1% F1 score on OntoNotes dataset. Ye et al. (2020) proposed a novel language representation model CorefBERT, which can capture the coreferential relations in context. However, these general coreference systems do not work well in the biomedical domain due to the lack of domain knowledge. For example, the end-to-end model (Lee et al., 2017) only achieved 33.85% and 61.25% F1 scores on CRAFT-CR and BioNLP datasets respectively (Trieu et al., 2018), but achieved 68.8% F1 score on OntoNotes dataset (Hovy et al., 2006), which covers multiple genres, such as newswire, broadcast news and web data.",How do general coreference resolution models perform on biomedical datasets compared to general domain datasets?,"General coreference resolution models do not work well in the biomedical domain due to the lack of domain knowledge. For example, the end-to-end model proposed by Lee et al. (2017) achieved only 33.85% and 61.25% F1 scores on the CRAFT-CR and BioNLP datasets respectively (Trieu et al., 2018). In contrast, the same model achieved a 68.8% F1 score on the OntoNotes dataset (Hovy et al., 2006), which covers multiple genres such as newswire, broadcast news, and web data.",How do general coreference resolution models perform on biomedical datasets compared to general domain datasets?,"General coreference resolution models tend to perform poorly in the biomedical domain due to their lack of domain-specific knowledge. For instance, the end-to-end model proposed by Lee et al. (2017) achieved F1 scores of only 33.85% on the CRAFT-CR dataset and 61.25% on the BioNLP dataset, as reported by Trieu et al. (2018). In contrast, the same model performed significantly better on the OntoNotes dataset, achieving a 68.8% F1 score. The OntoNotes dataset encompasses a variety of genres, including newswire, broadcast news, and web data, which are more aligned with the general domain knowledge that these models are typically trained on (Hovy et al., 2006).",8,4,7,3,"Question:

How do general coreference resolution models perform on biomedical datasets compared to general domain datasets?

Answer:

General coreference resolution models do not work well in the biomedical domain due to the lack of domain knowledge. For example, the end-to-end model proposed by Lee et al. (2017) achieved only 33.85% and 61.25% F1 scores on the CRAFT-CR and BioNLP datasets respectively (Trieu et al., 2018). In contrast, the same model achieved a 68.8% F1 score on the OntoNotes dataset (Hovy et al., 2006), which covers multiple genres such as newswire, broadcast news, and web data.","Question:

How do general coreference resolution models perform on biomedical datasets compared to general domain datasets?

Answer:

General coreference resolution models tend to perform poorly in the biomedical domain due to their lack of domain-specific knowledge. For instance, the end-to-end model proposed by Lee et al. (2017) achieved F1 scores of only 33.85% on the CRAFT-CR dataset and 61.25% on the BioNLP dataset, as reported by Trieu et al. (2018). In contrast, the same model performed significantly better on the OntoNotes dataset, achieving a 68.8% F1 score. The OntoNotes dataset encompasses a variety of genres, including newswire, broadcast news, and web data, which are more aligned with the general domain knowledge that these models are typically trained on (Hovy et al., 2006).",NO,True,665,True,True
237941087-s7,Coreference Resolution for the Biomedical Domain: A Survey,2021-09-25,Rule-based models,"Early approaches to biomedical coreference resolution are primarily rule-based. These models rely on syntactic parsers to extract hand-crafted features and rules. Nguyen et al. (2012) implemented a protein coreference system that makes use of syntactic information from the parser output, and proteinindicated information. The results showed that domain-specific semantic information is important for coreference resolution.  developed a rule-based coreference system, as a part of the EventMine event extraction system. A set of rules was developed based on syntactic trees and predicate-argument structures. The system achieved 55.9% F1 score on BioNLP 2011 protein coreference task. Kilicoglu and Demner-Fushman (2016) developed a new corpus of structured drug labels and proposed a general framework based on a smorgasbord architecture for finegrained biomedical coreference resolution. The framework adopted different strategies for each coreference type and mention type, and combined them to reach desired performance, like selecting dishes from a smorgasbord. Li et al. (2018) presented two methods for bio-entity coreference resolution: a rule-based method and a recurrent neural network (RNN) model. The rule-based model created a set of syntactic rules or semantic constraints for coreference and achieved a state-of-the-art performance with 62.0% F1 score on BioNLP 2011 protein coreference task.

These rule-based models mostly designed rules for specific type of coreference relation and even specific corpus, which limits the scope of the resolution.",What are the limitations of rule-based models in biomedical coreference resolution?,"Early approaches to biomedical coreference resolution are primarily rule-based. These rule-based models mostly designed rules for specific type of coreference relation and even specific corpus, which limits the scope of the resolution.",What are the limitations of using rule-based models for biomedical coreference resolution?,"Early approaches to biomedical coreference resolution predominantly rely on rule-based models. These models are often designed with rules tailored to specific types of coreference relations and even particular corpora, which significantly limits their applicability and scope in broader contexts.",8,2,7,2,"Question:

What are the limitations of rule-based models in biomedical coreference resolution?

Answer:

Early approaches to biomedical coreference resolution are primarily rule-based. These rule-based models mostly designed rules for specific type of coreference relation and even specific corpus, which limits the scope of the resolution.","Question:

What are the limitations of using rule-based models for biomedical coreference resolution?

Answer:

Early approaches to biomedical coreference resolution predominantly rely on rule-based models. These models are often designed with rules tailored to specific types of coreference relations and even particular corpora, which significantly limits their applicability and scope in broader contexts.",NO,True,296,False,False
237941087-s8,Coreference Resolution for the Biomedical Domain: A Survey,2021-09-25,Machine learning-based models,"In the early years, due to the lack of publicly available annotated corpora, researchers have to annotate their own corpora for developing machine learning approaches (Yang et al., 2004, Torii and Vijay-Shanker, 2005, Su et al., 2008, Gasperin, 2009.

After the BioNLP 2011 protein coreference dataset was made publicly available, several machine learning-based models were developed for this task.  adapted a general coreference system Reconcile (Stoyanov et al., 2010) for the biomedical domain by modifying several components to biomedical texts. It trained two separate classifiers for detecting anaphora and antecedent mentions.

In addition to using machine learning-based methods only, several models adopted hydrid approach, i.e., combining both machine learning-  based and rule-based methods. D'Souza and Ng (2012) proposed a hybrid approach that used a classifier with syntactic path-based features. It investigated five different learning-based methods, and a rule-based approach for anaphora resolution. This model achieved a superior performance than previous either rule-based or learning-based models on BioNLP 2011 protein coreference task. Li et al. (2014) later also used a hybrid approach, adopting the rule-based method or the machine learning method for three types of anaphora. As the method of D'Souza and Ng (2012), they also used different rules for different types of anaphora. The system achieved better performance with 68.6% F1 score than previous methods on BioNLP 2011 protein coreference development data.",How have hybrid approaches improved performance in biomedical coreference resolution tasks?,"After the BioNLP 2011 protein coreference dataset was made publicly available, several machine learning-based models were developed for this task. In addition to using machine learning-based methods only, several models adopted a hybrid approach, i.e., combining both machine learning-based and rule-based methods. D'Souza and Ng (2012) proposed a hybrid approach that used a classifier with syntactic path-based features. It investigated five different learning-based methods, and a rule-based approach for anaphora resolution. This model achieved superior performance than previous either rule-based or learning-based models on the BioNLP 2011 protein coreference task. Li et al. (2014) later also used a hybrid approach, adopting the rule-based method or the machine learning method for three types of anaphora. As the method of D'Souza and Ng (2012), they also used different rules for different types of anaphora. The system achieved better performance with a 68.6% F1 score than previous methods on BioNLP 2011 protein coreference development data.",How have hybrid approaches enhanced the performance of biomedical coreference resolution tasks?,"The release of the BioNLP 2011 protein coreference dataset spurred the development of several machine learning-based models for coreference resolution in the biomedical domain. Beyond purely machine learning-based methods, some models adopted a hybrid approach, integrating both machine learning and rule-based techniques. For instance, D'Souza and Ng (2012) introduced a hybrid model that utilized a classifier with syntactic path-based features, exploring five different learning-based methods alongside a rule-based approach for anaphora resolution. This model outperformed previous models, whether rule-based or learning-based, on the BioNLP 2011 protein coreference task. Similarly, Li et al. (2014) employed a hybrid approach, selecting either rule-based or machine learning methods for three types of anaphora, akin to the method of D'Souza and Ng (2012), which also applied different rules for different anaphora types. Their system achieved a superior performance with a 68.6% F1 score on the BioNLP 2011 protein coreference development data, surpassing earlier methods.",7,4,5,4,"Question:

How have hybrid approaches improved performance in biomedical coreference resolution tasks?

Answer:

After the BioNLP 2011 protein coreference dataset was made publicly available, several machine learning-based models were developed for this task. In addition to using machine learning-based methods only, several models adopted a hybrid approach, i.e., combining both machine learning-based and rule-based methods. D'Souza and Ng (2012) proposed a hybrid approach that used a classifier with syntactic path-based features. It investigated five different learning-based methods, and a rule-based approach for anaphora resolution. This model achieved superior performance than previous either rule-based or learning-based models on the BioNLP 2011 protein coreference task. Li et al. (2014) later also used a hybrid approach, adopting the rule-based method or the machine learning method for three types of anaphora. As the method of D'Souza and Ng (2012), they also used different rules for different types of anaphora. The system achieved better performance with a 68.6% F1 score than previous methods on BioNLP 2011 protein coreference development data.","Question:

How have hybrid approaches enhanced the performance of biomedical coreference resolution tasks?

Answer:

The release of the BioNLP 2011 protein coreference dataset spurred the development of several machine learning-based models for coreference resolution in the biomedical domain. Beyond purely machine learning-based methods, some models adopted a hybrid approach, integrating both machine learning and rule-based techniques. For instance, D'Souza and Ng (2012) introduced a hybrid model that utilized a classifier with syntactic path-based features, exploring five different learning-based methods alongside a rule-based approach for anaphora resolution. This model outperformed previous models, whether rule-based or learning-based, on the BioNLP 2011 protein coreference task. Similarly, Li et al. (2014) employed a hybrid approach, selecting either rule-based or machine learning methods for three types of anaphora, akin to the method of D'Souza and Ng (2012), which also applied different rules for different anaphora types. Their system achieved a superior performance with a 68.6% F1 score on the BioNLP 2011 protein coreference development data, surpassing earlier methods.",NO,True,1079,True,True
237941087-s9,Coreference Resolution for the Biomedical Domain: A Survey,2021-09-25,Deep learning-based models,"In recent years, much effort has been made on using deep learning methods for biomedical coreference. Trieu et al. (2018) applied general domain endto-end neural coreference resolution system (Lee et al., 2017) to biomedical text, integrating the domain specific features to enhance the system. The model was evaluated on BioNLP 2011 protein coreference dataset and CRAFT-CR dataset. The results indicated that in-domain embeddings and domain-specific features helped improve the performance. Then, Trieu et al. (2019) proposed a system to address the challenge of coreference resolution in the full-text articles in the CRAFT-CR dataset. The model also applied end-to-end system (Lee et al., 2017), but enhanced the system by utilizing a syntax-based mention filtering method and replacing LSTM with BERT. This model achieved better performance on the CRAFT-CR dataset.

Different from the models above, Li et al. (2021) integrated external knowledge to enhance the neural coreference system for biomedical texts. A knowledge attention module was developed to select the most related and helpful knowledge triplets. This model achieved the state-of-the-art perfor-   53.5 69.8 60.5 50.4 62.7 55.9 (D' Souza and Ng, 2012) 59.9 77.1 67.4 55.6 67.2 60.9 (Li et al., 2014) 69.8 67.5 68.6 ---Simple system (Choi et al., 2016) 64  (Trieu et al., 2018) 56.7 71.7 63.1 47.5 55.6 51.2 KB-attention (Li et al., 2021) 63.4 68.1 65.6 69.4 69.6 69.5 ","How have deep learning models been enhanced for biomedical coreference resolution, and what are their performance outcomes?","In recent years, much effort has been made on using deep learning methods for biomedical coreference. Trieu et al. (2018) applied a general domain end-to-end neural coreference resolution system (Lee et al., 2017) to biomedical text, integrating domain-specific features to enhance the system. The model was evaluated on the BioNLP 2011 protein coreference dataset and CRAFT-CR dataset. The results indicated that in-domain embeddings and domain-specific features helped improve the performance. Then, Trieu et al. (2019) proposed a system to address the challenge of coreference resolution in the full-text articles in the CRAFT-CR dataset. The model also applied an end-to-end system (Lee et al., 2017), but enhanced the system by utilizing a syntax-based mention filtering method and replacing LSTM with BERT. This model achieved better performance on the CRAFT-CR dataset. Different from the models above, Li et al. (2021) integrated external knowledge to enhance the neural coreference system for biomedical texts. A knowledge attention module was developed to select the most related and helpful knowledge triplets. This model achieved the state-of-the-art performance.","How have advancements in deep learning models improved biomedical coreference resolution, and what are the outcomes of these enhancements?","In recent years, significant advancements have been made in applying deep learning methods to biomedical coreference resolution. Trieu et al. (2018) adapted a general domain end-to-end neural coreference resolution system (Lee et al., 2017) for biomedical text by integrating domain-specific features. This model was evaluated on the BioNLP 2011 protein coreference dataset and the CRAFT-CR dataset, demonstrating that in-domain embeddings and domain-specific features significantly improved performance. Building on this, Trieu et al. (2019) addressed coreference resolution challenges in full-text articles from the CRAFT-CR dataset. They enhanced the end-to-end system by incorporating a syntax-based mention filtering method and replacing LSTM with BERT, resulting in improved performance on the CRAFT-CR dataset. In a different approach, Li et al. (2021) integrated external knowledge to enhance the neural coreference system for biomedical texts. They developed a knowledge attention module to select the most relevant and helpful knowledge triplets, achieving state-of-the-art performance.",7,4,7,6,"Question:

How have deep learning models been enhanced for biomedical coreference resolution, and what are their performance outcomes?

Answer:

In recent years, much effort has been made on using deep learning methods for biomedical coreference. Trieu et al. (2018) applied a general domain end-to-end neural coreference resolution system (Lee et al., 2017) to biomedical text, integrating domain-specific features to enhance the system. The model was evaluated on the BioNLP 2011 protein coreference dataset and CRAFT-CR dataset. The results indicated that in-domain embeddings and domain-specific features helped improve the performance. Then, Trieu et al. (2019) proposed a system to address the challenge of coreference resolution in the full-text articles in the CRAFT-CR dataset. The model also applied an end-to-end system (Lee et al., 2017), but enhanced the system by utilizing a syntax-based mention filtering method and replacing LSTM with BERT. This model achieved better performance on the CRAFT-CR dataset. Different from the models above, Li et al. (2021) integrated external knowledge to enhance the neural coreference system for biomedical texts. A knowledge attention module was developed to select the most related and helpful knowledge triplets. This model achieved the state-of-the-art performance.","Question:

How have advancements in deep learning models improved biomedical coreference resolution, and what are the outcomes of these enhancements?

Answer:

In recent years, significant advancements have been made in applying deep learning methods to biomedical coreference resolution. Trieu et al. (2018) adapted a general domain end-to-end neural coreference resolution system (Lee et al., 2017) for biomedical text by integrating domain-specific features. This model was evaluated on the BioNLP 2011 protein coreference dataset and the CRAFT-CR dataset, demonstrating that in-domain embeddings and domain-specific features significantly improved performance. Building on this, Trieu et al. (2019) addressed coreference resolution challenges in full-text articles from the CRAFT-CR dataset. They enhanced the end-to-end system by incorporating a syntax-based mention filtering method and replacing LSTM with BERT, resulting in improved performance on the CRAFT-CR dataset. In a different approach, Li et al. (2021) integrated external knowledge to enhance the neural coreference system for biomedical texts. They developed a knowledge attention module to select the most relevant and helpful knowledge triplets, achieving state-of-the-art performance.",NO,True,1096,True,True
237941087-s16,Coreference Resolution for the Biomedical Domain: A Survey,2021-09-25,Experimental setup,"We conduct experiment using following models:

biomedical PLMs+c2f-coref: we refer to the higher-order coreference model (Lee et al., 2018) as c2f-coref. We build the c2f-coref system on top of different biomedical PLMs respectively, including BioBERT, SciBERT, Bio_ClinicalBERT, PubMedBERT, UMLS-BERT, and Clinical KB-BERT. Among these models, UMLSBERT and Clinical KB-BERT integrate external biomedical knowledge base, i.e., UMLS, while other models are pretrained on large-scale biomedical datasets.

-BERT_base+c2f-coref (Joshi et al., 2019): the c2f-coref system on top of BERT representation.

-SpanBERT_base+c2f-coref (Joshi et al., 2020): the c2f-coref system on top of Span-BERT_base, which pre-trained span representations to better represent and predict spans of text.

We run these models on the CRAFT-CR dataset of latest released version 4.0.1 3 . CRAFT-CR con-  sists of 97 full-text journal articles from PMC. As shown in Table 2, 60 documents are used for finetuning these models. These models are fine-tuned using learning rate of 1×10 −5 for PLMs parameters and 2×10 −4 for task parameters with Adam optimizer, a dropout of 0.3, and max_training_len of 384 for Span-BERT_base and 128 for other PLMs respectively. For SciBERT and PubMedBERT, we use the specific domain vocabulary, while general BERT vocabulary is used for other models.

For evaluation, we calculate F1 scores on six common metrics including B 3 , BLANC, CEAFE, CEAFM, LEA and MUC using the official evaluation script 4 provided by the CRAFT shared task organizers, which is also used by previous models (Trieu et al. 2018, Trieu et al. 2019, Li et al. 2021.",How is the evaluation of coreference resolution models in the biomedical domain conducted?,"The evaluation of coreference resolution models in the biomedical domain is conducted by calculating F1 scores on six common metrics. These metrics include B3, BLANC, CEAFE, CEAFM, LEA, and MUC. The official evaluation script provided by the CRAFT shared task organizers is used for this purpose. This script is also used by previous models (Trieu et al. 2018, Trieu et al. 2019, Li et al. 2021).",How are coreference resolution models evaluated in the biomedical domain?,"In the biomedical domain, the evaluation of coreference resolution models is primarily conducted by calculating F1 scores across six widely recognized metrics: B3, BLANC, CEAFE, CEAFM, LEA, and MUC. To ensure consistency and accuracy, the official evaluation script provided by the CRAFT shared task organizers is utilized. This script has been employed by previous models, as documented in studies by Trieu et al. (2018, 2019) and Li et al. (2021).",7,2,7,2,"Question:

How is the evaluation of coreference resolution models in the biomedical domain conducted?

Answer:

The evaluation of coreference resolution models in the biomedical domain is conducted by calculating F1 scores on six common metrics. These metrics include B3, BLANC, CEAFE, CEAFM, LEA, and MUC. The official evaluation script provided by the CRAFT shared task organizers is used for this purpose. This script is also used by previous models (Trieu et al. 2018, Trieu et al. 2019, Li et al. 2021).","Question:

How are coreference resolution models evaluated in the biomedical domain?

Answer:

In the biomedical domain, the evaluation of coreference resolution models is primarily conducted by calculating F1 scores across six widely recognized metrics: B3, BLANC, CEAFE, CEAFM, LEA, and MUC. To ensure consistency and accuracy, the official evaluation script provided by the CRAFT shared task organizers is utilized. This script has been employed by previous models, as documented in studies by Trieu et al. (2018, 2019) and Li et al. (2021).",NO,True,449,True,True
11591301-s3,A Survey on the Role of Negation in Sentiment Analysis,2010-07-10,Negation and Bag of Words in Supervised Machine Learning,"Several research efforts in polarity classification employ supervised machine-learning algorithms, like Support Vector Machines, Naïve Bayes Classifiers or Maximum Entropy Classifiers. For these algorithms, already a low-level representation using bag of words is fairly effective (Pang et al., 2002). Using a bag-of-words representation, the supervised classifier has to figure out by itself which words in the dataset, or more precisely feature set, are polar and which are not. One either considers all words occurring in a dataset or, as in the case of Pang et al. (2002), one carries out a simple feature selection, such as removing infrequent words. Thus, the standard bag-of-words representation does not contain any explicit knowledge of polar expressions. As a consequence of this simple level of representation, the reversal of the polarity type of polar expressions as it is caused by a negation cannot be explicitly modeled.

The usual way to incorporate negation modeling into this representation is to add artificial words: i.e. if a word x is preceded by a negation word, then rather than considering this as an occurrence of the feature x, a new feature NOT x is created. The scope of negation cannot be properly modeled with this representation either. Pang et al. (2002), for example, consider every word until the next punctuation mark. Sentence 2 would, therefore, result in the following representation:

8. I do not NOT like NOT this NOT new NOT Nokia NOT model.

The advantage of this feature design is that a plain occurrence and a negated occurrence of a word are reflected by two separate features. The disadvantage, however, is that these two contexts treat the same word as two completely different entities.

Since the words to be considered are unrestricted, any word -no matter whether it is an actual polar expression or not -is subjected to this negation modification. This is not only linguistically inaccurate but also increases the feature space with more sparse features (since the majority of words will only be negated once or twice in a corpus). Considering these shortcomings, it comes to no surprise that the impact of negation modeling on this level of representation is limited. Pang et al. (2002) report only a negligible improvement by adding the artificial features compared to plain bag of words in which negation is not considered. Despite the lack of linguistic plausibility, supervised polarity classifiers using bag of words (in particular, if training and testing are done on the same domain) offer fairly good performance. This is, in particular, the case on coarse-grained classification, such as on document level. The success of these methods can be explained by the fact that larger texts contain redundant information, e.g. it does not matter whether a classifier cannot model a negation if the text to be classified contains twenty polar opinions and only one or two contain a negation. Another advantage of these machine learning approaches on coarsegrained classification is their usage of higher order n-grams. Imagine a labeled training set of documents contains frequent bigrams, such as not appealing or less entertaining. Then a feature set using higher order n-grams implicitly contains negation modeling. This also partially explains the effectiveness of bigrams and trigrams for this task as stated in (Ng et al., 2006). The dataset used for the experiments in (Pang et al., 2002;Ng et al., 2006) has been established as a popular benchmark dataset for sentiment analysis and is publicly available 1 .",How is negation typically incorporated into a bag-of-words representation for sentiment analysis?,"The usual way to incorporate negation modeling into a bag-of-words representation is to add artificial words. If a word x is preceded by a negation word, then rather than considering this as an occurrence of the feature x, a new feature NOT x is created. The advantage of this feature design is that a plain occurrence and a negated occurrence of a word are reflected by two separate features. The disadvantage, however, is that these two contexts treat the same word as two completely different entities. Since the words to be considered are unrestricted, any word—no matter whether it is an actual polar expression or not—is subjected to this negation modification. This is not only linguistically inaccurate but also increases the feature space with more sparse features, since the majority of words will only be negated once or twice in a corpus. Considering these shortcomings, it comes as no surprise that the impact of negation modeling on this level of representation is limited. Pang et al. (2002) report only a negligible improvement by adding the artificial features compared to plain bag of words in which negation is not considered.",How is negation typically handled in a bag-of-words model for sentiment analysis?,"Incorporating negation into a bag-of-words representation for sentiment analysis is typically done by introducing artificial words. When a word x is preceded by a negation word, it is not simply counted as an occurrence of x. Instead, a new feature, NOT x, is created. This approach allows for distinguishing between plain occurrences and negated occurrences of a word by treating them as two separate features. However, this method has its drawbacks. It treats the same word in different contexts as entirely different entities, which can be linguistically inaccurate. Additionally, it increases the feature space with more sparse features, as most words are only negated once or twice in a corpus. Due to these limitations, the impact of negation modeling at this level is often minimal. For instance, Pang et al. (2002) reported only a negligible improvement when adding these artificial features compared to a plain bag-of-words model that does not account for negation.",9,2,9,2,"Question:

How is negation typically incorporated into a bag-of-words representation for sentiment analysis?

Answer:

The usual way to incorporate negation modeling into a bag-of-words representation is to add artificial words. If a word x is preceded by a negation word, then rather than considering this as an occurrence of the feature x, a new feature NOT x is created. The advantage of this feature design is that a plain occurrence and a negated occurrence of a word are reflected by two separate features. The disadvantage, however, is that these two contexts treat the same word as two completely different entities. Since the words to be considered are unrestricted, any word—no matter whether it is an actual polar expression or not—is subjected to this negation modification. This is not only linguistically inaccurate but also increases the feature space with more sparse features, since the majority of words will only be negated once or twice in a corpus. Considering these shortcomings, it comes as no surprise that the impact of negation modeling on this level of representation is limited. Pang et al. (2002) report only a negligible improvement by adding the artificial features compared to plain bag of words in which negation is not considered.","Question:

How is negation typically handled in a bag-of-words model for sentiment analysis?

Answer:

Incorporating negation into a bag-of-words representation for sentiment analysis is typically done by introducing artificial words. When a word x is preceded by a negation word, it is not simply counted as an occurrence of x. Instead, a new feature, NOT x, is created. This approach allows for distinguishing between plain occurrences and negated occurrences of a word by treating them as two separate features. However, this method has its drawbacks. It treats the same word in different contexts as entirely different entities, which can be linguistically inaccurate. Additionally, it increases the feature space with more sparse features, as most words are only negated once or twice in a corpus. Due to these limitations, the impact of negation modeling at this level is often minimal. For instance, Pang et al. (2002) reported only a negligible improvement when adding these artificial features compared to a plain bag-of-words model that does not account for negation.",NO,True,974,True,True
11591301-s5,A Survey on the Role of Negation in Sentiment Analysis,2010-07-10,Contextual Valence Shifters,"The first computational model that accounts for negation in a model that includes knowledge of polar expressions is (Polanyi and Zaenen, 2004). The different types of negations are modeled via contextual valence shifting. The model assigns scores to polar expressions, i.e. positive scores to positive polar expressions and negative scores to negative polar expressions, respectively. If a polar expression is negated, its polarity score is simply inverted (see Example 1).

In a similar fashion, diminishers are taken into consideration. The difference is, however, that the score is only reduced rather than shifted to the other polarity type (see Example 2).

Beyond that the model also accounts for modals, presuppositional items and even discourse-based valence shifting. Unfortunately, this model is not implemented and, therefore, one can only speculate about its real effectiveness.

Kennedy and Inkpen (2005) evaluate a negation model which is fairly identical to the one proposed by Polanyi and Zaenen (2004) (as far as simple negation words and diminishers are concerned) in document-level polarity classification. A simple scope for negation is chosen. A polar expression is thought to be negated if the negation word immediately precedes it. In an extension of this work (Kennedy and Inkpen, 2006) a parser is considered for scope computation. Unfortunately, no precise description of how the parse is used for scope modeling is given in that work. Neither is there a comparison of these two scope models measuring their respective impacts.

Final results show that modeling negation is important and relevant, even in the case of such simple methods. The consideration of negation words is more important than that of diminishers. Wilson et al. (2005) carry out more advanced negation modeling on expression-level polarity classification. The work uses supervised machine learning where negation modeling is mostly encoded as features using polar expressions. The features for negation modeling are organized in three groups:",How do Kennedy and Inkpen's models address the scope and impact of negation in sentiment analysis?,"Kennedy and Inkpen (2005) evaluate a negation model which is fairly identical to the one proposed by Polanyi and Zaenen (2004) (as far as simple negation words and diminishers are concerned) in document-level polarity classification. A simple scope for negation is chosen. A polar expression is thought to be negated if the negation word immediately precedes it. In an extension of this work (Kennedy and Inkpen, 2006) a parser is considered for scope computation. Unfortunately, no precise description of how the parse is used for scope modeling is given in that work. Neither is there a comparison of these two scope models measuring their respective impacts. Final results show that modeling negation is important and relevant, even in the case of such simple methods. The consideration of negation words is more important than that of diminishers.",How do Kennedy and Inkpen's models handle the scope and impact of negation in sentiment analysis?,"Kennedy and Inkpen (2005) explore a negation model in the context of document-level polarity classification, which closely resembles the approach proposed by Polanyi and Zaenen (2004) regarding simple negation words and diminishers. They adopt a straightforward method for determining the scope of negation, where a polar expression is considered negated if a negation word directly precedes it. In a subsequent study (Kennedy and Inkpen, 2006), they introduce a parser to compute the scope of negation, although they do not provide a detailed explanation of how the parser is utilized for scope modeling. Additionally, there is no comparison between the two scope models to assess their respective impacts. Nonetheless, their findings indicate that incorporating negation is crucial and beneficial, even with such basic methods. They also conclude that accounting for negation words is more significant than considering diminishers.",7,2,7,2,"Question:

How do Kennedy and Inkpen's models address the scope and impact of negation in sentiment analysis?

Answer:

Kennedy and Inkpen (2005) evaluate a negation model which is fairly identical to the one proposed by Polanyi and Zaenen (2004) (as far as simple negation words and diminishers are concerned) in document-level polarity classification. A simple scope for negation is chosen. A polar expression is thought to be negated if the negation word immediately precedes it. In an extension of this work (Kennedy and Inkpen, 2006) a parser is considered for scope computation. Unfortunately, no precise description of how the parse is used for scope modeling is given in that work. Neither is there a comparison of these two scope models measuring their respective impacts. Final results show that modeling negation is important and relevant, even in the case of such simple methods. The consideration of negation words is more important than that of diminishers.","Question:

How do Kennedy and Inkpen's models handle the scope and impact of negation in sentiment analysis?

Answer:

Kennedy and Inkpen (2005) explore a negation model in the context of document-level polarity classification, which closely resembles the approach proposed by Polanyi and Zaenen (2004) regarding simple negation words and diminishers. They adopt a straightforward method for determining the scope of negation, where a polar expression is considered negated if a negation word directly precedes it. In a subsequent study (Kennedy and Inkpen, 2006), they introduce a parser to compute the scope of negation, although they do not provide a detailed explanation of how the parser is utilized for scope modeling. Additionally, there is no comparison between the two scope models to assess their respective impacts. Nonetheless, their findings indicate that incorporating negation is crucial and beneficial, even with such basic methods. They also conclude that accounting for negation words is more significant than considering diminishers.",YES,False,933,True,False
11591301-s6,A Survey on the Role of Negation in Sentiment Analysis,2010-07-10,Features for Negation Modeling,"• negation features • shifter features • polarity modification features Negation features directly relate to negation expressions negating a polar expression. One feature checks whether a negation expression occurs in a fixed window of four words preceding the polar expression. The other feature accounts for a polar predicate having a negated subject. This frequent long-range relationship is illustrated in Sentence 9. All negation expressions are additionally disambiguated as some negation words do not function as a negation word in certain contexts, e.g. not to mention or not just. Shifter features are binary features checking the presence of different types of polarity shifters. Polarity shifters, such as little, are weaker than ordinary negation expressions. They can be grouped into three categories, general polarity shifters, positive polarity shifters, and negative polarity shifters. General polarity shifters reverse polarity like negations. The latter two types only reverse a particular polarity type, e.g. the positive shifter abate only modifies negative polar expressions as in abate the damage. Thus, the presence of a positive shifter may indicate positive polarity. The set of words that are denoted by these three features can be approximately equated with diminishers. Finally, polarity modification features describe polar expressions of a particular type modifying or being modified by other polar expressions. Though these features do not explicitly contain negations, language constructions which are similar to negation may be captured. In the phrase [disappointed − hope + ] − , for instance, a negative polar expression modifies a positive polar expression which results in an overall negative phrase. Adding these three feature groups to a feature set comprising bag of words and features counting polar expressions results in a significant improvement. In (Wilson et al., 2009), the experiments of Wilson et al. (2005) are extended by a detailed analysis on the individual effectiveness of the three feature groups mentioned above. The results averaged over four different supervised learning algorithms suggest that the actual negation features are most effective whereas the binary polarity shifters have the smallest impact. This is consistent with Kennedy and Inkpen (2005) given the similarity of polarity shifters and diminishers.

Considering the amount of improvement that is achieved by negation modeling, the improvement seems to be larger in (Wilson et al., 2005). There might be two explanations for this. Firstly, the negation modeling in (Wilson et al., 2005) is considerably more complex and, secondly, Wilson et al. (2005) evaluate on a more fine-grained level (i.e. expression level) than Kennedy and Inkpen (2005) (they evaluate on document level). As already pointed out in §3.1, document-level polarity classification contains more redundant information than sentence-level or expression-level polarity classification, therefore complex negation modeling on these levels might be more effective since the correct contextual interpretation of an individual polar expression is far more important 2 . The fine-grained opinion corpus used in (Wilson et al., 2005;Wilson et al., 2009) and all the resources necessary to replicate the features used in these experiments are also publicly available 3 .",How do polarity modification features capture language constructions similar to negation in sentiment analysis?,"Polarity modification features describe polar expressions of a particular type modifying or being modified by other polar expressions. Though these features do not explicitly contain negations, language constructions which are similar to negation may be captured. In the phrase [disappointed − hope + ] − , for instance, a negative polar expression modifies a positive polar expression which results in an overall negative phrase.",How do polarity modification features account for language constructions that resemble negation in sentiment analysis?,"Polarity modification features involve the interaction between polar expressions, where one type of polar expression modifies or is modified by another. While these features do not explicitly include negations, they can capture language constructions that function similarly to negation. For example, in the phrase ""[disappointed − hope + ] −,"" a negative polar expression (""disappointed"") modifies a positive polar expression (""hope""), resulting in an overall negative sentiment for the phrase.",7,2,5,4,"Question:

How do polarity modification features capture language constructions similar to negation in sentiment analysis?

Answer:

Polarity modification features describe polar expressions of a particular type modifying or being modified by other polar expressions. Though these features do not explicitly contain negations, language constructions which are similar to negation may be captured. In the phrase [disappointed − hope + ] − , for instance, a negative polar expression modifies a positive polar expression which results in an overall negative phrase.","Question:

How do polarity modification features account for language constructions that resemble negation in sentiment analysis?

Answer:

Polarity modification features involve the interaction between polar expressions, where one type of polar expression modifies or is modified by another. While these features do not explicitly include negations, they can capture language constructions that function similarly to negation. For example, in the phrase ""[disappointed − hope + ] −,"" a negative polar expression (""disappointed"") modifies a positive polar expression (""hope""), resulting in an overall negative sentiment for the phrase.",NO,True,495,True,True
265158335-s2,Exploring undergraduate translation students' perceptions towards machine translation: A qualitative questionnaire survey,2023,Methods and data,"After ethical clearance was obtained from the university, 20 students from an Applied Translation Studies programme at a university based in China were recruited.

The participants were all in the second year of an undergraduate translation programme; they had similar educational backgrounds and hence comparable language proficiency and translation competencies.They had taken three translation courses covering fundamental translation theories and practices.No specialised training in MT, post-editing (PE) or translation technology was provided in the classroom.

The survey included ten open-ended questions to solicit the participants' knowledge, experience, perceptions of and attitudes to MT.As few previous studies have used open-ended questions, the design of this questionnaire mainly drew on González Pastor (2021)'s paper, which had a similar design and goal as the current project, and referenced three other relevant papers (Çetiner & İşisağ, 2019;de Faria Pires, 2020;Schmidhofer & Mair, 2018).González Pastor (2021) investigated students' attitudes to and perceptions of translation technology before and after being taught about translation technology.The questions were adapted and narrowed down to MT-and PE-specific questions.Questions regarding the students' understanding of translation concepts, processes and products were added as answers to these questions provide an alternative perspective to understand the impact of MT and PE on students' translation processes and products.

This paper mainly analysed answers to questions regarding the students' knowledge, attitudes to and perceptions of MT.Therefore, only the answers to the following six (out of 10) questions were analysed and presented.6. Do you think there are ethical concerns related to MT, such as legal or moral issues, biases, justice or privacy?Can you explain your thoughts in detail?

The students were told that they could answer the survey in Chinese or English, whichever they felt most comfortable using.The answers in Chinese were translated by the author, a certified translator in China and Australia with over 15 years of professional experience.

All the data were de-identified with students' names being indicated as ""s + participant number"" and were imported into NVivo 14 for thematic analysis.The author conducted the analysis twice at two different times to ensure the reliability of the results.",How was the survey data processed and analyzed to ensure reliability in the study?,"The survey data were de-identified with students' names being indicated as ""s + participant number"" and were imported into NVivo 14 for thematic analysis. The author conducted the analysis twice at two different times to ensure the reliability of the results.",What steps were taken to process and analyze the survey data to ensure the study's reliability?,"To ensure the reliability of the study, the survey data were first de-identified by replacing students' names with ""s + participant number."" This anonymized data was then imported into NVivo 14 for thematic analysis. To further enhance reliability, the author conducted the analysis twice at two different times.",9,2,9,4,"Question:

How was the survey data processed and analyzed to ensure reliability in the study?

Answer:

The survey data were de-identified with students' names being indicated as ""s + participant number"" and were imported into NVivo 14 for thematic analysis. The author conducted the analysis twice at two different times to ensure the reliability of the results.","Question:

What steps were taken to process and analyze the survey data to ensure the study's reliability?

Answer:

To ensure the reliability of the study, the survey data were first de-identified by replacing students' names with ""s + participant number."" This anonymized data was then imported into NVivo 14 for thematic analysis. To further enhance reliability, the author conducted the analysis twice at two different times.",NO,True,312,False,False
10690796-s5,A Survey of Ellipsis in Chinese,2015-08-01,Null complement anaphora,"Null complement anaphora (Hankamer andSag 1976, Depiante 2000) is a mechanism that elides a complement clause, to-phrase, or prepositional phrase, e.g.

(36) Jim promised he would help, and promised Bill also would he help

Bill also promised he would help.

(37) Sam refuses to help, and refuses Sue also to help

Sue also refuses to help.

The predicates that license null complement anaphora in English (e.g. ask, know, promise, refuse, try) are limited. Similar predicates that one might expect to also license null complement anaphora fail to do so (e.g. imagine, intend, pretend, say, think, etc. These two examples suggest that the similar predicates across the languages allow for the ellipsis of a complement clause or phrase. However, concluding that Mandarin has null complement anaphora in the same way that English does is difficult. The difficulty is due to the fact that Mandarin seems to freely allow the ellipsis of most all complements that can be easily recovered from context. When the elided complement is a verb phrase, one can acknowl-edge VP-ellipsis as discussed above, and when the elided complement can be interpreted as a definite or indefinite noun phrase, an analysis in terms of zero anaphora is available (see the next section). Thus the extent to which null complement anaphora is present in Mandarin is unclear.",Why is it challenging to determine the presence of null complement anaphora in Mandarin compared to English?,"Concluding that Mandarin has null complement anaphora in the same way that English does is difficult. The difficulty is due to the fact that Mandarin seems to freely allow the ellipsis of most all complements that can be easily recovered from context. When the elided complement is a verb phrase, one can acknowledge VP-ellipsis, and when the elided complement can be interpreted as a definite or indefinite noun phrase, an analysis in terms of zero anaphora is available. Thus, the extent to which null complement anaphora is present in Mandarin is unclear.",What makes it difficult to identify null complement anaphora in Mandarin compared to English?,"Determining the presence of null complement anaphora in Mandarin, as it exists in English, is challenging. This difficulty arises because Mandarin allows the ellipsis of most complements that can be easily inferred from context. When the omitted complement is a verb phrase, it can be recognized as VP-ellipsis. Similarly, when the omitted complement can be interpreted as a definite or indefinite noun phrase, it can be analyzed in terms of zero anaphora. Consequently, the extent to which null complement anaphora is present in Mandarin remains unclear.",7,2,7,2,"Question:

Why is it challenging to determine the presence of null complement anaphora in Mandarin compared to English?

Answer:

Concluding that Mandarin has null complement anaphora in the same way that English does is difficult. The difficulty is due to the fact that Mandarin seems to freely allow the ellipsis of most all complements that can be easily recovered from context. When the elided complement is a verb phrase, one can acknowledge VP-ellipsis, and when the elided complement can be interpreted as a definite or indefinite noun phrase, an analysis in terms of zero anaphora is available. Thus, the extent to which null complement anaphora is present in Mandarin is unclear.","Question:

What makes it difficult to identify null complement anaphora in Mandarin compared to English?

Answer:

Determining the presence of null complement anaphora in Mandarin, as it exists in English, is challenging. This difficulty arises because Mandarin allows the ellipsis of most complements that can be easily inferred from context. When the omitted complement is a verb phrase, it can be recognized as VP-ellipsis. Similarly, when the omitted complement can be interpreted as a definite or indefinite noun phrase, it can be analyzed in terms of zero anaphora. Consequently, the extent to which null complement anaphora is present in Mandarin remains unclear.",NO,True,555,True,True
260899983-s17,Through the Lens of Core Competency: Survey on Evaluation of Large Language Models,2023-08-15,Harmfulness,"The harmful contents include the offensive language or others that have the explicit harm towards the specific object, such content that has been widely discussed. However, there is not a unified definition of the constitution of harmful contents, based on our surveys, we conclude the relevant themes into five aspects, including offensiveness, violence, crime, sexual-explicit, and unauthorized expertise. Many researches focus on the language detection for the outputs of LLMs to ensure the harmlessness (Wulczyn et al., 2017;Zampieri et al., 2019;Dinan et al., 2019a), while other techniques are proposed to stimulate LLMs to generate safe outputs directly (Krause et al., 2021;Atwell et al., 2022). For the unauthorized expertise, a general LLM should avoid any unauthorized expertise before the establishment of accountability system , which involves the psychological orientation and any medical advice. Besides, the impact of conversation context on safety gains more attention recently, as a results, detective and generative algorithms base on the context are proposed successively (Dinan et al., 2019a;Baheti et al., 2021;. RealToxicityPrompts (Gehman et al., 2020) is a dataset derived from English web texts, where prompts are automatically truncated from sentences classified as toxicity from a widely-used toxicity classifier. RealToxicityPrompts consists of 100K natural prompts, with average 11.7 tokens in length. BAD (Xu et al., 2021) is a dataset collected by the human-in-the-loop strategy, where crowdworkers are ask to prob harmful model outputs. BAD consist of 5k conversations with around 70k utterances in total, which could be used in both non-adversarially and adversarially testing the model weakness.",How do datasets like RealToxicityPrompts and BAD contribute to understanding and testing harmful content in LLMs?,"RealToxicityPrompts (Gehman et al., 2020) is a dataset derived from English web texts, where prompts are automatically truncated from sentences classified as toxicity from a widely-used toxicity classifier. RealToxicityPrompts consists of 100K natural prompts, with an average of 11.7 tokens in length. BAD (Xu et al., 2021) is a dataset collected by the human-in-the-loop strategy, where crowdworkers are asked to probe harmful model outputs. BAD consists of 5k conversations with around 70k utterances in total, which could be used in both non-adversarially and adversarially testing the model weakness.",How do datasets like RealToxicityPrompts and BAD help in understanding and evaluating harmful content in large language models (LLMs)?,"RealToxicityPrompts, introduced by Gehman et al. in 2020, is a dataset derived from English web texts. It consists of 100,000 natural prompts, each averaging 11.7 tokens in length. These prompts are automatically truncated from sentences identified as toxic by a widely-used toxicity classifier. This dataset is instrumental in assessing how LLMs handle potentially harmful content by providing a diverse range of prompts that can trigger toxic responses.

On the other hand, the BAD dataset, developed by Xu et al. in 2021, employs a human-in-the-loop strategy. In this approach, crowdworkers are tasked with probing harmful model outputs. BAD comprises 5,000 conversations with approximately 70,000 utterances in total. This dataset is valuable for both non-adversarial and adversarial testing of model weaknesses, offering insights into how LLMs can be improved to mitigate harmful content generation. Together, these datasets play a crucial role in understanding and evaluating the handling of harmful content in LLMs.",7,4,8,7,"Question:

How do datasets like RealToxicityPrompts and BAD contribute to understanding and testing harmful content in LLMs?

Answer:

RealToxicityPrompts (Gehman et al., 2020) is a dataset derived from English web texts, where prompts are automatically truncated from sentences classified as toxicity from a widely-used toxicity classifier. RealToxicityPrompts consists of 100K natural prompts, with an average of 11.7 tokens in length. BAD (Xu et al., 2021) is a dataset collected by the human-in-the-loop strategy, where crowdworkers are asked to probe harmful model outputs. BAD consists of 5k conversations with around 70k utterances in total, which could be used in both non-adversarially and adversarially testing the model weakness.","Question:

How do datasets like RealToxicityPrompts and BAD help in understanding and evaluating harmful content in large language models (LLMs)?

Answer:

RealToxicityPrompts, introduced by Gehman et al. in 2020, is a dataset derived from English web texts. It consists of 100,000 natural prompts, each averaging 11.7 tokens in length. These prompts are automatically truncated from sentences identified as toxic by a widely-used toxicity classifier. This dataset is instrumental in assessing how LLMs handle potentially harmful content by providing a diverse range of prompts that can trigger toxic responses.

On the other hand, the BAD dataset, developed by Xu et al. in 2021, employs a human-in-the-loop strategy. In this approach, crowdworkers are tasked with probing harmful model outputs. BAD comprises 5,000 conversations with approximately 70,000 utterances in total. This dataset is valuable for both non-adversarial and adversarial testing of model weaknesses, offering insights into how LLMs can be improved to mitigate harmful content generation. Together, these datasets play a crucial role in understanding and evaluating the handling of harmful content in LLMs.",NO,True,1022,True,True
260899983-s18,Through the Lens of Core Competency: Survey on Evaluation of Large Language Models,2023-08-15,Unfairness and Social Bias,"Unfairness and social bias present more covertly and widely for LLMs. Following the previous studies, we conclude that social bias is an inherent characteristic of a LLM, which mainly embody in the dis-tribution difference of a LLM in language selection based on different demographic groups. Compared to the social bias, unfairness is the external form, which reflected in the output performance of specific tasks, for example, the African American English (AAE) is frequently mis-classified as the offensive language by some language detector (Lwowski et al., 2022). However, issues of unfairness and social bias are inevitable as they are widely distributed in human languages, and LLMs are required to memorize language as accurately as possible in the training stage (Weidinger et al., 2021). With respect to evaluate this important aspect, CrowS-Pairs (Nangia et al., 2020) is benchmark proposed to evaluating social bias. There are 1508 examples in CrowS-Pairs that involves nine types of social bias, like gender, race, and Nationality. StereoSet (Nadeem et al., 2021) is a dataset that could be used to evaluate social bias level in both word-level and sentence level, which examples are in four domains: race, gender,religion, and profession. For the StereoSet, the bias level is computed by the difference between model generation probabilities of biased and anti-biased sentence.",How is social bias evaluated in large language models using datasets like CrowS-Pairs and StereoSet?,"CrowS-Pairs (Nangia et al., 2020) is a benchmark proposed for evaluating social bias. There are 1508 examples in CrowS-Pairs that involve nine types of social bias, such as gender, race, and nationality. StereoSet (Nadeem et al., 2021) is a dataset that can be used to evaluate social bias levels at both the word-level and sentence level, with examples in four domains: race, gender, religion, and profession. For StereoSet, the bias level is computed by the difference between model generation probabilities of biased and anti-biased sentences.",How is social bias assessed in large language models using datasets like CrowS-Pairs and StereoSet?,"Social bias in large language models is assessed using datasets such as CrowS-Pairs and StereoSet. CrowS-Pairs, introduced by Nangia et al. in 2020, serves as a benchmark for evaluating social bias. It comprises 1508 examples that cover nine types of social bias, including gender, race, and nationality. On the other hand, StereoSet, developed by Nadeem et al. in 2021, is designed to evaluate social bias at both the word-level and sentence-level. It includes examples across four domains: race, gender, religion, and profession. In StereoSet, the bias level is determined by calculating the difference between the model's generation probabilities for biased and anti-biased sentences.",7,2,7,2,"Question:

How is social bias evaluated in large language models using datasets like CrowS-Pairs and StereoSet?

Answer:

CrowS-Pairs (Nangia et al., 2020) is a benchmark proposed for evaluating social bias. There are 1508 examples in CrowS-Pairs that involve nine types of social bias, such as gender, race, and nationality. StereoSet (Nadeem et al., 2021) is a dataset that can be used to evaluate social bias levels at both the word-level and sentence level, with examples in four domains: race, gender, religion, and profession. For StereoSet, the bias level is computed by the difference between model generation probabilities of biased and anti-biased sentences.","Question:

How is social bias assessed in large language models using datasets like CrowS-Pairs and StereoSet?

Answer:

Social bias in large language models is assessed using datasets such as CrowS-Pairs and StereoSet. CrowS-Pairs, introduced by Nangia et al. in 2020, serves as a benchmark for evaluating social bias. It comprises 1508 examples that cover nine types of social bias, including gender, race, and nationality. On the other hand, StereoSet, developed by Nadeem et al. in 2021, is designed to evaluate social bias at both the word-level and sentence-level. It includes examples across four domains: race, gender, religion, and profession. In StereoSet, the bias level is determined by calculating the difference between the model's generation probabilities for biased and anti-biased sentences.",NO,True,687,True,True
260899983-s23,Through the Lens of Core Competency: Survey on Evaluation of Large Language Models,2023-08-15,Sentiment Generation,"We categorize sentiment generation into two manifestations. One is to generate text that contains sentiments, and the other is to generate text that elicits sentiments. The former requires specifying the desired sentiment, and the latter requires a combination of commonsense knowledge (Speer et al., 2017;Hwang et al., 2021) or theory of mind (Sodian and Kristen, 2010). A classic application scenario is in open-domain dialogue, specifically, emotional dialogue (Zhou et al., 2018), empathetic dialogue (Rashkin et al., 2019), and emotional support conversation (Liu et al., 2021). To measure the quality of the generated text, it is necessary to employ both automatic metrics (such as sentiment accuracy, BLEU (Papineni et al., 2002), perplexity) and human evaluations (human ratings or preference tests). Currently, no work has comprehensively explored this aspect, but it is an essential path towards artificial general intelligence (AGI) (Bubeck et al., 2023).",What methods are used to measure the quality of text generated with sentiment in NLP tasks?,"To measure the quality of the generated text, it is necessary to employ both automatic metrics and human evaluations. Automatic metrics include sentiment accuracy, BLEU (Papineni et al., 2002), and perplexity. Human evaluations involve human ratings or preference tests.",What are the methods used to assess the quality of text generated with sentiment in NLP tasks?,"To assess the quality of sentiment-driven text generated in NLP tasks, a combination of automatic metrics and human evaluations is typically employed. Automatic metrics include sentiment accuracy, BLEU (Papineni et al., 2002), and perplexity. In addition to these, human evaluations are crucial, often involving human ratings or preference tests to provide a more nuanced understanding of the text's quality.",7,7,7,4,"Question:

What methods are used to measure the quality of text generated with sentiment in NLP tasks?

Answer:

To measure the quality of the generated text, it is necessary to employ both automatic metrics and human evaluations. Automatic metrics include sentiment accuracy, BLEU (Papineni et al., 2002), and perplexity. Human evaluations involve human ratings or preference tests.","Question:

What are the methods used to assess the quality of text generated with sentiment in NLP tasks?

Answer:

To assess the quality of sentiment-driven text generated in NLP tasks, a combination of automatic metrics and human evaluations is typically employed. Automatic metrics include sentiment accuracy, BLEU (Papineni et al., 2002), and perplexity. In addition to these, human evaluations are crucial, often involving human ratings or preference tests to provide a more nuanced understanding of the text's quality.",NO,True,408,True,True
260899983-s24,Through the Lens of Core Competency: Survey on Evaluation of Large Language Models,2023-08-15,Planning,"Planning is the thinking before the actions take place. Given a specific goal, planning is the process to decide the means to achieve the goal. There're few works (Valmeekam et al., 2023;Valmeekam et al., 2022;Pallagani et al., 2023;) that look at the planning ability of LLMs. Some of them focus on commonsense areas  like wedding or menu planning. Others adopted automated planning problems, formal language translators, and verifiers to automatically evaluate LLMs' competency (Valmeekam et al., 2023). With PDDL 6 represented problem descriptions and the translation of such problems into text and back, LLMs can thus sequence a series of actions to reach the planning goal. Whether the planning purpose is achieved can be easily verified via automatic verifiers. Possessing web-scale knowledge, LLMs have great potential for executing planning tasks or assisting planners.",How is the planning competency of large language models evaluated using automated methods?,"There are few works that look at the planning ability of large language models (LLMs). Some of them focus on commonsense areas like wedding or menu planning. Others adopted automated planning problems, formal language translators, and verifiers to automatically evaluate LLMs' competency (Valmeekam et al., 2023). With PDDL 6 represented problem descriptions and the translation of such problems into text and back, LLMs can sequence a series of actions to reach the planning goal. Whether the planning purpose is achieved can be easily verified via automatic verifiers.",How is the planning ability of large language models evaluated using automated methods?,"The evaluation of the planning ability of large language models (LLMs) has been explored in several studies, with some focusing on commonsense scenarios such as wedding or menu planning. Other approaches involve using automated planning problems, formal language translators, and verifiers to assess the competency of LLMs (Valmeekam et al., 2023). By representing problem descriptions in PDDL 6 and translating these problems into text and back, LLMs can sequence a series of actions to achieve a planning goal. The success of this planning process can be easily verified using automatic verifiers.",7,4,7,4,"Question:

How is the planning competency of large language models evaluated using automated methods?

Answer:

There are few works that look at the planning ability of large language models (LLMs). Some of them focus on commonsense areas like wedding or menu planning. Others adopted automated planning problems, formal language translators, and verifiers to automatically evaluate LLMs' competency (Valmeekam et al., 2023). With PDDL 6 represented problem descriptions and the translation of such problems into text and back, LLMs can sequence a series of actions to reach the planning goal. Whether the planning purpose is achieved can be easily verified via automatic verifiers.","Question:

How is the planning ability of large language models evaluated using automated methods?

Answer:

The evaluation of the planning ability of large language models (LLMs) has been explored in several studies, with some focusing on commonsense scenarios such as wedding or menu planning. Other approaches involve using automated planning problems, formal language translators, and verifiers to assess the competency of LLMs (Valmeekam et al., 2023). By representing problem descriptions in PDDL 6 and translating these problems into text and back, LLMs can sequence a series of actions to achieve a planning goal. The success of this planning process can be easily verified using automatic verifiers.",NO,True,599,True,True
2044324-s2,A Survey of Current Datasets for Vision and Language Research,2015-06-23,Language Quality,"We define the following criteria for evaluating the captions or instructions of the datasets:

• Vocabulary Size (#vocab), the number of unique vocabulary words.

2 http://visionandlanguage.net • Syntactic Complexity (Frazier, Yngve) measures the amount of embedding/branching in a sentence's syntax. We report mean Yngve (Yngve, 1960) and Frazier measurements (Frazier, 1985); each provides a different counting on the number of nodes in the phrase markers of syntactic trees.

• Part of Speech Distribution measures the distribution of nouns, verbs, adjectives, and other parts of speech.

• Abstract:Concrete Ratio (#Conc, #Abs, %Abs) indicates the range of visual and non-visual concepts the dataset covers. Abstract terms are ideas or concepts, such as 'love' or 'think' and concrete terms are all the objects or events that are mainly available to the senses. For this purpose, we use a list of most common abstract terms in English (Vanderwende et al., 2015), and define concrete terms as all other words except for a small set of function words.

• Average Sentence Length (Sent Len.) shows how rich and descriptive the sentences are.

• Perplexity provides a measure of data skew by measuring how expected sentences are from one corpus according to a model trained on another corpus. We analyze perplexity (Ppl) for each dataset against a 5-gram language model learned on a generic 30B words English dataset. We further analyze pair-wise perplexity of datasets against each other in Section 4.",What criteria are used to evaluate the language quality of vision and language datasets?,"The criteria for evaluating the language quality of vision and language datasets include:Vocabulary Size (#vocab), which is the number of unique vocabulary words. Syntactic Complexity, measured by mean Yngve and Frazier measurements, which assess the amount of embedding/branching in a sentence's syntax. Part of Speech Distribution, which measures the distribution of nouns, verbs, adjectives, and other parts of speech. Abstract:Concrete Ratio (#Conc, #Abs, %Abs), which indicates the range of visual and non-visual concepts the dataset covers, distinguishing between abstract terms (ideas or concepts) and concrete terms (objects or events available to the senses).Average Sentence Length (Sent Len.), which shows how rich and descriptive the sentences are. Perplexity, which provides a measure of data skew by measuring how expected sentences are from one corpus according to a model trained on another corpus.",What are the key criteria for assessing the language quality in vision and language datasets?,"The language quality of vision and language datasets is evaluated using several key criteria. First, Vocabulary Size (#vocab) is considered, which refers to the number of unique vocabulary words present in the dataset. Next, Syntactic Complexity is assessed through mean Yngve and Frazier measurements, which evaluate the amount of embedding or branching in a sentence's syntax. Additionally, Part of Speech Distribution is analyzed to understand the distribution of nouns, verbs, adjectives, and other parts of speech within the dataset. The Abstract:Concrete Ratio (#Conc, #Abs, %Abs) is another important criterion, indicating the range of visual and non-visual concepts covered by the dataset, and distinguishing between abstract terms (ideas or concepts) and concrete terms (objects or events available to the senses). Average Sentence Length (Sent Len.) is also measured to determine how rich and descriptive the sentences are. Finally, Perplexity is used to provide a measure of data skew by evaluating how expected sentences are from one corpus according to a model trained on another corpus.",8,4,8,4,"Question:

What criteria are used to evaluate the language quality of vision and language datasets?

Answer:

The criteria for evaluating the language quality of vision and language datasets include:Vocabulary Size (#vocab), which is the number of unique vocabulary words. Syntactic Complexity, measured by mean Yngve and Frazier measurements, which assess the amount of embedding/branching in a sentence's syntax. Part of Speech Distribution, which measures the distribution of nouns, verbs, adjectives, and other parts of speech. Abstract:Concrete Ratio (#Conc, #Abs, %Abs), which indicates the range of visual and non-visual concepts the dataset covers, distinguishing between abstract terms (ideas or concepts) and concrete terms (objects or events available to the senses).Average Sentence Length (Sent Len.), which shows how rich and descriptive the sentences are. Perplexity, which provides a measure of data skew by measuring how expected sentences are from one corpus according to a model trained on another corpus.","Question:

What are the key criteria for assessing the language quality in vision and language datasets?

Answer:

The language quality of vision and language datasets is evaluated using several key criteria. First, Vocabulary Size (#vocab) is considered, which refers to the number of unique vocabulary words present in the dataset. Next, Syntactic Complexity is assessed through mean Yngve and Frazier measurements, which evaluate the amount of embedding or branching in a sentence's syntax. Additionally, Part of Speech Distribution is analyzed to understand the distribution of nouns, verbs, adjectives, and other parts of speech within the dataset. The Abstract:Concrete Ratio (#Conc, #Abs, %Abs) is another important criterion, indicating the range of visual and non-visual concepts covered by the dataset, and distinguishing between abstract terms (ideas or concepts) and concrete terms (objects or events available to the senses). Average Sentence Length (Sent Len.) is also measured to determine how rich and descriptive the sentences are. Finally, Perplexity is used to provide a measure of data skew by evaluating how expected sentences are from one corpus according to a model trained on another corpus.",NO,True,1100,True,True
2044324-s7,A Survey of Current Datasets for Vision and Language Research,2015-06-23,Crowd-sourced Captions,"• UIUC Pascal Dataset (Farhadi et al., 2010) is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image.

• Flickr 30K Images (Young et al., 2014) extends previous Flickr datasets , and includes 158,915 crowd-sourced captions that describe 31,783 images of people involved in everyday activities and events.

• Microsoft COCO Dataset (MS COCO) (Lin et al., 2014) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled using per-instance segmentations. In total, this dataset contains photos of 91 basic object types with 2.5 million labeled instances in 328k images, each paired with 5 captions. This dataset gave rise to the CVPR 2015 image captioning challenge and is continuing to be a benchmark for comparing various aspects of vision and language research. • Abstract Scenes Dataset (Clipart) (Zitnick et al., 2013) was created with the goal of representing real-world scenes with clipart to study scene semantics isolated from object recognition and segmentation issues in image processing. This removes the burden of low-level vision tasks. This dataset contains 10,020 images of children playing outdoors associated with total 60,396 descriptions.",What are the features and significance of the Microsoft COCO Dataset in vision and language research?,"The Microsoft COCO Dataset (MS COCO) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled using per-instance segmentations. In total, this dataset contains photos of 91 basic object types with 2.5 million labeled instances in 328k images, each paired with 5 captions. This dataset gave rise to the CVPR 2015 image captioning challenge and is continuing to be a benchmark for comparing various aspects of vision and language research.","What makes the Microsoft COCO Dataset important for research in vision and language, and what are its key features?","The Microsoft COCO Dataset (MS COCO) is a pivotal resource in vision and language research due to its comprehensive and detailed representation of complex everyday scenes. It features common objects situated in naturally occurring contexts, with each object meticulously labeled using per-instance segmentations. The dataset encompasses photos of 91 basic object types, amounting to 2.5 million labeled instances across 328,000 images. Each image is accompanied by five descriptive captions, enhancing its utility for tasks that require understanding both visual and textual information. MS COCO's significance is further underscored by its role in the CVPR 2015 image captioning challenge, and it continues to serve as a benchmark for evaluating advancements in various aspects of vision and language research.",9,4,8,4,"Question:

What are the features and significance of the Microsoft COCO Dataset in vision and language research?

Answer:

The Microsoft COCO Dataset (MS COCO) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled using per-instance segmentations. In total, this dataset contains photos of 91 basic object types with 2.5 million labeled instances in 328k images, each paired with 5 captions. This dataset gave rise to the CVPR 2015 image captioning challenge and is continuing to be a benchmark for comparing various aspects of vision and language research.","Question:

What makes the Microsoft COCO Dataset important for research in vision and language, and what are its key features?

Answer:

The Microsoft COCO Dataset (MS COCO) is a pivotal resource in vision and language research due to its comprehensive and detailed representation of complex everyday scenes. It features common objects situated in naturally occurring contexts, with each object meticulously labeled using per-instance segmentations. The dataset encompasses photos of 91 basic object types, amounting to 2.5 million labeled instances across 328,000 images. Each image is accompanied by five descriptive captions, enhancing its utility for tasks that require understanding both visual and textual information. MS COCO's significance is further underscored by its role in the CVPR 2015 image captioning challenge, and it continues to serve as a benchmark for evaluating advancements in various aspects of vision and language research.",NO,True,811,True,True
2044324-s10,A Survey of Current Datasets for Vision and Language Research,2015-06-23,Beyond Visual Description,"Recent work has demonstrated that n-gram language modeling paired with scene-level understanding of an image trained on large enough datasets can result in reasonable automatically generated captions (Fang et al., 2014;Donahue et al., 2014). Some works have proposed to step beyond description generation, towards deeper AI tasks such as question answering (Ren et al., 2015;Malinowski and Fritz, 2014). We present two of these attempts below:

• Visual Madlibs Dataset (VML) (Yu et al., 2015) is a subset of 10,783 images from the MS COCO dataset which aims to go beyond describing which objects are in the image. For a given image, three Amazon Turkers were prompted to complete one of 12 fill-in-the-blank template questions, such as 'when I look at this picture, I feel -', selected automatically based on the image content. This dataset contains a total of 360,001 MadLib question and answers.

• Visual Question Answering (VQA) Dataset (Antol et al., 2015) is created for the task of openended VQA, where a system can be presented with an image and a free-form natural-language question (e.g., 'how many people are in the photo?'), and should be able to answer the question. This dataset contains both real images and abstract scenes, paired with questions and answers. Real images include 123,285 images from MS COCO dataset, and 10,000 clip-art abstract scenes, made up from 20 'paperdoll' human models with adjustable limbs and over 100 objects and 31 animals. Amazon Turkers were prompted to create 'interesting' questions, resulting in 215,150 questions and 430,920 answers.

• Toronto COCO-QA Dataset (CQA) (Ren et al., 2015) is also a visual question answering dataset, where the questions are automatically generated from image captions of MS COCO dataset. This dataset has a total of 123,287 images with 117,684 questions with one-word answer about objects, numbers, colors, or locations.",What are the characteristics and contents of the Visual Question Answering (VQA) Dataset?,"The Visual Question Answering (VQA) Dataset is created for the task of open-ended VQA, where a system can be presented with an image and a free-form natural-language question (e.g., 'how many people are in the photo?'), and should be able to answer the question. This dataset contains both real images and abstract scenes, paired with questions and answers. Real images include 123,285 images from the MS COCO dataset, and 10,000 clip-art abstract scenes, made up from 20 'paperdoll' human models with adjustable limbs and over 100 objects and 31 animals. Amazon Turkers were prompted to create 'interesting' questions, resulting in 215,150 questions and 430,920 answers.",What are the characteristics and contents of the Visual Question Answering (VQA) Dataset?,"The Visual Question Answering (VQA) Dataset is designed for the task of open-ended visual question answering, where a system is presented with an image and a free-form natural-language question, such as ""how many people are in the photo?"" The system is then expected to provide an accurate answer to the question. This dataset includes both real images and abstract scenes. The real images are sourced from the MS COCO dataset, comprising 123,285 images. Additionally, there are 10,000 clip-art abstract scenes, which are constructed using 20 'paperdoll' human models with adjustable limbs, over 100 objects, and 31 animals. To generate the questions, Amazon Mechanical Turk workers were asked to create 'interesting' questions, resulting in a total of 215,150 questions and 430,920 answers.",9,2,9,2,"Question:

What are the characteristics and contents of the Visual Question Answering (VQA) Dataset?

Answer:

The Visual Question Answering (VQA) Dataset is created for the task of open-ended VQA, where a system can be presented with an image and a free-form natural-language question (e.g., 'how many people are in the photo?'), and should be able to answer the question. This dataset contains both real images and abstract scenes, paired with questions and answers. Real images include 123,285 images from the MS COCO dataset, and 10,000 clip-art abstract scenes, made up from 20 'paperdoll' human models with adjustable limbs and over 100 objects and 31 animals. Amazon Turkers were prompted to create 'interesting' questions, resulting in 215,150 questions and 430,920 answers.","Question:

What are the characteristics and contents of the Visual Question Answering (VQA) Dataset?

Answer:

The Visual Question Answering (VQA) Dataset is designed for the task of open-ended visual question answering, where a system is presented with an image and a free-form natural-language question, such as ""how many people are in the photo?"" The system is then expected to provide an accurate answer to the question. This dataset includes both real images and abstract scenes. The real images are sourced from the MS COCO dataset, comprising 123,285 images. Additionally, there are 10,000 clip-art abstract scenes, which are constructed using 20 'paperdoll' human models with adjustable limbs, over 100 objects, and 31 animals. To generate the questions, Amazon Mechanical Turk workers were asked to create 'interesting' questions, resulting in a total of 215,150 questions and 430,920 answers.",NO,True,791,True,True
2044324-s11,A Survey of Current Datasets for Vision and Language Research,2015-06-23,Analysis,"We analyze the datasets introduced in Section 3 according to the metrics defined in Section 2, using the Stanford CoreNLP suite to acquire parses and part-of-speech tags (Manning et al., 2014). We also include the Brown corpus (Francis and Kucera, 1979;Marcus et al., 1999) as a reference point. We find evidence that the VQA dataset captures more abstract concepts than other datasets, with almost 20% of the words found in our abstract concept resource. The Deja corpus has the least number of abstract concepts, followed by COCO and VDC. This reflects differences in col-  Table 2: Perplexities across corpora, where rows represent test sets (20k sentences) and columns training sets (remaining sentences). To make perplexities comparable, we used the same vocabulary frequency cutoff of 3. All models are 5-grams.  Figure 1: Simplified part-of-speech distributions for the eight datasets. We include the POS tags from the balanced Brown corpus (Marcus et al., 1999) to contextualize any very shallow syntactic biases. We mapped all nouns to ""N,"" all verbs to ""V,"" all adjectives to ""J"" and all other POS tags to ""O."" lecting the various corpora: For example, the Deja corpus was collected to find specifically visual phrases that can be used to describe multiple images. This corpus also has the most syntactically simple phrases, as measured by both Frazier and Yngve; this is likely caused by the phrases needing to be general enough to capture multiple images. The most syntactically complex sentences are found in the Flickr30K, COCO and CQA datasets. However, the CQA dataset suffers from a high perplexity against a background corpus relative to the other datasets, at odds with relatively short sentence lengths. This suggests that the automatic caption-to-question conversion may be creating unexpectedly complex sentences that are less reflective of general language usage. In contrast, the COCO and Flickr30K dataset's relatively high syntactic complexity is in line with their relatively high sentence length. Table 2 illustrates further similarities between datasets, and a more fine-grained use of perplexity to measure the usefulness of a given training set for predicting words of a given test set. Some datasets such as COCO, Flickr30K, and Clipart are generally more useful as out-domain data compared to the QA datasets. Test sets for VQA and CQA are quite idiosyncratic and yield poor perplexity unless trained on in-domain data. As shown in Figure 1, the COCO dataset is balanced across POS tags most similarly to the balanced Brown corpus (Marcus et al., 1999). The Clipart dataset provides the highest proportion of verbs, which often correspond to actions/poses in vision research, while the Flickr30K corpus provides the most nouns, which often correspond to object/stuff categories in vision research.

We emphasize here that the distinction between a qualitatively good or bad dataset is task dependent. Therefore, all these metrics and the obtained results provide the researchers with an objective set of criteria so that they can make the decision whether a dataset is suitable to a particular task.",How do the COCO and Flickr30K datasets differ in terms of syntactic complexity and sentence length?,The COCO and Flickr30K datasets have relatively high syntactic complexity. This complexity is in line with their relatively high sentence length.,How do the COCO and Flickr30K datasets compare in terms of syntactic complexity and sentence length?,"The COCO and Flickr30K datasets exhibit relatively high syntactic complexity, which corresponds with their relatively long sentence lengths. This relationship between complexity and length is a notable characteristic of both datasets.",8,2,8,2,"Question:

How do the COCO and Flickr30K datasets differ in terms of syntactic complexity and sentence length?

Answer:

The COCO and Flickr30K datasets have relatively high syntactic complexity. This complexity is in line with their relatively high sentence length.","Question:

How do the COCO and Flickr30K datasets compare in terms of syntactic complexity and sentence length?

Answer:

The COCO and Flickr30K datasets exhibit relatively high syntactic complexity, which corresponds with their relatively long sentence lengths. This relationship between complexity and length is a notable characteristic of both datasets.",NO,True,234,False,False
258740687-s18,A Survey on Zero Pronoun Translation,2023-05-17,Data-Level Methods Do Not Change Model,"Architecture. This is more friendly to NMT. Some researchers targeted making better usage of the limited training data (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021). They trained an external model on the ZP data to recover the ZP information in the input sequence of the MT model (Tan et al., 2019;Ohtani et al., 2019;Tan et al., 2021) or correct the errors in the translation outputs (Voita et al., 2019). Others aimed to up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019;Kimura et al., 2019;Ri et al., 2021). They preferred to  improve the ZPT performance via a data augmentation without modifying the MT architecture (Wang et al., 2016a;Sugiyama and Yoshinaga, 2019). Kimura et al. (2019); Ri et al. (2021) verified that the performance can be further improved by denoising the pseudo data. 4. Multitask and Multi-Lingual Learning. ZPT is a hard task to be done alone, researchers are investigating how to leverage other related NLP tasks to improve ZPT by training models to perform multiple tasks simultaneously (Wang et al., 2018a). Since ZPT is a cross-lingual problem, researchers are exploring techniques for training models that can work across multiple languages, rather than being limited to a single language (Aloraini and Poesio, 2020).

6 Evaluation Methods",How do researchers aim to enhance zero pronoun translation performance without altering the machine translation architecture?,"Some researchers targeted making better usage of the limited training data (Tan et al., 2019; Ohtani et al., 2019; Tan et al., 2021). They trained an external model on the ZP data to recover the ZP information in the input sequence of the MT model (Tan et al., 2019; Ohtani et al., 2019; Tan et al., 2021) or correct the errors in the translation outputs (Voita et al., 2019). Others aimed to up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019; Kimura et al., 2019; Ri et al., 2021). They preferred to improve the ZPT performance via a data augmentation without modifying the MT architecture (Wang et al., 2016a; Sugiyama and Yoshinaga, 2019). Kimura et al. (2019); Ri et al. (2021) verified that the performance can be further improved by denoising the pseudo data.",How are researchers working to improve zero pronoun translation performance without changing the machine translation architecture?,"Researchers are exploring several strategies to enhance zero pronoun translation (ZPT) performance without modifying the machine translation (MT) architecture. One approach involves optimizing the use of limited training data. For instance, some researchers have trained external models specifically on zero pronoun (ZP) data to recover ZP information in the input sequence of the MT model (Tan et al., 2019; Ohtani et al., 2019; Tan et al., 2021) or to correct errors in the translation outputs (Voita et al., 2019). Another strategy focuses on data augmentation, where researchers up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019; Kimura et al., 2019; Ri et al., 2021). This method aims to improve ZPT performance through data augmentation techniques without altering the MT architecture (Wang et al., 2016a; Sugiyama and Yoshinaga, 2019). Additionally, Kimura et al. (2019) and Ri et al. (2021) have demonstrated that denoising pseudo data can further enhance performance.",7,4,7,4,"Question:

How do researchers aim to enhance zero pronoun translation performance without altering the machine translation architecture?

Answer:

Some researchers targeted making better usage of the limited training data (Tan et al., 2019; Ohtani et al., 2019; Tan et al., 2021). They trained an external model on the ZP data to recover the ZP information in the input sequence of the MT model (Tan et al., 2019; Ohtani et al., 2019; Tan et al., 2021) or correct the errors in the translation outputs (Voita et al., 2019). Others aimed to up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019; Kimura et al., 2019; Ri et al., 2021). They preferred to improve the ZPT performance via a data augmentation without modifying the MT architecture (Wang et al., 2016a; Sugiyama and Yoshinaga, 2019). Kimura et al. (2019); Ri et al. (2021) verified that the performance can be further improved by denoising the pseudo data.","Question:

How are researchers working to improve zero pronoun translation performance without changing the machine translation architecture?

Answer:

Researchers are exploring several strategies to enhance zero pronoun translation (ZPT) performance without modifying the machine translation (MT) architecture. One approach involves optimizing the use of limited training data. For instance, some researchers have trained external models specifically on zero pronoun (ZP) data to recover ZP information in the input sequence of the MT model (Tan et al., 2019; Ohtani et al., 2019; Tan et al., 2021) or to correct errors in the translation outputs (Voita et al., 2019). Another strategy focuses on data augmentation, where researchers up-sample the training data for the ZPT task (Sugiyama and Yoshinaga, 2019; Kimura et al., 2019; Ri et al., 2021). This method aims to improve ZPT performance through data augmentation techniques without altering the MT architecture (Wang et al., 2016a; Sugiyama and Yoshinaga, 2019). Additionally, Kimura et al. (2019) and Ri et al. (2021) have demonstrated that denoising pseudo data can further enhance performance.",NO,True,1001,True,True
258740687-s19,A Survey on Zero Pronoun Translation,2023-05-17,Overview,"There are three kinds of automatic metrics to evaluate performances of related models:

• Accuracy of ZP Recovery: this aims to measure model performance on detecting and predicting ZPs of sentences in one pro-drop language. For instance, the micro F1-score is used to evaluating Chinese ZPR systems Song et al. (2020). 11 • General Translation Quality: there are a number of automatic evaluation metrics for measuring general performance of MT systems (Snover  Table 4: Correlation between the manual evaluation and other automatic metrics, which are applied on different ZPT benchmarks, which are same as in Table 2. et al., 2006). BLEU (Papineni et al., 2002) is the most widely-used one, which measures the precision of n-grams of the MT output compared to the reference, weighted by a brevity penalty to punish overly short translations. ME-TEOR (Banerjee and Lavie, 2005) incorporates semantic information by calculating either exact match, stem match, or synonymy match. Furthermore, COMET (Rei et al., 2020) is a neural framework for training multilingual MT evaluation models which obtains new SOTA levels of correlation with human judgements. • Pronoun-Aware Translation Quality: Previous works usually evaluate ZPT using the BLEU metric (Wang et al., 2016a(Wang et al., , 2018aRi et al., 2021), however, general-purpose metrics cannot characterize the performance of ZP translation. As shown in Table 3, the missed or incorrect pronouns may not affect BLEU scores but severely harm true performances. To fix this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018).",Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality in zero pronoun translation?,"General-purpose metrics cannot characterize the performance of zero pronoun (ZP) translation. Previous works usually evaluate ZP translation using the BLEU metric (Wang et al., 2016a; Wang et al., 2018a; Ri et al., 2021). However, as shown in Table 3, the missed or incorrect pronouns may not affect BLEU scores but severely harm true performances. To fix this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018).",Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality in zero pronoun translation?,"General-purpose metrics, such as BLEU, are insufficient for evaluating the quality of zero pronoun (ZP) translation because they fail to accurately reflect the performance in handling pronouns. Previous studies, including those by Wang et al. (2016a), Wang et al. (2018a), and Ri et al. (2021), have typically relied on the BLEU metric for evaluating ZP translation. However, as demonstrated in Table 3, missing or incorrect pronouns may not significantly impact BLEU scores, yet they can severely undermine the true performance of the translation. To address this gap, some researchers have proposed pronoun-targeted evaluation metrics, as seen in the works of Werlen and Popescu-Belis (2017) and Läubli et al. (2018).",7,2,7,2,"Question:

Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality in zero pronoun translation?

Answer:

General-purpose metrics cannot characterize the performance of zero pronoun (ZP) translation. Previous works usually evaluate ZP translation using the BLEU metric (Wang et al., 2016a; Wang et al., 2018a; Ri et al., 2021). However, as shown in Table 3, the missed or incorrect pronouns may not affect BLEU scores but severely harm true performances. To fix this gap, some works proposed pronoun-targeted evaluation metrics (Werlen and Popescu-Belis, 2017; Läubli et al., 2018).","Question:

Why are general-purpose metrics insufficient for evaluating pronoun-aware translation quality in zero pronoun translation?

Answer:

General-purpose metrics, such as BLEU, are insufficient for evaluating the quality of zero pronoun (ZP) translation because they fail to accurately reflect the performance in handling pronouns. Previous studies, including those by Wang et al. (2016a), Wang et al. (2018a), and Ri et al. (2021), have typically relied on the BLEU metric for evaluating ZP translation. However, as demonstrated in Table 3, missing or incorrect pronouns may not significantly impact BLEU scores, yet they can severely undermine the true performance of the translation. To address this gap, some researchers have proposed pronoun-targeted evaluation metrics, as seen in the works of Werlen and Popescu-Belis (2017) and Läubli et al. (2018).",NO,True,719,True,True
258557362-s3,Large Language Models Meet NL2Code: A Survey,2022-12-19,Large Model Size,"As shown in Figure 2 and Table 2, recent LLMs for NL2Code exhibit larger sizes and superior performance. This is consistent with prior findings that an increased number of model parameters can enhance model capabilities (Radford et al., 2019;Thoppilan et al., 2022;Chowdhery et al., 2022). We further demonstrate the correlation between model size and performance in Figure 3a, which compares the pass@1 results of 10 representative models on the HumanEval benchmark. It is clear that larger models generally result in better performance. Furthermore, we also find that current models, regardless of size, still have the potential for improvement through further increases in size. Additional results on the HumanEval and MBPP benchmarks can be found in Appendix Figure 7, which also support this conclusion.

Additionally, we conduct an experiment on the HumanEval benchmark to examine the syntax error rates of the code generated by different models of varying sizes. Specifically, we make the models predict 10 code samples for each programming problem, and then calculate the percentage of code samples that have syntax errors. As shown in Figure 3b, results indicate that larger models tend to have lower syntax error rates. It is noteworthy that the largest version of the CodeGen-Mono model exhibits a remarkably low rate of syntax errors, i.e., 6%. However, as evidenced by Figure 3a and Table 2, the CodeGen-Mono model with 16 billion parameters still has unsatisfactory performance in terms of pass@k , e.g., pass@1 to be 29%. This highlights the fact that the current limitation for large pre-trained models is the generation of semantically correct code.",What is the relationship between model size and syntax error rates in code generated by large language models?,"Larger models generally result in better performance. Additionally, results indicate that larger models tend to have lower syntax error rates. It is noteworthy that the largest version of the CodeGen-Mono model exhibits a remarkably low rate of syntax errors, i.e., 6%.",How does the size of a model affect the syntax error rates in code generated by large language models?,"In general, larger models tend to deliver better performance, which includes producing code with fewer syntax errors. Notably, as the size of the model increases, the rate of syntax errors typically decreases. For instance, the largest version of the CodeGen-Mono model demonstrates a particularly low syntax error rate of just 6%.",8,4,7,4,"Question:

What is the relationship between model size and syntax error rates in code generated by large language models?

Answer:

Larger models generally result in better performance. Additionally, results indicate that larger models tend to have lower syntax error rates. It is noteworthy that the largest version of the CodeGen-Mono model exhibits a remarkably low rate of syntax errors, i.e., 6%.","Question:

How does the size of a model affect the syntax error rates in code generated by large language models?

Answer:

In general, larger models tend to deliver better performance, which includes producing code with fewer syntax errors. Notably, as the size of the model increases, the rate of syntax errors typically decreases. For instance, the largest version of the CodeGen-Mono model demonstrates a particularly low syntax error rate of just 6%.",NO,True,331,False,False
258557362-s4,Large Language Models Meet NL2Code: A Survey,2022-12-19,Large and Premium Data,"As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. This highlights the importance of selecting and pre-processing high-quality data. In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.

Early models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet (Husain et al., 2019), CoST (Zhu et al., 2022b), and XL-CoST (Zhu et al., 2022a). However, manual annotation is labour-intensive and time-consuming. There are also models like GPT-3 (Brown et al., 2020), GPT-Neo (Black et al., 2021), and GPT-J (Wang and Komatsuzaki, 2021)  In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities. One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.",What are the common data pre-processing strategies used to ensure high-quality training corpora for LLMs in NL2Code?,"As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. One common strategy is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.",What data pre-processing strategies are commonly used to ensure high-quality training corpora for large language models (LLMs) in the NL2Code domain?,"As large language models (LLMs) in the NL2Code domain continue to grow in size, the scale of the training corpus must also expand. To maintain the quality of this corpus, extensive data pre-processing is essential. One prevalent strategy involves removing code files that are likely auto-generated or unfinished, as these are considered meaningless. Additionally, specific filtering rules are applied to exclude uncommon code files. These rules take into account factors such as the repository's star rating, file size, line length, and alphanumeric rate. Ultimately, the aim of these pre-processing strategies is to create a code corpus that is unduplicated, complete, correct, clean, and general in nature.",7,2,7,4,"Question:

What are the common data pre-processing strategies used to ensure high-quality training corpora for LLMs in NL2Code?

Answer:

As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. One common strategy is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.","Question:

What data pre-processing strategies are commonly used to ensure high-quality training corpora for large language models (LLMs) in the NL2Code domain?

Answer:

As large language models (LLMs) in the NL2Code domain continue to grow in size, the scale of the training corpus must also expand. To maintain the quality of this corpus, extensive data pre-processing is essential. One prevalent strategy involves removing code files that are likely auto-generated or unfinished, as these are considered meaningless. Additionally, specific filtering rules are applied to exclude uncommon code files. These rules take into account factors such as the repository's star rating, file size, line length, and alphanumeric rate. Ultimately, the aim of these pre-processing strategies is to create a code corpus that is unduplicated, complete, correct, clean, and general in nature.",NO,True,708,True,True
254877175-s3,A Survey of Deep Learning for Mathematical Reasoning,2022-12-20,Seq2Seq-based Networks for Math,"Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).

Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).",How do Seq2Seq models compare to previous statistical learning approaches in mathematical reasoning tasks?,"A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017; Wang et al., 2018a; Huang et al., 2018; Wang et al., 2019; Li et al., 2019).",How do Seq2Seq models perform in comparison to earlier statistical learning methods in the context of mathematical reasoning tasks?,"Extensive research has demonstrated that Seq2Seq models significantly outperform earlier statistical learning methods in mathematical reasoning tasks. This performance advantage has been documented in various studies, including those by Ling et al. (2017), Wang et al. (2018a), Huang et al. (2018), Wang et al. (2019), and Li et al. (2019).",7,4,7,4,"Question:

How do Seq2Seq models compare to previous statistical learning approaches in mathematical reasoning tasks?

Answer:

A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017; Wang et al., 2018a; Huang et al., 2018; Wang et al., 2019; Li et al., 2019).","Question:

How do Seq2Seq models perform in comparison to earlier statistical learning methods in the context of mathematical reasoning tasks?

Answer:

Extensive research has demonstrated that Seq2Seq models significantly outperform earlier statistical learning methods in mathematical reasoning tasks. This performance advantage has been documented in various studies, including those by Ling et al. (2017), Wang et al. (2018a), Huang et al. (2018), Wang et al. (2019), and Li et al. (2019).",NO,True,340,False,False
254877175-s10,A Survey of Deep Learning for Mathematical Reasoning,2022-12-20,In-context Learning for Mathematical Reasoning,"Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).

Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.

Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.

Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).

Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.

Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.",How does Chain-of-thought prompting (CoT) enhance mathematical reasoning in large language models (LLMs)?,"Chain-of-thought prompting (CoT) leverages intermediate natural language rationales as prompts to enable large language models (LLMs) to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving a math word problem could involve breaking down the problem into steps, such as calculating the total number of tennis balls Roger has by adding the initial number of balls to the number obtained from additional cans. This method allows LLMs to process the problem step by step, leading to a more accurate final answer. Most recent work has focused on improving chain-of-thought reasoning under the few-shot setting by selecting better in-context examples and creating better reasoning chains.",How does Chain-of-thought prompting (CoT) improve mathematical reasoning in large language models (LLMs)?,"Chain-of-thought prompting (CoT) enhances mathematical reasoning in large language models (LLMs) by using intermediate natural language rationales as prompts. This approach enables LLMs to generate reasoning chains before predicting an answer to a given question. For instance, when solving a math word problem, a CoT prompt might involve breaking down the problem into sequential steps, such as calculating the total number of tennis balls Roger has by adding the initial number of balls to those obtained from additional cans. By processing the problem step by step, LLMs can arrive at a more accurate final answer. Recent research has concentrated on improving chain-of-thought reasoning in few-shot settings by selecting better in-context examples and constructing more effective reasoning chains.",7,3,8,4,"Question:

How does Chain-of-thought prompting (CoT) enhance mathematical reasoning in large language models (LLMs)?

Answer:

Chain-of-thought prompting (CoT) leverages intermediate natural language rationales as prompts to enable large language models (LLMs) to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving a math word problem could involve breaking down the problem into steps, such as calculating the total number of tennis balls Roger has by adding the initial number of balls to the number obtained from additional cans. This method allows LLMs to process the problem step by step, leading to a more accurate final answer. Most recent work has focused on improving chain-of-thought reasoning under the few-shot setting by selecting better in-context examples and creating better reasoning chains.","Question:

How does Chain-of-thought prompting (CoT) improve mathematical reasoning in large language models (LLMs)?

Answer:

Chain-of-thought prompting (CoT) enhances mathematical reasoning in large language models (LLMs) by using intermediate natural language rationales as prompts. This approach enables LLMs to generate reasoning chains before predicting an answer to a given question. For instance, when solving a math word problem, a CoT prompt might involve breaking down the problem into sequential steps, such as calculating the total number of tennis balls Roger has by adding the initial number of balls to those obtained from additional cans. By processing the problem step by step, LLMs can arrive at a more accurate final answer. Recent research has concentrated on improving chain-of-thought reasoning in few-shot settings by selecting better in-context examples and constructing more effective reasoning chains.",NO,True,801,True,True
254877175-s11,A Survey of Deep Learning for Mathematical Reasoning,2022-12-20,In-context Example Selection,"Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. 

Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. ",What methods have been explored to optimize the selection of in-context examples for mathematical reasoning tasks?,"Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022; Liu et al., 2022a). Therefore, which in-context reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022; Zhang et al., 2023; Lu et al., 2022b; Yu et al., 2023; Fu et al., 2023).",What strategies have been explored to enhance the selection of in-context examples for mathematical reasoning tasks?,"Initially, chain-of-thought approaches relied on random or heuristic selection of in-context examples. However, recent studies have highlighted the instability of few-shot learning across different selections of these examples (Rubin et al., 2022; Liu et al., 2022a). This indicates that identifying the most effective in-context reasoning examples remains an unresolved issue in the field. To address this challenge, recent research has explored various methods to optimize the selection process for in-context examples (Rubin et al., 2022; Zhang et al., 2023; Lu et al., 2022b; Yu et al., 2023; Fu et al., 2023).",7,4,7,6,"Question:

What methods have been explored to optimize the selection of in-context examples for mathematical reasoning tasks?

Answer:

Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022; Liu et al., 2022a). Therefore, which in-context reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022; Zhang et al., 2023; Lu et al., 2022b; Yu et al., 2023; Fu et al., 2023).","Question:

What strategies have been explored to enhance the selection of in-context examples for mathematical reasoning tasks?

Answer:

Initially, chain-of-thought approaches relied on random or heuristic selection of in-context examples. However, recent studies have highlighted the instability of few-shot learning across different selections of these examples (Rubin et al., 2022; Liu et al., 2022a). This indicates that identifying the most effective in-context reasoning examples remains an unresolved issue in the field. To address this challenge, recent research has explored various methods to optimize the selection process for in-context examples (Rubin et al., 2022; Zhang et al., 2023; Lu et al., 2022b; Yu et al., 2023; Fu et al., 2023).",NO,True,614,True,True
254877175-s14,A Survey of Deep Learning for Mathematical Reasoning,2022-12-20,Analysis of Deep Learning Methods,"Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but

Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but",How do recent studies evaluate the effectiveness of subword tokenization techniques for numeracy in language models?,"Recent studies have shown that subword tokenization approaches are suboptimal (Wallace et al., 2019; Lin et al., 2020; Zhang et al., 2020d; Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1,598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers.",How do recent studies assess the effectiveness of subword tokenization techniques in handling numeracy within language models?,"Recent studies have highlighted the limitations of subword tokenization techniques when it comes to handling numeracy in language models (Wallace et al., 2019; Lin et al., 2020; Zhang et al., 2020d; Thawani et al., 2022). These approaches are often suboptimal because numbers that are close on a number line can have surface forms with no shared common tokens. For instance, the number 1598 might be tokenized as ""15"" and ""98"" in GPT-3, whereas the same number formatted as 1,598 could be split into three different tokens: ""1"", "","", and ""598"". This inconsistency in representation poses challenges for deep learning models in processing numbers effectively, especially when compared to handling pure text. The inadequate representation of numbers can lead to out-of-distribution (OOD) problems. Although increasing the scale of models might offer some improvement, even advanced models like GPT-3 struggle with reasoning over large numbers.",7,4,7,4,"Question:

How do recent studies evaluate the effectiveness of subword tokenization techniques for numeracy in language models?

Answer:

Recent studies have shown that subword tokenization approaches are suboptimal (Wallace et al., 2019; Lin et al., 2020; Zhang et al., 2020d; Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1,598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers.","Question:

How do recent studies assess the effectiveness of subword tokenization techniques in handling numeracy within language models?

Answer:

Recent studies have highlighted the limitations of subword tokenization techniques when it comes to handling numeracy in language models (Wallace et al., 2019; Lin et al., 2020; Zhang et al., 2020d; Thawani et al., 2022). These approaches are often suboptimal because numbers that are close on a number line can have surface forms with no shared common tokens. For instance, the number 1598 might be tokenized as ""15"" and ""98"" in GPT-3, whereas the same number formatted as 1,598 could be split into three different tokens: ""1"", "","", and ""598"". This inconsistency in representation poses challenges for deep learning models in processing numbers effectively, especially when compared to handling pure text. The inadequate representation of numbers can lead to out-of-distribution (OOD) problems. Although increasing the scale of models might offer some improvement, even advanced models like GPT-3 struggle with reasoning over large numbers.",NO,True,941,True,True
254877175-s15,A Survey of Deep Learning for Mathematical Reasoning,2022-12-20,Problems,"GPT-3 (text-davinci-002)

John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.

John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?

Mary has 5 balls.

John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.

John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.

John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.

John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.

Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work

GPT-3 (text-davinci-002)

John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.

John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?

Mary has 5 balls.

John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.

John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.

John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.

John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.

Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work","How do state-of-the-art deep learning models perform on the SVAMP dataset, and what does this indicate about their consistency?","It is surprising that current state-of-the-art (SOTA) methods perform poorly on the SVAMP dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models.","How effective are state-of-the-art deep learning models on the SVAMP dataset, and what does this reveal about their consistency in performance?","It is quite surprising that current state-of-the-art (SOTA) methods exhibit poor performance on the SVAMP dataset. For instance, the Graph2Tree model achieves only a 43.8% accuracy, while the zero-shot-CoT GPT-3 (175B) model reaches just 63.7%, which is barely above an ""F"" grade. This performance indicates a significant lack of consistency in the mathematical reasoning abilities of these SOTA large language models.",7,4,7,4,"Question:

How do state-of-the-art deep learning models perform on the SVAMP dataset, and what does this indicate about their consistency?

Answer:

It is surprising that current state-of-the-art (SOTA) methods perform poorly on the SVAMP dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models.","Question:

How effective are state-of-the-art deep learning models on the SVAMP dataset, and what does this reveal about their consistency in performance?

Answer:

It is quite surprising that current state-of-the-art (SOTA) methods exhibit poor performance on the SVAMP dataset. For instance, the Graph2Tree model achieves only a 43.8% accuracy, while the zero-shot-CoT GPT-3 (175B) model reaches just 63.7%, which is barely above an ""F"" grade. This performance indicates a significant lack of consistency in the mathematical reasoning abilities of these SOTA large language models.",NO,True,418,True,True
254877175-s42,A Survey of Deep Learning for Mathematical Reasoning,2022-12-20,Seq2Seq-based Networks for Math,"Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).

Sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) neural networks have been successfully applied to mathematical reasoning tasks, such as math word problem solving (Wang et al., 2017), theorem proving (Yang and Deng, 2019), geometry problem solving (Robaidek et al., 2018), and math question answering (Tafjord et al., 2019). A Seq2Seq model uses an encoder-decoder architecture and usually formalizes mathematical reasoning as a sequence generation task. The basic idea behind this approach is to map an input sequence (e.g. a mathematical problem) to an output sequence (e.g. an equation, program, and proof). Common encoders and decoders include Long Short Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), and their bidirectional variants: BiLSTM and BiGRU. A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017;Wang et al., 2018a;Huang et al., 2018;Wang et al., 2019;Li et al., 2019).",How do Seq2Seq models compare to previous statistical learning approaches in mathematical reasoning tasks?,"A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017; Wang et al., 2018a; Huang et al., 2018; Wang et al., 2019; Li et al., 2019).",How do Seq2Seq models perform in mathematical reasoning tasks compared to earlier statistical learning methods?,"Extensive research has demonstrated the superior performance of Seq2Seq models over earlier statistical learning methods in mathematical reasoning tasks. This advantage has been highlighted in several studies, including those by Ling et al. (2017), Wang et al. (2018a), Huang et al. (2018), Wang et al. (2019), and Li et al. (2019).",7,4,7,4,"Question:

How do Seq2Seq models compare to previous statistical learning approaches in mathematical reasoning tasks?

Answer:

A large amount of work has shown the performance advantage of Seq2Seq models over previous statistical learning approaches (Ling et al., 2017; Wang et al., 2018a; Huang et al., 2018; Wang et al., 2019; Li et al., 2019).","Question:

How do Seq2Seq models perform in mathematical reasoning tasks compared to earlier statistical learning methods?

Answer:

Extensive research has demonstrated the superior performance of Seq2Seq models over earlier statistical learning methods in mathematical reasoning tasks. This advantage has been highlighted in several studies, including those by Ling et al. (2017), Wang et al. (2018a), Huang et al. (2018), Wang et al. (2019), and Li et al. (2019).",NO,True,332,False,False
254877175-s49,A Survey of Deep Learning for Mathematical Reasoning,2022-12-20,In-context Learning for Mathematical Reasoning,"Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).

Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.

Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.

Large language models (LLMs), such as GPT-3 (Brown et al., 2020), have recently revolutionized the field of natural language processing (NLP), especially on account of their powerful few-shot incontext learning capabilities (Brown et al., 2020). In-context Learning (ICL) enables LLMs to perform target tasks by providing some task examples as conditions at inference time, without updating model parameters (Radford et al., 2020;Brown et al., 2020). ICL allows users to quickly build models for new use cases without worrying about fine-tuning and storing a large amount of new parameters for each task, so it is widely used in fewshot settings nowadays (Min et al., 2022). An in-context example typically contains an input-output pair with some prompt words, e.g., Please select the largest number from the list. Input: [2,4,1,5,8]. Output: 8, and few-shot works by giving multiple examples, and then a final input example, where the model is expected to predict the output. However, such standard few-shot promptings, in which the LLM is given in-context examples of input-output pairs in front of test-time examples, have not yet proved sufficient to achieve high performance on challenging tasks such as mathematical reasoning (Rae et al., 2021).

Chain-of-thought prompting (CoT) (Wei et al., 2022) leverages intermediate natural language rationales as prompts to enable LLMs to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving the math word problem could be Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. Then, how many tennis balls does Roger have now? Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each are 6 tennis balls. 5 + 6 = 11. The answer is 11.

Apart from Kojima et al. (2022) showing that LLMs are decent zero-shot reasoners when given the ""Let's think step by step!"" prompt, most of the recent work has focused on how to improve chainof-thought reasoning under the few-shot setting. This work is mainly divided into two parts, (i) selecting better in-context examples and (ii) creating better reasoning chains.",How does Chain-of-thought prompting (CoT) enhance mathematical reasoning in large language models?,"Chain-of-thought prompting (CoT) leverages intermediate natural language rationales as prompts to enable large language models (LLMs) to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving a math word problem could involve breaking down the problem into steps, such as calculating the total number of tennis balls Roger has by adding the initial number of balls to the number obtained from additional cans. This approach allows LLMs to systematically work through the problem, enhancing their ability to perform mathematical reasoning. Most recent work has focused on improving chain-of-thought reasoning under the few-shot setting by selecting better in-context examples and creating better reasoning chains.",How does Chain-of-thought prompting (CoT) improve the mathematical reasoning capabilities of large language models?,"Chain-of-thought prompting (CoT) enhances the mathematical reasoning abilities of large language models (LLMs) by using intermediate natural language rationales as prompts. This method enables LLMs to generate reasoning chains before predicting an answer to a given question. For instance, when solving a math word problem, a CoT prompt might involve breaking down the problem into sequential steps, such as calculating the total number of tennis balls Roger has by adding the initial number of balls to those obtained from additional cans. This structured approach allows LLMs to systematically work through the problem, thereby improving their mathematical reasoning skills. Recent research has concentrated on refining chain-of-thought reasoning in few-shot settings by selecting more effective in-context examples and developing better reasoning chains.",8,4,8,4,"Question:

How does Chain-of-thought prompting (CoT) enhance mathematical reasoning in large language models?

Answer:

Chain-of-thought prompting (CoT) leverages intermediate natural language rationales as prompts to enable large language models (LLMs) to first generate reasoning chains and then predict an answer for an input question. For example, a CoT prompt for solving a math word problem could involve breaking down the problem into steps, such as calculating the total number of tennis balls Roger has by adding the initial number of balls to the number obtained from additional cans. This approach allows LLMs to systematically work through the problem, enhancing their ability to perform mathematical reasoning. Most recent work has focused on improving chain-of-thought reasoning under the few-shot setting by selecting better in-context examples and creating better reasoning chains.","Question:

How does Chain-of-thought prompting (CoT) improve the mathematical reasoning capabilities of large language models?

Answer:

Chain-of-thought prompting (CoT) enhances the mathematical reasoning abilities of large language models (LLMs) by using intermediate natural language rationales as prompts. This method enables LLMs to generate reasoning chains before predicting an answer to a given question. For instance, when solving a math word problem, a CoT prompt might involve breaking down the problem into sequential steps, such as calculating the total number of tennis balls Roger has by adding the initial number of balls to those obtained from additional cans. This structured approach allows LLMs to systematically work through the problem, thereby improving their mathematical reasoning skills. Recent research has concentrated on refining chain-of-thought reasoning in few-shot settings by selecting more effective in-context examples and developing better reasoning chains.",NO,True,857,True,True
254877175-s50,A Survey of Deep Learning for Mathematical Reasoning,2022-12-20,In-context Example Selection,"Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. 

Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022;Liu et al., 2022a). Therefore, which incontext reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022;Zhang et al., 2023;Lu et al., 2022b;Yu et al., 2023;Fu et al., 2023). For example, Rubin et al. ",What methods have been explored to optimize the selection of in-context examples for mathematical reasoning tasks?,"Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022; Liu et al., 2022a). Therefore, which in-context reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022; Zhang et al., 2023; Lu et al., 2022b; Yu et al., 2023; Fu et al., 2023).",What strategies have been explored to enhance the selection of in-context examples for mathematical reasoning tasks?,"Initially, chain-of-thought approaches relied on random or heuristic selection of in-context examples. However, recent studies, such as those by Rubin et al. (2022) and Liu et al. (2022a), have demonstrated that this method of few-shot learning can be highly unstable, with performance varying significantly depending on the chosen examples. As a result, determining which in-context reasoning examples serve as the most effective prompts remains an unresolved issue in the field. To address this challenge, recent research has explored various strategies to optimize the selection process of in-context examples, as highlighted in studies by Rubin et al. (2022), Zhang et al. (2023), Lu et al. (2022b), Yu et al. (2023), and Fu et al. (2023).",7,4,7,6,"Question:

What methods have been explored to optimize the selection of in-context examples for mathematical reasoning tasks?

Answer:

Early chain-of-thought work randomly or heuristically selects in-context examples. However, recent studies have shown that this type of few-shot learning can be highly unstable across different selections of in-context examples (Rubin et al., 2022; Liu et al., 2022a). Therefore, which in-context reasoning examples make the most effective prompts is still an unknown problem in the literature. To address the limitation, recent work has investigated various methods to optimize the in-context examples selection process (Rubin et al., 2022; Zhang et al., 2023; Lu et al., 2022b; Yu et al., 2023; Fu et al., 2023).","Question:

What strategies have been explored to enhance the selection of in-context examples for mathematical reasoning tasks?

Answer:

Initially, chain-of-thought approaches relied on random or heuristic selection of in-context examples. However, recent studies, such as those by Rubin et al. (2022) and Liu et al. (2022a), have demonstrated that this method of few-shot learning can be highly unstable, with performance varying significantly depending on the chosen examples. As a result, determining which in-context reasoning examples serve as the most effective prompts remains an unresolved issue in the field. To address this challenge, recent research has explored various strategies to optimize the selection process of in-context examples, as highlighted in studies by Rubin et al. (2022), Zhang et al. (2023), Lu et al. (2022b), Yu et al. (2023), and Fu et al. (2023).",NO,True,743,True,True
254877175-s53,A Survey of Deep Learning for Mathematical Reasoning,2022-12-20,Analysis of Deep Learning Methods,"Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but

Is the current representation of numeracy sufficient? The standard practice for deep learning techniques is to treat numbers in the same way as words. Early neural network methods create a vocabulary that maps input words and numbers to token IDs, resulting in less frequent numbers being collapsed into an ""UNK"" token. Recent language models use subword tokenization techniques (Wu et al., 2016;Sennrich et al., 2016) to split numbers into atomic tokens. Recent studies have shown that these tokenization approaches are suboptimal (Wallace et al., 2019;Lin et al., 2020;Zhang et al., 2020d;Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1, 598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Table 5 provides examples of where language models tend to struggle with large numbers. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers. Some recent work suggests that using scientific notation (Zhang et al., 2020d) and digitlevel decomposition (Geva et al., 2020) may be helpful in improving numeracy representation, but",How do recent studies evaluate the effectiveness of subword tokenization techniques for numeracy in language models?,"Recent studies have shown that subword tokenization approaches are suboptimal (Wallace et al., 2019; Lin et al., 2020; Zhang et al., 2020d; Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1,598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers.",How do recent studies assess the effectiveness of subword tokenization techniques in handling numeracy within language models?,"Recent studies have highlighted the limitations of subword tokenization techniques when it comes to handling numeracy in language models (Wallace et al., 2019; Lin et al., 2020; Zhang et al., 2020d; Thawani et al., 2022). These approaches are often suboptimal because numbers that are close on a number line can have surface forms with no shared tokens. For instance, the number 1598 might be tokenized as ""15"" and ""98"" in GPT-3, whereas the same number formatted as 1,598 could be split into three different tokens: ""1"", "","", and ""598"". This inconsistency in representation poses challenges for deep learning models in processing numbers effectively, especially when compared to handling pure text. Such inadequate representations can lead to out-of-distribution (OOD) issues. Although increasing the scale of models might offer some improvement, even advanced models like GPT-3 struggle with reasoning over large numbers.",8,4,7,4,"Question:

How do recent studies evaluate the effectiveness of subword tokenization techniques for numeracy in language models?

Answer:

Recent studies have shown that subword tokenization approaches are suboptimal (Wallace et al., 2019; Lin et al., 2020; Zhang et al., 2020d; Thawani et al., 2022). Two numbers on the same or close number line could have surface forms with no shared common tokens. For example, a number like 1598 is tokenized as ""15"" and ""98"" in GPT-3, while another format like 1,598 is split as three different tokens: ""1"", "","", and ""598"". This lack of consistent representation can make it difficult for deep learning models to effectively process numbers, especially when compared to pure text. The insufficient representations of numbers can lead to out-of-distribution (OOD) problems. Although increasing model scales could help, even the state-of-the-art large language model GPT-3 performs poorly when reasoning over large numbers.","Question:

How do recent studies assess the effectiveness of subword tokenization techniques in handling numeracy within language models?

Answer:

Recent studies have highlighted the limitations of subword tokenization techniques when it comes to handling numeracy in language models (Wallace et al., 2019; Lin et al., 2020; Zhang et al., 2020d; Thawani et al., 2022). These approaches are often suboptimal because numbers that are close on a number line can have surface forms with no shared tokens. For instance, the number 1598 might be tokenized as ""15"" and ""98"" in GPT-3, whereas the same number formatted as 1,598 could be split into three different tokens: ""1"", "","", and ""598"". This inconsistency in representation poses challenges for deep learning models in processing numbers effectively, especially when compared to handling pure text. Such inadequate representations can lead to out-of-distribution (OOD) issues. Although increasing the scale of models might offer some improvement, even advanced models like GPT-3 struggle with reasoning over large numbers.",NO,True,923,True,True
254877175-s54,A Survey of Deep Learning for Mathematical Reasoning,2022-12-20,Problems,"GPT-3 (text-davinci-002)

John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.

John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?

Mary has 5 balls.

John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.

John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.

John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.

John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.

Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work

GPT-3 (text-davinci-002)

John had 8 balls and he gave 3 to Mary. How many balls does John have now? John has 5 balls.

John had 3 apples. John had 8 balls and he gave 3 to Mary. How many balls does Mary have now?

Mary has 5 balls.

John had 8 balls and he gave 3 to Mary. Who has more balls now? John has more balls.

John had 8 balls and he gave 3 to Mary. Does John have more balls now? No, John has 5 balls now.

John had 8 balls and he gave 4 to Mary. Does John have more balls now? No, John has 4 balls now.

John had 8 balls and he gave 4 to Mary. Who has more balls now? John has more balls. this remains an open problem.

Are deep learning methods consistent for mathematical reasoning? Recent developments in deep learning have led to impressive results on various mathematical reasoning tasks. The zero-shot-CoT Minerva 540B achieves a score of 75.0% on the MMLU-STEM benchmark (Hendrycks et al., 2021a), which assesses multitask reasoning ability in the fields of science, technology, engineering, and mathematics (STEM) at both high school and college levels. Similarly, few-shot-CoT GPT-3 175B achieves a high accuracy of 93.0% on the MultiArith task. However, the question remains as to whether these methods are sufficiently advanced to tackle more complex problems. There is strong evidence that deep learning methods for mathematical reasoning are not robust and susceptible to adversarial attacks (Lin et al., 2020;Patel et al., 2021;Mishra et al., 2022b,a;Welleck et al., 2022b). The SVAMP (Patel et al., 2021) dataset is a collection of one-unknown arithmetic word problems up to grade 4, with slight word variations from previous datasets. It is surprising that current state-of-the-art (SOTA) methods perform poorly on this dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. Table 6 also shows the inconsistent performance of the zero-shot GPT-3 model in scenarios with slightly different descriptions, while human performance remains unchanged. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models. 7 Future Work","How do state-of-the-art deep learning models perform on the SVAMP dataset, and what does this indicate about their consistency?","It is surprising that current state-of-the-art (SOTA) methods perform poorly on the SVAMP dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models.","How effective are state-of-the-art deep learning models on the SVAMP dataset, and what does this reveal about their consistency?","It is surprising that current state-of-the-art (SOTA) methods perform poorly on the SVAMP dataset. For instance, the Graph2Tree model achieves only a 43.8% accuracy, while the zero-shot-CoT GPT-3 (175B) model reaches just 63.7%, which is barely above an ""F"" grade. This performance indicates a lack of consistency in the mathematical reasoning abilities of these SOTA large language models.",7,4,7,4,"Question:

How do state-of-the-art deep learning models perform on the SVAMP dataset, and what does this indicate about their consistency?

Answer:

It is surprising that current state-of-the-art (SOTA) methods perform poorly on the SVAMP dataset, with Graph2Tree achieving only a 43.8% accuracy and zero-shot-CoT GPT-3 (175B) only reaching 63.7%, which is just above an ""F"" grade. This indicates a lack of consistency in the mathematical reasoning ability of SOTA large language models.","Question:

How effective are state-of-the-art deep learning models on the SVAMP dataset, and what does this reveal about their consistency?

Answer:

It is surprising that current state-of-the-art (SOTA) methods perform poorly on the SVAMP dataset. For instance, the Graph2Tree model achieves only a 43.8% accuracy, while the zero-shot-CoT GPT-3 (175B) model reaches just 63.7%, which is barely above an ""F"" grade. This performance indicates a lack of consistency in the mathematical reasoning abilities of these SOTA large language models.",NO,True,390,False,False
254564474-s5,A Survey on Natural Language Processing for Programming,2022-12-12,Synthesis Tasks,"The synthesis task generates a program given a context (which can be NL, PL, or their mixture), thus can accelerate the development process. It can be further divided into program synthesis and code completion by the formal completeness of the output. The output of program synthesis is a relatively independent unit, such as a function and a class, while the output of code completion is less restricted, ranging from tokens to code snippets.

Program synthesis is also called code generation. It is the systematic derivation of a program from a given specification (Manna and Waldinger, 1980). Conventional deductive approaches (Manna and Waldinger, 1980;Polozov and Gulwani, 2015) take logical specifications, which are logically complete but hard to write. Inductive approaches (Lieberman, 2001) list inputoutput examples as specifications, which are more accessible but incomplete. In contrast, an NL specification is sufficient to describe the logic of a program. Meanwhile, it is compatible with inputoutput examples by including them in a docstring. Therefore, it can take advantage of both the deductive and inductive approaches.

Code completion is also called code suggestion in early research (Tu et al., 2014;Hindle et al., 2016). It suggests the next program token given a context and has been widely applied to IDEs . The application scenario includes the completion of method calls, keywords, variables, and arguments. With the bloom of the pre-trained models, the scenario has been extended to punctuations, statements, and even code snippets , further blurring the line between program synthesis and code completion.",How do natural language specifications enhance program synthesis compared to traditional deductive and inductive approaches?,"An NL specification is sufficient to describe the logic of a program. Meanwhile, it is compatible with input-output examples by including them in a docstring. Therefore, it can take advantage of both the deductive and inductive approaches.",How do natural language specifications improve program synthesis compared to traditional deductive and inductive methods?,"Natural language (NL) specifications enhance program synthesis by providing a comprehensive description of a program's logic. They are versatile enough to incorporate input-output examples within a docstring, allowing them to leverage the strengths of both deductive and inductive approaches. This dual compatibility makes NL specifications a powerful tool in program synthesis.",7,4,7,4,"Question:

How do natural language specifications enhance program synthesis compared to traditional deductive and inductive approaches?

Answer:

An NL specification is sufficient to describe the logic of a program. Meanwhile, it is compatible with input-output examples by including them in a docstring. Therefore, it can take advantage of both the deductive and inductive approaches.","Question:

How do natural language specifications improve program synthesis compared to traditional deductive and inductive methods?

Answer:

Natural language (NL) specifications enhance program synthesis by providing a comprehensive description of a program's logic. They are versatile enough to incorporate input-output examples within a docstring, allowing them to leverage the strengths of both deductive and inductive approaches. This dual compatibility makes NL specifications a powerful tool in program synthesis.",NO,True,378,False,False
254564474-s6,A Survey on Natural Language Processing for Programming,2022-12-12,Transcription Tasks,"The transcription task converts a given program to meet a specific requirement. Concretely, program translation aims to convert between highlevel PL (Roziere et al., 2020;Zhu et al., 2022), e.g., C# and Java. It can accelerate the update of projects written by deprecated PL, and the migration of algorithms implemented by various PLs. Code refinement aims to convert a buggy program into correct one (Wang et al., 2021). It is closely related to vulnerability identification but is required to fix the detected bugs simultaneously. The transcription task differs from the synthesis task in two aspects. First, its input program is formally complete (input program is None or a function header in program synthesis, a partial code snippet in code completion). Second, its output can be strictly aligned with the input in both the format and the content.","How do transcription tasks differ from synthesis tasks in programming, particularly regarding input and output characteristics?","The transcription task differs from the synthesis task in two aspects. First, its input program is formally complete, whereas in program synthesis, the input program is None or a function header, and in code completion, it is a partial code snippet. Second, its output can be strictly aligned with the input in both the format and the content.","What are the differences between transcription tasks and synthesis tasks in programming, particularly in terms of input and output characteristics?","Transcription tasks and synthesis tasks in programming differ primarily in two key aspects. Firstly, in transcription tasks, the input program is formally complete, meaning it is fully specified. In contrast, program synthesis typically starts with no input program or just a function header, while code completion begins with a partial code snippet. Secondly, the output of a transcription task can be strictly aligned with the input in both format and content, ensuring a direct correspondence between the two.",7,2,7,2,"Question:

How do transcription tasks differ from synthesis tasks in programming, particularly regarding input and output characteristics?

Answer:

The transcription task differs from the synthesis task in two aspects. First, its input program is formally complete, whereas in program synthesis, the input program is None or a function header, and in code completion, it is a partial code snippet. Second, its output can be strictly aligned with the input in both the format and the content.","Question:

What are the differences between transcription tasks and synthesis tasks in programming, particularly in terms of input and output characteristics?

Answer:

Transcription tasks and synthesis tasks in programming differ primarily in two key aspects. Firstly, in transcription tasks, the input program is formally complete, meaning it is fully specified. In contrast, program synthesis typically starts with no input program or just a function header, while code completion begins with a partial code snippet. Secondly, the output of a transcription task can be strictly aligned with the input in both format and content, ensuring a direct correspondence between the two.",NO,True,512,True,True
254564474-s18,A Survey on Natural Language Processing for Programming,2022-12-12,Backbone Models,"Most of the functionality-oriented algorithms are model-agnostic and have little impact on the choice of a backbone model. In this section, we focus on the match between the structure-based representation (e.g., AST) and backbone models.

Recurrent Neural Network (RNN, Mikolov et al., 2010) and its variant LSTM (Hochreiter and Schmidhuber, 1997) are capable of processing variable-length inputs. Therefore, it is well-suited for representing NL description (Liu et al., 2016a;Weigelt et al., 2020) and PL token sequence (Wei et al., 2019). Meanwhile, it accepts the structurebased representation formatted as a sequence, e.g., the pre-order traversal of AST. However, such transforms are lossy in that the AST cannot be recovered. To this end, Hu et al. (2018) propose a structurebased traversal (SBT) approach, adding parentheses into the sequence to mark hierarchical relationships. Distinct from SBT that adapts data to a model, Shido et al. (2019) propose a Multi-way Tree-LSTM, which directly takes input as AST. It first encodes the children of a node with a standard LSTM, and subsequently integrates the results into the node with a Tree-LSTM.

Convolutional Neural Network (CNN, LeCun et al., 1989) extracts the features by scanning an input with a sliding window and applying stacked convolution and pooling operations on the window. Both two operations can be parallelized, making CNN more time-efficient than RNN. CNN in NLP4P usually takes input as execution traces (Gupta et al., 2020), input-output pairs (Bunel et al., 2018), and encodes them into an embedding as the output. Similar to Tree-LSTM,   CNN can also be adapted to the structure-based representation. For instance, Mou et al. (2016) propose a tree-based convolutional neural network (TBCNN), which encodes AST by a weight-base and positional features.

Transformer (Vaswani et al., 2017) has a similar interface to RNN. The difference lies in the following two aspects. First, it is more time-efficient by solely depending on the attention mechanism, rather than the recurrent unit. Second, it can better capture long-term dependencies, which is essential for processing PL since programs can be pretty long (Ahmad et al., 2020).

Despite these approaches, some studies explore the usage of feed-forward neural network (Iyer et al., 2016;Loyola et al., 2017), recursive neural network (Liang and Zhu, 2018), and graph neural network (Liu et al., 2020a). The architecture of the models, as well as RNN and CNN, can be flexibly adapted to customized data, e.g., the primitive AST. While for large-scale general data, the transformer is suggested due to its high capacity and easy access to pre-training.","How do RNN, CNN, and Transformer models differ in processing structure-based representations in NLP for programming?","Recurrent Neural Network (RNN) and its variant LSTM are capable of processing variable-length inputs, making them well-suited for representing NL description and PL token sequence. RNN accepts the structure-based representation formatted as a sequence, such as the pre-order traversal of AST, but such transforms are lossy as the AST cannot be recovered. To address this, a structure-based traversal (SBT) approach adds parentheses into the sequence to mark hierarchical relationships. Distinct from SBT, a Multi-way Tree-LSTM directly takes input as AST, encoding the children of a node with a standard LSTM and integrating the results into the node with a Tree-LSTM. Convolutional Neural Network (CNN) extracts features by scanning an input with a sliding window and applying stacked convolution and pooling operations, which can be parallelized, making CNN more time-efficient than RNN. CNN in NLP for programming usually takes input as execution traces or input-output pairs and encodes them into an embedding as the output. Similar to Tree-LSTM, CNN can also be adapted to the structure-based representation, such as a tree-based convolutional neural network (TBCNN) that encodes AST by weight-base and positional features. Transformer is more time-efficient by solely depending on the attention mechanism rather than the recurrent unit. It can better capture long-term dependencies, which is essential for processing programming languages since programs can be quite long. For large-scale general data, the transformer is suggested due to its high capacity and easy access to pre-training.","How do RNN, CNN, and Transformer models differ in processing structure-based representations in NLP for programming?","Recurrent Neural Networks (RNNs), including their variant Long Short-Term Memory (LSTM) networks, are adept at handling variable-length inputs, making them suitable for representing natural language (NL) descriptions and programming language (PL) token sequences. RNNs process structure-based representations formatted as sequences, such as the pre-order traversal of Abstract Syntax Trees (ASTs). However, this transformation can be lossy, as the original AST cannot be fully recovered. To mitigate this, a structure-based traversal (SBT) approach introduces parentheses into the sequence to denote hierarchical relationships. Alternatively, a Multi-way Tree-LSTM can directly process ASTs by encoding the children of a node with a standard LSTM and integrating the results into the node using a Tree-LSTM.

Convolutional Neural Networks (CNNs) extract features by scanning inputs with a sliding window and applying stacked convolution and pooling operations. This process can be parallelized, making CNNs more time-efficient than RNNs. In the context of NLP for programming, CNNs typically take inputs such as execution traces or input-output pairs and encode them into embeddings. Similar to Tree-LSTMs, CNNs can be adapted for structure-based representations, such as in a tree-based convolutional neural network (TBCNN), which encodes ASTs using weight-based and positional features.

Transformers, on the other hand, rely solely on the attention mechanism, eschewing recurrent units, which makes them more time-efficient and better at capturing long-term dependencies. This capability is crucial for processing programming languages, as programs can be quite lengthy. For large-scale general data, transformers are recommended due to their high capacity and the availability of pre-trained models.",7,4,4,4,"Question:

How do RNN, CNN, and Transformer models differ in processing structure-based representations in NLP for programming?

Answer:

Recurrent Neural Network (RNN) and its variant LSTM are capable of processing variable-length inputs, making them well-suited for representing NL description and PL token sequence. RNN accepts the structure-based representation formatted as a sequence, such as the pre-order traversal of AST, but such transforms are lossy as the AST cannot be recovered. To address this, a structure-based traversal (SBT) approach adds parentheses into the sequence to mark hierarchical relationships. Distinct from SBT, a Multi-way Tree-LSTM directly takes input as AST, encoding the children of a node with a standard LSTM and integrating the results into the node with a Tree-LSTM. Convolutional Neural Network (CNN) extracts features by scanning an input with a sliding window and applying stacked convolution and pooling operations, which can be parallelized, making CNN more time-efficient than RNN. CNN in NLP for programming usually takes input as execution traces or input-output pairs and encodes them into an embedding as the output. Similar to Tree-LSTM, CNN can also be adapted to the structure-based representation, such as a tree-based convolutional neural network (TBCNN) that encodes AST by weight-base and positional features. Transformer is more time-efficient by solely depending on the attention mechanism rather than the recurrent unit. It can better capture long-term dependencies, which is essential for processing programming languages since programs can be quite long. For large-scale general data, the transformer is suggested due to its high capacity and easy access to pre-training.","Question:

How do RNN, CNN, and Transformer models differ in processing structure-based representations in NLP for programming?

Answer:

Recurrent Neural Networks (RNNs), including their variant Long Short-Term Memory (LSTM) networks, are adept at handling variable-length inputs, making them suitable for representing natural language (NL) descriptions and programming language (PL) token sequences. RNNs process structure-based representations formatted as sequences, such as the pre-order traversal of Abstract Syntax Trees (ASTs). However, this transformation can be lossy, as the original AST cannot be fully recovered. To mitigate this, a structure-based traversal (SBT) approach introduces parentheses into the sequence to denote hierarchical relationships. Alternatively, a Multi-way Tree-LSTM can directly process ASTs by encoding the children of a node with a standard LSTM and integrating the results into the node using a Tree-LSTM.

Convolutional Neural Networks (CNNs) extract features by scanning inputs with a sliding window and applying stacked convolution and pooling operations. This process can be parallelized, making CNNs more time-efficient than RNNs. In the context of NLP for programming, CNNs typically take inputs such as execution traces or input-output pairs and encode them into embeddings. Similar to Tree-LSTMs, CNNs can be adapted for structure-based representations, such as in a tree-based convolutional neural network (TBCNN), which encodes ASTs using weight-based and positional features.

Transformers, on the other hand, rely solely on the attention mechanism, eschewing recurrent units, which makes them more time-efficient and better at capturing long-term dependencies. This capability is crucial for processing programming languages, as programs can be quite lengthy. For large-scale general data, transformers are recommended due to their high capacity and the availability of pre-trained models.",NO,True,1803,True,True
261100760-s2,GPTEval: A Survey on Assessments of ChatGPT and GPT-4,2023-08-24,Reasoning,"Logical Reasoning was tested by Bang et al. (2023). They found 56 out of 60 answers correct (with appropriate prompts) for deductive reasoning, i.e. applying general rules to specific situations or cases. This was stronger than other types of reasoning. 26 out of 30 were scored for abductive reasoning, i.e. forming plausible explanations or hypotheses, based on limited evidence or incomplete information. 33 out of 60 were scored for inductive reasoning, i.e., drawing generalized conclusions from examples or specific observations. Commonsense Reasoning. Bang et al. (2023) tested ChatGPT via three commonsense datasets, showing that 80 out of 90 of ChatGPT's predictions were correct. ChatGPT was able to give good explanations of the reasoning steps to support its answer. However, Qin et al. (2023); Laskar et al. (2023) showed that the commonsense reasoning accuracy of ChatGPT was lower than fine-tuned baselines by a large margin. Davis (2023) found significant flaws in common benchmarks for common sense, including the CommonsenseQA dataset used by Bang et al. (2023), which he explicitly addressed. Davis (2023) listed several examples of commonsense and particularly physical reasoning failures that had been found shortly after the release of ChatGPT, and pointed to others. However, there does not exist a thorough assessment of the GPT models' commonsense reasoning ability. More generally, Davis (2023) pointed out that ""many important aspects of commonsense reasoning and commonsense knowledge are not tested in any existing benchmark"". Bubeck et al. (2023) probed a small number of their own real-world physical reasoning tasks with GPT-4, finding that it had good knowledge. They concluded that the model was able to learn an understanding of the real-world environment through the medium of text.

Causal Reasoning. Bang et al. (2023) found that 24 out of 30 causes or effects could be correctly identified.  systematically evaluated event causality identification, causal discovery, and causal explanation generation. Compared to SOTA models, ChatGPT and GPT-4 yielded lower scores in causality identification. They outperformed baseline models on the causal discovery, although the compared models, e.g., BERT-(Devlin et al., 2019) and RoBERTa-base (Liu et al., 2019) were relatively weak. The generation of causal explanations yielded inconsistent findings in terms of AVG-BLEU and ROUGE-l metrics, while the human evaluation affirmed that both GPT models attained a level of accuracy comparable to that of human performance. Kıcıman et al. (2023) examined ChatGPT and GPT-4 on the causal discovery, counterfactual reasoning, and actual causality inferring, finding that they outperformed other LLMs and SOTA models largely on the first two tasks.

Psychological Reasoning is the ability of humans to reason about other's unobservable mental states (a.k.a Theory of Mind (ToM)). Kosinski (2023) and Moghaddam and Honey (2023) designed sets of False-Belief questions and quantified results suggested that both ChatGPT and GPT-4 had ToM ability, but that was still inferior to a human's. However, Marcus and Davis (2023) pointed out flaws in the Kosinski (2023) study because the test material was in the training data. Holterman and van Deemter (2023) further tested GPT-3 and GPT-4 on more ToM tasks summarized in Kahneman (2000). They acknowledged the potential problem of the test material being in the training data, so they substituted various nouns in the scenario. However, this is unlikely to be adequate to stop a neural model from generalising from those examples. Borji (2023) found that chatGPT failed on a variant of a classic 'Sally-Anne Test' (used to test children).  found that chatGPT answered correctly on a similar variant Sally-Anne, and they further tested on a range of more advanced ToM scenarios, with probing questions, e.g. to infer the counterfactual impact of actions on mental states. They found that GPT-4 had superior abilities and suggested that GPT-4 had a very advanced level of ToM.

Task-Oriented Reasoning. Qin et al. (2023) evaluated dialogue, logical reasoning, complex yes/no QA, symbolic reasoning (last letter concatenation and coin flip), date understanding-, and tracking shuffled objects-oriented logical reasoning. However, ChatGPT underperformed finetuned baselines on most of the tasks, excluding the logical reasoning tasks. To ascertain whether Chat-GPT relies on profound comprehension of truth and logic in their reasoning or merely exploits shallow memorized patterns, Wang et al. (2023a) proposed a dialectical evaluation task, finding that despite dis-playing high confidence, ChatGPT demonstrated an inability to hold its belief in the truth in a wide range of reasoning tasks, e.g., mathematics, firstorder logic, commonsense, and generic reasoning.

Natural Language Inference (NLI) aims to examine if a statement can be inferred, contradicted, or neutral, compared to another statement. Liu et al. (2023b) compared ChatGPT and GPT-4 with RoBERTa. However, both models encountered difficulties when dealing with novel and out-ofdistribution data. They yielded relatively modest performance on NLI that needed logical reasoning. Qin et al. (2023) also proved that the NLI ability of ChatGPT was lower than that of supervised models. Ambiguity is one of the difficulties of NLI. For example, whether ""John and Anna are not a couple"" contradicts ""John and Anna are married"" depends on whether ""married"" means ""both married"" or ""married to each other"". Given an ambiguous NLI premise,  asked ChatGPT and GPT-4 to either generate disambiguations of a premise with respect to the hypothesis or recognize disambiguation (i.e., deciding whether the disambiguation is an interpretation of an ambiguous premise). Their human evaluation showed that for the first task, GPT-4 achieved correctness at 32%, while, for the second task it was at the level of random guessing, suggesting that resolving tricky ambiguity remained challenging for ChatGPT.",How did ChatGPT and GPT-4 perform in resolving ambiguity in Natural Language Inference tasks?,"Given an ambiguous NLI premise, ChatGPT and GPT-4 were asked to either generate disambiguations of a premise with respect to the hypothesis or recognize disambiguation (i.e., deciding whether the disambiguation is an interpretation of an ambiguous premise). Their human evaluation showed that for the first task, GPT-4 achieved correctness at 32%, while, for the second task it was at the level of random guessing, suggesting that resolving tricky ambiguity remained challenging for ChatGPT.",How effective were ChatGPT and GPT-4 in handling ambiguity in Natural Language Inference tasks?,"In tasks involving ambiguous Natural Language Inference (NLI) premises, ChatGPT and GPT-4 were evaluated on their ability to either generate disambiguations of a premise in relation to a given hypothesis or to recognize whether a disambiguation accurately interpreted an ambiguous premise. Human evaluations indicated that GPT-4 achieved a correctness rate of 32% in the task of generating disambiguations. However, in the task of recognizing disambiguations, GPT-4's performance was akin to random guessing. This suggests that resolving complex ambiguities remains a significant challenge for ChatGPT.",7,4,7,4,"Question:

How did ChatGPT and GPT-4 perform in resolving ambiguity in Natural Language Inference tasks?

Answer:

Given an ambiguous NLI premise, ChatGPT and GPT-4 were asked to either generate disambiguations of a premise with respect to the hypothesis or recognize disambiguation (i.e., deciding whether the disambiguation is an interpretation of an ambiguous premise). Their human evaluation showed that for the first task, GPT-4 achieved correctness at 32%, while, for the second task it was at the level of random guessing, suggesting that resolving tricky ambiguity remained challenging for ChatGPT.","Question:

How effective were ChatGPT and GPT-4 in handling ambiguity in Natural Language Inference tasks?

Answer:

In tasks involving ambiguous Natural Language Inference (NLI) premises, ChatGPT and GPT-4 were evaluated on their ability to either generate disambiguations of a premise in relation to a given hypothesis or to recognize whether a disambiguation accurately interpreted an ambiguous premise. Human evaluations indicated that GPT-4 achieved a correctness rate of 32% in the task of generating disambiguations. However, in the task of recognizing disambiguations, GPT-4's performance was akin to random guessing. This suggests that resolving complex ambiguities remains a significant challenge for ChatGPT.",NO,True,602,True,True
16442276-s1,Content Models for Survey Generation: A Factoid-Based Evaluation,2015,Data,"Prior research in automatic survey generation has explored using text from different parts of scientific papers. Some of the recent work has treated survey generation as a direct extension of single paper summarization (Qazvinian and Radev, 2008) and used citing sentences to a set of relevant papers as the input for the summarizer (Mohammad et al., 2009;Qazvinian et al., 2013). However, in our prior work, we have observed that it's difficult to generate coherent and readable summaries using just citing sentences and have proposed the use of sentences from introductory texts of papers that cite a number of important papers on a topic (Jha et al., 2015). The use of full text allows for the use of discourse structure of these documents in framing coherent and readable surveys. Since the content models we explore are meant to be part of a larger system that should be able to generate coherent and readable survey articles, we use the introduction sentences for our experiments as well.

The corpus we used for extracting our experimental data was the ACL Anthology Network, a comprehensive bibliographic dataset that contains full text and citations for papers in most of the important venues in natural language processing . An oracle method is used for selecting the initial set of papers for each topic. For each topic, the bibliographies of at least three human-written surveys were extracted, and any papers that appeared in more than one survey were added to the target document set for the topic.

The text for summarization is extracted from introductory sections of papers that cite papers in the target document set. The intuition behind this is that the introductory sections of papers that cite these target document summarize the research in papers from the target document set as well as the relationships between these papers. Thus, these introductions can be thought of as mini-surveys for specific aspects of the topic; combining text from these introductory sections should allow us to generate good comprehensive survey articles for the topic 1 . For our experiments, we sort the citing papers based on the number of papers they cite Input sentence Factoids According to [1] , the corpus based supervised machine learning methods are the most successful approaches to WSD where contextual features have been used mainly to distinguish ambiguous words in these methods. supervised wsd, corpus based wsd Compared with supervised methods, unsupervised methods do not require tagged corpus, but the precision is usually lower than that of the supervised methods.

supervised wsd, unsupervised wsd Word sense disambiguation (WSD) has been a hot topic in natural language processing, which is to determine the sense of an ambiguous word in a specific context. definition of word sense disambiguation Improvement in the accuracy of identifying the correct word sense will result in better machine translation systems, information retrieval systems, etc.

wsd for machine translation, wsd for information retrieval The SENSEVAL evaluation framework ( Kilgarriff 1998 ) was a DARPA-style competition designed to bring some conformity to the field of WSD, although it has yet to achieve that aim completely. senseval Table 3: Sample input sentences from the topic of word sense disambiguation annotated with factoids.

in the target document set, pick the top 20 papers, and extract sentences from their introductions to form the input text for the summarizer. The seven topics used in our experiments and input size for each topic are shown in Table 2.

Once the input text for each topic has been extracted, we annotate the sentences in the input text with factoids for that topic. Some annotated sentences in the topic of word sense disambiguation are shown in Table 3. Given this new annotated data, we can compare how the factoids are distributed across different citing sentences (as annotated by Jha et al. (2013)) and introduction sentences that we have annotated. For this, we divide the factoids into five categories: definitions, venue, resources, methodology, and applications. The fractional distribution of factoids in these categories is shown in Table 4. We can see that the distribution of factoids relating to venues, methodology and applications is similar for the two datasets. However, factoids related to definitional sentences are almost completely missing in the citing sentences data. This lack of background information in citing sentences is one of the motivations for using introduction sentences for survey article generation as opposed to previous work.

The complete set of factoids as well as annotated sentences for all the topics is available for download at http: //clair.si.umich.edu/corpora/ Surveyor_CM_Data.tar.gz.",What motivates the use of introduction sentences over citing sentences for generating survey articles?,"Prior research in automatic survey generation has explored using text from different parts of scientific papers. Some of the recent work has treated survey generation as a direct extension of single paper summarization and used citing sentences to a set of relevant papers as the input for the summarizer. However, it has been observed that it's difficult to generate coherent and readable summaries using just citing sentences. The use of full text allows for the use of discourse structure of these documents in framing coherent and readable surveys. The intuition behind this is that the introductory sections of papers that cite these target documents summarize the research in papers from the target document set as well as the relationships between these papers. Thus, these introductions can be thought of as mini-surveys for specific aspects of the topic; combining text from these introductory sections should allow for the generation of good comprehensive survey articles for the topic. Factoids related to definitional sentences are almost completely missing in the citing sentences data. This lack of background information in citing sentences is one of the motivations for using introduction sentences for survey article generation as opposed to previous work.",Why are introduction sentences preferred over citing sentences for generating survey articles?,"In the field of automatic survey generation, researchers have experimented with using text from various sections of scientific papers. Recent approaches have extended single paper summarization techniques by using citing sentences from a set of relevant papers as input for the summarizer. However, it has been found that relying solely on citing sentences often results in summaries that lack coherence and readability. This is because citing sentences typically do not provide sufficient background information or definitional content. In contrast, using the full text, particularly the introductory sections of papers, leverages the discourse structure to create more coherent and readable surveys. The introductory sections of papers that cite the target documents effectively summarize the research and highlight the relationships between the cited papers. These introductions can be seen as mini-surveys focusing on specific aspects of the topic. By combining text from these introductory sections, it is possible to generate comprehensive and well-structured survey articles. This approach addresses the deficiency of background information in citing sentences, making introduction sentences a more effective choice for survey article generation.",7,4,7,2,"Question:

What motivates the use of introduction sentences over citing sentences for generating survey articles?

Answer:

Prior research in automatic survey generation has explored using text from different parts of scientific papers. Some of the recent work has treated survey generation as a direct extension of single paper summarization and used citing sentences to a set of relevant papers as the input for the summarizer. However, it has been observed that it's difficult to generate coherent and readable summaries using just citing sentences. The use of full text allows for the use of discourse structure of these documents in framing coherent and readable surveys. The intuition behind this is that the introductory sections of papers that cite these target documents summarize the research in papers from the target document set as well as the relationships between these papers. Thus, these introductions can be thought of as mini-surveys for specific aspects of the topic; combining text from these introductory sections should allow for the generation of good comprehensive survey articles for the topic. Factoids related to definitional sentences are almost completely missing in the citing sentences data. This lack of background information in citing sentences is one of the motivations for using introduction sentences for survey article generation as opposed to previous work.","Question:

Why are introduction sentences preferred over citing sentences for generating survey articles?

Answer:

In the field of automatic survey generation, researchers have experimented with using text from various sections of scientific papers. Recent approaches have extended single paper summarization techniques by using citing sentences from a set of relevant papers as input for the summarizer. However, it has been found that relying solely on citing sentences often results in summaries that lack coherence and readability. This is because citing sentences typically do not provide sufficient background information or definitional content. In contrast, using the full text, particularly the introductory sections of papers, leverages the discourse structure to create more coherent and readable surveys. The introductory sections of papers that cite the target documents effectively summarize the research and highlight the relationships between the cited papers. These introductions can be seen as mini-surveys focusing on specific aspects of the topic. By combining text from these introductory sections, it is possible to generate comprehensive and well-structured survey articles. This approach addresses the deficiency of background information in citing sentences, making introduction sentences a more effective choice for survey article generation.",NO,True,1253,True,True
225062337-s9,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,2020-10-23,Learning with Noisy Labels,"The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.

Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.

In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","What are the main methods for handling noise in distantly supervised NLP tasks, and how do they function?","The main methods for handling noise in distantly supervised NLP tasks are noise filtering and noise modeling. Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent (Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019). The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., 2019a,c; Chen et al., 2019; Wang et al., 2019; Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances.","What are the primary strategies for managing noise in distantly supervised NLP tasks, and how do they operate?","In distantly supervised NLP tasks, managing noise is crucial for improving model performance. The two primary strategies employed are noise filtering and noise modeling. 

Noise filtering involves identifying and removing instances from the training data that are likely to be incorrectly labeled. This can be achieved through various methods, such as setting a probability threshold (Jia et al., 2019), using a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or employing a reinforcement-based agent (Nooralahzadeh et al., 2019). Instead of completely removing instances, soft filtering techniques may re-weight instances based on their likelihood of being correctly labeled (Le and Titov, 2019) or use an attention measure (Hu et al., 2019).

On the other hand, noise modeling focuses on understanding the relationship between clean and noisy labels. A common approach is to use a confusion matrix to estimate this relationship (Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., 2019a,c; Chen et al., 2019; Wang et al., 2019; Hedderich et al., 2021b). In this approach, the classifier is not directly trained on the noisy data. Instead, a noise model is appended to adjust the noisy label distribution to align with the clean label distribution, effectively training the classifier on a ""cleaned"" version of the data. During testing, predictions can be shifted from the noisy to the clean distribution (Ye et al., 2019). Additionally, reinforcement agents can be used to relabel noisy instances (Chen et al., 2020a).

These strategies, through either filtering or modeling, aim to mitigate the impact of noise, thereby enhancing the accuracy and reliability of NLP models in distantly supervised settings.",7,4,7,4,"Question:

What are the main methods for handling noise in distantly supervised NLP tasks, and how do they function?

Answer:

The main methods for handling noise in distantly supervised NLP tasks are noise filtering and noise modeling. Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent (Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019). The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., 2019a,c; Chen et al., 2019; Wang et al., 2019; Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances.","Question:

What are the primary strategies for managing noise in distantly supervised NLP tasks, and how do they operate?

Answer:

In distantly supervised NLP tasks, managing noise is crucial for improving model performance. The two primary strategies employed are noise filtering and noise modeling. 

Noise filtering involves identifying and removing instances from the training data that are likely to be incorrectly labeled. This can be achieved through various methods, such as setting a probability threshold (Jia et al., 2019), using a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or employing a reinforcement-based agent (Nooralahzadeh et al., 2019). Instead of completely removing instances, soft filtering techniques may re-weight instances based on their likelihood of being correctly labeled (Le and Titov, 2019) or use an attention measure (Hu et al., 2019).

On the other hand, noise modeling focuses on understanding the relationship between clean and noisy labels. A common approach is to use a confusion matrix to estimate this relationship (Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., 2019a,c; Chen et al., 2019; Wang et al., 2019; Hedderich et al., 2021b). In this approach, the classifier is not directly trained on the noisy data. Instead, a noise model is appended to adjust the noisy label distribution to align with the clean label distribution, effectively training the classifier on a ""cleaned"" version of the data. During testing, predictions can be shifted from the noisy to the clean distribution (Ye et al., 2019). Additionally, reinforcement agents can be used to relabel noisy instances (Chen et al., 2020a).

These strategies, through either filtering or modeling, aim to mitigate the impact of noise, thereby enhancing the accuracy and reliability of NLP models in distantly supervised settings.",NO,True,1797,True,True
256231532-s1,Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks,2022-10-11,Social Influence Dialogue Systems,"""Social influence is a fact of everyday life"" (Gass, 2015). It is the change in thoughts, feelings, attitudes, or behaviors resulting from interaction with an individual or a group (Rashotte, 2007). Influence is measured by quantifiable proxies of the observed change, like the interest to indulge in physical exercise before or after the interaction with a system, or the final deal in a negotiation as opposed to one person taking it all. Social influence dialogue systems act interactively and influence their partners in decision-making and behavioral contexts (Zhang et al., 2020a;Lee et al., 2020). This calls for an active role by the system, distinguishing them from other well-studied scenarios, such as purely task-oriented, where systems passively assist their partners to complete tasks, and opendomain, that target social companionship. Key social influence tasks include persuasion , aiming to change users' attitudes or behaviors, and negotiation, aiming to change the users' perspective to achieve a common ground (Lewis et al., 2017). Conceptual overview: Figure 1 distinguishes between the kinds of conversational content in social influence interactions. The task-oriented content focuses on influencing for a domain-specific goal, like persuading for donation, bargaining with tradeoffs, or encouraging healthier habits. These interactions may also contain social content, such as small talk, empathy, or self-disclosure. The task-oriented content provides a context for social interactions. Depending on the task, social content is optional, but if present, can in turn build rapport and enhance user-system relationship for improved task outcomes (Liao et al., 2021). Connections with task-oriented and opendomain systems: Similar to a task-oriented or an open-domain scenario, social influence dialogue can also be seen as a sequential decision making process with the goal of maximizing the expected reward Gao et al., 2018). Our proposed category is not meant to be disjoint from these traditional categories. However, it still uniquely brings together the tasks that capture social influence, which is fundamentally absent from how we primarily define dialogue tasks in the community. Defining a new category that captures social influence dialogue would foster a dedicated effort towards this important aspect of real-world conversations.

Task-oriented scenarios focus on collaborative information exchange for a common goal of task completion. In social influence tasks, the goals of the system and the user can be different and even conflicting, leading to collaborative or noncollaborative interactions. Further, the goals can go beyond the current task (e.g. multiple therapy interactions, repeated negotiations), leading to social interactions for long-term relationships. If a scenario involves the system's goal to influence its partner, we consider it under social influence in this paper. For instance, He et al. (2018) studied buyerseller price negotiations. The task of the buyer is to negotiate for a reasonable price (arguably making it task-oriented), but achieving it requires social influence skills of engaging in trade-offs and building a rapport with the seller so as to reach an agreement. Measures of Success: The above discussion indicates that a comprehensive evaluation of social influence systems must draw from both task-oriented and open-domain dialogue research. Since there exist surveys that discuss the evaluation in these settings (Deriu et al., 2021;Li et al., 2021), we don't cover them here in detail. However, we define three essential axes for evaluation: 1) Linguistic Performance, or the system's linguistic sophistication based on automatic (e.g. perplexity, BLEU) and human (e.g. fluency, consistency, coherency) evaluation. 2) Influence Outcome, or the ability to influence defined by objective goals like the negotiated price or weight loss after therapy. 3) Partner Perception, or the subjective evaluation of the user, for instance, the user's satisfaction, likeness towards the system, and interest in interacting again. In a buyer-seller negotiation, if the seller hates the buyer in the end, no matter how favorable the deal is for the buyer, one might argue that this is still a failed negotiation for the buyer. Hence, we encourage future work to take all three dimensions into account collectively.",How do social influence dialogue systems differ from task-oriented and open-domain systems in their roles and goals?,"Social influence dialogue systems act interactively and influence their partners in decision-making and behavioral contexts (Zhang et al., 2020a; Lee et al., 2020). This calls for an active role by the system, distinguishing them from other well-studied scenarios, such as purely task-oriented, where systems passively assist their partners to complete tasks, and open-domain, that target social companionship. Key social influence tasks include persuasion, aiming to change users' attitudes or behaviors, and negotiation, aiming to change the users' perspective to achieve a common ground (Lewis et al., 2017). In social influence tasks, the goals of the system and the user can be different and even conflicting, leading to collaborative or non-collaborative interactions. Further, the goals can go beyond the current task (e.g., multiple therapy interactions, repeated negotiations), leading to social interactions for long-term relationships. If a scenario involves the system's goal to influence its partner, it is considered under social influence.",How do social influence dialogue systems differ from task-oriented and open-domain systems in their roles and goals?,"Social influence dialogue systems are designed to interactively influence their partners in decision-making and behavioral contexts (Zhang et al., 2020a; Lee et al., 2020). This active role sets them apart from other dialogue systems, such as task-oriented systems, which passively assist users in completing tasks, and open-domain systems, which focus on providing social companionship. Key tasks for social influence systems include persuasion, which aims to change users' attitudes or behaviors, and negotiation, which seeks to alter users' perspectives to reach a common ground (Lewis et al., 2017). In these social influence tasks, the goals of the system and the user can differ and even conflict, leading to either collaborative or non-collaborative interactions. Additionally, the objectives of social influence systems can extend beyond the immediate task, such as in scenarios involving multiple therapy sessions or repeated negotiations, fostering long-term social interactions. When a system's goal is to influence its partner, it falls under the category of social influence dialogue systems.",7,4,7,4,"Question:

How do social influence dialogue systems differ from task-oriented and open-domain systems in their roles and goals?

Answer:

Social influence dialogue systems act interactively and influence their partners in decision-making and behavioral contexts (Zhang et al., 2020a; Lee et al., 2020). This calls for an active role by the system, distinguishing them from other well-studied scenarios, such as purely task-oriented, where systems passively assist their partners to complete tasks, and open-domain, that target social companionship. Key social influence tasks include persuasion, aiming to change users' attitudes or behaviors, and negotiation, aiming to change the users' perspective to achieve a common ground (Lewis et al., 2017). In social influence tasks, the goals of the system and the user can be different and even conflicting, leading to collaborative or non-collaborative interactions. Further, the goals can go beyond the current task (e.g., multiple therapy interactions, repeated negotiations), leading to social interactions for long-term relationships. If a scenario involves the system's goal to influence its partner, it is considered under social influence.","Question:

How do social influence dialogue systems differ from task-oriented and open-domain systems in their roles and goals?

Answer:

Social influence dialogue systems are designed to interactively influence their partners in decision-making and behavioral contexts (Zhang et al., 2020a; Lee et al., 2020). This active role sets them apart from other dialogue systems, such as task-oriented systems, which passively assist users in completing tasks, and open-domain systems, which focus on providing social companionship. Key tasks for social influence systems include persuasion, which aims to change users' attitudes or behaviors, and negotiation, which seeks to alter users' perspectives to reach a common ground (Lewis et al., 2017). In these social influence tasks, the goals of the system and the user can differ and even conflict, leading to either collaborative or non-collaborative interactions. Additionally, the objectives of social influence systems can extend beyond the immediate task, such as in scenarios involving multiple therapy sessions or repeated negotiations, fostering long-term social interactions. When a system's goal is to influence its partner, it falls under the category of social influence dialogue systems.",NO,True,1105,True,True
233219920-s1,Relational World Knowledge Representation in Contextual Language Models: A Review,2021-04-12,Preliminaries,"We briefly review preliminaries and assumptions necessary for our survey.

Knowledge bases We use the term ""knowledge base"" (KB) to refer to a relational data structure comprising a set of entities E, relation types R, and triples (s, r, o) ∈ E × R × E, where s, o ∈ E are subject and object entities, respectively. 1 We consider two types of KBs under the umbrella of ""relational world knowledge."" Encyclopedic KBs store facts about typed, disambiguated entities; a well-known example is the Wikidata KB (Vrandečić and Krötzsch, 2014), which, like its sister project Wikipedia, is publicly accessible and collaboratively constructed. By contrast, in commonsense KBs, ""entities"" are typically represented by non-canonicalized free-text phrases. Examples include the publicly accessible, crowdsourced Con-ceptNet (Liu and Singh, 2004;Speer et al., 2017) and ATOMIC (Sap et al., 2019) KBs.

Language models Following the contemporary NLP literature, we use the term ""language model"" (LM) to refer to a deep neural network that is trained to learn contextual text representations. LMs generally come pretrained, with parameters pre-initialized for generic text representation via self-supervised training on large corpora, and may be used as-is after pretraining, or further finetuned with supervision on downstream task(s). This work considers LMs based on the Transformer architecture (Vaswani et al., 2017), examples of which include the encoder-only BERT family (Devlin et al., 2019;, the decoder-only GPT family (Brown et al., 2020), and the encoder-decoder T5  and BART  families.","What are the differences between encyclopedic and commonsense knowledge bases, and can you provide examples of each?","Encyclopedic knowledge bases store facts about typed, disambiguated entities. A well-known example is the Wikidata KB (Vrandečić and Krötzsch, 2014), which, like its sister project Wikipedia, is publicly accessible and collaboratively constructed. By contrast, in commonsense knowledge bases, ""entities"" are typically represented by non-canonicalized free-text phrases. Examples include the publicly accessible, crowdsourced ConceptNet (Liu and Singh, 2004; Speer et al., 2017) and ATOMIC (Sap et al., 2019) KBs.","What distinguishes encyclopedic knowledge bases from commonsense knowledge bases, and could you provide examples of each?","Encyclopedic knowledge bases are designed to store facts about typed, disambiguated entities. A prominent example of this is the Wikidata knowledge base (Vrandečić and Krötzsch, 2014), which, similar to its sister project Wikipedia, is both publicly accessible and collaboratively constructed. In contrast, commonsense knowledge bases represent ""entities"" using non-canonicalized free-text phrases. Examples of such knowledge bases include ConceptNet (Liu and Singh, 2004; Speer et al., 2017) and ATOMIC (Sap et al., 2019), both of which are publicly accessible and crowdsourced.",9,2,9,2,"Question:

What are the differences between encyclopedic and commonsense knowledge bases, and can you provide examples of each?

Answer:

Encyclopedic knowledge bases store facts about typed, disambiguated entities. A well-known example is the Wikidata KB (Vrandečić and Krötzsch, 2014), which, like its sister project Wikipedia, is publicly accessible and collaboratively constructed. By contrast, in commonsense knowledge bases, ""entities"" are typically represented by non-canonicalized free-text phrases. Examples include the publicly accessible, crowdsourced ConceptNet (Liu and Singh, 2004; Speer et al., 2017) and ATOMIC (Sap et al., 2019) KBs.","Question:

What distinguishes encyclopedic knowledge bases from commonsense knowledge bases, and could you provide examples of each?

Answer:

Encyclopedic knowledge bases are designed to store facts about typed, disambiguated entities. A prominent example of this is the Wikidata knowledge base (Vrandečić and Krötzsch, 2014), which, similar to its sister project Wikipedia, is both publicly accessible and collaboratively constructed. In contrast, commonsense knowledge bases represent ""entities"" using non-canonicalized free-text phrases. Examples of such knowledge bases include ConceptNet (Liu and Singh, 2004; Speer et al., 2017) and ATOMIC (Sap et al., 2019), both of which are publicly accessible and crowdsourced.",NO,True,579,True,True
233219920-s4,Relational World Knowledge Representation in Contextual Language Models: A Review,2021-04-12,Statement scoring,"Beyond probing, pretrained LM ""knowledge"" can be purposed toward downstream KB-level tasks in a zero-shot manner via statement scoring. Here, a pretrained LM is fed natural language statements corresponding to KB triples, and its token probabilities across each statement are pooled to yield statement scores. These scores are then treated as input to a downstream decision, mirroring the way that supervised LMs can be trained to output probabilities for triple-level prediction tasks ( § 5). We categorize statement scoring strategies as single-or dual-LM approaches. The single-LM approach pools the pretrained LM's token scores over a candidate set of sequences, then takes the highest-scoring sequence as the LM's ""prediction"" or choice (Tamborrino et al., 2020;Bouraoui et al., 2020;Brown et al., 2020). The dual-LM framework first uses one pretrained LM to generate useful context (e.g., clarification text) for the task, then feeds this context to another, possibly different pretrained LM to obtain a final score (Davison et al., 2019;Shwartz et al., 2020). Both categories have shown promise over comparable unsupervised (and, under some conditions, supervised) methods for tasks like multiple-choice QA (Tamborrino et al., 2020;Shwartz et al., 2020;Brown et al., 2020) and commonsense KB completion (Davison et al., 2019). However, LM scores have also shown to be sensitive to small perturbations in text , so this approach may be less effective on noisy or long-tail inputs.",How are statement scoring strategies categorized and what are the key characteristics of each approach?,"Statement scoring strategies are categorized as single-or dual-LM approaches. The single-LM approach pools the pretrained LM's token scores over a candidate set of sequences, then takes the highest-scoring sequence as the LM's ""prediction"" or choice (Tamborrino et al., 2020; Bouraoui et al., 2020; Brown et al., 2020). The dual-LM framework first uses one pretrained LM to generate useful context (e.g., clarification text) for the task, then feeds this context to another, possibly different pretrained LM to obtain a final score (Davison et al., 2019; Shwartz et al., 2020). Both categories have shown promise over comparable unsupervised (and, under some conditions, supervised) methods for tasks like multiple-choice QA (Tamborrino et al., 2020; Shwartz et al., 2020; Brown et al., 2020) and commonsense KB completion (Davison et al., 2019).","How are statement scoring strategies categorized, and what are the key characteristics of each approach?","Statement scoring strategies are primarily categorized into single-LM and dual-LM approaches. In the single-LM approach, a pretrained language model (LM) evaluates token scores across a set of candidate sequences and selects the sequence with the highest score as its ""prediction"" or choice (Tamborrino et al., 2020; Bouraoui et al., 2020; Brown et al., 2020). On the other hand, the dual-LM framework involves two steps: first, one pretrained LM generates useful context, such as clarification text, for the task at hand. This context is then fed into another, possibly different, pretrained LM to obtain a final score (Davison et al., 2019; Shwartz et al., 2020). Both approaches have demonstrated effectiveness over comparable unsupervised methods and, in some cases, even supervised methods for tasks like multiple-choice question answering (Tamborrino et al., 2020; Shwartz et al., 2020; Brown et al., 2020) and commonsense knowledge base completion (Davison et al., 2019).",7,4,7,4,"Question:

How are statement scoring strategies categorized and what are the key characteristics of each approach?

Answer:

Statement scoring strategies are categorized as single-or dual-LM approaches. The single-LM approach pools the pretrained LM's token scores over a candidate set of sequences, then takes the highest-scoring sequence as the LM's ""prediction"" or choice (Tamborrino et al., 2020; Bouraoui et al., 2020; Brown et al., 2020). The dual-LM framework first uses one pretrained LM to generate useful context (e.g., clarification text) for the task, then feeds this context to another, possibly different pretrained LM to obtain a final score (Davison et al., 2019; Shwartz et al., 2020). Both categories have shown promise over comparable unsupervised (and, under some conditions, supervised) methods for tasks like multiple-choice QA (Tamborrino et al., 2020; Shwartz et al., 2020; Brown et al., 2020) and commonsense KB completion (Davison et al., 2019).","Question:

How are statement scoring strategies categorized, and what are the key characteristics of each approach?

Answer:

Statement scoring strategies are primarily categorized into single-LM and dual-LM approaches. In the single-LM approach, a pretrained language model (LM) evaluates token scores across a set of candidate sequences and selects the sequence with the highest score as its ""prediction"" or choice (Tamborrino et al., 2020; Bouraoui et al., 2020; Brown et al., 2020). On the other hand, the dual-LM framework involves two steps: first, one pretrained LM generates useful context, such as clarification text, for the task at hand. This context is then fed into another, possibly different, pretrained LM to obtain a final score (Davison et al., 2019; Shwartz et al., 2020). Both approaches have demonstrated effectiveness over comparable unsupervised methods and, in some cases, even supervised methods for tasks like multiple-choice question answering (Tamborrino et al., 2020; Shwartz et al., 2020; Brown et al., 2020) and commonsense knowledge base completion (Davison et al., 2019).",NO,True,978,True,True
233219920-s8,Relational World Knowledge Representation in Contextual Language Models: A Review,2021-04-12,Linking with late fusion,"The next-strongest level of entity supervision is to train the LM to link entity-centric textual contexts to a KB's entity set E. Here, we cover late fusion approaches, which operate at the word level in terms of input to the LM and incorporate entities at the LM's output layer only, as exemplified in Figure 3c. The simplest representatives of this category train LMs to match individual tokens (Broscheit, 2019) or mentions  in a text corpus to an entity bank, without any external resources. The minimally ""entity-aware"" BERT proposed by Broscheit (2019), which adds a single classification layer on top of a pretrained BERT encoder, achieves competitive results with a state-of-the-art specialized entity linking architec- ture (Kolitsas et al., 2018).

Entity meta-information such as names and descriptions are viable external resources for LMpowered entity linking (Botha et al., 2020). For example, in zero-shot entity linking (Logeswaran et al., 2019), textual mentions must be linked to entities unseen during training using only entity descriptions as additional data. Here, competitive solutions train separate BERT models to select and rank candidate entities by encoding their descriptions (Logeswaran et al., 2019;. More recently, encoder-decoder LMs have been trained to retrieve entities by generating their unique names (De Cao et al., 2021), which has the advantage of scaling with the LM's vocabulary size (usually tens of thousands) instead of the KB entity set size (potentially tens of millions). De Cao et al.

(2021) achieve results competitive to discriminative approaches on entity linking and QA, suggesting the potential of generative entity-aware LMs.

External entity embeddings pretrained by a separate model have been used as strong sources of inductive bias for LMs. For example, several variants of BERT further pretrain the base model by linearly fusing external entity embeddings with contextual word representations at the output of the BERT encoder (Zhang et al., 2019;He et al., 2020). BERT has also been fine-tuned to match its output token representations to external entity embeddings for the task of end-to-end entity linking . Such approaches rely heavily on the quality of the externally-learned embeddings, which is both a strength and a drawback: Such embeddings may contain useful implicit structural information about the KB, but on the other hand may propagate errors into the LM (Shen et al., 2020).","How are external entity embeddings integrated into BERT models for entity linking, and what are their strengths and drawbacks?","External entity embeddings pretrained by a separate model have been used as strong sources of inductive bias for LMs. Several variants of BERT further pretrain the base model by linearly fusing external entity embeddings with contextual word representations at the output of the BERT encoder (Zhang et al., 2019; He et al., 2020). BERT has also been fine-tuned to match its output token representations to external entity embeddings for the task of end-to-end entity linking. Such approaches rely heavily on the quality of the externally-learned embeddings, which is both a strength and a drawback. These embeddings may contain useful implicit structural information about the KB, but on the other hand may propagate errors into the LM (Shen et al., 2020).","How are external entity embeddings integrated into BERT models for entity linking, and what are the strengths and drawbacks of this approach?","External entity embeddings, which are pretrained by separate models, serve as powerful sources of inductive bias for language models. In several BERT variants, the base model undergoes further pretraining by linearly fusing these external entity embeddings with the contextual word representations produced by the BERT encoder (Zhang et al., 2019; He et al., 2020). Additionally, BERT can be fine-tuned to align its output token representations with external entity embeddings specifically for the task of end-to-end entity linking. The effectiveness of these approaches largely depends on the quality of the externally-learned embeddings, which presents both a strength and a drawback. While these embeddings can encapsulate valuable implicit structural information about the knowledge base, they also have the potential to introduce errors into the language model (Shen et al., 2020).",7,3,7,4,"Question:

How are external entity embeddings integrated into BERT models for entity linking, and what are their strengths and drawbacks?

Answer:

External entity embeddings pretrained by a separate model have been used as strong sources of inductive bias for LMs. Several variants of BERT further pretrain the base model by linearly fusing external entity embeddings with contextual word representations at the output of the BERT encoder (Zhang et al., 2019; He et al., 2020). BERT has also been fine-tuned to match its output token representations to external entity embeddings for the task of end-to-end entity linking. Such approaches rely heavily on the quality of the externally-learned embeddings, which is both a strength and a drawback. These embeddings may contain useful implicit structural information about the KB, but on the other hand may propagate errors into the LM (Shen et al., 2020).","Question:

How are external entity embeddings integrated into BERT models for entity linking, and what are the strengths and drawbacks of this approach?

Answer:

External entity embeddings, which are pretrained by separate models, serve as powerful sources of inductive bias for language models. In several BERT variants, the base model undergoes further pretraining by linearly fusing these external entity embeddings with the contextual word representations produced by the BERT encoder (Zhang et al., 2019; He et al., 2020). Additionally, BERT can be fine-tuned to align its output token representations with external entity embeddings specifically for the task of end-to-end entity linking. The effectiveness of these approaches largely depends on the quality of the externally-learned embeddings, which presents both a strength and a drawback. While these embeddings can encapsulate valuable implicit structural information about the knowledge base, they also have the potential to introduce errors into the language model (Shen et al., 2020).",NO,True,886,True,True
233219920-s13,Relational World Knowledge Representation in Contextual Language Models: A Review,2021-04-12,Linearizing KB triples,"The main advantage of templating is that it converts symbolic triples into sequences, which can be straightforwardly input to LMs. However, handcrafting templates is a manual process, and distant supervision can be noisy. To maintain the advantage of templates while avoiding the drawbacks, triples can alternatively be fed to an LM by linearizing them-that is, flattening the subject, relation, and object into an input sequence (Figure 4). With linearization, relation-level supervision becomes as simple as feeding the linearized sequences to the LM and training again with task-specific losses (Yao et al., 2019;Kim et al., 2020;Ribeiro et al., 2020;Wang et al., 2021a) or injecting the sequences into the pretraining corpus . A notable recent example of the former approach (Agarwal et al., 2021) trains T5 on linearized Wikidata triples in order to generate fully natural language versions of those triples. These verbalized triples are used as retrieval ""documents"" for improved LM-based QA over traditional document corpora; note, however, that they can also be used as LM training data for other downstream tasks in place of handcrafted templates ( § 5.1).",How can linearization of KB triples be used to train language models for improved performance?,"The main advantage of templating is that it converts symbolic triples into sequences, which can be straightforwardly input to LMs. However, handcrafting templates is a manual process, and distant supervision can be noisy. To maintain the advantage of templates while avoiding the drawbacks, triples can alternatively be fed to an LM by linearizing them—that is, flattening the subject, relation, and object into an input sequence. With linearization, relation-level supervision becomes as simple as feeding the linearized sequences to the LM and training again with task-specific losses (Yao et al., 2019; Kim et al., 2020; Ribeiro et al., 2020; Wang et al., 2021a) or injecting the sequences into the pretraining corpus. A notable recent example of the former approach (Agarwal et al., 2021) trains T5 on linearized Wikidata triples in order to generate fully natural language versions of those triples. These verbalized triples are used as retrieval ""documents"" for improved LM-based QA over traditional document corpora; note, however, that they can also be used as LM training data for other downstream tasks in place of handcrafted templates.",How does linearizing knowledge base triples enhance the training of language models?,"Linearizing knowledge base (KB) triples offers a significant advantage by transforming symbolic triples into sequences that can be easily input into language models (LMs). This approach maintains the benefits of templating, which converts triples into sequences, but avoids the manual effort of crafting templates and the potential noise from distant supervision. By flattening the subject, relation, and object into a single input sequence, linearization simplifies relation-level supervision. This is achieved by feeding the linearized sequences to the LM and retraining with task-specific losses (Yao et al., 2019; Kim et al., 2020; Ribeiro et al., 2020; Wang et al., 2021a) or by incorporating these sequences into the pretraining corpus. A notable example of this method is the work by Agarwal et al. (2021), which involves training the T5 model on linearized Wikidata triples to generate natural language versions of these triples. These verbalized triples serve as retrieval ""documents"" for improved LM-based question answering over traditional document corpora. Additionally, they can replace handcrafted templates as LM training data for other downstream tasks.",7,4,7,4,"Question:

How can linearization of KB triples be used to train language models for improved performance?

Answer:

The main advantage of templating is that it converts symbolic triples into sequences, which can be straightforwardly input to LMs. However, handcrafting templates is a manual process, and distant supervision can be noisy. To maintain the advantage of templates while avoiding the drawbacks, triples can alternatively be fed to an LM by linearizing them—that is, flattening the subject, relation, and object into an input sequence. With linearization, relation-level supervision becomes as simple as feeding the linearized sequences to the LM and training again with task-specific losses (Yao et al., 2019; Kim et al., 2020; Ribeiro et al., 2020; Wang et al., 2021a) or injecting the sequences into the pretraining corpus. A notable recent example of the former approach (Agarwal et al., 2021) trains T5 on linearized Wikidata triples in order to generate fully natural language versions of those triples. These verbalized triples are used as retrieval ""documents"" for improved LM-based QA over traditional document corpora; note, however, that they can also be used as LM training data for other downstream tasks in place of handcrafted templates.","Question:

How does linearizing knowledge base triples enhance the training of language models?

Answer:

Linearizing knowledge base (KB) triples offers a significant advantage by transforming symbolic triples into sequences that can be easily input into language models (LMs). This approach maintains the benefits of templating, which converts triples into sequences, but avoids the manual effort of crafting templates and the potential noise from distant supervision. By flattening the subject, relation, and object into a single input sequence, linearization simplifies relation-level supervision. This is achieved by feeding the linearized sequences to the LM and retraining with task-specific losses (Yao et al., 2019; Kim et al., 2020; Ribeiro et al., 2020; Wang et al., 2021a) or by incorporating these sequences into the pretraining corpus. A notable example of this method is the work by Agarwal et al. (2021), which involves training the T5 model on linearized Wikidata triples to generate natural language versions of these triples. These verbalized triples serve as retrieval ""documents"" for improved LM-based question answering over traditional document corpora. Additionally, they can replace handcrafted templates as LM training data for other downstream tasks.",NO,True,1170,True,True
231709697-s2,English Machine Reading Comprehension Datasets: A Survey,2021-01-25,Answer Type,"Cloze The question is formulated as a sentence with a missing word or phrase which should be inserted into the sentence or should complete the sentence. The answer candidates may be included as in (1) from ReciteQA (Yagcioglu et al., 2018), and may not, as in (2)  We distinguish cloze multiple choice datasets from other multiple choice datasets. The difference is the form of question: in the cloze datasets, the answer is a missing part of the question context and, combined together, they form a grammatically correct sentence, whereas for other multiple choice datasets, the question has no missing words.

Boolean A Yes/No answer is expected, e.g. (4) from the BoolQ dataset (Clark et al., 2019). Some datasets which we include here have a third ""Cannot be answered"" or ""Maybe"" option, e.g. (5) from PubMedQuestions (Jin et al., 2019).

(4) P: The series is filmed partially in Prince Edward Island as well as locations in ... Q: Is anne with an e filmed on pei? A: Yes (5) P: ... Young adults whose families were abstainers in 2000 drank substantially less across quintiles in 2010 than offspring of nonabstaining families. The difference, however, was not statistically significant between quintiles of the conditional distribution. Actual drinking levels in drinking families were not at all or weakly associated with drinking in offspring. ... Q: Does the familial transmission of drinking patterns persist into young adulthood? A: Maybe

Extractive or Span Extractive The answer is a substring of the passage. In other words, the task is to determine the answer character start and end index in the original passage, as shown in (6)  Generative or Free Form Answer The answer must be generated based on information presented in the passage. Although the answer might be in the text, as illustrated in (7) from Narra-tiveQA (Kočiský et al., 2018), no passage index connections are provided.

(7) P: ...Mark decides to broadcast his final message as himself. They finally drive up to the crowd of protesting students, .... The police step in and arrest Mark and Nora.... Q: What are the students doing when Mark and Nora drive up? A: Protesting.",How do cloze multiple choice datasets differ from other multiple choice datasets in machine reading comprehension?,"Cloze multiple choice datasets differ from other multiple choice datasets in the form of the question. In cloze datasets, the answer is a missing part of the question context and, combined together, they form a grammatically correct sentence. For other multiple choice datasets, the question has no missing words.",How are cloze multiple choice datasets distinct from other multiple choice datasets in the context of machine reading comprehension?,"Cloze multiple choice datasets are distinct from other multiple choice datasets primarily in the structure of the questions. In cloze datasets, the question includes a missing part within the context, and the correct answer fills this gap to create a grammatically correct sentence. In contrast, other multiple choice datasets present questions that do not contain missing words.",7,2,7,2,"Question:

How do cloze multiple choice datasets differ from other multiple choice datasets in machine reading comprehension?

Answer:

Cloze multiple choice datasets differ from other multiple choice datasets in the form of the question. In cloze datasets, the answer is a missing part of the question context and, combined together, they form a grammatically correct sentence. For other multiple choice datasets, the question has no missing words.","Question:

How are cloze multiple choice datasets distinct from other multiple choice datasets in the context of machine reading comprehension?

Answer:

Cloze multiple choice datasets are distinct from other multiple choice datasets primarily in the structure of the questions. In cloze datasets, the question includes a missing part within the context, and the correct answer fills this gap to create a grammatically correct sentence. In contrast, other multiple choice datasets present questions that do not contain missing words.",NO,True,379,False,False
231709697-s10,English Machine Reading Comprehension Datasets: A Survey,2021-01-25,Evaluation Metrics,"Accuracy is defined as the ratio of correctly answered questions out of all questions. For those datasets where the answer should be found or generated (extractive or generative tasks) accuracy is the same as Exact Match (EM), implying the system answer is exactly the same as the gold answer.

In contrast with selective and boolean tasks, extractive or generative tasks can have ambiguous, incomplete, or redundant answers. In order to assign credit when the system answer does not exactly match the gold answer, Precision and Recall, and their harmonic mean, F1, can be calculated over words or characters. Accuracy is used for all boolean, multiple choice, and cloze datasets. 19 For extractive and generative tasks it is common to report EM (accuracy) and F1. For cloze datasets, the metrics depends on the form of answer. If there are options available, accuracy can be calculated. If words have to be generated, the F1 measure can also be applied.

One can view the MRC task from the perspective of Information Retrieval, providing a ranked list of answers instead of one definitive answer. In this case, a Mean Reciprocal Rank (Craswell, 2009) (MRR) and Mean Average Precision (MAP) can be used, as well as the accuracy of the top hit (Hits@1) (single answer) over all possible answers (all entities). 20 All metrics mentioned above work well for welldefined answers but might not reflect performance for generative datasets as there could be several alternative ways to answer the same question. Some datasets provide more than one gold answer. A number of different automatic metrics used in language generation evaluation are also used: Bilingual Evaluation Understudy Score (BLEU) (Papineni et al., 2002), Recall Oriented Understudy for Gisting Evaluation (ROUGE-L) (Lin, 2004), and Metric for Evaluation of Translation with Explicit 19 Except MultiRC as there are multiple correct answers and all of them should be found, and CliCR and ReCoRD which use exact match and F1. This is because even though the task is cloze, the answer should be generated (in case of CliCR) or extracted (ReCoRD). 20 MRR and MAP are used only by (Yang et al., 2015) in the WikiQA dataset, as well as precision, recall and F1. (Miller et al., 2016) in the WikiMovies datasets used the accuracy of the top hit (Hits@1).

ORdering (METEOR) (Lavie and Agarwal, 2007). MSMarco, NarrativeQA, and TweetQA are generative datasets which use these metrics. Choi et al. (2018) introduced the human equivalence score (HEQ). It measures the percentage of examples where the system F1 matches or exceeds human F1, implying a system's output is as good as that of an average human. There are two variants: HEQ-Q based on questions and HEQ-D based on dialogues.","How do evaluation metrics differ for extractive, generative, and cloze machine reading comprehension tasks?","Accuracy is defined as the ratio of correctly answered questions out of all questions. For extractive and generative tasks, accuracy is the same as Exact Match (EM), implying the system answer is exactly the same as the gold answer. In contrast with selective and boolean tasks, extractive or generative tasks can have ambiguous, incomplete, or redundant answers. In order to assign credit when the system answer does not exactly match the gold answer, Precision and Recall, and their harmonic mean, F1, can be calculated over words or characters. Accuracy is used for all boolean, multiple choice, and cloze datasets. For extractive and generative tasks, it is common to report EM (accuracy) and F1. For cloze datasets, the metrics depend on the form of the answer. If there are options available, accuracy can be calculated. If words have to be generated, the F1 measure can also be applied. One can view the MRC task from the perspective of Information Retrieval, providing a ranked list of answers instead of one definitive answer. In this case, a Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) can be used, as well as the accuracy of the top hit (Hits@1) over all possible answers. All metrics mentioned above work well for well-defined answers but might not reflect performance for generative datasets as there could be several alternative ways to answer the same question. Some datasets provide more than one gold answer. A number of different automatic metrics used in language generation evaluation are also used: Bilingual Evaluation Understudy Score (BLEU), Recall Oriented Understudy for Gisting Evaluation (ROUGE-L), and Metric for Evaluation of Translation with Explicit ORdering (METEOR). MSMarco, NarrativeQA, and TweetQA are generative datasets which use these metrics. Choi et al. (2018) introduced the human equivalence score (HEQ). It measures the percentage of examples where the system F1 matches or exceeds human F1, implying a system's output is as good as that of an average human. There are two variants: HEQ-Q based on questions and HEQ-D based on dialogues.","How do evaluation metrics vary for extractive, generative, and cloze machine reading comprehension tasks?","In machine reading comprehension tasks, evaluation metrics differ based on the nature of the task—extractive, generative, or cloze. For extractive and generative tasks, accuracy is often equated with Exact Match (EM), which means the system's answer must be exactly the same as the gold standard answer. However, these tasks can present challenges such as ambiguous, incomplete, or redundant answers. To address this, Precision, Recall, and their harmonic mean, F1, are calculated over words or characters to give partial credit when the system's answer doesn't exactly match the gold answer.

For cloze tasks, the evaluation metrics depend on the format of the answer. If the task involves selecting from given options, accuracy can be used. If the task requires generating words, the F1 measure is applicable. In all boolean, multiple-choice, and cloze datasets, accuracy is a common metric. For extractive and generative tasks, both EM (accuracy) and F1 are typically reported.

Viewing the machine reading comprehension task from an Information Retrieval perspective, one might provide a ranked list of answers rather than a single definitive answer. In such cases, metrics like Mean Reciprocal Rank (MRR), Mean Average Precision (MAP), and the accuracy of the top hit (Hits@1) are used. These metrics are effective for well-defined answers but may not fully capture performance for generative datasets, where multiple valid answers can exist.

Some datasets offer more than one gold answer, and various automatic metrics from language generation evaluation are employed, such as Bilingual Evaluation Understudy Score (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE-L), and Metric for Evaluation of Translation with Explicit ORdering (METEOR). Generative datasets like MSMarco, NarrativeQA, and TweetQA utilize these metrics.

Additionally, Choi et al. (2018) introduced the human equivalence score (HEQ), which measures the percentage of examples where the system's F1 score matches or exceeds that of a human, indicating the system's output is as good as an average human's. HEQ has two variants: HEQ-Q, based on questions, and HEQ-D, based on dialogues.",7,4,8,4,"Question:

How do evaluation metrics differ for extractive, generative, and cloze machine reading comprehension tasks?

Answer:

Accuracy is defined as the ratio of correctly answered questions out of all questions. For extractive and generative tasks, accuracy is the same as Exact Match (EM), implying the system answer is exactly the same as the gold answer. In contrast with selective and boolean tasks, extractive or generative tasks can have ambiguous, incomplete, or redundant answers. In order to assign credit when the system answer does not exactly match the gold answer, Precision and Recall, and their harmonic mean, F1, can be calculated over words or characters. Accuracy is used for all boolean, multiple choice, and cloze datasets. For extractive and generative tasks, it is common to report EM (accuracy) and F1. For cloze datasets, the metrics depend on the form of the answer. If there are options available, accuracy can be calculated. If words have to be generated, the F1 measure can also be applied. One can view the MRC task from the perspective of Information Retrieval, providing a ranked list of answers instead of one definitive answer. In this case, a Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) can be used, as well as the accuracy of the top hit (Hits@1) over all possible answers. All metrics mentioned above work well for well-defined answers but might not reflect performance for generative datasets as there could be several alternative ways to answer the same question. Some datasets provide more than one gold answer. A number of different automatic metrics used in language generation evaluation are also used: Bilingual Evaluation Understudy Score (BLEU), Recall Oriented Understudy for Gisting Evaluation (ROUGE-L), and Metric for Evaluation of Translation with Explicit ORdering (METEOR). MSMarco, NarrativeQA, and TweetQA are generative datasets which use these metrics. Choi et al. (2018) introduced the human equivalence score (HEQ). It measures the percentage of examples where the system F1 matches or exceeds human F1, implying a system's output is as good as that of an average human. There are two variants: HEQ-Q based on questions and HEQ-D based on dialogues.","Question:

How do evaluation metrics vary for extractive, generative, and cloze machine reading comprehension tasks?

Answer:

In machine reading comprehension tasks, evaluation metrics differ based on the nature of the task—extractive, generative, or cloze. For extractive and generative tasks, accuracy is often equated with Exact Match (EM), which means the system's answer must be exactly the same as the gold standard answer. However, these tasks can present challenges such as ambiguous, incomplete, or redundant answers. To address this, Precision, Recall, and their harmonic mean, F1, are calculated over words or characters to give partial credit when the system's answer doesn't exactly match the gold answer.

For cloze tasks, the evaluation metrics depend on the format of the answer. If the task involves selecting from given options, accuracy can be used. If the task requires generating words, the F1 measure is applicable. In all boolean, multiple-choice, and cloze datasets, accuracy is a common metric. For extractive and generative tasks, both EM (accuracy) and F1 are typically reported.

Viewing the machine reading comprehension task from an Information Retrieval perspective, one might provide a ranked list of answers rather than a single definitive answer. In such cases, metrics like Mean Reciprocal Rank (MRR), Mean Average Precision (MAP), and the accuracy of the top hit (Hits@1) are used. These metrics are effective for well-defined answers but may not fully capture performance for generative datasets, where multiple valid answers can exist.

Some datasets offer more than one gold answer, and various automatic metrics from language generation evaluation are employed, such as Bilingual Evaluation Understudy Score (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE-L), and Metric for Evaluation of Translation with Explicit ORdering (METEOR). Generative datasets like MSMarco, NarrativeQA, and TweetQA utilize these metrics.

Additionally, Choi et al. (2018) introduced the human equivalence score (HEQ), which measures the percentage of examples where the system's F1 score matches or exceeds that of a human, indicating the system's output is as good as an average human's. HEQ has two variants: HEQ-Q, based on questions, and HEQ-D, based on dialogues.",NO,True,2177,True,True
222133962-s18,A Survey of Unsupervised Dependency Parsing,2020-10-04,Lexicalization,"In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a;He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010 use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007, Pate andSpitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance,  use neural networks to predict dependency probabilities that are automatically smoothed.

In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similarities between words. Recently proposed contextual word embeddings  Table 2: Reported directed dependency accuracies on section 23 of the WSJ corpus, evaluated on sentences of length ≤ 10 and all lengths. *: without gold POS tags. †: with more training data in addition to WSJ. (Devlin et al., 2019) are even more informative, capturing contextual information. However, word embeddings have not been widely used in unsupervised dependency parsing. One concern is that word embeddings are too informative and may make unsupervised models more prone to overfitting. One exception is He et al. (2018), who propose to use invertible neural projections to map word embeddings into a latent space that is more amenable to unsupervised parsing.",What are the challenges and potential solutions for incorporating lexical information in unsupervised dependency parsing?,"In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. A major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similarities between words. However, word embeddings have not been widely used in unsupervised dependency parsing. One concern is that word embeddings are too informative and may make unsupervised models more prone to overfitting. One exception is He et al. (2018), who propose to use invertible neural projections to map word embeddings into a latent space that is more amenable to unsupervised parsing.","What challenges arise when incorporating lexical information into unsupervised dependency parsing, and what are the potential solutions?","In unsupervised dependency parsing, parsers are typically unlexicalized, meaning that they use POS tags as tokens in sentences. However, words sharing the same POS tag can exhibit different syntactic behaviors, suggesting that incorporating lexical information could be beneficial. A significant challenge with full lexicalization is the resulting increase in grammar size, which demands more data for effective learning. To address data scarcity, smoothing techniques can be employed. Additionally, lexicalized approaches might leverage pretrained word embeddings, which capture syntactic and semantic similarities between words. Despite their potential, word embeddings are not widely used in unsupervised dependency parsing due to concerns that they might be too informative, leading to overfitting. An exception is the work by He et al. (2018), which proposes using invertible neural projections to map word embeddings into a latent space more suitable for unsupervised parsing.",7,4,7,4,"Question:

What are the challenges and potential solutions for incorporating lexical information in unsupervised dependency parsing?

Answer:

In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. A major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similarities between words. However, word embeddings have not been widely used in unsupervised dependency parsing. One concern is that word embeddings are too informative and may make unsupervised models more prone to overfitting. One exception is He et al. (2018), who propose to use invertible neural projections to map word embeddings into a latent space that is more amenable to unsupervised parsing.","Question:

What challenges arise when incorporating lexical information into unsupervised dependency parsing, and what are the potential solutions?

Answer:

In unsupervised dependency parsing, parsers are typically unlexicalized, meaning that they use POS tags as tokens in sentences. However, words sharing the same POS tag can exhibit different syntactic behaviors, suggesting that incorporating lexical information could be beneficial. A significant challenge with full lexicalization is the resulting increase in grammar size, which demands more data for effective learning. To address data scarcity, smoothing techniques can be employed. Additionally, lexicalized approaches might leverage pretrained word embeddings, which capture syntactic and semantic similarities between words. Despite their potential, word embeddings are not widely used in unsupervised dependency parsing due to concerns that they might be too informative, leading to overfitting. An exception is the work by He et al. (2018), which proposes using invertible neural projections to map word embeddings into a latent space more suitable for unsupervised parsing.",NO,True,982,True,True
222133962-s20,A Survey of Unsupervised Dependency Parsing,2020-10-04,Unsupervised Multilingual Parsing,"To tackle the lack of supervision in unsupervised dependency parsing, some previous work considers learning models of multiple languages simultaneously Liu et al., 2013;Jiang et al., 2019;Han et al., 2019b). Ideally, these models can learn from each other by identifying shared syntactic behaviors of different languages, especially those in the same language family. For example,  propose to utilize the similarity of different languages defined by a phylogenetic tree and learn several dependency parsers jointly. Han et al. (2019b) propose to learn a unified multilingual parser with language embeddings as input. Jiang et al. (2019) propose to guide the learning process of unsupervised dependency parser from the knowledge of another language by using three types of regularization to encourage similarity between model parameters, dependency edge scores, and parse trees respectively.",How do models leverage the similarity of different languages to improve unsupervised multilingual dependency parsing?,"To tackle the lack of supervision in unsupervised dependency parsing, some previous work considers learning models of multiple languages simultaneously (Liu et al., 2013; Jiang et al., 2019; Han et al., 2019b). Ideally, these models can learn from each other by identifying shared syntactic behaviors of different languages, especially those in the same language family. For example, some approaches propose to utilize the similarity of different languages defined by a phylogenetic tree and learn several dependency parsers jointly. Han et al. (2019b) propose to learn a unified multilingual parser with language embeddings as input. Jiang et al. (2019) propose to guide the learning process of an unsupervised dependency parser from the knowledge of another language by using three types of regularization to encourage similarity between model parameters, dependency edge scores, and parse trees respectively.",How can models utilize the similarities between different languages to enhance unsupervised multilingual dependency parsing?,"In the realm of unsupervised dependency parsing, where supervision is limited, leveraging the similarities between languages can significantly enhance model performance. Previous research has explored the simultaneous learning of models across multiple languages (Liu et al., 2013; Jiang et al., 2019; Han et al., 2019b). The core idea is that models can benefit from each other by identifying and learning shared syntactic behaviors, particularly among languages within the same family. For instance, some approaches suggest using a phylogenetic tree to define language similarities and jointly train several dependency parsers. Han et al. (2019b) introduced a method to develop a unified multilingual parser by incorporating language embeddings as input. Meanwhile, Jiang et al. (2019) proposed guiding the learning process of an unsupervised dependency parser using knowledge from another language. This is achieved through three types of regularization that promote similarity in model parameters, dependency edge scores, and parse trees, respectively.",7,4,8,7,"Question:

How do models leverage the similarity of different languages to improve unsupervised multilingual dependency parsing?

Answer:

To tackle the lack of supervision in unsupervised dependency parsing, some previous work considers learning models of multiple languages simultaneously (Liu et al., 2013; Jiang et al., 2019; Han et al., 2019b). Ideally, these models can learn from each other by identifying shared syntactic behaviors of different languages, especially those in the same language family. For example, some approaches propose to utilize the similarity of different languages defined by a phylogenetic tree and learn several dependency parsers jointly. Han et al. (2019b) propose to learn a unified multilingual parser with language embeddings as input. Jiang et al. (2019) propose to guide the learning process of an unsupervised dependency parser from the knowledge of another language by using three types of regularization to encourage similarity between model parameters, dependency edge scores, and parse trees respectively.","Question:

How can models utilize the similarities between different languages to enhance unsupervised multilingual dependency parsing?

Answer:

In the realm of unsupervised dependency parsing, where supervision is limited, leveraging the similarities between languages can significantly enhance model performance. Previous research has explored the simultaneous learning of models across multiple languages (Liu et al., 2013; Jiang et al., 2019; Han et al., 2019b). The core idea is that models can benefit from each other by identifying and learning shared syntactic behaviors, particularly among languages within the same family. For instance, some approaches suggest using a phylogenetic tree to define language similarities and jointly train several dependency parsers. Han et al. (2019b) introduced a method to develop a unified multilingual parser by incorporating language embeddings as input. Meanwhile, Jiang et al. (2019) proposed guiding the learning process of an unsupervised dependency parser using knowledge from another language. This is achieved through three types of regularization that promote similarity in model parameters, dependency edge scores, and parse trees, respectively.",NO,True,1056,True,True
219177284-s4,Conversational Machine Comprehension: a Literature Review,2020-06-01,CoQA,"Conversational QA (CoQA) dataset consists of 126k questions sourced from 8k conversations.

• Dataset preparation: Conversations are prepared over passages collected across 7 different domains, each with its source dataset, such as news articles derived from CNN (Hermann et al., 2015). Amongst the 7 domains, two are used for out-of-domain evaluation (only for evaluation, not training), while the other five aid in-domain evaluation (both training and evaluation). The dialog is prepared in a two annotator setting with one questioning and another answering, both referring to the entire context.

• Questions: Questions are factoid but require sufficient co-referencing and pragmatic reasoning (Bell, 1999).

• Answers: Answers are free-form, with their corresponding rationale highlighted in the passage. However, Yatskar (2019) identified that the answers are slightly modified versions of the rationale, and therefore optimizing an extractive model to predict the answer span with maximum F1 overlap to the gold answer can achieve up to 97.8 F1.

• Dialog features: The dialogs mostly involve drilling-down for details (about 60% of all questions) but lack other dialog features like topic-shift, clarification, or definition.

• Evaluation: Macro-average F1 score of word overlap is used as an evaluation metric and is computed separately for in-domain and out-of-domain.",How are conversations and dialogs structured and evaluated in the CoQA dataset?,"The Conversational QA (CoQA) dataset consists of 126k questions sourced from 8k conversations. Conversations are prepared over passages collected across 7 different domains, each with its source dataset, such as news articles derived from CNN (Hermann et al., 2015). Amongst the 7 domains, two are used for out-of-domain evaluation (only for evaluation, not training), while the other five aid in-domain evaluation (both training and evaluation). The dialog is prepared in a two annotator setting with one questioning and another answering, both referring to the entire context. Questions are factoid but require sufficient co-referencing and pragmatic reasoning (Bell, 1999). Answers are free-form, with their corresponding rationale highlighted in the passage. The dialogs mostly involve drilling-down for details (about 60% of all questions) but lack other dialog features like topic-shift, clarification, or definition. Macro-average F1 score of word overlap is used as an evaluation metric and is computed separately for in-domain and out-of-domain.",How is the structure and evaluation of conversations and dialogs organized in the CoQA dataset?,"The Conversational QA (CoQA) dataset is composed of 126,000 questions derived from 8,000 conversations. These conversations are based on passages collected from seven different domains, each associated with a specific source dataset, such as news articles from CNN (Hermann et al., 2015). Among these domains, five are used for both training and in-domain evaluation, while the remaining two are reserved solely for out-of-domain evaluation purposes.

The dialog structure in CoQA involves a two-annotator setting, where one annotator poses questions and the other provides answers, both referring to the entire context of the passage. The questions are factoid in nature but require significant co-referencing and pragmatic reasoning (Bell, 1999). Answers are provided in a free-form manner, with the corresponding rationale highlighted within the passage.

The dialogs primarily focus on drilling down for details, accounting for about 60% of all questions. However, they lack other dialog features such as topic shifts, clarifications, or definitions. For evaluation, the CoQA dataset uses the macro-average F1 score of word overlap, which is calculated separately for in-domain and out-of-domain scenarios.",7,2,7,2,"Question:

How are conversations and dialogs structured and evaluated in the CoQA dataset?

Answer:

The Conversational QA (CoQA) dataset consists of 126k questions sourced from 8k conversations. Conversations are prepared over passages collected across 7 different domains, each with its source dataset, such as news articles derived from CNN (Hermann et al., 2015). Amongst the 7 domains, two are used for out-of-domain evaluation (only for evaluation, not training), while the other five aid in-domain evaluation (both training and evaluation). The dialog is prepared in a two annotator setting with one questioning and another answering, both referring to the entire context. Questions are factoid but require sufficient co-referencing and pragmatic reasoning (Bell, 1999). Answers are free-form, with their corresponding rationale highlighted in the passage. The dialogs mostly involve drilling-down for details (about 60% of all questions) but lack other dialog features like topic-shift, clarification, or definition. Macro-average F1 score of word overlap is used as an evaluation metric and is computed separately for in-domain and out-of-domain.","Question:

How is the structure and evaluation of conversations and dialogs organized in the CoQA dataset?

Answer:

The Conversational QA (CoQA) dataset is composed of 126,000 questions derived from 8,000 conversations. These conversations are based on passages collected from seven different domains, each associated with a specific source dataset, such as news articles from CNN (Hermann et al., 2015). Among these domains, five are used for both training and in-domain evaluation, while the remaining two are reserved solely for out-of-domain evaluation purposes.

The dialog structure in CoQA involves a two-annotator setting, where one annotator poses questions and the other provides answers, both referring to the entire context of the passage. The questions are factoid in nature but require significant co-referencing and pragmatic reasoning (Bell, 1999). Answers are provided in a free-form manner, with the corresponding rationale highlighted within the passage.

The dialogs primarily focus on drilling down for details, accounting for about 60% of all questions. However, they lack other dialog features such as topic shifts, clarifications, or definitions. For evaluation, the CoQA dataset uses the macro-average F1 score of word overlap, which is calculated separately for in-domain and out-of-domain scenarios.",NO,True,1210,True,True
219177284-s7,Conversational Machine Comprehension: a Literature Review,2020-06-01,Encoder:,"The lexical tokens of the context passage C, selected conversational turns H i , and the current question Q i need to be transformed into input embeddings for the reasoning module. Encoder facilitates this transition. The encoder steps may vary with every approach and reasoning inputs, at a high level, encoding involves transformation and combination of context-independent word embeddings called lexical embeddings such as GloVE (Pennington et al., 2014), intra-sequence contextual  Figure 1: Generic framework of a CMC model. A typical CMC model would consist of (1) History selection module, that selects a subset H i of conversational history H i relevant to the current question Q i ;

(2) Encoder, that encodes the lexical tokens of context C, Q i and H i into input embeddings for contextual integration layer; (3) Reasoning module, that performs contextual integration of input embeddings into contextualized embeddings; and finally, (4) Output predictor, that predicts the output set O i based on contextualized embeddings. embeddings e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) or RNN, question-aware embeddings, and additional feature embeddings like POS tags , history embedding (Qu et al., 2019c) or conversation count. Conversational history H i is generally integrated with this module into any or all of the contextual input embeddings. This process is called History modeling and is the most significant aspect of a CMC encoder.

3. Contextual Integration layer: Contextual information accumulated in the passage, query, and/or history embeddings individually must be fused to generate query-aware and/or history-aware contextualized output embeddings. This process may involve a single layer (single-step reasoning) or repetition across multiple layers (multi-step reasoning). Input for this module generally consists of two (or more) sequence sets for every history turn, or aggregated across all turns, which are then fused in each layer and often inter-weaved (Huang et al., 2018b) with attention.",What are the components and functions of a typical Conversational Machine Comprehension (CMC) model?,"A typical Conversational Machine Comprehension (CMC) model consists of several components, each with specific functions. The first component is the History selection module, which selects a subset \( H_i \) of conversational history relevant to the current question \( Q_i \). The second component is the Encoder, which encodes the lexical tokens of context \( C \), \( Q_i \), and \( H_i \) into input embeddings for the contextual integration layer. The third component is the Reasoning module, which performs contextual integration of input embeddings into contextualized embeddings. Finally, the Output predictor predicts the output set \( O_i \) based on contextualized embeddings.",What are the components and functions of a typical Conversational Machine Comprehension (CMC) model?,"A typical Conversational Machine Comprehension (CMC) model is composed of several key components, each serving a distinct function. The first component is the History Selection module, which is responsible for selecting a relevant subset \( H_i \) of the conversational history that pertains to the current question \( Q_i \). Next, the Encoder component comes into play, encoding the lexical tokens of the context \( C \), the question \( Q_i \), and the selected history \( H_i \) into input embeddings. These embeddings are then used by the contextual integration layer. The third component is the Reasoning module, which integrates these input embeddings into contextualized embeddings, allowing for deeper understanding and reasoning. Finally, the Output Predictor component utilizes these contextualized embeddings to predict the output set \( O_i \), providing the final answer to the question.",8,4,7,4,"Question:

What are the components and functions of a typical Conversational Machine Comprehension (CMC) model?

Answer:

A typical Conversational Machine Comprehension (CMC) model consists of several components, each with specific functions. The first component is the History selection module, which selects a subset \( H_i \) of conversational history relevant to the current question \( Q_i \). The second component is the Encoder, which encodes the lexical tokens of context \( C \), \( Q_i \), and \( H_i \) into input embeddings for the contextual integration layer. The third component is the Reasoning module, which performs contextual integration of input embeddings into contextualized embeddings. Finally, the Output predictor predicts the output set \( O_i \) based on contextualized embeddings.","Question:

What are the components and functions of a typical Conversational Machine Comprehension (CMC) model?

Answer:

A typical Conversational Machine Comprehension (CMC) model is composed of several key components, each serving a distinct function. The first component is the History Selection module, which is responsible for selecting a relevant subset \( H_i \) of the conversational history that pertains to the current question \( Q_i \). Next, the Encoder component comes into play, encoding the lexical tokens of the context \( C \), the question \( Q_i \), and the selected history \( H_i \) into input embeddings. These embeddings are then used by the contextual integration layer. The third component is the Reasoning module, which integrates these input embeddings into contextualized embeddings, allowing for deeper understanding and reasoning. Finally, the Output Predictor component utilizes these contextualized embeddings to predict the output set \( O_i \), providing the final answer to the question.",NO,True,901,True,True
219177284-s15,Conversational Machine Comprehension: a Literature Review,2020-06-01,C. Contextual Integration using Pre-trained Language Models,"Large-scale pre-trained LMs such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018) and RoBERTa , have become the current state-of-the-art approaches for contextual reasoning in CMC models, with leaderboards of both datasets stacked with these models or their variants. The approach is based on the fine-tune BERT-based MRC modeling outlined by Devlin et al. (2019), in which question and context are packed together (with marker embeddings to distinguish) in an input sequence to BERT that outputs contextualized question-aware embeddings for each input token.

Using pre-trained models for reasoning is advantageous in two aspects: Firstly, it simplifies the architecture by fusing encoding and reasoning modules into a single module. Secondly, it provides a ready-to-tune architecture that abstracts out complex contextual interactions between query and context while providing sufficient flexibility to control interactivity via augmentation of input embeddings i.e. concatenation of special embeddings to input tokens that signal the model to incorporate a desirable characteristic in contextualization.

(a) HAM uses a dynamic attention-based history selection policy. Contextualized representations are generated by the model's encoder (BERT+PosHAE) for every history turn at word and sequence levels. Sequence-level embeddings are used to compute attention weights via scaled-dot product, and aggregate representations are generated by a weighted combination of embeddings of each turn in the proportion of their attention weights. Thus, attention weights help in determining the degree of selection (relevance) of each history turn.

(b) HAM's BERT based Encoder (Reasoning Architecture) for every conversation turn. The encoder is provided with input sequence consisting of query tokens (yellow) and context tokens (green) separated by [SEP]. It outputs contextualized representations Ti corresponding to aligned question/passage tokens. The Token embeddings are augmented with segment embeddings(to differentiate query and context), positional embeddings (for distinct position in the sequence), and Positional HAE embeddings (for encoding history answer and relative conversational turn). Figure 3: Illustration of (a) history selection module and (b) encoder/reasoning module of History Attention Mechanism (HAM) model (Qu et al., 2019b).

However, incorporating history into these models is a key challenge in this approach as most of the transformer models such as BERT only accept 2 segments ids in the input sequence. Based on recent research in CMC, two main trends in solving the history integration issue are discussed below:

1. Modify the input embeddings for a single-turn MRC model to incorporate history. This is done by either appending the entire conversation to the question, such as Ju et al. (2019) which uses RoBERTa ) as the base model and truncates query if it exceeds the limit, or add special embeddings to highlight conversational history for the model, such as HAE (Qu et al., 2019a) embeds history answer embeddings with each context token if it is present in any of the history turns (detailed in section 6.2). This approach does not effectively use the model to capture interactions between every dialog-turn and context.

2. Use separate model for each conversational turn to capture one-to-one interaction between history and context, and merge the per-turn contextualized embeddings into aggregated history-aware embeddings. Two models follow this trend. Ohsugi et al. (2019) uses BERT models to capture contextual interaction for every question (history and current) and answer (2N+1 sequences for N turns) and concatenates all sequences together. Finally, it runs Bi-GRU (Cho et al., 2014) over the aggregated sequence to capture inter-turn interactions before sending for prediction. On the other hand, HAM (Qu et al., 2019b) ignores the history questions and uses the current question as a query with positional History Answer Embeddings (section 6.2), thus generating one output sequence per conversation turn. Fig. 3b illustrates HAM encoder. The final sequence is generated using token-level soft-attention based aggregation across all per-turn contextualized sequences.",What are the two main trends in addressing history integration in conversational machine comprehension models?,"Based on recent research in CMC, two main trends in solving the history integration issue are discussed. Firstly, modify the input embeddings for a single-turn MRC model to incorporate history. This is done by either appending the entire conversation to the question, such as Ju et al. (2019) which uses RoBERTa as the base model and truncates the query if it exceeds the limit, or add special embeddings to highlight conversational history for the model, such as HAE (Qu et al., 2019a) embeds history answer embeddings with each context token if it is present in any of the history turns. This approach does not effectively use the model to capture interactions between every dialog-turn and context. Secondly, use a separate model for each conversational turn to capture one-to-one interaction between history and context, and merge the per-turn contextualized embeddings into aggregated history-aware embeddings. Two models follow this trend. Ohsugi et al. (2019) uses BERT models to capture contextual interaction for every question (history and current) and answer (2N+1 sequences for N turns) and concatenates all sequences together. Finally, it runs Bi-GRU (Cho et al., 2014) over the aggregated sequence to capture inter-turn interactions before sending for prediction. On the other hand, HAM (Qu et al., 2019b) ignores the history questions and uses the current question as a query with positional History Answer Embeddings, thus generating one output sequence per conversation turn. The final sequence is generated using token-level soft-attention based aggregation across all per-turn contextualized sequences.",What are the two main approaches to integrating conversational history in machine comprehension models?,"Recent research in conversational machine comprehension (CMC) highlights two primary approaches to addressing the integration of conversational history. The first approach involves modifying the input embeddings of a single-turn machine reading comprehension (MRC) model to incorporate historical context. This can be achieved by appending the entire conversation to the current question, as demonstrated by Ju et al. (2019), which utilizes RoBERTa as the base model and truncates the query if it exceeds the input limit. Alternatively, special embeddings can be added to emphasize conversational history, such as in the HAE model (Qu et al., 2019a), which embeds history answer embeddings with each context token if it appears in any previous turns. However, this method may not fully leverage the model's ability to capture interactions between each dialog turn and the context.

The second approach involves using a separate model for each conversational turn to capture one-to-one interactions between history and context, subsequently merging the per-turn contextualized embeddings into aggregated history-aware embeddings. For instance, Ohsugi et al. (2019) employs BERT models to capture contextual interactions for every question (both historical and current) and answer, resulting in 2N+1 sequences for N turns. These sequences are concatenated, and a Bi-GRU (Cho et al., 2014) is applied to the aggregated sequence to capture inter-turn interactions before making predictions. In contrast, the HAM model (Qu et al., 2019b) disregards historical questions and uses the current question as a query with positional History Answer Embeddings, generating one output sequence per conversation turn. The final sequence is produced using token-level soft-attention-based aggregation across all per-turn contextualized sequences.",7,4,8,2,"Question:

What are the two main trends in addressing history integration in conversational machine comprehension models?

Answer:

Based on recent research in CMC, two main trends in solving the history integration issue are discussed. Firstly, modify the input embeddings for a single-turn MRC model to incorporate history. This is done by either appending the entire conversation to the question, such as Ju et al. (2019) which uses RoBERTa as the base model and truncates the query if it exceeds the limit, or add special embeddings to highlight conversational history for the model, such as HAE (Qu et al., 2019a) embeds history answer embeddings with each context token if it is present in any of the history turns. This approach does not effectively use the model to capture interactions between every dialog-turn and context. Secondly, use a separate model for each conversational turn to capture one-to-one interaction between history and context, and merge the per-turn contextualized embeddings into aggregated history-aware embeddings. Two models follow this trend. Ohsugi et al. (2019) uses BERT models to capture contextual interaction for every question (history and current) and answer (2N+1 sequences for N turns) and concatenates all sequences together. Finally, it runs Bi-GRU (Cho et al., 2014) over the aggregated sequence to capture inter-turn interactions before sending for prediction. On the other hand, HAM (Qu et al., 2019b) ignores the history questions and uses the current question as a query with positional History Answer Embeddings, thus generating one output sequence per conversation turn. The final sequence is generated using token-level soft-attention based aggregation across all per-turn contextualized sequences.","Question:

What are the two main approaches to integrating conversational history in machine comprehension models?

Answer:

Recent research in conversational machine comprehension (CMC) highlights two primary approaches to addressing the integration of conversational history. The first approach involves modifying the input embeddings of a single-turn machine reading comprehension (MRC) model to incorporate historical context. This can be achieved by appending the entire conversation to the current question, as demonstrated by Ju et al. (2019), which utilizes RoBERTa as the base model and truncates the query if it exceeds the input limit. Alternatively, special embeddings can be added to emphasize conversational history, such as in the HAE model (Qu et al., 2019a), which embeds history answer embeddings with each context token if it appears in any previous turns. However, this method may not fully leverage the model's ability to capture interactions between each dialog turn and the context.

The second approach involves using a separate model for each conversational turn to capture one-to-one interactions between history and context, subsequently merging the per-turn contextualized embeddings into aggregated history-aware embeddings. For instance, Ohsugi et al. (2019) employs BERT models to capture contextual interactions for every question (both historical and current) and answer, resulting in 2N+1 sequences for N turns. These sequences are concatenated, and a Bi-GRU (Cho et al., 2014) is applied to the aggregated sequence to capture inter-turn interactions before making predictions. In contrast, the HAM model (Qu et al., 2019b) disregards historical questions and uses the current question as a query with positional History Answer Embeddings, generating one output sequence per conversation turn. The final sequence is produced using token-level soft-attention-based aggregation across all per-turn contextualized sequences.",NO,True,1830,True,True
219177284-s20,Conversational Machine Comprehension: a Literature Review,2020-06-01,Conversation setting,"Questioner-Answerer setting where both have access to the entire context.

Teacher-Student setting where the teacher has access to the full context for answering, while the student has only the title and summary of the article.   (Reddy et al., 2019) and QuAC (Choi et al., 2018) based on different characteristics defined in their papers and the analysis paper by Yatskar (2019). Figure 4: Architecture of the Reasoning Layer of GraphFlow. Context graph-based flow sequence is processed using GNNs and alternated with bi-LSTM and co-attention mechanisms. Source :  (a) A QA dialog example in the CoQA dataset. Every dialog is based on a context and each turn of the dialog contains a question (Qi), an answer (Ai) and a rationale (Ri) that supports the answer. There is sufficient co-referencing between dialog turns as seen in this example -'Where' in Q2 follows on the candidature mentioned in Q1, 'his' in Q4 points to A3, 'he' in Q5 references A4, and 'them' in Q6 refers to people mentioned in both A3 and A4. Source: (Reddy et  The questions are open-ended due to the asymmetric nature of dataset. There is also sufficient co-referencing -'she' in Q3 refers to the protagonist and is a succession of Q2, similarly Q7 is a follow-up on Q5, 'it' in Q6 refers to song mentioned in A5. C A summary of the common CMC models",How is co-referencing demonstrated in dialog turns within the CoQA dataset?,"Every dialog in the CoQA dataset is based on a context, and each turn of the dialog contains a question (Qi), an answer (Ai), and a rationale (Ri) that supports the answer. There is sufficient co-referencing between dialog turns. For example, 'Where' in Q2 follows on the candidature mentioned in Q1, 'his' in Q4 points to A3, 'he' in Q5 references A4, and 'them' in Q6 refers to people mentioned in both A3 and A4. Additionally, 'she' in Q3 refers to the protagonist and is a succession of Q2, similarly Q7 is a follow-up on Q5, and 'it' in Q6 refers to the song mentioned in A5.",How is co-referencing utilized in the dialog turns within the CoQA dataset?,"In the CoQA dataset, each dialog is structured around a context, with each turn comprising a question (Qi), an answer (Ai), and a rationale (Ri) that supports the answer. Co-referencing is effectively demonstrated between dialog turns. For instance, in Q2, the word 'Where' builds upon the candidate mentioned in Q1. Similarly, 'his' in Q4 refers back to A3, while 'he' in Q5 references A4. The term 'them' in Q6 points to people mentioned in both A3 and A4. Additionally, 'she' in Q3 refers to the protagonist, continuing the narrative from Q2. Likewise, Q7 follows up on Q5, and 'it' in Q6 refers to the song mentioned in A5. This co-referencing ensures a coherent flow of information throughout the dialog.",7,2,7,2,"Question:

How is co-referencing demonstrated in dialog turns within the CoQA dataset?

Answer:

Every dialog in the CoQA dataset is based on a context, and each turn of the dialog contains a question (Qi), an answer (Ai), and a rationale (Ri) that supports the answer. There is sufficient co-referencing between dialog turns. For example, 'Where' in Q2 follows on the candidature mentioned in Q1, 'his' in Q4 points to A3, 'he' in Q5 references A4, and 'them' in Q6 refers to people mentioned in both A3 and A4. Additionally, 'she' in Q3 refers to the protagonist and is a succession of Q2, similarly Q7 is a follow-up on Q5, and 'it' in Q6 refers to the song mentioned in A5.","Question:

How is co-referencing utilized in the dialog turns within the CoQA dataset?

Answer:

In the CoQA dataset, each dialog is structured around a context, with each turn comprising a question (Qi), an answer (Ai), and a rationale (Ri) that supports the answer. Co-referencing is effectively demonstrated between dialog turns. For instance, in Q2, the word 'Where' builds upon the candidate mentioned in Q1. Similarly, 'his' in Q4 refers back to A3, while 'he' in Q5 references A4. The term 'them' in Q6 points to people mentioned in both A3 and A4. Additionally, 'she' in Q3 refers to the protagonist, continuing the narrative from Q2. Likewise, Q7 follows up on Q5, and 'it' in Q6 refers to the song mentioned in A5. This co-referencing ensures a coherent flow of information throughout the dialog.",NO,True,709,True,True
7384097-s2,A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena,2015-02-17,Phrase-Based SMT,"Phrase-based SMT (PSMT) is the currently dominant approach in string-based SMT. PSMT ruled out the early word-based SMT framework (Brown et al. 1990(Brown et al. , 1993Berger et al. 1996) thanks to two important novelties: the use of multi-word translation units (Och 1999;Zens, Och, and Ney 2002;Koehn, Och, and Marcu 2003), and the move from a generative to a discriminative modeling framework . The search process (1) in PSMT is guided by the target string e, built from left to right, and the alignment variable b that embeds both segmentation and reordering of the source phrases. This is defined as

such that K 1 , . . . , K I are consecutive intervals partitioning the target word positions, and J 1 , . . . , J I are corresponding but not necessarily consecutive intervals partitioning the source word positions. A phrase segmentation for our running example is shown in Figure 2. The use of phrases mainly results in a better handling of ambiguous words and many-to-many word equivalences, but it also makes it possible to capture a considerable amount of local reordering phenomena within a translation unit (intra-phrase 3 Automatic measures of translation quality are discussed in Section 3. ",What are the key innovations that enabled phrase-based SMT to surpass early word-based SMT frameworks?,"Phrase-based SMT (PSMT) surpassed early word-based SMT frameworks due to two important innovations. The first innovation is the use of multi-word translation units (Och 1999; Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003). The second innovation is the shift from a generative to a discriminative modeling framework. These innovations allowed for better handling of ambiguous words and many-to-many word equivalences. Additionally, they enabled the capture of a considerable amount of local reordering phenomena within a translation unit.",What were the key innovations that allowed phrase-based Statistical Machine Translation (SMT) to outperform early word-based SMT models?,"Phrase-based SMT (PSMT) outperformed early word-based SMT models primarily due to two significant innovations. The first was the introduction of multi-word translation units, as highlighted in studies by Och (1999), Zens, Och, and Ney (2002), and Koehn, Och, and Marcu (2003). This approach allowed for the translation of phrases rather than individual words, which improved the handling of ambiguous words and many-to-many word equivalences. The second innovation was the transition from a generative to a discriminative modeling framework. This shift enabled PSMT to better capture local reordering phenomena within a translation unit, further enhancing translation accuracy and fluency.",7,2,8,2,"Question:

What are the key innovations that enabled phrase-based SMT to surpass early word-based SMT frameworks?

Answer:

Phrase-based SMT (PSMT) surpassed early word-based SMT frameworks due to two important innovations. The first innovation is the use of multi-word translation units (Och 1999; Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003). The second innovation is the shift from a generative to a discriminative modeling framework. These innovations allowed for better handling of ambiguous words and many-to-many word equivalences. Additionally, they enabled the capture of a considerable amount of local reordering phenomena within a translation unit.","Question:

What were the key innovations that allowed phrase-based Statistical Machine Translation (SMT) to outperform early word-based SMT models?

Answer:

Phrase-based SMT (PSMT) outperformed early word-based SMT models primarily due to two significant innovations. The first was the introduction of multi-word translation units, as highlighted in studies by Och (1999), Zens, Och, and Ney (2002), and Koehn, Och, and Marcu (2003). This approach allowed for the translation of phrases rather than individual words, which improved the handling of ambiguous words and many-to-many word equivalences. The second innovation was the transition from a generative to a discriminative modeling framework. This shift enabled PSMT to better capture local reordering phenomena within a translation unit, further enhancing translation accuracy and fluency.",NO,True,689,True,True
7384097-s6,A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena,2015-02-17,n-gram Based SMT,"n-gram based SMT (Casacuberta and Vidal 2004;Mariño et al. 2006) is a string-based alternative to PSMT. In this framework, smoothed n-gram models are learned over sequences of minimal translation units (called tuples), which, like phrase pairs, are pairs of word sequences extracted from word-aligned parallel sentences. Tuples, however, are typically shorter than phrase pairs and are extracted from a unique, monotonic segmentation of the sentence pair. Thus, the problem of spurious phrase segmentation is avoided but non-local reordering becomes an issue. For instance, in Figure 2, a monotonic phrase segmentation could be achieved only by treating the large block [jdd ... AlsAds]-[The ... renewed] as a single tuple. Reordering is then addressed by ""tuple unfolding"" (Crego, Mariño, and de Gispert 2005): that is, during training the source words of each translation unit are rearranged in a target-like order so that more, shorter tuples can be extracted. At test time, input sentences have to be pre-ordered for translation. To this end, Crego and Mariño (2006) propose to precompute a number of likely permutations of the input using POS-based rewrite rules learned during tuple unfolding. The reorderings thus obtained are used to extend the search graph of a monotonic decoder. 8 Reordering is often considered as a shortcoming of n-gram-based SMT as reordering decisions are largely decoupled from decoding and mostly based on source-side information.","How is reordering handled in n-gram based SMT, and what are its limitations?","n-gram based SMT is a string-based alternative to PSMT. In this framework, smoothed n-gram models are learned over sequences of minimal translation units (called tuples), which are pairs of word sequences extracted from word-aligned parallel sentences. Tuples are typically shorter than phrase pairs and are extracted from a unique, monotonic segmentation of the sentence pair. Thus, the problem of spurious phrase segmentation is avoided, but non-local reordering becomes an issue. Reordering is addressed by ""tuple unfolding,"" where during training, the source words of each translation unit are rearranged in a target-like order so that more, shorter tuples can be extracted. At test time, input sentences have to be pre-ordered for translation. Crego and Mariño (2006) propose to precompute a number of likely permutations of the input using POS-based rewrite rules learned during tuple unfolding. The reorderings thus obtained are used to extend the search graph of a monotonic decoder. Reordering is often considered a shortcoming of n-gram-based SMT as reordering decisions are largely decoupled from decoding and mostly based on source-side information.","How is reordering managed in n-gram based Statistical Machine Translation (SMT), and what are the limitations of this approach?","In n-gram based Statistical Machine Translation (SMT), reordering is a critical aspect that is handled differently compared to Phrase-based SMT (PSMT). This approach involves learning smoothed n-gram models over sequences of minimal translation units known as tuples. These tuples are pairs of word sequences extracted from word-aligned parallel sentences and are typically shorter than phrase pairs. They are derived from a unique, monotonic segmentation of the sentence pair, which helps avoid the problem of spurious phrase segmentation. However, this method introduces challenges with non-local reordering.

To address reordering, the technique of ""tuple unfolding"" is employed. During training, the source words of each translation unit are rearranged into a target-like order, allowing for the extraction of more, shorter tuples. At the testing phase, input sentences must be pre-ordered for translation. Crego and Mariño (2006) suggest precomputing several likely permutations of the input using POS-based rewrite rules learned during tuple unfolding. These reorderings are then used to extend the search graph of a monotonic decoder.

Despite these strategies, reordering remains a limitation of n-gram based SMT. The reordering decisions are largely decoupled from the decoding process and rely heavily on source-side information, which can limit the effectiveness of the translation.",7,2,7,2,"Question:

How is reordering handled in n-gram based SMT, and what are its limitations?

Answer:

n-gram based SMT is a string-based alternative to PSMT. In this framework, smoothed n-gram models are learned over sequences of minimal translation units (called tuples), which are pairs of word sequences extracted from word-aligned parallel sentences. Tuples are typically shorter than phrase pairs and are extracted from a unique, monotonic segmentation of the sentence pair. Thus, the problem of spurious phrase segmentation is avoided, but non-local reordering becomes an issue. Reordering is addressed by ""tuple unfolding,"" where during training, the source words of each translation unit are rearranged in a target-like order so that more, shorter tuples can be extracted. At test time, input sentences have to be pre-ordered for translation. Crego and Mariño (2006) propose to precompute a number of likely permutations of the input using POS-based rewrite rules learned during tuple unfolding. The reorderings thus obtained are used to extend the search graph of a monotonic decoder. Reordering is often considered a shortcoming of n-gram-based SMT as reordering decisions are largely decoupled from decoding and mostly based on source-side information.","Question:

How is reordering managed in n-gram based Statistical Machine Translation (SMT), and what are the limitations of this approach?

Answer:

In n-gram based Statistical Machine Translation (SMT), reordering is a critical aspect that is handled differently compared to Phrase-based SMT (PSMT). This approach involves learning smoothed n-gram models over sequences of minimal translation units known as tuples. These tuples are pairs of word sequences extracted from word-aligned parallel sentences and are typically shorter than phrase pairs. They are derived from a unique, monotonic segmentation of the sentence pair, which helps avoid the problem of spurious phrase segmentation. However, this method introduces challenges with non-local reordering.

To address reordering, the technique of ""tuple unfolding"" is employed. During training, the source words of each translation unit are rearranged into a target-like order, allowing for the extraction of more, shorter tuples. At the testing phase, input sentences must be pre-ordered for translation. Crego and Mariño (2006) suggest precomputing several likely permutations of the input using POS-based rewrite rules learned during tuple unfolding. These reorderings are then used to extend the search graph of a monotonic decoder.

Despite these strategies, reordering remains a limitation of n-gram based SMT. The reordering decisions are largely decoupled from the decoding process and rely heavily on source-side information, which can limit the effectiveness of the translation.",NO,True,1393,True,True
7384097-s8,A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena,2015-02-17,Syntax-Based,"SMT. An important motivation for using syntax in SMT is that reordering among natural languages very often involves the permutation of whole syntactic constituents (e.g., Fox 2002). For instance, in our running example (Figure 2), knowing the span of the Arabic subject would be enough to predict the reordering of the verb for translation into English.

Syntax-based SMT encompasses a variety of frameworks that use syntactic annotation either on the source or on the target language, or both. So-called tree-to-string methods (Huang, Knight, and Joshi 2006;Liu, Liu, and Lin 2006) use a given input sentence parse tree to restrict the application of translation/reordering rules to word spans that coincide with syntactic constituents of specific categories. For instance, the swap of Alr}ys Alfrnsy may only be dictated by a rule applying to noun phrases composed of a noun and an adjective. On the other hand, string-to-tree methods (Yamada and Knight 2002;Galley et al. 2004;Marcu et al. 2006;Shen, Xu, and Weischedel 2010) use syntax as a way to restrict translation hypotheses to well-formed target language sentences-ruling out, for instance, a translation that fails to reorder the translated verb renewed with respect to its subject. Using syntax on both source and target sides (treeto-tree) (Imamura, Okuma, and Sumita 2005;Ding and Palmer 2005;Smith and Eisner 2006;Watanabe, Tsukada, and Isozaki 2006;Zhang et al. 2008) has proven rather difficult in practice due to the complexity of aligning potentially very different tree topologies and to the large size of the resulting translation grammars. Moreover, the need for highquality parsers in both language sides seriously limits the applicability of this approach.

Syntax-based SMT approaches also differ in the formalism they use to represent the trees. Those based on phrase structure (constituency) grammars typically comply with the principle that each translation/reordering rule should match a complete constituent, whereas those based on dependency grammars opt for a more flexible use of structure. For example, in string-to-dependency SMT (Shen, Xu, and Weischedel 2010) rules can correspond to partial constituents but must be either a single rooted tree, with each child being a complete sub-tree, or a sequence of siblings, each being a complete subtree. Partial dependency rules are then combined during decoding, which means that not all reordering decisions are governed by the translation model.

An even more flexible use of structure is advocated by the treelet-based SMT framework (Quirk, Menezes, and Cherry 2005), where translation rules can correspond to any connected subgraph of the dependency tree (i.e., treelet). As illustrated by Figure 4, treelet pairs are extracted from pairs of source dependency parse tree and target-side projected trees. Treelets can be seen as phrases that are not limited to sets of adjacent words, but rather to sets of words that are connected by dependency relations, which in turn make it possible to learn non-local reordering patterns. As reordering jdd AlEAhl Almgrby Almlk mHmd AlsAds dEm -h l-m$rwE Alr}ys Alfrnsy! ",How do syntax-based SMT approaches differ in their representation and use of tree structures?,"Syntax-based SMT approaches differ in the formalism they use to represent the trees. Those based on phrase structure (constituency) grammars typically comply with the principle that each translation/reordering rule should match a complete constituent. In contrast, those based on dependency grammars opt for a more flexible use of structure. For example, in string-to-dependency SMT (Shen, Xu, and Weischedel 2010), rules can correspond to partial constituents but must be either a single rooted tree, with each child being a complete sub-tree, or a sequence of siblings, each being a complete subtree. Partial dependency rules are then combined during decoding, which means that not all reordering decisions are governed by the translation model. An even more flexible use of structure is advocated by the treelet-based SMT framework (Quirk, Menezes, and Cherry 2005), where translation rules can correspond to any connected subgraph of the dependency tree (i.e., treelet). Treelets can be seen as phrases that are not limited to sets of adjacent words, but rather to sets of words that are connected by dependency relations, which in turn make it possible to learn non-local reordering patterns.",How do syntax-based Statistical Machine Translation (SMT) approaches utilize tree structures differently?,"Syntax-based SMT approaches vary in how they represent and utilize tree structures, primarily based on the type of grammar they employ. Approaches using phrase structure (constituency) grammars adhere to the principle that each translation or reordering rule should align with a complete constituent. In contrast, those using dependency grammars offer more flexibility in their structural use. For instance, in string-to-dependency SMT (Shen, Xu, and Weischedel 2010), rules can correspond to partial constituents but must form either a single rooted tree, where each child is a complete subtree, or a sequence of siblings, each being a complete subtree. These partial dependency rules are then combined during decoding, meaning not all reordering decisions are dictated by the translation model. An even more flexible approach is seen in the treelet-based SMT framework (Quirk, Menezes, and Cherry 2005), where translation rules can correspond to any connected subgraph of the dependency tree, known as a treelet. Treelets function like phrases but are not restricted to adjacent words; instead, they consist of words connected by dependency relations, allowing for the learning of non-local reordering patterns.",7,2,7,2,"Question:

How do syntax-based SMT approaches differ in their representation and use of tree structures?

Answer:

Syntax-based SMT approaches differ in the formalism they use to represent the trees. Those based on phrase structure (constituency) grammars typically comply with the principle that each translation/reordering rule should match a complete constituent. In contrast, those based on dependency grammars opt for a more flexible use of structure. For example, in string-to-dependency SMT (Shen, Xu, and Weischedel 2010), rules can correspond to partial constituents but must be either a single rooted tree, with each child being a complete sub-tree, or a sequence of siblings, each being a complete subtree. Partial dependency rules are then combined during decoding, which means that not all reordering decisions are governed by the translation model. An even more flexible use of structure is advocated by the treelet-based SMT framework (Quirk, Menezes, and Cherry 2005), where translation rules can correspond to any connected subgraph of the dependency tree (i.e., treelet). Treelets can be seen as phrases that are not limited to sets of adjacent words, but rather to sets of words that are connected by dependency relations, which in turn make it possible to learn non-local reordering patterns.","Question:

How do syntax-based Statistical Machine Translation (SMT) approaches utilize tree structures differently?

Answer:

Syntax-based SMT approaches vary in how they represent and utilize tree structures, primarily based on the type of grammar they employ. Approaches using phrase structure (constituency) grammars adhere to the principle that each translation or reordering rule should align with a complete constituent. In contrast, those using dependency grammars offer more flexibility in their structural use. For instance, in string-to-dependency SMT (Shen, Xu, and Weischedel 2010), rules can correspond to partial constituents but must form either a single rooted tree, where each child is a complete subtree, or a sequence of siblings, each being a complete subtree. These partial dependency rules are then combined during decoding, meaning not all reordering decisions are dictated by the translation model. An even more flexible approach is seen in the treelet-based SMT framework (Quirk, Menezes, and Cherry 2005), where translation rules can correspond to any connected subgraph of the dependency tree, known as a treelet. Treelets function like phrases but are not restricted to adjacent words; instead, they consist of words connected by dependency relations, allowing for the learning of non-local reordering patterns.",NO,True,1213,True,True
7384097-s13,A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena,2015-02-17,Main Pre-ordering Strategies.,"A large number of pre-ordering strategies have been proposed. As a first classification, we divide them into deterministic, non-deterministic, and hybrid. Deterministic pre-ordering aims at finding a single optimal permutation of the input sentence, which is then translated monotonically or with a low distortion limit (Nießen and Ney 2001; Xia and McCord 2004;Collins, Koehn, and Kucerova 2005;Popović and Ney 2006;Costa-jussà and Fonollosa 2006;Wang, Collins, and Koehn 2007;   the opposite direction by placing Japanese syntactic heads in the middle is not a trivial problem. We utilize the Head-Finalization rules to generate intermediate head-finalized English sentences called Head-Final English (HFE) and decompose Japanese-to-English translation into 1) Japaneseto-HFE translation and 2) HFE-to-English postordering. We achieved significant improvements from baseline (phrase-based, hierarchical phrasebased, and string-to-tree) translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively, in the experiment of patent translation.

The remainder of this paper is organized as follows. Section 2 briefly reviews related studies on the reordering problem and another related technology called post-editing. Section 3 presents the proposed method in detail taking Japanese-to-English translation as a test case. Section 4 reports our experiments and discusses the results. Section 5 concludes this paper with our prospects for future work.",What are the main classifications of pre-ordering strategies in word reordering for statistical machine translation?,"A large number of pre-ordering strategies have been proposed. As a first classification, they are divided into deterministic, non-deterministic, and hybrid.",What are the primary categories of pre-ordering strategies used in word reordering for statistical machine translation?,"Pre-ordering strategies in word reordering for statistical machine translation can be broadly classified into three main categories: deterministic, non-deterministic, and hybrid. These classifications encompass a wide range of approaches that have been proposed to address the challenges of word reordering in translation tasks.",7,2,7,2,"Question:

What are the main classifications of pre-ordering strategies in word reordering for statistical machine translation?

Answer:

A large number of pre-ordering strategies have been proposed. As a first classification, they are divided into deterministic, non-deterministic, and hybrid.","Question:

What are the primary categories of pre-ordering strategies used in word reordering for statistical machine translation?

Answer:

Pre-ordering strategies in word reordering for statistical machine translation can be broadly classified into three main categories: deterministic, non-deterministic, and hybrid. These classifications encompass a wide range of approaches that have been proposed to address the challenges of word reordering in translation tasks.",NO,True,328,False,False
7384097-s20,A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena,2015-02-17,General-Purpose Metrics,"BLEU (Papineni et al. 2001) is a lexical match-based score that represents the de facto standard for SMT evaluation. Here, proximity between candidate and reference translations is measured in terms of overlapping word n-grams, with n typically ranging from 1 to 4. For each order n a modified precision score (see Papineni et al. [2001] for details) is computed on the whole test set and combined in a geometric mean. The resulting score is then multiplied by a brevity penalty that accounts for length mismatches between reference and candidate translations. Al-Onaizan and Papineni (2006) use BLEU to measure word order similarity between two languages: that is, by computing the BLEU score between the original target sentence e and a source-like permutation of e. Using n-grams, though, is a limited solution to the problem of word ordering evaluation. First, because only exact surface matches are counted, without any consideration of morphology or synonymy. Second, because the absolute positioning of words in the sentence is not captured, but only their proximity within a small context.

The former issue is addressed to some extent by METEOR (Banerjee and Lavie 2005), which relies on language-specific stemmers and synonymy modules to go beyond the surface-level similarity. As for word order, METEOR treats it separately with a fragmentation penalty proportional to the smallest number of chunks that the hypothesis must be divided into to align with the reference translation. This quantity can be interpreted as the number of times that a human reader would have to ""jump"" between words to recover the correct translation order. However, no distinction is made between short and long-range reordering errors.

The weakness of BLEU and METEOR with respect to word order was demonstrated by Birch, Osborne, and Blunsom (2010) with a significant example that we report in Table 2. For simplicity, the example assumes that the reference order is monotonic and that hypotheses and reference translations contain exactly the same words. According to both metrics, hypothesis (a) is worse than (b), although in (a) only two adjacent words are swapped whereas in (b) the two halves of the sentence are swapped.","How do BLEU and METEOR metrics evaluate word order, and what are their limitations in this context?","BLEU is a lexical match-based score that measures proximity between candidate and reference translations in terms of overlapping word n-grams, with n typically ranging from 1 to 4.  Using n-grams is a limited solution to the problem of word ordering evaluation because only exact surface matches are counted, without any consideration of morphology or synonymy. Additionally, the absolute positioning of words in the sentence is not captured, but only their proximity within a small context. METEOR addresses the former issue to some extent by relying on language-specific stemmers and synonymy modules to go beyond surface-level similarity. As for word order, METEOR treats it separately with a fragmentation penalty proportional to the smallest number of chunks that the hypothesis must be divided into to align with the reference translation. This quantity can be interpreted as the number of times that a human reader would have to ""jump"" between words to recover the correct translation order. However, no distinction is made between short and long-range reordering errors. The weakness of BLEU and METEOR with respect to word order was demonstrated by Birch, Osborne, and Blunsom (2010) with an example where hypothesis (a) is worse than (b) according to both metrics, although in (a) only two adjacent words are swapped whereas in (b) the two halves of the sentence are swapped.","How do BLEU and METEOR metrics evaluate word order, and what are their limitations in this context?","BLEU is a metric that evaluates translations by measuring the overlap of word n-grams between candidate and reference translations, typically using n-grams ranging from 1 to 4. This approach, however, has limitations in assessing word order because it only considers exact surface matches, ignoring aspects like morphology and synonymy. Additionally, BLEU does not capture the absolute positioning of words in a sentence, focusing instead on their proximity within a small context.

METEOR, on the other hand, attempts to address some of these limitations by incorporating language-specific stemmers and synonymy modules, which allow it to go beyond surface-level similarity. When it comes to word order, METEOR introduces a fragmentation penalty. This penalty is based on the smallest number of chunks into which the hypothesis must be divided to align with the reference translation. This can be interpreted as the number of times a human reader would need to ""jump"" between words to achieve the correct translation order. However, METEOR does not differentiate between short and long-range reordering errors.

The limitations of both BLEU and METEOR in evaluating word order were highlighted by Birch, Osborne, and Blunsom (2010). They provided an example where a hypothesis with two adjacent words swapped was rated worse than another hypothesis where the two halves of the sentence were swapped, according to both metrics. This demonstrates the metrics' inability to accurately assess the severity of different types of word order errors.",8,2,8,2,"Question:

How do BLEU and METEOR metrics evaluate word order, and what are their limitations in this context?

Answer:

BLEU is a lexical match-based score that measures proximity between candidate and reference translations in terms of overlapping word n-grams, with n typically ranging from 1 to 4.  Using n-grams is a limited solution to the problem of word ordering evaluation because only exact surface matches are counted, without any consideration of morphology or synonymy. Additionally, the absolute positioning of words in the sentence is not captured, but only their proximity within a small context. METEOR addresses the former issue to some extent by relying on language-specific stemmers and synonymy modules to go beyond surface-level similarity. As for word order, METEOR treats it separately with a fragmentation penalty proportional to the smallest number of chunks that the hypothesis must be divided into to align with the reference translation. This quantity can be interpreted as the number of times that a human reader would have to ""jump"" between words to recover the correct translation order. However, no distinction is made between short and long-range reordering errors. The weakness of BLEU and METEOR with respect to word order was demonstrated by Birch, Osborne, and Blunsom (2010) with an example where hypothesis (a) is worse than (b) according to both metrics, although in (a) only two adjacent words are swapped whereas in (b) the two halves of the sentence are swapped.","Question:

How do BLEU and METEOR metrics evaluate word order, and what are their limitations in this context?

Answer:

BLEU is a metric that evaluates translations by measuring the overlap of word n-grams between candidate and reference translations, typically using n-grams ranging from 1 to 4. This approach, however, has limitations in assessing word order because it only considers exact surface matches, ignoring aspects like morphology and synonymy. Additionally, BLEU does not capture the absolute positioning of words in a sentence, focusing instead on their proximity within a small context.

METEOR, on the other hand, attempts to address some of these limitations by incorporating language-specific stemmers and synonymy modules, which allow it to go beyond surface-level similarity. When it comes to word order, METEOR introduces a fragmentation penalty. This penalty is based on the smallest number of chunks into which the hypothesis must be divided to align with the reference translation. This can be interpreted as the number of times a human reader would need to ""jump"" between words to achieve the correct translation order. However, METEOR does not differentiate between short and long-range reordering errors.

The limitations of both BLEU and METEOR in evaluating word order were highlighted by Birch, Osborne, and Blunsom (2010). They provided an example where a hypothesis with two adjacent words swapped was rated worse than another hypothesis where the two halves of the sentence were swapped, according to both metrics. This demonstrates the metrics' inability to accurately assess the severity of different types of word order errors.",NO,True,1543,True,True
7384097-s34,A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena,2015-02-17,English and Turkish,"English and Japanese [ Main order: different; CDiff: 6; PDiff: 1.5 ] Japanese is the prototypical example of head-final language. In this pair all clause-level features are discordant, whereas at the phrase level, Japanese differs from English for the use of postpositions and the strictly head-final genitive construction. This pair, like the previous one, is extremely challenging for PSMT because of the hierarchical nature of its reordering phenomena and the high frequency of long-range word movements. Indeed, translation between English and Japanese has spurred a remarkable amount of work on pre-ordering, post-ordering, and decoding-time reordering. In 2013 the PatentMT evaluation campaign of the NTCIR conference (Goto et al. 2013a) saw rule-based and hybrid systems largely outperform the purely statistical ones in Japanese-to-English. The highest-ranked SMT submission was actually a combination of three SMT systems, including a baseline PSMT method, a rule-based pre-ordering method, and a post-ordering method based on string-to-tree syntax-based SMT . Interestingly, the trends were different in the opposite translation direction, English-to-Japanese, where all rule-based MT systems were significantly outperformed by a PSMT system that performed pre-ordering of the English input with few manual rules for head finalization based on dependency parse trees .

English and Chinese [ Main order: same; CDiff: 3.5; PDiff: 1 ] Despite belonging to the same main order type, these two languages differ in the positioning of oblique phrases, relative clauses, interrogative phrases, and subordinating words. 19 Moreover, word order variations are quite common in Chinese to mark the topic of a sentence, (i.e., what is being talked about). Comparing the two languages at the phrase level, we find partial disagreement in the use of genitive and adpositions (Chinese has both prepositions and postpositions). Thus, this pair too is characterized by very complex reordering, hardly manageable by a PSMT system. This is confirmed by a number of empirical results showing that tree-based approaches (particularly HSMT) consistently outperform PSMT in Chinese-to-English evaluations (Zollmann et al. 2008;Birch, Blunsom, and Osborne 2009). It is worth noting that translation between Chinese and English has been the main motivation and test bed for the development of HSMT.

French and Arabic [ Main order: different; CDiff: 1.5; PDiff: 1 ] At the clause level, this pair differs in main word order (SVO versus VSO or SVO) like the English-Arabic pair, but also in the order of negation and verb. On the other hand, phrase-level order is notably more similar, with only one discordant feature of minor importance (adjective and degree word). Less research was published on this language pair. Nevertheless, Hasan and Ney (2008) and Schwenk and Senellart (2009) chose a PSMT approach to experiment with an Arabic-to-French task. Figure 8 illustrates the reordering characteristics of three language pairs by means of sentence examples that were automatically word-aligned with GIZA++ (Och and Ney 2003) (intersection of direct and inverse alignments). In the first row, we see two English-German sentence pairs; in both cases, most of the points lie close to the diagonal representing an overall monotonic translation, whereas few isolated points denote the very long-range reordering of verbs. Similarly, in the two English-Arabic sentence pairs, we mostly observe local reorderings, with the exception of few isolated points corresponding to the Arabic clause-initial verbs. Finally, the two Turkish-English examples display global reordering, due to the high number of clause-level order differences.

Where possible, it is interesting to relate our analysis with previously published measures of reordering based on parallel data. To our knowledge, the most comprehensive results of this kind are reported by Birch (2011), who formulates reordering as a binary process occurring between two blocks that are adjacent in the source (cf. ITG constraints in Section 2.1). Here, the general amount of reordering in a language pair is estimated by the RQuantity, defined as the sum of the spans of all the reordered blocks on the target side, normalized by the length of the target sentence and averaged over a corpus. Based on the Europarl corpus (Koehn 2002) and automatic word alignments, Birch (2011) reports average RQuantity values of 0.586/0.608 in English-to-German/German-to-English, versus only 0.402/0.395 in English-to-French/French-to-English. The manually aligned GALE corpus (LDC2006E93) is instead used to measure the distribution of reordering widths, defined as the sum of the swapped blocks' target spans. Widths are binned into short (2-4 words), medium (5-8), and long (>8). In Chinese-to-English there are about 0.8/0.9/0.9 short/medium/long reordered blocks per sentence, whereas in Arabic-to-English there are 1.1/0.4/0.2 short/medium/long reordered blocks per sentence. These figures align nicely with our classification of phrase-and clause-level differences, which we have related to longer and shorter-range English and German:

English and Arabic: English and Turkish:","How do English and Japanese differ at the clause and phrase levels, and why is this challenging for PSMT?","Japanese is the prototypical example of a head-final language. In this pair, all clause-level features are discordant, whereas at the phrase level, Japanese differs from English in the use of postpositions and the strictly head-final genitive construction. This pair is extremely challenging for PSMT because of the hierarchical nature of its reordering phenomena and the high frequency of long-range word movements.","What are the differences between English and Japanese at the clause and phrase levels, and why do these differences pose challenges for Phrase-Based Statistical Machine Translation (PSMT)?","Japanese is a prime example of a head-final language, which means that in Japanese, the head of a phrase typically appears at the end. At the clause level, English and Japanese exhibit significant differences, as all clause-level features are discordant between the two languages. At the phrase level, Japanese differs from English through its use of postpositions and its strictly head-final genitive construction. These structural differences make translating between English and Japanese particularly challenging for Phrase-Based Statistical Machine Translation (PSMT). The primary difficulty arises from the hierarchical nature of reordering phenomena in Japanese and the frequent occurrence of long-range word movements, which PSMT systems struggle to handle effectively.",7,4,8,2,"Question:

How do English and Japanese differ at the clause and phrase levels, and why is this challenging for PSMT?

Answer:

Japanese is the prototypical example of a head-final language. In this pair, all clause-level features are discordant, whereas at the phrase level, Japanese differs from English in the use of postpositions and the strictly head-final genitive construction. This pair is extremely challenging for PSMT because of the hierarchical nature of its reordering phenomena and the high frequency of long-range word movements.","Question:

What are the differences between English and Japanese at the clause and phrase levels, and why do these differences pose challenges for Phrase-Based Statistical Machine Translation (PSMT)?

Answer:

Japanese is a prime example of a head-final language, which means that in Japanese, the head of a phrase typically appears at the end. At the clause level, English and Japanese exhibit significant differences, as all clause-level features are discordant between the two languages. At the phrase level, Japanese differs from English through its use of postpositions and its strictly head-final genitive construction. These structural differences make translating between English and Japanese particularly challenging for Phrase-Based Statistical Machine Translation (PSMT). The primary difficulty arises from the hierarchical nature of reordering phenomena in Japanese and the frequent occurrence of long-range word movements, which PSMT systems struggle to handle effectively.",NO,True,776,True,True
247627890-s19,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",2022-03-22,Data Augmentation,"Trajectory-Instruction Augmentation Augmented path-instruction pairs could be used in VLN directly. Currently the common practice is to train a speaker module to generate instructions given a navigation path (Fried et al., 2018). This generated data have varying quality (Zhao et al., 2021). Therefore an alignment scorer (Huang et al., 2019) or adversarial discriminator (Fu et al., 2020) can select high-quality pairs for augmentation. Environment Augmentation Generating more environment data not only helps generate more trajectories, but also alleviates the problem of overfitting in seen environments. Randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or simply splitting the house scenes and remixing them (Liu et al., 2021) could create new environments, which could further be used to generate more trajectory-instruction pairs (Fried et al., 2018). Training data may also be augmented by replacing some visual features with counterfactual ones (Parvaneh et al., 2020).",What methods are used to enhance data quality and diversity in Vision-and-Language Navigation through augmentation?,"Trajectory-Instruction Augmentation involves using augmented path-instruction pairs directly in VLN. The common practice is to train a speaker module to generate instructions given a navigation path (Fried et al., 2018). The generated data have varying quality (Zhao et al., 2021). An alignment scorer (Huang et al., 2019) or adversarial discriminator (Fu et al., 2020) can select high-quality pairs for augmentation. Environment Augmentation involves generating more environment data to help generate more trajectories and alleviate the problem of overfitting in seen environments. Randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or splitting and remixing house scenes (Liu et al., 2021) could create new environments. These new environments could further be used to generate more trajectory-instruction pairs (Fried et al., 2018). Training data may also be augmented by replacing some visual features with counterfactual ones (Parvaneh et al., 2020).",What strategies are employed to improve data quality and diversity in Vision-and-Language Navigation through augmentation techniques?,"In Vision-and-Language Navigation (VLN), enhancing data quality and diversity is crucial, and several augmentation methods are employed to achieve this. One key approach is Trajectory-Instruction Augmentation, which involves using augmented path-instruction pairs directly in VLN. A common practice here is to train a speaker module to generate instructions based on a given navigation path, as demonstrated by Fried et al. (2018). However, the quality of the generated data can vary, as noted by Zhao et al. (2021). To ensure high-quality augmentation, an alignment scorer (Huang et al., 2019) or an adversarial discriminator (Fu et al., 2020) can be used to select the best pairs.

Another important method is Environment Augmentation, which focuses on generating additional environment data to create more trajectories and mitigate the issue of overfitting in familiar environments. Techniques such as randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or splitting and remixing house scenes (Liu et al., 2021) can be employed to create new environments. These newly generated environments can then be used to produce more trajectory-instruction pairs, as suggested by Fried et al. (2018). Additionally, training data can be further augmented by replacing some visual features with counterfactual ones, as explored by Parvaneh et al. (2020).",7,4,7,4,"Question:

What methods are used to enhance data quality and diversity in Vision-and-Language Navigation through augmentation?

Answer:

Trajectory-Instruction Augmentation involves using augmented path-instruction pairs directly in VLN. The common practice is to train a speaker module to generate instructions given a navigation path (Fried et al., 2018). The generated data have varying quality (Zhao et al., 2021). An alignment scorer (Huang et al., 2019) or adversarial discriminator (Fu et al., 2020) can select high-quality pairs for augmentation. Environment Augmentation involves generating more environment data to help generate more trajectories and alleviate the problem of overfitting in seen environments. Randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or splitting and remixing house scenes (Liu et al., 2021) could create new environments. These new environments could further be used to generate more trajectory-instruction pairs (Fried et al., 2018). Training data may also be augmented by replacing some visual features with counterfactual ones (Parvaneh et al., 2020).","Question:

What strategies are employed to improve data quality and diversity in Vision-and-Language Navigation through augmentation techniques?

Answer:

In Vision-and-Language Navigation (VLN), enhancing data quality and diversity is crucial, and several augmentation methods are employed to achieve this. One key approach is Trajectory-Instruction Augmentation, which involves using augmented path-instruction pairs directly in VLN. A common practice here is to train a speaker module to generate instructions based on a given navigation path, as demonstrated by Fried et al. (2018). However, the quality of the generated data can vary, as noted by Zhao et al. (2021). To ensure high-quality augmentation, an alignment scorer (Huang et al., 2019) or an adversarial discriminator (Fu et al., 2020) can be used to select the best pairs.

Another important method is Environment Augmentation, which focuses on generating additional environment data to create more trajectories and mitigate the issue of overfitting in familiar environments. Techniques such as randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or splitting and remixing house scenes (Liu et al., 2021) can be employed to create new environments. These newly generated environments can then be used to produce more trajectory-instruction pairs, as suggested by Fried et al. (2018). Additionally, training data can be further augmented by replacing some visual features with counterfactual ones, as explored by Parvaneh et al. (2020).",NO,True,1381,True,True
235422524-s1,An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,2021-06-14,Data Augmentation for NLP,"Data augmentation increases both the amount (the number of data points) and the diversity (the variety of data) of a given dataset (Cubuk et al., 2019). Limited labeled data often leads to overfitting on the training set and data augmentation works to alleviate this issue by manipulating data either automatically or manually to create additional augmented data.Such techniques have been widely explored in the computer vision field, with methods like geometric/color space transformations (Simard et al., 2003;Krizhevsky et al., 2012;Taylor and Nitschke, 2018), mixup (Zhang et al., 2018), and random erasing (Zhong et al., 2020;DeVries and Taylor, 2017). Although the discrete nature of textual data and its complex syntactic and semantic structures make finding labelpreserving transformation more difficult, there nevertheless exists a wide range of methods for augmenting text data that in practice preserve labels. In the following subsections, we describe four broad classes of data augmentation methods:",What are the challenges and methods for augmenting textual data in NLP?,"The discrete nature of textual data and its complex syntactic and semantic structures make finding label-preserving transformations more difficult. Nevertheless, there exists a wide range of methods for augmenting text data that in practice preserve labels.",What are the challenges and methods involved in augmenting textual data for NLP tasks?,"Augmenting textual data in NLP presents unique challenges due to the discrete nature of text and its intricate syntactic and semantic structures, which complicate the process of finding transformations that preserve the original labels. Despite these challenges, there is a wide array of methods available for text data augmentation that effectively maintain label integrity in practice.",8,6,8,4,"Question:

What are the challenges and methods for augmenting textual data in NLP?

Answer:

The discrete nature of textual data and its complex syntactic and semantic structures make finding label-preserving transformations more difficult. Nevertheless, there exists a wide range of methods for augmenting text data that in practice preserve labels.","Question:

What are the challenges and methods involved in augmenting textual data for NLP tasks?

Answer:

Augmenting textual data in NLP presents unique challenges due to the discrete nature of text and its intricate syntactic and semantic structures, which complicate the process of finding transformations that preserve the original labels. Despite these challenges, there is a wide array of methods available for text data augmentation that effectively maintain label integrity in practice.",NO,True,387,False,False
235422524-s2,An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,2021-06-14,Token-Level Augmentation,"Token-level augmentations manipulate words and phrases in a sentence to generate augmented text while ideally retaining the semantic meaning and labels of the original text.

Designed Replacement. Intuitively, the semantic meaning of a sentence remains unchanged if some of its tokens are replaced with other tokens that have the same meaning. A simple approach is to fetch synonyms as words for substitutions (Kolomiyets et al., 2011;Yang, 2015;Zhang et al., 2015a;Wei and Zou, 2019;Miao et al., 2020). The synonyms are discovered based on pre-defined dictionaries such as WordNet (Kolomiyets et al., 2011), or similarities in word embedding space (Yang, 2015). However, improvements from this technique are usually minimal (Kolomiyets et al., 2011) and in some cases, performance may even degrade (Zhang et al., 2015a). A major drawback stems from the lack of contextual information when fetching synonyms-especially for words with multiple meanings and few synonyms. To resolve this, language models (LMs) have been used to replace the sampled words given their context (Kolomiyets et al., 2011;Fadaee et al., 2017;Kobayashi, 2018;Kumar et al., 2020). Other work preserves the labels of the text by conditioning on the label when generating the LMs' predictions (Kobayashi, 2018;Wu et al., 2019a). In addition, different sampling strategies for word replacement have been explored. For example, instead of sampling one specific word from candidates by LMs, Gao et al. (2019) propose to compute a weighted average over embeddings of possible words predicted by LMs as the replaced input since the averaged representations could augment text with richer information.

Random Insertion, Replacement, Deletion and Swapping. While well-designed local modifications can preserve the syntax and semantic meaning of a sentence (Niu and Bansal, 2018), random local modifications such as deleting certain tokens (Iyyer et al., 2015;Wei and Zou, 2019;Miao et al., 2020), inserting random tokens (Wei and Zou, 2019;Miao et al., 2020), replacing non-important tokens with random tokens (Xie et al., 2017(Xie et al., , 2020Niu and Bansal, 2018) or randomly swapping tokens in one sentence (Artetxe et al., 2018;Lample et al., 2018;Wei and Zou, 2019;Miao et al., 2020) can preserve the meaning in practice. Different  (2017)  kinds of operations can be further combined (Wei and Zou, 2019), where each example is randomly augmented with one of insertion, deletion, and swapping. These noise-injection methods can efficiently be applied to training, and show improvements when they augment simple models trained on small training sets. However, the improvements might be unstable due to the possibility that random perturbations change the meanings of sentences (Niu and Bansal, 2018). Also, finetuning large pre-trained models on specific tasks might attenuate improvements due to preexisting generalization abilities of the model (Shleifer, 2019).

Compositional Augmentation. To increase the compositional generalization abilities of models, recent efforts have also focused on compositional augmentations (Jia and Liang, 2016; Andreas, 2020) where different fragments from different sentences are re-combined to create augmented examples. Compared to random swapping, compositional augmentation often requires more carefully-designed rules such as lexical overlap (Andreas, 2020), neural-symbolic stack machines (Chen et al., 2020e), and neural program synthesis (Nye et al., 2020). With the potential to greatly improve the generalization abilities to out-of-distribution data, compositional augmentation has been utilized in sequence labeling (Guo et al., 2020), semantic parsing (Andreas, 2020; Nye et al., 2020;Furrer et al., 2020), language modeling (Andreas, 2020; Shaw et al., 2020), and text generation (Feng et al., 2020).",How do language models address the limitations of synonym replacement in token-level augmentation?,"Language models (LMs) have been used to replace sampled words given their context to address the limitations of synonym replacement in token-level augmentation (Kolomiyets et al., 2011; Fadaee et al., 2017; Kobayashi, 2018; Kumar et al., 2020). Other work preserves the labels of the text by conditioning on the label when generating the LMs' predictions (Kobayashi, 2018; Wu et al., 2019a). Additionally, different sampling strategies for word replacement have been explored. For example, instead of sampling one specific word from candidates by LMs, Gao et al. (2019) propose to compute a weighted average over embeddings of possible words predicted by LMs as the replaced input since the averaged representations could augment text with richer information.",How do language models overcome the challenges associated with synonym replacement in token-level augmentation?,"Language models (LMs) address the challenges of synonym replacement in token-level augmentation by replacing sampled words based on their context, as demonstrated in studies by Kolomiyets et al. (2011), Fadaee et al. (2017), Kobayashi (2018), and Kumar et al. (2020). To maintain the integrity of the text's labels, some approaches condition the language model's predictions on the label, as explored by Kobayashi (2018) and Wu et al. (2019a). Furthermore, various sampling strategies for word replacement have been investigated. For instance, instead of selecting a single word from the candidates suggested by LMs, Gao et al. (2019) propose calculating a weighted average over the embeddings of potential words predicted by LMs. This approach enriches the text with more comprehensive information through the averaged representations.",7,4,7,4,"Question:

How do language models address the limitations of synonym replacement in token-level augmentation?

Answer:

Language models (LMs) have been used to replace sampled words given their context to address the limitations of synonym replacement in token-level augmentation (Kolomiyets et al., 2011; Fadaee et al., 2017; Kobayashi, 2018; Kumar et al., 2020). Other work preserves the labels of the text by conditioning on the label when generating the LMs' predictions (Kobayashi, 2018; Wu et al., 2019a). Additionally, different sampling strategies for word replacement have been explored. For example, instead of sampling one specific word from candidates by LMs, Gao et al. (2019) propose to compute a weighted average over embeddings of possible words predicted by LMs as the replaced input since the averaged representations could augment text with richer information.","Question:

How do language models overcome the challenges associated with synonym replacement in token-level augmentation?

Answer:

Language models (LMs) address the challenges of synonym replacement in token-level augmentation by replacing sampled words based on their context, as demonstrated in studies by Kolomiyets et al. (2011), Fadaee et al. (2017), Kobayashi (2018), and Kumar et al. (2020). To maintain the integrity of the text's labels, some approaches condition the language model's predictions on the label, as explored by Kobayashi (2018) and Wu et al. (2019a). Furthermore, various sampling strategies for word replacement have been investigated. For instance, instead of selecting a single word from the candidates suggested by LMs, Gao et al. (2019) propose calculating a weighted average over the embeddings of potential words predicted by LMs. This approach enriches the text with more comprehensive information through the averaged representations.",NO,True,836,True,True
235422524-s3,An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,2021-06-14,Sentence-Level Augmentation,"Instead of modifying tokens, sentence-level augmentation modifies the entire sentence at once.

Paraphrasing. Paraphrasing has been widely adopted as a data augmentation technique in various NLP tasks (Yu et al., 2018;Xie et al., 2020;Kumar et al., 2019;He et al., 2020;Chen et al., 2020b,c;Cai et al., 2020), as it generally provides more diverse augmented text with different word choices and sentence structures while preserving the meaning of the original text. The most popular is round-trip translation (Sennrich et al., 2015;Edunov et al., 2018) -Tavor et al., 2020;Zhang and Bansal, 2019;Kumar et al., 2020;). An extra filtering process is often used to ensure high-quality augmented data. For example, in text classification, Anaby-Tavor et al. (2020) first fine-tune GPT-2 (Radford et al., 2019) with the original examples prepended with their labels, and then generate augmented examples by feeding the finetuned model certain labels. Only confident examples as judged by a baseline classifier trained on the original data are kept. Similarly, new answers are generated on the basis of given questions in question answering and are filtered by customized metrics like question answering probability (Zhang and Bansal, 2019) and n-gram diversity . Generative models used in this setting have been based on conditional VAE (Bowman et al., 2016;Hu et al., 2017;Guu et al., 2017;Malandrakis et al., 2019), GAN (Iyyer et al., 2018;Xu et al., 2018) or pre-trained language models like GPT-2 (Anaby- Tavor et al., 2020;Kumar et al., 2020). Overall, these conditional generation methods can create novel and diverse data that might be unseen in the original dataset, but require significant training effort.  (2020) proposes two simple yet effective adversarial transformations that reverse the position of subject and object or the position of premise and hypothesis.",What are the benefits and challenges of using conditional generation methods for sentence-level data augmentation in NLP?,"Paraphrasing has been widely adopted as a data augmentation technique in various NLP tasks, as it generally provides more diverse augmented text with different word choices and sentence structures while preserving the meaning of the original text. Generative models used in this setting have been based on conditional VAE, GAN, or pre-trained language models like GPT-2. Overall, these conditional generation methods can create novel and diverse data that might be unseen in the original dataset. However, they require significant training effort.",What are the advantages and difficulties associated with using conditional generation methods for sentence-level data augmentation in NLP?,"Conditional generation methods, such as those based on conditional VAE, GAN, or pre-trained language models like GPT-2, offer significant benefits for sentence-level data augmentation in NLP. One of the primary advantages is their ability to produce diverse augmented text by varying word choices and sentence structures while maintaining the original meaning. This diversity can enhance the robustness and performance of NLP models by introducing novel and unseen data into the training set. However, these methods also present challenges, particularly the substantial training effort required to effectively implement them. Despite these challenges, the potential for generating high-quality, varied data makes conditional generation a valuable tool in the NLP toolkit.",7,6,7,4,"Question:

What are the benefits and challenges of using conditional generation methods for sentence-level data augmentation in NLP?

Answer:

Paraphrasing has been widely adopted as a data augmentation technique in various NLP tasks, as it generally provides more diverse augmented text with different word choices and sentence structures while preserving the meaning of the original text. Generative models used in this setting have been based on conditional VAE, GAN, or pre-trained language models like GPT-2. Overall, these conditional generation methods can create novel and diverse data that might be unseen in the original dataset. However, they require significant training effort.","Question:

What are the advantages and difficulties associated with using conditional generation methods for sentence-level data augmentation in NLP?

Answer:

Conditional generation methods, such as those based on conditional VAE, GAN, or pre-trained language models like GPT-2, offer significant benefits for sentence-level data augmentation in NLP. One of the primary advantages is their ability to produce diverse augmented text by varying word choices and sentence structures while maintaining the original meaning. This diversity can enhance the robustness and performance of NLP models by introducing novel and unseen data into the training set. However, these methods also present challenges, particularly the substantial training effort required to effectively implement them. Despite these challenges, the potential for generating high-quality, varied data makes conditional generation a valuable tool in the NLP toolkit.",NO,True,771,True,True
235422524-s6,An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,2021-06-14,Consistency Training with DA,"While data augmentation (DA) can be applied in the supervised setting to produce better results when only a small labeled training dataset is available, data augmentation is also commonly used in semi-supervised learning (SSL). SSL is an alternative approach for learning from limited data that provides a framework for taking advantage of unlabeled data. Specifically, SSL assumes that our training set comprises labeled examples in addition to unlabeled examples drawn from the same distribution. Currently, one of the most common methods for performing SSL with deep neural networks is ""consistency regularization"" (Bachman et al., 2014;Tarvainen and Valpola, 2017). Consistency regularization-based SSL (or ""consistency training"" for short) regularizes a model by enforcing that its output doesn't change significantly when the input is perturbed. In practice, the input is perturbed by applying data augmentation, and consistency is enforced through a loss term that measures the difference between the model's predictions on a clean input and a corresponding perturbed version of the same input.

Formally, let f θ be a model with parameters θ, fθ be a fixed copy of the model where no gradients  are allowed to flow, x l be a labeled datapoint with label y, x u be an unlabeled datapoint, and α(x) be a data augmentation method. Then, a typical loss function for consistency training is

where CE is the cross entropy loss and λ u is a tunable hyperparameter that determines the weight of the consistency regularization term. In practice, various other measures have been used to minimize the difference between fθ(x u ) and f θ (α(x u )), such as the KL divergence (Miyato et al., 2018;Xie et al., 2020) and the mean-squared error (Tarvainen and Valpola, 2017;Laine and Aila, 2017;Berthelot et al., 2019). Because gradients are not allowed to flow through the model when it was fed the clean unlabeled input x u , this objective can be viewed as using the clean unlabeled datapoint to generate a synthetic target distribution for the augmented unlabeled datapoint. Xie et al. (2020) showed that consistency training can be effectively applied to semi-supervised learning for NLP. To achieve stronger results, they introduce several other tricks including confidence thresholding, training signal annealing, and entropy minimization. Confidence thresholding applies the unsupervised loss only when the model assigns a class probability above a pre-defined threshold. Training signal annealing prevents the model from overfitting on easy examples by applying the supervised loss only when the model is less confident about predictions. Entropy minimization trains the model to output low-entropy (highly-confident) predictions when fed unlabeled data. We refer the reader to (Xie et al., 2020) for more details on these tricks.",How is data augmentation utilized in consistency training for semi-supervised learning in NLP?,"Data augmentation is commonly used in semi-supervised learning (SSL) to enhance learning from limited data. SSL provides a framework for leveraging unlabeled data alongside labeled examples drawn from the same distribution. One prevalent method for SSL with deep neural networks is ""consistency regularization,"" which regularizes a model by ensuring its output remains stable when the input is perturbed. In practice, data augmentation is applied to perturb the input, and consistency is enforced through a loss term that measures the difference between the model's predictions on a clean input and a corresponding perturbed version. Various measures, such as KL divergence and mean-squared error, have been used to minimize the difference between the model's predictions on clean and augmented inputs. Because gradients are not allowed to flow through the model when it processes the clean unlabeled input, this objective can be viewed as using the clean unlabeled datapoint to generate a synthetic target distribution for the augmented unlabeled datapoint. Xie et al. (2020) demonstrated that consistency training can be effectively applied to semi-supervised learning for NLP. To achieve stronger results, they introduced additional techniques such as confidence thresholding, training signal annealing, and entropy minimization.",How is data augmentation used in consistency training for semi-supervised learning in natural language processing (NLP)?,"In semi-supervised learning (SSL) for NLP, data augmentation plays a crucial role in enhancing learning from limited labeled data by leveraging unlabeled data. SSL provides a framework where both labeled and unlabeled examples from the same distribution are used to improve model performance. A popular approach within SSL is ""consistency regularization,"" which aims to ensure that a model's output remains stable when the input data is perturbed. Data augmentation is employed to create these perturbations, and a loss term is introduced to measure the difference between the model's predictions on the original input and its augmented version. Techniques such as KL divergence and mean-squared error are used to minimize this difference, ensuring consistency between predictions on clean and augmented inputs. Importantly, gradients are not allowed to flow through the model when processing the clean unlabeled input, effectively using it to generate a synthetic target distribution for the augmented input. Xie et al. (2020) demonstrated the effectiveness of consistency training in semi-supervised learning for NLP. To further enhance results, they incorporated additional techniques like confidence thresholding, training signal annealing, and entropy minimization.",7,2,7,3,"Question:

How is data augmentation utilized in consistency training for semi-supervised learning in NLP?

Answer:

Data augmentation is commonly used in semi-supervised learning (SSL) to enhance learning from limited data. SSL provides a framework for leveraging unlabeled data alongside labeled examples drawn from the same distribution. One prevalent method for SSL with deep neural networks is ""consistency regularization,"" which regularizes a model by ensuring its output remains stable when the input is perturbed. In practice, data augmentation is applied to perturb the input, and consistency is enforced through a loss term that measures the difference between the model's predictions on a clean input and a corresponding perturbed version. Various measures, such as KL divergence and mean-squared error, have been used to minimize the difference between the model's predictions on clean and augmented inputs. Because gradients are not allowed to flow through the model when it processes the clean unlabeled input, this objective can be viewed as using the clean unlabeled datapoint to generate a synthetic target distribution for the augmented unlabeled datapoint. Xie et al. (2020) demonstrated that consistency training can be effectively applied to semi-supervised learning for NLP. To achieve stronger results, they introduced additional techniques such as confidence thresholding, training signal annealing, and entropy minimization.","Question:

How is data augmentation used in consistency training for semi-supervised learning in natural language processing (NLP)?

Answer:

In semi-supervised learning (SSL) for NLP, data augmentation plays a crucial role in enhancing learning from limited labeled data by leveraging unlabeled data. SSL provides a framework where both labeled and unlabeled examples from the same distribution are used to improve model performance. A popular approach within SSL is ""consistency regularization,"" which aims to ensure that a model's output remains stable when the input data is perturbed. Data augmentation is employed to create these perturbations, and a loss term is introduced to measure the difference between the model's predictions on the original input and its augmented version. Techniques such as KL divergence and mean-squared error are used to minimize this difference, ensuring consistency between predictions on clean and augmented inputs. Importantly, gradients are not allowed to flow through the model when processing the clean unlabeled input, effectively using it to generate a synthetic target distribution for the augmented input. Xie et al. (2020) demonstrated the effectiveness of consistency training in semi-supervised learning for NLP. To further enhance results, they incorporated additional techniques like confidence thresholding, training signal annealing, and entropy minimization.",NO,True,1270,True,True
253736389-s10,Transformers for Tabular Data Representation: A Survey of Models and Applications,2023-03-01,Vanilla Transformer,"The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Residual connections and layer-normalization modules are also used. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions.

The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only , or decoder-only (Radford et al., 2019;Brown et al., 2020) model. The choice of the architecture depends on the final task. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).",How is the vanilla transformer architecture structured and what are its key components?,"The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Residual connections and layer-normalization modules are also used. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions. The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017; Raffel et al., 2020), an encoder-only, or decoder-only (Radford et al., 2019; Brown et al., 2020) model. The choice of the architecture depends on the final task. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).","What is the structure of the vanilla transformer architecture, and what are its key components?","The vanilla transformer architecture, introduced by Vaswani et al. in 2017, is a sequence-to-sequence (seq2seq) model that consists of an encoder and a decoder, each built from a stack of N identical modules. The encoder block features a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention enables the model to simultaneously focus on information from various representation subspaces at different positions. Additionally, residual connections and layer normalization are incorporated to enhance the model's performance.

In the decoder blocks, cross-attention modules are integrated between the multi-head self-attention modules and the position-wise feed-forward networks. Masking is applied to ensure that each position does not attend to subsequent positions, maintaining the autoregressive property of the decoder.

The transformer architecture can be adapted for different tasks: as an encoder-decoder model (Vaswani et al., 2017; Raffel et al., 2020), an encoder-only model, or a decoder-only model (Radford et al., 2019; Brown et al., 2020). The choice of architecture depends on the specific task at hand. Encoder-only models are primarily used for classification tasks and are popular for extensions involving tabular data. In these cases, pretraining is conducted using a masked language modeling (MLM) task, which aims to predict masked tokens in an altered input. The encoder-decoder architecture is typically employed for models focused on sequence generation tasks, such as RPT and TAPEX.",9,2,9,2,"Question:

How is the vanilla transformer architecture structured and what are its key components?

Answer:

The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Residual connections and layer-normalization modules are also used. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions. The transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017; Raffel et al., 2020), an encoder-only, or decoder-only (Radford et al., 2019; Brown et al., 2020) model. The choice of the architecture depends on the final task. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).","Question:

What is the structure of the vanilla transformer architecture, and what are its key components?

Answer:

The vanilla transformer architecture, introduced by Vaswani et al. in 2017, is a sequence-to-sequence (seq2seq) model that consists of an encoder and a decoder, each built from a stack of N identical modules. The encoder block features a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention enables the model to simultaneously focus on information from various representation subspaces at different positions. Additionally, residual connections and layer normalization are incorporated to enhance the model's performance.

In the decoder blocks, cross-attention modules are integrated between the multi-head self-attention modules and the position-wise feed-forward networks. Masking is applied to ensure that each position does not attend to subsequent positions, maintaining the autoregressive property of the decoder.

The transformer architecture can be adapted for different tasks: as an encoder-decoder model (Vaswani et al., 2017; Raffel et al., 2020), an encoder-only model, or a decoder-only model (Radford et al., 2019; Brown et al., 2020). The choice of architecture depends on the specific task at hand. Encoder-only models are primarily used for classification tasks and are popular for extensions involving tabular data. In these cases, pretraining is conducted using a masked language modeling (MLM) task, which aims to predict masked tokens in an altered input. The encoder-decoder architecture is typically employed for models focused on sequence generation tasks, such as RPT and TAPEX.",NO,True,1547,True,True
251402499-s8,Abstractive Meeting Summarization: A Survey,2022-08-08,Evaluation methods,"As noted in Section 2, the subjectivity of meeting summaries, stemming in part from the preference for abstractive summarization, complicates the evaluation task. Evaluation requires both verifying that the content in a system-based summary follows from the original transcript and measuring the overlap with a gold-standard summary. However, given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used.

Unfortunately, the ROUGE metric (Lin, 2004), which remains the standard for both meeting and general text summarization, scores systemproduced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries. If we take the abstractive summary from Figure 1 (""Some part of the casing will be made of a spongy material"") as a gold example, ROUGE would assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" than one that output ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false).

A reasonable way to try to improve over ROUGE would be to take advantage of massive, pretrained, contextual word embeddings and a notion of lexical similarity rather than strict lexical overlap. Some recent efforts pursue this direction (Sai et al., 2022), including BERTScore (Zhang et al., 2020a) and MoverScore (Zhao et al., 2019a), which aim to measure the semantic distance between the contextualized mapping of a generated summary and the reference, or BARTScore (Yuan et al., 2021), which calculates the log-likelihood of a summary to have been generated, motivated by the fact that a good summary should have a high probability of being generated from a source text. Building upon these methods, DATScore and FrugalScore (Eddine et al., 2022;Kamal Eddine et al., 2022) incorporate data augmentation and knowledge distillation techniques to further improve performance and overcome their drawbacks.

Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range, e.g., there is no consensus among metrics regarding which summary is better than another in the high scoring range in which modern systems now operate. Bhandari et al. (2020b) argue that there is no one-size-fitsall metric that correlates better with human judgement than the others, and that can outperform others on all datasets.

Clearly, evaluation is itself a very challenging task. And we note that none of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency. When we summarize a meeting or detail decisions and actions items in our own words, it is important to get the facts straight. Otherwise, the resulting summaries are not reliable for an end user. While we do not know of current work that focuses on evaluation of factuality explicitly for the meeting domain, the study of factual consistency in summarization more generally is a budding research area that we discuss in Section 6.

In the absence of a clear winner for summary evaluation metrics, none of the alternatives has yet to be widely adopted. In our comparison of different summarization systems in Section 5, we therefore stick with ROUGE, which offers the additional advantage of conceptual simplicity and lower computational cost compared to metrics based on contextual embeddings and pretrained language models.",What are the challenges and limitations of using ROUGE for evaluating abstractive meeting summaries?,"The ROUGE metric scores system-produced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries. Given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used. For example, ROUGE would assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" than one that outputs ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false). Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range. Bhandari et al. (2020b) argue that there is no one-size-fits-all metric that correlates better with human judgment than the others, and that can outperform others on all datasets. Clearly, evaluation is itself a very challenging task. None of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency. When summarizing a meeting or detailing decisions and action items in one's own words, it is important to get the facts straight. Otherwise, the resulting summaries are not reliable for an end user.",What are the challenges and limitations of using the ROUGE metric for evaluating abstractive meeting summaries?,"The ROUGE metric presents several challenges and limitations when used to evaluate abstractive meeting summaries. Primarily, ROUGE scores system-generated summaries based solely on surface-level lexicographic matches with a typically single gold summary. This approach is not ideal for assessing abstractive summaries, which encourage expressive freedom and require a deeper semantic understanding. For instance, ROUGE might assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" over one that outputs ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former is incorrect and misleading.

Recent studies on the meta-evaluation of these metrics, such as those by Peyrard (2019) and Bhandari et al. (2020a), show mixed results. They compare the performance of various metrics in the context of document summarization and reveal that these metrics often strongly disagree when ranking summaries within a narrow scoring range. Furthermore, Bhandari et al. (2020b) argue that no single metric consistently correlates better with human judgment across all datasets, highlighting the complexity of evaluation itself.

Another significant challenge for summary evaluation is ensuring factual consistency. When summarizing a meeting or detailing decisions and action items in one's own words, it is crucial to maintain factual accuracy. Without this, the resulting summaries may not be reliable for end users. Therefore, while ROUGE and similar metrics have their uses, they fall short in capturing the nuanced requirements of evaluating abstractive meeting summaries.",7,4,7,4,"Question:

What are the challenges and limitations of using ROUGE for evaluating abstractive meeting summaries?

Answer:

The ROUGE metric scores system-produced summaries based purely on surface lexicographic matches with a (usually single) gold summary, making it unideal for assessing abstractive summaries. Given the expressive freedom encouraged by abstractive approaches, checking for semantic entailment and overlap of summary-worthy content requires deep semantic understanding, not just recognition that the same words are used. For example, ROUGE would assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" than one that outputs ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former says something very different (and false). Despite their rapid development, recent studies on meta-evaluation of these metrics show mixed results. Peyrard (2019) and Bhandari et al. (2020a) compare their performance in the context of document summarization and show that metrics strongly disagree in ranking summaries from any narrow scoring range. Bhandari et al. (2020b) argue that there is no one-size-fits-all metric that correlates better with human judgment than the others, and that can outperform others on all datasets. Clearly, evaluation is itself a very challenging task. None of these metrics even touches on another central challenge for summary evaluation, namely that of factual consistency. When summarizing a meeting or detailing decisions and action items in one's own words, it is important to get the facts straight. Otherwise, the resulting summaries are not reliable for an end user.","Question:

What are the challenges and limitations of using the ROUGE metric for evaluating abstractive meeting summaries?

Answer:

The ROUGE metric presents several challenges and limitations when used to evaluate abstractive meeting summaries. Primarily, ROUGE scores system-generated summaries based solely on surface-level lexicographic matches with a typically single gold summary. This approach is not ideal for assessing abstractive summaries, which encourage expressive freedom and require a deeper semantic understanding. For instance, ROUGE might assign a higher score to a system that produces ""Some part of the casing will be made of broccoli"" over one that outputs ""A portion of the outer layer will be constructed from a sponge-like material,"" even though the latter is a perfect reformulation of the gold summary, while the former is incorrect and misleading.

Recent studies on the meta-evaluation of these metrics, such as those by Peyrard (2019) and Bhandari et al. (2020a), show mixed results. They compare the performance of various metrics in the context of document summarization and reveal that these metrics often strongly disagree when ranking summaries within a narrow scoring range. Furthermore, Bhandari et al. (2020b) argue that no single metric consistently correlates better with human judgment across all datasets, highlighting the complexity of evaluation itself.

Another significant challenge for summary evaluation is ensuring factual consistency. When summarizing a meeting or detailing decisions and action items in one's own words, it is crucial to maintain factual accuracy. Without this, the resulting summaries may not be reliable for end users. Therefore, while ROUGE and similar metrics have their uses, they fall short in capturing the nuanced requirements of evaluating abstractive meeting summaries.",NO,True,1714,True,True
251402499-s11,Abstractive Meeting Summarization: A Survey,2022-08-08,Discursive information,"While most work on abstractive meeting summarization implicitly assumes that conversation is merely a linear sequence of utterances without semantic relations between them, example (1) underscores the importance of semantic relations for conversational understanding. Drawing on similar insights, certain recent approaches exploit independent theories of discourse structure, such as Rhetorical Structure Theory (RST; Mann and Thompson, 1987) and Segmented Discourse Representation Theory (SDRT; Asher, 1993;Lascarides and Asher, 2008), to improve summarization. Accounts like RST and SDRT maintain that in a coherent conversation, each (roughly) clause-level unit should be semantically related to some other part of the conversation via a discourse relation such as Question-Answer Pair (QAP), Acknowledgement (Ack), Explanation, Contrast, etc. to reflect its contribution to the larger discourse. Each coherent discourse can thus be represented as a weakly-connected graph whose edges are labeled with discourse relations. Figure 2 il-  , 2021a). The * indicates summary-level (with sentence split) ROUGE-L score (Lin, 2004).

lustrates a possible SDRT graph for example (1).

To the best of our knowledge, Feng et al. (2020) is the first work to exploit discourse graphs to generate abstractive meeting summaries. They employ a sequential discourse parser (Shi and Huang, 2019) trained on the STAC corpus of multi-party chats (Asher et al., 2016) to automatically obtain discourse graphs for the AMI and ICSI meeting corpora. Levi graph transformation (Gross and Yellen, 2003) is then used to turn graph edges labeled with discourse relation types into vertices. Their graph-to-sequence model consists of a graph convolutional network encoder (Schlichtkrull et al., 2018) that takes a meeting discourse graph as input and a PGN decoder (See et al., 2017) to generate the final summary. A dialogue-aware data augmentation strategy for constructing pseudo-summaries is introduced to pretrain the model.

An alternative approach to discourse interpretation that developed largely independently of RST and SDRT is dialogue act classification. Detailing the differences between the approaches is out of the scope of this paper, but in a nutshell, dialogue acts provide a shallower notion of discourse structure (Jurafsky et al., 1997) in that they do not entail a full graph structure over a conversation. On the other hand, systems for dialogue act labeling, such as DAMSL (Allen and Core, 1997; Core and Allen, 1997) or DiAML (Bunt et al., 2010(Bunt et al., , 2012, place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting or informing.

Both the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.

Dialogue acts have also been used to good effect for summarizing decisions. Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution. Then, key fragments of the decision related utterances are retained to form an extractive decision summary. Similar ideas can also be found in the literature on detecting and summarizing action-item-specific dialogue acts in meetings (Purver et al., 2006(Purver et al., , 2007.","How are dialogue acts utilized to enhance abstractive meeting summarization, particularly in decision-related contexts?","Dialogue acts provide a shallower notion of discourse structure and place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting, or informing. Both the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder. Dialogue acts have also been used to good effect for summarizing decisions. Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution. Then, key fragments of the decision-related utterances are retained to form an extractive decision summary.","How do dialogue acts contribute to improving abstractive meeting summarization, especially in contexts involving decisions?","Dialogue acts play a crucial role in enhancing abstractive meeting summarization by providing a framework for understanding the interactive elements of discourse, such as stalling to maintain control of the conversation, evaluating other participants' contributions, suggesting ideas, or providing information. The AMI and ICSI corpora offer gold-standard dialogue act labels, which have been utilized by researchers like Goo and Chen (2018) to develop a sentence-gated mechanism. This mechanism models the relationships between dialogue acts and topic summaries, thereby improving the quality of abstractive meeting summarization. Their research highlights that dialogue acts categorized as ""inform"" are more likely to contain summary-worthy content compared to acts like ""stall"" or ""assess."" Their model, which employs LSTM, includes three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.

Furthermore, dialogue acts have proven effective in summarizing decisions. Studies by Fernández et al. (2008) and Bui et al. (2009) focus on identifying decision-related discussions within meetings and classifying them into decision-related dialogue acts. These acts encompass the issue under discussion, its resolution, or agreement with the proposed resolution. By retaining key fragments of these decision-related utterances, they form an extractive decision summary, thereby enhancing the summarization process in decision-making contexts.",7,4,7,4,"Question:

How are dialogue acts utilized to enhance abstractive meeting summarization, particularly in decision-related contexts?

Answer:

Dialogue acts provide a shallower notion of discourse structure and place more emphasis on interactive acts such as stalling to hold the floor, assessing other discourse moves, suggesting, or informing. Both the AMI and ICSI corpora provide gold dialogue act labels, and Goo and Chen (2018) use the labels from the AMI corpus to develop a sentence-gated mechanism that jointly models the relationships between dialogue acts and topic summaries to improve abstractive meeting summarization. In particular, they show that dialogue acts of the type inform are more closely linked to summary-worthy material than acts such as stall or assess. Using LSTM, their model consists of three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder. Dialogue acts have also been used to good effect for summarizing decisions. Fernández et al. (2008) and Bui et al. (2009) first identify relevant areas of decision-related discussion in meetings and classify them as decision-related dialogue acts including the issue under discussion, its resolution, or agreement with the proposed resolution. Then, key fragments of the decision-related utterances are retained to form an extractive decision summary.","Question:

How do dialogue acts contribute to improving abstractive meeting summarization, especially in contexts involving decisions?

Answer:

Dialogue acts play a crucial role in enhancing abstractive meeting summarization by providing a framework for understanding the interactive elements of discourse, such as stalling to maintain control of the conversation, evaluating other participants' contributions, suggesting ideas, or providing information. The AMI and ICSI corpora offer gold-standard dialogue act labels, which have been utilized by researchers like Goo and Chen (2018) to develop a sentence-gated mechanism. This mechanism models the relationships between dialogue acts and topic summaries, thereby improving the quality of abstractive meeting summarization. Their research highlights that dialogue acts categorized as ""inform"" are more likely to contain summary-worthy content compared to acts like ""stall"" or ""assess."" Their model, which employs LSTM, includes three components enhanced with various attention mechanisms: an utterance encoder, a dialogue act labeler, and a summary decoder.

Furthermore, dialogue acts have proven effective in summarizing decisions. Studies by Fernández et al. (2008) and Bui et al. (2009) focus on identifying decision-related discussions within meetings and classifying them into decision-related dialogue acts. These acts encompass the issue under discussion, its resolution, or agreement with the proposed resolution. By retaining key fragments of these decision-related utterances, they form an extractive decision summary, thereby enhancing the summarization process in decision-making contexts.",NO,True,1510,True,True
251402499-s19,Abstractive Meeting Summarization: A Survey,2022-08-08,Long input processing,"A straightforward solution to the length problem is to segment a long document into smaller segments to be processed. Koay et al. (2021) separate ICSI meetings with a sliding window, and then apply BART on each segment to produce smaller summaries, which are concatenated into an overall extractive summary. Zhang et al. (2022) propose SUMM N , a multi-stage split-thensummarize framework. Within each stage, it first splits the source input into sufficiently short segments. Coarse summaries are then generated for each segment and then concatenated as the input to the next stage. This process is conducted repetitively until a final, fine-grained abstractive meeting summary is produced.

While such segmenting approaches address the length problem, they can lose important crosspartition information (Beltagy et al., 2020), a risk that has led researchers to seek more sophisticated solutions to the length problem. Long-sequence transformers. Multiple variants of adapting transformer-based approaches to address the lengthy input problem exist in the literature (Dai et al., 2019;Beltagy et al., 2020;Martins et al., 2022). Longformer (Beltagy et al., 2020), for example, introduces a multi-layer selfattention operation that scales linearly with sequence length, enabling it to process long meet-ing transcriptions. Although these models are not initially proposed for abstractive meeting summarization, recent benchmarks show their promise (Fabbri et al., 2021) in comparison with certain dedicated systems. Hierarchical transformers. Some systems leverage transformers in a hierarchical manner, breaking down a long meeting transcript to multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors. That sequence is then processed by the turn-level encoder. The transformer decoder makes use of both levels of representation via cross-attention layers. Rohde et al. (2021) propose Hierachical Attention Transformer (HAT). Utterances are first prepended with a special BOS token. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations. Finally, the decoder leverages the outputs at both levels to produce a final summary. Similarly, the hierarchical encoder of (Zhao et al., 2019b) consists of three levels, sequentially encoding word, utterance and topic segment embeddings.",How do hierarchical transformers utilize different levels of representation to process long meeting transcripts?,"Hierarchical transformers leverage transformers in a hierarchical manner, breaking down a long meeting transcript into multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors. That sequence is then processed by the turn-level encoder. The transformer decoder makes use of both levels of representation via cross-attention layers. Rohde et al. (2021) propose Hierarchical Attention Transformer (HAT). Utterances are first prepended with a special BOS token. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations. Finally, the decoder leverages the outputs at both levels to produce a final summary. Similarly, the hierarchical encoder of Zhao et al. (2019b) consists of three levels, sequentially encoding word, utterance, and topic segment embeddings.",How do hierarchical transformers process long meeting transcripts using different levels of representation?,"Hierarchical transformers process long meeting transcripts by utilizing a multi-level approach that mirrors the inherent hierarchical structure of text. This involves breaking down the transcript into shorter sequences at different levels, such as words forming utterances and utterances forming the entire transcription (Yang et al., 2016). For instance, the HMNet model, introduced by Zhu et al. (2020), employs a two-level structure. Initially, each utterance in the meeting is encoded separately using a word-level transformer encoder, resulting in a sequence of utterance vectors. These vectors are then processed by a turn-level encoder. The transformer decoder integrates both levels of representation through cross-attention layers.

Similarly, the Hierarchical Attention Transformer (HAT) proposed by Rohde et al. (2021) begins by prepending utterances with a special BOS token. After obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an additional layer to produce sentence-level representations. The decoder then utilizes outputs from both levels to generate a final summary. Another approach by Zhao et al. (2019b) involves a hierarchical encoder with three levels, sequentially encoding word, utterance, and topic segment embeddings.",7,4,7,4,"Question:

How do hierarchical transformers utilize different levels of representation to process long meeting transcripts?

Answer:

Hierarchical transformers leverage transformers in a hierarchical manner, breaking down a long meeting transcript into multiple relatively shorter sequences of different levels, mirroring the underlying hierarchical structure of text (Yang et al., 2016), i.e., words combine into an utterance, and utterances form a transcription. The HMNet model, proposed by Zhu et al. (2020), follows a two-level structure. First, each utterance in the meeting is separately encoded by the same word-level transformer encoder, resulting in a sequence of utterance vectors. That sequence is then processed by the turn-level encoder. The transformer decoder makes use of both levels of representation via cross-attention layers. Rohde et al. (2021) propose Hierarchical Attention Transformer (HAT). Utterances are first prepended with a special BOS token. Then, after obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an extra layer, yielding sentence-level representations. Finally, the decoder leverages the outputs at both levels to produce a final summary. Similarly, the hierarchical encoder of Zhao et al. (2019b) consists of three levels, sequentially encoding word, utterance, and topic segment embeddings.","Question:

How do hierarchical transformers process long meeting transcripts using different levels of representation?

Answer:

Hierarchical transformers process long meeting transcripts by utilizing a multi-level approach that mirrors the inherent hierarchical structure of text. This involves breaking down the transcript into shorter sequences at different levels, such as words forming utterances and utterances forming the entire transcription (Yang et al., 2016). For instance, the HMNet model, introduced by Zhu et al. (2020), employs a two-level structure. Initially, each utterance in the meeting is encoded separately using a word-level transformer encoder, resulting in a sequence of utterance vectors. These vectors are then processed by a turn-level encoder. The transformer decoder integrates both levels of representation through cross-attention layers.

Similarly, the Hierarchical Attention Transformer (HAT) proposed by Rohde et al. (2021) begins by prepending utterances with a special BOS token. After obtaining token-level embeddings with a standard transformer encoder, the BOS token embeddings are fed into an additional layer to produce sentence-level representations. The decoder then utilizes outputs from both levels to generate a final summary. Another approach by Zhao et al. (2019b) involves a hierarchical encoder with three levels, sequentially encoding word, utterance, and topic segment embeddings.",NO,True,1304,True,True
256662721-s2,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems",2023-02-07,Training Strategies of LMRS,"Given the significant impact that PLMs have had on NLP tasks in the pre-train and fine-tune paradigm, there has been a surge recently in adapting such paradigms to multiple recommendation tasks. As illustrated in Figure 1, there are mainly two classes regarding different training paradigms: pre-train, fine-tune paradigm and prompt learning paradigm. Each class is further classified into subclasses regarding different training efforts on different parts of the recommendation model. This section will go through various training strategies w.r.t. specific recommendation purposes. Figure  2(a) presents the statistics of recent publications of LMRSs grouped by different training strategies and the total number of published research works each year. Figure 2(b) shows the taxonomy and some corresponding representative LMRSs.

4.1 Pre-train, fine-tune paradigm for RS

The ""pre-train, fine-tune"" paradigm attracts increasing attention from researchers in the recommendation field due to several advantages: 1) Pre-training provides a better model initialization, which usually leads to better generalization on different downstream recommendation tasks, improves recommendation performance from various perspectives, and speeds up convergence on the fine-tuning stage; 2) Pre-training on huge source corpus can learn universal knowledge which can be beneficial for the downstream recommenders; 3) Pre-training can be regarded as a kind of regularization to avoid overfitting on lowresource, and small datasets (Erhan et al., 2010).

Pre-train This training strategy can be seen as traditional end-to-end training with domain input. Differently, we only focus on research works adapting LM-based learning objectives into the training phase. Many typical LM-based RSs fall into this category, such as BERT4Rec , which models sequential user behaviour with a bidirectional self-attention network through Cloze task, and Transformers4Rec (de Souza Pereira Moreira et al., 2021) which adopts a haggingface transformer-based architecture as the base model for next-item prediction and explores four different LM tasks, namely Causal LM, MLM, Permutation LM, and Replacement Token Detection during training. These two models laid the foundation for LM-based recommender systems and have become popular baselines for their successors.

Pre-train, fine-tune holistic model Under this category, the model is pre-trained and fine-tuned with different data sources, and the fine-tuning process will go through adjusting the whole model parameters. The learning objectives can also vary between the pre-training and fine-tuning stages. Pre-training and fine-tuning with different domains of data sources, also called cross-domain recommendation, can refer to the works of Kang et al. (2021) and Qiu et al. (2021). Kang et al. (2021) pre-trained a GPT model using segmented source API code and fine-tuned it with API code snippets from another library for cross-library recommendation. Wang et al. (2022a) fine-tuned the pre-trained DialoGPT model on domain-specific datasets for conversational recommendation together with an R-GCN model to inject knowledge from DBpedia to enhance recommendation perfor- mance. Xiao et al. (2022) fine-tuned the PTM to learn news embedding together with a user embedding part in an auto-regressive manner for news recommendation. They also explored different fine-tuning strategies like tuning part of the PTM and tuning the last layer of the PTM but empirically found fine-tuning the whole model resulted in better performance, which gives us an insight into balancing the recommendation accuracy and training efficiency.

Pre-train, fine-tune partial model Since finetuning the whole model is usually time-consuming and less flexible, many LMRSs choose to fine-tune partial parameters of the model to achieve a balance between training overhead and recommendation performance (Hou et al., 2022;Yu et al., 2022;Wu et al., 2022a). For instance, to deal with the domain bias problem that BERT induces a non-smooth anisotropic semantic space for general texts resulting in a large language gap for texts from different domains of items, Hou et al. (2022) applied a linear transformation layer to transform BERT representations of items from different domains followed by an adaptive combination strategy to derive a universal item representation. Meanwhile, considering the seesaw phenomenon that learning from multiple domainspecific behavioural patterns can be a conflict, they proposed sequence-item and sequence-sequence contrastive tasks for multi-task learning during the pre-training stage. They found only fine-tuning a small proportion of model parameters could quickly adapt the model to unseen domains with cold-start or new items. Pre-train, fine-tune extra part of the model With the increase in the depth of PTMs, the representation captured by them makes the downstream recommendation easier. Apart from the aforementioned two fine-tuning strategies, some works leverage a task-specific layer on top of the PTMs for recommendation tasks. Fine-tuning only goes through such extra parts of the PTMs by optimizing the parameters of the task-specific layer. Shang et al. (2019) pre-trained a GPT and a BERT model to learn patient visit embeddings, which were then used as input to fine-tune the extra prediction layer for medication recommendation. Another approach is to use the PTM to initialize a new model with a similar architecture in the finetuning stage, and the fine-tuned model is used for recommendations. In , a bidirectional Transformer-based model was first pretrained on four different self-supervised learning objectives (associated attribute prediction, masked item prediction, masked attribute prediction and segment prediction) to learn item embeddings. Then, the learned model parameters were adopted to initialize a unidirectional Transformer-based model for fine-tuning with pairwise rank loss for recommendation. In (McKee et al., 2023), the authors leveraged the pre-trained BLOOM-176B to generate natural languages descriptions of music given a set of music tags. Subsequently, two distinct pre-trained models, namely CLIP and the D2T pipeline, were employed to initialize textual, video, and audio representations of the provided music content. Following this, a transformerbased architecture model was fine-tuned for multimodal music recommendation.","How is the ""pre-train, fine-tune extra part of the model"" strategy applied in recommendation systems?","With the increase in the depth of pre-trained models (PTMs), the representation captured by them makes the downstream recommendation easier. Apart from the aforementioned two fine-tuning strategies, some works leverage a task-specific layer on top of the PTMs for recommendation tasks. Fine-tuning only goes through such extra parts of the PTMs by optimizing the parameters of the task-specific layer. Shang et al. (2019) pre-trained a GPT and a BERT model to learn patient visit embeddings, which were then used as input to fine-tune the extra prediction layer for medication recommendation. Another approach is to use the PTM to initialize a new model with a similar architecture in the fine-tuning stage, and the fine-tuned model is used for recommendations. In one example, a bidirectional Transformer-based model was first pre-trained on four different self-supervised learning objectives (associated attribute prediction, masked item prediction, masked attribute prediction, and segment prediction) to learn item embeddings. Then, the learned model parameters were adopted to initialize a unidirectional Transformer-based model for fine-tuning with pairwise rank loss for recommendation. In another example, McKee et al. (2023) leveraged the pre-trained BLOOM-176B to generate natural language descriptions of music given a set of music tags. Subsequently, two distinct pre-trained models, namely CLIP and the D2T pipeline, were employed to initialize textual, video, and audio representations of the provided music content. Following this, a transformer-based architecture model was fine-tuned for multimodal music recommendation.","How is the ""pre-train, fine-tune extra part of the model"" strategy utilized in recommendation systems?","The ""pre-train, fine-tune extra part of the model"" strategy is increasingly applied in recommendation systems due to the depth and capability of pre-trained models (PTMs) to capture rich representations, which simplify downstream recommendation tasks. Beyond the basic fine-tuning strategies, some approaches incorporate a task-specific layer on top of PTMs specifically for recommendation tasks. This involves fine-tuning by optimizing only the parameters of this task-specific layer. For instance, Shang et al. (2019) pre-trained GPT and BERT models to learn patient visit embeddings, which were then used as inputs to fine-tune an additional prediction layer for medication recommendations.

Another method involves using the PTM to initialize a new model with a similar architecture during the fine-tuning stage, which is then used for recommendations. An example of this is a bidirectional Transformer-based model that was pre-trained on four different self-supervised learning objectives—associated attribute prediction, masked item prediction, masked attribute prediction, and segment prediction—to learn item embeddings. The learned model parameters were then used to initialize a unidirectional Transformer-based model, which was fine-tuned with pairwise rank loss for recommendation.

Additionally, McKee et al. (2023) utilized the pre-trained BLOOM-176B model to generate natural language descriptions of music based on a set of music tags. Following this, two distinct pre-trained models, CLIP and the D2T pipeline, were used to initialize textual, video, and audio representations of the provided music content. A transformer-based architecture model was then fine-tuned for multimodal music recommendation.",7,4,7,4,"Question:

How is the ""pre-train, fine-tune extra part of the model"" strategy applied in recommendation systems?

Answer:

With the increase in the depth of pre-trained models (PTMs), the representation captured by them makes the downstream recommendation easier. Apart from the aforementioned two fine-tuning strategies, some works leverage a task-specific layer on top of the PTMs for recommendation tasks. Fine-tuning only goes through such extra parts of the PTMs by optimizing the parameters of the task-specific layer. Shang et al. (2019) pre-trained a GPT and a BERT model to learn patient visit embeddings, which were then used as input to fine-tune the extra prediction layer for medication recommendation. Another approach is to use the PTM to initialize a new model with a similar architecture in the fine-tuning stage, and the fine-tuned model is used for recommendations. In one example, a bidirectional Transformer-based model was first pre-trained on four different self-supervised learning objectives (associated attribute prediction, masked item prediction, masked attribute prediction, and segment prediction) to learn item embeddings. Then, the learned model parameters were adopted to initialize a unidirectional Transformer-based model for fine-tuning with pairwise rank loss for recommendation. In another example, McKee et al. (2023) leveraged the pre-trained BLOOM-176B to generate natural language descriptions of music given a set of music tags. Subsequently, two distinct pre-trained models, namely CLIP and the D2T pipeline, were employed to initialize textual, video, and audio representations of the provided music content. Following this, a transformer-based architecture model was fine-tuned for multimodal music recommendation.","Question:

How is the ""pre-train, fine-tune extra part of the model"" strategy utilized in recommendation systems?

Answer:

The ""pre-train, fine-tune extra part of the model"" strategy is increasingly applied in recommendation systems due to the depth and capability of pre-trained models (PTMs) to capture rich representations, which simplify downstream recommendation tasks. Beyond the basic fine-tuning strategies, some approaches incorporate a task-specific layer on top of PTMs specifically for recommendation tasks. This involves fine-tuning by optimizing only the parameters of this task-specific layer. For instance, Shang et al. (2019) pre-trained GPT and BERT models to learn patient visit embeddings, which were then used as inputs to fine-tune an additional prediction layer for medication recommendations.

Another method involves using the PTM to initialize a new model with a similar architecture during the fine-tuning stage, which is then used for recommendations. An example of this is a bidirectional Transformer-based model that was pre-trained on four different self-supervised learning objectives—associated attribute prediction, masked item prediction, masked attribute prediction, and segment prediction—to learn item embeddings. The learned model parameters were then used to initialize a unidirectional Transformer-based model, which was fine-tuned with pairwise rank loss for recommendation.

Additionally, McKee et al. (2023) utilized the pre-trained BLOOM-176B model to generate natural language descriptions of music based on a set of music tags. Following this, two distinct pre-trained models, CLIP and the D2T pipeline, were used to initialize textual, video, and audio representations of the provided music content. A transformer-based architecture model was then fine-tuned for multimodal music recommendation.",NO,True,1720,True,True
256662721-s3,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems",2023-02-07,Prompting paradigm for RSs,"Instead of adapting PLMs to different downstream recommendation tasks by designing specific objective functions, a rising trend in recent years is to use the ""pre-train, prompt, and inference"" paradigm to reformulate downstream recommendations through hard/soft prompts. In this paradigm, fine-tuning can be avoided, and the pretrained model itself can be directly employed to predict item ratings, generate top-k item ranking lists, make conversations, recommend similar libraries for programmers while coding, or even output subtasks related to recommendation targets such as explanations (Li et al., 2023b). Prompt learning breaks through the problem of data constraints and bridges the gap of objective forms between pre-training and fine-tuning. Fixed-PTM prompt tuning Prompt-tuning only requires tuning a small set of parameters for the prompts and labels, which is especially efficient for few-shot recommendation tasks. Despite the promising results achieved through constructing prompt information without significantly changing the structure and parameters of PTMs, it also calls for the necessity of choosing the most appropriate prompt template and verbalizer, which can greatly impact recommendation performance. Prompt tuning can be both in the form of discrete textual templates (Penha and Hauff, 2020), which are more human-readable, and soft continuous vectors (Wang et al., 2022c;Wu et al., 2022b). For instance, Penha and Hauff (2020) manually designed several prompt templates to test the performance of movie/book recommendations on a pre-trained BERT model with a similarity measure. Wu et al. (2022b) proposed a personalized prompt generator tuned to generate a soft prompt as a prefix before the user behaviour sequence for sequential recommendation. Fixed-prompt PTM tuning Fixed-prompt PTM tuning tunes the parameters of PTMs similarly to the ""pre-train, fine-tune"" strategy but additionally uses prompts with fixed parameters to steer the recommendation task. One natural way is to use artificially designed discrete prompt to specify recommendation items. For instance, Zhang et al.

(2021b) designed a prompt ""A user watched item A, item B, and item C. Now the user may want to watch () "" to reformulate the recommendation as a multi-token cloze task during fine-tuning of the LM-based PTM. The prompts can also be one or several tokens/words to seamlessly shift/lead the conversations from various tasks. Deng et al. (2023)  token as a prompt to indicate the start of the recommendation process and to summarize the dialogue context for the conversational recommendation.

Tuning-free prompting This training strategy can be referred to as zero-shot recommendations, which directly generate recommendations or/and related subtasks without changing the parameters of the PTMs but based only on the input prompts. Zero-shot recommendation has been shown to be effective in dealing with new users/items in one domain or cross-domain settings (Sileo et al., 2022;Geng et al., 2022c), compared to state-ofthe-art baselines. Specifically, Geng et al. (2022c) learned multiple tasks, such as sequential recommendation, rating prediction, explanation generation, review summarization and direct recommendation, in a unified way with the same Negative Log-likelihood (NLL) training objectives during pre-training. At the inference stage, a series of carefully designed discrete textual template prompts were taken as input, including prompts for recommending items in the new domain (not appearing in the pre-training phase), and the trained model outputs the preferable results without a fine-tuning stage. The reason for the effectiveness of zero-shot recommendation is that the training data and pre-training tasks are able to distil rich knowledge of semantics and correlations from diverse modalities into user and item tokens, which can comprehend user preference behaviours w.r.t. item characteristics (Geng et al., 2022c). Building upon this research, Geng et al. (2023) extended their efforts to train an adapter for diverse multimodal assignments, including sequential recommendations, direct recommendations, and the generation of explanations. In particular, they utilized the pre-trained CLIP component to convert images into image tokens. These tokens were added to the textual tokens of an item to create a personalized multimodal soft prompt. This com-bined prompt was then used as input to fine-tune the adapter in an autoregressive manner. Prompt+PTM tuning In this setting, the parameters include two parts: prompt-relevant parameters and model parameters. The tuning phase involves optimizing all parameters for specific recommendation tasks. Prompt+PTM tuning differs from the ""pre-train, fine-tune the holistic model"" strategy by providing additional prompts that can provide additional bootstrapping at the start of model training. For example, Li et al. (2023b) proposed a continuous prompt learning approach by first fixing the PTM, tuning the prompt to bridge the gap between the continuous prompts and the loaded PTM, and then fine-tuning both the prompt and PTM, resulting in a higher BLUE score in empirical results. They combined both discrete prompts (three user/item feature keywords, such as gym, breakfast, and Wi-Fi) and soft prompts (user/item embeddings) to generate recommendation explanations. Case studies showed improvements in the readability and fluency of generated explanations using the proposed prompts. Note that the Prompt+PTM tuning stage does not necessarily mean the fine-tuning stage but can be any possible stage for tuning parameters from both sides for specific data input. Xin et al. (2022) adapted a reinforcement learning framework as a Prompt+PTM tuning strategy by learning rewardstate pairs as soft prompt encodings w.r.t. observed actions during training. At the inference stage, the trained prompt generator can directly generate soft prompt embeddings for the recommendation model to generate actions (items).","How does the ""pre-train, prompt, and inference"" paradigm benefit downstream recommendation tasks without fine-tuning?","Instead of adapting PLMs to different downstream recommendation tasks by designing specific objective functions, a rising trend in recent years is to use the ""pre-train, prompt, and inference"" paradigm to reformulate downstream recommendations through hard/soft prompts. In this paradigm, fine-tuning can be avoided, and the pretrained model itself can be directly employed to predict item ratings, generate top-k item ranking lists, make conversations, recommend similar libraries for programmers while coding, or even output subtasks related to recommendation targets such as explanations (Li et al., 2023b). Prompt learning breaks through the problem of data constraints and bridges the gap of objective forms between pre-training and fine-tuning. Tuning-free prompting can be referred to as zero-shot recommendations, which directly generate recommendations or/and related subtasks without changing the parameters of the PTMs but based only on the input prompts. Zero-shot recommendation has been shown to be effective in dealing with new users/items in one domain or cross-domain settings (Sileo et al., 2022; Geng et al., 2022c), compared to state-of-the-art baselines. The reason for the effectiveness of zero-shot recommendation is that the training data and pre-training tasks are able to distil rich knowledge of semantics and correlations from diverse modalities into user and item tokens, which can comprehend user preference behaviours w.r.t. item characteristics (Geng et al., 2022c).","What are the advantages of using the ""pre-train, prompt, and inference"" paradigm for downstream recommendation tasks without the need for fine-tuning?","The ""pre-train, prompt, and inference"" paradigm offers significant benefits for downstream recommendation tasks by eliminating the need for fine-tuning. Instead of adapting pre-trained language models (PLMs) to various recommendation tasks through specific objective functions, this approach reformulates recommendations using hard or soft prompts. This allows the pre-trained model to be directly employed for tasks such as predicting item ratings, generating top-k item ranking lists, making conversational recommendations, suggesting similar libraries for programmers, or even providing explanations related to recommendation targets (Li et al., 2023b).

Prompt learning effectively addresses data constraints and bridges the gap between the objective forms of pre-training and fine-tuning. This tuning-free prompting, also known as zero-shot recommendations, enables the generation of recommendations and related subtasks without altering the parameters of the pre-trained models, relying solely on input prompts. Zero-shot recommendation has proven effective in handling new users or items within a domain or across different domains, outperforming state-of-the-art baselines (Sileo et al., 2022; Geng et al., 2022c).

The effectiveness of zero-shot recommendations stems from the ability of training data and pre-training tasks to distill rich semantic knowledge and correlations from diverse modalities into user and item tokens. This allows the model to comprehend user preference behaviors concerning item characteristics (Geng et al., 2022c).",7,4,7,4,"Question:

How does the ""pre-train, prompt, and inference"" paradigm benefit downstream recommendation tasks without fine-tuning?

Answer:

Instead of adapting PLMs to different downstream recommendation tasks by designing specific objective functions, a rising trend in recent years is to use the ""pre-train, prompt, and inference"" paradigm to reformulate downstream recommendations through hard/soft prompts. In this paradigm, fine-tuning can be avoided, and the pretrained model itself can be directly employed to predict item ratings, generate top-k item ranking lists, make conversations, recommend similar libraries for programmers while coding, or even output subtasks related to recommendation targets such as explanations (Li et al., 2023b). Prompt learning breaks through the problem of data constraints and bridges the gap of objective forms between pre-training and fine-tuning. Tuning-free prompting can be referred to as zero-shot recommendations, which directly generate recommendations or/and related subtasks without changing the parameters of the PTMs but based only on the input prompts. Zero-shot recommendation has been shown to be effective in dealing with new users/items in one domain or cross-domain settings (Sileo et al., 2022; Geng et al., 2022c), compared to state-of-the-art baselines. The reason for the effectiveness of zero-shot recommendation is that the training data and pre-training tasks are able to distil rich knowledge of semantics and correlations from diverse modalities into user and item tokens, which can comprehend user preference behaviours w.r.t. item characteristics (Geng et al., 2022c).","Question:

What are the advantages of using the ""pre-train, prompt, and inference"" paradigm for downstream recommendation tasks without the need for fine-tuning?

Answer:

The ""pre-train, prompt, and inference"" paradigm offers significant benefits for downstream recommendation tasks by eliminating the need for fine-tuning. Instead of adapting pre-trained language models (PLMs) to various recommendation tasks through specific objective functions, this approach reformulates recommendations using hard or soft prompts. This allows the pre-trained model to be directly employed for tasks such as predicting item ratings, generating top-k item ranking lists, making conversational recommendations, suggesting similar libraries for programmers, or even providing explanations related to recommendation targets (Li et al., 2023b).

Prompt learning effectively addresses data constraints and bridges the gap between the objective forms of pre-training and fine-tuning. This tuning-free prompting, also known as zero-shot recommendations, enables the generation of recommendations and related subtasks without altering the parameters of the pre-trained models, relying solely on input prompts. Zero-shot recommendation has proven effective in handling new users or items within a domain or across different domains, outperforming state-of-the-art baselines (Sileo et al., 2022; Geng et al., 2022c).

The effectiveness of zero-shot recommendations stems from the ability of training data and pre-training tasks to distill rich semantic knowledge and correlations from diverse modalities into user and item tokens. This allows the model to comprehend user preference behaviors concerning item characteristics (Geng et al., 2022c).",NO,True,1552,True,True
256662721-s5,"Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems",2023-02-07,Language modelling objectives to recommendation,"The expensive manual efforts required for annotated datasets have led many language learning objectives to adopt self-supervised labels, converting them to classic probabilistic density estimation problems. Among language modelling objectives, autoregressive, reconstruction, and auxiliary are three categories commonly used (Liu et al., 2023b). Here, we only introduce several language modelling objectives used for RSs.

Partial/ Auto-regressive Modelling (P/AM) Given a text sequence X 1:T = [x 1 , x 2 , · · · x T ], the training objective of AM can be summarized as a joint negative log-likelihood of each variable given all previous variables:

Modern LMRS typically utilize popular pretrained left-to-right LMs such as GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al., 2022a,c) as the backbone for explainable and conversational recommendations, respectively, to avoid the laborious task of pre-training from scratch. While auto-regressive objectives can effectively model context dependency, the modelling context can only be accessed from one direction, primarily left-to-right. To address this limitation, PAM is introduced, which extends AM by enabling the factorization step to be a span. For each input X, one factorization order M is sampled. One popular PTM that includes PAM as an objective is UniLMv2 (Bao et al., 2020). The pretrained UniLMv2 model can be utilized to initialize the news embedding model for news recommendation (Yu et al., 2022). Besides directly leveraging PTMs trained on textual inputs, some researchers apply this objective to train inputs with sequential patterns, such as graphs (Geng et al., 2022b) and user-item interactions . These patterns serve as either scoring functions to select suitable paths from the start node/user to the end node/item or detectors to explore novel user-item pairs. Masked Language Modelling (MLM) Taking a sequence of textual sentences as input, MLM first masks a token or multi-tokens with a special token such as [M ASK]. Then the model is trained to predict the masked tokens taking the rest of the tokens as context. The objective is as follows:

where M (X) and X M (X) represent the masked tokens in the input sequence X and the rest of the tokens in X respectively. A typical example of MLM training strategy can be found on BERT, which is leveraged as backbone in (Zhang et al., 2021a) to capture user-news matching signals for news recommendation. Concurrently, some research works propose multiple enhanced versions of MLM. RoBERTa  

where x and y represent two segments from the input corpus, and c = 1 if x and y are consecutive, otherwise c = 0. The NSP objective involves reasoning about the relationships between pairs of sentences and can be utilized for better representation learning of textual items such as news articles, item descriptions, and conversational data for recommendation purposes. Moreover, it can be employed to model the intimate relationships between two components. Malkiel et al. (2020) used the NSP to capture the relationship between the title and description of an item for next-item prediction. Furthermore, models pre-trained with NSP (such as BERT) can be leveraged for probing the learned knowledge with prompts, which are then infused in the fine-tuning stage to improve model training on adversarial data for conversational recommendation (Penha and Hauff, 2020). Sentence Order Prediction (SOP) as a variation of the NSP takes two consecutive segments from the same document as positive examples, which are then swapped in order as negative examples. SOP has been used to learn the inner coherence of title, description, and code for tag recommendation on StackOverflow (He et al., 2022).

Nevertheless, some researchers have questioned the necessity and effectiveness of the NSP and SOP for downstream tasks (He et al., 2022), which highlights the need for further investigation in recommendation scenarios. Replaced Token Detection(RTD) It is used to predict whether a token is replaced given its surrounding context:

MovieLens Link","How is the training objective of Auto-regressive Modelling (AM) summarized, and what are its limitations and solutions?","The training objective of Auto-regressive Modelling (AM) can be summarized as a joint negative log-likelihood of each variable given all previous variables. Modern language models for recommendation systems (LMRS) typically utilize popular pretrained left-to-right language models such as GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al., 2022a,c) as the backbone for explainable and conversational recommendations, respectively, to avoid the laborious task of pre-training from scratch. While auto-regressive objectives can effectively model context dependency, the modelling context can only be accessed from one direction, primarily left-to-right. To address this limitation, Partial/Auto-regressive Modelling (PAM) is introduced, which extends AM by enabling the factorization step to be a span. For each input, one factorization order is sampled. One popular pretrained model that includes PAM as an objective is UniLMv2 (Bao et al., 2020).","What is the training objective of Auto-regressive Modelling (AM), and what are its limitations and potential solutions?","The training objective of Auto-regressive Modelling (AM) is essentially the joint negative log-likelihood of each variable, conditioned on all preceding variables. This approach is widely used in modern language models for recommendation systems (LMRS), which often leverage popular pretrained left-to-right language models like GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al., 2022a,c). These models serve as the backbone for creating explainable and conversational recommendations, thus avoiding the labor-intensive process of pre-training from scratch.

While auto-regressive objectives are effective in modeling context dependency, they have a notable limitation: the context can only be accessed in a single direction, typically left-to-right. To overcome this limitation, Partial/Auto-regressive Modelling (PAM) has been introduced. PAM extends AM by allowing the factorization step to encompass a span, rather than being strictly sequential. For each input, a factorization order is sampled, which enhances flexibility. A prominent pretrained model that incorporates PAM as an objective is UniLMv2 (Bao et al., 2020).",7,2,7,2,"Question:

How is the training objective of Auto-regressive Modelling (AM) summarized, and what are its limitations and solutions?

Answer:

The training objective of Auto-regressive Modelling (AM) can be summarized as a joint negative log-likelihood of each variable given all previous variables. Modern language models for recommendation systems (LMRS) typically utilize popular pretrained left-to-right language models such as GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al., 2022a,c) as the backbone for explainable and conversational recommendations, respectively, to avoid the laborious task of pre-training from scratch. While auto-regressive objectives can effectively model context dependency, the modelling context can only be accessed from one direction, primarily left-to-right. To address this limitation, Partial/Auto-regressive Modelling (PAM) is introduced, which extends AM by enabling the factorization step to be a span. For each input, one factorization order is sampled. One popular pretrained model that includes PAM as an objective is UniLMv2 (Bao et al., 2020).","Question:

What is the training objective of Auto-regressive Modelling (AM), and what are its limitations and potential solutions?

Answer:

The training objective of Auto-regressive Modelling (AM) is essentially the joint negative log-likelihood of each variable, conditioned on all preceding variables. This approach is widely used in modern language models for recommendation systems (LMRS), which often leverage popular pretrained left-to-right language models like GPT-2 (Hada and Shevade, 2021) and DialoGPT (Wang et al., 2022a,c). These models serve as the backbone for creating explainable and conversational recommendations, thus avoiding the labor-intensive process of pre-training from scratch.

While auto-regressive objectives are effective in modeling context dependency, they have a notable limitation: the context can only be accessed in a single direction, typically left-to-right. To overcome this limitation, Partial/Auto-regressive Modelling (PAM) has been introduced. PAM extends AM by allowing the factorization step to encompass a span, rather than being strictly sequential. For each input, a factorization order is sampled, which enhances flexibility. A prominent pretrained model that incorporates PAM as an objective is UniLMv2 (Bao et al., 2020).",NO,True,1133,True,True
258426970-s8,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,2023-05-01,Optimizing for Human Feedback,"Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be ""optimizable"", i.e., possibly formulated as an optimization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f ∈ R), we can create the following optimization problem:

( 2) Where D is the distribution of possible inputs. Various techniques have been suggested to optimize the model parameters, θ, using the collected human feedback. These can be divided into three main categories based on the training mechanisms, which we will call feedback-based imitation learning, joint-feedback modeling, and reinforcement learning (RL).

The feedback-based imitation learning approach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled generations together with the corresponding inputs, D + . This can be achieved by minimizing the loss:

An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model's answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine translation model on a set of positively-labeled translations, and Glaese et al. (2022) performed supervised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), according to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human utterances as targets to fine-tune the model. Scheurer et al. (2022Scheurer et al. ( , 2023 leverage the fact that LLMs can follow instructions and start by collecting natural language human feedback about the model generations, which often describes what an improved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corresponding feedback. The highest similarity refinements for each generation are then used to finetune the LLM. OpenAI's text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disregard the generations which do not receive positive feedback, which may contain useful information to optimize the model. On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural language). Having D as the dataset of inputs x, generations y, and human feedback f collected, this can be achieved by minimizing the following loss of the form

Over all examples in D. These equation can be factorized as

. Some works simply train the model to predict the feedback given to each generation (Weston, 2016, forward prediction), disregarding the second term of the factorization. One example of this approach is the work of Li et al. (2017), in which the authors asked humans to give natural language feedback (e.g., positive/negative feedback, providing the correct answer to the model, or giving a hint about the correct answer) to a dialogue question answering model. Then, after having collected the feedback, the model is trained to predict it. Hancock et al. (2019) proposed having an auxiliary model predicting the satisfaction of the human speaking with the model. Then, if the satisfaction score is lower than a pre-defined threshold, the model will ask the human for feedback. The model then leverages the natural language feedback humans give by learning to predict it. Yuan et al. (2023); Rafailov et al. (2023) showed that having summarization models predict the rankings of different summaries helps the model generate better summaries, and might even outperform more complicated approaches using feedback models ( §5).

Other works train the model to predict the generations and the corresponding human feedback. Xu et al. (2022) proposed using the DIRECTOR model introduced by Arora et al. (2022) to leverage human feedback. As this model has a unified decoderclassifier architecture, Xu et al. (2022) proposed using positively-labeled examples to train its language modeling head (similarly to feedback-based imitation learning) and using both the positive and negatively-labeled examples to train a classifier head that directs the model away from generating undesirable sequences. Thoppilan et al. (2022a) follow this approach to enforce the model's quality and safety. First, they collect dialogues between crowd-workers and the proposed language model LaMDA, which are annotated with feedback provided by the crowd-workers. This feedback states each response's quality (sensible, specific, and interesting) or safety. Then, LaMDA is fine-tuned to predict the high-quality responses and the rewards given to every response regarding its quality attributes and safety. At inference time, LaMDA is also used to filter out candidate responses for which its safety prediction is below a threshold.

Finally, this can also be achieved by training the model to predict generation and conditioning on the feedback. This corresponds to minimizing the following loss: Liu et al. (2023) proposed prompt-based finetuning, where they create prompts containing previous generations rated by humans, in the order of preference. They also suggest inserting languagebased feedback (e.g., ""... is a worse answer than ..."") to the prompt, between the generations. Then, the model is fine-tuned to maximize the likelihood of generating the most preferred answer.

Finally, reinforcement learning (RL) offers a more versatile approach, allowing for direct optimization of a model's parameters based on human feedback, regardless of the feedback's differentiability. A common RL algorithm used in this context is the REINFORCE algorithm (Williams, 1992), which updates the policy parameters using the following gradient:

(7) Here, D represents the set of inputs x, and p θ is the policy. This flexibility enables RL to handle various types of feedback and better align the generated output with human preferences. For instance, Kreutzer et al. (2018) proposed using task-based implicit feedback from user queries as a reward signal to train a machine translation model using a word-level variant of minimum risk training (Shen et al., 2016), while Jaques et al. (2019) used implicit human reactions in chat to improve open-domain dialog systems through off-policy Q-learning (Watkins and Dayan, 1992). Given that collecting human feedback can be expensive and time-consuming, learning is done offline from logged data, which is typically more favorable than on-policy settings that need feedback on the fly. Later in §5.2.1, we discuss several works that attempt to optimize feedback models using RL instead of directly optimizing human feedback. In conjuction, these aproaches are commonly known as Reinforcement Learning from Human Feedback (RLHF).","What are the main techniques for optimizing model parameters using human feedback, and how are they categorized?","Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. Various techniques have been suggested to optimize the model parameters, θ, using the collected human feedback. These can be divided into three main categories based on the training mechanisms, which are called feedback-based imitation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning approach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled generations together with the corresponding inputs, D+. Joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. As the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural language). Finally, reinforcement learning (RL) offers a more versatile approach, allowing for direct optimization of a model's parameters based on human feedback, regardless of the feedback's differentiability.","What are the primary techniques for optimizing model parameters using human feedback, and how are these techniques categorized?","To optimize model parameters using human feedback, several techniques have been developed, which can be categorized into three main approaches: feedback-based imitation learning, joint-feedback modeling, and reinforcement learning (RL). 

Feedback-based imitation learning involves using human feedback to perform supervised learning. This is achieved by creating a dataset composed of positively-labeled generations along with their corresponding inputs, denoted as D+. The model is then optimized based on this dataset.

Joint-feedback modeling takes a more comprehensive approach by directly utilizing all the collected human feedback to optimize the model. This method allows the model to process feedback in various formats, not limited to numerical or ranking-based inputs, but also including natural language.

Reinforcement learning (RL) offers a versatile method for directly optimizing a model's parameters based on human feedback. This approach is particularly advantageous as it does not require the feedback to be differentiable, thus providing greater flexibility in how feedback is incorporated into the model optimization process.",7,4,8,4,"Question:

What are the main techniques for optimizing model parameters using human feedback, and how are they categorized?

Answer:

Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. Various techniques have been suggested to optimize the model parameters, θ, using the collected human feedback. These can be divided into three main categories based on the training mechanisms, which are called feedback-based imitation learning, joint-feedback modeling, and reinforcement learning (RL). The feedback-based imitation learning approach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled generations together with the corresponding inputs, D+. Joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. As the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural language). Finally, reinforcement learning (RL) offers a more versatile approach, allowing for direct optimization of a model's parameters based on human feedback, regardless of the feedback's differentiability.","Question:

What are the primary techniques for optimizing model parameters using human feedback, and how are these techniques categorized?

Answer:

To optimize model parameters using human feedback, several techniques have been developed, which can be categorized into three main approaches: feedback-based imitation learning, joint-feedback modeling, and reinforcement learning (RL). 

Feedback-based imitation learning involves using human feedback to perform supervised learning. This is achieved by creating a dataset composed of positively-labeled generations along with their corresponding inputs, denoted as D+. The model is then optimized based on this dataset.

Joint-feedback modeling takes a more comprehensive approach by directly utilizing all the collected human feedback to optimize the model. This method allows the model to process feedback in various formats, not limited to numerical or ranking-based inputs, but also including natural language.

Reinforcement learning (RL) offers a versatile method for directly optimizing a model's parameters based on human feedback. This approach is particularly advantageous as it does not require the feedback to be differentiable, thus providing greater flexibility in how feedback is incorporated into the model optimization process.",NO,True,1146,True,True
258426970-s18,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,2023-05-01,Subjectivity and variance in judgment,"Considering K annotators with feedback functions h i K i=1 , judgments are given on data D = d 1 , ..., d N . Inter-rater reliability metrics, such as Cohen's Kappa, Fleiss' Kappa, or Krippendorff's alpha, can assess annotator agreement (Hayes and Krippendorff, 2007;Fleiss, 1971;Cohen, 1960). Low reliability may result from unclear tasks or evaluation criteria (Gehrmann et al., 2022b;Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022;Nie et al., 2020;Gordon et al., 2022).

Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021;Zerva et al., 2022). Clear annotation guidelines and including rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019).",What strategies can mitigate low inter-rater reliability in annotator judgments for natural language generation tasks?,"Inter-rater reliability metrics, such as Cohen's Kappa, Fleiss' Kappa, or Krippendorff's alpha, can assess annotator agreement (Hayes and Krippendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evaluation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and including rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019).",What strategies can be employed to improve low inter-rater reliability in annotator judgments for natural language generation tasks?,"To address low inter-rater reliability in annotator judgments for natural language generation tasks, several strategies can be employed. First, it is essential to use inter-rater reliability metrics such as Cohen's Kappa, Fleiss' Kappa, or Krippendorff's alpha to assess the level of agreement among annotators (Hayes and Krippendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability often stems from unclear tasks or evaluation criteria, inherent subjectivity, or multiple plausible interpretations (Gehrmann et al., 2022b; Thomson and Reiter, 2021; Plank, 2022; Nie et al., 2020; Gordon et al., 2022). 

To mitigate these issues, one approach is to view human annotators as making noisily-rational choices, which acknowledges the variability in human judgment (Ghosal et al., 2023). Additionally, learning the reliability level of feedback from multiple annotators can provide insights into the consistency of their judgments (Yamagata et al., 2021). Augmenting evaluation metrics like COMET with confidence intervals can also help in understanding the variability in annotations (Glushkova et al., 2021; Zerva et al., 2022). 

Furthermore, providing clear annotation guidelines and including rationales with rankings can reduce biases and improve the clarity of the task for annotators (Ziegler et al., 2019). By implementing these strategies, the reliability of annotator judgments in natural language generation tasks can be significantly improved.",9,4,8,4,"Question:

What strategies can mitigate low inter-rater reliability in annotator judgments for natural language generation tasks?

Answer:

Inter-rater reliability metrics, such as Cohen's Kappa, Fleiss' Kappa, or Krippendorff's alpha, can assess annotator agreement (Hayes and Krippendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evaluation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022). Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and including rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019).","Question:

What strategies can be employed to improve low inter-rater reliability in annotator judgments for natural language generation tasks?

Answer:

To address low inter-rater reliability in annotator judgments for natural language generation tasks, several strategies can be employed. First, it is essential to use inter-rater reliability metrics such as Cohen's Kappa, Fleiss' Kappa, or Krippendorff's alpha to assess the level of agreement among annotators (Hayes and Krippendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability often stems from unclear tasks or evaluation criteria, inherent subjectivity, or multiple plausible interpretations (Gehrmann et al., 2022b; Thomson and Reiter, 2021; Plank, 2022; Nie et al., 2020; Gordon et al., 2022). 

To mitigate these issues, one approach is to view human annotators as making noisily-rational choices, which acknowledges the variability in human judgment (Ghosal et al., 2023). Additionally, learning the reliability level of feedback from multiple annotators can provide insights into the consistency of their judgments (Yamagata et al., 2021). Augmenting evaluation metrics like COMET with confidence intervals can also help in understanding the variability in annotations (Glushkova et al., 2021; Zerva et al., 2022). 

Furthermore, providing clear annotation guidelines and including rationales with rankings can reduce biases and improve the clarity of the task for annotators (Ziegler et al., 2019). By implementing these strategies, the reliability of annotator judgments in natural language generation tasks can be significantly improved.",NO,True,1455,True,True
132053537-s2,A Short Survey on Sense-Annotated Corpora,2018-02-13,WordNet,"WordNet (Fellbaum, 1998) has been one of the most widely used knowledge resource in lexical semantics. In fact, it is the de-facto standard sense inventory for Word Sense Disambiguation since many years. The core unit in WordNet is the synset. A synset represents a concept or a meaning which is represented by its various lexicalizations (i.e. senses). For example, the synset defined as motor vehicle with four wheels can be expressed by its synonym senses auto, automobile, machine and motorcar. In what follows we list the main WordNet sense-annotated corpora, using WordNet 3.0 as reference sense inventory. SemCor. The first and most prominent example of senseannotated corpora is SemCor (Miller et al., 1993b). Sem-Cor was manually annotated and consists of 352 documents from the Brown Corpus (Kucera and Francis, 1979) and 226,040 sense annotations. SemCor is the largest manually-annotated corpus and the most used in the literature to train WSD supervised systems Figure 1: Overview of sense inventories with their corresponding sense-annotated corpora. Zhong and Ng, 2010;Raganato et al., 2017b;Luo et al., 2018;Loureiro and Jorge, 2019;Huang et al., 2019). SemEval. SemEval datasets provide reliable benchmarks for testing WSD systems. The main datasets from Senseval and SemEval competitions have been compiled and unified by Raganato et al. (2017a). In particular, the datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 task 1 (Snyder and Palmer, 2004), SemEval-2007 task 17 (Pradhan et al., 2007), SemEval-2013 task 12 , and SemEval-2015 task 13 (Moro and Navigli, 2015). These datasets, which have been mainly used as evaluation benchmarks for WSD systems, contain a total of 7,253 sense annotations. MASC-WSA. The MASC Word Sense Annotation (MASC-WSA) corpus (Ide et al., 2010)is an excerpt of the Manually Annotated Sub-Corpus of American English (Ide et al., 2008, MASC) and the Open American National Corpus (Ide et al., 2002, ANC) containing annotations for 45 distinct lexemes, i.e., lemma-pos pairs, for a total of 441 distinct WordNet word senses. 2 Each word occurrence has been manually annotated on Amazon Mechanichal Turk by roughly 25 persons for a total of 1M annotations. Princeton WordNet Gloss. The Princeton WordNet Gloss Corpus 3 is a sense-annotated corpus of textual definitions (glosses) from WordNet synsets. The corpus was tagged semi-automatically: 330,499 instances were annotated manually while the remaining annotations (i.e. 118,856) were obtained automatically. This corpus of disambiguated glosses has already proved to be useful in shtml tasks such as semantic similarity (Pilehvar et al., 2013), domain labeling (González et al., 2012) and Word Sense Disambiguation (Baldwin et al., 2008;Agirre and Soroa, 2009;Camacho-Collados et al., 2015).

OntoNotes. OntoNotes (Weischedel et al., 2013) is a corpus from the Linguistic Data Consortium which comprises different kinds of explicitly-tagged syntactic and semantic information, including annotations at the sense level. The OntoNotes corpus consists of documents from diverse genres such as news, weblogs and telephone conversation. Its 5.0 released version contains 264,622 sense annotations.

OMSTI. The task of gathering sense annotations has proved expensive and not easily scalable. That is the reason why more recent approaches have attempted to exploit semi-automatic or automatic techniques. OMSTI 4 (Taghipour and Ng, 2015a, One Million Sense-Tagged Instances), which is a semi-automatically constructed corpus annotated with WordNet senses, is a prominent example. It was built by exploiting the alignment-based WSD approach of Chan and Ng (2005) on a large English-Chinese parallel corpus (Eisele and Chen, 2010, MultiUN corpus). OM-STI, coupled with SemCor, has already been successfully leveraged as training data for training supervised systems (Taghipour and Ng, 2015a;Iacobacci et al., 2016;Raganato et al., 2017a).",What are the main WordNet sense-annotated corpora and their significance in Word Sense Disambiguation?,"WordNet (Fellbaum, 1998) has been one of the most widely used knowledge resources in lexical semantics and is the de-facto standard sense inventory for Word Sense Disambiguation. The main WordNet sense-annotated corpora include:1. * *SemCor**: The first and most prominent example of sense-annotated corpora, SemCor (Miller et al., 1993b), was manually annotated and consists of 352 documents from the Brown Corpus and 226,040 sense annotations. SemCor is the largest manually-annotated corpus and the most used in the literature to train WSD supervised systems. 2. **SemEval**: SemEval datasets provide reliable benchmarks for testing WSD systems. The main datasets from Senseval and SemEval competitions have been compiled and unified by Raganato et al. (2017a), containing a total of 7,253 sense annotations. 3. **MASC-WSA**: The MASC Word Sense Annotation corpus is an excerpt of the Manually Annotated Sub-Corpus of American English and the Open American National Corpus, containing annotations for 45 distinct lexemes for a total of 441 distinct WordNet word senses. Each word occurrence has been manually annotated on Amazon Mechanical Turk by roughly 25 persons for a total of 1M annotations. 4. **Princeton WordNet Gloss**: This is a sense-annotated corpus of textual definitions from WordNet synsets, tagged semi-automatically with 330,499 instances annotated manually. It has been useful in tasks such as semantic similarity, domain labeling, and Word Sense Disambiguation.5. **OntoNotes**: A corpus from the Linguistic Data Consortium comprising different kinds of explicitly-tagged syntactic and semantic information, including annotations at the sense level. Its 5.0 released version contains 264,622 sense annotations. 6. **OMSTI**: A semi-automatically constructed corpus annotated with WordNet senses, built by exploiting the alignment-based WSD approach on a large English-Chinese parallel corpus. OMSTI, coupled with SemCor, has been successfully leveraged as training data for training supervised systems. These corpora are significant in Word Sense Disambiguation as they provide essential training and evaluation data for developing and testing WSD systems.","What are the main WordNet sense-annotated corpora, and why are they significant for Word Sense Disambiguation?","WordNet, as established by Fellbaum in 1998, is a pivotal resource in lexical semantics and serves as the standard sense inventory for Word Sense Disambiguation (WSD). Several key sense-annotated corpora have been developed using WordNet, each playing a crucial role in advancing WSD systems:

1. **SemCor**: Introduced by Miller et al. in 1993, SemCor is the first and most prominent sense-annotated corpus. It includes 352 documents from the Brown Corpus and features 226,040 sense annotations. As the largest manually-annotated corpus, SemCor is extensively used in the literature to train supervised WSD systems.

2. **SemEval**: The SemEval datasets are renowned for providing reliable benchmarks for testing WSD systems. Compiled and unified by Raganato et al. in 2017, these datasets encompass a total of 7,253 sense annotations from various Senseval and SemEval competitions.

3. **MASC-WSA**: The MASC Word Sense Annotation corpus is derived from the Manually Annotated Sub-Corpus of American English and the Open American National Corpus. It contains annotations for 45 distinct lexemes, totaling 441 distinct WordNet word senses. Each word occurrence has been manually annotated on Amazon Mechanical Turk by approximately 25 individuals, resulting in 1 million annotations.

4. **Princeton WordNet Gloss**: This corpus consists of sense-annotated textual definitions from WordNet synsets, tagged semi-automatically with 330,499 instances manually annotated. It has proven useful in tasks such as semantic similarity, domain labeling, and WSD.

5. **OntoNotes**: Provided by the Linguistic Data Consortium, OntoNotes includes various types of explicitly-tagged syntactic and semantic information, with annotations at the sense level. Its 5.0 release contains 264,622 sense annotations.

6. **OMSTI**: This semi-automatically constructed corpus is annotated with WordNet senses and built using an alignment-based WSD approach on a large English-Chinese parallel corpus. OMSTI, when combined with SemCor, has been effectively used as training data for supervised systems.

These corpora are significant for Word Sense Disambiguation as they offer essential training and evaluation data, facilitating the development and testing of WSD systems.",7,2,9,2,"Question:

What are the main WordNet sense-annotated corpora and their significance in Word Sense Disambiguation?

Answer:

WordNet (Fellbaum, 1998) has been one of the most widely used knowledge resources in lexical semantics and is the de-facto standard sense inventory for Word Sense Disambiguation. The main WordNet sense-annotated corpora include:1. * *SemCor**: The first and most prominent example of sense-annotated corpora, SemCor (Miller et al., 1993b), was manually annotated and consists of 352 documents from the Brown Corpus and 226,040 sense annotations. SemCor is the largest manually-annotated corpus and the most used in the literature to train WSD supervised systems. 2. **SemEval**: SemEval datasets provide reliable benchmarks for testing WSD systems. The main datasets from Senseval and SemEval competitions have been compiled and unified by Raganato et al. (2017a), containing a total of 7,253 sense annotations. 3. **MASC-WSA**: The MASC Word Sense Annotation corpus is an excerpt of the Manually Annotated Sub-Corpus of American English and the Open American National Corpus, containing annotations for 45 distinct lexemes for a total of 441 distinct WordNet word senses. Each word occurrence has been manually annotated on Amazon Mechanical Turk by roughly 25 persons for a total of 1M annotations. 4. **Princeton WordNet Gloss**: This is a sense-annotated corpus of textual definitions from WordNet synsets, tagged semi-automatically with 330,499 instances annotated manually. It has been useful in tasks such as semantic similarity, domain labeling, and Word Sense Disambiguation.5. **OntoNotes**: A corpus from the Linguistic Data Consortium comprising different kinds of explicitly-tagged syntactic and semantic information, including annotations at the sense level. Its 5.0 released version contains 264,622 sense annotations. 6. **OMSTI**: A semi-automatically constructed corpus annotated with WordNet senses, built by exploiting the alignment-based WSD approach on a large English-Chinese parallel corpus. OMSTI, coupled with SemCor, has been successfully leveraged as training data for training supervised systems. These corpora are significant in Word Sense Disambiguation as they provide essential training and evaluation data for developing and testing WSD systems.","Question:

What are the main WordNet sense-annotated corpora, and why are they significant for Word Sense Disambiguation?

Answer:

WordNet, as established by Fellbaum in 1998, is a pivotal resource in lexical semantics and serves as the standard sense inventory for Word Sense Disambiguation (WSD). Several key sense-annotated corpora have been developed using WordNet, each playing a crucial role in advancing WSD systems:

1. **SemCor**: Introduced by Miller et al. in 1993, SemCor is the first and most prominent sense-annotated corpus. It includes 352 documents from the Brown Corpus and features 226,040 sense annotations. As the largest manually-annotated corpus, SemCor is extensively used in the literature to train supervised WSD systems.

2. **SemEval**: The SemEval datasets are renowned for providing reliable benchmarks for testing WSD systems. Compiled and unified by Raganato et al. in 2017, these datasets encompass a total of 7,253 sense annotations from various Senseval and SemEval competitions.

3. **MASC-WSA**: The MASC Word Sense Annotation corpus is derived from the Manually Annotated Sub-Corpus of American English and the Open American National Corpus. It contains annotations for 45 distinct lexemes, totaling 441 distinct WordNet word senses. Each word occurrence has been manually annotated on Amazon Mechanical Turk by approximately 25 individuals, resulting in 1 million annotations.

4. **Princeton WordNet Gloss**: This corpus consists of sense-annotated textual definitions from WordNet synsets, tagged semi-automatically with 330,499 instances manually annotated. It has proven useful in tasks such as semantic similarity, domain labeling, and WSD.

5. **OntoNotes**: Provided by the Linguistic Data Consortium, OntoNotes includes various types of explicitly-tagged syntactic and semantic information, with annotations at the sense level. Its 5.0 release contains 264,622 sense annotations.

6. **OMSTI**: This semi-automatically constructed corpus is annotated with WordNet senses and built using an alignment-based WSD approach on a large English-Chinese parallel corpus. OMSTI, when combined with SemCor, has been effectively used as training data for supervised systems.

These corpora are significant for Word Sense Disambiguation as they offer essential training and evaluation data, facilitating the development and testing of WSD systems.",NO,True,2251,True,True
132053537-s4,A Short Survey on Sense-Annotated Corpora,2018-02-13,BabelNet,"BabelNet (Navigli and Ponzetto, 2012) is a wide-coverage multilingual semantic network obtained from the integration of various encyclopedias and dictionaries (WordNet and Wikipedia, inter alia). Being a superset of all these resources, BabelNet brings together lexicographic and encyclopedic knowledge, thus containing both named entities and concepts, and, unlike Wikipedia covering only noun instances, instances have diverse Part-Of-Speech (PoS) tags: nouns, verbs, adjectives and adverbs. Given its multilingual nature (i.e. BabelNet covers over 250 languages), Babel-Net has been used as a sense inventory for annotating text in languages other than English.

SenseDefs. SenseDefs 8 (Camacho-Collados et al., 2019) extends the effort from the Princeton WordNet Gloss Corpus project (see Section 2.1) by automatically disambiguating textual definitions from various heterogeneous sources in 263 languages. The underlying idea lies on leveraging the cross-complementarities of definitions of identical concepts from different languages and resources. The approach couples a graph-based disambiguation method (Moro et al., 2014) with a refinement based on distributional similarity (Camacho-Collados et al., 2016). The proposed method was evaluated on four European languages (English, Spanish, French and Italian) with an estimated precision of over 80%.

EuroSense. The construction of EuroSense 9 (Delli Bovi et al., 2017) follows a similar approach to SenseDefs. In this case, parallel corpora is exploited for a single multilingual disambiguation. The output is a sense-annotated corpus for 21 languages for the Europarl parallel corpus (Koehn, 2005). The estimated precision for four languages isover 80% on average, with a peak of almost 90% for German.",How does EuroSense achieve multilingual disambiguation and what is its precision across different languages?,"The construction of EuroSense follows a similar approach to SenseDefs. In this case, parallel corpora is exploited for a single multilingual disambiguation. The output is a sense-annotated corpus for 21 languages for the Europarl parallel corpus (Koehn, 2005). The estimated precision for four languages is over 80% on average, with a peak of almost 90% for German.","How does EuroSense perform multilingual disambiguation, and what is its precision across various languages?","EuroSense achieves multilingual disambiguation by utilizing parallel corpora, similar to the approach used in SenseDefs. This method allows for a unified multilingual disambiguation process, resulting in a sense-annotated corpus for 21 languages based on the Europarl parallel corpus (Koehn, 2005). The estimated precision for this disambiguation process is over 80% on average for four languages, with German reaching a peak precision of almost 90%.",7,2,4,2,"Question:

How does EuroSense achieve multilingual disambiguation and what is its precision across different languages?

Answer:

The construction of EuroSense follows a similar approach to SenseDefs. In this case, parallel corpora is exploited for a single multilingual disambiguation. The output is a sense-annotated corpus for 21 languages for the Europarl parallel corpus (Koehn, 2005). The estimated precision for four languages is over 80% on average, with a peak of almost 90% for German.","Question:

How does EuroSense perform multilingual disambiguation, and what is its precision across various languages?

Answer:

EuroSense achieves multilingual disambiguation by utilizing parallel corpora, similar to the approach used in SenseDefs. This method allows for a unified multilingual disambiguation process, resulting in a sense-annotated corpus for 21 languages based on the Europarl parallel corpus (Koehn, 2005). The estimated precision for this disambiguation process is over 80% on average for four languages, with German reaching a peak precision of almost 90%.",NO,True,450,True,True
53296658-s3,A Survey on Natural Language Processing for Fake News Detection,2018-11-02,Short Claims,"A recent benchmark dataset for fake news detection is LIAR (Wang, 2017). This dataset includes 12,836 real-world short statements collected from PolitiFact, where editors handpicked the claims from a variety of occasions such as debate, campaign, Facebook, Twitter, interviews, ads, etc. Each statement is labeled with six-grade truthfulness. The information about the subjects, party, context, and speakers are also included in this dataset. Vlachos and Riedel (2014) and Ferreira and Vlachos (2016) are the first to study PolitiFact data, but LIAR is orders of magnitude larger and more comprehensive. However, note that the original LIAR paper does not include the editor's justification or evidence due to copyright concerns, and users will need to retrieve the justification/evidence separately using an API. Also, even though both the claims and the evidence are from real-world occasions, they are highly un-structured. Fact-checking remains relatively challenging for this dataset. Fever ) is a dataset providing related evidences for fake news detection. Fever contains 185,445 claims generated from Wikipedia data. Each statement is labeled as Supported, Refuted, or Not Enough Info. They also marked which sentences from Wikipedia they use as evidence. Fever makes it possible to develop a system which can predict the truthfulness of a claim together with the evidence, even though the type of facts and evidence from Wikipedia may still exhibit some major stylistic differences from those in real-world political campaigns.

POLITIFACT, CHANNEL4.COM 2 , and SNOPES 3 are three sources for manually labeled short claims in news, which is collected and labeled manually. Many datasets, such as Wang (2017) and Rashkin et al. (2017), are created based on these websites.","What challenges are associated with fact-checking using the LIAR dataset, and why is it difficult?","A recent benchmark dataset for fake news detection is LIAR (Wang, 2017). However, note that the original LIAR paper does not include the editor's justification or evidence due to copyright concerns, and users will need to retrieve the justification/evidence separately using an API. Also, even though both the claims and the evidence are from real-world occasions, they are highly un-structured. Fact-checking remains relatively challenging for this dataset.","What are the challenges associated with using the LIAR dataset for fact-checking, and why is it considered difficult?","The LIAR dataset, introduced by Wang in 2017, is a notable benchmark for fake news detection. However, it presents several challenges that make fact-checking difficult. One significant issue is that the original LIAR paper does not provide the editor's justification or evidence due to copyright concerns. As a result, users must independently retrieve this justification or evidence using an API. Additionally, although both the claims and the evidence in the dataset are derived from real-world events, they are highly unstructured. This lack of structure contributes to the complexity and difficulty of fact-checking using the LIAR dataset.",7,4,7,4,"Question:

What challenges are associated with fact-checking using the LIAR dataset, and why is it difficult?

Answer:

A recent benchmark dataset for fake news detection is LIAR (Wang, 2017). However, note that the original LIAR paper does not include the editor's justification or evidence due to copyright concerns, and users will need to retrieve the justification/evidence separately using an API. Also, even though both the claims and the evidence are from real-world occasions, they are highly un-structured. Fact-checking remains relatively challenging for this dataset.","Question:

What are the challenges associated with using the LIAR dataset for fact-checking, and why is it considered difficult?

Answer:

The LIAR dataset, introduced by Wang in 2017, is a notable benchmark for fake news detection. However, it presents several challenges that make fact-checking difficult. One significant issue is that the original LIAR paper does not provide the editor's justification or evidence due to copyright concerns. As a result, users must independently retrieve this justification or evidence using an API. Additionally, although both the claims and the evidence in the dataset are derived from real-world events, they are highly unstructured. This lack of structure contributes to the complexity and difficulty of fact-checking using the LIAR dataset.",NO,True,643,True,True
53296658-s13,A Survey on Natural Language Processing for Fake News Detection,2018-11-02,Preprocessing,"Preprocessing usually includes tokenization, stemming, and generalization or weighting words.

To convert tokenized texts into features, Term Frequency-Inverse Document Frequency (TF-IDF) and Linguistic Inquiry and Word Count (LIWC) are frequently used.

For word sequences, pre-learned word embedding vectors such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) are commonly used.

Appropriate preprocessing is necessary for a better understanding of fake news. Mihalcea and Strapparava (2009) use LIWC and find there is a difference in word usage between deceptive language and non-deceptive ones, so using word classification may have significant meaning on detection.

When using entire articles as inputs, an additional preprocessing step is to identify the central claims from raw texts.  rank the sentences using TF-IDF and DrQA system (Chen et al., 2017). Solutions to the text summarization task can also be applied.",What additional preprocessing steps are necessary when using entire articles as inputs for fake news detection?,"When using entire articles as inputs, an additional preprocessing step is to identify the central claims from raw texts. This involves ranking the sentences using TF-IDF and the DrQA system (Chen et al., 2017). Solutions to the text summarization task can also be applied.",What additional preprocessing steps are necessary when using entire articles as inputs for fake news detection?,"When using entire articles as inputs for fake news detection, it is crucial to implement additional preprocessing steps to effectively analyze the content. One key step is to identify the central claims within the raw text. This can be achieved by ranking the sentences using techniques such as TF-IDF and the DrQA system (Chen et al., 2017). Additionally, applying solutions from the text summarization task can further enhance the extraction of essential information from the articles.",9,4,9,4,"Question:

What additional preprocessing steps are necessary when using entire articles as inputs for fake news detection?

Answer:

When using entire articles as inputs, an additional preprocessing step is to identify the central claims from raw texts. This involves ranking the sentences using TF-IDF and the DrQA system (Chen et al., 2017). Solutions to the text summarization task can also be applied.","Question:

What additional preprocessing steps are necessary when using entire articles as inputs for fake news detection?

Answer:

When using entire articles as inputs for fake news detection, it is crucial to implement additional preprocessing steps to effectively analyze the content. One key step is to identify the central claims within the raw text. This can be achieved by ranking the sentences using techniques such as TF-IDF and the DrQA system (Chen et al., 2017). Additionally, applying solutions from the text summarization task can further enhance the extraction of essential information from the articles.",NO,True,487,True,True
14225710-s1,Survey on the Use of Typological Information in Natural Language Processing,2016-10-11,Overview of Linguistic Typology,"Languages may share universal features on a deep, abstract level, but the structures found in real-world, surface-level natural language vary significantly. This variation is conventionally characterised into 'languages' (e.g. French, Hindi, Korean) 2 , and linguistic typology describes how these languages resemble or differ from one another. The field comprises three pursuits: the definition of language features and their capacity for variance, the measurement and analysis of feature variance across empirical data, and the explanation of patterns observed in this data analysis. Bickel (2007) terms these three pursuits qualitative, quantitative and theoretical typology, respectively. Typological classifications of languages have strict empirical foundations. These classifications do often support theories of causation, such as historical, areal or phylogenetic relations, but importantly, these hypotheses come second to quantitative data (Bickel, 2007). Indeed, patterns of variance may even run contrary to established theories of relations between languages based on geographical or historical proximity. For instance, Turkish and Korean are typically considered to be highly divergent in lexical features, yet their shared syntactic features make the two languages structurally quite similar. Such indications of similarity are of value for NLP which primarily seeks to model (rather than explain) cross-linguistic variation.

Typologists define and measure features according to the task at hand. Early studies, focused on word order, simply classified languages as SVO (Subject, Verb, Object), VSO, SOV, and so forth (Greenberg, 1963). There are now more various and fine-grained studies based on a wide range of features, including phonological, semantic, lexical and morphosyntactic properties (see (Bickel, 2007;Daniel, 2011) for an overview and further references). While a lot of valuable information is contained in these linguistic studies, this information is often not readily usable by NLP due to factors such as information overlap and differing definitions across studies. However, there is also a current trend towards systematically collecting typological information from individual studies in publicly-accessible databases, which are suitable for direct application in NLP (e.g., for defining features and their values). Table 1 presents a selection of current major databases, including the Syntactic Structures of the World's Languages (SSWL) (Collins and Kayne, 2009), the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013), the Phonetics Information Base and Lexicon (PHOIBLE) (Moran et al., 2014), the URIEL Typological Compendium (Littel et al., 2016), the Atlas of Pidgin and Creole Language Structures (APiCS) (Michaelis et al., 2013), and the Lyon-Albuquerque Phonological Systems Database (LAPSyD) (Maddieson et al., 2013). The table provides some basic information about these databases, including type, coverage, and additional notes. From these databases, WALS is currently by far the most commonly-used typological resource in NLP due to its broad coverage of features and languages.

We next discuss the potential of typological information to guide multilingual NLP and the means by which this can be done.","What are the three pursuits of linguistic typology, and how are they characterized by Bickel (2007)?","The field of linguistic typology comprises three pursuits: the definition of language features and their capacity for variance, the measurement and analysis of feature variance across empirical data, and the explanation of patterns observed in this data analysis. Bickel (2007) terms these three pursuits qualitative, quantitative, and theoretical typology, respectively.","What are the three main pursuits of linguistic typology, and how does Bickel (2007) characterize them?","Linguistic typology is a field that involves three main pursuits: defining language features and their potential for variation, measuring and analyzing the variation of these features across empirical data, and explaining the patterns observed in this data analysis. According to Bickel (2007), these pursuits are characterized as qualitative typology, quantitative typology, and theoretical typology, respectively.",7,2,7,2,"Question:

What are the three pursuits of linguistic typology, and how are they characterized by Bickel (2007)?

Answer:

The field of linguistic typology comprises three pursuits: the definition of language features and their capacity for variance, the measurement and analysis of feature variance across empirical data, and the explanation of patterns observed in this data analysis. Bickel (2007) terms these three pursuits qualitative, quantitative, and theoretical typology, respectively.","Question:

What are the three main pursuits of linguistic typology, and how does Bickel (2007) characterize them?

Answer:

Linguistic typology is a field that involves three main pursuits: defining language features and their potential for variation, measuring and analyzing the variation of these features across empirical data, and explaining the patterns observed in this data analysis. According to Bickel (2007), these pursuits are characterized as qualitative typology, quantitative typology, and theoretical typology, respectively.",YES,False,415,True,False
14225710-s6,Survey on the Use of Typological Information in Natural Language Processing,2016-10-11,Uses of Typological Information in NLP,"Multilingual Syntactic Parsing As mentioned in § 4.1, the main area of NLP in which information from structural typology has been exploited thus far is multilingual dependency parsing. In this task, a priori information about the predominant orderings of syntactic categories across languages are used to guide models when parsing a resource-poor language and using training data from other languages. This information is available in typological resources (e.g., WALS) which, among a variety of other syntactic features, list the dominant word orderings for many languages (see Table 1).

A seminal work that integrates typological word order information in multilingual dependency parsing (Naseem et al., 2012) presents the idea of ""selective sharing"" between source and target languages. In brief, while the identity of possible dependents for a given syntactic category is (hypothesised to be) language-universal, their ordering is language-specific. The work then presents a generative multilingual parsing model in which dependent ordering parameters are conditioned on word order typology, obtained from WALS. Specifically, the paper utilises the following word order features (henceforth WALS Basic word Order, WBO): 81A (Subject Verb and Object), 85A (Adposition and Noun), 86A (Genitive and Noun), 87A (Adjective and Noun), 88A (Demonstrative and Noun) and 89A (Numeral and Noun). This information enables the model to take into account dependent orderings only when the source language has a similar word order typology to the target language. In a similar vain, Täckström et al. (2013) present an instance of the typologically guided selective sharing idea within a discriminative parsing framework. They group the model features into features that encode arc directionality and word order, and those that do not. The former group is then coupled with the same WBO features used by Naseem et al. (2012) via feature templates that match the WALS properties with their corresponding POS tags. Additional features that group languages according to combinations of WALS features as well as coarse language groups (Indo-European versus Altaic), result in further improvements in parsing performance. Zhang and Barzilay (2015) extended the selective sharing approach for discriminative parsing to tensor-based models using the same WBO features as in (Naseem et al., 2012) and (Täckström et al., 2013). While traditional tensor-based parsers represent and assign non-zero weights to all possible combinations of atomic features, this work presents a hierarchical architecture that enables discarding chosen feature combinations. This allows the model to integrate prior typological knowledge, while ignoring uninformative combinations of typological and dependency features. At the same time, it capitalises on the automatisation of feature construction inherent to tensor models to generate combinations of informative typology-based features, further enhancing the added value of typological priors.

Another successful integration of externally-defined typological information in parsing is the work of Ammar et al. (2016). They present a multilingual parser trained on a concatenation of syntactic treebanks of multiple languages. To reduce the adverse impact of contradicting syntactic information in treebanks of typologically distinct languages, while still maintaining the benefits of additional training data for cross-linguistically consistent syntactic patterns, the parser encodes a language-specific bias for each given input language. This bias is based on the identity of the language and its WBO features as used in (Naseem et al., 2012;Täckström et al., 2013;Zhang and Barzilay, 2015). Differently from prior work, their parsing model also encodes all other features in the WALS profile of the relevant language. Overall, this strategy leads to improved parsing performance compared to monolingually trained baseline parsers.

While the papers surveyed above use prior information about word order typology extracted from WALS, word order information for guiding multilingual parsing can also be extracted in a bottom-up, data-driven fashion, without explicit reference to typological taxonomies. For example, in Søgaard (2011), training sentences in a source language are selected based on the perplexity of their coarse POS tag sequence under a target language POS language model. This approach essentially chooses sentences that exhibit similar word orderings in both source and target languages, thus realizing a bottom-up variant of the typology-based selective sharing methods discussed above.

There are also several methods which have made use of less explicit typological information. For instance, Berg-Kirkpatrick and Klein (2010) selectively combine languages in their method for cross-lingual dependency grammar induction using a phylogeny tree, which has been constructed from external (unspecified) knowledge of language families. Zeman and Resnik (2008) demonstrate improved performance of cross-lingually transferred dependency parsers within sets of typologically similar languages (e.g. Swedish-Danish, Hindi-Urdu); they do not explain how languages may be determined as ""closely-related"", though presumably this decision was based on the intuition of the researchers or on widely-acknowledged generalisations.

POS Tagging, Phonological Modeling and Language Learning Besides dependency parsing, several other areas have started integrating typological information in various forms. A number of such works revolve around the task of POS tagging. For example, in Zhang et al. (2012), the previously discussed WBO features were used to inform mappings from language-specific to a universal POS tagset. In (Zhang et al., 2016), WBO feature values are used to evaluate the quality of a multilingual POS tagger.

Another application area which benefited from integration of typological knowledge are phonological models of text. In (Tsvetkov et al., 2016) a multilingual neural phoneme-based language model is trained on several languages using a shared phonological inventory. The model is conditioned on the identity of the language at hand, as well as its phonological features obtained from a concatenation of phonological features from WALS, PHOIBLE and Ethnologue, extracted from URIEL. The resulting model subsumes and outperforms monolingually trained models for phone sequence prediction. Deri and Knight (2016) use URIEL to obtain phone and language similarity metrics, which are used for adjusting Grapheme to Phoneme (G2P) models from resource rich to resource poor languages. Berzak et al. (2015) use typological classifications to study language learning. Formalizing the theory of ""Contrastive Analysis"" which aims to analyse learning difficulties in a foreign language by comparing native and foreign language structures, they build a regression model that predicts language-specific grammatical error distributions by comparing typological features in the native and foreign languages.

5 Typological Information and NLP: What's Next? § 4.2 surveyed the current uses of typological information in NLP. Here we discuss several future research avenues that might benefit from tighter integration of linguistic typologies and multilingual NLP.

Encoding Typological Information in Traditional Machine Learning-based NLP One of the major open challenges for typologically-driven NLP is the construction of principled mechanisms for the integration of typological knowledge in machine learning-based algorithms. Here, we briefly discuss a few traditional machine learning frameworks which support encoding of expert information, and as such hold promise for integrating typological information in NLP.

Encoding typological knowledge into machine learning requires mechanisms that can bias learning (parameter estimation) and inference (prediction) of the model towards predefined knowledge. Algorithms such as the structured perceptron (Collins, 2002) and structured SVM (Taskar et al., 2004) iterate between an inference step and a parameter update step with respect to gold training labels. The inference step is a natural place for encoding external knowledge through constraints. It biases the prediction of the model to agree with external knowledge, which, in turn, affects both the training process and the final model prediction. As typological information often reflects tendencies rather than strict rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010;Rush et al., 2012), information extraction (Riedel and McCallum, 2011;, and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP.

Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations -i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010;Collobert et al., 2011;Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012;Hermann and Blunsom, 2014;Coulmance et al., 2015;Vulić and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually improves the alignment between the induced WEs and the meaning of the participating words in each of the involved languages. Naturally, as these models become more established and better understood, the challenge of external knowledge encoding becomes more prominent. Recent work has examined the ability to map from word embeddings to interpretable typological representations (Qian et al., 2016). Furthermore, a number of works (Faruqui et al., 2015;Rothe and Schütze, 2015;Osborne et al., 2016;Mrkšić et al., 2016) proposed means through which external knowledge from structured knowledge bases and specialised linguistic resources can be encoded in these models. The success of these works suggests that more extensive integration of external linguistic knowledge in general, and typological knowledge in particular, is likely to play a key role in the future development of WE representations.

Can NLP Support Typology Construction? As discussed in §4, typological resources are commonly constructed manually by linguists. Despite the progress made in recent years in the digitisation and collection of typological knowledge in centralised repositories, their coverage remains limited. Following the work surveyed in §4.1 on automatic learning of typological information, we believe that NLP could play a much larger role in the study of linguistic typology and the expansion of such resources. Future work in these directions will not only assist in the global efforts for language documentation, but also substentially extend the usability of such resources for NLP purposes.",How is typological information integrated into multilingual dependency parsing to improve parsing performance?,"Multilingual dependency parsing uses a priori information about the predominant orderings of syntactic categories across languages to guide models when parsing a resource-poor language and using training data from other languages. This information is available in typological resources like WALS, which list the dominant word orderings for many languages. A seminal work by Naseem et al. (2012) integrates typological word order information in multilingual dependency parsing through ""selective sharing"" between source and target languages. Dependent ordering parameters are conditioned on word order typology obtained from WALS, using features such as 81A (Subject Verb and Object), 85A (Adposition and Noun), and others. This enables the model to consider dependent orderings only when the source language has a similar word order typology to the target language. Täckström et al. (2013) present a similar approach within a discriminative parsing framework, coupling model features with WBO features via feature templates that match WALS properties with POS tags. Zhang and Barzilay (2015) extend this approach to tensor-based models, allowing the integration of prior typological knowledge while ignoring uninformative combinations of typological and dependency features. Ammar et al. (2016) present a multilingual parser trained on concatenated syntactic treebanks of multiple languages, encoding a language-specific bias based on the language identity and its WBO features. This strategy leads to improved parsing performance compared to monolingually trained baseline parsers.",How does incorporating typological information enhance the performance of multilingual dependency parsing?,"In multilingual dependency parsing, integrating typological information about the predominant syntactic orderings across languages can significantly enhance parsing performance, especially for resource-poor languages. This approach leverages typological resources like the World Atlas of Language Structures (WALS), which provides data on dominant word orderings for numerous languages. A key study by Naseem et al. (2012) introduced the concept of ""selective sharing"" between source and target languages, where dependent ordering parameters are conditioned on word order typology from WALS. This involves using features such as 81A (Subject, Verb, and Object) and 85A (Adposition and Noun) to ensure that the model considers dependent orderings only when the source language shares a similar word order typology with the target language.

Building on this, Täckström et al. (2013) applied a similar methodology within a discriminative parsing framework. They linked model features with word order typology features through feature templates that align WALS properties with part-of-speech tags. Zhang and Barzilay (2015) further advanced this approach by employing tensor-based models, which integrate prior typological knowledge while disregarding uninformative combinations of typological and dependency features.

Additionally, Ammar et al. (2016) developed a multilingual parser trained on concatenated syntactic treebanks from multiple languages. This parser encodes a language-specific bias based on the language identity and its word order typology features, leading to improved parsing performance compared to parsers trained on monolingual data alone. These strategies collectively demonstrate the effectiveness of incorporating typological information in enhancing multilingual dependency parsing.",7,2,7,4,"Question:

How is typological information integrated into multilingual dependency parsing to improve parsing performance?

Answer:

Multilingual dependency parsing uses a priori information about the predominant orderings of syntactic categories across languages to guide models when parsing a resource-poor language and using training data from other languages. This information is available in typological resources like WALS, which list the dominant word orderings for many languages. A seminal work by Naseem et al. (2012) integrates typological word order information in multilingual dependency parsing through ""selective sharing"" between source and target languages. Dependent ordering parameters are conditioned on word order typology obtained from WALS, using features such as 81A (Subject Verb and Object), 85A (Adposition and Noun), and others. This enables the model to consider dependent orderings only when the source language has a similar word order typology to the target language. Täckström et al. (2013) present a similar approach within a discriminative parsing framework, coupling model features with WBO features via feature templates that match WALS properties with POS tags. Zhang and Barzilay (2015) extend this approach to tensor-based models, allowing the integration of prior typological knowledge while ignoring uninformative combinations of typological and dependency features. Ammar et al. (2016) present a multilingual parser trained on concatenated syntactic treebanks of multiple languages, encoding a language-specific bias based on the language identity and its WBO features. This strategy leads to improved parsing performance compared to monolingually trained baseline parsers.","Question:

How does incorporating typological information enhance the performance of multilingual dependency parsing?

Answer:

In multilingual dependency parsing, integrating typological information about the predominant syntactic orderings across languages can significantly enhance parsing performance, especially for resource-poor languages. This approach leverages typological resources like the World Atlas of Language Structures (WALS), which provides data on dominant word orderings for numerous languages. A key study by Naseem et al. (2012) introduced the concept of ""selective sharing"" between source and target languages, where dependent ordering parameters are conditioned on word order typology from WALS. This involves using features such as 81A (Subject, Verb, and Object) and 85A (Adposition and Noun) to ensure that the model considers dependent orderings only when the source language shares a similar word order typology with the target language.

Building on this, Täckström et al. (2013) applied a similar methodology within a discriminative parsing framework. They linked model features with word order typology features through feature templates that align WALS properties with part-of-speech tags. Zhang and Barzilay (2015) further advanced this approach by employing tensor-based models, which integrate prior typological knowledge while disregarding uninformative combinations of typological and dependency features.

Additionally, Ammar et al. (2016) developed a multilingual parser trained on concatenated syntactic treebanks from multiple languages. This parser encodes a language-specific bias based on the language identity and its word order typology features, leading to improved parsing performance compared to parsers trained on monolingual data alone. These strategies collectively demonstrate the effectiveness of incorporating typological information in enhancing multilingual dependency parsing.",NO,True,1807,True,True
245144787-s3,Measure and Improve Robustness in NLP Models: A Survey,2021-12-15,Robustness under Distribution Shift,"Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-jussà, 2019).",How is robustness under distribution shift distinct from domain adaptation and transfer learning in NLP?,"Robustness under distribution shift focuses on models' performance when (x, y) are drawn from a different, naturally occurring distribution (Hendrycks et al., 2021). This concept is distinct from domain adaptation and transfer learning, as it is closer to domain generalization (Muandet et al., 2013; Gulrajani and Lopez-Paz, 2021) or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a). In this context, the test data, whether labeled or unlabeled, is assumed not to be available during training, implying generalization without adaptation. In NLP, robustness to natural distribution shifts means that models' performance should not degrade due to differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002; Blodgett et al., 2016; Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).",What distinguishes robustness under distribution shift from domain adaptation and transfer learning in NLP?,"Robustness under distribution shift is concerned with how well models perform when the input-output pairs (x, y) are drawn from a different, naturally occurring distribution (Hendrycks et al., 2021). This concept is distinct from domain adaptation and transfer learning because it aligns more closely with domain generalization (Muandet et al., 2013; Gulrajani and Lopez-Paz, 2021) or out-of-distribution generalization, which deals with unforeseen distribution shifts (Hendrycks et al., 2020a). In this scenario, the test data, whether labeled or unlabeled, is assumed to be unavailable during training, necessitating generalization without adaptation. In the context of NLP, robustness to natural distribution shifts implies that a model's performance should remain stable despite variations such as grammar errors, dialects, speakers, languages (Craig and Washington, 2002; Blodgett et al., 2016; Demszky et al., 2021), or when dealing with newly collected datasets for the same task but in different domains (Miller et al., 2020).",7,2,7,2,"Question:

How is robustness under distribution shift distinct from domain adaptation and transfer learning in NLP?

Answer:

Robustness under distribution shift focuses on models' performance when (x, y) are drawn from a different, naturally occurring distribution (Hendrycks et al., 2021). This concept is distinct from domain adaptation and transfer learning, as it is closer to domain generalization (Muandet et al., 2013; Gulrajani and Lopez-Paz, 2021) or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a). In this context, the test data, whether labeled or unlabeled, is assumed not to be available during training, implying generalization without adaptation. In NLP, robustness to natural distribution shifts means that models' performance should not degrade due to differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002; Blodgett et al., 2016; Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).","Question:

What distinguishes robustness under distribution shift from domain adaptation and transfer learning in NLP?

Answer:

Robustness under distribution shift is concerned with how well models perform when the input-output pairs (x, y) are drawn from a different, naturally occurring distribution (Hendrycks et al., 2021). This concept is distinct from domain adaptation and transfer learning because it aligns more closely with domain generalization (Muandet et al., 2013; Gulrajani and Lopez-Paz, 2021) or out-of-distribution generalization, which deals with unforeseen distribution shifts (Hendrycks et al., 2020a). In this scenario, the test data, whether labeled or unlabeled, is assumed to be unavailable during training, necessitating generalization without adaptation. In the context of NLP, robustness to natural distribution shifts implies that a model's performance should remain stable despite variations such as grammar errors, dialects, speakers, languages (Craig and Washington, 2002; Blodgett et al., 2016; Demszky et al., 2021), or when dealing with newly collected datasets for the same task but in different domains (Miller et al., 2020).",NO,True,1034,True,True
245144787-s6,Measure and Improve Robustness in NLP Models: A Survey,2021-12-15,Continuous vs. Discrete in Search Space,"The most obvious characteristic is probably the discrete nature of the space of text. This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP (Lei et al., 2019;Zhang et al., 2020c), in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space, and multiple novel attack methods are proposed to fill the gap, as we will discuss in later sections.

Perceptible to Human vs. Not On a related topic, one of the most impressive property of ad-versarial attack in vision is that small perturbation of the image data imperceptible to human are sufficient to deceive the model (Szegedy et al., 2013), while this can hardly be true for NLP attacks. Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible). On the other hand, there are ways to generate samples where the changes, although being perceptible, are often ignored by human brain due to some psychological prior on how a human processes the text (Anastasopoulos et al., 2019;. Support vs. Density Difference of the Data Distributions Another difference is more likely seen in the discussion of the domain adaptation of vision and NLP study. In vision study, although the images from training distribution and test distribution can be sufficiently different, the train and test distributions mostly share the same support (the pixels are always sampled from a 0-255 integer space), although the density of these distributions can be very different (e.g., photos vs. sketches). On the other hand, domain adaptation of NLP sometimes studies the regime where the supports of the data differ, e.g., the vocabularies can be significantly different in cross-lingual studies (Abad et al., 2020;Zhang et al., 2020a).

A Common Theme Despite the disparities between vision and NLP, the common theme of pushing the model to generalize from D to D preserves. The practical difference between D and D is more than often defined by the human's understanding of the data, and can differ in vision and NLP as humans perceive and process images and texts in subtly different ways, which creates both opportunities for learning and barriers for direct transfer. Certain lines of research try to bridge the learning in the vision domain to the embedding space in the NLP domain, while other lines of research create more interpretable attacks in the discrete text space (see Table 1 for these two lines of work). How those two lines of research transfer to each other, or complement each other, is not fully explored and calls for additional research.",How do adversarial attacks in vision and NLP differ in terms of perceptibility and sentence meaning alteration?,"The most obvious characteristic is probably the discrete nature of the space of text. This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP (Lei et al., 2019; Zhang et al., 2020c), in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space. Perceptible to Human vs. Not: One of the most impressive properties of adversarial attack in vision is that small perturbations of the image data imperceptible to humans are sufficient to deceive the model (Szegedy et al., 2013), while this can hardly be true for NLP attacks. Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences is not altered (despite being perceptible). On the other hand, there are ways to generate samples where the changes, although being perceptible, are often ignored by the human brain due to some psychological prior on how a human processes the text (Anastasopoulos et al., 2019).",How do adversarial attacks in computer vision and natural language processing differ in terms of perceptibility and the alteration of sentence meaning?,"One of the key differences between adversarial attacks in computer vision and natural language processing (NLP) lies in the discrete nature of text. This presents a challenge when transferring adversarial attack and defense strategies from vision to NLP (Lei et al., 2019; Zhang et al., 2020c). In vision, small perturbations to image data, which are imperceptible to humans, can effectively deceive models (Szegedy et al., 2013). However, this is not typically the case for NLP attacks. In NLP, adversarial attacks are often perceptible because they must ensure that the meaning of the sentences remains unchanged. Despite being noticeable, these changes are sometimes overlooked by the human brain due to psychological factors influencing how text is processed (Anastasopoulos et al., 2019).",7,2,7,4,"Question:

How do adversarial attacks in vision and NLP differ in terms of perceptibility and sentence meaning alteration?

Answer:

The most obvious characteristic is probably the discrete nature of the space of text. This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP (Lei et al., 2019; Zhang et al., 2020c), in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space. Perceptible to Human vs. Not: One of the most impressive properties of adversarial attack in vision is that small perturbations of the image data imperceptible to humans are sufficient to deceive the model (Szegedy et al., 2013), while this can hardly be true for NLP attacks. Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences is not altered (despite being perceptible). On the other hand, there are ways to generate samples where the changes, although being perceptible, are often ignored by the human brain due to some psychological prior on how a human processes the text (Anastasopoulos et al., 2019).","Question:

How do adversarial attacks in computer vision and natural language processing differ in terms of perceptibility and the alteration of sentence meaning?

Answer:

One of the key differences between adversarial attacks in computer vision and natural language processing (NLP) lies in the discrete nature of text. This presents a challenge when transferring adversarial attack and defense strategies from vision to NLP (Lei et al., 2019; Zhang et al., 2020c). In vision, small perturbations to image data, which are imperceptible to humans, can effectively deceive models (Szegedy et al., 2013). However, this is not typically the case for NLP attacks. In NLP, adversarial attacks are often perceptible because they must ensure that the meaning of the sentences remains unchanged. Despite being noticeable, these changes are sometimes overlooked by the human brain due to psychological factors influencing how text is processed (Anastasopoulos et al., 2019).",NO,True,793,True,True
173188095-s1,A Survey on Biomedical Image Captioning,2019-05-26,Datasets,"Datasets for biomedical image captioning comprise medical images and associated texts. Publicly available datasets contain X-rays (IU X-RAY in Table 1), clinical photographs (PEIR GROSS in Table 1), or a mixture of X-rays and photographs (ICLEF-CAPTION in Table 1). The associated texts may be single sentences describing the images, or longer medical reports based on the images (e.g., as in Figure 1b). Current publicly available datasets are rather small (IU X-RAY, PEIR GROSS) or noisy (e.g., IMAGE-CLEF, which is the largest dataset, was created by automatic means that introduced a lot of noise). We do not include in Table 1 datasets like the one of Wang et al. (2017), because their medical reports are not publicly available. 2 Furthermore, we observe that all three publicly available biomedical image captioning datasets suffer from two main shortcomings:

• There is a great class imbalance, with most images having no reported findings.

• The wide range of diseases leads to very scarce occurrences of disease-related terms, making it difficult for models to generalize. -Fushman et al. (2015) presented an approach for developing a collection of radiology examinations, including images and narrative reports by radiologists. The authors suggested an accurate anonymization approach for textual radiology reports and provided public access to their dataset through the Open Access Biomedical Image Search Engine (OpenI). 3 The images are 7,470 frontal and lateral chest X-rays, and each radiology report consists of four sections. The 'comparison' section contains previous information about the patient (e.g., preceding medical exams); the 'indication' section contains symptoms (e.g., hypoxia) or reasons of examination (e.g., age); 'findings' lists the radiology observations; and 'impression' outlines the final diagnosis. A system would ideally generate the 'findings' and 'impression' sections, possibly concatenated (Jing et al., 2018). The 'impression' and 'findings' sections of the dataset of Demner-Fushman et al. (2015) were used to manually associate each report with a number of tags (called manual encoding), which were Medical Subject Heading (MESH) 4 and RadLex 5 terms assigned by two trained coders. Additionally, each report was associated with automatically extracted tags, produced by Medical Text Indexer 6 (called MTI encoding). These tags allow systems to learn to initially generate terms describing the image and then use the image along with the generated terms to produce the caption. Hence, this dataset, which is the only one in the field with manually annotated tags, has an added value. From our processing, we found that 104 reports contained no image, 489 were missing 'findings', 6 were missing 'impression', and 25 were missing both 'findings' and 'impression'; the 40 image-caption-tags triplets corresponding to the latter 25 reports were discarded in our later experiments. We shuffled the instances of the dataset (image-text-tags triplets) and used 6,674 of them as the training set (images from the 90% of the reports), with average caption length 38 words and vocabulary size 2,091. Only 2,745 training captions were unique, because 59% of them were the same in more than one image (e.g., similar images with the same condition). Table 1 provides more information about the datasets and their splits.","What are the main shortcomings of publicly available biomedical image captioning datasets, and how do they affect model generalization?","Publicly available biomedical image captioning datasets suffer from two main shortcomings. There is a great class imbalance, with most images having no reported findings. The wide range of diseases leads to very scarce occurrences of disease-related terms, making it difficult for models to generalize.","What are the primary limitations of publicly available biomedical image captioning datasets, and how do these limitations impact the ability of models to generalize?","Publicly available biomedical image captioning datasets face two significant limitations. Firstly, there is a substantial class imbalance, as most images do not report any findings. Secondly, the vast array of diseases results in very infrequent occurrences of disease-related terms. These factors make it challenging for models to generalize effectively.",7,4,7,4,"Question:

What are the main shortcomings of publicly available biomedical image captioning datasets, and how do they affect model generalization?

Answer:

Publicly available biomedical image captioning datasets suffer from two main shortcomings. There is a great class imbalance, with most images having no reported findings. The wide range of diseases leads to very scarce occurrences of disease-related terms, making it difficult for models to generalize.","Question:

What are the primary limitations of publicly available biomedical image captioning datasets, and how do these limitations impact the ability of models to generalize?

Answer:

Publicly available biomedical image captioning datasets face two significant limitations. Firstly, there is a substantial class imbalance, as most images do not report any findings. Secondly, the vast array of diseases results in very infrequent occurrences of disease-related terms. These factors make it challenging for models to generalize effectively.",NO,True,355,False,False
208541377-s2,The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations Conference or Workshop Item The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations,2019,Rating and Likert scales,"In this section we use illustrative examples to underline the differences between rating and Likert scales. We use the term scale with the following two meanings:

• Given a statement, the term scale is the group of points making up the options offered to respondents. We refer to the combination of the statement and the scale as an item.

• In the case of an aggregate scale 2 , such as the Likert scale, we use the term scale to indicate a collection of items.

Rating scales: Rating scales are items used in surveys to estimate feeling, opinions or attitudes of responders. The data collected through a rating scale can be interpreted both as ordinal and interval. A rating scale is composed of an n-point scale. Scales with 3, 5, 7, 10 or 11 points are used most often. Rating scales can be both numerical and verbal.

In a numerical rating scale, a number is associated with each point. A variation of a numerical scale uses label words at the extreme values and leaves the intermediate values with a numerical label, as for example shown in Figure 1. A rat- ing scale that uses words as labels for the points is named a graphic rating scale 3 . An example of this kind of rating scale is pictured in Figure 2. Some- times the points of a graphic rating scale can also be labelled with numbers. Another sort of rating scale is the comparative rating scale. This kind of scale is used to ask respondents to answer a question in terms of a comparison. An example of a comparative rating scale is given in Figure 3.

Likert scale: A Likert scale is an aggregate scale. The items that make a Likert scale are In other words, it is a composite of items which are summed or averaged all together to get an overall positive or negative orientation towards the object under examination in the survey.

3 Sometimes a graphic rating scale is called Likert item or Likert-style scale. However, Likert items and Likert-style scale are particular cases of graphic rating scales. graphic rating scales. In this context, each graphic rating scale is called a Likert item. Likert scales are usually expressed in terms of agreement and disagreement. An example of a Likert scale is shown in Figure 4. The items that make a Lik- ert scales are designed to collectively capture the phenomenon under analysis. Accordingly, they shouldn't be considered in isolation and they should be summed or averaged to produce a total score. However, individual items by themselves are often considered as a single scale. Because of this ambivalent use of the Likert scale and its items, the nature of the Likert scale is highly controversial. Researchers are split between who consider it an interval scale and those who consider it an ordinal scale; see for example Jamieson (2004), Pell (2005), Norman (2010).

The confusion generated by the ambivalent use of the Likert scale and its items is well illustrated and explained in Joshi et al. (2015), where an image similar to Figure 5 is introduced. Likert scales are built in such a way that respondents express their level of agreement or disagreement with the sentences expressed by the Likert items. Because all the items are presented all together and with the same point labels, it is assumed that each respondent gives the same interpretation to the answer points -that is, as suggested by Likert, the distances between the points in the scale can be considered equal. 4 This assumption licenses use of the scale as an interval scale. Consequently, adding or averaging the items annotated by the same respondent is justified. This raises the interval interpretation, depicted by the vertical arrow of Figure 5.

Otherwise, an item-by-item analysis -that is a separate analysis of a single item extracted from an aggregate scale -cannot justify the assumption that the difference between adjacent points is equal. Indeed, we cannot assume that different respondents perceive the difference between adjacent label points as being of equal distance. The difference between ""agree"" and ""strongly agree"" can be perceived differently from one respondent to another. Consequently, the addition or the average of the items extracted from an aggregate scale is not justified. In such cases, the median or mode can be used as the measure of central tendency. This follows the ordinal interpretation, depicted by the horizontal arrow of Figure 5.

Unfortunately, in many cases there is not a clear understanding of the difference between the horizontal and the vertical direction of the aggregate scale. It is common to see item-by-item analysis (that is the horizontal direction) that makes use of parametric statistics without a justification of this choice. Indeed, as shown in Section 4, the interpretation of Likert items as interval scales has become a common practice. This particularly applies to the use of the mean for measuring the central tendency for the analysis of Likert items. 4 Some authors, for example Jamieson (2004), do not accept such an assumption and do not consider the points as equally distant. In this case the Likert scales themselves, and not only the Likert items, are considered ordinal.

In this section we use illustrative examples to underline the differences between rating and Likert scales. We use the term scale with the following two meanings:

• Given a statement, the term scale is the group of points making up the options offered to respondents. We refer to the combination of the statement and the scale as an item.

• In the case of an aggregate scale 2 , such as the Likert scale, we use the term scale to indicate a collection of items.

Rating scales: Rating scales are items used in surveys to estimate feeling, opinions or attitudes of responders. The data collected through a rating scale can be interpreted both as ordinal and interval. A rating scale is composed of an n-point scale. Scales with 3, 5, 7, 10 or 11 points are used most often. Rating scales can be both numerical and verbal.

In a numerical rating scale, a number is associated with each point. A variation of a numerical scale uses label words at the extreme values and leaves the intermediate values with a numerical label, as for example shown in Figure 1. A rat- ing scale that uses words as labels for the points is named a graphic rating scale 3 . An example of this kind of rating scale is pictured in Figure 2. Some- times the points of a graphic rating scale can also be labelled with numbers. Another sort of rating scale is the comparative rating scale. This kind of scale is used to ask respondents to answer a question in terms of a comparison. An example of a comparative rating scale is given in Figure 3.

Likert scale: A Likert scale is an aggregate scale. The items that make a Likert scale are In other words, it is a composite of items which are summed or averaged all together to get an overall positive or negative orientation towards the object under examination in the survey.

3 Sometimes a graphic rating scale is called Likert item or Likert-style scale. However, Likert items and Likert-style scale are particular cases of graphic rating scales. graphic rating scales. In this context, each graphic rating scale is called a Likert item. Likert scales are usually expressed in terms of agreement and disagreement. An example of a Likert scale is shown in Figure 4. The items that make a Lik- ert scales are designed to collectively capture the phenomenon under analysis. Accordingly, they shouldn't be considered in isolation and they should be summed or averaged to produce a total score. However, individual items by themselves are often considered as a single scale. Because of this ambivalent use of the Likert scale and its items, the nature of the Likert scale is highly controversial. Researchers are split between who consider it an interval scale and those who consider it an ordinal scale; see for example Jamieson (2004), Pell (2005), Norman (2010).

The confusion generated by the ambivalent use of the Likert scale and its items is well illustrated and explained in Joshi et al. (2015), where an image similar to Figure 5 is introduced. Likert scales are built in such a way that respondents express their level of agreement or disagreement with the sentences expressed by the Likert items. Because all the items are presented all together and with the same point labels, it is assumed that each respondent gives the same interpretation to the answer points -that is, as suggested by Likert, the distances between the points in the scale can be considered equal. 4 This assumption licenses use of the scale as an interval scale. Consequently, adding or averaging the items annotated by the same respondent is justified. This raises the interval interpretation, depicted by the vertical arrow of Figure 5.

Otherwise, an item-by-item analysis -that is a separate analysis of a single item extracted from an aggregate scale -cannot justify the assumption that the difference between adjacent points is equal. Indeed, we cannot assume that different respondents perceive the difference between adjacent label points as being of equal distance. The difference between ""agree"" and ""strongly agree"" can be perceived differently from one respondent to another. Consequently, the addition or the average of the items extracted from an aggregate scale is not justified. In such cases, the median or mode can be used as the measure of central tendency. This follows the ordinal interpretation, depicted by the horizontal arrow of Figure 5.

Unfortunately, in many cases there is not a clear understanding of the difference between the horizontal and the vertical direction of the aggregate scale. It is common to see item-by-item analysis (that is the horizontal direction) that makes use of parametric statistics without a justification of this choice. Indeed, as shown in Section 4, the interpretation of Likert items as interval scales has become a common practice. This particularly applies to the use of the mean for measuring the central tendency for the analysis of Likert items. 4 Some authors, for example Jamieson (2004), do not accept such an assumption and do not consider the points as equally distant. In this case the Likert scales themselves, and not only the Likert items, are considered ordinal.",What are the key differences between rating scales and Likert scales in survey design?,"Rating scales are items used in surveys to estimate feelings, opinions, or attitudes of responders. The data collected through a rating scale can be interpreted both as ordinal and interval. A rating scale is composed of an n-point scale, with scales of 3, 5, 7, 10, or 11 points being used most often. Rating scales can be both numerical and verbal. In a numerical rating scale, a number is associated with each point. A variation of a numerical scale uses label words at the extreme values and leaves the intermediate values with a numerical label. A rating scale that uses words as labels for the points is named a graphic rating scale. Another sort of rating scale is the comparative rating scale, which is used to ask respondents to answer a question in terms of a comparison. A Likert scale is an aggregate scale. It is a composite of items which are summed or averaged all together to get an overall positive or negative orientation towards the object under examination in the survey. Likert scales are usually expressed in terms of agreement and disagreement. The items that make a Likert scale are designed to collectively capture the phenomenon under analysis and should not be considered in isolation. Because all the items are presented together and with the same point labels, it is assumed that each respondent gives the same interpretation to the answer points, allowing the scale to be used as an interval scale. However, individual items by themselves are often considered as a single scale, leading to controversy over whether Likert scales are interval or ordinal.",What distinguishes rating scales from Likert scales in survey design?,"Rating scales and Likert scales are both tools used in surveys to gauge respondents' feelings, opinions, or attitudes, but they have distinct characteristics. Rating scales can be interpreted as either ordinal or interval data and typically consist of an n-point scale, commonly with 3, 5, 7, 10, or 11 points. These scales can be numerical, where each point is associated with a number, or verbal, where words label the points. A variation includes using label words at the extremes and numerical labels for intermediate values. Graphic rating scales use words as labels for all points, while comparative rating scales ask respondents to make comparisons.

On the other hand, a Likert scale is an aggregate scale composed of multiple items that are summed or averaged to provide an overall positive or negative orientation towards the subject being surveyed. Likert scales are usually framed in terms of agreement or disagreement. The items are designed to collectively capture the phenomenon under analysis and should not be considered in isolation. Because all items are presented together with the same point labels, it is assumed that respondents interpret the answer points consistently, allowing the scale to be used as an interval scale. However, when individual items are considered separately, there is debate over whether Likert scales should be treated as interval or ordinal.",9,2,9,2,"Question:

What are the key differences between rating scales and Likert scales in survey design?

Answer:

Rating scales are items used in surveys to estimate feelings, opinions, or attitudes of responders. The data collected through a rating scale can be interpreted both as ordinal and interval. A rating scale is composed of an n-point scale, with scales of 3, 5, 7, 10, or 11 points being used most often. Rating scales can be both numerical and verbal. In a numerical rating scale, a number is associated with each point. A variation of a numerical scale uses label words at the extreme values and leaves the intermediate values with a numerical label. A rating scale that uses words as labels for the points is named a graphic rating scale. Another sort of rating scale is the comparative rating scale, which is used to ask respondents to answer a question in terms of a comparison. A Likert scale is an aggregate scale. It is a composite of items which are summed or averaged all together to get an overall positive or negative orientation towards the object under examination in the survey. Likert scales are usually expressed in terms of agreement and disagreement. The items that make a Likert scale are designed to collectively capture the phenomenon under analysis and should not be considered in isolation. Because all the items are presented together and with the same point labels, it is assumed that each respondent gives the same interpretation to the answer points, allowing the scale to be used as an interval scale. However, individual items by themselves are often considered as a single scale, leading to controversy over whether Likert scales are interval or ordinal.","Question:

What distinguishes rating scales from Likert scales in survey design?

Answer:

Rating scales and Likert scales are both tools used in surveys to gauge respondents' feelings, opinions, or attitudes, but they have distinct characteristics. Rating scales can be interpreted as either ordinal or interval data and typically consist of an n-point scale, commonly with 3, 5, 7, 10, or 11 points. These scales can be numerical, where each point is associated with a number, or verbal, where words label the points. A variation includes using label words at the extremes and numerical labels for intermediate values. Graphic rating scales use words as labels for all points, while comparative rating scales ask respondents to make comparisons.

On the other hand, a Likert scale is an aggregate scale composed of multiple items that are summed or averaged to provide an overall positive or negative orientation towards the subject being surveyed. Likert scales are usually framed in terms of agreement or disagreement. The items are designed to collectively capture the phenomenon under analysis and should not be considered in isolation. Because all items are presented together with the same point labels, it is assumed that respondents interpret the answer points consistently, allowing the scale to be used as an interval scale. However, when individual items are considered separately, there is debate over whether Likert scales should be treated as interval or ordinal.",NO,True,1388,True,True
51623319-s2,Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2018-09-01,Other Terminologies Describing Similar Phenomena,"The phenomenon we discuss in this article has been a subject of interest for linguists, philosophers, and computational linguists for decades. Consequently, it has been addressed in various contexts from a variety of perspectives, as discussed in the following sections.

2.1.1 Abstract Anaphora. Asher (1993), Navarretta (2007), and Dipper et al. (2011) use the terms abstract anaphora or abstract object anaphora, as in this phenomenon the anaphor refers to an abstract object, such as a fact, an event, a proposition, or a situation, in Ashers's typology of saturated abstract objects (Asher 1993, page 57). contrast to a concrete object, such as a person or a location. Asher formalized the notion of an abstract object by extending Vendler's (1967) approach of using linguistic tests to differentiate various types of abstract objects. The resulting typology ( Figure 1) makes a broad distinction between eventualities (i.e., events and states, which have spatial, temporal, and causal properties and can be observed by the senses) and purely abstract objects (i.e., facts and propositions, which do not have a spatiotemporal location and are not perceivable by the senses but are only mentally conceivable; e.g., Asher 1993, page 57). According to Asher (1993, page 86), eventualities are similar to concrete objects in that they can be directly introduced into the discourse model by some syntactic construction. Whereas concrete objects are introduced by noun phrases (or, more precisely, by their determiners), eventualities are introduced by finite clauses (or, more precisely, by their inflectional marking). In contrast, facts or propositions are introduced by the semantic constraints imposed by specific nouns, such as fact, or verbs, such as believe, which require their arguments (e.g., a that clause) to be of a certain type (Asher 1993, pages 116 and 175). Asher collectively calls the events, states, processes, propositions, facts, and similar entities that populate these two categories saturated abstract objects. They are ""saturated"" in the Fregean sense that they are themselves either true or false, whereas properties or concepts, although abstract, are only true or false as applied to their arguments (Asher 1993, page 15). It is primarily this category of objects-saturated abstract objects-to which non-NA anaphors refer.

2.1.2 Discourse Deixis. Another popular term is discourse deixis (e.g., Webber 1988Webber , 1991Eckert and Strube 2000;Byron 2004;Recasens 2008). 7 Webber (1988Webber ( , 1991, attributing the term to Lakoff (1974), 8 calls non-NA anaphors discourse-deictic because the anaphor deictically points to some part of the discourse model from which it gets its reference. 7 The term deixis refers to the linguistic phenomenon in which an expression's reference is determined in relation to its extra-linguistic context, e.g., the time (now), place (here), or participants (I, you) of the utterance. Such expressions are called deictic (Huddleston andPullum 2002, page 1451). 8 Lakoff (1974) uses the term in a broader sense, including both NA and non-NA anaphora. Webber (1991) states that it makes sense to call the phenomenon discourse deixis because such relations are usually signaled by deictic expressions, that is, demonstratives this and that, compared with it. Cornish (2007) contrasts deixis with anaphora, describing them as the poles of a scale: Whereas anaphora involves the retrieval of an existing discourse entity from the current model, deixis shifts the focus to a new discourse entity or a new aspect of an existing entity.

The term discourse deixis has also been used in the literature with a different meaning: According to Levinson (1983), discourse deixis occurs when reference is made to the linguistic form of an utterance rather than its referent or when demonstrative expressions refer meta-linguistically to the preceding or following discourse segments (e.g., this section, this chapter). One can argue that the antecedents in such cases (i.e., this chapter and this section) are big chunks of text and therefore non-nominal. However, though these are certainly interesting cases, we do not focus on them in this article.

2.1.3 Impure Textual Deixis. Lyons (1977) distinguishes between three different types of entities: First-order entities are physical objects. Second-order entities are events, states of affairs, and processes (Asher's eventualities), which are located in time and involve first-order entities and interactions between them. Third-order entities are propositions and facts (Asher's purely abstract objects), which have no spatiotemporal location, and involve first-and second-order entities and the interactions between them.

For anaphoric relations, Lyons introduced the term textual deixis, which describes the deictic relation obtained between a referring expression such as a pronoun and a piece of text. He distinguishes between pure textual deixis, where the referring expression refers to a textual unit as such (similar to Levinson's [1983] notion of discourse deixis), and impure textual deixis, where the expression is related to the third-order entity denoted by a textual unit, such as a fact or a proposition. If the relation involves a second-order entity (e.g., an event), it is not clear whether Lyons considers this relation an instance of impure textual deixis or simply ordinary anaphora.

2.1.4 Situational Reference. Fraurud (1992) uses the term situational reference. She defines situations as entities representing eventualities (e.g., events, processes, and states) and factualities (e.g., facts and propositions). She uses the term antecedent for the clause or sentence that provides the anaphor's referent, but often the anaphor refers to a ""larger situation""-for example, a whole sequence of events.

2.1.5 Non-nominal Direct and Indirect Anaphora. Gundel, Hedberg, and Zacharski (2004) and Hedberg, Gundel, and Zacharski (2007) use the terms non-nominal direct and indirect anaphora. They operationalize this terminology as follows. An anaphoric relation is direct if the anaphor's referent is the same as the antecedent's referent, and it is indirect if the interpretation of the anaphor depends on that of the antecedent but they are not coreferential because the interpretation involves an additional step. Example (6), from Hedberg, Gundel, and Zacharski (2007), is an example of a direct anaphoric relation because both the anaphor and the antecedent refer to the event of the stock doubling on its first day of trading. In contrast, in Example (7), from Hedberg, Gundel, and Zacharski (2007), the clausal antecedent introduces the state of Giuliani being sleepy and the marked anaphor refers to the fact that he was sleepy, so the anaphor is not coreferential with the antecedent here, and it would be classified as an instance of indirect anaphora.

(6) The winner was Internet Capital Group, a company that invests in other Internet companies. It more than doubled its first day of trading, Aug. 5., and that was just the beginning.","How do Gundel, Hedberg, and Zacharski define and differentiate between direct and indirect anaphoric relations?","Gundel, Hedberg, and Zacharski (2004) and Hedberg, Gundel, and Zacharski (2007) use the terms non-nominal direct and indirect anaphora. They operationalize this terminology as follows. An anaphoric relation is direct if the anaphor's referent is the same as the antecedent's referent. It is indirect if the interpretation of the anaphor depends on that of the antecedent but they are not coreferential because the interpretation involves an additional step. Example (6), from Hedberg, Gundel, and Zacharski (2007), is an example of a direct anaphoric relation because both the anaphor and the antecedent refer to the event of the stock doubling on its first day of trading. In contrast, in Example (7), from Hedberg, Gundel, and Zacharski (2007), the clausal antecedent introduces the state of Giuliani being sleepy and the marked anaphor refers to the fact that he was sleepy, so the anaphor is not coreferential with the antecedent here, and it would be classified as an instance of indirect anaphora.","How do Gundel, Hedberg, and Zacharski distinguish between direct and indirect anaphoric relations?","Gundel, Hedberg, and Zacharski (2004) and Hedberg, Gundel, and Zacharski (2007) define anaphoric relations using the terms non-nominal direct and indirect anaphora. An anaphoric relation is considered direct if the referent of the anaphor is the same as the referent of the antecedent. In contrast, an anaphoric relation is classified as indirect if the interpretation of the anaphor depends on the antecedent but involves an additional interpretative step, meaning they are not coreferential. For example, in Hedberg, Gundel, and Zacharski (2007), a direct anaphoric relation is illustrated where both the anaphor and the antecedent refer to the event of a stock doubling on its first day of trading. Conversely, another example from the same work shows an indirect anaphoric relation where the clausal antecedent introduces the state of Giuliani being sleepy, and the marked anaphor refers to the fact that he was sleepy. Here, the anaphor is not coreferential with the antecedent, thus classifying it as indirect anaphora.",7,2,7,2,"Question:

How do Gundel, Hedberg, and Zacharski define and differentiate between direct and indirect anaphoric relations?

Answer:

Gundel, Hedberg, and Zacharski (2004) and Hedberg, Gundel, and Zacharski (2007) use the terms non-nominal direct and indirect anaphora. They operationalize this terminology as follows. An anaphoric relation is direct if the anaphor's referent is the same as the antecedent's referent. It is indirect if the interpretation of the anaphor depends on that of the antecedent but they are not coreferential because the interpretation involves an additional step. Example (6), from Hedberg, Gundel, and Zacharski (2007), is an example of a direct anaphoric relation because both the anaphor and the antecedent refer to the event of the stock doubling on its first day of trading. In contrast, in Example (7), from Hedberg, Gundel, and Zacharski (2007), the clausal antecedent introduces the state of Giuliani being sleepy and the marked anaphor refers to the fact that he was sleepy, so the anaphor is not coreferential with the antecedent here, and it would be classified as an instance of indirect anaphora.","Question:

How do Gundel, Hedberg, and Zacharski distinguish between direct and indirect anaphoric relations?

Answer:

Gundel, Hedberg, and Zacharski (2004) and Hedberg, Gundel, and Zacharski (2007) define anaphoric relations using the terms non-nominal direct and indirect anaphora. An anaphoric relation is considered direct if the referent of the anaphor is the same as the referent of the antecedent. In contrast, an anaphoric relation is classified as indirect if the interpretation of the anaphor depends on the antecedent but involves an additional interpretative step, meaning they are not coreferential. For example, in Hedberg, Gundel, and Zacharski (2007), a direct anaphoric relation is illustrated where both the anaphor and the antecedent refer to the event of a stock doubling on its first day of trading. Conversely, another example from the same work shows an indirect anaphoric relation where the clausal antecedent introduces the state of Giuliani being sleepy, and the marked anaphor refers to the fact that he was sleepy. Here, the anaphor is not coreferential with the antecedent, thus classifying it as indirect anaphora.",YES,False,1025,True,False
51623319-s3,Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2018-09-01,554,"(7) Mayor Rudolph Giuliani, who gave himself the job of ubiquitous master of ceremonies of the city's New Year celebration, said he began his last day of the 1900s at 5:30 a.m. having trouble getting his lights on. ""I was convinced that it was Y2K,"" the mayor said, but ""actually I was sleepy."" This perhaps explains an interesting mishap. . . Botley (2006) provides the following characteristic properties for indirect anaphora: (a) The antecedent is not nominal and is difficult to define directly, (b) the link between anaphor and antecedent is not one of coreference, and (c) the hearer may have to carry out a complex process of inference to arrive at the antecedent. Botley considers three main types of indirect anaphora: textual deixis 9 (Lyons 1977), situational reference (Fraurud 1992), and labeling (Francis 1986). We discussed textual deixis and situational reference in the previous subsections, and we will discuss labeling (i.e., shell nouns) in Section 3.1.

2.1.6 Complex Anaphora. Consten, Knees, and Schwarz-Friesel (2007) coin the term complex anaphora, where anaphors are nominal expressions referring to propositionally structured referents, such as propositions, states, facts, and events. They define two criteria for complex anaphora: First, the antecedent has to be a syntactically complex entity-it must consist of at least one clause; and second, the antecedent must denote a conceptually complex item. 10 Consten, Knees, and Schwarz-Friesel define a conceptually complex item as a second-or third-order entity, according to Lyons' (1977) hierarchy (see Section 2.1.3).

2.1.7 Extended Reference and Text Reference. Halliday and Hasan (1976, pages 52-53, 66-70) distinguish between two kinds of references of demonstrative pronouns and the pronoun it: extended reference and text reference. 11 An example from Halliday and Hasan (1976, page 52) is given in Example (8). 12 The first instance of it in the example refers to curtseying while you're thinking what to say, which they call extended reference, as the reference is no longer to a person or object but to a whole process or complex phenomenon, and the referent is expressed by a clause or string of clauses instead of a simple noun phrase. In contrast, the second instance of it is a case of text reference because it requires its referent to be transmuted into the fact that curtseying while you're thinking what to say saves time. Alice wondered a little at this, but she was too much in awe of the Queen to disbelieve it. 9 Botley calls this type text/discourse deixis. Discourse deixis is here understood in the sense of Levinson (1983), which is very close to Lyons' pure textual deixis. 10 Both conditions are necessary to distinguish non-NA anaphora from bridging relations (see footnote 6):

Example (ia) is a case of a non-NA anaphor with this incident referring to the biting event reported in the previous sentence. Example (ib) is a case of a bridging relation: The expression the scars does not refer to an event but to a concrete entity, which is inferred from an entity involved in the biting event. ( 555 Table 1 Overview of terminology used for non-NA anaphora.",How do Halliday and Hasan differentiate between extended reference and text reference in anaphora?  ,"Halliday and Hasan (1976, pages 52-53, 66-70) distinguish between two kinds of references of demonstrative pronouns and the pronoun it: extended reference and text reference. An example from Halliday and Hasan (1976, page 52) illustrates this distinction. The first instance of it in the example refers to curtseying while you're thinking what to say, which they call extended reference, as the reference is no longer to a person or object but to a whole process or complex phenomenon, and the referent is expressed by a clause or string of clauses instead of a simple noun phrase. In contrast, the second instance of it is a case of text reference because it requires its referent to be transmuted into the fact that curtseying while you're thinking what to say saves time.",How do Halliday and Hasan differentiate between extended reference and text reference in anaphora?,"Halliday and Hasan (1976, pages 52-53, 66-70) differentiate between two types of references used by demonstrative pronouns and the pronoun ""it"": extended reference and text reference. They illustrate this distinction with an example (1976, page 52). The first instance of ""it"" in their example refers to ""curtseying while you're thinking what to say."" This is termed extended reference because the reference is not to a specific person or object but to an entire process or complex phenomenon. Here, the referent is expressed by a clause or a series of clauses rather than a simple noun phrase. In contrast, the second instance of ""it"" is an example of text reference. This requires the referent to be transformed into the fact that ""curtseying while you're thinking what to say saves time.""",7,2,7,2,"Question:

How do Halliday and Hasan differentiate between extended reference and text reference in anaphora?  

Answer:

Halliday and Hasan (1976, pages 52-53, 66-70) distinguish between two kinds of references of demonstrative pronouns and the pronoun it: extended reference and text reference. An example from Halliday and Hasan (1976, page 52) illustrates this distinction. The first instance of it in the example refers to curtseying while you're thinking what to say, which they call extended reference, as the reference is no longer to a person or object but to a whole process or complex phenomenon, and the referent is expressed by a clause or string of clauses instead of a simple noun phrase. In contrast, the second instance of it is a case of text reference because it requires its referent to be transmuted into the fact that curtseying while you're thinking what to say saves time.","Question:

How do Halliday and Hasan differentiate between extended reference and text reference in anaphora?

Answer:

Halliday and Hasan (1976, pages 52-53, 66-70) differentiate between two types of references used by demonstrative pronouns and the pronoun ""it"": extended reference and text reference. They illustrate this distinction with an example (1976, page 52). The first instance of ""it"" in their example refers to ""curtseying while you're thinking what to say."" This is termed extended reference because the reference is not to a specific person or object but to an entire process or complex phenomenon. Here, the referent is expressed by a clause or a series of clauses rather than a simple noun phrase. In contrast, the second instance of ""it"" is an example of text reference. This requires the referent to be transformed into the fact that ""curtseying while you're thinking what to say saves time.""",YES,False,791,True,False
51623319-s14,Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2018-09-01,Structurally-determined relations,"3 N-be-to Our plan is to hire and retain the best managers we can. 4

N-be-that The major reason is that doctors are uncomfortable with uncertainty. 5 N-be-wh Of course, the central, and probably insoluble, issue is whether animal testing is cruel. 6

Sub-be-N * If the money is available, however, cutting the sales tax is a good idea. 7

N-to The decision to disconnect the ventilator came after doctors found no brain activity. decision, that have the capability to encapsulate and refer to propositional content. These nouns are known by a great variety of names in the literature, including container nouns (Vendler 1968), type-3 vocabulary (Winter 1977), anaphoric nouns (Francis 1986), label nouns (Francis 1994), and carrier nouns (Ivanič 1991). They are likewise included in Halliday and Hasan's (1976) concepts of extended reference and text reference. In this article, we use Schmid's (2000) term shell nouns, which derives from these nouns' tendency to function as conceptual shells for propositional content.

To be a shell noun is not an inherent property of nouns themselves; rather, it is a property of particular instances of these nouns, which can be characterized individually as shell noun usages. In the context of shell nouns, the term shell content is often used to refer to the text that provides the interpretation of the shell noun phrase. 21 Schmid (2000) observed a number of lexico-grammatical patterns in which shell nouns tend to occur. Table 3 shows these patterns. Shell nouns may refer anaphorically to their shell content, as shown on lines 1 and 2, 22 or they can be structurally related to their shell content, as shown on lines 3 to 10. These relations include copula structures (lines 3-6) 23 and postnominal complement and modifier clauses (lines 7-10). As the pattern labels 21 The concept of shell content is similar to the concept of an antecedent except that in some shell noun constructions, the term antecedent is not quite appropriate. That said, to be consistent, we use the term antecedent for shell content except when it is absolutely necessary to use the term shell content. 22 Schmid (2000) clarifies that ""[i]n a way, the pattern th-be-N is a blend of the copular type N-be-cl and the anaphoric type th-N."" (page 25). 23 The pattern Sub-be-N is not part of Schmid's (2000) original list but he discusses it in his example (3.5') on page 26.","How do shell nouns relate to their shell content, and what are some examples of these relations?","Shell nouns may refer anaphorically to their shell content, as shown on lines 1 and 2, or they can be structurally related to their shell content, as shown on lines 3 to 10. These relations include copula structures (lines 3-6) and postnominal complement and modifier clauses (lines 7-10). The concept of shell content is similar to the concept of an antecedent except that in some shell noun constructions, the term antecedent is not quite appropriate. To be consistent, the term antecedent is used for shell content except when it is absolutely necessary to use the term shell content.","What are shell nouns, and how do they relate to their shell content? Can you provide examples of these relationships?","Shell nouns are nouns that encapsulate complex information, often referring anaphorically to their shell content, as demonstrated in lines 1 and 2. They can also be structurally related to their shell content, as illustrated in lines 3 to 10. These structural relations include copula structures (lines 3-6) and postnominal complement and modifier clauses (lines 7-10). The concept of shell content is akin to that of an antecedent, although in some shell noun constructions, the term antecedent may not be entirely appropriate. For consistency, the term antecedent is used for shell content, except when it is absolutely necessary to use the term shell content.",7,4,7,2,"Question:

How do shell nouns relate to their shell content, and what are some examples of these relations?

Answer:

Shell nouns may refer anaphorically to their shell content, as shown on lines 1 and 2, or they can be structurally related to their shell content, as shown on lines 3 to 10. These relations include copula structures (lines 3-6) and postnominal complement and modifier clauses (lines 7-10). The concept of shell content is similar to the concept of an antecedent except that in some shell noun constructions, the term antecedent is not quite appropriate. To be consistent, the term antecedent is used for shell content except when it is absolutely necessary to use the term shell content.","Question:

What are shell nouns, and how do they relate to their shell content? Can you provide examples of these relationships?

Answer:

Shell nouns are nouns that encapsulate complex information, often referring anaphorically to their shell content, as demonstrated in lines 1 and 2. They can also be structurally related to their shell content, as illustrated in lines 3 to 10. These structural relations include copula structures (lines 3-6) and postnominal complement and modifier clauses (lines 7-10). The concept of shell content is akin to that of an antecedent, although in some shell noun constructions, the term antecedent may not be entirely appropriate. For consistency, the term antecedent is used for shell content, except when it is absolutely necessary to use the term shell content.",NO,True,662,True,True
51623319-s19,Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2018-09-01,Syntactic Preferences.,"According to Asher (1993, page 226), the range of syntactic constructs of abstract antecedents is quite broad. Some examples of the different linguistic constructions that may function as abstract antecedents are:

1. That clauses; e.g., John believed that Mary was sick 2. Infinitival phrases; e.g., Fred wanted to go to the movies 3. Naked infinitive complements; e.g., John saw Mary arrive 4. Noun phrases that appear to denote proposition-like entities; e.g., The claim that Susan got a C on the test was surprising

Moreover, it seems evident that the semantic type of the antecedent also suggests corresponding syntactic realizations. Schmid (2000, page 381) provides the frequencies of 670 shell nouns from the 225 million-word corpus of the British section of the Bank of English corpus. For each shell noun, he provides the frequency distribution of that shell noun across the different lexico-grammatical patterns from Table 3. Among other tendencies, it is evident from these frequencies that purposes and decisions are more likely to be represented by to-clauses; explanations and facts by that-clauses; and questions and issues by wh-question clauses. Passonneau (1989) analyzed local contexts of it and that in four career counseling interviews. She observed that grammatical functions (e.g., subject) play an important role: If both the anaphor and its antecedent are subjects, it is far more likely than that; if either one or both are not subjects, that is more likely. Moreover, the syntactic type of the antecedent is also relevant: If the antecedent is pronominal, it is more likely than that. If the antecedent is non-nominal or a gerund, that is more likely. With canonical NP antecedents, both are equally likely.","How does the syntactic type of an antecedent influence the choice between using ""it"" or ""that"" in anaphora?  ","The syntactic type of the antecedent is relevant in influencing the choice between using ""it"" or ""that"" in anaphora. If both the anaphor and its antecedent are subjects, ""it"" is far more likely than ""that."" If either one or both are not subjects, ""that"" is more likely. If the antecedent is pronominal, ""it"" is more likely than ""that."" If the antecedent is non-nominal or a gerund, ""that"" is more likely. With canonical NP antecedents, both ""it"" and ""that"" are equally likely.","How does the syntactic type of an antecedent affect the decision to use ""it"" or ""that"" in anaphora?","The syntactic type of an antecedent plays a significant role in determining whether ""it"" or ""that"" is used in anaphora. When both the anaphor and its antecedent function as subjects, ""it"" is much more likely to be chosen over ""that."" Conversely, if either the anaphor or its antecedent, or both, are not subjects, ""that"" becomes the more probable choice. Additionally, when the antecedent is pronominal, ""it"" is preferred over ""that."" In cases where the antecedent is non-nominal or a gerund, ""that"" is more commonly used. For canonical noun phrase (NP) antecedents, both ""it"" and ""that"" are equally likely to be used.",8,2,7,2,"Question:

How does the syntactic type of an antecedent influence the choice between using ""it"" or ""that"" in anaphora?  

Answer:

The syntactic type of the antecedent is relevant in influencing the choice between using ""it"" or ""that"" in anaphora. If both the anaphor and its antecedent are subjects, ""it"" is far more likely than ""that."" If either one or both are not subjects, ""that"" is more likely. If the antecedent is pronominal, ""it"" is more likely than ""that."" If the antecedent is non-nominal or a gerund, ""that"" is more likely. With canonical NP antecedents, both ""it"" and ""that"" are equally likely.","Question:

How does the syntactic type of an antecedent affect the decision to use ""it"" or ""that"" in anaphora?

Answer:

The syntactic type of an antecedent plays a significant role in determining whether ""it"" or ""that"" is used in anaphora. When both the anaphor and its antecedent function as subjects, ""it"" is much more likely to be chosen over ""that."" Conversely, if either the anaphor or its antecedent, or both, are not subjects, ""that"" becomes the more probable choice. Additionally, when the antecedent is pronominal, ""it"" is preferred over ""that."" In cases where the antecedent is non-nominal or a gerund, ""that"" is more commonly used. For canonical noun phrase (NP) antecedents, both ""it"" and ""that"" are equally likely to be used.",NO,True,618,True,True
51623319-s20,Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2018-09-01,Discourse-Level Preferences.,"In the literature, discourse-level properties of non-NA anaphora are discussed in terms of three notions: salience, focus, and topic. These notions are usually grounded in Centering Theory (Grosz and Sidner 1986) and theories explaining the cognitive status of these expressions.

Centering Theory models coherence and salience. It assumes that each utterance introduces new discourse entities into the discourse, which are organized in a focus space. Focus spaces, which constitute the global focus, are ordered in a stack so that only entities of the most recent space are in the local focus and, e.g., accessible for subsequent reference by pronouns. The discourse entities introduced by an utterance are ranked and the most highly ranked entity is referred to as the preferred center (CP). The backward-looking center (CB) of an utterance is defined as the highest ranked element of the previous utterance that is realized in the current utterance. The theory itself keeps many notions, such as utterance or ranking, deliberately open, and researchers define these notions differently, depending on their theory and the language under investigation. An utterance is generally defined as a sentence or a clause. Ranking is generally based on the grammatical function (e.g., subject is ranked higher than object) or on information status (e.g., hearer-old entities are ranked higher than hearer-new entities) (cf. Poesio et al. 2004).

The CP of an utterance is considered the most salient entity. The notion of CB is the closest concept to the traditional notion of topic (e.g., Taboada and Wiesemann 2010). Another relevant term is activation, which can be defined in the framework of Centering Theory in different ways. A discourse referent can be considered activated if it is in the local focus, or in the global focus and sufficiently salient (Poesio and Modjeska 2005).

Givenness Hierarchy from Gundel, Hedberg, and Zacharski (1993, page 275) The general idea behind the cognitive status of an anaphoric expression is as follows. In language, we use different expressions to refer to the same thing. For instance, a particular fact can be referred to as a fact, the fact, this fact, that fact, this, that, or it. The question is, when do writers use a particular anaphoric expression, and what enables readers to interpret this expression appropriately? Many researchers have made claims about the cognitive status of the antecedents of different expressions, and they support their claims with appropriately annotated data. Gundel, Hedberg, and Zacharski (1993) propose a Givenness Hierarchy of six cognitive statuses of discourse referents, which reflect a speaker's assumptions about the addressee's knowledge and current state of attention. These statuses determine the necessary and sufficient conditions on the use of each referring form in discourse (cf. Figure 2). For instance, by using it as an anaphor, the speaker signals that the expression refers to an entity in focus, whereas with a form such as this N, the speaker refers to an activated entity. Gundel, Hedberg, and Zacharski note that pronominal anaphors, as a universal tendency, prefer their referent to at least be activated, which makes sense because the pronouns only have minimal descriptive content, so in order to facilitate identification of their referents they have to be at least activated.

Based on these notions, the following observations have been made in the literature. 29

Referring to the entities in focus. It has been demonstrated by different researchers in different domains that the pronoun it requires its referent to be in an addressee's focus of attention, whereas demonstratives only require them to be activated, namely, present in working memory (the global focus) but not necessarily in focus (i.e., in the local focus). Hegarty, Gundel, and Borthen (2001) start from the hypothesis that the Givenness Hierarchy can explain the findings of Schiffman (1985) and Webber (1991) that nominal anaphora are preferably realized with it and non-NA anaphora with demonstratives (see Section 3.1.1). These findings can be explained if entities introduced by nominals are more easily brought into focus than their non-nominal counterparts. They link an entity's property of being in focus with its degree of world immanence (compare Section 2.1). Accordingly, concrete objects are often brought into focus, eventualities less so, and factualities only rarely. Eventualities are more accessible than factualities because they can be directly introduced by clauses, whereas factualities have to be derived from them (see Section 2.1). The findings from their corpus support this: Gundel, Hedberg, and Zacharski (2002) observed that of 2,046 instances of third-person personal pronouns (including she, they, etc.) from the Santa Barbara Corpus of Spoken American English, 83.34% had nominal antecedents and 5.38% were instances of the anaphor it with non-NAs. Among these instances, only 14.54% involved facts or propositions, 57.27% involved situations, and 27.27% eventualities. Gundel, Hedberg, and Zacharski classify situations as less abstract than facts and propositions and note that the distinction between eventualities and situations is not always clear. Hedberg, Gundel, and Zacharski (2007) analyze 321 instances of pronominal this (44%) and that (56%) in a corpus composed of two issues of the New York Times. They define an entity as activated if it is in the local focus but not salient (i.e., it has been introduced in the previous sentence but not in a syntactically prominent position). It is not clear from their paper how many instances are non-NA anaphora but the majority probably are. They observe that of 256 instances for which the annotators were in agreement, 96% of the demonstrative pronouns refer to activated entities that are not in focus. They also find that the majority of these cases are indirect anaphora, requiring coercion. An annotated example from Hedberg, Gundel, and Zacharski (page 35) is shown in Example (27). (27) <P num=""405""> With the exception of Japanese equities, <1> Americans have been selling more foreign stocks than they have been buying in recent months. </1> </P> <P num=""406""> But <that ACTIVATED INDIRECT ""the situation that Americans have been selling foreign stocks more than buying them"" 405.1 A3 num=""06""> that </that> could change. </P>

In the example, the antecedent is located in the sentence identified as 405, and is marked by the SGML tags <1>. . . </1>. The tag containing the anaphor that carries all annotated features, for example, ACTIVATED for the cognitive status, INDIRECT for the type of anaphoric relation, a description of the anaphor's referent (""the situation that . . . ""), and a pointer to the antecedent (405.1).

Referring to salient (highly ranked) entities. If being in (local) focus is defined via the focus space, all entities introduced in the same utterance share the same degree of being in focus. Such entities, however, can be differentiated by another property, salience. An entity is made salient if it is introduced in a syntactically prominent position (e.g., as the subject or object) or if it is mentioned repeatedly (Gundel, Hedberg, and Zacharski 1993;Gundel, Hegarty, and Borthen 2003). Gundel and colleagues have shown that (unstressed) personal pronouns in general are used to refer to highly ranked, salient entities in discourse, operationalized as the preferred center in Centering Theory. In contrast, demonstratives are associated with focus shift (Gundel, Hedberg, and Zacharski 1988), when the anaphor does not refer to the preferred center, contrary to the unmarked case. Hegarty, Gundel, and Borthen (2001) assume that clausal propositions, facts, or situations are more accessible to reference with it if they have already been mentally represented by the addressee. If a speaker refers to that entity, it causes the addressee to reprocess that entity, which renders it more salient. Two examples from Hegarty, Gundel, and Borthen illustrate this. Example (28a) shows that it cannot immediately be used to refer to the situation. Instead, it can only refer to the snake here. In contrast, in Example (28b), reference to the situation by that is possible, and due to that prior mention, subsequent reference by it is also possible.

(28) There was a snake on my desk.

a. # It scared me.

b. That scared me. It scared my office-mate too.

The second example illustrates that it vs. that can be used to indicate prior beliefs. In Example (29), the alternative replies (29a) and (29b) imply different background knowledge. After the statement by speaker A, the fact that linguists earn less than psychologists is at least activated. In Example (29a), speaker B uses that, thereby signalling the activated cognitive status of the abstract entity. However, in Example (29b), speaker B' uses it, thereby implying that she already knew about this fact (such that it must already have been activated before the statement by speaker A), and due to being mentioned by speaker A, it has become salient and an entity in focus.

(29) A: I just read that linguists earn less than psychologists. Poesio and Modjeska (2005) analyzed 112 instances of this and these (used as pronouns and determiners). Forty-nine percent referred to nominal antecedents and 17% to non-nominal ones. They observed that 75-80% of the instances referred to entities other than highest-ranked entity (CP).

Referring to the topic of the utterance. The pronoun it tends to refer to the topic of the conversation, whereas demonstratives tend to refer to more peripheral antecedents. Poesio and Modjeska (2005) in their study of 112 instances of this and these found evidence for the hypothesis that these anaphors are used to refer to entities other than the CB of the current utterance (this was supported by 61-65% of the instances) or the CB of the previous utterance (supported by 90-93%). 30 3.2.4 Distance Between Anaphor and Antecedent. In anaphora resolution, an important factor that affects the accessibility of antecedents is the distance between the anaphor and antecedent. Recency plays an important role in anaphora resolution systems (Mitkov 2002;Poesio, Ponzetto, and Versley 2010). A short distance between the anaphor and the antecedent implies a smaller search space and a smaller number of competing antecedent candidates, whereas a long distance implies a larger search space with many competing antecedent candidates. The distance can be measured in terms of tokens, sentences, spatiotemporal proximity, or the number of edges between nodes in some discourse structure, such as those posited by Rhetorical Structure Theory (RST) (Mann and Thompson 1988).

The distance preferences vary according to the anaphoric expressions used. We now list some tentative suggestions about these preferences from the literature.

Linear distance: demonstrative pronouns vs. personal pronouns. Comparing pronominal instances of NA and non-NA anaphora, Byron (2003, pages 34-35) observed that the more semantic information a pronoun has, the larger the average distance to its antecedent. The average distance of all pronouns in one of her data sets was 8.81 words. The largest 30 Poesio and Modjeska (2005, Section 3.1) call the CB (discourse) focus as well as topic.

568 average distance occurred with gender-marked personal pronouns (she, hers, her: 9.84; he, his, him: 9.2), followed by neuter personal pronouns (it, its, they, them, their: 8.72). Demonstrative pronouns occur closest to their antecedents (this, that, these, those: 8.18). The differences between the types do not seem large, though. Instances of non-NA anaphora would fall into one of the last two classes.

Linear distance: pronominal demonstratives vs. this NPs. The pronominal demonstratives this and that and the personal pronoun it are typically closer to their antecedents than this NPs (Schmid 2000;Kolhatkar 2015). In particular, demonstrative pronouns on their own are not particularly informative, and so the distance between the anaphor and the antecedent is fairly small and the textual coherence fairly strong (i.e., there are fewer competing candidates). In contrast, this NPs are informative because they are headed by a content noun. They license long-distance as well as short-distance antecedents, as shown in the following examples.

(30) Once an international poverty line is set, it must be converted to local currencies. This is trickier than it sounds. Currency exchange rates are inappropriate because most of the items that the poor consume are not traded on world markets. Living expenses are much lower in rural India than in New York, but this fact is not fully captured if prices are converted with currency exchange rates. (NYT)

Here, the distance between the anaphor and the antecedent is small: The antecedent of this fact occurs in the preceding clause. In contrast, in Example (31), the antecedent of this question occurs four sentences away from the anaphor sentence. 31

(31) Among Roman Catholics, the differences were even more striking. Only 28 percent of Catholics who said religion was very or extremely important to them favored keeping abortion legal, but 72 percent of Catholics for whom religion was less important favored the legal status quo.

The sense of a public struggling with a morally difficult issue was dramatically conveyed when the survey asked: ""Would you approve or disapprove of someone you know having an abortion?""

Thirty-nine percent said they would approve and 32 percent said they would disapprove. But 25 percent more volunteered a response not included in the question: They said their view would depend on the circumstances involved. An additional 5 percent did not know. The lack of a clear majority for either of the unequivocal responses to this question may be the best indicator of where public opinion really stands on abortion. (NYT)

Temporal and spatial distance: this vs. that. Among demonstrative pronouns, this tends to be associated with the entities that are spatially, temporally, or textually close to the speaker, whereas that tends to be associated with the entities that are not close to the speaker (Halliday and Hasan 1976). Textual proximity can be defined in terms of the relation between participants in a dialogue, that is, something said by the speaker versus something said by an interlocutor (Halliday and Hasan 1976). For instance, imagine a dialogue between two speakers, as shown in Example (32). Note how speaker A uses the pronoun this to refer to their own statement (i.e., it's time we take action), whereas speaker B uses that to refer to the same statement.

(32) A. It's time we take action. I know I have said this before, but now I really mean it. B. What do you mean by that?

Example (33), from Halliday and Hasan (1976), demonstrates temporal proximity preferences: That tends to be associated with a past-time referent, whereas this for one in the present or immediate future.

(33) a. We went to the opera last night. That was our first outing for months.

b. We're going to the opera tonight. This'll be our first outing for months.

Distance in a discourse structure. There are suggestions in the literature regarding the accessibility of antecedents in a discourse representation. This constraint is typically referred to as the right-frontier constraint (Polanyi 1985;Webber 1991) or the principle of availability (Asher 1993, page 313). The formal definition of the constraint varies according to the discourse representation theory and structure under consideration. That said, the general idea is that only those discourse segments can yield referents for anaphors that correspond to nodes on the right frontier of a formal discourse tree (Polanyi 1985;Webber 1991;Asher 1993, page 270;Asher 2008). The constraint is a visual representation of which salient nodes in a given discourse are accessible for later reference. The intuition is that given a discourse structure, represented as a tree, a referring expression cannot attach to a constituent to the left of the current constituent. An example from Webber (1991) is given in Example (34).

(34) a. There's two houses you might be interested in. b. House A is in Palo Alto. It's got 3 bedrooms and 2 baths, and was built in 1950. It's on a quarter acre, with a lovely garden, and the owner is asking $425K. But that's all I know about it. c. House B is in Portola Valley. It's got 3 bedrooms, 4 baths and a kidney-shaped pool, and was also built in 1950. It's on 4 acres of steep wooded slope, with a view of the mountains. The owner is asking $600K. I heard all this from a real-estate friend of mine. d. Is that enough information for you to decide which to look at? e. # But that's all I know about House A.

Here, parts (b) and (c) are central parts of the text. According to Webber, the continuation (e) is ill-formed, because, at this point, the information about House A is closed off and no longer accessible. The only accessible antecedents are the ones on the right frontier: (1) the information on both houses, that is, the information spanned by the root node and (2) the information on House B. Figure 3, from Webber (1991), illustrates the structure of the discourse tree at that stage. Only the nodes on the right side of the tree can serve as attachment points for (e). Asher (1993Asher ( , 2008 and Afantenos and Asher (2010) present a version of rightfrontier constraint in the Segmented Discourse Representation Theory (SDRT) framework. They even demonstrate that SDRT's version of this constraint is respected about 95% of the time in their corpus of texts in French from different genres.

From a computational linguistics perspective, there are two problems with this constraint. First, researchers have demonstrated violations of the constraint (e.g., Poesio, 570 !""#","How does Centering Theory model coherence and salience in discourse, and what are its key assumptions?","Centering Theory models coherence and salience by assuming that each utterance introduces new discourse entities into the discourse, which are organized in a focus space. Focus spaces, which constitute the global focus, are ordered in a stack so that only entities of the most recent space are in the local focus and accessible for subsequent reference by pronouns. The discourse entities introduced by an utterance are ranked, and the most highly ranked entity is referred to as the preferred center (CP). The backward-looking center (CB) of an utterance is defined as the highest ranked element of the previous utterance that is realized in the current utterance. The theory itself keeps many notions, such as utterance or ranking, deliberately open, and researchers define these notions differently, depending on their theory and the language under investigation. An utterance is generally defined as a sentence or a clause. Ranking is generally based on the grammatical function (e.g., subject is ranked higher than object) or on information status (e.g., hearer-old entities are ranked higher than hearer-new entities). The CP of an utterance is considered the most salient entity. The notion of CB is the closest concept to the traditional notion of topic.","What are the key assumptions of Centering Theory, and how does it model coherence and salience in discourse?","Centering Theory models coherence and salience in discourse by organizing discourse entities introduced by each utterance into a focus space. These focus spaces are structured in a stack, where only the entities from the most recent space are in the local focus and accessible for subsequent reference, often through pronouns. The discourse entities are ranked, with the most highly ranked entity referred to as the preferred center (CP). The backward-looking center (CB) of an utterance is the highest ranked element from the previous utterance that is realized in the current one. The theory intentionally leaves many concepts, such as utterance or ranking, open to interpretation, allowing researchers to define them based on their specific theoretical framework and the language being studied. Typically, an utterance is defined as a sentence or a clause, and ranking is based on grammatical function (e.g., subjects are ranked higher than objects) or information status (e.g., entities familiar to the hearer are ranked higher than new ones). The CP of an utterance is considered the most salient entity, while the CB is closely related to the traditional notion of a topic.",7,2,7,2,"Question:

How does Centering Theory model coherence and salience in discourse, and what are its key assumptions?

Answer:

Centering Theory models coherence and salience by assuming that each utterance introduces new discourse entities into the discourse, which are organized in a focus space. Focus spaces, which constitute the global focus, are ordered in a stack so that only entities of the most recent space are in the local focus and accessible for subsequent reference by pronouns. The discourse entities introduced by an utterance are ranked, and the most highly ranked entity is referred to as the preferred center (CP). The backward-looking center (CB) of an utterance is defined as the highest ranked element of the previous utterance that is realized in the current utterance. The theory itself keeps many notions, such as utterance or ranking, deliberately open, and researchers define these notions differently, depending on their theory and the language under investigation. An utterance is generally defined as a sentence or a clause. Ranking is generally based on the grammatical function (e.g., subject is ranked higher than object) or on information status (e.g., hearer-old entities are ranked higher than hearer-new entities). The CP of an utterance is considered the most salient entity. The notion of CB is the closest concept to the traditional notion of topic.","Question:

What are the key assumptions of Centering Theory, and how does it model coherence and salience in discourse?

Answer:

Centering Theory models coherence and salience in discourse by organizing discourse entities introduced by each utterance into a focus space. These focus spaces are structured in a stack, where only the entities from the most recent space are in the local focus and accessible for subsequent reference, often through pronouns. The discourse entities are ranked, with the most highly ranked entity referred to as the preferred center (CP). The backward-looking center (CB) of an utterance is the highest ranked element from the previous utterance that is realized in the current one. The theory intentionally leaves many concepts, such as utterance or ranking, open to interpretation, allowing researchers to define them based on their specific theoretical framework and the language being studied. Typically, an utterance is defined as a sentence or a clause, and ranking is based on grammatical function (e.g., subjects are ranked higher than objects) or information status (e.g., entities familiar to the hearer are ranked higher than new ones). The CP of an utterance is considered the most salient entity, while the CB is closely related to the traditional notion of a topic.",NO,True,1179,True,True
51623319-s24,Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2018-09-01,Challenges Associated with Annotating Non-NA Anaphora,"Annotating non-NA anaphora is a challenging problem and researchers have discussed associated difficulties, low inter-annotator agreement, and how they worked around the difficulties.

Classifying an anaphor as an instance of non-NA anaphora. Halliday and Hasan (1976, pages 66-68) informally analyzed 51 demonstratives (determiners and pronouns) in the last two chapters of Alice's Adventures in Wonderland as part of their investigations of text cohesion. They mention that it is not always easy to distinguish between NA and non-NA uses.

Identifying suitable non-nominal antecedents. Botley and McEnery (2001) and Botley (2006) make the point that non-NA anaphora poses difficulties for corpus-based linguistics in that 29% of the cases (186 instances) were hard to analyze. Botley (2006) points out two main reasons for this difficulty: the lack of clear surface linguistic boundaries and the complex or unclear inference process for retrieving antecedents. Poesio andModjeska (2002, 2005) analyzed 112 this NPs. They were interested in the cognitive status of this NPs in the given discourse. Because of the difficulties associated with identifying the precise antecedents of this NPs, they developed an annotation scheme where the annotators do not have to mark the actual antecedents. Rather, the scheme instructs the annotators to classify this NPs into different categories such as visual deixis, discourse deixis, and anaphoric, and, based on these categories, they assign a cognitive status to each this-NPs instance. The annotators achieved agreement of κ 0.82 in this classification task. 32 Artstein and Poesio (2006) report two experiments where 20 untrained annotators were asked to mark antecedents of NA and non-NA anaphora in a TRAINS91 dialog (Allen and Heeman 1995). 33 In the first experiment, they asked naive annotators to mark unconstrained regions of text as antecedents for NPs in general, including non-NA anaphors. In the second experiment, four annotators with prior experience annotated another dialog. This time, only (sets of) entire utterances could be marked as the antecedent. In the first experiment, in only 42% of the cases did annotators agree with the most popular choice for the beginning of the antecedent, and in 64% they agreed with the most popular choice for the end. The second experiment showed a similar tendency for annotators to agree more on the ends of the segments than on their beginnings.

Determining semantic and cognitive features. Gundel, Hedberg, and Zacharski (2004) analyzed 99 instances of demonstrative pronouns. Initially, they planned to annotate the semantic types of both the anaphor and its antecedent (which might differ because of coercion). They assumed that there are clear correlations between the syntactic form and the semantic type of an antecedent (e.g., VPs denote either activities or states). In contrast, the semantic type of the anaphor is hard to determine and can only in certain cases be deduced easily from the predicate's semantic restrictions. They therefore abandoned annotating exact semantic types and instead marked the relation as direct when the referent of the anaphor was the same as the referent of the antecedent and indirect otherwise. They classified the pronouns into six categories: nominal direct, nominal indirect, non-nominal direct, non-nominal indirect, pleonastic, and other. All three coders agreed on the classification of only 56 out of 99 instances. Hedberg, Gundel, and Zacharski (2007) analyzed 321 instances of demonstrative pronouns and report κ 0.46 (moderate agreement) for identifying the cognitive status of the antecedent (activated or in focus), and κ 0.70 (substantial agreement) for identifying the type of the antecedent (direct or indirect). 34 They do not report agreement in identifying the actual antecedents.","How do researchers address the challenges of annotating non-NA anaphora, particularly in identifying precise antecedents?","Annotating non-NA anaphora is a challenging problem and researchers have discussed associated difficulties, low inter-annotator agreement, and how they worked around the difficulties. Botley and McEnery (2001) and Botley (2006) make the point that non-NA anaphora poses difficulties for corpus-based linguistics in that 29% of the cases (186 instances) were hard to analyze. Botley (2006) points out two main reasons for this difficulty: the lack of clear surface linguistic boundaries and the complex or unclear inference process for retrieving antecedents. Poesio and Modjeska (2002, 2005) analyzed 112 this NPs. They were interested in the cognitive status of this NPs in the given discourse. Because of the difficulties associated with identifying the precise antecedents of this NPs, they developed an annotation scheme where the annotators do not have to mark the actual antecedents. Rather, the scheme instructs the annotators to classify this NPs into different categories such as visual deixis, discourse deixis, and anaphoric, and, based on these categories, they assign a cognitive status to each this-NPs instance. The annotators achieved agreement of κ 0.82 in this classification task. Artstein and Poesio (2006) report two experiments where 20 untrained annotators were asked to mark antecedents of NA and non-NA anaphora in a TRAINS91 dialog (Allen and Heeman 1995). In the first experiment, they asked naive annotators to mark unconstrained regions of text as antecedents for NPs in general, including non-NA anaphors. In the second experiment, four annotators with prior experience annotated another dialog. This time, only (sets of) entire utterances could be marked as the antecedent. In the first experiment, in only 42% of the cases did annotators agree with the most popular choice for the beginning of the antecedent, and in 64% they agreed with the most popular choice for the end. The second experiment showed a similar tendency for annotators to agree more on the ends of the segments than on their beginnings.","How do researchers tackle the challenges of annotating non-nominal anaphora, especially in identifying precise antecedents?","Annotating non-nominal anaphora presents significant challenges, as highlighted by researchers who have noted difficulties and low inter-annotator agreement in this area. Botley and McEnery (2001) and Botley (2006) emphasize that non-nominal anaphora is particularly problematic for corpus-based linguistics, with 29% of cases (186 instances) being difficult to analyze. Botley (2006) identifies two main reasons for these challenges: the absence of clear surface linguistic boundaries and the complex or unclear inference process required to retrieve antecedents.

To address these issues, Poesio and Modjeska (2002, 2005) analyzed 112 instances of ""this"" noun phrases (NPs), focusing on their cognitive status within discourse. Due to the difficulties in pinpointing precise antecedents for ""this"" NPs, they developed an annotation scheme that does not require annotators to mark actual antecedents. Instead, annotators classify ""this"" NPs into categories such as visual deixis, discourse deixis, and anaphoric, and assign a cognitive status to each instance. This approach resulted in an inter-annotator agreement of κ 0.82.

Further experiments by Artstein and Poesio (2006) involved 20 untrained annotators marking antecedents of both nominal and non-nominal anaphora in a TRAINS91 dialog (Allen and Heeman 1995). In the first experiment, naive annotators marked unconstrained text regions as antecedents for NPs, including non-nominal anaphors. In the second experiment, four experienced annotators marked another dialog, with the constraint that only entire utterances could be marked as antecedents. The first experiment showed that annotators agreed with the most popular choice for the beginning of the antecedent in only 42% of cases, and 64% for the end. The second experiment revealed a similar trend, with annotators more likely to agree on the ends of segments than on their beginnings.",7,4,7,4,"Question:

How do researchers address the challenges of annotating non-NA anaphora, particularly in identifying precise antecedents?

Answer:

Annotating non-NA anaphora is a challenging problem and researchers have discussed associated difficulties, low inter-annotator agreement, and how they worked around the difficulties. Botley and McEnery (2001) and Botley (2006) make the point that non-NA anaphora poses difficulties for corpus-based linguistics in that 29% of the cases (186 instances) were hard to analyze. Botley (2006) points out two main reasons for this difficulty: the lack of clear surface linguistic boundaries and the complex or unclear inference process for retrieving antecedents. Poesio and Modjeska (2002, 2005) analyzed 112 this NPs. They were interested in the cognitive status of this NPs in the given discourse. Because of the difficulties associated with identifying the precise antecedents of this NPs, they developed an annotation scheme where the annotators do not have to mark the actual antecedents. Rather, the scheme instructs the annotators to classify this NPs into different categories such as visual deixis, discourse deixis, and anaphoric, and, based on these categories, they assign a cognitive status to each this-NPs instance. The annotators achieved agreement of κ 0.82 in this classification task. Artstein and Poesio (2006) report two experiments where 20 untrained annotators were asked to mark antecedents of NA and non-NA anaphora in a TRAINS91 dialog (Allen and Heeman 1995). In the first experiment, they asked naive annotators to mark unconstrained regions of text as antecedents for NPs in general, including non-NA anaphors. In the second experiment, four annotators with prior experience annotated another dialog. This time, only (sets of) entire utterances could be marked as the antecedent. In the first experiment, in only 42% of the cases did annotators agree with the most popular choice for the beginning of the antecedent, and in 64% they agreed with the most popular choice for the end. The second experiment showed a similar tendency for annotators to agree more on the ends of the segments than on their beginnings.","Question:

How do researchers tackle the challenges of annotating non-nominal anaphora, especially in identifying precise antecedents?

Answer:

Annotating non-nominal anaphora presents significant challenges, as highlighted by researchers who have noted difficulties and low inter-annotator agreement in this area. Botley and McEnery (2001) and Botley (2006) emphasize that non-nominal anaphora is particularly problematic for corpus-based linguistics, with 29% of cases (186 instances) being difficult to analyze. Botley (2006) identifies two main reasons for these challenges: the absence of clear surface linguistic boundaries and the complex or unclear inference process required to retrieve antecedents.

To address these issues, Poesio and Modjeska (2002, 2005) analyzed 112 instances of ""this"" noun phrases (NPs), focusing on their cognitive status within discourse. Due to the difficulties in pinpointing precise antecedents for ""this"" NPs, they developed an annotation scheme that does not require annotators to mark actual antecedents. Instead, annotators classify ""this"" NPs into categories such as visual deixis, discourse deixis, and anaphoric, and assign a cognitive status to each instance. This approach resulted in an inter-annotator agreement of κ 0.82.

Further experiments by Artstein and Poesio (2006) involved 20 untrained annotators marking antecedents of both nominal and non-nominal anaphora in a TRAINS91 dialog (Allen and Heeman 1995). In the first experiment, naive annotators marked unconstrained text regions as antecedents for NPs, including non-nominal anaphors. In the second experiment, four experienced annotators marked another dialog, with the constraint that only entire utterances could be marked as antecedents. The first experiment showed that annotators agreed with the most popular choice for the beginning of the antecedent in only 42% of cases, and 64% for the end. The second experiment revealed a similar trend, with annotators more likely to agree on the ends of segments than on their beginnings.",NO,True,1901,True,True
51623319-s39,Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2018-09-01,587,"Another distinction among these corpora is with respect to the domain, which affects the difficulty level of the annotation. If we assume a spectrum with closed-domain corpora at one end and open-domain corpora at the other, Byron's (2003) corpus and parts of Uryupina et al.'s (2018) ARRAU corpus would be close to the closed-domain end, as TRAINS93 dialogues and the GNOME corpus are closed domain corpora, where the topics or objects of discussion are fixed. On the other end of the spectrum, we have Eckert and Strube's (2000) annotation of a subset of the Switchboard corpus and Müller's (2008) annotation of unrestricted multi-party dialogues from the ICSI Meeting corpus. Kolhatkar and Hirst's (2012) corpus of MEDLINE abstracts is more varied than the TRAINS93 dialogue and the GNOME corpus, but it is still on the closed-domain side. The corpora of Kolhatkar, Zinsmeister, and Hirst (2013a,b) fall somewhere in the middle of the spectrum.

Another difference concerns the coverage of the annotation. We see three kinds of approaches to annotate non-NAs: annotating semantic type, annotating representative verbal head antecedents (which act as proxies for clausal linguistic antecedents), and annotating linguistic antecedents.

Annotating semantic type. Some approaches annotate semantic types, like event or fact. As we discussed in Section 3.2.1, the semantic type of the anaphor is determined by the anaphor's context. However, the semantic type of the antecedent, as determined by the meaning of the antecedent, also plays a role. The two types can differ, a phenomenon referred to as referent coercion.

In a first attempt, Gundel, Hedberg, and Zacharski (2004) annotated the semantic types of antecedents. They assumed that clauses denote eventualities (either events or states of affairs/situations, depending on whether the predicate is eventive or stative), and VPs denote either activities or states, so determining the antecedent's type was relatively straightforward in these cases. For the anaphor, semantic constraints of the predicate had to be used as cues. This proved difficult for multiple reasons: Either the predicates of the pronouns did not force a single interpretation, there was no suitable term to label the type, or the referent was too vague. They switched to annotating the kind of relation as direct or indirect instead. Byron (2003) also annotated the anaphor's semantic type. In the TRAINS corpus, semantic types are strongly domain-dependent (e.g., a plan or the time taken by an action). The BUR corpus shows a greater variety of types, with types like events or processes. She also tried to label the relation between the antecedent and the anaphor, but the student annotators had too many problems with this task, and so it was abandoned.

Annotating verbal head antecedents. Müller's (2008) scheme (as well as Weischedel et al. [2013] and the Prague Dependency Treebank [Nedoluzhko et al. 2016]) marks representative verbal heads for non-NAs, assuming that they act as proxies for clausal or sentential antecedents. This scheme thus provides a degree of flexibility and is able to avoid some problems associated with annotating precise antecedents. However, there are two problems with this scheme. First, the verb gives only partial information about the antecedent and its type. Only marking a verb as the antecedent would not tell us whether we are talking about an event, a concept, or a fact. Moreover, if it is an event, for instance, it is not clear which arguments of the verb should be included in the antecedent. Second, antecedents with multiple verbs or with discontinuous antecedents cannot be expressed effectively with this annotation scheme.

Annotating linguistic antecedents. Eckert and Strube (2000), Byron (2003), Artstein and Poesio (2006), Kolhatkar and Hirst (2012) and Kolhatkar, Zinsmeister, and Hirst (2013a) mark clausal, sentential, and verbal syntactic constituents. The main issue is the underspecification of such antecedents-all references do not need to be fully specified for successful communication. Recasens (2008) suggests that computational approaches should bear this in mind and that annotation efforts must not insist on setting fixed boundaries in every case.

Several researchers point out the difficulties associated with annotating different aspects of this phenomenon, in particular with respect to identifying the precise boundaries of non-NAs. There is no standard way to report inter-annotator agreement for this kind of annotation. Some studies use Krippendorff's α with distance metrics such as Dice and Jaccard; others use Krippendorff's unitizing alpha. The agreement numbers in either case are rather low, especially for open domains such as newswire. Some studies report only observed percentage agreement with results in the range of about 0.40-0.55 Vieira et al. (2002); Dipper et al. (2011). 76 Table 4 summarizes prominent annotation efforts in non-NA anaphora. The primary focus of annotation has been on the demonstratives this and that and the personal pronoun it. Most of the studies were carried out as preliminary investigations, and very few corpora are available for reuse. Also, the size of most of the corpora is relatively small for training a machine learning system. In Table 4, we mark publicly available corpora with an asterisk (*).

The data format and the tool used in the annotation process often have an impact on the design decisions of the annotation schemes or the workflow. The most commonly used annotation tools in non-NA anaphora annotation are: MMAX2 (Strube and Müller 2003), the AnCoraPipe annotation suite (Bertran et al. 2008), TrEd (""TreeEditor"") (Pajas andŠtěpánek 2008), 77 and PALinkA (Orȃsan 2003). See  for a review of the currently available corpora for anaphora and tools to create such corpora. 78 Although a few projects have attempted to annotate non-NA anaphora in a way that can be useful for the development of computational systems (e.g., the ASN Corpus by Kolhatkar, Zinsmeister, and Hirst and the ARRAU corpus by Uryupina et al.), if we want to see real progress in computational methods for this phenomenon, we will need larger, systematically annotated corpora for benchmarking computational systems. We have created a GitHub repository 79 for documenting all the relevant resources for non-NA anaphora.","What are the three approaches to annotating non-nominal antecedents, and what challenges do they face?","Another difference concerns the coverage of the annotation. There are three kinds of approaches to annotate non-nominal antecedents: annotating semantic type, annotating representative verbal head antecedents, and annotating linguistic antecedents. Annotating semantic type involves annotating semantic types, like event or fact. The semantic type of the anaphor is determined by the anaphor's context, while the semantic type of the antecedent is determined by the meaning of the antecedent. Challenges include the difficulty in determining the antecedent's type and the lack of suitable terms to label the type. Annotating verbal head antecedents involves marking representative verbal heads for non-nominal antecedents, assuming they act as proxies for clausal or sentential antecedents. Challenges include the verb providing only partial information about the antecedent and its type, and the difficulty in expressing antecedents with multiple verbs or discontinuous antecedents. Annotating linguistic antecedents involves marking clausal, sentential, and verbal syntactic constituents. The main issue is the underspecification of such antecedents, as not all references need to be fully specified for successful communication. There is no standard way to report inter-annotator agreement for this kind of annotation, and agreement numbers are rather low, especially for open domains.","What are the three approaches to annotating non-nominal antecedents, and what challenges do they face?","There are three primary approaches to annotating non-nominal antecedents: annotating semantic type, annotating representative verbal head antecedents, and annotating linguistic antecedents. 

1. **Annotating Semantic Type**: This approach involves identifying and labeling the semantic types of antecedents, such as events or facts. The semantic type of the anaphor is determined by its context, while the antecedent's type is derived from its meaning. The challenges here include the difficulty in accurately determining the antecedent's type and the lack of suitable terms for labeling these types.

2. **Annotating Representative Verbal Head Antecedents**: This method focuses on marking representative verbal heads for non-nominal antecedents, assuming these verbs act as proxies for clausal or sentential antecedents. The challenges include the fact that a verb may provide only partial information about the antecedent and its type, and the complexity of expressing antecedents that involve multiple verbs or are discontinuous.

3. **Annotating Linguistic Antecedents**: This approach involves marking clausal, sentential, and verbal syntactic constituents. The main challenge is the underspecification of such antecedents, as not all references need to be fully specified for effective communication. Additionally, there is no standard method for reporting inter-annotator agreement in this type of annotation, and agreement levels tend to be low, particularly in open domains.",7,2,7,2,"Question:

What are the three approaches to annotating non-nominal antecedents, and what challenges do they face?

Answer:

Another difference concerns the coverage of the annotation. There are three kinds of approaches to annotate non-nominal antecedents: annotating semantic type, annotating representative verbal head antecedents, and annotating linguistic antecedents. Annotating semantic type involves annotating semantic types, like event or fact. The semantic type of the anaphor is determined by the anaphor's context, while the semantic type of the antecedent is determined by the meaning of the antecedent. Challenges include the difficulty in determining the antecedent's type and the lack of suitable terms to label the type. Annotating verbal head antecedents involves marking representative verbal heads for non-nominal antecedents, assuming they act as proxies for clausal or sentential antecedents. Challenges include the verb providing only partial information about the antecedent and its type, and the difficulty in expressing antecedents with multiple verbs or discontinuous antecedents. Annotating linguistic antecedents involves marking clausal, sentential, and verbal syntactic constituents. The main issue is the underspecification of such antecedents, as not all references need to be fully specified for successful communication. There is no standard way to report inter-annotator agreement for this kind of annotation, and agreement numbers are rather low, especially for open domains.","Question:

What are the three approaches to annotating non-nominal antecedents, and what challenges do they face?

Answer:

There are three primary approaches to annotating non-nominal antecedents: annotating semantic type, annotating representative verbal head antecedents, and annotating linguistic antecedents. 

1. **Annotating Semantic Type**: This approach involves identifying and labeling the semantic types of antecedents, such as events or facts. The semantic type of the anaphor is determined by its context, while the antecedent's type is derived from its meaning. The challenges here include the difficulty in accurately determining the antecedent's type and the lack of suitable terms for labeling these types.

2. **Annotating Representative Verbal Head Antecedents**: This method focuses on marking representative verbal heads for non-nominal antecedents, assuming these verbs act as proxies for clausal or sentential antecedents. The challenges include the fact that a verb may provide only partial information about the antecedent and its type, and the complexity of expressing antecedents that involve multiple verbs or are discontinuous.

3. **Annotating Linguistic Antecedents**: This approach involves marking clausal, sentential, and verbal syntactic constituents. The main challenge is the underspecification of such antecedents, as not all references need to be fully specified for effective communication. Additionally, there is no standard method for reporting inter-annotator agreement in this type of annotation, and agreement levels tend to be low, particularly in open domains.",NO,True,1484,True,True
51623319-s42,Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,2018-09-01,Work Corpus data Anaphora,"Schiffman ( Botley (2006) and American Printing that: 244, these: 9, those: 1) House for the Blind Gundel, Hedberg, and Santa Barbara Corpus of ( ‡) 110 personal pronouns ( syntactically represented with a non-NA (albeit not referring to an abstract entity in our sense).

(41) A number is multiplied by 6. This product is increased by 44. The result is 68. Find the number.

Winograd's (1972) SHRDLU system resolves it and that by remembering the last possible event. Furthermore, Winograd's heuristic stipulates that it refers to the event mentioned by the speaker, whereas that can refer to the last event mentioned by anyone.

Though they provide interesting clues, these heuristics do not cover the entire range of the phenomenon and they are unlikely to be particularly effective outside of the considerably restricted domains for which they were conceived. Since then, more sophisticated approaches have been developed that attempt to meet the challenges particular to this task.

What makes the resolution of non-NA anaphora particularly challenging with respect to conventional anaphora resolution or coreference resolution is that precisely those features that make those tasks tractable problems are the ones that are missing in this domain. Thus, systems that attempt the resolution of non-NA anaphora must attempt to access semantic or discourse-related information, which is not always easily accessible, in order to make resolution decisions.

Whereas NP coreference algorithms can easily and efficiently select an appropriate set of candidate antecedents, namely, by considering NPs only, this is not possible for non-NA anaphora. The antecedents can have a great variety of syntactic shapes and can be difficult to distinguish from one another, as in Example (2), repeated here. Here, whether may or may not be included in the antecedent and it is unclear to what degree these are different antecedents. Furthermore, the same constituent may represent various semantic entities, partly depending on which anaphor is used to refer to it and on the anaphor's context. It is unclear which of these potential candidates ought to be offered to an algorithm and when, since it would be inefficient to consider all their possible variations proactively.

(2) The municipal council had to decide whether to balance the budget by raising revenue or cutting spending. The council had to come to a resolution by the end of the month. This issue was dividing communities across the country.

Second, agreement features, such as number and gender, critical to the resolution of nominal anaphora, are generally absent for non-NA anaphora. Rather, the features that are useful for determining the compatibility of anaphors and their non-NAs tend to refer to levels of annotation, such as semantic and discourse structures, that are not easily accessible or generally available.

Finally, existing NP coreference algorithms can also generally depend on there being multiple references to a single entity, in which case each mention offers additional information about the entity being described, which in turn can be useful to resolution algorithms. Though some resources, such as ARRAU (Poesio et al. 2013), 80 include such referential chains, many others do not. However, even where these chains are available, their usefulness for non-NA anaphora is limited by the lack of agreement features and the ability of anaphors to adjust the types of their antecedents (cf. referent coercion in Section 3.2.1, Example (22)). As a result, resolution algorithms for non-NA anaphora generally must consider each instance in isolation.

In this section, we will present some attempts that have been made to address these particular aspects of non-NA anaphora resolution and discuss their effectiveness as well as the implications this has for future work in this area.",What are the challenges and considerations for resolution algorithms in handling non-nominal antecedent anaphora?,"What makes the resolution of non-NA anaphora particularly challenging with respect to conventional anaphora resolution or coreference resolution is that precisely those features that make those tasks tractable problems are the ones that are missing in this domain. Thus, systems that attempt the resolution of non-NA anaphora must attempt to access semantic or discourse-related information, which is not always easily accessible, in order to make resolution decisions. Whereas NP coreference algorithms can easily and efficiently select an appropriate set of candidate antecedents, namely, by considering NPs only, this is not possible for non-NA anaphora. The antecedents can have a great variety of syntactic shapes and can be difficult to distinguish from one another. Furthermore, the same constituent may represent various semantic entities, partly depending on which anaphor is used to refer to it and on the anaphor's context. It is unclear which of these potential candidates ought to be offered to an algorithm and when, since it would be inefficient to consider all their possible variations proactively. Second, agreement features, such as number and gender, critical to the resolution of nominal anaphora, are generally absent for non-NA anaphora. Rather, the features that are useful for determining the compatibility of anaphors and their non-NAs tend to refer to levels of annotation, such as semantic and discourse structures, that are not easily accessible or generally available. Finally, existing NP coreference algorithms can also generally depend on there being multiple references to a single entity, in which case each mention offers additional information about the entity being described, which in turn can be useful to resolution algorithms. However, even where these chains are available, their usefulness for non-NA anaphora is limited by the lack of agreement features and the ability of anaphors to adjust the types of their antecedents. As a result, resolution algorithms for non-NA anaphora generally must consider each instance in isolation.","What challenges do resolution algorithms face when dealing with non-nominal antecedent anaphora, and what considerations must be taken into account?","Resolving non-nominal antecedent (non-NA) anaphora presents unique challenges compared to conventional anaphora or coreference resolution. The features that typically make these tasks manageable are often absent in the context of non-NA anaphora. Systems attempting to resolve non-NA anaphora must rely on semantic or discourse-related information, which is not always readily accessible, to make informed decisions. Unlike NP coreference algorithms, which can efficiently select candidate antecedents by focusing on noun phrases (NPs), non-NA anaphora lacks this straightforward approach. The antecedents can exhibit a wide variety of syntactic forms, making them difficult to distinguish. Additionally, the same constituent may represent different semantic entities, influenced by the specific anaphor used and its context. It is challenging to determine which potential candidates should be presented to an algorithm and when, as considering all possible variations proactively would be inefficient.

Moreover, agreement features such as number and gender, which are crucial for resolving nominal anaphora, are generally absent in non-NA anaphora. Instead, useful features for determining the compatibility of anaphors and their non-NAs often involve levels of annotation, such as semantic and discourse structures, which are not easily accessible or widely available. Existing NP coreference algorithms typically benefit from multiple references to a single entity, where each mention provides additional information about the entity, aiding resolution. However, even when these reference chains are available, their usefulness for non-NA anaphora is limited due to the lack of agreement features and the ability of anaphors to modify the types of their antecedents. Consequently, resolution algorithms for non-NA anaphora generally need to consider each instance in isolation.",7,4,7,4,"Question:

What are the challenges and considerations for resolution algorithms in handling non-nominal antecedent anaphora?

Answer:

What makes the resolution of non-NA anaphora particularly challenging with respect to conventional anaphora resolution or coreference resolution is that precisely those features that make those tasks tractable problems are the ones that are missing in this domain. Thus, systems that attempt the resolution of non-NA anaphora must attempt to access semantic or discourse-related information, which is not always easily accessible, in order to make resolution decisions. Whereas NP coreference algorithms can easily and efficiently select an appropriate set of candidate antecedents, namely, by considering NPs only, this is not possible for non-NA anaphora. The antecedents can have a great variety of syntactic shapes and can be difficult to distinguish from one another. Furthermore, the same constituent may represent various semantic entities, partly depending on which anaphor is used to refer to it and on the anaphor's context. It is unclear which of these potential candidates ought to be offered to an algorithm and when, since it would be inefficient to consider all their possible variations proactively. Second, agreement features, such as number and gender, critical to the resolution of nominal anaphora, are generally absent for non-NA anaphora. Rather, the features that are useful for determining the compatibility of anaphors and their non-NAs tend to refer to levels of annotation, such as semantic and discourse structures, that are not easily accessible or generally available. Finally, existing NP coreference algorithms can also generally depend on there being multiple references to a single entity, in which case each mention offers additional information about the entity being described, which in turn can be useful to resolution algorithms. However, even where these chains are available, their usefulness for non-NA anaphora is limited by the lack of agreement features and the ability of anaphors to adjust the types of their antecedents. As a result, resolution algorithms for non-NA anaphora generally must consider each instance in isolation.","Question:

What challenges do resolution algorithms face when dealing with non-nominal antecedent anaphora, and what considerations must be taken into account?

Answer:

Resolving non-nominal antecedent (non-NA) anaphora presents unique challenges compared to conventional anaphora or coreference resolution. The features that typically make these tasks manageable are often absent in the context of non-NA anaphora. Systems attempting to resolve non-NA anaphora must rely on semantic or discourse-related information, which is not always readily accessible, to make informed decisions. Unlike NP coreference algorithms, which can efficiently select candidate antecedents by focusing on noun phrases (NPs), non-NA anaphora lacks this straightforward approach. The antecedents can exhibit a wide variety of syntactic forms, making them difficult to distinguish. Additionally, the same constituent may represent different semantic entities, influenced by the specific anaphor used and its context. It is challenging to determine which potential candidates should be presented to an algorithm and when, as considering all possible variations proactively would be inefficient.

Moreover, agreement features such as number and gender, which are crucial for resolving nominal anaphora, are generally absent in non-NA anaphora. Instead, useful features for determining the compatibility of anaphors and their non-NAs often involve levels of annotation, such as semantic and discourse structures, which are not easily accessible or widely available. Existing NP coreference algorithms typically benefit from multiple references to a single entity, where each mention provides additional information about the entity, aiding resolution. However, even when these reference chains are available, their usefulness for non-NA anaphora is limited due to the lack of agreement features and the ability of anaphors to modify the types of their antecedents. Consequently, resolution algorithms for non-NA anaphora generally need to consider each instance in isolation.",NO,True,1881,True,True
215238860-s6,"More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction",2020-04-07,Utilizing More Data,"Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009;Nguyen and Moschitti, 2011;Min et al., 2013). As shown in Figure 3, for any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme.  Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label ""Bill Gates retired from Microsoft"" with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs.

The existing methods to alleviate the noise problem can be divided into three major approaches:

(1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017;Han et al., 2018b;Zhang et al., 2019a;Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity Wang et al., 2018).

(3) Many methods tend to utilize sophisticated mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks.  incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018;Zeng et al., 2018) and adversarial training (Wu et al., 2017;Wang et al., 2018;Han et al., 2018a) have also been adopted in DS.

The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring:

(1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schemes are still similar to the original one in (Mintz et al., 2009), which just covers the case that the entity pairs are mentioned in the same sentences. To achieve better coverage and less noise, exploring better DS schemes for autolabeling data is also valuable.

(2) Inspired by recent work in adopting pretrained language models Wu and He, 2019;Baldini Soares et al., 2019) and active learning (Zheng et al., 2019) for RE, to perform unsupervised or semi-supervised learning for utilizing large-scale unlabeled data as well as using knowledge from KGs and introducing human experts in the loop is also promising.

Besides addressing existing approaches and future directions, we also propose a new DS dataset to advance this field, which will be released once the paper is published. The most used benchmark for DS, NYT-10 (Riedel et al., 2010), suffers from small amount of relations, limited relation domains and extreme long-tail relation performance. To alleviate these drawbacks, we utilize Wikipedia and Wikidata (Vrandečić and Krötzsch, 2014) to construct Wiki-Distant in the same way as Riedel et al. (2010). As demonstrated in Table 1, Wiki-Distant covers more relations and possesses more instances, with a more reasonable N/A proportion. Comparison results of state-of-the-art models on these two datasets 2 are shown in Table 2, indicating that Wiki-Distant is more challenging and there is a long way to resolve distantly supervised RE.",What are the existing methods to address the noise problem in distant supervision for relation extraction?,"The existing methods to address the noise problem in distant supervision for relation extraction can be divided into three major approaches. First, some methods adopt multi-instance learning by combining sentences with the same entity pairs and then selecting informative instances from them. Second, incorporating extra context information to denoise DS data has been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for information consistency and complementarity (Wang et al., 2018). Third, many methods tend to utilize sophisticated mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016) and Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks and incorporate a soft-label scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS.",What strategies are currently employed to mitigate the noise issue in distant supervision for relation extraction?,"To tackle the noise problem in distant supervision for relation extraction, researchers have developed several strategies that can be broadly categorized into three main approaches. Firstly, multi-instance learning is employed by grouping sentences with the same entity pairs and selecting the most informative instances from them. Secondly, additional context information is incorporated to reduce noise in distant supervision data. This includes using knowledge graphs (KGs) as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and leveraging multi-lingual corpora to ensure information consistency and complementarity (Wang et al., 2018). Thirdly, sophisticated mechanisms and training strategies are utilized to enhance distantly supervised neural relation extraction (NRE) models. For instance, Vu et al. (2016) and Beltagy et al. (2019) combine different architectures and training strategies to create hybrid frameworks and implement a soft-label scheme by adjusting unconfident labels during training. Additionally, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have been adopted to further improve the performance of distant supervision.",7,4,7,4,"Question:

What are the existing methods to address the noise problem in distant supervision for relation extraction?

Answer:

The existing methods to address the noise problem in distant supervision for relation extraction can be divided into three major approaches. First, some methods adopt multi-instance learning by combining sentences with the same entity pairs and then selecting informative instances from them. Second, incorporating extra context information to denoise DS data has been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for information consistency and complementarity (Wang et al., 2018). Third, many methods tend to utilize sophisticated mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016) and Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks and incorporate a soft-label scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS.","Question:

What strategies are currently employed to mitigate the noise issue in distant supervision for relation extraction?

Answer:

To tackle the noise problem in distant supervision for relation extraction, researchers have developed several strategies that can be broadly categorized into three main approaches. Firstly, multi-instance learning is employed by grouping sentences with the same entity pairs and selecting the most informative instances from them. Secondly, additional context information is incorporated to reduce noise in distant supervision data. This includes using knowledge graphs (KGs) as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and leveraging multi-lingual corpora to ensure information consistency and complementarity (Wang et al., 2018). Thirdly, sophisticated mechanisms and training strategies are utilized to enhance distantly supervised neural relation extraction (NRE) models. For instance, Vu et al. (2016) and Beltagy et al. (2019) combine different architectures and training strategies to create hybrid frameworks and implement a soft-label scheme by adjusting unconfident labels during training. Additionally, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have been adopted to further improve the performance of distant supervision.",NO,True,1317,True,True
56657817-s2,Analysis Methods in Neural Language Processing: A Survey,2018-12-21,Methods,"The most common approach for associating neural network components with linguistic properties is to predict such properties from activations of the neural network. Typically, in this approach a neural network model is trained on some task (say, MT) and its weights are frozen. Then, the trained model is used for generating feature representations for another task by running it on a corpus with linguistic annotations and recording the representations (say, hidden state activations). Another classifier is then used for predicting the property of interest (say, part-of-speech [POS] tags). The performance of this classifier is used for evaluating the quality of the generated representations, and by proxy that of the original model. This kind of approach has been used in numerous papers in recent years; see Table SM1 for references. 5 It is referred to by various names, including ''auxiliary prediction tasks'' (Adi et al., 2017b), ''diagnostic classifiers'' (Veldhoen et al., 2016), and ''probing tasks' ' (Conneau et al., 2018).

As an example of this approach, let us walk through an application to analyzing syntax in neural machine translation (NMT) by Shi et al. (2016b). In this work, two NMT models were trained on standard parallel data-English→ French and English→German. The trained models (specifically, the encoders) were run on an annotated corpus and their hidden states were used for training a logistic regression classifier that predicts different syntactic properties. The authors concluded that the NMT encoders learn significant syntactic information at both word level and sentence level. They also compared representations at different encoding layers and found that ''local features are somehow preserved in the lower layer whereas more global, abstract information tends to be stored in the upper layer.'' These results demonstrate the kind of insights that the classification analysis may lead to, especially when comparing different models or model components.

Other methods for finding correspondences between parts of the neural network and certain properties include counting how often attention weights agree with a linguistic property like anaphora resolution (Voita et al., 2018) or directly computing correlations between neural network activations and some property; for example, correlating RNN state activations with depth in a syntactic tree (Qian et al., 2016a) or with Melfrequency cepstral coefficient (MFCC) acoustic features (Wu and King, 2016). Such correspondence may also be computed indirectly. For instance, Alishahi et al. (2017) defined an ABX discrimination task to evaluate how a neural model of speech (grounded in vision) encoded phonology. Given phoneme representations from different layers in their model, and three phonemes, A, B, and X, they compared whether the model representation for X is closer to A or B. This discrimination task enabled them to draw conclusions about which layers encoder phonology better, observing that lower layers generally encode more phonological information.",How is the most common approach for associating neural network components with linguistic properties implemented and evaluated?,"The most common approach for associating neural network components with linguistic properties is to predict such properties from activations of the neural network. Typically, in this approach, a neural network model is trained on some task (say, MT) and its weights are frozen. Then, the trained model is used for generating feature representations for another task by running it on a corpus with linguistic annotations and recording the representations (say, hidden state activations). Another classifier is then used for predicting the property of interest (say, part-of-speech [POS] tags). The performance of this classifier is used for evaluating the quality of the generated representations, and by proxy that of the original model. This kind of approach has been used in numerous papers in recent years.","What is the most common method for linking neural network components to linguistic properties, and how is it implemented and evaluated?","The most common method for linking neural network components to linguistic properties involves predicting these properties from the activations of the neural network. Typically, this process begins with training a neural network model on a specific task, such as machine translation (MT), and then freezing its weights. The trained model is subsequently used to generate feature representations for a different task. This is done by running the model on a corpus that includes linguistic annotations and recording the resulting representations, such as hidden state activations. To predict the linguistic property of interest, such as part-of-speech (POS) tags, another classifier is employed. The performance of this classifier serves as an evaluation metric for the quality of the generated representations, and indirectly, the effectiveness of the original model. This approach has been widely adopted in numerous research papers in recent years.",7,2,7,2,"Question:

How is the most common approach for associating neural network components with linguistic properties implemented and evaluated?

Answer:

The most common approach for associating neural network components with linguistic properties is to predict such properties from activations of the neural network. Typically, in this approach, a neural network model is trained on some task (say, MT) and its weights are frozen. Then, the trained model is used for generating feature representations for another task by running it on a corpus with linguistic annotations and recording the representations (say, hidden state activations). Another classifier is then used for predicting the property of interest (say, part-of-speech [POS] tags). The performance of this classifier is used for evaluating the quality of the generated representations, and by proxy that of the original model. This kind of approach has been used in numerous papers in recent years.","Question:

What is the most common method for linking neural network components to linguistic properties, and how is it implemented and evaluated?

Answer:

The most common method for linking neural network components to linguistic properties involves predicting these properties from the activations of the neural network. Typically, this process begins with training a neural network model on a specific task, such as machine translation (MT), and then freezing its weights. The trained model is subsequently used to generate feature representations for a different task. This is done by running the model on a corpus that includes linguistic annotations and recording the resulting representations, such as hidden state activations. To predict the linguistic property of interest, such as part-of-speech (POS) tags, another classifier is employed. The performance of this classifier serves as an evaluation metric for the quality of the generated representations, and indirectly, the effectiveness of the original model. This approach has been widely adopted in numerous research papers in recent years.",NO,True,949,True,True
56657817-s12,Analysis Methods in Neural Language Processing: A Survey,2018-12-21,Construction Method,"Challenge sets are usually created either programmatically or manually, by handcrafting specific examples. Often, semi-automatic methods are used to compile an initial list of examples that is manually verified by annotators. The specific method also affects the kind of language use and how natural or artificial/synthetic the examples are. We describe here some trends in dataset construction methods in the hope that they may be useful for researchers contemplating new datasets. Several datasets were constructed by modifying or extracting examples from existing datasets. For instance, Sanchez et al. (2018) and Glockner et al. (2018) extracted examples from SNLI (Bowman et al., 2015) and replaced specific words such as hypernyms, synonyms, and antonyms, followed by manual verification. Linzen et al. (2016), on the other hand, extracted examples of subject-verb agreement from raw texts using heuristics, resulting in a large-scale dataset. Gulordava et al. (2018) extended this to other agreement phenomena, but they relied on syntactic information available in treebanks, resulting in a smaller dataset.

Several challenge sets utilize existing test suites, either as a direct source of examples (Burchardt et al., 2017) or for searching similar naturally occurring examples . 12 Sennrich (2017) introduced a method for evaluating NMT systems via contrastive translation pairs, where the system is asked to estimate the probability of two candidate translations that are designed to reflect specific linguistic properties. Sennrich generated such pairs programmatically by applying simple heuristics, such as changing gender and number to induce agreement errors, resulting in a large-scale challenge set of close to 100 thousand examples. This framework was extended to evaluate other properties, but often requiring more sophisticated generation methods like using morphological analyzers/ generators (Burlot and Yvon, 2017) or more manual involvement in generation (Bawden et al., 2018) or verification (Rios Gonzales et al., 2017).

Finally, a few studies define templates that capture certain linguistic properties and instantiate them with word lists (Dasgupta et al., 2018;Rudinger et al., 2018;Zhao et al., 2018a). Template-based generation has the advantage of providing more control, for example for obtaining a specific vocabulary distribution, but this comes at the expense of how natural the examples are.",What are the advantages and disadvantages of using template-based generation for constructing challenge sets?,"Template-based generation has the advantage of providing more control, for example for obtaining a specific vocabulary distribution. However, this comes at the expense of how natural the examples are.",What are the pros and cons of using template-based generation for creating challenge sets?,"Template-based generation offers the advantage of providing greater control, such as achieving a specific vocabulary distribution. However, this advantage comes at the cost of the naturalness of the examples.",7,4,7,4,"Question:

What are the advantages and disadvantages of using template-based generation for constructing challenge sets?

Answer:

Template-based generation has the advantage of providing more control, for example for obtaining a specific vocabulary distribution. However, this comes at the expense of how natural the examples are.","Question:

What are the pros and cons of using template-based generation for creating challenge sets?

Answer:

Template-based generation offers the advantage of providing greater control, such as achieving a specific vocabulary distribution. However, this advantage comes at the cost of the naturalness of the examples.",NO,True,208,False,False
56657817-s15,Analysis Methods in Neural Language Processing: A Survey,2018-12-21,Adversary's Knowledge,"Adversarial examples can be generated using access to model parameters, also known as white-box attacks, or without such access, with black-box attacks (Papernot et al., 2016a(Papernot et al., , 2017Narodytska and Kasiviswanathan, 2017;Liu et al., 2017).

White-box attacks are difficult to adapt to the text world as they typically require computing gradients with respect to the input, which would be discrete in the text case. One option is to compute gradients with respect to the input word embeddings, and perturb the embeddings. Since this may result in a vector that does not correspond to any word, one could search for the closest word embedding in a given dictionary (Papernot et al., 2016b); Cheng et al. (2018) extended this idea to seq2seq models. Others computed gradients with respect to input word embeddings to identify and rank words to be modified (Samanta and Mehta, 2017;Liang et al., 2018). Ebrahimi et al. (2018b) developed an alternative method by representing text edit operations in vector space (e.g., a binary vector specifying which characters in a word would be changed) and approximating the change in loss with the derivative along this vector.

Given the difficulty in generating white-box adversarial examples for text, much research has been devoted to black-box examples. Often, the adversarial examples are inspired by text edits that are thought to be natural or commonly generated by humans, such as typos, misspellings, and so 14 These criteria are partly taken from Yuan et al. (2017), where a more elaborate taxonomy is laid out. At present, though, the work on adversarial examples in NLP is more limited than in computer vision, so our criteria will suffice. on (Sakaguchi et al., 2017;Heigold et al., 2018;Belinkov and Bisk, 2018). Gao et al. (2018) defined scoring functions to identify tokens to modify. Their functions do not require access to model internals, but they do require the model prediction score. After identifying the important tokens, they modify characters with common edit operations. Zhao et al. (2018c) used generative adversarial networks (GANs)  to minimize the distance between latent representations of input and adversarial examples, and performed perturbations in latent space. Since the latent representations do not need to come from the attacked model, this is a black-box attack.

Finally, Alzantot et al. (2018) developed an interesting population-based genetic algorithm for crafting adversarial examples for text classification by maintaining a population of modifications of the original sentence and evaluating fitness of modifications at each generation. They do not require access to model parameters, but do use prediction scores. A similar idea was proposed by Kuleshov et al. (2018).",What are the methods for generating adversarial examples in NLP without accessing model parameters?,"Given the difficulty in generating white-box adversarial examples for text, much research has been devoted to black-box examples. Often, the adversarial examples are inspired by text edits that are thought to be natural or commonly generated by humans, such as typos, misspellings, and so on (Sakaguchi et al., 2017; Heigold et al., 2018; Belinkov and Bisk, 2018). Gao et al. (2018) defined scoring functions to identify tokens to modify. Their functions do not require access to model internals, but they do require the model prediction score. After identifying the important tokens, they modify characters with common edit operations. Zhao et al. (2018c) used generative adversarial networks (GANs) to minimize the distance between latent representations of input and adversarial examples, and performed perturbations in latent space. Since the latent representations do not need to come from the attacked model, this is a black-box attack. Finally, Alzantot et al. (2018) developed an interesting population-based genetic algorithm for crafting adversarial examples for text classification by maintaining a population of modifications of the original sentence and evaluating fitness of modifications at each generation. They do not require access to model parameters, but do use prediction scores. A similar idea was proposed by Kuleshov et al. (2018).",What are some methods for generating adversarial examples in NLP without needing access to model parameters?,"Generating adversarial examples in natural language processing (NLP) without accessing model parameters is a challenging task, often addressed through black-box approaches. One common strategy involves creating adversarial examples inspired by natural text edits, such as typos and misspellings, as explored by Sakaguchi et al. (2017), Heigold et al. (2018), and Belinkov and Bisk (2018). Gao et al. (2018) introduced scoring functions to identify which tokens to modify, relying on model prediction scores rather than internal parameters. Once important tokens are identified, they apply common character edit operations.

Another approach by Zhao et al. (2018c) utilizes generative adversarial networks (GANs) to perform perturbations in latent space, minimizing the distance between latent representations of the original and adversarial examples. This method is considered a black-box attack since the latent representations do not need to originate from the attacked model.

Additionally, Alzantot et al. (2018) developed a population-based genetic algorithm for crafting adversarial examples in text classification. This method involves maintaining a population of sentence modifications and evaluating their fitness at each generation, using prediction scores but not model parameters. A similar concept was proposed by Kuleshov et al. (2018).",7,4,7,4,"Question:

What are the methods for generating adversarial examples in NLP without accessing model parameters?

Answer:

Given the difficulty in generating white-box adversarial examples for text, much research has been devoted to black-box examples. Often, the adversarial examples are inspired by text edits that are thought to be natural or commonly generated by humans, such as typos, misspellings, and so on (Sakaguchi et al., 2017; Heigold et al., 2018; Belinkov and Bisk, 2018). Gao et al. (2018) defined scoring functions to identify tokens to modify. Their functions do not require access to model internals, but they do require the model prediction score. After identifying the important tokens, they modify characters with common edit operations. Zhao et al. (2018c) used generative adversarial networks (GANs) to minimize the distance between latent representations of input and adversarial examples, and performed perturbations in latent space. Since the latent representations do not need to come from the attacked model, this is a black-box attack. Finally, Alzantot et al. (2018) developed an interesting population-based genetic algorithm for crafting adversarial examples for text classification by maintaining a population of modifications of the original sentence and evaluating fitness of modifications at each generation. They do not require access to model parameters, but do use prediction scores. A similar idea was proposed by Kuleshov et al. (2018).","Question:

What are some methods for generating adversarial examples in NLP without needing access to model parameters?

Answer:

Generating adversarial examples in natural language processing (NLP) without accessing model parameters is a challenging task, often addressed through black-box approaches. One common strategy involves creating adversarial examples inspired by natural text edits, such as typos and misspellings, as explored by Sakaguchi et al. (2017), Heigold et al. (2018), and Belinkov and Bisk (2018). Gao et al. (2018) introduced scoring functions to identify which tokens to modify, relying on model prediction scores rather than internal parameters. Once important tokens are identified, they apply common character edit operations.

Another approach by Zhao et al. (2018c) utilizes generative adversarial networks (GANs) to perform perturbations in latent space, minimizing the distance between latent representations of the original and adversarial examples. This method is considered a black-box attack since the latent representations do not need to originate from the attacked model.

Additionally, Alzantot et al. (2018) developed a population-based genetic algorithm for crafting adversarial examples in text classification. This method involves maintaining a population of sentence modifications and evaluating their fitness at each generation, using prediction scores but not model parameters. A similar concept was proposed by Kuleshov et al. (2018).",NO,True,1350,True,True
49564714-s3,Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing,2018-07-02,Language Transfer,"Linguistic information can be transferred from resource-rich languages to resource-poor languages: these are commonly referred to as source languages and target languages, respectively. Language transfer is challenging as it requires us to match word sequences with different lexica and word orders, or syntactic trees with different (anisomorphic) structures (Ponti et al. 2018a). As a consequence, the information obtained from the source languages typically needs to be adapted, by tailoring it to the properties of the target languages. The methods developed for language transfer include annotation projection, (de)lexicalized model transfer, and translation (Agić et al. 2014). We will illustrate them below using dependency parsing as an example.

Annotation projection was introduced in the seminal work of Yarowsky, Ngai, and Wicentowski (2001) and Hwa et al. (2005). In its original formulation, as illustrated in Figure 1a, a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g. PoS tags and dependency trees) is then projected directly and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014;Agić et al. 2016) or sets of most likely labels (Khapra et al. 2011;Wisniewski et al. 2014) can be projected instead of single categorical labels. These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or support 'ambiguous learning' on the target language, respectively.

Model transfer instead involves training a model (e.g. a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure  1b. Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3).

Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1c, a source sentence is machine translated into a target language, (Banea et al. 2008) or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translated documents can also be employed to generate multilingual sentence representations, which facilitate language transfer (Zhou, Wan, and Xiao 2016).

Some of these methods are hampered by their resource requirements. In fact, annotation projection and translation need parallel texts to align words and train translation systems, respectively (Agić, Hovy, and Søgaard 2015). Moreover, comparisons of state-of-the-art algorithms revealed that model transfer is competitive with machine translation in terms of performance (Conneau et al. 2018). Partly owing to these reasons, typological knowledge has been mostly harnessed in connection with model transfer, as we will discuss in § 5.2. Moreover, typological features can guide the selection of the best source language to match to a target language for language transfer (Agić et al. 2016, inter alia), which benefits all the above-mentioned methods (see § 5.3).","What are the methods developed for language transfer, and how are they illustrated using dependency parsing?","Linguistic information can be transferred from resource-rich languages to resource-poor languages, commonly referred to as source languages and target languages, respectively. The methods developed for language transfer include annotation projection, (de)lexicalized model transfer, and translation (Agić et al. 2014). Annotation projection was introduced in the seminal work of Yarowsky, Ngai, and Wicentowski (2001) and Hwa et al. (2005). In its original formulation, a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agić et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or support 'ambiguous learning' on the target language, respectively. Model transfer involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations. Machine translation offers an alternative to lexicalization in the absence of annotated parallel data. A source sentence is machine translated into a target language, or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translated documents can also be employed to generate multilingual sentence representations, which facilitate language transfer (Zhou, Wan, and Xiao 2016).","What methods have been developed for language transfer, particularly in the context of dependency parsing?","Language transfer techniques enable the transfer of linguistic information from resource-rich languages (source languages) to resource-poor languages (target languages). Several methods have been developed for this purpose, including annotation projection, (de)lexicalized model transfer, and translation (Agić et al. 2014).

Annotation projection, introduced by Yarowsky, Ngai, and Wicentowski (2001) and Hwa et al. (2005), involves parsing a source text and aligning it with a target parallel raw text. The annotations, such as part-of-speech tags and dependency trees, are then projected onto the target language to train a supervised model. This process has been refined into soft projection, which uses constraints based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009) to complement alignment. Additionally, source model expectations on labels (Wang and Manning 2014; Agić et al. 2016) or sets of likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected to constrain unsupervised models, reducing divergence between source and target labels or supporting 'ambiguous learning' in the target language.

Model transfer involves training a parser on a source language and applying it to a target language (Zeman and Resnik 2008). Due to vocabulary differences, models are typically delexicalized before transfer, using language-independent (Nivre et al. 2016) or harmonized features (Zhang et al. 2012) as input. To bridge the vocabulary gap, model transfer can be enhanced with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations.

Machine translation offers an alternative to lexicalization when annotated parallel data is unavailable. A source sentence can be machine translated into a target language or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). The resulting annotation is projected to train a target-side supervised model. Translated documents can also generate multilingual sentence representations, facilitating language transfer (Zhou, Wan, and Xiao 2016).",7,4,8,4,"Question:

What are the methods developed for language transfer, and how are they illustrated using dependency parsing?

Answer:

Linguistic information can be transferred from resource-rich languages to resource-poor languages, commonly referred to as source languages and target languages, respectively. The methods developed for language transfer include annotation projection, (de)lexicalized model transfer, and translation (Agić et al. 2014). Annotation projection was introduced in the seminal work of Yarowsky, Ngai, and Wicentowski (2001) and Hwa et al. (2005). In its original formulation, a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agić et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or support 'ambiguous learning' on the target language, respectively. Model transfer involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations. Machine translation offers an alternative to lexicalization in the absence of annotated parallel data. A source sentence is machine translated into a target language, or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translated documents can also be employed to generate multilingual sentence representations, which facilitate language transfer (Zhou, Wan, and Xiao 2016).","Question:

What methods have been developed for language transfer, particularly in the context of dependency parsing?

Answer:

Language transfer techniques enable the transfer of linguistic information from resource-rich languages (source languages) to resource-poor languages (target languages). Several methods have been developed for this purpose, including annotation projection, (de)lexicalized model transfer, and translation (Agić et al. 2014).

Annotation projection, introduced by Yarowsky, Ngai, and Wicentowski (2001) and Hwa et al. (2005), involves parsing a source text and aligning it with a target parallel raw text. The annotations, such as part-of-speech tags and dependency trees, are then projected onto the target language to train a supervised model. This process has been refined into soft projection, which uses constraints based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009) to complement alignment. Additionally, source model expectations on labels (Wang and Manning 2014; Agić et al. 2016) or sets of likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected to constrain unsupervised models, reducing divergence between source and target labels or supporting 'ambiguous learning' in the target language.

Model transfer involves training a parser on a source language and applying it to a target language (Zeman and Resnik 2008). Due to vocabulary differences, models are typically delexicalized before transfer, using language-independent (Nivre et al. 2016) or harmonized features (Zhang et al. 2012) as input. To bridge the vocabulary gap, model transfer can be enhanced with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations.

Machine translation offers an alternative to lexicalization when annotated parallel data is unavailable. A source sentence can be machine translated into a target language or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). The resulting annotation is projected to train a target-side supervised model. Translated documents can also generate multilingual sentence representations, facilitating language transfer (Zhou, Wan, and Xiao 2016).",NO,True,2129,True,True
49564714-s4,Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing,2018-07-02,Multilingual Joint Supervised Learning,"NLP models can be learned jointly from the data in multiple languages. In addition to facilitating intrinsically multilingual applications, such as Neural Machine Translation and Information Extraction, this approach often surpasses language-specific monolingual models as it can leverage more (although noisier) data (Ammar et al. 2016, inter alia). This is particularly true in scenarios where either a target or all languages are resource-lean (Khapra et al. 2011) or in code-switching scenarios (Adel, Vu, and Schultz 2013). In fact, Figure 2: In multilingual joint learning, representations can be private or shared across languages. Tied parameters are shown as neurons with identical color. Image adapted from Fang and Cohn (2017). multilingual joint learning improves over pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017). 4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b) or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Malagasy on the left and Polish on the right). Parameter sharing, however, does not necessarily imply parameter identity: it can be enforced by minimizing the distance between parameters (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011;Zhou et al. 2015) in separate language-specific models.

Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016;Östling and Tiedemann 2017) or neural machine translation tasks (Johnson et al. 2017;Ha, Niehues, and Waibel 2016). Ammar et al. (2016) instead used language vectors as a prior for language identity or typological features. In § 5.2, we will discuss ways in which typological knowledge is used to balance private and shared neural network components and provide informative input language vectors. In § 6.3, we will argue that language vectors do not need to be limited to features extracted from typological databases, but also include automatically induced typological information (Malaviya, Neubig, and Littell 2017, see § 4.3).","How does parameter sharing function in multilingual joint learning, and what are its implications for model design?","NLP models can be learned jointly from the data in multiple languages. A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). In state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Parameter sharing does not necessarily imply parameter identity; it can be enforced by minimizing the distance between parameters (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. This approach often surpasses language-specific monolingual models as it can leverage more (although noisier) data (Ammar et al. 2016, inter alia). This is particularly true in scenarios where either a target or all languages are resource-lean (Khapra et al. 2011) or in code-switching scenarios (Adel, Vu, and Schultz 2013). Multilingual joint learning improves over pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).","How does parameter sharing work in multilingual joint learning, and what are its implications for designing models?","In multilingual joint learning for NLP models, parameter sharing is a crucial strategy that allows models to be trained using data from multiple languages (Johnson et al. 2017). In advanced neural architectures, input and hidden representations can be either language-specific or shared across languages. Shared representations are achieved by tying the parameters of a network component across different languages. This can involve sharing word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Importantly, parameter sharing does not necessarily mean the parameters are identical; it can be enforced by minimizing the distance between parameters (Duong et al. 2015a) or between latent representations of parallel sentences in separate language-specific models (Niehues et al. 2011; Zhou et al. 2015). This approach often outperforms language-specific monolingual models because it can leverage a larger, albeit noisier, dataset (Ammar et al. 2016, inter alia). This is especially beneficial in scenarios where the target or all languages involved are resource-lean (Khapra et al. 2011) or in code-switching contexts (Adel, Vu, and Schultz 2013). Additionally, multilingual joint learning offers advantages over pure model transfer, particularly in situations with limited labeled data in the target language(s) (Fang and Cohn 2017).",7,4,7,3,"Question:

How does parameter sharing function in multilingual joint learning, and what are its implications for model design?

Answer:

NLP models can be learned jointly from the data in multiple languages. A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). In state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Parameter sharing does not necessarily imply parameter identity; it can be enforced by minimizing the distance between parameters (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. This approach often surpasses language-specific monolingual models as it can leverage more (although noisier) data (Ammar et al. 2016, inter alia). This is particularly true in scenarios where either a target or all languages are resource-lean (Khapra et al. 2011) or in code-switching scenarios (Adel, Vu, and Schultz 2013). Multilingual joint learning improves over pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).","Question:

How does parameter sharing work in multilingual joint learning, and what are its implications for designing models?

Answer:

In multilingual joint learning for NLP models, parameter sharing is a crucial strategy that allows models to be trained using data from multiple languages (Johnson et al. 2017). In advanced neural architectures, input and hidden representations can be either language-specific or shared across languages. Shared representations are achieved by tying the parameters of a network component across different languages. This can involve sharing word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Importantly, parameter sharing does not necessarily mean the parameters are identical; it can be enforced by minimizing the distance between parameters (Duong et al. 2015a) or between latent representations of parallel sentences in separate language-specific models (Niehues et al. 2011; Zhou et al. 2015). This approach often outperforms language-specific monolingual models because it can leverage a larger, albeit noisier, dataset (Ammar et al. 2016, inter alia). This is especially beneficial in scenarios where the target or all languages involved are resource-lean (Khapra et al. 2011) or in code-switching contexts (Adel, Vu, and Schultz 2013). Additionally, multilingual joint learning offers advantages over pure model transfer, particularly in situations with limited labeled data in the target language(s) (Fang and Cohn 2017).",NO,True,1470,True,True
49564714-s5,Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing,2018-07-02,Multilingual Representation Learning,"The multilingual algorithms reviewed in § 3.1 and § 3.2 are facilitated by dense realvalued vector representations of words, known as multilingual word embeddings (MWEs). These can be learned from corpora and provide pivotal lexical features to several downstream NLP applications. In MWEs, similar words (regardless of the actual language) obtain similar representations. Various methods to generate MWEs have been developed. We follow the classification proposed by Ruder (2018), whereas we refer the reader to Upadhyay et al. (2016) for an empirical comparison.

Monolingual mapping generates independent monolingual representations and subsequently learns a map between a source language and a target language based on a bilingual lexicon (Mikolov, Le, and Sutskever 2013) or in an unsupervised fashion through adversarial networks (Conneau et al. 2017). Alternatively, both spaces can be cast into a new, lower-dimensional space through canonical correlation analysis (CCA) based on dictionaries (Ammar et al. 2016) or word alignments (Guo et al. 2015). Figure 3 shows how equivalent words in the separate semantic spaces of different languages X and Y can be re-orientated through a learned transformation W.

Pseudo-cross-lingual approaches merge words with contexts of other languages and generate representations based on this mixed corpus. Substitutions are based on Wiktionary (Xiao and Guo 2014) or machine translation (Gouws and Søgaard 2015;Duong et al. 2016). Moreover, the mixed corpus can be produced by randomly shuffling words between aligned documents in two languages (Vulić and Moens 2015).

Cross-lingual training approaches jointly learn embeddings from parallel corpora and enforce cross-lingual constraints. This involves minimizing the distance of the hidden sentence representations of the two languages (Hermann and Blunsom 2014) or decoding one from the other (Lauly, Boulanger, and Larochelle 2013), possibly adding a correlation term to the loss (Chandar et al. 2014).

Joint optimization typically involves learning distinct monolingual embeddings, while enforcing cross-lingual constraints. These can be based on alignment-based translations (Klementiev, Titov, and Bhattarai 2012), cross-lingual word contexts (Luong, Pham, and Manning 2015), the average representations of parallel sentences (Gouws, Bengio, and Corrado 2015), or images (Rotman, Vulić, and Reichart 2018).

In this section, we have briefly outlined the most widely used methods in multilingual NLP. Although they offer a solution to data scarcity, cross-lingual variation remains a challenge for transferring knowledge across languages or learning from several languages simultaneously. Typological information offers promising ways to address this problem. In particular, we have noted that it can support model transfer, parameter sharing, and input biasing through language vectors. In the next two sections, we will elaborate on these solutions. In particular, we will review the development of typological information and the specific features which are selected for various NLP tasks ( § 4). Afterwards, we discuss ways in which these features are integrated in NLP algorithms, for which applications they have been harnessed, and whether they truly benefit system performance ( § 5).",How do cross-lingual training approaches address the challenge of cross-lingual variation in multilingual NLP?,"Cross-lingual training approaches jointly learn embeddings from parallel corpora and enforce cross-lingual constraints. This involves minimizing the distance of the hidden sentence representations of the two languages (Hermann and Blunsom 2014) or decoding one from the other (Lauly, Boulanger, and Larochelle 2013), possibly adding a correlation term to the loss (Chandar et al. 2014). Joint optimization typically involves learning distinct monolingual embeddings, while enforcing cross-lingual constraints. These can be based on alignment-based translations (Klementiev, Titov, and Bhattarai 2012), cross-lingual word contexts (Luong, Pham, and Manning 2015), the average representations of parallel sentences (Gouws, Bengio, and Corrado 2015), or images (Rotman, Vulić, and Reichart 2018).",How do cross-lingual training approaches tackle the issue of cross-lingual variation in multilingual NLP?,"Cross-lingual training approaches address the challenge of cross-lingual variation by jointly learning embeddings from parallel corpora while enforcing cross-lingual constraints. This process often involves minimizing the distance between hidden sentence representations of two languages, as suggested by Hermann and Blunsom (2014), or decoding one language from the other, as proposed by Lauly, Boulanger, and Larochelle (2013). Additionally, a correlation term may be added to the loss function, as noted by Chandar et al. (2014). Joint optimization typically involves learning distinct monolingual embeddings while enforcing cross-lingual constraints. These constraints can be based on alignment-based translations (Klementiev, Titov, and Bhattarai 2012), cross-lingual word contexts (Luong, Pham, and Manning 2015), the average representations of parallel sentences (Gouws, Bengio, and Corrado 2015), or even images (Rotman, Vulić, and Reichart 2018).",7,4,8,4,"Question:

How do cross-lingual training approaches address the challenge of cross-lingual variation in multilingual NLP?

Answer:

Cross-lingual training approaches jointly learn embeddings from parallel corpora and enforce cross-lingual constraints. This involves minimizing the distance of the hidden sentence representations of the two languages (Hermann and Blunsom 2014) or decoding one from the other (Lauly, Boulanger, and Larochelle 2013), possibly adding a correlation term to the loss (Chandar et al. 2014). Joint optimization typically involves learning distinct monolingual embeddings, while enforcing cross-lingual constraints. These can be based on alignment-based translations (Klementiev, Titov, and Bhattarai 2012), cross-lingual word contexts (Luong, Pham, and Manning 2015), the average representations of parallel sentences (Gouws, Bengio, and Corrado 2015), or images (Rotman, Vulić, and Reichart 2018).","Question:

How do cross-lingual training approaches tackle the issue of cross-lingual variation in multilingual NLP?

Answer:

Cross-lingual training approaches address the challenge of cross-lingual variation by jointly learning embeddings from parallel corpora while enforcing cross-lingual constraints. This process often involves minimizing the distance between hidden sentence representations of two languages, as suggested by Hermann and Blunsom (2014), or decoding one language from the other, as proposed by Lauly, Boulanger, and Larochelle (2013). Additionally, a correlation term may be added to the loss function, as noted by Chandar et al. (2014). Joint optimization typically involves learning distinct monolingual embeddings while enforcing cross-lingual constraints. These constraints can be based on alignment-based translations (Klementiev, Titov, and Bhattarai 2012), cross-lingual word contexts (Luong, Pham, and Manning 2015), the average representations of parallel sentences (Gouws, Bengio, and Corrado 2015), or even images (Rotman, Vulić, and Reichart 2018).",NO,True,955,True,True
49564714-s10,Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing,2018-07-02,Heuristics based on morphosyntactic annotation.,"Morphosyntactic feature values can be extracted via heuristics from morphologically and syntactically annotated texts.

For example, word order features can be calculated by counting the average direction of dependency relations or constituency hierarchies (Liu 2010). Consider the tree of a sentence in Welsh from Bender et al. (2013)  Morphosyntactic annotation is often unavailable for resource-lean languages. In such cases, it can be projected from a source directly to several target languages through language transfer. For instance, Östling (2015) project source morpho-syntactic annotation directly to several languages through a multilingual alignment. After the alignment and projection, word order features are calculated by the average direction of dependency relations. Similarly, Zhang et al. (2016) transfer POS annotation with a model transfer technique relying on multilingual embeddings, created through monolingual mapping. After the projection, they predict feature values with a multiclass Support Vector Machine (SVM) using POS tag n-gram features.

Finally, typological information can be extracted from Interlinear Glossed Texts (IGT). Such collections of example sentences are collated by linguists and contain grammatical glosses with morphological information. These can guide alignment between the example sentence and its English translation. Lewis and Xia (2008) and Bender et al. (2013) project chunking information from English and train Context Free Grammars on target languages. After collapsing identical rules, they arrange them by frequency and infer word order features.","How can morphosyntactic annotation be projected to resource-lean languages, and what techniques are used for feature prediction?","Morphosyntactic annotation can be projected to resource-lean languages through language transfer. For instance, Östling (2015) projects source morpho-syntactic annotation directly to several languages through a multilingual alignment. After the alignment and projection, word order features are calculated by the average direction of dependency relations. Similarly, Zhang et al. (2016) transfer POS annotation with a model transfer technique relying on multilingual embeddings, created through monolingual mapping. After the projection, they predict feature values with a multiclass Support Vector Machine (SVM) using POS tag n-gram features. Finally, typological information can be extracted from Interlinear Glossed Texts (IGT), which are collections of example sentences collated by linguists containing grammatical glosses with morphological information. These can guide alignment between the example sentence and its English translation. Lewis and Xia (2008) and Bender et al. (2013) project chunking information from English and train Context Free Grammars on target languages. After collapsing identical rules, they arrange them by frequency and infer word order features.","How can morphosyntactic annotation be effectively projected to resource-lean languages, and what techniques are employed for feature prediction?","Morphosyntactic annotation can be effectively projected to resource-lean languages through various language transfer techniques. One approach, as demonstrated by Östling (2015), involves projecting source morphosyntactic annotations directly to multiple languages using multilingual alignment. After this alignment and projection, word order features are determined by calculating the average direction of dependency relations. Another method, employed by Zhang et al. (2016), involves transferring part-of-speech (POS) annotations using a model transfer technique that relies on multilingual embeddings created through monolingual mapping. Following the projection, feature values are predicted using a multiclass Support Vector Machine (SVM) that utilizes POS tag n-gram features. Additionally, typological information can be extracted from Interlinear Glossed Texts (IGT), which are collections of example sentences compiled by linguists that include grammatical glosses with morphological information. These texts can guide the alignment between the example sentence and its English translation. Furthermore, Lewis and Xia (2008) and Bender et al. (2013) have demonstrated the projection of chunking information from English to train Context-Free Grammars on target languages. By collapsing identical rules and arranging them by frequency, they infer word order features.",7,4,7,4,"Question:

How can morphosyntactic annotation be projected to resource-lean languages, and what techniques are used for feature prediction?

Answer:

Morphosyntactic annotation can be projected to resource-lean languages through language transfer. For instance, Östling (2015) projects source morpho-syntactic annotation directly to several languages through a multilingual alignment. After the alignment and projection, word order features are calculated by the average direction of dependency relations. Similarly, Zhang et al. (2016) transfer POS annotation with a model transfer technique relying on multilingual embeddings, created through monolingual mapping. After the projection, they predict feature values with a multiclass Support Vector Machine (SVM) using POS tag n-gram features. Finally, typological information can be extracted from Interlinear Glossed Texts (IGT), which are collections of example sentences collated by linguists containing grammatical glosses with morphological information. These can guide alignment between the example sentence and its English translation. Lewis and Xia (2008) and Bender et al. (2013) project chunking information from English and train Context Free Grammars on target languages. After collapsing identical rules, they arrange them by frequency and infer word order features.","Question:

How can morphosyntactic annotation be effectively projected to resource-lean languages, and what techniques are employed for feature prediction?

Answer:

Morphosyntactic annotation can be effectively projected to resource-lean languages through various language transfer techniques. One approach, as demonstrated by Östling (2015), involves projecting source morphosyntactic annotations directly to multiple languages using multilingual alignment. After this alignment and projection, word order features are determined by calculating the average direction of dependency relations. Another method, employed by Zhang et al. (2016), involves transferring part-of-speech (POS) annotations using a model transfer technique that relies on multilingual embeddings created through monolingual mapping. Following the projection, feature values are predicted using a multiclass Support Vector Machine (SVM) that utilizes POS tag n-gram features. Additionally, typological information can be extracted from Interlinear Glossed Texts (IGT), which are collections of example sentences compiled by linguists that include grammatical glosses with morphological information. These texts can guide the alignment between the example sentence and its English translation. Furthermore, Lewis and Xia (2008) and Bender et al. (2013) have demonstrated the projection of chunking information from English to train Context-Free Grammars on target languages. By collapsing identical rules and arranging them by frequency, they infer word order features.",NO,True,1375,True,True
44130961-s18,"TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation",2018-05-11,Dataset and Annotation Statistics,"We created reading lists for 182 of the 200 topics we identify in Section 4.2. Resources were not found for 18 topics due to the granularity of the topic (e.g., Radial Basis Function Networks) as well as our intended restriction of the chosen resources to PowerPoint presentations and HTML pages. The average number of resources per reading list for the 182 topics is 3.94. As an extension to the reading lists we collected Wikipedia pages for 184 of the topics and present these urls as part of the dataset. We annotated prerequisite relations for the 200 topics described above. We present a subset of our annotations in Figure 1, which shows the network of topic relations (nodes without incoming edges were not annotated for their prerequisites as part of this shown inter-annotation round). Our network consists of 794 unidirectional edges and 33 bidirectional edges. The presence of bidirectional edges stems from our definition of a prerequisite, which does not preclude bidirectionality (one topic can help explain another and viceversa) as well as the similarity of the topics. The set of bidirectional edges consists of topic pairs (BLEU -ROUGE; Word Embedding -Distributional Semantics; Backpropagation -Gradient descent) which could be collapsed into one topic to create a directed acyclic graph in the future.

For survey extraction, we automatically split 313 resources into content cards which we annotated for usefulness in survey extraction. These resources are a subset of the reading lists limited in number due to constraints in downloading urls and parsing to our annotation interface. The total number of cards which were not marked as repeats/mis-parsed totals 17,088, with 54.59 per resource. 6,099 cards were labeled as somewhat relevant or relevant for the target topic. The resources marked as non-relevant may be poorly  presented or may not pertain fully to the topic of that survey. These numbers confirm the appropriateness of this survey corpus as a non-trivial information retrieval task.

To better understand the difficulty of our annotation tasks, we performed inter-annotator agreement experiments for each of our annotations. We randomly sampled twenty-five resources and had annotators label for pedagogical function. Additionally, we sampled twenty-five topics for prerequisite annotations and five topics with reading list lengths of five for survey annotation. We used Fleiss's Kappa (Fleiss et al., 2004), a variant of Cohen's Kappa (Cohen, 1960) designed to measure annotator agreement for more than two annotators. The results are shown in Table 5. Using the scale as defined in Landis and Koch (1977), pedagogical function annotation exhibits substantial agreement while prerequisite annotation and survey extraction annotation show fair agreement. The Kappa score for pedagogical function is comparable to that of Sheng et al. (2017) (0.68) while the prerequisite annotation is slightly lower than the agreement metric used in Gordon et al. (2016) (0.36) although they measure agreement through Pearson correlation. We believe that the sparsity of the labels plays a role in these scores.","How is the inter-annotator agreement measured for different annotation tasks, and what factors influence these scores?","To better understand the difficulty of the annotation tasks, inter-annotator agreement experiments were performed for each of the annotations. Fleiss's Kappa (Fleiss et al., 2004), a variant of Cohen's Kappa (Cohen, 1960) designed to measure annotator agreement for more than two annotators, was used. The results are shown in Table 5. Using the scale as defined in Landis and Koch (1977), pedagogical function annotation exhibits substantial agreement while prerequisite annotation and survey extraction annotation show fair agreement. The Kappa score for pedagogical function is comparable to that of Sheng et al. (2017) (0.68) while the prerequisite annotation is slightly lower than the agreement metric used in Gordon et al. (2016) (0.36) although they measure agreement through Pearson correlation. It is believed that the sparsity of the labels plays a role in these scores.","How is inter-annotator agreement assessed in various annotation tasks, and what factors can affect these scores?","Inter-annotator agreement is crucial for understanding the difficulty of annotation tasks. To assess this, experiments were conducted using Fleiss's Kappa (Fleiss et al., 2004), an extension of Cohen's Kappa (Cohen, 1960) that is suitable for measuring agreement among more than two annotators. The results, detailed in Table 5, indicate varying levels of agreement across different tasks. According to the scale defined by Landis and Koch (1977), the pedagogical function annotation task shows substantial agreement, whereas the prerequisite annotation and survey extraction tasks exhibit only fair agreement. The Kappa score for pedagogical function is similar to that reported by Sheng et al. (2017) (0.68), while the score for prerequisite annotation is slightly lower than the agreement metric used by Gordon et al. (2016) (0.36), who employed Pearson correlation for their measurements. It is believed that the sparsity of the labels significantly influences these scores.",9,4,8,4,"Question:

How is the inter-annotator agreement measured for different annotation tasks, and what factors influence these scores?

Answer:

To better understand the difficulty of the annotation tasks, inter-annotator agreement experiments were performed for each of the annotations. Fleiss's Kappa (Fleiss et al., 2004), a variant of Cohen's Kappa (Cohen, 1960) designed to measure annotator agreement for more than two annotators, was used. The results are shown in Table 5. Using the scale as defined in Landis and Koch (1977), pedagogical function annotation exhibits substantial agreement while prerequisite annotation and survey extraction annotation show fair agreement. The Kappa score for pedagogical function is comparable to that of Sheng et al. (2017) (0.68) while the prerequisite annotation is slightly lower than the agreement metric used in Gordon et al. (2016) (0.36) although they measure agreement through Pearson correlation. It is believed that the sparsity of the labels plays a role in these scores.","Question:

How is inter-annotator agreement assessed in various annotation tasks, and what factors can affect these scores?

Answer:

Inter-annotator agreement is crucial for understanding the difficulty of annotation tasks. To assess this, experiments were conducted using Fleiss's Kappa (Fleiss et al., 2004), an extension of Cohen's Kappa (Cohen, 1960) that is suitable for measuring agreement among more than two annotators. The results, detailed in Table 5, indicate varying levels of agreement across different tasks. According to the scale defined by Landis and Koch (1977), the pedagogical function annotation task shows substantial agreement, whereas the prerequisite annotation and survey extraction tasks exhibit only fair agreement. The Kappa score for pedagogical function is similar to that reported by Sheng et al. (2017) (0.68), while the score for prerequisite annotation is slightly lower than the agreement metric used by Gordon et al. (2016) (0.36), who employed Pearson correlation for their measurements. It is believed that the sparsity of the labels significantly influences these scores.",NO,True,978,True,True
254854669-s23,Let's Negotiate! A Survey of Negotiation Dialogue Systems,2022-12-18,Supervised Learning,"Supervised learning (SL) is another popular paradigm for policy learning. (Lewis et al., 2017) adopt a Seq2Seq model to learn what action should be taken by maximizing the likelihood of the training data. However, supervised learning only aims to mimic the average human behavior, so He et al. (2018) propose to finetune the supervised model to directly optimize for a particular dialogue reward function, which is defined as i) the utility function of the final price for the buyer and seller ii) the difference between two agents' utilities iii) the number of utterances in the dialogue. Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. The system response would be generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy. In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm. Recently, Dutt et al. (2021b) propose a generalised framework for identifying resisting strategies in persuasive negotiations using a pre-trained BERT model (Devlin et al., 2019).",How is supervised learning applied to negotiation dialogue systems to predict negotiation strategies and generate responses?,"Supervised learning (SL) is a popular paradigm for policy learning in negotiation dialogue systems. Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. The system response is generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy. In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm.",How is supervised learning utilized in negotiation dialogue systems to predict strategies and generate responses?,"Supervised learning is a widely used approach for policy learning in negotiation dialogue systems. For instance, Zhou et al. (2020) developed a strategy predictor through supervised training to forecast whether a specific negotiation strategy will be employed in the next utterance. The system then generates responses based on the user's input, the dialogue context, and the predicted negotiation strategy. Additionally, Joshi et al. (2021) enhanced this approach by integrating a pragmatic strategies graph network with a sequence-to-sequence (seq2seq) model, creating a more interpretable policy learning framework.",7,4,7,4,"Question:

How is supervised learning applied to negotiation dialogue systems to predict negotiation strategies and generate responses?

Answer:

Supervised learning (SL) is a popular paradigm for policy learning in negotiation dialogue systems. Zhou et al. (2020) train a strategy predictor to predict whether a certain negotiation strategy occurred in the next utterance using supervised training. The system response is generated conditioned on the user utterance, dialogue context, and the predicted negotiation strategy. In addition, Joshi et al. (2021) incorporate a pragmatic strategies graph network with the seq2seq model to create an interpretable policy learning paradigm.","Question:

How is supervised learning utilized in negotiation dialogue systems to predict strategies and generate responses?

Answer:

Supervised learning is a widely used approach for policy learning in negotiation dialogue systems. For instance, Zhou et al. (2020) developed a strategy predictor through supervised training to forecast whether a specific negotiation strategy will be employed in the next utterance. The system then generates responses based on the user's input, the dialogue context, and the predicted negotiation strategy. Additionally, Joshi et al. (2021) enhanced this approach by integrating a pragmatic strategies graph network with a sequence-to-sequence (seq2seq) model, creating a more interpretable policy learning framework.",NO,True,618,True,True
232320269-s8,A Survey on Predicting the Factuality and the Bias of News Media,2021-03-16,Textual Content Representation,"The most natural representation for a source is as a sample of articles it has published, which in turn can be represented using linguistic features or as continuous representations. Linguistic Features: These features focus on language use, and they have been shown to be useful for detecting fake articles, as well as for predicting the political bias and the factuality of reporting of news media [Horne et al., 2018;Baly et al., 2018]. For example, [Horne and Adali, 2017] showed that ""fake news"" pack a lot of information in the title (as many people do not read beyond the title, e.g., in social media), and use shorter, simpler, and repetitive content in the body (as writing fake information takes a lot of effort). Such features can be generated based on the Linguistic Inquiry and Word Count (LIWC) lexicon and used to distinguish articles from trusted sources vs. hoaxes vs. satire vs. propaganda [Pennebaker et al., 2001]. They can be also modeled using linguistic markers [Mihaylova et al., 2018] such as factives from [Hooper, 1975], assertives from [Hooper, 1975] , 2003], and sentiment cues from [Liu et al., 2005]; see Table 1 for examples. There are 141 such features implemented in the NELA toolkit [Horne et al., 2018], grouped in the following categories:

• Style: part-of-speech tags, use of specific words (function words, pronouns, etc.), and features for clickbait title classification;

• Complexity: type-token ratio, readability, number of cognitive process words (identifying discrepancy, insight, certainty, etc.); • Bias: features modeling bias using lexicons and subjectivity as calculated using pre-trained classifiers; • Affect: sentiment scores from lexicons and full systems; • Morality: features based on the Moral Foundation Theory [Graham et al., 2009] and lexicons; • Event: features modeling time and location.

Embedding representations: An alternative way to represent an article is to use embedding representations, typically based on BERT [Devlin et al., 2019]. This can be done without fine-tuning, e.g., by encoding an article (possibly truncated, e.g., BERT can take up to 512 tokens as an input) and then averaging the word representations extracted from the second-to-last layer. 2 Alternatively, one can use pre-trained sentence encoders such as Sentence BERT [Reimers and Gurevych, 2019] or the Universal Sentence Encoder (USE) [Cer et al., 2018]. Finally, one can obtain representations that are relevant to the target task, e.g., by fine-tuning BERT to predict the label (bias or factuality) of the medium that an article comes from, in the form of distant supervision [Baly et al., 2020b]. One issue with distant supervision is that the model can end up learning to detect the source of the target news article instead of predicting its factuality/bias, which can be fixed using adversarial media adaptation and a specially adapted triplet loss [Baly et al., 2020a].",How can linguistic features be categorized and utilized in the NELA toolkit for news media analysis?,"Linguistic features can be categorized into several groups in the NELA toolkit for news media analysis. These categories include Style, which involves part-of-speech tags, the use of specific words (such as function words and pronouns), and features for clickbait title classification. Complexity features include type-token ratio, readability, and the number of cognitive process words that identify discrepancy, insight, certainty, etc. Bias features model bias using lexicons and subjectivity as calculated using pre-trained classifiers. Affect features involve sentiment scores from lexicons and full systems. Morality features are based on the Moral Foundation Theory [Graham et al., 2009] and lexicons. Event features model time and location. These linguistic features have been shown to be useful for detecting fake articles, as well as for predicting the political bias and the factuality of reporting of news media [Horne et al., 2018; Baly et al., 2018].",How are linguistic features categorized and applied in the NELA toolkit for analyzing news media?,"In the NELA toolkit, linguistic features are systematically categorized to enhance news media analysis. These categories include Style, which examines part-of-speech tags, the use of specific words such as function words and pronouns, and features relevant to clickbait title classification. Complexity features assess elements like type-token ratio, readability, and the presence of cognitive process words that indicate discrepancy, insight, and certainty. Bias features are designed to model bias using lexicons and measure subjectivity through pre-trained classifiers. Affect features involve sentiment scores derived from lexicons and comprehensive systems. Morality features are grounded in the Moral Foundation Theory [Graham et al., 2009] and associated lexicons. Event features focus on modeling time and location. These linguistic features have proven effective in detecting fake articles and predicting the political bias and factuality of news media reporting [Horne et al., 2018; Baly et al., 2018].",7,2,7,2,"Question:

How can linguistic features be categorized and utilized in the NELA toolkit for news media analysis?

Answer:

Linguistic features can be categorized into several groups in the NELA toolkit for news media analysis. These categories include Style, which involves part-of-speech tags, the use of specific words (such as function words and pronouns), and features for clickbait title classification. Complexity features include type-token ratio, readability, and the number of cognitive process words that identify discrepancy, insight, certainty, etc. Bias features model bias using lexicons and subjectivity as calculated using pre-trained classifiers. Affect features involve sentiment scores from lexicons and full systems. Morality features are based on the Moral Foundation Theory [Graham et al., 2009] and lexicons. Event features model time and location. These linguistic features have been shown to be useful for detecting fake articles, as well as for predicting the political bias and the factuality of reporting of news media [Horne et al., 2018; Baly et al., 2018].","Question:

How are linguistic features categorized and applied in the NELA toolkit for analyzing news media?

Answer:

In the NELA toolkit, linguistic features are systematically categorized to enhance news media analysis. These categories include Style, which examines part-of-speech tags, the use of specific words such as function words and pronouns, and features relevant to clickbait title classification. Complexity features assess elements like type-token ratio, readability, and the presence of cognitive process words that indicate discrepancy, insight, and certainty. Bias features are designed to model bias using lexicons and measure subjectivity through pre-trained classifiers. Affect features involve sentiment scores derived from lexicons and comprehensive systems. Morality features are grounded in the Moral Foundation Theory [Graham et al., 2009] and associated lexicons. Event features focus on modeling time and location. These linguistic features have proven effective in detecting fake articles and predicting the political bias and factuality of news media reporting [Horne et al., 2018; Baly et al., 2018].",NO,True,1012,True,True
264439179-s3,A Survey on Detection of LLMs-Generated Content,2023-10-24,Metrics,"Previous studies (Mitchell et al., 2023;Sadasivan et al., 2023) predominantly used the Area Under the Receiver Operating Characteristic (AUROC) score to gauge the effectiveness of detection algorithms.As a binary classification problem, AUROC shows the results under different thresholds, and the F1 score is also helpful.Krishna et al. (2023); Yang et al. (2023b) suggest that AUROC may not consistently provide a precise evaluation, particularly as the AUROC score nears the optimal limit of 1.0 since two detectors with identical AUROC score of 0.99 could exhibit substantial variations in detection quality from a user's perspective.From a practical point of view, ensuring a high True Positive Rate (TPR) is imperative while keeping the False Positive Rate (FPR) to a minimum.As such, current research (Krishna et al., 2023;Yang et al., 2023b) both report TPR scores at a fixed 1% FPR, along with the AUROC.Other work (Sadasivan et al., 2023) also refer to Type I and Type II errors following the binary hypothesis test and even report TPR at 10 −6 FPR (Fernandez et al., 2023).","What are the limitations of using AUROC as a metric for evaluating detection algorithms, and what alternative metrics are suggested?","Previous studies predominantly used the Area Under the Receiver Operating Characteristic (AUROC) score to gauge the effectiveness of detection algorithms. Krishna et al. (2023) and Yang et al. (2023b) suggest that AUROC may not consistently provide a precise evaluation, particularly as the AUROC score nears the optimal limit of 1.0 since two detectors with identical AUROC scores of 0.99 could exhibit substantial variations in detection quality from a user's perspective. From a practical point of view, ensuring a high True Positive Rate (TPR) is imperative while keeping the False Positive Rate (FPR) to a minimum. As such, current research reports TPR scores at a fixed 1% FPR, along with the AUROC. Other work also refers to Type I and Type II errors following the binary hypothesis test and even reports TPR at 10^-6 FPR (Fernandez et al., 2023).","What are the limitations of using AUROC as a metric for evaluating detection algorithms, and what alternative metrics are suggested?","The Area Under the Receiver Operating Characteristic (AUROC) score is commonly used to evaluate the effectiveness of detection algorithms. However, studies by Krishna et al. (2023) and Yang et al. (2023b) highlight that AUROC may not always provide an accurate assessment, especially as the score approaches its optimal value of 1.0. For instance, two detectors with identical AUROC scores of 0.99 could still show significant differences in detection quality from a user's perspective. Practically, it is crucial to maintain a high True Positive Rate (TPR) while minimizing the False Positive Rate (FPR). Therefore, recent research often reports TPR scores at a fixed 1% FPR, in addition to the AUROC. Other studies, such as those by Fernandez et al. (2023), also consider Type I and Type II errors in the context of binary hypothesis testing and even report TPR at an extremely low FPR of 10^-6.",7,2,8,2,"Question:

What are the limitations of using AUROC as a metric for evaluating detection algorithms, and what alternative metrics are suggested?

Answer:

Previous studies predominantly used the Area Under the Receiver Operating Characteristic (AUROC) score to gauge the effectiveness of detection algorithms. Krishna et al. (2023) and Yang et al. (2023b) suggest that AUROC may not consistently provide a precise evaluation, particularly as the AUROC score nears the optimal limit of 1.0 since two detectors with identical AUROC scores of 0.99 could exhibit substantial variations in detection quality from a user's perspective. From a practical point of view, ensuring a high True Positive Rate (TPR) is imperative while keeping the False Positive Rate (FPR) to a minimum. As such, current research reports TPR scores at a fixed 1% FPR, along with the AUROC. Other work also refers to Type I and Type II errors following the binary hypothesis test and even reports TPR at 10^-6 FPR (Fernandez et al., 2023).","Question:

What are the limitations of using AUROC as a metric for evaluating detection algorithms, and what alternative metrics are suggested?

Answer:

The Area Under the Receiver Operating Characteristic (AUROC) score is commonly used to evaluate the effectiveness of detection algorithms. However, studies by Krishna et al. (2023) and Yang et al. (2023b) highlight that AUROC may not always provide an accurate assessment, especially as the score approaches its optimal value of 1.0. For instance, two detectors with identical AUROC scores of 0.99 could still show significant differences in detection quality from a user's perspective. Practically, it is crucial to maintain a high True Positive Rate (TPR) while minimizing the False Positive Rate (FPR). Therefore, recent research often reports TPR scores at a fixed 1% FPR, in addition to the AUROC. Other studies, such as those by Fernandez et al. (2023), also consider Type I and Type II errors in the context of binary hypothesis testing and even report TPR at an extremely low FPR of 10^-6.",NO,True,897,True,True
264439179-s15,A Survey on Detection of LLMs-Generated Content,2023-10-24,Zero-Shot,"In the zero-shot setting, we do not require extensive training data to train a discriminator.Instead, we can leverage the inherent distinctions between machine-generated and human-written text, making the detector training-free.The key advantage of training-free detection is its adaptability to new data distributions without the need for additional data collection and model tuning.It's worth noting that while watermarking methods can also be considered zero-shot, we treat them as an independent track.Previous work utilizes entropy (Lavergne et al., 2008), average log-probability score (Solaiman et al., 2019), perplexity (Beresneva, 2016), uncommon n-gram frequencies (Grechnikov et al., 2009;Badaskar et al., 2008) obtained from a language model as the judge for determining its origin.However, those simple features fail as LLMs are becoming diverse and high-quality text generators.

Similarly, there are also black-and white-box detection, as summarized below.",What are the limitations of using simple features for detecting LLM-generated content in zero-shot settings?,"Previous work utilizes entropy (Lavergne et al., 2008), average log-probability score (Solaiman et al., 2019), perplexity (Beresneva, 2016), and uncommon n-gram frequencies (Grechnikov et al., 2009; Badaskar et al., 2008) obtained from a language model as the judge for determining its origin. However, those simple features fail as LLMs are becoming diverse and high-quality text generators.",What challenges arise when using basic features to identify content generated by large language models in zero-shot scenarios?,"Previous research has employed various metrics such as entropy (Lavergne et al., 2008), average log-probability score (Solaiman et al., 2019), perplexity (Beresneva, 2016), and uncommon n-gram frequencies (Grechnikov et al., 2009; Badaskar et al., 2008) derived from a language model to determine the origin of text. However, these simple features are increasingly inadequate as large language models (LLMs) evolve into more diverse and high-quality text generators.",7,4,8,6,"Question:

What are the limitations of using simple features for detecting LLM-generated content in zero-shot settings?

Answer:

Previous work utilizes entropy (Lavergne et al., 2008), average log-probability score (Solaiman et al., 2019), perplexity (Beresneva, 2016), and uncommon n-gram frequencies (Grechnikov et al., 2009; Badaskar et al., 2008) obtained from a language model as the judge for determining its origin. However, those simple features fail as LLMs are becoming diverse and high-quality text generators.","Question:

What challenges arise when using basic features to identify content generated by large language models in zero-shot scenarios?

Answer:

Previous research has employed various metrics such as entropy (Lavergne et al., 2008), average log-probability score (Solaiman et al., 2019), perplexity (Beresneva, 2016), and uncommon n-gram frequencies (Grechnikov et al., 2009; Badaskar et al., 2008) derived from a language model to determine the origin of text. However, these simple features are increasingly inadequate as large language models (LLMs) evolve into more diverse and high-quality text generators.",NO,True,466,True,True
264439179-s19,A Survey on Detection of LLMs-Generated Content,2023-10-24,Black-Box Watermarking,"In black-box setting, such as API-based applications, the proprietary nature of the language models used by LLM providers precludes downstream users from accessing the sampling process for commercial reasons.Alternatively, a user may wish to watermark human-authored text via postprocessing.In such cases, black-box watermarking aims to automatically manipulate generated text to embed watermarks readable by third parties.Traditional works designed complex linguistic rules such as paraphrasing (Atallah et al., 2003), syntax tree manipulations (Topkara et al., 2005;Meral et al., 2009) and synonym substitution (Topkara et al., 2006), but lack scalability.Later work turns to pretrained language models for efficient watermarking.For example, Yang et al. (2022) proposes a natural language watermarking scheme based on contextaware lexical substitution (LS).Specifically, they employ BERT (Devlin et al., 2019) to suggest LS candidates by inferring the semantic relatedness between the candidates and the original sentence.Yang et al. (2023a) first defines a binary encoding function to compute a random binary encoding corresponding to a word.The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 is approximately 0.5.To inject a watermark, they alter the distribution by selectively replacing words representing bit-0 with contextbased synonyms that represent bit-1.A statistical test is then used to identify the watermark.",How has the efficiency of watermarking improved with the use of pretrained language models?,"Later work turns to pretrained language models for efficient watermarking. For example, Yang et al. (2022) proposes a natural language watermarking scheme based on context-aware lexical substitution (LS). Specifically, they employ BERT (Devlin et al., 2019) to suggest LS candidates by inferring the semantic relatedness between the candidates and the original sentence. Yang et al. (2023a) first defines a binary encoding function to compute a random binary encoding corresponding to a word. The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 is approximately 0.5. To inject a watermark, they alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. A statistical test is then used to identify the watermark.",How have pretrained language models enhanced the efficiency of watermarking techniques?,"Recent advancements in watermarking have leveraged pretrained language models to improve efficiency. For instance, Yang et al. (2022) introduced a natural language watermarking scheme that utilizes context-aware lexical substitution (LS). This approach employs BERT (Devlin et al., 2019) to suggest LS candidates by assessing the semantic relatedness between the candidates and the original sentence. In a subsequent study, Yang et al. (2023a) defined a binary encoding function to generate a random binary encoding for each word. In non-watermarked text, these encodings follow a Bernoulli distribution, where the probability of a word representing bit-1 is approximately 0.5. To embed a watermark, the distribution is altered by selectively replacing words that represent bit-0 with context-based synonyms that represent bit-1. A statistical test is then applied to detect the presence of the watermark.",5,4,7,4,"Question:

How has the efficiency of watermarking improved with the use of pretrained language models?

Answer:

Later work turns to pretrained language models for efficient watermarking. For example, Yang et al. (2022) proposes a natural language watermarking scheme based on context-aware lexical substitution (LS). Specifically, they employ BERT (Devlin et al., 2019) to suggest LS candidates by inferring the semantic relatedness between the candidates and the original sentence. Yang et al. (2023a) first defines a binary encoding function to compute a random binary encoding corresponding to a word. The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 is approximately 0.5. To inject a watermark, they alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. A statistical test is then used to identify the watermark.","Question:

How have pretrained language models enhanced the efficiency of watermarking techniques?

Answer:

Recent advancements in watermarking have leveraged pretrained language models to improve efficiency. For instance, Yang et al. (2022) introduced a natural language watermarking scheme that utilizes context-aware lexical substitution (LS). This approach employs BERT (Devlin et al., 2019) to suggest LS candidates by assessing the semantic relatedness between the candidates and the original sentence. In a subsequent study, Yang et al. (2023a) defined a binary encoding function to generate a random binary encoding for each word. In non-watermarked text, these encodings follow a Bernoulli distribution, where the probability of a word representing bit-1 is approximately 0.5. To embed a watermark, the distribution is altered by selectively replacing words that represent bit-0 with context-based synonyms that represent bit-1. A statistical test is then applied to detect the presence of the watermark.",NO,True,905,True,True
232075945-s2,A Survey on Stance Detection for Mis-and Disinformation Identification,2021-02-27,Source(s) Target,"Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.",How is stance detection defined and differentiated from related NLP tasks?,"Stance detection is defined as a classification problem where the stance of the author of a text is sought in the form of a category label from the set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text (Augenstein et al., 2016; Mohammad et al., 2016). Stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognize emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the point-of-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","What is stance detection, and how does it differ from other related NLP tasks?","Stance detection is a classification task in natural language processing (NLP) that involves determining the stance of the author of a text, categorizing it into labels such as Favor, Against, or Neither. Sometimes, a Neutral category is also included (Mohammad et al., 2016). The target of the stance may or may not be explicitly mentioned in the text (Augenstein et al., 2016; Mohammad et al., 2016). 

Stance detection is distinct from several other related NLP tasks. For instance, biased language detection focuses on identifying any inclination or tendency towards a particular perspective within a text. Emotion recognition aims to identify emotions like love or anger expressed in the text. Perspective identification seeks to determine the author's point of view, such as political affiliation (e.g., Democrat vs. Republican), with the target always being explicit. Sarcasm detection is concerned with identifying satirical or ironic text, often intended for ridicule or mockery. Lastly, sentiment analysis examines the polarity of a text, determining whether it is positive, negative, or neutral.",9,2,9,2,"Question:

How is stance detection defined and differentiated from related NLP tasks?

Answer:

Stance detection is defined as a classification problem where the stance of the author of a text is sought in the form of a category label from the set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text (Augenstein et al., 2016; Mohammad et al., 2016). Stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognize emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the point-of-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Question:

What is stance detection, and how does it differ from other related NLP tasks?

Answer:

Stance detection is a classification task in natural language processing (NLP) that involves determining the stance of the author of a text, categorizing it into labels such as Favor, Against, or Neither. Sometimes, a Neutral category is also included (Mohammad et al., 2016). The target of the stance may or may not be explicitly mentioned in the text (Augenstein et al., 2016; Mohammad et al., 2016). 

Stance detection is distinct from several other related NLP tasks. For instance, biased language detection focuses on identifying any inclination or tendency towards a particular perspective within a text. Emotion recognition aims to identify emotions like love or anger expressed in the text. Perspective identification seeks to determine the author's point of view, such as political affiliation (e.g., Democrat vs. Republican), with the target always being explicit. Sarcasm detection is concerned with identifying satirical or ironic text, often intended for ridicule or mockery. Lastly, sentiment analysis examines the polarity of a text, determining whether it is positive, negative, or neutral.",NO,True,1106,True,True
232075945-s5,A Survey on Stance Detection for Mis-and Disinformation Identification,2021-02-27,Stance as a (Mis-/Dis-)information Detection Component,"Fully automated systems can assist in gauging the extent and studying the spread of false information online. This is in contrast to the previously discussed applications of stance detection -as a stand-alone system for detecting mis-and disinformation. Here, we review its potency to serve as a component in an automated pipeline. Figure 1b illustrates the setup, which can also include steps such as modelling the user or profiling the media outlet among others. We discuss in more detail media profiling and misconceptions in Appendix B.

Rumors Stance detection can be used for rumour detection and debunking, where the stance of the crowd, media, or other sources towards a claim are used to determine the veracity of a currently circulating story or report of uncertain or doubtful factuality. More formally, for a textual input and a rumour expressed as text, stance detection here is to determine the position of the text towards the rumour as a category label from the set {Support, Deny, Query, Comment}. Zubiaga et al. (2016b) define these categories as whether the author: supports (Support) or denies (Deny) the veracity of the rumour they are responding to, ""asks for additional evidence in relation to the veracity of the rumour"" (Query) or ""makes their own comment without a clear contribution to assessing the veracity of the rumour"" (Comment). This setup was widely explored for microblogs and social media. Qazvinian et al. (2011) started with five rumours and classified the user's stance as endorse, deny, unrelated, question, or neutral. While they were among the first to demonstrate the feasibility of this task formulation, the limited size of their study and the focus on assessing the stance of individual posts limited its real-world applicability. Zubiaga et al. (2016b) analysed how people spread rumours on social media based on conversational threads. They included rumour threads associated with nine newsworthy events, and users' stance before and after the rumours were confirmed or denied. Ferreira and Vlachos (2016) collected claims and news articles from rumour sites with annotations for stance and veracity by journalists as part of the Emergent project. The goal was to use the stance of a news article, summarised into a single sentence, towards a claim as one of the components to determine its veracity. A downside is the need to summarise, in contrast to FNC-1 (Pomerleau and Rao, 2017), where entire news articles were used.  Zubiaga et al. (2016b). Bošnjak and Karan (2019) studied stance detection and claim verification of comments for Croatian news articles.","How is stance detection utilized for rumor detection and debunking, and what are its formal definitions and categories?","Fully automated systems can assist in gauging the extent and studying the spread of false information online. Stance detection can be used for rumour detection and debunking, where the stance of the crowd, media, or other sources towards a claim are used to determine the veracity of a currently circulating story or report of uncertain or doubtful factuality. More formally, for a textual input and a rumour expressed as text, stance detection here is to determine the position of the text towards the rumour as a category label from the set {Support, Deny, Query, Comment}. Zubiaga et al. (2016b) define these categories as whether the author: supports (Support) or denies (Deny) the veracity of the rumour they are responding to, ""asks for additional evidence in relation to the veracity of the rumour"" (Query) or ""makes their own comment without a clear contribution to assessing the veracity of the rumour"" (Comment). This setup was widely explored for microblogs and social media.","How is stance detection applied in the context of rumor detection and debunking, and what are its formal definitions and categories?","Stance detection plays a crucial role in the automated systems designed to assess and analyze the spread of false information online. It is particularly useful in the context of rumor detection and debunking. By evaluating the stance of the crowd, media, or other sources towards a specific claim, stance detection helps determine the truthfulness of a circulating story or report that may be of uncertain or doubtful factuality.

Formally, stance detection involves analyzing a textual input in relation to a rumor expressed as text. The goal is to categorize the position of the text towards the rumor using one of the following labels: Support, Deny, Query, or Comment. According to Zubiaga et al. (2016b), these categories are defined as follows: ""Support"" indicates that the author supports the veracity of the rumor; ""Deny"" means the author denies the rumor's veracity; ""Query"" is used when the author asks for additional evidence regarding the rumor's veracity; and ""Comment"" applies when the author makes a remark without a clear contribution to assessing the rumor's truthfulness. This framework has been extensively explored in the context of microblogs and social media platforms.",7,4,7,4,"Question:

How is stance detection utilized for rumor detection and debunking, and what are its formal definitions and categories?

Answer:

Fully automated systems can assist in gauging the extent and studying the spread of false information online. Stance detection can be used for rumour detection and debunking, where the stance of the crowd, media, or other sources towards a claim are used to determine the veracity of a currently circulating story or report of uncertain or doubtful factuality. More formally, for a textual input and a rumour expressed as text, stance detection here is to determine the position of the text towards the rumour as a category label from the set {Support, Deny, Query, Comment}. Zubiaga et al. (2016b) define these categories as whether the author: supports (Support) or denies (Deny) the veracity of the rumour they are responding to, ""asks for additional evidence in relation to the veracity of the rumour"" (Query) or ""makes their own comment without a clear contribution to assessing the veracity of the rumour"" (Comment). This setup was widely explored for microblogs and social media.","Question:

How is stance detection applied in the context of rumor detection and debunking, and what are its formal definitions and categories?

Answer:

Stance detection plays a crucial role in the automated systems designed to assess and analyze the spread of false information online. It is particularly useful in the context of rumor detection and debunking. By evaluating the stance of the crowd, media, or other sources towards a specific claim, stance detection helps determine the truthfulness of a circulating story or report that may be of uncertain or doubtful factuality.

Formally, stance detection involves analyzing a textual input in relation to a rumor expressed as text. The goal is to categorize the position of the text towards the rumor using one of the following labels: Support, Deny, Query, or Comment. According to Zubiaga et al. (2016b), these categories are defined as follows: ""Support"" indicates that the author supports the veracity of the rumor; ""Deny"" means the author denies the rumor's veracity; ""Query"" is used when the author asks for additional evidence regarding the rumor's veracity; and ""Comment"" applies when the author makes a remark without a clear contribution to assessing the rumor's truthfulness. This framework has been extensively explored in the context of microblogs and social media platforms.",NO,True,1191,True,True
232075945-s6,A Survey on Stance Detection for Mis-and Disinformation Identification,2021-02-27,Multiple languages,"In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.",How has transfer learning with pre-trained Transformers improved stance detection performance across different languages?,"Transfer learning with pre-trained Transformers has significantly improved the performance of stance detection across different languages. For instance, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for the Disagree class, and from 50 to 70 for the Agree class. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting, showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).",How has the use of transfer learning with pre-trained Transformers enhanced stance detection performance in various languages?,"The application of transfer learning with pre-trained Transformers has markedly enhanced stance detection performance across multiple languages. For example, employing the pre-trained Transformer RoBERTa has led to a significant increase in F1 scores, from 18 to 58 for the Disagree class and from 50 to 70 for the Agree class. This improvement is also evident in cross-lingual contexts. In Arabic, Khouja (2020) achieved an F1 score of 76.7 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) demonstrated substantial improvements on 15 datasets by applying pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting. Furthermore, Alhindi et al. (2021) highlighted the importance of language-specific pre-training, surpassing the state of the art with an F1 score of 52 on AraStance and 78 on Arabic FC.",7,4,8,4,"Question:

How has transfer learning with pre-trained Transformers improved stance detection performance across different languages?

Answer:

Transfer learning with pre-trained Transformers has significantly improved the performance of stance detection across different languages. For instance, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for the Disagree class, and from 50 to 70 for the Agree class. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting, showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).","Question:

How has the use of transfer learning with pre-trained Transformers enhanced stance detection performance in various languages?

Answer:

The application of transfer learning with pre-trained Transformers has markedly enhanced stance detection performance across multiple languages. For example, employing the pre-trained Transformer RoBERTa has led to a significant increase in F1 scores, from 18 to 58 for the Disagree class and from 50 to 70 for the Agree class. This improvement is also evident in cross-lingual contexts. In Arabic, Khouja (2020) achieved an F1 score of 76.7 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) demonstrated substantial improvements on 15 datasets by applying pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting. Furthermore, Alhindi et al. (2021) highlighted the importance of language-specific pre-training, surpassing the state of the art with an F1 score of 52 on AraStance and 78 on Arabic FC.",NO,True,869,True,True
259833865-s8,A Survey of Evaluation Methods of Generated Medical Textual Reports,2023,Automatic Evaluation,"Automatic evaluation is popular because it is cheap and fast and it is widely used in benchmarking activity and for system development.

There is a wide range of automatic evaluation metrics used in NLG (Sai et al., 2022) and we will restrict to the two most popular: 1) the corpusbased metrics and 2) the trainable metrics. The corpus-based metrics rely on a set of reference texts (i.e. gold standard outputs) to which system outputs are compared. For instance, it can be based on n-grams, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004); or on edit distance: WER (Woodard and Nelson, 1982), TER (Snover et al., 2006), etc.

Most automatic metrics require gold references, but these are not always available. Reference-less metrics, where neural models are trained to predict human ratings from texts (e.g. regression models trained on ratings data), are getting more and more attention recently. For example, BLEURT (Sellam et al., 2020) is a learned evaluation metric for English to predict human judgments. It relies on the BERT model using unsupervised techniques with millions of synthetic examples. There are also metrics based on question-answer pairs on a given source document Rebuffel et al., 2021), for example QuestEval  uses pre-trained models to assess if two different inputs contain the same information. Note that QuestEval can also be used with references.

To summarize, ROUGE scores assess the similarity between candidates and references based on the overlap of unigrams, bigrams, and the longest common sequence, likewise for BLEU; while BLEU focuses on precision, ROUGE focuses on recall. BERTScore evaluates the similarity between candidates and references at token level, using contextual embeddings from BERT, while QuestEval assesses whether a summary contains all the relevant information from its source document and BLEURT attempts to model human judgments.

However, automatic evaluation metrics have their limitations and do not sufficiently reflect human judgments of system performance (Novikova et al., 2017).","What are the two most popular types of automatic evaluation metrics in NLG, and how do they differ?","The two most popular types of automatic evaluation metrics in NLG are corpus-based metrics and trainable metrics. The corpus-based metrics rely on a set of reference texts (i.e., gold standard outputs) to which system outputs are compared. For instance, they can be based on n-grams, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), or on edit distance, such as WER (Woodard and Nelson, 1982) and TER (Snover et al., 2006). Most automatic metrics require gold references, but these are not always available. Reference-less metrics, where neural models are trained to predict human ratings from texts (e.g., regression models trained on ratings data), are gaining more attention recently. For example, BLEURT (Sellam et al., 2020) is a learned evaluation metric for English to predict human judgments. It relies on the BERT model using unsupervised techniques with millions of synthetic examples. There are also metrics based on question-answer pairs on a given source document, such as QuestEval, which uses pre-trained models to assess if two different inputs contain the same information. QuestEval can also be used with references.","What are the two most popular types of automatic evaluation metrics in Natural Language Generation (NLG), and how do they differ?","The two most popular types of automatic evaluation metrics in NLG are corpus-based metrics and trainable metrics. Corpus-based metrics rely on a set of reference texts, also known as gold standard outputs, to which system outputs are compared. These metrics often utilize n-grams, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), or edit distance measures like WER (Woodard and Nelson, 1982) and TER (Snover et al., 2006). However, these metrics require gold references, which are not always available.

In contrast, trainable metrics do not rely on reference texts. Instead, they involve neural models trained to predict human ratings from texts, often using regression models trained on ratings data. An example of this is BLEURT (Sellam et al., 2020), a learned evaluation metric for English that predicts human judgments. BLEURT leverages the BERT model and employs unsupervised techniques with millions of synthetic examples. Additionally, there are metrics based on question-answer pairs on a given source document, such as QuestEval, which uses pre-trained models to assess if two different inputs contain the same information. QuestEval can also be used with references.",9,2,9,2,"Question:

What are the two most popular types of automatic evaluation metrics in NLG, and how do they differ?

Answer:

The two most popular types of automatic evaluation metrics in NLG are corpus-based metrics and trainable metrics. The corpus-based metrics rely on a set of reference texts (i.e., gold standard outputs) to which system outputs are compared. For instance, they can be based on n-grams, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), or on edit distance, such as WER (Woodard and Nelson, 1982) and TER (Snover et al., 2006). Most automatic metrics require gold references, but these are not always available. Reference-less metrics, where neural models are trained to predict human ratings from texts (e.g., regression models trained on ratings data), are gaining more attention recently. For example, BLEURT (Sellam et al., 2020) is a learned evaluation metric for English to predict human judgments. It relies on the BERT model using unsupervised techniques with millions of synthetic examples. There are also metrics based on question-answer pairs on a given source document, such as QuestEval, which uses pre-trained models to assess if two different inputs contain the same information. QuestEval can also be used with references.","Question:

What are the two most popular types of automatic evaluation metrics in Natural Language Generation (NLG), and how do they differ?

Answer:

The two most popular types of automatic evaluation metrics in NLG are corpus-based metrics and trainable metrics. Corpus-based metrics rely on a set of reference texts, also known as gold standard outputs, to which system outputs are compared. These metrics often utilize n-grams, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), or edit distance measures like WER (Woodard and Nelson, 1982) and TER (Snover et al., 2006). However, these metrics require gold references, which are not always available.

In contrast, trainable metrics do not rely on reference texts. Instead, they involve neural models trained to predict human ratings from texts, often using regression models trained on ratings data. An example of this is BLEURT (Sellam et al., 2020), a learned evaluation metric for English that predicts human judgments. BLEURT leverages the BERT model and employs unsupervised techniques with millions of synthetic examples. Additionally, there are metrics based on question-answer pairs on a given source document, such as QuestEval, which uses pre-trained models to assess if two different inputs contain the same information. QuestEval can also be used with references.",NO,True,1189,True,True
259833865-s13,A Survey of Evaluation Methods of Generated Medical Textual Reports,2023,Medical Correctness: Report-based,"In the study by Joshi et al. (2020) two measures are defined: Medical Concept Coverage and Negation Correctness. The former captures the coverage of medical terms in the predicted summaries to the gold standard reference, while the latter identifies the negated status of medical concepts. In the healthcare domain, it is crucial to ensure highquality results in terms of accurate usage of medical terms and capturing negation.

The evaluation of Concept involves using specific and in-house extractors and Named Entity Recognition (NER) models. They refer to domainspecific knowledge and compare the match of extracted concepts to standardized health and biomedical vocabularies, such as the Unified Medical Language Systems (UMLS). Several studies have utilized concept correctness measures, such as F1score, precision, recall, and false positives, at various levels of granularity, including the report level and section level.

For instance, Joshi et al. (2020) used an inhouse medical entity extractor to match concepts in the summary to UMLS, and they used Negex (Harkema et al., 2009) to determine negated concepts. Medical concepts in the predicted summary that were not present in the original conversation would be false positives, and vice versa for false negatives. Among the concepts present in the predicted summary, they assessed precision and recall to see whether the predicted negation was accurate for the decoded concepts and computed a Negation F1. The set of automatic metrics proposed was then used in several works (Chintagunta et al., 2021;Nair et al., 2021).

If in-house entity extractor to match concepts in the summary to UMLS have been frequent (Soldaini and Goharian, 2016;Joshi et al., 2020;, entity extraction using machine learning has appeared recently, which is even more specific to the task. For instance, Enarvi et al. (2020) employed a machine learning-based clinical fact extractor to measure factual correctness by extracting medical facts from both the predicted reports and the ground-truth reports, such as conditions and medications, as well as their attributes such as body part, severity, or dosage, then calculat-ing the F1 score from these two sets. To compute Concept-F1, Chen et al. (2022) used the medical entity extractor -BERT-CRF (Devlin et al., 2019) trained on their NER task to match entities in the predicted summary to the reference summary.

Similarly, Ben Abacha et al. (2023) used ""factbased metrics (Fact Scores)"", which is a machine learning-based medical fact extraction system. The Fact Score metric measures the F1-score of medically relevant facts extraction, is used to assess the factual consistency of the generated summaries. The Fact-based metrics consist of two variants: Fact-Core, which relies on the extraction of seven core fact attributes, and Fact-Full, which combines these core facts and five additional attributes.

In addition, there is also work combining the two approaches to extract concepts:  extracted medical relevant concepts via one of two systems: their in-house rule-based system and quickUMLS (Soldaini and Goharian, 2016) -a Python implementation of UMLS. Their rulebased system was found to be effective in capturing symptom-related findings in clinical reports, and quickUMLS is capable of extracting a wide scope of medical findings such as symptoms, diseases, medication and procedures.

Moreover, Molenaar et al. (2020) measured the quality of the dialogue summarization pipeline for healthcare reporting by establishing the number of items included in the generated and gold standards, using precision, recall and false positives (FPs) as metrics. They followed the SOEP/SOAP format -Subjective (S), Objective (O), Evaluation (E) / Assessment (A) and Plan (P) -commonly used by general practitioners in the Netherlands. It appears that they manually calculated the number of items included in each section of the SOEP format for the eight reports generated.

However, concept-based evaluation can have its own limitations, particularly with regard to false positives errors,  employed filtering rules to attempt to mitigate this issue.

Additionally, Chen et al. (2022) reported Regexbased Diagnostic Accuracy (RD-Acc), which measures the model's ability to diagnose the disease. Their reference reports written by annotators contain six parts, RD-Acc is calculated using the regexbased approach based on the diagnosis part. They calculated for what percentage of the generated reports, the content of their diagnosis part contains the actual disease text or key concepts.",How is the evaluation of medical concept coverage and negation correctness conducted in generated medical reports?,"In the study by Joshi et al. (2020), two measures are defined: Medical Concept Coverage and Negation Correctness. The former captures the coverage of medical terms in the predicted summaries to the gold standard reference, while the latter identifies the negated status of medical concepts. The evaluation of Concept involves using specific and in-house extractors and Named Entity Recognition (NER) models. They refer to domain-specific knowledge and compare the match of extracted concepts to standardized health and biomedical vocabularies, such as the Unified Medical Language Systems (UMLS). Several studies have utilized concept correctness measures, such as F1 score, precision, recall, and false positives, at various levels of granularity, including the report level and section level. For instance, Joshi et al. (2020) used an in-house medical entity extractor to match concepts in the summary to UMLS, and they used Negex (Harkema et al., 2009) to determine negated concepts. Medical concepts in the predicted summary that were not present in the original conversation would be false positives, and vice versa for false negatives. Among the concepts present in the predicted summary, they assessed precision and recall to see whether the predicted negation was accurate for the decoded concepts and computed a Negation F1.",How is the evaluation of medical concept coverage and negation correctness performed in generated medical reports?,"In the study by Joshi et al. (2020), the evaluation of medical reports involves two key measures: Medical Concept Coverage and Negation Correctness. Medical Concept Coverage assesses how well the predicted summaries include medical terms compared to a gold standard reference. Negation Correctness, on the other hand, evaluates the accuracy of identifying the negated status of medical concepts. To conduct these evaluations, specific in-house extractors and Named Entity Recognition (NER) models are employed. These tools leverage domain-specific knowledge and compare extracted concepts against standardized health and biomedical vocabularies, such as the Unified Medical Language Systems (UMLS).

Several studies have applied concept correctness measures, including F1 score, precision, recall, and false positives, at different levels of granularity, such as the report level and section level. For example, Joshi et al. (2020) utilized an in-house medical entity extractor to align concepts in the summary with UMLS and employed Negex (Harkema et al., 2009) to identify negated concepts. Concepts present in the predicted summary but absent in the original conversation are considered false positives, while the reverse scenario results in false negatives. Among the concepts present in the predicted summary, precision and recall are assessed to determine the accuracy of predicted negation for the decoded concepts, culminating in the computation of a Negation F1 score.",7,2,7,4,"Question:

How is the evaluation of medical concept coverage and negation correctness conducted in generated medical reports?

Answer:

In the study by Joshi et al. (2020), two measures are defined: Medical Concept Coverage and Negation Correctness. The former captures the coverage of medical terms in the predicted summaries to the gold standard reference, while the latter identifies the negated status of medical concepts. The evaluation of Concept involves using specific and in-house extractors and Named Entity Recognition (NER) models. They refer to domain-specific knowledge and compare the match of extracted concepts to standardized health and biomedical vocabularies, such as the Unified Medical Language Systems (UMLS). Several studies have utilized concept correctness measures, such as F1 score, precision, recall, and false positives, at various levels of granularity, including the report level and section level. For instance, Joshi et al. (2020) used an in-house medical entity extractor to match concepts in the summary to UMLS, and they used Negex (Harkema et al., 2009) to determine negated concepts. Medical concepts in the predicted summary that were not present in the original conversation would be false positives, and vice versa for false negatives. Among the concepts present in the predicted summary, they assessed precision and recall to see whether the predicted negation was accurate for the decoded concepts and computed a Negation F1.","Question:

How is the evaluation of medical concept coverage and negation correctness performed in generated medical reports?

Answer:

In the study by Joshi et al. (2020), the evaluation of medical reports involves two key measures: Medical Concept Coverage and Negation Correctness. Medical Concept Coverage assesses how well the predicted summaries include medical terms compared to a gold standard reference. Negation Correctness, on the other hand, evaluates the accuracy of identifying the negated status of medical concepts. To conduct these evaluations, specific in-house extractors and Named Entity Recognition (NER) models are employed. These tools leverage domain-specific knowledge and compare extracted concepts against standardized health and biomedical vocabularies, such as the Unified Medical Language Systems (UMLS).

Several studies have applied concept correctness measures, including F1 score, precision, recall, and false positives, at different levels of granularity, such as the report level and section level. For example, Joshi et al. (2020) utilized an in-house medical entity extractor to align concepts in the summary with UMLS and employed Negex (Harkema et al., 2009) to identify negated concepts. Concepts present in the predicted summary but absent in the original conversation are considered false positives, while the reverse scenario results in false negatives. Among the concepts present in the predicted summary, precision and recall are assessed to determine the accuracy of predicted negation for the decoded concepts, culminating in the computation of a Negation F1 score.",NO,True,1477,True,True
259833865-s14,A Survey of Evaluation Methods of Generated Medical Textual Reports,2023,Medical Correctness: Auxiliary or Intermediate Tasks,"Another subcategory of automatic measures of concept correctness is those that evaluate auxiliary or intermediate tasks, there are two types: classification of electronic health record (EHR) categories and utterances classification.

To generate case notes from digital transcripts of doctor-patient conversations, Kazi and Kahanda (2019) divided the task into two subtasks: (1) predict semantic topics for segments of the transcripts (EHR categories) and then (2) generate a more formal version of the text that goes into the corresponding section of the EHR form. They used the AUROC (Area Under the ROC Curve) scale (Bewick et al., 2005) to assess their first task of predicting EHR categories, which could be any of the following: Client Details, Chief Complaint, Family History, Social History, Medical History and Others. Correct prediction of EHR categories could be useful for subsequent formal text generation.

For utterances classification, there are different types of classification such as classifying noteworthy utterance (Krishna et al., 2021), label prediction for medical conversation utterances (Song et al., 2020), and dialogue turn classification (Lacson et al., 2006).

In detail, Krishna et al. (2021) evaluated the multi-label classification of noteworthy utterances that are relevant to each summary section before clustering related utterances and generating one summary sentence per cluster. Their modular summarization technique outperforms its purely abstractive counterpart, producing much more factual and coherent sentences. Besides, Song et al. (2020) first identified two types of utterances (problem statements and treatment recommendations) and then generated summaries, they showed that for the particular dataset used, high-quality summaries can be generated by extracting these two types of utterances. Thus, in addition to reporting ROUGE scores, they also reported the precision, recall, and F scores of the predicted labels for utterances of medical conversations, compared to the standard labels. In addition, Lacson et al. (2006) also measured precision, recall, and F measure of dialogue turns classification.",What are the methods and metrics used for classifying utterances in medical text evaluation?,"For utterances classification in medical text evaluation, there are different types of classification such as classifying noteworthy utterance (Krishna et al., 2021), label prediction for medical conversation utterances (Song et al., 2020), and dialogue turn classification (Lacson et al., 2006). Krishna et al. (2021) evaluated the multi-label classification of noteworthy utterances that are relevant to each summary section before clustering related utterances and generating one summary sentence per cluster. Song et al. (2020) first identified two types of utterances (problem statements and treatment recommendations) and then generated summaries. They showed that for the particular dataset used, high-quality summaries can be generated by extracting these two types of utterances. In addition to reporting ROUGE scores, they also reported the precision, recall, and F scores of the predicted labels for utterances of medical conversations, compared to the standard labels. Lacson et al. (2006) also measured precision, recall, and F measure of dialogue turns classification.",What methods and metrics are used for classifying utterances in the evaluation of medical texts?,"In the evaluation of medical texts, various methods are employed for classifying utterances. One approach is the multi-label classification of noteworthy utterances, as demonstrated by Krishna et al. (2021). They focused on identifying utterances relevant to each summary section, clustering related utterances, and generating a summary sentence for each cluster. Another method, explored by Song et al. (2020), involves label prediction for medical conversation utterances. They identified two primary types of utterances: problem statements and treatment recommendations, and used these to generate high-quality summaries. Their evaluation included metrics such as ROUGE scores, precision, recall, and F scores, comparing predicted labels to standard labels. Additionally, Lacson et al. (2006) investigated dialogue turn classification, also measuring precision, recall, and F measures to assess the effectiveness of their classification approach.",7,4,7,7,"Question:

What are the methods and metrics used for classifying utterances in medical text evaluation?

Answer:

For utterances classification in medical text evaluation, there are different types of classification such as classifying noteworthy utterance (Krishna et al., 2021), label prediction for medical conversation utterances (Song et al., 2020), and dialogue turn classification (Lacson et al., 2006). Krishna et al. (2021) evaluated the multi-label classification of noteworthy utterances that are relevant to each summary section before clustering related utterances and generating one summary sentence per cluster. Song et al. (2020) first identified two types of utterances (problem statements and treatment recommendations) and then generated summaries. They showed that for the particular dataset used, high-quality summaries can be generated by extracting these two types of utterances. In addition to reporting ROUGE scores, they also reported the precision, recall, and F scores of the predicted labels for utterances of medical conversations, compared to the standard labels. Lacson et al. (2006) also measured precision, recall, and F measure of dialogue turns classification.","Question:

What methods and metrics are used for classifying utterances in the evaluation of medical texts?

Answer:

In the evaluation of medical texts, various methods are employed for classifying utterances. One approach is the multi-label classification of noteworthy utterances, as demonstrated by Krishna et al. (2021). They focused on identifying utterances relevant to each summary section, clustering related utterances, and generating a summary sentence for each cluster. Another method, explored by Song et al. (2020), involves label prediction for medical conversation utterances. They identified two primary types of utterances: problem statements and treatment recommendations, and used these to generate high-quality summaries. Their evaluation included metrics such as ROUGE scores, precision, recall, and F scores, comparing predicted labels to standard labels. Additionally, Lacson et al. (2006) investigated dialogue turn classification, also measuring precision, recall, and F measures to assess the effectiveness of their classification approach.",NO,True,949,True,True
259833865-s16,A Survey of Evaluation Methods of Generated Medical Textual Reports,2023,Intrinsic Approaches,"The intrinsic human evaluation of generated reports comprises two categories as for automated metrics: text quality and medical correctness.

Text quality is important in MRG as in general NLG output. For text quality, a wide variety of properties can be considered and various linguistic parameters can be used, e.g. relevance, consistency, fluency, coherence, missing, hallucination, repetition and contraction. As an example,  used four standard linguistic parameters: relevance (selection of relevant content), consistency (factual alignment between the summary and the source), fluency (linguistic quality of each sentence), and coherence (structure and organization of summary). In addition to these commonly used and well-studied criteria, the evaluation of MRG also concludes other medical correctness criteria, such as factually correct and medically relevant information (Joshi et al., 2020;Chintagunta et al., 2021), which are specific to MRG tasks. As another example, Ben Abacha et al. (2023) performed expert-based manual evaluation using NLG criteria such as Fluency and Non-redundancy, and medical criteria such as Critical Omissions, Hallucinations, Correct Facts, Incorrect Facts based on fact extraction. Furthermore, depending on whether evaluators assess the output directly or by comparing different texts, intrinsic human evaluation can be classified into direct and relative evaluation. As for the articles involving human evaluation, they all used at least direct evaluation, i.e. the evaluators judged the generated texts directly on a defined scale. Some authors also performed relative evaluation in addition to direct evaluation: Joshi et al. (2020) and Chintagunta et al. (2021) performed a comparison task in which, given two summaries generated by different models and the associated dialogue, annotators had to choose which summary was better, they could also choose ""both"" and ""none"". Yim and Yetisgen (2021) ranked the four systems against each other, with 1 being the best, in addition to evaluating each system independently with a score from 1 to 5 for the categories relevancy, factual accuracy, writing-style, completeness, and overall.

In general, MRG outputs are evaluated at the report level, however, depending on the design of the model, some are additionally evaluated at the sentence/section/part level. For example, Krishna et al. (2021) divided SOAP notes into several subsections: Family Medical History, Past Surgical History, Chief Complaint, etc. Therefore, they evaluated the generated SOAP notes in two ways: 1) SOAP note sentence level and 2) SOAP note level.","How is medical correctness evaluated in the context of medical report generation, and what criteria are used?","In the context of medical report generation, medical correctness is evaluated using criteria such as factually correct and medically relevant information (Joshi et al., 2020; Chintagunta et al., 2021), which are specific to MRG tasks. Ben Abacha et al. (2023) performed expert-based manual evaluation using medical criteria such as Critical Omissions, Hallucinations, Correct Facts, and Incorrect Facts based on fact extraction.","How is medical correctness assessed in the generation of medical reports, and what are the criteria involved?","Medical correctness in the generation of medical reports is assessed using specific criteria tailored to Medical Report Generation (MRG) tasks. These criteria include ensuring that the information is factually correct and medically relevant, as highlighted by Joshi et al. (2020) and Chintagunta et al. (2021). Additionally, Ben Abacha et al. (2023) conducted an expert-based manual evaluation using medical criteria such as Critical Omissions, Hallucinations, Correct Facts, and Incorrect Facts, which are based on fact extraction.",8,4,8,6,"Question:

How is medical correctness evaluated in the context of medical report generation, and what criteria are used?

Answer:

In the context of medical report generation, medical correctness is evaluated using criteria such as factually correct and medically relevant information (Joshi et al., 2020; Chintagunta et al., 2021), which are specific to MRG tasks. Ben Abacha et al. (2023) performed expert-based manual evaluation using medical criteria such as Critical Omissions, Hallucinations, Correct Facts, and Incorrect Facts based on fact extraction.","Question:

How is medical correctness assessed in the generation of medical reports, and what are the criteria involved?

Answer:

Medical correctness in the generation of medical reports is assessed using specific criteria tailored to Medical Report Generation (MRG) tasks. These criteria include ensuring that the information is factually correct and medically relevant, as highlighted by Joshi et al. (2020) and Chintagunta et al. (2021). Additionally, Ben Abacha et al. (2023) conducted an expert-based manual evaluation using medical criteria such as Critical Omissions, Hallucinations, Correct Facts, and Incorrect Facts, which are based on fact extraction.",NO,True,532,True,True
259833865-s17,A Survey of Evaluation Methods of Generated Medical Textual Reports,2023,Extrinsic Approaches,"As for extrinsic human evaluation: to evaluate generated summaries, a team of mental health experts used clinical acceptability framework (Sekhon et al., 2017), which includes six parameters: affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness . In addition, to perform a task-based evaluation and measure the usefulness of summaries for preserving important information in the medical setting, Lacson et al. (2006) asked physicians and nurses to create a list of key questions based on topics that commonly arise between hemodialysis patients and caregivers, and then asked five physicians to answer each of the six ""yes/no"" questions using each of 40 dialogues. Furthermore, in a study evaluating the correlation between human evaluation and automatic metrics in consultation note generation, Moramarco et al. (2022) asked 5 clinicians to post-edit generated notes and extract all errors.",How is extrinsic human evaluation applied to assess the quality of generated medical summaries?,"As for extrinsic human evaluation: to evaluate generated summaries, a team of mental health experts used a clinical acceptability framework (Sekhon et al., 2017), which includes six parameters: affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness. In addition, to perform a task-based evaluation and measure the usefulness of summaries for preserving important information in the medical setting, Lacson et al. (2006) asked physicians and nurses to create a list of key questions based on topics that commonly arise between hemodialysis patients and caregivers, and then asked five physicians to answer each of the six ""yes/no"" questions using each of 40 dialogues. Furthermore, in a study evaluating the correlation between human evaluation and automatic metrics in consultation note generation, Moramarco et al. (2022) asked 5 clinicians to post-edit generated notes and extract all errors.",How is extrinsic human evaluation used to assess the quality of generated medical summaries?,"Extrinsic human evaluation of generated medical summaries involves several approaches to ensure their quality and usefulness. One method is using a clinical acceptability framework, as outlined by Sekhon et al. (2017), which evaluates summaries based on six parameters: affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness. Additionally, Lacson et al. (2006) conducted a task-based evaluation to measure the summaries' usefulness in preserving important information in medical settings. They asked physicians and nurses to create a list of key questions commonly arising between hemodialysis patients and caregivers, and then had five physicians answer each of six ""yes/no"" questions using 40 different dialogues. Furthermore, Moramarco et al. (2022) explored the correlation between human evaluation and automatic metrics in consultation note generation by having five clinicians post-edit generated notes and extract all errors. These methods collectively contribute to a comprehensive assessment of the quality of generated medical summaries.",7,4,7,4,"Question:

How is extrinsic human evaluation applied to assess the quality of generated medical summaries?

Answer:

As for extrinsic human evaluation: to evaluate generated summaries, a team of mental health experts used a clinical acceptability framework (Sekhon et al., 2017), which includes six parameters: affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness. In addition, to perform a task-based evaluation and measure the usefulness of summaries for preserving important information in the medical setting, Lacson et al. (2006) asked physicians and nurses to create a list of key questions based on topics that commonly arise between hemodialysis patients and caregivers, and then asked five physicians to answer each of the six ""yes/no"" questions using each of 40 dialogues. Furthermore, in a study evaluating the correlation between human evaluation and automatic metrics in consultation note generation, Moramarco et al. (2022) asked 5 clinicians to post-edit generated notes and extract all errors.","Question:

How is extrinsic human evaluation used to assess the quality of generated medical summaries?

Answer:

Extrinsic human evaluation of generated medical summaries involves several approaches to ensure their quality and usefulness. One method is using a clinical acceptability framework, as outlined by Sekhon et al. (2017), which evaluates summaries based on six parameters: affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness. Additionally, Lacson et al. (2006) conducted a task-based evaluation to measure the summaries' usefulness in preserving important information in medical settings. They asked physicians and nurses to create a list of key questions commonly arising between hemodialysis patients and caregivers, and then had five physicians answer each of six ""yes/no"" questions using 40 different dialogues. Furthermore, Moramarco et al. (2022) explored the correlation between human evaluation and automatic metrics in consultation note generation by having five clinicians post-edit generated notes and extract all errors. These methods collectively contribute to a comprehensive assessment of the quality of generated medical summaries.",NO,True,1090,True,True
259833865-s18,A Survey of Evaluation Methods of Generated Medical Textual Reports,2023,Presence of Domain Experts,"Most of the articles we reviewed that included a human evaluation involved domain experts, such as doctors serving patients on their telehealth platform (Chintagunta et al., 2021;Joshi et al., 2020), five licensed physicians (Lacson et al., 2006), three general practice physicians (Moramarco et al., 2021), an annotator with a medical degree (Yim and Yetisgen, 2021), etc. Sometimes, the expertise of the annotators is not specified, e.g. ""trained human annotators"" (Krishna et al., 2021).

We also note that of the 9 articles including human evaluation, 5 of them reported Inter-Evaluator Agreement: three of the medical dialogue summarization articles Moramarco et al., 2022;Ben Abacha et al., 2023), and two medical (report) summarization articles (Moramarco et al., 2021;Karn et al., 2022). It would be prefer-able to indicate Inter-Evaluator Agreement in the presence of several annotators.",What role do domain experts play in the human evaluation of generated medical textual reports?,"Most of the articles reviewed that included a human evaluation involved domain experts, such as doctors serving patients on their telehealth platform (Chintagunta et al., 2021; Joshi et al., 2020), five licensed physicians (Lacson et al., 2006), three general practice physicians (Moramarco et al., 2021), an annotator with a medical degree (Yim and Yetisgen, 2021), etc. Sometimes, the expertise of the annotators is not specified, e.g., ""trained human annotators"" (Krishna et al., 2021). Of the 9 articles including human evaluation, 5 of them reported Inter-Evaluator Agreement: three of the medical dialogue summarization articles (Moramarco et al., 2022; Ben Abacha et al., 2023), and two medical (report) summarization articles (Moramarco et al., 2021; Karn et al., 2022). It would be preferable to indicate Inter-Evaluator Agreement in the presence of several annotators.",What is the significance of involving domain experts in the human evaluation of generated medical textual reports?,"In the human evaluation of generated medical textual reports, domain experts, such as doctors and licensed physicians, play a crucial role. For instance, studies have involved doctors serving patients on telehealth platforms (Chintagunta et al., 2021; Joshi et al., 2020), five licensed physicians (Lacson et al., 2006), and three general practice physicians (Moramarco et al., 2021). Additionally, some evaluations include annotators with medical degrees (Yim and Yetisgen, 2021). However, there are instances where the expertise of annotators is not specified, as seen with ""trained human annotators"" (Krishna et al., 2021). Among the nine articles that included human evaluation, five reported Inter-Evaluator Agreement, which is crucial when multiple annotators are involved. This agreement was noted in three medical dialogue summarization articles (Moramarco et al., 2022; Ben Abacha et al., 2023) and two medical report summarization articles (Moramarco et al., 2021; Karn et al., 2022). Indicating Inter-Evaluator Agreement is preferable to ensure consistency and reliability in evaluations involving several annotators.",9,4,9,4,"Question:

What role do domain experts play in the human evaluation of generated medical textual reports?

Answer:

Most of the articles reviewed that included a human evaluation involved domain experts, such as doctors serving patients on their telehealth platform (Chintagunta et al., 2021; Joshi et al., 2020), five licensed physicians (Lacson et al., 2006), three general practice physicians (Moramarco et al., 2021), an annotator with a medical degree (Yim and Yetisgen, 2021), etc. Sometimes, the expertise of the annotators is not specified, e.g., ""trained human annotators"" (Krishna et al., 2021). Of the 9 articles including human evaluation, 5 of them reported Inter-Evaluator Agreement: three of the medical dialogue summarization articles (Moramarco et al., 2022; Ben Abacha et al., 2023), and two medical (report) summarization articles (Moramarco et al., 2021; Karn et al., 2022). It would be preferable to indicate Inter-Evaluator Agreement in the presence of several annotators.","Question:

What is the significance of involving domain experts in the human evaluation of generated medical textual reports?

Answer:

In the human evaluation of generated medical textual reports, domain experts, such as doctors and licensed physicians, play a crucial role. For instance, studies have involved doctors serving patients on telehealth platforms (Chintagunta et al., 2021; Joshi et al., 2020), five licensed physicians (Lacson et al., 2006), and three general practice physicians (Moramarco et al., 2021). Additionally, some evaluations include annotators with medical degrees (Yim and Yetisgen, 2021). However, there are instances where the expertise of annotators is not specified, as seen with ""trained human annotators"" (Krishna et al., 2021). Among the nine articles that included human evaluation, five reported Inter-Evaluator Agreement, which is crucial when multiple annotators are involved. This agreement was noted in three medical dialogue summarization articles (Moramarco et al., 2022; Ben Abacha et al., 2023) and two medical report summarization articles (Moramarco et al., 2021; Karn et al., 2022). Indicating Inter-Evaluator Agreement is preferable to ensure consistency and reliability in evaluations involving several annotators.",NO,True,1128,True,True
3176028-s16,Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review,1981-04-01,Discourse-cohesion approaches to anaphora resolution,"Another approach to coreference resolution attempts to exploit local discourse cohesion, building a representation of the discourse with which references can be resolved. This approach has been taken by (inter alia) Klappholz and Lockman (1977;Lockman 1978). By using only cues to the discourse structure at the sentence level or lower, one avoids the need to search for referents in pre-determined dialogue models such as those of Grosz's task-oriented dialogues, or rigidly predefined knowledge structures such as scripts (Schank and Abelson 1977) and frames (Minsky 1975), which Klappholz and Lockman, for example, call overweight structures that inflexibly dominate processing of text. Klappholz and Lockman emphasize that the structure through which reference is resolved must be dynamically built up as the text is processed; frames or scripts could assist in this building, but cannot, however, be reliably used for refer-ence resolution, because deviations by the text from the pre-defined structure will cause errors.

The basis of this approach is that there is a strong interrelationship between coreference and the cohesive ties in a discourse that make it coherent. By determining what the cohesive ties in a discourse are, one can put each new sentence or clause, as it comes in, into the appropriate place in a growing structure that represents the discourse. This structure can then be used as a focus to search for coreference antecedents, since not only do coherently connected sentences tend to refer to the same things, but knowledge of the cohesion relation can provide additional reference resolution restraints.  in particular sees the problem of coreference resolution as being automatically solved in the process of discovering the coherence relations in a text. (An example of this will be given in Section 6.2.) Conversely, it is frequently helpful or necessary to resolve coreference relations in order to discover the coherence relations. This is not a vicious circle, claims Hobbs, but a spiral staircase.

In our discussion below, we will cover four issues:

1. deciding on a set of possible coherence relations;

2. detecting them when they occur in a text;

3. using the coherence relations to build a focus structure; and 4. searching for referents in the structure.",How does the discourse-cohesion approach utilize cohesive ties to assist in coreference resolution?,"Another approach to coreference resolution attempts to exploit local discourse cohesion, building a representation of the discourse with which references can be resolved. By using only cues to the discourse structure at the sentence level or lower, one avoids the need to search for referents in pre-determined dialogue models or rigidly predefined knowledge structures. The structure through which reference is resolved must be dynamically built up as the text is processed. The basis of this approach is that there is a strong interrelationship between coreference and the cohesive ties in a discourse that make it coherent. By determining what the cohesive ties in a discourse are, one can put each new sentence or clause, as it comes in, into the appropriate place in a growing structure that represents the discourse. This structure can then be used as a focus to search for coreference antecedents, since not only do coherently connected sentences tend to refer to the same things, but knowledge of the cohesion relation can provide additional reference resolution restraints. The problem of coreference resolution is seen as being automatically solved in the process of discovering the coherence relations in a text. Conversely, it is frequently helpful or necessary to resolve coreference relations in order to discover the coherence relations.",How does the discourse-cohesion approach use cohesive ties to aid in coreference resolution?,"The discourse-cohesion approach to coreference resolution leverages local discourse cohesion by constructing a dynamic representation of the discourse to resolve references. This method focuses on cues to the discourse structure at the sentence level or lower, avoiding the need to search for referents in pre-determined dialogue models or rigidly predefined knowledge structures. The approach is based on the strong interrelationship between coreference and the cohesive ties that make a discourse coherent. By identifying these cohesive ties, each new sentence or clause can be appropriately placed into a growing structure that represents the discourse. This structure serves as a focal point for searching for coreference antecedents, as coherently connected sentences often refer to the same entities. Additionally, understanding the cohesion relations provides further constraints for reference resolution. In this approach, coreference resolution is inherently addressed through the process of discovering coherence relations in a text. Conversely, resolving coreference relations is often helpful or necessary for uncovering these coherence relations.",7,2,7,2,"Question:

How does the discourse-cohesion approach utilize cohesive ties to assist in coreference resolution?

Answer:

Another approach to coreference resolution attempts to exploit local discourse cohesion, building a representation of the discourse with which references can be resolved. By using only cues to the discourse structure at the sentence level or lower, one avoids the need to search for referents in pre-determined dialogue models or rigidly predefined knowledge structures. The structure through which reference is resolved must be dynamically built up as the text is processed. The basis of this approach is that there is a strong interrelationship between coreference and the cohesive ties in a discourse that make it coherent. By determining what the cohesive ties in a discourse are, one can put each new sentence or clause, as it comes in, into the appropriate place in a growing structure that represents the discourse. This structure can then be used as a focus to search for coreference antecedents, since not only do coherently connected sentences tend to refer to the same things, but knowledge of the cohesion relation can provide additional reference resolution restraints. The problem of coreference resolution is seen as being automatically solved in the process of discovering the coherence relations in a text. Conversely, it is frequently helpful or necessary to resolve coreference relations in order to discover the coherence relations.","Question:

How does the discourse-cohesion approach use cohesive ties to aid in coreference resolution?

Answer:

The discourse-cohesion approach to coreference resolution leverages local discourse cohesion by constructing a dynamic representation of the discourse to resolve references. This method focuses on cues to the discourse structure at the sentence level or lower, avoiding the need to search for referents in pre-determined dialogue models or rigidly predefined knowledge structures. The approach is based on the strong interrelationship between coreference and the cohesive ties that make a discourse coherent. By identifying these cohesive ties, each new sentence or clause can be appropriately placed into a growing structure that represents the discourse. This structure serves as a focal point for searching for coreference antecedents, as coherently connected sentences often refer to the same entities. Additionally, understanding the cohesion relations provides further constraints for reference resolution. In this approach, coreference resolution is inherently addressed through the process of discovering coherence relations in a text. Conversely, resolving coreference relations is often helpful or necessary for uncovering these coherence relations.",NO,True,1159,True,True
3176028-s17,Discourse-Oriented Anaphora Resolution in Natural Language Understanding: A Review,1981-04-01,Coherence relations,"The first thing required by this approach is a complete and computable set of the coherence relations that may obtain between sentences and/or clauses. Various sets have been suggested by many people, including Eisenstadt (1976), Phillips (1977), Pitkin (1977aPitkin ( , 1977b, Hirst (1977bHirst ( , 1978, Lockman (1978),  and Reichman (1978). 15 None of these sets fulfill all desiderata; and while Halliday and Hasan (1976) provide an extensive analysis of cohesion, it does not fit within our computational framework of coherence relations, and those, such as Hobbs, Lockman, Eisenstadt and Hirst, who emphasize computability, provide sets insufficient, I believe, to capture all the semantic subtleties of discourse cohesion. Nevertheless, the works cited above undoubtedly serve as a useful starting point for development of this area.

To illustrate what a very preliminary set of cohesion relations could look like, I will briefly present a set abstracted from the various sets of Eisenstadt, Hirst, Hobbs, Lockman and Phillips (but not faithful to any one of these).

The set contains two basic classes of coherence relations: expansion or elaboration on an entity, concept or event in the discourse, and temporal continuation or time flow. Expansion includes relations like EFFECT, CAUSE, SYLLOGISM, ELABORATION, CONTRAST, PARALLEL and EXEMPLIFICATION. In the following examples, ""u"" is used to indicate the point where the cohesive tie illustrated is acting: (One may disagree with my classification of some of the relations above; the boundaries between categories are yet ill-defined, and it is to be expected that some people's intuitions will differ from mine.)

Temporal flow relations involve some continuation forwards or backwards over time:

(6-8) VICTORIA --A suntanned Prince Charles arrived here Sunday afternoon, • and was greeted with a big kiss by a pretty English au pair girl. 17 (6-9) SAN JUAN, Puerto Rico --Travel officials tackled a major job here Sunday to find new accommodations for 650 passengers from the burned Italian cruise liner

Angelina Lauro.

• The vessel caught fire Friday while docked at Charlotte Amalie in the Virgin Islands, but most passengers were ashore at the time. 18

Temporal flow may be treated as a single relation, as Phillips, for example, does, or it may be subdivided, as by Eisenstadt and Hirst, into categories like TIME STEP, FLASHBACK, FLASHFORWARD, TIME EDIT, and so on. Certainly, time flow in a text may be quite contorted, as in (6-10) (from Hirst 1978); ""m"" indicates a point where the direction of the time flow changes:

(6-10) Slowly, hesitantly, Ross approached Nadia. • He had waited for this moment for many days. • Now he was going to say the words • which he had agonized over • and in the very room • he had often dreamed about. • He gazed lovingly at her soft green eyes.

It is not clear, however, to what extent an analysis of time flow is necessary for anaphor resolution. I suspect that relatively little is necessary --less than is required for other aspects of discourse understanding.

I see relations like those exemplified above as primitives from which more complex relations could be built. For example, the relation between the two sentences of (6-3) above clearly involves FORWARD TIME STEP as well as EFFECT. I have hypothesized elsewhere (Hirst 1978) the possibility of constructing a small set of discourse relations (with cardinality about twenty or less) from which more complex relations may be built up by simple combination, and, one hopes, in such a way that the effects of relation Ri+R 2 would be the sum of the individual effects of relations R 1 and R 2. Rules for permitted combinations would be needed; for example, FORWARD TIME STEP could combine with EFFECT, but not with BACKWARD TIME STEP. What would the formal definition of a coherence relation be like? Here is Hobbs's (1979:73) definition of ELABORATION: Sentence S 1 is an ELABORATION of sentence S O if some proposition P follows from the assertions of both S O and $1, but S 1 contains a property of one of the elements of P that is not in S 0. The example in the next section will clarify this.","What are the two basic classes of coherence relations, and how are they exemplified in discourse?","The set contains two basic classes of coherence relations: expansion or elaboration on an entity, concept or event in the discourse, and temporal continuation or time flow. Expansion includes relations like EFFECT, CAUSE, SYLLOGISM, ELABORATION, CONTRAST, PARALLEL, and EXEMPLIFICATION. Temporal flow relations involve some continuation forwards or backwards over time.","What are the two basic classes of coherence relations, and how are they exemplified in discourse?","Coherence relations in discourse can be broadly categorized into two basic classes: expansion and temporal continuation. Expansion involves elaborating on an entity, concept, or event within the discourse. This class includes relations such as EFFECT, CAUSE, SYLLOGISM, ELABORATION, CONTRAST, PARALLEL, and EXEMPLIFICATION. On the other hand, temporal continuation pertains to the flow of time within the discourse, involving a progression either forwards or backwards.",9,2,9,2,"Question:

What are the two basic classes of coherence relations, and how are they exemplified in discourse?

Answer:

The set contains two basic classes of coherence relations: expansion or elaboration on an entity, concept or event in the discourse, and temporal continuation or time flow. Expansion includes relations like EFFECT, CAUSE, SYLLOGISM, ELABORATION, CONTRAST, PARALLEL, and EXEMPLIFICATION. Temporal flow relations involve some continuation forwards or backwards over time.","Question:

What are the two basic classes of coherence relations, and how are they exemplified in discourse?

Answer:

Coherence relations in discourse can be broadly categorized into two basic classes: expansion and temporal continuation. Expansion involves elaborating on an entity, concept, or event within the discourse. This class includes relations such as EFFECT, CAUSE, SYLLOGISM, ELABORATION, CONTRAST, PARALLEL, and EXEMPLIFICATION. On the other hand, temporal continuation pertains to the flow of time within the discourse, involving a progression either forwards or backwards.",NO,True,469,True,True
234093015-s10,A Survey of Data Augmentation Approaches for NLP,2021-05-07,Tasks,"In this section, we discuss several DA works for common NLP tasks. 2 We focus on nonclassification tasks as classification is worked on by default, and well covered in earlier sections (e.g.

§3 and §4 which substitutes a portion of the input text with its translation in another language, improving performance across multiple languages on NLI tasks including the SQuAD QA task. Asai and Hajishirzi (2020) use logical and linguistic knowledge to generate additional training data to improve the accuracy and consistency of QA responses by models. Yu et al. (2018) introduce a new QA architecture called QANet that shows improved performance on SQuAD when combined with augmented data generated using backtranslation. Dai and Adel (2020) modify DA techniques proposed for sentence-level tasks for named entity recognition (NER), including label-wise token and synonym replacement, and show improved performance using both recurrent and transformer models. Zhang et al. (2020) propose a DA method based on MIXUP called SEQMIX for active sequence labeling by augmenting queried samples, showing improvements on NER and Event Detection.",How have data augmentation techniques been adapted for named entity recognition tasks to improve model performance?,"Dai and Adel (2020) modify data augmentation techniques proposed for sentence-level tasks for named entity recognition (NER), including label-wise token and synonym replacement, and show improved performance using both recurrent and transformer models. Zhang et al. (2020) propose a data augmentation method based on MIXUP called SEQMIX for active sequence labeling by augmenting queried samples, showing improvements on NER and Event Detection.",How have data augmentation techniques been adapted for named entity recognition tasks to enhance model performance?,"Data augmentation techniques have been effectively adapted for named entity recognition (NER) tasks to boost model performance. Dai and Adel (2020) have modified existing data augmentation methods, originally designed for sentence-level tasks, to suit NER. Their approach includes label-wise token and synonym replacement, which has shown to improve performance in both recurrent and transformer models. Additionally, Zhang et al. (2020) introduced a data augmentation method called SEQMIX, based on the MIXUP technique, specifically for active sequence labeling. This method involves augmenting queried samples and has demonstrated improvements in NER and Event Detection tasks.",8,4,8,4,"Question:

How have data augmentation techniques been adapted for named entity recognition tasks to improve model performance?

Answer:

Dai and Adel (2020) modify data augmentation techniques proposed for sentence-level tasks for named entity recognition (NER), including label-wise token and synonym replacement, and show improved performance using both recurrent and transformer models. Zhang et al. (2020) propose a data augmentation method based on MIXUP called SEQMIX for active sequence labeling by augmenting queried samples, showing improvements on NER and Event Detection.","Question:

How have data augmentation techniques been adapted for named entity recognition tasks to enhance model performance?

Answer:

Data augmentation techniques have been effectively adapted for named entity recognition (NER) tasks to boost model performance. Dai and Adel (2020) have modified existing data augmentation methods, originally designed for sentence-level tasks, to suit NER. Their approach includes label-wise token and synonym replacement, which has shown to improve performance in both recurrent and transformer models. Additionally, Zhang et al. (2020) introduced a data augmentation method called SEQMIX, based on the MIXUP technique, specifically for active sequence labeling. This method involves augmenting queried samples and has demonstrated improvements in NER and Event Detection tasks.",NO,True,679,True,True
235790370-s12,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,2021-07-08,Generative Environments,"A difficulty with statically-initialized environments is that because their structure is identical each time the simulation is run, rather than learning general skills, agents quickly overfit to a particular task and environment, and rarely generalize to unseen environments (Chaudhury et al., 2020). Procedurally generated environments help address this need by generating variations of environments centered around specific goal conditions. The TextWorld simulator (Côté et al., 2018) allows specifying high-level parameters such as the number of rooms, objects, and winning conditions, then uses a random walk to procedurally generate environment maps in the Inform7 language meeting those specifications, using either forward or backward chaining during generation to verify tasks can be successfully completed in the random environment. As an example, the First TextWorld Problems shared task 2 used TextWorld to generate 5k variations of a cooking environment, divided into train, development, and test sets. Similarly, Murugesan et al. (2020a) introduce TextWorld Com-monSense (TWC), a simple generative environment for household cleaning tasks, modelled as a pick-and-place task where agents must pick up common objects from the floor, and place them in their common household locations (such as placing shoes in a shoe cabinet). Other related environments include Coin Collector (Yuan et al., 2018), a generative environment for a navigation task, and Yin et al.'s (2019b) procedurally generated environment for cooking tasks. Adhikari et al. (2020) generate a large set of recipe-based cooking games, where an agent must precisely follow a cooking recipe that requires collecting tools (e.g. a knife) and ingredients (e.g. carrots), and processing those ingredients correctly (e.g. dice carrots, cook carrots) in the correct order. Jain et al. (2020) propose a similar synthetic benchmark for multi-step compositional reasoning called SaladWorld. In the context of question answering,  procedurally generate a simple environment that requires an agent to search and investigate attributes of objects, such as verifying their existence, locations, or specific attributes (like edibility). On the balance, while tooling exists to generate simple procedural environments, when compared to classic interactive fiction games (such as Zork), the current state-of-the-art allows for generating only relatively simple environments with comparatively simple tasks and near-term goals than human-authored interactive fiction games.",How do procedurally generated environments improve generalization in simulations compared to statically-initialized environments?,"A difficulty with statically-initialized environments is that because their structure is identical each time the simulation is run, rather than learning general skills, agents quickly overfit to a particular task and environment, and rarely generalize to unseen environments (Chaudhury et al., 2020). Procedurally generated environments help address this need by generating variations of environments centered around specific goal conditions. The TextWorld simulator (Côté et al., 2018) allows specifying high-level parameters such as the number of rooms, objects, and winning conditions, then uses a random walk to procedurally generate environment maps in the Inform7 language meeting those specifications, using either forward or backward chaining during generation to verify tasks can be successfully completed in the random environment.","How do procedurally generated environments enhance the ability of simulations to generalize, compared to environments that are statically initialized?","Statically-initialized environments present a challenge because their structure remains the same each time the simulation is run. This consistency leads agents to quickly overfit to specific tasks and environments, making it difficult for them to generalize to new, unseen environments (Chaudhury et al., 2020). In contrast, procedurally generated environments address this issue by creating variations of environments that focus on specific goal conditions. For instance, the TextWorld simulator (Côté et al., 2018) allows users to specify high-level parameters such as the number of rooms, objects, and winning conditions. It then employs a random walk to procedurally generate environment maps in the Inform7 language that meet these specifications. During this generation process, either forward or backward chaining is used to ensure that tasks can be successfully completed in the randomly generated environment.",7,4,7,6,"Question:

How do procedurally generated environments improve generalization in simulations compared to statically-initialized environments?

Answer:

A difficulty with statically-initialized environments is that because their structure is identical each time the simulation is run, rather than learning general skills, agents quickly overfit to a particular task and environment, and rarely generalize to unseen environments (Chaudhury et al., 2020). Procedurally generated environments help address this need by generating variations of environments centered around specific goal conditions. The TextWorld simulator (Côté et al., 2018) allows specifying high-level parameters such as the number of rooms, objects, and winning conditions, then uses a random walk to procedurally generate environment maps in the Inform7 language meeting those specifications, using either forward or backward chaining during generation to verify tasks can be successfully completed in the random environment.","Question:

How do procedurally generated environments enhance the ability of simulations to generalize, compared to environments that are statically initialized?

Answer:

Statically-initialized environments present a challenge because their structure remains the same each time the simulation is run. This consistency leads agents to quickly overfit to specific tasks and environments, making it difficult for them to generalize to new, unseen environments (Chaudhury et al., 2020). In contrast, procedurally generated environments address this issue by creating variations of environments that focus on specific goal conditions. For instance, the TextWorld simulator (Côté et al., 2018) allows users to specify high-level parameters such as the number of rooms, objects, and winning conditions. It then employs a random walk to procedurally generate environment maps in the Inform7 language that meet these specifications. During this generation process, either forward or backward chaining is used to ensure that tasks can be successfully completed in the randomly generated environment.",NO,True,918,True,True
236460241-s2,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,2023-01-05,"Code-switching, Borrowing, Transfer, Loan Translation","While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009). 4.İlkokul-uİstanbul-da yap-tı-m.

primary.school-ACCİstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher après 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","How do loan translation and transfer differ, and what examples illustrate these linguistic phenomena?  ","Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In loan translation, an example is provided where the Turkish verb ""yapmak,"" meaning 'to do,' takes on the Dutch meaning of ""doen"" in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009). In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction ""chercher après"" 'look after' (for 'look for') is a translation of the Dutch equivalent, and in Ontario French (Canada), ""chercher pour"" is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb.","What are the differences between loan translation and transfer in linguistics, and can you provide examples of these phenomena?","Loan translation and transfer are linguistic phenomena that involve the interaction between two languages, where one language influences the semantics or grammatical constructions of the other. In loan translation, the meaning of a word or phrase is directly translated from one language to another. For example, in Turkish spoken in the Netherlands, the Turkish verb ""yapmak,"" which means 'to do,' adopts the Dutch meaning of ""doen"" (Dogruöz and Backus, 2009).

On the other hand, transfer involves borrowing grammatical constructions from one language to another without borrowing the actual words. An example of this is the transfer of verb particles from Germanic languages into French. Treffers-Daller (2012) illustrates this with Brussels French (Belgium), where the construction ""chercher après"" ('look after') is used instead of the standard French 'look for,' mirroring the Dutch equivalent. Similarly, in Ontario French (Canada), ""chercher pour"" is used as a translation equivalent of the English 'look for.' In standard French (France), there is typically no particle following the verb.",9,4,9,2,"Question:

How do loan translation and transfer differ, and what examples illustrate these linguistic phenomena?  

Answer:

Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In loan translation, an example is provided where the Turkish verb ""yapmak,"" meaning 'to do,' takes on the Dutch meaning of ""doen"" in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009). In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction ""chercher après"" 'look after' (for 'look for') is a translation of the Dutch equivalent, and in Ontario French (Canada), ""chercher pour"" is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb.","Question:

What are the differences between loan translation and transfer in linguistics, and can you provide examples of these phenomena?

Answer:

Loan translation and transfer are linguistic phenomena that involve the interaction between two languages, where one language influences the semantics or grammatical constructions of the other. In loan translation, the meaning of a word or phrase is directly translated from one language to another. For example, in Turkish spoken in the Netherlands, the Turkish verb ""yapmak,"" which means 'to do,' adopts the Dutch meaning of ""doen"" (Dogruöz and Backus, 2009).

On the other hand, transfer involves borrowing grammatical constructions from one language to another without borrowing the actual words. An example of this is the transfer of verb particles from Germanic languages into French. Treffers-Daller (2012) illustrates this with Brussels French (Belgium), where the construction ""chercher après"" ('look after') is used instead of the standard French 'look for,' mirroring the Dutch equivalent. Similarly, in Ontario French (Canada), ""chercher pour"" is used as a translation equivalent of the English 'look for.' In standard French (France), there is typically no particle following the verb.",NO,True,1098,True,True
11250379-s1,A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin,1996-08-01,Continuous Speech Recognition of Mandarin,"There have been many attempts to find so called distinctive features of speech (e.g. [Fant 1973]) which are invariant to a number of factors. Certain distinctive (phonetic) features, such as nasality and voicing, can be used to represent the place and manner of articulation of speech sounds so that speech can be uniquely identified by detecting the acoustic-phonetic properties of the signal. By organizing such knowledge in a systematic manner, speech recognition can (in theory) be performed by first identifying and labeling the sequence of feature vectors and then identifying the corresponding sounds in the speech signal, followed by decoding the corresponding sequence of words using lexical access to a dictionary of words. This has been demonstrated in spectrogram reading by a human expert who can visually segment and identify some speech sounds based on knowledge of acoustic-phonetics of English. Although the collection of distinctive features, in theory, offers a set of invariant features for speech recognition, it is not generally used in most speech recognitions systems. This is due to the fact that the set of distinctive features are usually difficult to identify in spontaneous continuous speech and the recognition results are generally unreliable.

A more successful approach to automatic speech recognition is to treat the speech signal as a stochastic pattern and to adopt a statistical pattern recognition approach. For this approach we assume a source-channel speech generation model (e.g. [Bahl et al. 1983]) shown in Figure  1, in which the source produces a sequence of words, W. Because of uncertainty and inaccuracy in converting from words to speech, we model the conversion from W to an observed speech waveform, S, as a noisy channel. Speech recognition is then formulated as a maximum a posteriori (MAP) decoding problem, as shown in Figure 1. Instead of working with the speech signal S directly, one way to simplify the problem is to assume that S is first parametrically represented as a sequence of acoustic vectors A. We then use the Bayes rule to reformulate the decoding problemas follows, arg max ( | ) arg max ( | ) ( )

where Γ is the set of all possible sequences of words, P(A|W)is the conditional probability of the acoustic vector sequence, A, given a particular sequence of words W, and P (W) is the a priori probability of generating the sequence of words W. The first term, P (A|W), is often referred to as an acoustic model, and the second term, P (W), is known as a language model. The noisy channel in Figure 1 is a model jointly characterizing the speech production system, the speaker variability, the speaking environment, and the transmission medium. Since it is not feasible to have a complete knowledge about such a noisy channel, the statistical approach often assumes particular parametric forms for Pθ (A|W) and Pω (W), i.e. according to specific models. All the parameters of the statistical models (i.e. θ and ω ) needed in evaluating the acoustic probability, Pθ (W|A), and the language probability, Pω(W), are usually estimated from a large collection (the so-called training set) of speech and text training data. This process is often referred to as model training or learning. We will discuss this important issue later in the paper.

There is some recent attempt trying to separate the speech production part from the source-channel model by incorporating knowledge about the human speech production mechanism. Knowledge about the transducers used for capturing speech and the channel used for transmitting speech can also be explicitly modeled. However, the effectiveness of such approaches is yet to be shown.","How is the statistical pattern recognition approach applied to automatic speech recognition, and what are its key components?","A more successful approach to automatic speech recognition is to treat the speech signal as a stochastic pattern and to adopt a statistical pattern recognition approach. For this approach, a source-channel speech generation model is assumed, in which the source produces a sequence of words, W. Because of uncertainty and inaccuracy in converting from words to speech, the conversion from W to an observed speech waveform, S, is modeled as a noisy channel. Speech recognition is then formulated as a maximum a posteriori (MAP) decoding problem. Instead of working with the speech signal S directly, one way to simplify the problem is to assume that S is first parametrically represented as a sequence of acoustic vectors A. The Bayes rule is used to reformulate the decoding problem as follows: arg max ( | ) arg max ( | ) ( ) where Γ is the set of all possible sequences of words, P(A|W) is the conditional probability of the acoustic vector sequence, A, given a particular sequence of words W, and P(W) is the a priori probability of generating the sequence of words W. The first term, P(A|W), is often referred to as an acoustic model, and the second term, P(W), is known as a language model. The noisy channel is a model jointly characterizing the speech production system, the speaker variability, the speaking environment, and the transmission medium. Since it is not feasible to have complete knowledge about such a noisy channel, the statistical approach often assumes particular parametric forms for Pθ(A|W) and Pω(W), according to specific models. All the parameters of the statistical models needed in evaluating the acoustic probability, Pθ(W|A), and the language probability, Pω(W), are usually estimated from a large collection of speech and text training data. This process is often referred to as model training or learning.","How is the statistical pattern recognition approach utilized in automatic speech recognition, and what are its essential components?","In automatic speech recognition, a more effective method is to view the speech signal as a stochastic pattern and apply a statistical pattern recognition approach. This involves assuming a source-channel speech generation model, where the source generates a sequence of words, W. Due to the inherent uncertainty and inaccuracy in converting words to speech, the transformation from W to an observed speech waveform, S, is modeled as a noisy channel. Consequently, speech recognition is framed as a maximum a posteriori (MAP) decoding problem.

To simplify the problem, instead of directly working with the speech signal S, it is often assumed that S is first represented parametrically as a sequence of acoustic vectors, A. Using Bayes' rule, the decoding problem is reformulated as: arg max ( | ) arg max ( | ) ( ), where Γ represents the set of all possible sequences of words. Here, P(A|W) denotes the conditional probability of the acoustic vector sequence A given a specific sequence of words W, and P(W) signifies the a priori probability of generating the sequence of words W. The term P(A|W) is commonly referred to as the acoustic model, while P(W) is known as the language model.

The noisy channel model jointly characterizes the speech production system, speaker variability, speaking environment, and transmission medium. Given the impracticality of having complete knowledge about such a noisy channel, the statistical approach typically assumes specific parametric forms for Pθ(A|W) and Pω(W) based on particular models. The parameters of these statistical models, necessary for evaluating the acoustic probability Pθ(W|A) and the language probability Pω(W), are generally estimated from extensive collections of speech and text training data. This process is often called model training or learning.",8,2,8,2,"Question:

How is the statistical pattern recognition approach applied to automatic speech recognition, and what are its key components?

Answer:

A more successful approach to automatic speech recognition is to treat the speech signal as a stochastic pattern and to adopt a statistical pattern recognition approach. For this approach, a source-channel speech generation model is assumed, in which the source produces a sequence of words, W. Because of uncertainty and inaccuracy in converting from words to speech, the conversion from W to an observed speech waveform, S, is modeled as a noisy channel. Speech recognition is then formulated as a maximum a posteriori (MAP) decoding problem. Instead of working with the speech signal S directly, one way to simplify the problem is to assume that S is first parametrically represented as a sequence of acoustic vectors A. The Bayes rule is used to reformulate the decoding problem as follows: arg max ( | ) arg max ( | ) ( ) where Γ is the set of all possible sequences of words, P(A|W) is the conditional probability of the acoustic vector sequence, A, given a particular sequence of words W, and P(W) is the a priori probability of generating the sequence of words W. The first term, P(A|W), is often referred to as an acoustic model, and the second term, P(W), is known as a language model. The noisy channel is a model jointly characterizing the speech production system, the speaker variability, the speaking environment, and the transmission medium. Since it is not feasible to have complete knowledge about such a noisy channel, the statistical approach often assumes particular parametric forms for Pθ(A|W) and Pω(W), according to specific models. All the parameters of the statistical models needed in evaluating the acoustic probability, Pθ(W|A), and the language probability, Pω(W), are usually estimated from a large collection of speech and text training data. This process is often referred to as model training or learning.","Question:

How is the statistical pattern recognition approach utilized in automatic speech recognition, and what are its essential components?

Answer:

In automatic speech recognition, a more effective method is to view the speech signal as a stochastic pattern and apply a statistical pattern recognition approach. This involves assuming a source-channel speech generation model, where the source generates a sequence of words, W. Due to the inherent uncertainty and inaccuracy in converting words to speech, the transformation from W to an observed speech waveform, S, is modeled as a noisy channel. Consequently, speech recognition is framed as a maximum a posteriori (MAP) decoding problem.

To simplify the problem, instead of directly working with the speech signal S, it is often assumed that S is first represented parametrically as a sequence of acoustic vectors, A. Using Bayes' rule, the decoding problem is reformulated as: arg max ( | ) arg max ( | ) ( ), where Γ represents the set of all possible sequences of words. Here, P(A|W) denotes the conditional probability of the acoustic vector sequence A given a specific sequence of words W, and P(W) signifies the a priori probability of generating the sequence of words W. The term P(A|W) is commonly referred to as the acoustic model, while P(W) is known as the language model.

The noisy channel model jointly characterizes the speech production system, speaker variability, speaking environment, and transmission medium. Given the impracticality of having complete knowledge about such a noisy channel, the statistical approach typically assumes specific parametric forms for Pθ(A|W) and Pω(W) based on particular models. The parameters of these statistical models, necessary for evaluating the acoustic probability Pθ(W|A) and the language probability Pω(W), are generally estimated from extensive collections of speech and text training data. This process is often called model training or learning.",NO,True,1815,True,True
11250379-s8,A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin,1996-08-01,Continuous Speech Recognition of Mandarin,"• Units Other than Phones: Units smaller than a phone, such as phone-labeled acoustic states, have been used to reduce the number of states needed to represent the set of speech units. Larger units, including diphones, demisyllables, syllables, whole-words and even phrases, have all been used to better characterize coarticulation between adjacent sounds. Acoustic segment units have also been investigated [Lee 1988].

• Units with Linguistic Context Dependency: Different ways of incorporting linguistic context in a speech subword unit, such as double context dependent phones (often known as triphones) and generalized triphones, have been proposed (e.g. [Lee 1989]). It has been shown that the recognition accuracy of a task can be increased when linguistic context dependency is properly incorporated to reduce the acoustic variability of the speech units being modeled. In fluent continuous speech it has also been shown that incorporation of interword units takes into account cross-word coarticulation and therefore provides more accurate modeling of speech units than simply using intraword context-dependent units. Word-dependent units have also been used to model poorly articulated speech sounds such as function words like a, the, in, and, etc. (e.g. [Lee 1989]).

For a given task, high recognition accuracy can be achieved only when the subword unit set contains context-dependent phones which maximally covers the vocabulary and the task language and when these phone units are adequately modeled using a large training set [Hon 1992]. However, the collection of a large amount of task-specific training data for every individual application is not practical. Task and vocabulary independent acoustic training and task-specific vocabulary learning (e.g. [Hon 1992;Lee et al.1996]) are therefore important research topics. Task-independent modeling has also been applied to word spotting for training of acoustic models and rejection models [Rose and Hofstetter 1993;Sukkar and Lee 1996]. However, we do not yet know how to design a task-independent training database suitable for a wide range of vocabularies and applications.",How is linguistic context dependency utilized to improve recognition accuracy in continuous speech recognition of Mandarin?,"Different ways of incorporating linguistic context in a speech subword unit, such as double context dependent phones (often known as triphones) and generalized triphones, have been proposed (e.g. [Lee 1989]). It has been shown that the recognition accuracy of a task can be increased when linguistic context dependency is properly incorporated to reduce the acoustic variability of the speech units being modeled. In fluent continuous speech, it has also been shown that incorporation of interword units takes into account cross-word coarticulation and therefore provides more accurate modeling of speech units than simply using intraword context-dependent units. Word-dependent units have also been used to model poorly articulated speech sounds such as function words like a, the, in, and, etc. (e.g. [Lee 1989]).",How does incorporating linguistic context dependency enhance recognition accuracy in continuous Mandarin speech recognition?,"Incorporating linguistic context dependency in speech recognition involves using techniques such as double context-dependent phones, commonly known as triphones, and generalized triphones, as proposed by researchers like Lee (1989). Proper integration of linguistic context helps reduce the acoustic variability of speech units, thereby improving recognition accuracy. In fluent continuous speech, using interword units, which account for cross-word coarticulation, provides more accurate modeling of speech units compared to solely relying on intraword context-dependent units. Additionally, word-dependent units are employed to model poorly articulated speech sounds, such as function words like ""a,"" ""the,"" ""in,"" and ""and,"" as noted by Lee (1989).",7,4,8,2,"Question:

How is linguistic context dependency utilized to improve recognition accuracy in continuous speech recognition of Mandarin?

Answer:

Different ways of incorporating linguistic context in a speech subword unit, such as double context dependent phones (often known as triphones) and generalized triphones, have been proposed (e.g. [Lee 1989]). It has been shown that the recognition accuracy of a task can be increased when linguistic context dependency is properly incorporated to reduce the acoustic variability of the speech units being modeled. In fluent continuous speech, it has also been shown that incorporation of interword units takes into account cross-word coarticulation and therefore provides more accurate modeling of speech units than simply using intraword context-dependent units. Word-dependent units have also been used to model poorly articulated speech sounds such as function words like a, the, in, and, etc. (e.g. [Lee 1989]).","Question:

How does incorporating linguistic context dependency enhance recognition accuracy in continuous Mandarin speech recognition?

Answer:

Incorporating linguistic context dependency in speech recognition involves using techniques such as double context-dependent phones, commonly known as triphones, and generalized triphones, as proposed by researchers like Lee (1989). Proper integration of linguistic context helps reduce the acoustic variability of speech units, thereby improving recognition accuracy. In fluent continuous speech, using interword units, which account for cross-word coarticulation, provides more accurate modeling of speech units compared to solely relying on intraword context-dependent units. Additionally, word-dependent units are employed to model poorly articulated speech sounds, such as function words like ""a,"" ""the,"" ""in,"" and ""and,"" as noted by Lee (1989).",NO,True,750,True,True
11250379-s9,A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin,1996-08-01,Acoustic Modeling of Speech Units,"Training of subword unit models consists of estimating the model parameters from a training set of continuous speech utterances in which all of the relevant subword units are known to occur 'sufficiently' often. The way in which training is performed greatly affects the overall recognition system performance. A key issue in training is the size of the training set. Since infinite size training sets are impossible to obtain (and computationally unmanageable), we must use a finite size training set. This immediately implies that some subword units may not occur as often as others. Hence there is a tradeoff between using fewer subword units (where we get good coverage of individual units, but poor resolution of linguistic context), and more subword units (where we get poor coverage of the infrequently occurring units, but good resolution of linguistic context).

An alternative to using a large training set is to start with some initial set of subword unit models and adapt the models over time (with new training material, possibly derived from actual test utterances) to the task, the speaker and/or the environment. Such methods of adaptive training are usable for new speakers, tasks and environments, and provide an effective way of creating a good set of application-specific models from a more general set of models (which are speaker, environment, task, and context independent).

Speech patterns not only exhibit highly variable spectral properties but also show considerable temporal variation. There are not many modeling approaches that are both mathematically well-defined and computationally tractable, for modeling the speech signal. The most widely used and the most successful modeling approach to speech recognition is the use of hidden Markov models (HMMs). The reader is referred to a tutorial by Rabiner [1989] for an introduction to the HMM approach and its applications. Artificial neural network (ANN) approaches have also been used to provide an alternative modeling framework and a new computing paradigm [Bourlard and Wellekens 1992;Bourlard and Morgan 1994;Robinson 1994]. Almost all modern speech recognition systems use hidden Markov models and their extensions to model speech units. We will give a more detailed description of the HMM framework in the next section.",What are the benefits and challenges of using hidden Markov models in speech recognition systems?,"The most widely used and the most successful modeling approach to speech recognition is the use of hidden Markov models (HMMs). Almost all modern speech recognition systems use hidden Markov models and their extensions to model speech units. A key benefit of using HMMs is that they provide a mathematically well-defined and computationally tractable approach for modeling the speech signal. However, a challenge in using HMMs is that speech patterns exhibit highly variable spectral properties and considerable temporal variation, which can complicate the modeling process.",What are the advantages and difficulties associated with using hidden Markov models in speech recognition systems?,"Hidden Markov models (HMMs) are the most widely used and successful approach for modeling speech recognition systems. Almost all modern systems rely on HMMs and their extensions to effectively model speech units. One of the key advantages of using HMMs is that they offer a mathematically well-defined and computationally feasible method for representing the speech signal. However, a significant challenge in employing HMMs is the inherent variability in speech patterns, which exhibit highly variable spectral properties and considerable temporal variation. This variability can complicate the modeling process.",8,4,8,4,"Question:

What are the benefits and challenges of using hidden Markov models in speech recognition systems?

Answer:

The most widely used and the most successful modeling approach to speech recognition is the use of hidden Markov models (HMMs). Almost all modern speech recognition systems use hidden Markov models and their extensions to model speech units. A key benefit of using HMMs is that they provide a mathematically well-defined and computationally tractable approach for modeling the speech signal. However, a challenge in using HMMs is that speech patterns exhibit highly variable spectral properties and considerable temporal variation, which can complicate the modeling process.","Question:

What are the advantages and difficulties associated with using hidden Markov models in speech recognition systems?

Answer:

Hidden Markov models (HMMs) are the most widely used and successful approach for modeling speech recognition systems. Almost all modern systems rely on HMMs and their extensions to effectively model speech units. One of the key advantages of using HMMs is that they offer a mathematically well-defined and computationally feasible method for representing the speech signal. However, a significant challenge in employing HMMs is the inherent variability in speech patterns, which exhibit highly variable spectral properties and considerable temporal variation. This variability can complicate the modeling process.",NO,True,613,True,True
11250379-s19,A Survey on Automatic Speech Recognition with an Illustrative Example on Continuous Speech Recognition of Mandarin,1996-08-01,Continuous Speech Recognition of Mandarin,"In the following we illustrate, by example, how the pattern recognition approach we discussed above can be used to put together an integrated continuous speech recognition system for Mandarin. The algorithm described here was first developed for recognition of American English . The same strategy was also applied to recognition of Spanish [Alvarez-Cercardillo et al. 1995] and Mandarin Chinese as shown here. Only the language-dependent part of the algorithm was modified to deal with the lexical and language constraints of different languages. Although the selection of an appropriate set of fundamental speech units for recognition is a crucial part, we assume only little knowledge about the phonology of a particular language in our studies. It is important to note that the examples shown here are meant to explore new research dimensions in Mandarin recognition. It is not our intention to get the best recognition results for any of the tasks, discussed here. Therefore no system parameter tuning was attempted in all of our experiments. Although the tonal property is a unique feature of Mandarin (as well as in some other Oriental languages), we do not address the issue in this paper. We only consider Mandarin base syllables, i.e. we do not distinguish syllables with only tone differences. The readers are referred to a number of studies for tone properties of Mandarin (e.g. [Tseng 1981]), tone recognition (e.g. [Wang and Lee 1994;Wang and Chen 1994]), and combined tone and syllable recognition (e.g. [Lin et al. 1993]).

For some historical and practical reasons, most of the existing Mandarin recognition systems developed in Taiwan and Mainland China are syllable based. Although modeling of sub-syllabic units has been attempted recently, most systems still perform syllable recognition in terms of a syllable lattice followed by lexical and language processing to determine the recognized sentence (e.g. Chiang, Lin and Su 1996]). Because real-time system implementation has always been a major design factor in developing Mandarin recognition systems, this decoupled approach was largely adopted to avoid the search complexity implied in integrated approaches. Although some success has been reported for speaker-trained recognition, large-scale, speaker-independent Mandarin recognition has not been widely studied. Recall in our discussion above that due to error propagation from one stage to the next, decoupled approaches often degrade in performance more drastically than that in integrated approaches when inadequate intermediate search theories are saved for further processing. This is of major concern especially when testing in adverse conditions.",What are the challenges and limitations of using decoupled approaches in Mandarin speech recognition systems?,"Decoupled approaches in Mandarin speech recognition systems often degrade in performance more drastically than integrated approaches when inadequate intermediate search theories are saved for further processing. This is of major concern especially when testing in adverse conditions. Real-time system implementation has always been a major design factor in developing Mandarin recognition systems, and the decoupled approach was largely adopted to avoid the search complexity implied in integrated approaches. Although some success has been reported for speaker-trained recognition, large-scale, speaker-independent Mandarin recognition has not been widely studied.",What challenges and limitations are associated with using decoupled approaches in Mandarin speech recognition systems?,"Decoupled approaches in Mandarin speech recognition systems face several challenges and limitations. One significant issue is that these systems tend to experience a more pronounced degradation in performance compared to integrated approaches, particularly when inadequate intermediate search theories are preserved for further processing. This concern becomes especially critical when testing under adverse conditions. Real-time system implementation has consistently been a crucial design factor in the development of Mandarin recognition systems. The decoupled approach has been largely adopted to mitigate the search complexity inherent in integrated approaches. While there has been some success reported in speaker-trained recognition, large-scale, speaker-independent Mandarin recognition remains an area that has not been extensively explored.",7,4,7,4,"Question:

What are the challenges and limitations of using decoupled approaches in Mandarin speech recognition systems?

Answer:

Decoupled approaches in Mandarin speech recognition systems often degrade in performance more drastically than integrated approaches when inadequate intermediate search theories are saved for further processing. This is of major concern especially when testing in adverse conditions. Real-time system implementation has always been a major design factor in developing Mandarin recognition systems, and the decoupled approach was largely adopted to avoid the search complexity implied in integrated approaches. Although some success has been reported for speaker-trained recognition, large-scale, speaker-independent Mandarin recognition has not been widely studied.","Question:

What challenges and limitations are associated with using decoupled approaches in Mandarin speech recognition systems?

Answer:

Decoupled approaches in Mandarin speech recognition systems face several challenges and limitations. One significant issue is that these systems tend to experience a more pronounced degradation in performance compared to integrated approaches, particularly when inadequate intermediate search theories are preserved for further processing. This concern becomes especially critical when testing under adverse conditions. Real-time system implementation has consistently been a crucial design factor in the development of Mandarin recognition systems. The decoupled approach has been largely adopted to mitigate the search complexity inherent in integrated approaches. While there has been some success reported in speaker-trained recognition, large-scale, speaker-independent Mandarin recognition remains an area that has not been extensively explored.",NO,True,851,True,True
264833196-s13,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,2023-11-01,Transformer encoder-decoders,"A transformer encoder-decoder combines a transformer encoder and decoder, adding to each layer of the decoder an additional attention sublayer, known as cross attention, which attends to the output of the encoder.

In the literature surveyed here, only the construction of Pérez et al. (2021) and related constructions (Bhattamishra et al., 2020b;Wei et al., 2022) employ an encoder-decoder architecture.In these constructions, a string  is fed to the encoder, and the decoder is allowed to run for an arbitrary number of steps.Then  is accepted iff the decoder eventually outputs a vector belonging to a fixed set of accept vectors.

As we will see ( §7.2.1), this setup vastly increases the model's power.It could be likened to a language model that is allowed to ""think step by step"" (Kojima et al., 2022) before generating a final accept decision.",How does the encoder-decoder architecture enhance the processing and acceptance of strings in formal language recognition?,"A transformer encoder-decoder combines a transformer encoder and decoder, adding to each layer of the decoder an additional attention sublayer, known as cross attention, which attends to the output of the encoder. In the constructions of Pérez et al. (2021) and related works (Bhattamishra et al., 2020b; Wei et al., 2022), a string is fed to the encoder, and the decoder is allowed to run for an arbitrary number of steps. The string is accepted if and only if the decoder eventually outputs a vector belonging to a fixed set of accept vectors. This setup vastly increases the model's power, as it could be likened to a language model that is allowed to ""think step by step"" (Kojima et al., 2022) before generating a final accept decision.",How does the encoder-decoder architecture improve the processing and acceptance of strings in formal language recognition?,"The encoder-decoder architecture, particularly when implemented using transformers, enhances the processing and acceptance of strings in formal language recognition by integrating a transformer encoder and decoder. Each layer of the decoder includes an additional attention sublayer, known as cross attention, which focuses on the output of the encoder. In the constructions by Pérez et al. (2021) and related works (Bhattamishra et al., 2020b; Wei et al., 2022), a string is input into the encoder, and the decoder is allowed to operate for an arbitrary number of steps. The string is accepted if and only if the decoder eventually produces a vector that belongs to a predetermined set of accept vectors. This configuration significantly enhances the model's capability, as it can be compared to a language model that is permitted to ""think step by step"" (Kojima et al., 2022) before arriving at a final acceptance decision.",7,2,7,4,"Question:

How does the encoder-decoder architecture enhance the processing and acceptance of strings in formal language recognition?

Answer:

A transformer encoder-decoder combines a transformer encoder and decoder, adding to each layer of the decoder an additional attention sublayer, known as cross attention, which attends to the output of the encoder. In the constructions of Pérez et al. (2021) and related works (Bhattamishra et al., 2020b; Wei et al., 2022), a string is fed to the encoder, and the decoder is allowed to run for an arbitrary number of steps. The string is accepted if and only if the decoder eventually outputs a vector belonging to a fixed set of accept vectors. This setup vastly increases the model's power, as it could be likened to a language model that is allowed to ""think step by step"" (Kojima et al., 2022) before generating a final accept decision.","Question:

How does the encoder-decoder architecture improve the processing and acceptance of strings in formal language recognition?

Answer:

The encoder-decoder architecture, particularly when implemented using transformers, enhances the processing and acceptance of strings in formal language recognition by integrating a transformer encoder and decoder. Each layer of the decoder includes an additional attention sublayer, known as cross attention, which focuses on the output of the encoder. In the constructions by Pérez et al. (2021) and related works (Bhattamishra et al., 2020b; Wei et al., 2022), a string is input into the encoder, and the decoder is allowed to operate for an arbitrary number of steps. The string is accepted if and only if the decoder eventually produces a vector that belongs to a predetermined set of accept vectors. This configuration significantly enhances the model's capability, as it can be compared to a language model that is permitted to ""think step by step"" (Kojima et al., 2022) before arriving at a final acceptance decision.",NO,True,925,True,True
264833196-s15,Transformers as Recognizers of Formal Languages: A Survey on Expressivity,2023-11-01,Number representations,"Transformers, like most neural networks, operate, in principle, on real numbers.While hard attention transformers could be defined using only rational numbers, even rational numbers can represent an arbitrary amount of information.In the area of expressivity of RNNs, the use of real or rational numbers has led to results that make RNNs appear more powerful in theory than in practice (Siegelmann andSontag, 1995, 1994;Weiss et al., 2018).Consequently, some studies limit numeric representations to have  (1) bits, as floating-point numbers do in practice (Chiang et al., 2023).

But the need to handle arbitrary lengths ( §5.2) also makes it reasonable for precision to depend on .Merrill and Sabharwal (2023a) argue that in  (1) precision, an attention head cannot attend uniformly to a string of length , because the attention weights () would all round down to zero.So  (log ) bits of precision is a common choice (Yao et al., 2021;Merrill and Sabharwal, 2023b,a).Other choices are possible as well: Merrill and Sabharwal (2023b) use the set F of rational numbers whose denominator is unbounded but constrained to be a power of two.

Restricting the intermediate activations to limited precision introduces numerous decisions about when and how rounding should take place, which can potentially affect the expressivity of the model.For example, when summing  numbers, one could round after each addition or only at the end of the summation.Better formalizing these decisions and their impact on expressivity is an area for future research.",What challenges and considerations arise from limiting numeric precision in transformers?,"Transformers, like most neural networks, operate, in principle, on real numbers. Limiting numeric representations to have \(O(1)\) bits, as floating-point numbers do in practice, introduces challenges. The need to handle arbitrary lengths makes it reasonable for precision to depend on \(n\). In \(O(1)\) precision, an attention head cannot attend uniformly to a string of length \(n\), because the attention weights would all round down to zero. Thus, \(O(\log n)\) bits of precision is a common choice. Restricting the intermediate activations to limited precision introduces numerous decisions about when and how rounding should take place, which can potentially affect the expressivity of the model. For example, when summing \(n\) numbers, one could round after each addition or only at the end of the summation. Better formalizing these decisions and their impact on expressivity is an area for future research.",What are the challenges and considerations when limiting numeric precision in transformer models?,"Transformer models, like most neural networks, fundamentally operate on real numbers. However, in practice, these numbers are represented with a limited number of bits, as is the case with floating-point numbers. This limitation introduces several challenges. One significant issue is that transformers need to handle inputs of arbitrary lengths, which suggests that precision should ideally depend on the input size, denoted as \(n\). With only \(O(1)\) bits of precision, an attention head in a transformer cannot uniformly attend to a sequence of length \(n\) because the attention weights would round down to zero. Therefore, using \(O(\log n)\) bits of precision is a common approach to address this problem.

Restricting the precision of intermediate activations also involves making numerous decisions about when and how rounding should occur, which can impact the model's expressivity. For instance, when summing \(n\) numbers, one could choose to round after each addition or only at the end of the summation process. Understanding and formalizing these decisions and their effects on the model's expressivity is an important area for future research.",7,4,7,4,"Question:

What challenges and considerations arise from limiting numeric precision in transformers?

Answer:

Transformers, like most neural networks, operate, in principle, on real numbers. Limiting numeric representations to have \(O(1)\) bits, as floating-point numbers do in practice, introduces challenges. The need to handle arbitrary lengths makes it reasonable for precision to depend on \(n\). In \(O(1)\) precision, an attention head cannot attend uniformly to a string of length \(n\), because the attention weights would all round down to zero. Thus, \(O(\log n)\) bits of precision is a common choice. Restricting the intermediate activations to limited precision introduces numerous decisions about when and how rounding should take place, which can potentially affect the expressivity of the model. For example, when summing \(n\) numbers, one could round after each addition or only at the end of the summation. Better formalizing these decisions and their impact on expressivity is an area for future research.","Question:

What are the challenges and considerations when limiting numeric precision in transformer models?

Answer:

Transformer models, like most neural networks, fundamentally operate on real numbers. However, in practice, these numbers are represented with a limited number of bits, as is the case with floating-point numbers. This limitation introduces several challenges. One significant issue is that transformers need to handle inputs of arbitrary lengths, which suggests that precision should ideally depend on the input size, denoted as \(n\). With only \(O(1)\) bits of precision, an attention head in a transformer cannot uniformly attend to a sequence of length \(n\) because the attention weights would round down to zero. Therefore, using \(O(\log n)\) bits of precision is a common approach to address this problem.

Restricting the precision of intermediate activations also involves making numerous decisions about when and how rounding should occur, which can impact the model's expressivity. For instance, when summing \(n\) numbers, one could choose to round after each addition or only at the end of the summation process. Understanding and formalizing these decisions and their effects on the model's expressivity is an important area for future research.",NO,True,1160,True,True
