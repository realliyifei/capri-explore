{"corpusid_sectionid": "8729907-s1", "title": "The Language ENvironment Analysis (LENA) System: A Literature Review", "date": "2016-11-15", "section_title": "The LENA System", "section": "The LENA System's hardware includes a digital language processor (DLP) that can audio record for up to 16 hours. It measures 3-3/8\" x 2-3/16\" x 1/2\", weighs less than two ounces, and consists of a display screen, a USB port for uploading, and two buttons for powering and recording. The processor is held in a specially designed t-shirt or vest with a pocket on the front to secure the device. The audio quality is a 16-bit channel at a 16kHz sample rate (Ford, Baer, Xu, Yapanel, & Gray, 2008). Once the recording is complete it can be uploaded to the LENA software. Recordings are stored in the software by participant, allowing repeated recordings of one participant to be saved and compared over time. Once uploaded and recharged, the same participant or a new participant can use the DLP again without affecting the data stored in the software. The LENA System automatically segments the recordings into 12 categories including speakers, environmental sounds, and silence using Gaussian mixture models. A daylong audio file typically consists of 20,000 to 50,000 segments (VanDam et al., 2016). The software then estimates: adult word count (AWC), child vocalization count (CVC), and conversational turn count (CTC). The amount of background noise, electronic sounds, meaningful speech, and silence that were part of the child's listening environment are reported as percentages of the total sound present in the day and are displayed in user-friendly LENA generated graphs along with the AWC, CVC, and CTC. Additional details can be extracted using ADEX software provided by the LENA Foundation (Ford, et al., 2008;VanDam, Ambrose, & Moeller, 2012).\n\nIn addition to the raw data counts, Richards, Gilkerson, Paul, & Xu (2008) discuss the Automatic Vocalization Assessment (AVA) generated by the LENA System, which is correlated with traditional expressive language standard scores including those from the Preschool Language Scale -4 th Edition (PLS-4) (Zimmerman, Steiner, & Pond, 2002) and the Receptive-Expressive Emergent Language Test -3 rd Edition (REEL-3) (Bzoch, League, & Brown, 2003). To learn more about the LENA hardware and software, consult Ford et al. (2008) and .\n\nIn order to establish reliability, human transcribers coded 70 full day English recordings and their results were compared with those obtained by the automated software (Xu, Yapanel, Gray, & Baer, 2008). This data was collected as part of the Natural Language Study (NLS), the LENA Foundation's normative study . The LENA System correctly identified 82 and 76 percent of the segments humans coded as adult speech and child vocalizations respectively, indicating reasonable levels of agreement Warren et al., 2010;Xu et al., 2008;). Validity has also been shown in Spanish, French, Mandarin, Korean, and Vietnamese (Canault, Le Normand, Foudil, Loundon, & Thai- Van, 2015;Ganek & Eriks-Brophy, in revision;Gilkerson et al., 2015;Pae et al., 2016;Weisleder & Fernald, 2013). Although these studies show high fidelity, recording in a child's natural environment can produce a degraded auditory signal that may negatively impact validation. Possible causes of interference might include environmental factors such as background noise, overlapping speech, and reverberation, speaker variation like pitch or voice quality, and hardware variability. Although LENA clothing has been rigorously tested, fabric sound absorption rates may also impact accuracy ).", "filtered_refids": [[null, "b10", "b39", "b41"], ["b33", "b10", "b2", "b55"], ["b52", "b14", "b47", "b31", "b45", null, "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3438, "num_references": 15}
{"corpusid_sectionid": "8729907-s4", "title": "The Language ENvironment Analysis (LENA) System: A Literature Review", "date": "2016-11-15", "section_title": "Type of Study", "section": "Studies were divided into three types: comparative studies that examined LENA results between at least two cohorts, longitudinal studies that measured children's progress over time, and crosssectional studies that investigated children's ability at a specific point in time. Sixteen of the papers reviewed were comparative. They generally matched typically developing children to children with a communication disorder, though some compared language groups or treatment versus control groups. Eleven longitudinal studies evaluated child development over time. Both comparative and longitudinal studies measured the effects of treatment. Treatments including traditional speech therapy , formal established treatment programs such as Hanen's It Takes Two to Talk (Manolson, 1992;Weil & Middleton, 2011), and treatment associated specifically with provision of LENA feedback (Pae et al., 2016;Suskind et al., 2013). The remaining eleven cross-sectional studies often relied on a single day of recording.", "filtered_refids": [["b38", "b23", "b31", "b46"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1001, "num_references": 4}
{"corpusid_sectionid": "8729907-s15", "title": "The Language ENvironment Analysis (LENA) System: A Literature Review", "date": "2016-11-15", "section_title": "Socio-Economic Status (SES)", "section": "Socio-economic status (SES) is a measure of a person's social position based on income, education, and occupation. Hart and Risley (1995) famously reported a correlation between SES, language stimulation, and language abilities. Their study, and those like it, inspired the creation of the LENA System. Even though the impact of SES on language outcomes is widely known, few of the studies reported here were able to control for it. Ten studies failed to report SES and another six reported that comparative groups were matched either to each other or to census data. Six represented a range of maternal educational levels. Nine of the studies reported that their samples skewed towards high SES participants while five others reported collecting only low SES participants. Two studies also reported an SES mismatch between comparative groups (Jackson & Callender, 2014;Wood, et al., 2016).", "filtered_refids": [["b49", "b16", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 890, "num_references": 3}
{"corpusid_sectionid": "263153015-s2", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Background", "section": "In recent years, with the continuous expansion of computing power, large-scale language models have sprung up (Brown et al., 2020;OpenAI, 2023;Touvron et al., 2023a;Scao et al., 2022;Touvron et al., 2023b;Zhao et al., 2023b), and as the model size continues to grow, many new capabilities have emerged, such as in-context learning and chain-ofthought reasoning (Brown et al., 2020;Wei et al., 2022b,a;Schaeffer et al., 2023).Brown et al. (2020) finds that large-scale language models have excellent in-context learning (ICL) ability.ICL incorporates input-output demonstrations into the prompt text.With ICL, off-the-shelf LLMs can be employed without additional fine-tuning while achieving comparable performance.Nevertheless, this end-to-end approach tends to underperform when faced with complex reasoning tasks.Wei et al. (2022b) finds that the reasoning ability of LLMs can be improved by adding step-by-step reasoning processes to the demonstration, which is known as chain-of-thought prompting.CoT prompting enables the model to gain a more precise understanding of both the question's intricacies and the reasoning process.Furthermore, the model generates a sequence of reasoning steps, which grants us a transparent view of the model's cognitive process, further enhancing interpretability.", "filtered_refids": [["b158", null, "b8", "b124"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1299, "num_references": 4}
{"corpusid_sectionid": "263153015-s4", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Mathematical Reasoning", "section": "Mathematical reasoning is often used to measure the reasoning power of a model.Early benchmarks contain simple arithmetic operations (Hosseini et al., 2014;Koncel-Kedziorski et al., 2015;Roy and Roth, 2015;Koncel-Kedziorski et al., 2016).Ling et al. (2017) labels the reasoning process in natural language form, and Amini et al. (2019) builds on AQUA by labeling the reasoning process in program form.Later benchmarks (Miao et al., 2020;Patel et al., 2021;Cobbe et al., 2021;Gao et al., 2023) contain more complex and diverse questions.(Zhu et al., 2021;Chen et al., 2021Chen et al., , 2022b) ) require reasoning based on the table content.\n\nThere are also general benchmarks (Hendrycks et al., 2021;Mishra et al., 2022a,b) and reading comprehension form benchmarks (Dua et al., 2019;Chen et al., 2023).Recently, (Yu et al., 2021a) endowed pre-trained model with the ability of mathematical reasoning by using hierarchical reasoning and knowledge.", "filtered_refids": [["b43", "b14", "b203", "b13", "b31", "b112", "b120", null, "b99", "b16", "b83", "b2", "b65"], ["b26", "b44", "b179", "b41", null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 4, "num_chars": 947, "num_references": 18}
{"corpusid_sectionid": "263153015-s5", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "date": "2023-09-27", "section_title": "Commonsense Reasoning", "section": "Commonsense reasoning is the process of making inferences, judgments, and understandings based on knowledge that is generally known and commonly perceived in the everyday world.How to acquire and understand commonsense knowledge  (Ling et al., 2017) 100,000 Question Option Natural Language Math reasoning with NL rationale ASDiv (Miao et al., 2020) 2305 Question Number Equation Multi-step math reasoning SVAMP (Patel et al., 2021) 1,000 Question Number Equation Multi-step math reasoning GSM8K (Cobbe et al., 2021) 8,792 Question Number Natural Language Multi-step math reasoning GSM-Hard (Gao et al., 2023) 936 Question Number Natural Language GSM8K with larger number MathQA (Amini et al., 2019) 37,297 Question Number Operation Annotated based on AQUA DROP (Dua et al., 2019) 96,567 Question+Passage Number+Span Equation Reading comprehension form TheoremQA (Chen et al., 2023) 800 Question+Theorem Number \u2717 Answer based on theorems TAT-QA (Zhu et al., 2021) 16,552 Question+Table+Text Number+Span Operation Answer based on tables FinQA (Chen et al., 2021) 8  (Park et al., 2020) 1,465,704 Image+Event Action+Intent \u2717 Visual commonsense reasoning PMR (Dong et al., 2022) 15,360 Image+Background Option \u2717 Premise-based multi-modal reasoning ScienceQA (Lu et al., 2022) 21,208 Q+Image+Context Option Natural Language Multi-modal reasoning with NL rationales VLEP (Lei et al., 2020) 28,726 Premise+Video Option \u2717 Video event prediction CLEVRER (Yi et al., 2020) 305,280 Question+Video Option/Free-form Program Video temporal and causal reasoning STAR (Wu et al., 2021) 600,000 Question+Video Option \u2717 Video situated reasoning NEXT-QA (Xiao et al., 2021) 47 is a major impediment to models facing commonsense reasoning.Many benchmarks and tasks are proposed focusing on commonsense understanding (Talmor et al., 2019(Talmor et al., , 2021;;Bhakthavatsalam et al., 2021;Mihaylov et al., 2018;Geva et al., 2021;Huang et al., 2019;Bisk et al., 2020), event temporal commonsense reasoning (Rashkin et al., 2018;Zhou et al., 2019) , and commonsense verification (Wang et al., 2019).", "filtered_refids": [["b90", "b13", "b160", "b83", "b174", "b203", "b49", "b112", "b7", "b16", "b2", "b200", "b163", "b99", "b119", "b24", "b145", "b138", "b5", "b111", "b100", "b26", "b44", "b31", "b137", "b72", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 2078, "num_references": 27}
{"corpusid_sectionid": "1040974-s15", "title": "A SURVEY OF MACHINE TRANSLATION: ITS HISTORY, CURRENT STATUS, AND FUTURE PROSPECTS", "date": 1985, "section_title": "Jonathan Sloeum", "section": "A Survey of Machine Translation SYSTRAN SYSTRAN was one of the first MT systems to be marketed; the first installation replaced the IBM Mark II Russian-English system at the USAF FTD in 1970, and is still operational. NASA selected SYSTRAN in 1974 to translate materials relating to the Apollo-Soyuz collaboration, and EURATOM replaced GAT with SYSTRAN in 1976. Also by 1976, FTD was augmenting SYSTRAN with word-processing equipment to increase productivity (e.g., to eliminate the use of punched cards). The system has continued to evolve, for example by the shift toward a more modular design and by the allowance of topical glossaries (essentially, dictionaries specific to the subject area of the text). The system has been argued to be ad hoc -particularly in the assignment of semantic features (Pigott 1979). The USAF FTD dictionaries number over a million entries; Bostad (1982) reports that dictionary updating must be severely constrained, lest a change to one entry disrupt the activities of many others. (A study by Wilks (1978) reported an improvement/degradation ratio [after dictionary updates] of 7:3, but Bostad implies a much more stable situation after the introduction of stringent quality-control measures.)\n\nIn 1976 the Commission of the European Communities purchased an English-French version of SYSTRAN for evaluation and potential use. Unlike the FTD, NASA, and EURATOM installations, where the goal was information acquisition, the intended use by CEC was for information disseminationmeaning that the output was to be carefully edited before human consumption. Van Slype (1982) reports that \"the English-French standard vocabulary delivered by Prof. Toma to the Commission was found to be almost entirely useless for the Commission environment.\" Early evaluations were negative (e.g., Van Slype 1979), but the existing and projected overload on CEC human translators was such that investigation continued in the hope that dictionary additions would improve the system to the point of usability. Additional versions of SYSTRAN were purchased (French-English in 1978, andEnglish-Italian in 1979). The dream of acceptable quality for post-editing purposes was eventually realized: Pigott (1982) reports that \" . . . the enthusiasm demonstrated by [a few translators] seems to mark something of a turning point in [machine translation].\" Currently, about 20 CEC translators in Luxembourg are using SYSTRAN on a Siemens 7740 computer for routine translation; one factor accounting for success is that the English and French dictionaries now consist of well over 100,000 entries in the very few technical areas for which SYSTRAN is being employed.\n\nAlso in 1976, General Motors of Canada acquired SYSTRAN for translation of various manuals (for vehicle service, diesel locomotives, and highway transit coaches) from English into French on an IBM mainframe. GM's English-French dictionary had been expanded to over 130,000 terms by 1981 (Sereda 1982). Subseque~ly~ GM purchased an English-Spanish version of SYSTRAN, and began to build the necessary [very large] dictionary. Sereda (1982) reports a speed-up of 3-4 times in the productivity of his human translators (from about 1000 words per day); he also reveals that developing SYSTRAN dictionary entries costs the company approximately $4 per term (word-or idiom-pair).\n\nWhile other SYSTRAN users have applied the system to unrestricted texts (in selected subject areas), Xerox has developed a restricted input language (Multinational Customized English) after consultation with LATSEC. That is, Xerox requires its English technical writers to adhere to a specialized vocabulary and a strict manual of style. SYSTRAN is then employed to translate the resulting documents into French, Italian, Spanish, German, and Portuguese. Ruffino (1982) reports \"a five-to-one gain in translation time for most texts\" with the range of gains being 2-10 times. This approach is not necessarily feasible for all organizations, but Xerox is willing to employ it and claims it also enhances source-text clarity.\n\nCurrently, SYSTRAN is being used in the CEC for the routine translation, followed by human post-editing, of around 1,000 pages of text per month in the couples English-French, French-English, and English-Italian (Wheeler 1983). Given this relative success in the CEC environment, the Commission has recently ordered an English-German version as well as a French-German version. Judging by past experience, it will be quite some time before these are ready for production use, but when ready they will probably save the CEC translation bureau valuable time, if not real money as well.", "filtered_refids": [["b52", "b34", "b4"], ["b48", "b35", null], ["b39"], ["b36"], ["b51"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 4655, "num_references": 9}
{"corpusid_sectionid": "1040974-s16", "title": "A SURVEY OF MACHINE TRANSLATION: ITS HISTORY, CURRENT STATUS, AND FUTURE PROSPECTS", "date": 1985, "section_title": "LOGOS", "section": "Development of the LOGOS system was begun in 1964. The first installation, in 1971, was used by the U.S. Air Force to translate English maintenance manuals for military equipment into Vietnamese. Due to the termination of U.S. involvement in that war, its use was ended after two years. (A report by Sinaiko and Klare (1973) disparaged LOGOS's cost-effectiveness, but this claim was argued to be seriously flawed and was formally protested (Scott, personal communication).) The linguistic foundations of LOGOS are not well advertised, presumably for reasons involving trade secrecy. The system developer states that \"our linguistic approach ... has evolved in ways analogous to case grammar/valency theory . . . mapping natural language into a semantosyntactic abstraction language organized as a tree\" (Scott, personal communication).\n\nLOGOS continued to attract customers. In 1978, Siemens AG began funding the development of a LOGOS German-English system for telecommunications manuals. After three years LOGOS delivered a \"production\" system, but it was not found suitable for use (due in part to poor quality of the translations, and in part to the economic situation within Siemens which had resulted in ff much-reduced demand for translation, hence no imme-diate need for an MT system). Eventually LOGOS forged an agreement with the Wang computer company that allowed the :implementation of the German-English system (formerly restricted to large IBM mainframes) on Wang office computers.\n\nThis system reached the commercial market, and has been purchased by several multi-national organizations (e.g., Nixdorf, Triumph-Adler, Hewlett-Packard); development of other language pairs (e.g., English-French, English-German) is underway (Scott, personal communication). METEO TAUM-METEO is the world's only example of a truly fully-automatic MT system. Developed as a spin-off of the TAUM technology, as discussed earlier, it was fully integrated into the Canadian Meteorological Center's (CMC's) nation-wide weather communications network by 1977. METEO scans the network traffic for English weather reports, translates them \"directly\" into French, and sends the translations back out over the communications network automatically. Rather than relying on post-editors to discover and correct errors, METEO detects its own errors and passes the offending input to human editors; output deemed \"correct\" by METEO is dispatched without human intervention, or even overview.\n\nTAUM-METEO was probably also the first MT system where translators were involved in all phases of the design/development/refinement; indeed, a CMC translator instigated the entire project. Since the restrictions on input to METEO were already in place before the project started (i.e., METEO imposed no new restrictions on weather forecasters), METEO cannot quite be classed with the Xerox SYSTRAN system, which relies on restrictions geared to the characteristics of SYSTRAN. But METEO is not extensible -though similar systems could be built for equally restricted textual domains, if they exist.\n\nOne of the more remarkable side effects of the METEO installation is that the translator turnover rate within the CMC went from 6 months, prior to METEO, to several years, once the CMC translators began to trust METEO's operational decisions and not review its output (Brian Harris, personal communication). METEO's input constitutes over 24,000 words per day, or 8.5 million words per year. Of this, it now correctly translates 90-95%, shuttling the other (\"more interesting\") 5-10% to the human CMC translators.\n\nAlmost all of these \"analysis failures\" are attributable to communications noise (the CMC network garbles some traffic), misspellings (METEO does not attempt corrections), or words missing from the dictionary, though some failures are due to the inability of the system to handle certain linguistic constructions. METEO's computational requirements total about 15 CPU minutes per day on a CDC 7600 (Thouin 1982). By 1981, it appeared that the builtin limitations of METEO's theoretical basis had been reached, and further improvement was not likely to be cost-effective.", "filtered_refids": [[null, "b41"], [], [null], [], [], ["b45"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 4160, "num_references": 4}
{"corpusid_sectionid": "1040974-s18", "title": "A SURVEY OF MACHINE TRANSLATION: ITS HISTORY, CURRENT STATUS, AND FUTURE PROSPECTS", "date": 1985, "section_title": "SPANAM", "section": "Following a promising feasibility study, the Pan American Health Organization in Washington, D.C. decided in 1975 to undertake work on a machine translation system, utilizing some of the same techniques developed for GAT. Consultants were hired from nearby Georgetown University, the home of GAT. The official PAHO languages are English, French, Portuguese, and Spanish; Spanish-English was chosen as the initial language pair, due to the belief that \"This combination requires fewer parsing strategies in order to produce manageable output [and other reasons relating to expending effort on software rather than linguistic rules]\" (Vasconcellos 1983). Actual work started in 1976, and the first prototype was running in 1979, using punched card input on an IBM mainframe. With the subsequent integration of a word-processing system, production use could be seriously considered.\n\nAfter further upgrading, an in-house translation service based on SPANAM was created in 1980. Later that year, in its first major test, SPANAM reduced manpower requirements for a test translation effort by 45%, resulting in a monetary savings of 61% (Vasconcellos 1983). (Because these SPANAM translation and on-line post-editing figures appear to be contrasted against the purely manual, hardcopy translation tradition at PAHO, the gains from using SPANAM per se may be hopelessly confounded with the gains of working on-line; thus, it is difficult or impossible to say how much increase in productivity is accounted for by SPANAM alone.) Since 1980, SPANAM has been used to translate well over a million words of text, averaging about 4,000 words per day per post-editor. The post-editors have amassed \"a bag of tricks\" for speeding the revision work, and special string functions have also been built into the word processor for handling SPANAM's English output.\n\nConcerning the early status of SPANAM, sketchy details implied that the linguistic technology underlying it was essentially that of GAT; the grammar rules seemed to be built into the programs, in the GAT tradition. The software technology was updated in that the programs are modular. The system is not sophisticated: it adopts the direct translation strategy, and settles for local analysis of phrases and some clauses via a sequence of primitive, independent processing stages (e.g., homograph resolution) -again, in the Georgetown tradition. SPANAM is currently used by three PAHO translators in their routine work.\n\nA follow-on project to develop ENGSPAN (for English-Spanish), underway since 1981, has also delivered a production system -this one characterized by a more advanced design (e.g., an ATN parser), some features of which may find their way into SPANAM. (SPANAM is currently \"undergoing a major overhaul\" (Vasconcellos, personal communication).)\n\nFour PAHO translators already employ ENGSPAN in their daily work. Based on the successes of these two systems, development of ENGPORT (with Portuguese as the Target Language) has begun. In the future, \"all translators [in the Language Services bureau of PAHO will be] expected to use MT at least part of the time, and the corresponding duties are included in the post descriptions\". (Vasconcellos, personal communication).", "filtered_refids": [[null], [], [], [null], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 3233, "num_references": 3}
{"corpusid_sectionid": "245124108-s7", "title": "Surfer100: Generating Surveys From Web Resources, Wikipedia-style", "date": "2021-12-13", "section_title": "Content Selection", "section": "We first tested the quality of the content selection methods for generic retrieval of content relevant to a topic on our data. We choose the Semantic Search, WikiCite, and RoBERTa-Rank methods from Table 1 for analysis. For Semantic Search, we experiment with three types of sentence embeddings, the original sentence-transformer BERT embeddings (SS-BERT), embeddings fine-tuned with SciBERT (SS-SciBERT), and a version fine-tuned to differentiate whether two paragraphs belong to the same Wikipedia section (SS-Wiki). Surprisingly, we found such content was often returned during retrieval despite the poor grammaticality and relevance. We hypothesize that the tendency to return short sentences, often with odd punctuation may relate to the extension of these methods to paragraph levels while inherently being developed for sentencelevel tasks. We then remove sentences shorter than 6 tokenized words, as well as apply heuristics for removing sentences based on the number of parentheses, brackets, and other tokens such as equal signs. We required that each paragraph returned consist of at least two sentences and require that the topic word (or one word within the topic, for multi-word topics) appear in the paragraph. About 85 paragraphs per topic remain after this filtering. The comparison of results before and after preprocessing and filtering is found in Table 3. Notably, the WikiCite method performs much better than semantic search and close to RoBERTa. We believe this is because the method is trained for content selection based on a topic and not simply trained for returning content with high recall. A potential problem with current methods in this two-step approach is that content selection is trained and evaluated with recall in mind, to capture as large a range of the topic, which produces models without the precision necessary in a real-world application. This aligns with previous work in extractive summarization suggesting that optimizing for recall gives suboptimal results (Zopf et al., 2018). Section-Specific Content Selection: We investigated the ability of our content selection models to retrieve content specific for each chosen section, for example, querying \"History of BERT\"rather than \"BERT.\"We 2 https://github.com/IreneZihuiLi/Surfer100  observed large overlaps between the returned results, between 5 and 9 paragraph overlap between the top 10 results for each section. Among all methods, Wikicite has the least overlap. As an alternative method to select distinct content for each section, we investigate clustering methods, using out-of-the-box Agglomerative (M\u00fcllner, 2011) clustering provided by scikit-learn 3 . We cluster the embeddings obtained before the final output layer from the WikiCite and RoBERTa methods, and the Search-Wiki embeddings. We annotated the coherence of each cluster. Clusters obtained using embeddings from RoBERTa, Search-Wiki and Wi-kiCite had a corresponding average coherence of 3.07, 3.40, and 3.52 on a 1-5 scale, signaling slightly aboveaverage coherence for each clustering. Again, the poor performance of RoBERTa in clustering may be due to the more general topic training method. As suggested by Deutsch and Roth (2019), the WikiCite method may dilute topic information in the final layer despite topic attention in previous layers and thus benefit from using embeddings before the final layer as clustering.", "filtered_refids": [["b12", null, "b2", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 3395, "num_references": 4}
{"corpusid_sectionid": "251307817-s5", "title": "Rhetorical Structure Approach for Online Deception Detection: A Survey", "date": 2022, "section_title": "Online Deception Detection", "section": "Over the past year, on the sharp growth of the web and social media, cyber-crimes such as identity blows, thief, fraud, and misinformation have become increasingly common. Theses deceptive activities often are characterized by the ease of deception and concealment of one's real identity (P\u00e9rez-Rosas et al., 2017). The research area responsible for investigating and providing methods to detect deceptive activities is known as deception detection. According to , automated deception detection, as a field within NLP and Information Science (IS), is responsible for the development of methods to distinguish truth from deception in textual data, identifying linguistic predictors of deception with text processing and machine learning techniques. Deception detection in textual information has became a relevant study area within NLP, mainly due to the sharing of fake news on the web and social media around the world. Online deceptive activities are addressed by literature on different tasks, which handle a wide range of aspects, such as credibility of users and sources, information veracity, information verification, and linguistic aspects of deceptive language (Atanasova et al., 2019). Unless otherwise stated, these tasks include the discovery of fake news (Lazer et al., 2018); rumor detection in social media (Vosoughi et al., 2018); information verification in question answering systems (Mihaylova et al., 2018); detection of information manipulation agents (Chen et al., 2013;Mubarak et al., 2020); assertive technologies for investigative journalism (Hassan et al., 2015); detection of fake reviews (Ott et al., 2011); detection of deceptive discussions (Larcker and Zakolyukina, 2012). A definition with relevance for the area rotates around the concept of \"deceptive language\". Deceptive language is defined by Communication, Linguistics and Psychology literature as a type of language deliberately used with aim of attempting to mislead others. For instance, falsehoods communicated by people who are mistaken or self-deceived are not lies, nevertheless, literal truths designed to mislead are lies as a deliberate attempt to mislead others. Besides that, most relevant literature on deception refers mainly to levels of deceit and typology of media (e.g., face-to-face, voice, text) (Zhou et al., 2003). DePaulo et al. (2003) claim that deceptive linguistic style may present weak employment of singular and third-person pronouns, negative polarity, and high employment of movement verbs. Nahari et al. (2019) suggest that a basic assumption related to deceptive language is that liars differ from truth-tellers in their verbal behavior, making it possible to classify them by inspecting their verbal accounts. Additionally, a set of linguistic behaviors may predict deception, as tones of words and kinds of preposition, conjunctions, and pronouns (Newman et al., 2003). Taking advantage of the discourse-level analysis, Galasi\u0144ki (2000) presents a pioneer study on fictional deceptive stories. According to the author, discourse analysis of deceptive texts deception is intrinsically tied with \"information manipulation\", which consists of presenting a reality that is misrepresented. The author argues that deception should be classified in three different levels: (i) falsification (i.e., attributing false statements to a debater), (ii) distortion (i.e., manipulating by understating or overstating what a debater states), and (iii) de-contextualization (i.e., taking the words a debater uses out of their context).", "filtered_refids": [["b12", "b44", "b4", "b26", "b18", "b47", "b25", "b32", "b27", "b29", "b30", "b19", "b5", "b8", "b0"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3541, "num_references": 15}
{"corpusid_sectionid": "251307817-s7", "title": "Rhetorical Structure Approach for Online Deception Detection: A Survey", "date": 2022, "section_title": "Corpora, Models and Methods", "section": "A very plausible assumption, when one opts for the discourse-aware approach applied to deception detection, would be that there are significant differences between structures of truthful and deceptive stories. Indeed, it has been proposed by various authors. While the research community currently lacks discourse annotated corpora for deception detection tasks, recent works have proposed discourse-tagged corpora for the English, Portuguese and Russian languages. Table 1 provides a summary of the discourse-tagged corpora proposed in literature. As it is shown in Table 1, the discourse-tagged corpora for the fake news and fake reviews detection tasks were proposed for the English, Russian, and Portuguese languages. As being particularly a human time-onerous task and a kind of challenging annotation process, the corpora present a small set of documents. Furthermore, both monolingual and multilingual corpora were proposed.\n\nMoving forward, as it is known from research proposals on fake news and fake reviews, a wide variety of models have been proposed to tackle online deception detection. Most of them rely on linguistic features such as n-grams, language complexity, part-of-speech tags, and syntactic and semantic features. On the other hand, discourse-level structure approach is usually framed as a supervised learning problem, which embodies in a model coherence relations followed by hierarchical nuclearity information to build automatic classifiers. In Table 2, we also summarize discourse-aware models and methods proposed in literature. Notice that models use bag-of-rst, dependency parsing, embeddings and BERT tokenizer as features, and both classical and neural machine learning have been applied. Finally, f1score performance is reported in column \"%\", except for Karimi and Tang (2019), whose authors reported values related to accuracy. \n\nBag-of-rst SVM English 63% Fake News 4.2. Fake News Detection Kuzmin et al. (2020) Fake news prediction is a global problem, and most of approaches have been developed for the English language (Kuzmin et al., 2020). Nevertheless, fake news is spread around the world, and it may be written originally in several languages. In this proposal, the authors trained and compared different models for fake news detection in Russian. They assess whether different language-based features including the vectorization of rhetorical structure obtained from both -a RST parsing and a rst manually annotated corpus -could be helpful for the fake news detection task. This proposal was implemented and evaluated using classical machine learning methods, as Support Vector Machine (SVM) and Logistic Regression (LR) over bag-of-ngrams and bag-of-rst representations. Besides that, sophisticated machine learning techniques, as BERT (Devlin et al., 2019) were also implemented. The authors used three different corpora of fake news in Russian. The first one was proposed by Pisarevskaya (2017) (see Table 1 -manually annotated). The second one was proposed by Zaynutdinova et al. (2019); it is composed of 1,366 fake news and 7,501 true news. Finally, Taiga Corpus 2 was also applied. Furthermore, three distinct representations were used (i) bag-of-ngrams with tfidf preprocessing, (ii) bag-of-rst, which consists of the vectorization of coherence relations and nuclearity, and (iii) pre-trained BERT-based model, more specifically, the RuBERT2 obtained using DeepPavlov 3 (Burtsev et al., 2018) with Transformers (Wolf et al., 2020). The authors reported that classical approaches using bagof-n-grams and bag-of-rst presented high results (90% of F1-score) overcoming the neural network approach, which uses the RuBERT ((88% of F1-score). Moreover, the authors suggest that satire is similar to fake news, and satire differs from real news. The authors also concluded that humans rarely perform better than chance at detecting deceptive activities. Therefore, humans performed worse than the best automated model.", "filtered_refids": [[], ["b15"], ["b6", "b46", "b45", "b2", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 31, "num_chars": 3967, "num_references": 6}
{"corpusid_sectionid": "251307817-s8", "title": "Rhetorical Structure Approach for Online Deception Detection: A Survey", "date": 2022, "section_title": "Karimi and Tang (2019)", "section": "Discourse-level structure analysis of deceptive and truthful news is a tremendous challenge, mainly due to existing methods for capturing discourse-level structure rely on annotated corpora, which are not available for fake news datasets (Karimi and Tang, 2019). In this proposal, the authors provide a new dependency parsing approach, titled \"Hierarchical Discourse-level Structure for Fake news detection\". The HDSF consists of an automated manner to learn a discourselevel structure for a given document through an approach based on the dependency parsing at the sentence level. It should be noted that in this approach, sentences are classified as elementary discourse units (EDU's). An example of discourse-level structure of a document (fake news) using the proposed dependency tree is shown in Figure 5. Note that a document is segmented into sentences (S1, S2, S3, S4 and S5), and hierarchically organized. Figure 5: Hierarchical discourse-level structure of a document using a dependency tree. This fake news was extracted from Politifact.\n\nThe HDSF framework build a hierarchical structure between sentences without relying on an annotated corpus, as may be seen in Figure 6. Note that the HDSF receives as input a corpus of fake/real news documents (i.e., D). A model M may automatically learn hierarchical and structurally rich representations for documents in D. Meanwhile, given binary labels Y, model M uses the hierarchical representations to automatically predict the labels of unseen news documents. In order to compare the HDSF approach with baseline and state-of-art models, the authors implemented seven different models including the proposed methods: Ngrams, LIWC (Pennebaker et al., 2015), Bag-of-rst (Rubin and Lukoianova, 2015), BiGRNN-CNN (Ren and Zhang, 2016), LSTM and LSTM[w+s] (Karimi and Tang, 2019). Based on the obtained results, the HDSF overcame the other implemented approaches (82.19% of Accuracy). They concluded that discourselevel structure analysis is effectively rich for fake news prediction. In addition, the structures of fake news documents at the discourse level are substantially different from those of true ones, and real news documents indicate more degree of textual coherence.", "filtered_refids": [["b15"], ["b15", "b34", "b31", "b35"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2230, "num_references": 5}
{"corpusid_sectionid": "252461144-s4", "title": "A Survey of Machine Translation Tasks on Nigerian Languages", "date": 2022, "section_title": "Machine Translation Techniques", "section": "Machine translation has been extensively studied for decades (Bahdanau et al., 2016;Luong et al., 2015;Koehn et al., 2007b) with neural machine translation providing the most recent state-of-the art results. There are three types of machine translation techniques that have been explored in the literature -Rule-based machine translation (RBMT), Statistical machine translation (SMT), and Neural machine translation (NMT). A high level overview of these techniques are outlined as follows:\n\nRule-based Approach: This is one of the oldest form of machine translation technique used. This approach is based on understanding the linguistic properties of the source and target languages using dictionaries and expert knowledge to define grammar rules. This process involves morphology analysis, syntax, and lexical semantics. Linguistic analysis is performed on the source language to identify morphology, parts of speech, phrases, named entity, and word disambiguation. Each word is replaced in the target language using a dictionary which represents mappings between source and target words. In order to preserve sentence semantics across translated languages, most RBMT approach utilizes a combination of finite state machines to develop their knowledge graphs (Forcada et al., 2011;Scott and Barreiro, 2009). (Forcada et al., 2011) utilizes finite-state transducers for lexical processing, Hidden Markov models for part-ofspeech tagging, and multi-stage finite-state chunking for structural transfer. (Eisele et al., 2008) utilizes a modified phrase table with entries from translating various data with rule-based systems. One of the main advantage of this approach is that it does not require as much parallel sentence pairs as with most NMT approaches. Also, translation errors can be corrected by updating the dictionary. This allows for flexibility in updating language constructs. Consequently, one major drawback of this approach is that the translation quality is mostly defined by the strength of the dictionary which requires frequent updates from domain experts. RBMT also tends to produce translations that are more repetitive and less fluid which can be attributed to its mechanical approach of using rules for translation.\n\nStatistical-based Approach: This approach involves the use of statistical techniques such as probability distribution models to provide a means for machine translation between source and target languages. This is achieved by assigning a probability score to word or phrase contained in every target sentence where words or phrases with the highest probability contains the best translation for the target sentence (Koehn et al., 2007b;Brown et al., 1993). SMT can be applied at a word or phrase level and consists of a translation and language model. The translation model is defined as the probability that the source sentence is the translated version of the target word. The language model tries to describe how representative the target sentence is to the natural spoken language. It assigns probabilities to sentence similar to the sentence ordering. One approach utilized in developing the probability distributions is the use of Bayes theorem (Zens et al., 2002) and Hidden Markov Model (Deng and Byrne, 2008;Alkhouli et al., 2016). (Koehn et al., 2007a) developed Moses, an open-source machine translation toolkit which utilizes linguistic information that captures semantics in mapping text phrases and a confusion network decoding for translating ambiguous text inputs.\n\nOne advantage of SMT approach over RBMT is the improved translation quality. It allows for translation that captures not just linguistic morphology but the use of a probability distribution which improves with semantic quality.\n\nNeural-based Approach: This approach is referred to as the state of the art in machine translation as it is widely used and has shown to provide results with higher accuracy as compared to the other approaches (Bahdanau et al., 2016;Luong et al., 2015;Cho et al., 2014). Neural machine translation involves the use of deep learning techniques to provide a means of inferring high level semantics from language translations. A popular neural machine translation approach (Vaswani et al., 2017) utilize transformer based models with encoder-decoder architecture. These models consists of stacks of multiple hidden layers with multi-head attention mechanisms and have been shown (Vaswani et al., 2017) to outperform traditional neural architecture such as Recurrent Neural Networks for machine translation task.\n\nCurrent implementation for language models consists of multilingual language model embeddings (Pires et al., 2019;Lample and Conneau, 2019) where one language model is trained on multiple languages. This allows for zero-shot transfer learning where cross language representation is learned without the need for a parallel language corpus across all language pairs. This has been shown to produce better results than monolingual model training (Conneau et al., 2020) especially for low-resource languages. Supervised neural approach relies heavily on a large corpus of quality translated sentence pairs; as such this poses a limitation to the quality of language translation. There are some approaches that work well with limited datasets (Mikolov et al., 2013;Artetxe et al., 2018) and can provide a means of translating from one language to another based on translations derived from a similar language ( ", "filtered_refids": [["b10", "b34", "b32"], ["b24", "b48", "b20"], ["b6", "b58", "b14", "b31", "b32", "b19"], [], ["b53", "b10", "b34", "b17"], ["b18", "b47", "b33", "b8", "b37"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 33, "num_chars": 5464, "num_references": 21}
{"corpusid_sectionid": "252461144-s5", "title": "A Survey of Machine Translation Tasks on Nigerian Languages", "date": 2022, "section_title": "State of the Art", "section": "There has been a limited number of work centered on machine translation of Nigerian languages. Most of the cutting edge research on machine translation utilizes neural machine translation approaches (Stahlberg, 2020). However, most of the machine translation work on Nigerian languages focuses on rule-based approach using context free grammars while a few focuses on neural machine translation techniques such as transformer-based models. We outline the work that has been conducted over the years and categorize each work based on the different approach utilized.\n\n3.1. Rule-based Approach (Ayegba et al., 2014) utilizes a rule-based approach for machine translation of English to Igala language. This approach utilizes noun phrases from English language while performing a series of processes such as parts of speech tagging, morphological analysis which analyzes words based on its root or base form, and comparing noun phrases to components contained in a bilingual dictionary. Their approach was tested on 120 randomly selected English noun phrases and achieves a Bilingual Evaluation Understudy (BLEU) accuracy of 90.9%. (Akinwale et al., 2015) proposed a web-based English to Yoruba translation model utilizing a similar approach as (Ayegba et al., 2014). The translator component utilizes a set of twenty rules which were specified using context free grammar. This approach achieves an accuracy of 90.5%. (Eludiora and Ajibade, 2021) proposed a rule-based model for English to Yoruba translation of Yoruba verbs based on tone changing. It is their intuition that some Yoruba verbs change tone in the bilingual dictionary from low-tone to mid-tone which sometimes changes the meaning of the sentence. Their approach is implemented using 20 tone changing verbs. They evaluate the efficacy of their approach by performing language expert evaluation which entails comparing the output derived from their model with the output generated from Google translation. According to the authors, this approach is very time-consuming but very extensive. In addition, they evaluate their approach using human evaluators. In a total of 70 respondents, 69% of the respondents agree that their system correctly translates verb-phrases while 29% of the respondents agrees that Google translation works efficiently. (Ezeani et al., 2016) developed a model using the Igbo Bible corpus to detect and restore missing didactics in texts at word level toknization. Their approach on didatic replacement utilizes work conducted by (Simard, 1998) which consists of using Hidden Markov Model in which the input text is viewed as a stochastic process. (Onyenwe et al., 2019) develops a parts of speech (POS) tagger for Igbo language. Their approach utilizes a host of post tagging approach including Hidden Markov Model. They achieve an accuracy of of 93.17% to 98.11% on the overall words, and 7.13% to 83.95% on unknown words. (Orife, 2020) developed a neural machine translation model for translating Edoid languages to English. Edoid languages are primarily spoken by the southern Nigeria (Edo and Delta states) consisting of Edo, Esan, Urhobo, and Isoko languages. They utilize transformer models with encoder decoder and multi-head self attention. To evaluate the effectiveness of their approach, they trained their model using JW300 dataset (Agi\u0107 and Vuli\u0107, 2019) consisting of over 100 African languages The training was conducted using tokenization processeses such as Byte-pair encoding (BPE) and word-level tokenization . The results shows that Urhobo and Isoko consists of larger training dataset performed best with higher BLEU scores. BPE tokenization provided a 37% boost for the development and test dataset of Edo and Esan languages and a 32% boost for Urhobo language. However, BPE produced worse results when compared to word-level tokenization for Isoko languages. (Ahia and Ogueji, 2020) developed supervised and unsupervised neural machine translation models to serve as a baseline for future works to come in the translation of Nigerian pidgin. For their approach, they utilized a transformer architecture proposed by (Vaswani et al., 2017) while experimenting with word-level and Byte-Pair encoding subword tokenization. The supervised approach produced a BLEU score of 17.73 while the unsupervised model produced a BLEU score of 5.18 for English to Pidgin Translation. (Nguyen and Chiang, 2018) developed a model that improves the mistranslation of rare words. This approach is based on a modified version of attention based encoderdecoder models. Their approach hones on the premise that the output layer which consists of the inner product of the context vector and all possible word embeddings improperly rewards frequently occurring words. In their approach, instead of using the dot product, the norm vectors are set to a constant value. In addition, they include new terms which provides direct connection from source sentence and this makes the model properly memorize rare word translations. They evaluate their approach on 8 language pairs which includes Hausa to English language pair. (Hedderich et al., 2020) demonstrates that a transfer learning approach through multilingual transformer models (mBERT and XLM-RoBERTa) can be utilized for tasks such as name entity recognition and topic classification on low-resource languages. The approach involves fine-tuning the target language dataset on high-resource language models. Their approach is evaluated on three African languages Hausa, isiXhosa and Yoruba out of which two of the languages (Hausa, and Yoruba) are Nigerian languages. They produce results comparable to the state-of-the-art with as little as 10 or 100 labelled sentences. They achieve at least an improvement of 10 points in the F-1 score for a shared label of named entity recognition. Their result shows promise and is consistent with their hypothesis which also validates work shown in prior research. Their approach however does not produce good results for topic classification. This might be as a result of mismatch in the label set. (Ogueji et al., 2021) developed AfriBERTa, an approach which involves training multilingual models on low-resource language. According to the authors, it is a general assumption that low-resource multilingual language models benefit from being trained in combination with high-resource languages. low-resource multilingual models do not need to be trained in combination with highresource languages and does not require as much dataset used for training high-resource languages. The authors accomplish multilingual model training on low-resource languages with a dataset consisting of 11 African languages of which Igbo, Yoruba, Hausa, and Nigerian Pidgin are Nigerian languages. They also show that the state of the art accuracy can be achieved with training on less than 1GB of data. Furthermore, they apply their pre-trained transformer model on downstream tasks such as name entity recognition and text classification task. Their model outperforms the state of the art multilingual models such as mBERT and XLM-R.", "filtered_refids": [["b50"], ["b44", "b4", "b46", "b9", "b28", "b41", "b49", "b21", "b45", "b22", "b53", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 53, "num_chars": 7092, "num_references": 13}
{"corpusid_sectionid": "252461144-s8", "title": "A Survey of Machine Translation Tasks on Nigerian Languages", "date": 2022, "section_title": "Data Acquisition", "section": "One of the major impediments to corpus-based machine translation of low-resource languages is the quality and quantity of the dataset utilized for model training. However, there has been limited work in generating datasets for machine translation tasks of Nigerian languages. Some of the prominent dataset utilized for this task are outlined below.  developed an open-source dataset of Yoruba speech which consists of over four hours of recordings from 36 male and female volunteers with transcription and disfluency annotation. (Adelani et al., 2021) developed a publicly available parallel corpus known as MENYO-20K which consists of a parallel corpus of texts in English-Yoruba language with over 20,000 sentences obtained from news articles, TED talks, movie and radio transcripts, science and technology texts and short articles from the web which were annotated by professional translators with proficiency in Yoruba language. (Butryna et al., 2020) developed a crowd-sourced speech corpus for low-resource languages which consists of languages in South and Southeast Asia, Africa (South Africa, and Nigeria), Europe and South America. The only Nigerian language supported was Nigerian Pidgin. They achieve this task by partnering with local communities and universities in the region. (Agi\u0107 and Vuli\u0107, 2019) introduces JW300, a parallel corpus of over 300 languages containing around over 100,000 sentences per language pair. The corpus consists of a total of 1,335,376 articles with over 109 million sentences and 1.48 billion tokens. They achieve this by crawling publications from jw.org. OPUS (Tiedemann, 2012), is one of the largest open source parallel corpora repository of translated text. It consists of over 90 languages with a total of 3,800 language pairs comprising of over 40 billion tokens in 2.7 billion parallel units. (Goyal et al., 2021) introduces Flores-101, an open-source benchmark for evaluating lowresource multilingual machine translation task. This dataset consists of 3,000 sentences extracted from Wikipeadia. In addition, the sentences have been converted into 101 languages which includes three major languages in Nigeria (Igbo, Yoruba, and Hausa). (Ezeani et al., 2020) developed a publicly available standard evaluation benchmark dataset for Igbo to English machine translation. This includes over 10,000 high quality English to Igbo sentence pairs which were derived mostly from news (BBC Igbo 2 and PUNCH newspaper 3 ) domains.", "filtered_refids": [["b15", "b25", "b23", "b1", "b51", "b2"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 2469, "num_references": 6}
{"corpusid_sectionid": "246017121-s16", "title": "Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset", "date": "2022-01-07", "section_title": "Implementation Details", "section": "Data pre-processing. We implement spectral augmentation (SpecAugment), a state-of-the-art audio data augmentation method, which is implemented by masking certain frequency and time values on the spectrogram (Park et al., 2019). We use SpecAugment for the Common Voice zh-HK baseline, where it shows an improvement in overall results. Furthermore, we apply cepstral mean and variance normalisation (CMVN) for all the utterances (Strand and Egeberg, 2004). In Fairseq S2T, pre-processed audio can be used directly or stored in the form of .npy files. The latter is the way in which we store features extracted from Cantonese datasets to achieve faster training. For tokenization of the transcribed data, we use the SentencePiece tokenizer (Kudo and Richardson, 2018) with unigram subword tokenization (Kudo, 2018) and an 8,000-word vocabulary. The vocabulary covers 99.95% of the characters in the MDCC (the default coverage for character-based languages).", "filtered_refids": [["b11", "b21", "b22", "b10"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 954, "num_references": 4}
{"corpusid_sectionid": "14642384-s3", "title": "DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL", "date": "2004-07-01", "section_title": "Descriptions serving as disguised PROPER", "section": "NAMES, such as The Federal Communications Commission or the Iran-Iraq war. The heuristics for recognizing these definite descriptions were primarily based on capitalization (of the head or the modifiers).\n\n3. PREDICATIVE descriptions, i.e., descriptions semantically functioning as predicates rather than as referring. These include descriptions occurring in appositive position (as in Glenn Cox, the president of Phillips Petroleum) and in certain copular constructions (as in the man most likely to gain custody of all this is a career politician named Dinkins). The heuristics used to recognize these cases examined the syntactic structure of the NP and the clause in which it appeared.\n\n4. Descriptions ESTABLISHED (i.e., turned into functions in context) by restrictive modification, particularly by establishing relative clauses (Loebner, 1987) and prepositional phrases, as in The hotel where we stayed last night was pretty good.\n\nThese heuristics, as well, examined the syntactic structure of the NP. 5. LARGER SITUATION definite descriptions (Hawkins, 1978), i.e., definite descriptions like the sun, the pope or the long distance market which denote uniquely on the grounds of shared knowledge about the situation (these are Loebner's 'situational functions'). Vieira and Poesio's system had a small list of such definites.\n\nThese heuristics were included as tests both of a decision tree concerned only with the task of DN detection, and of decision trees determining the classification of DDs as anaphoric, bridging or discourse new. In both cases, the DN detection tests were intertwined with attempts to identify an antecedent for such DDs. Both hand-coded decision trees and automatically acquired ones (trained using ID3, (Quin-lan, 1986)) were used for the task of two-way classification into discourse-new and anaphoric. Vieira and Poesio found only small differences in the order of tests in the two decision trees, and small differences in performance. The hand-coded decision tree executes in the following order:\n\n1. Try the DN heuristics with the highest accuracy (recognition of some types of semantically functional DDs using special predicates, and of potentially predicative DDs occurring in appositions);\n\n2. Otherwise, attempt to resolve the DD as direct anaphora;\n\n3. Otherwise, attempt the remaining DN heuristics in the order: proper names, descriptions established by relatives and PPs, proper name modification, predicative DDs occurring in copular constructions.\n\nIf none of these tests succeeds, the algorithm can either leave the DD unclassified, or classify it as DN. The automatically learned decision tree attempts direct anaphora resolution first. The overall results on the 195 DDs on which the automatically trained decision tree was tested are shown in  ", "filtered_refids": [[], [], ["b4"], ["b2"], [null], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2799, "num_references": 3}
{"corpusid_sectionid": "14642384-s5", "title": "DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL", "date": "2004-07-01", "section_title": "Ng and Cardie", "section": "The discourse-new detectors proposed by Ng and Cardie are statistical classifiers taking as input 37 features and trained using either C4.5 (Quinlan, 1993) or RIPPER (Cohen, 1995). The 37 features of a candidate anaphoric expression specify, in addition to much of the information proposed in previous work, a few new types of information about NPs.\n\n\u2022 The four boolean so-called LEXICAL features are actually string-level features: for example, str_match is Y if a preceding NP string-matches the anaphoric expression (except for the determiner), and head_match = Y if a preceding NP's head string-matches the anaphoric expression's. embedded=Y if the anaphoric expression is a prenominal modifier.\n\n\u2022 The second group of 11 (mostly boolean) features specifies the type of NP: e.g., pronoun is Y if the anaphoric expression is a pronoun, else N.\n\n\u2022 The third group of 7 features specifies syntactic properties of the anaphoric expression, including number, whether NP j is the first of two NPs in an appositive or predicative construction, whether NP j is pre-or post-modified, whether it contains a proper noun, and whether it is modified by a superlative.\n\n\u2022 The next group of 8 features are mostly novel, and capture information not used by previous DN detectors about the exact composition of definite descriptions: e.g., the_2n=Y if the anaphoric expression starts with determiner the followed by exactly two common nouns, the_num_n=Y if the anaphoric expression starts with determiner the followed by a cardinal and a common noun, and the_sing_n=Y if the anaphoric expression starts with determiner the followed by a singular NP not containing a proper noun.\n\n\u2022 The next group of features consists of 4 features capturing a variety of 'semantic' information, including whether a previous NP is an 'alias' of NP j , or whether NP j is the title of a person (the president).\n\n\u2022 Finally, the last three features capture information about the position in the text in which NP j occurs: the header, the first sentence, or the first paragraph.\n\nNg and Cardie's discourse-new predictor was trained and tested over the MUC-6 and MUC-7 coreference data sets, achieving accuracies of 86.1% and 84%, respectively, against a baseline of 63.8% and 73.2%, respectively. Inspection of the top parts of the decision tree produced with the MUC-6 suggests that head_match is the most important feature, followed by the features specifying NP type, the alias feature, and the features specifying the structure of definite descriptions.\n\nNg and Cardie discuss two architectures for the integration of a DN detector in a coreference system. In the first architecture, the DN detector is run first, and the coreference resolution algorithm is run only if the DN detector classifies that NP as anaphoric. In the second architecture, the system first computes str_match and alias, and runs the anaphoric resolver if any of them is Y; otherwise, it proceeds as in the first architecture. The results obtained on the MUC-6 data with the baseline anaphoric resolver, the anaphoric resolver augmented by a DN detector as in the first architecture, and as in the second architecture (using C4.5), are shown in Table 3. The results for all NPs, pronouns only, proper names only, and common nouns only are shown. 2 As indicated in the Table, running the DN detector first leads to worse results-this is because the detector misclassifies a number of anaphoric NPs as nonanaphoric. However, looking first for a same-head antecedent leads to a statistically significant improvement over the results of the baseline anaphoric resolver. This confirms the finding both of Vieira and Poesio and of Bean and Riloff that the direct anaphora should be called very early.\n\n2 It's not clear to us why the overall performance of the algorithm is much better than the performance on the three individual types of anaphoric expressions considered-i.e., which other anaphoric expressions are handled by the coreference resolver.  Table 3: Evaluation of the three anaphoric resolvers discussed by Ng and Cardie. Uryupina (2003) trained two separate classifiers (using RIPPER, (Cohen, 1995)): a DN detector and a UNIQUENESS DETECTOR, i.e., a classifier that determines whether an NP refers to a unique object. This is useful to identify proper names (like 1998, or the United States of America), semantic definites (like the chairman of Microsoft) and larger situation definite descriptions (like the pope). Both classifiers use the same set of 32 features. The features of an NP encode, first, of all, string-level information: e.g., whether the NP contains capitalized words, digits, or special symbols. A second group of features specifies syntactic information: whether the NP is postmodified, and whether it contains an apposition. Two types of appositions are distinguished, with and without commas. CONTEXT features specify the distance between the NP and the previous NP with the same head, if any. Finally, Uryupina's system computes four features specifying the NP's definite probability. Unlike the definite probability used by Bean and Riloff, these features are computed from the Web, using Altavista. From each NP, its head H and entire NP without determiner Y are determined, and four ratios are then computed:", "filtered_refids": [["b15", "b1"], [], [], [], [], [], [], [], [null], ["b1", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 30, "num_chars": 5284, "num_references": 5}
{"corpusid_sectionid": "14642384-s9", "title": "DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL", "date": "2004-07-01", "section_title": "How much does DN-detection help the Vieira / Poesio algorithm?", "section": "GUITAR (Poesio and Alexandrov-Kabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkov's algorithm for pronoun resolution (Mitkov, 1998). It is implemented in Java, takes its input in XML format and returns as output its input augmented with the anaphoric relations it has discovered. GUITAR has been implemented in such a way as to be fully modular, making it possible, for example, to replace the DD resolution method with alternative implementations. It includes a pre-processor incorporating a chunker so that it can run over both hand-parsed and raw text. A version of GUITAR without the DN detection aspects of the Vieira / Poesio algorithm was evaluated on the GNOME corpus (Poesio, 2000;, which contains 554 definite descriptions, of which 180 anaphoric, and 305 third-person pronouns, of which 217 anaphoric. The results for definite descriptions over hand-parsed text are shown in Table 6   Notice that although these results are not particularly good, they are still better than the results reported by Ng and Cardie for pronouns and definite NPs.", "filtered_refids": [["b6", "b10", "b13"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1162, "num_references": 3}
{"corpusid_sectionid": "237353268-s3", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Visualization", "section": "A simple way to discover the role of a neuron is by visualizing its activations and manually identifying the underlying concept over a set of sentences (Karpathy et al., 2015;Fyshe et al., 2015;Li et al., 2016a). Given that deep NLP models are \u2020 Table 3 in Appendix gives a more comprehensive list. trained using billions of neurons, it is impossible to visualize all the neurons. A number of clues have been used to shortlist the neurons for visualization, for example, selecting saturated neurons, high/low variance neurons, or ignoring dead neurons (Karpathy et al., 2015) when using ReLU activation function. \u2021 Limitation While visualization is a simple approach to find an explanation for a neuron, it has some major limitations: i) it is qualitative and subjective, ii) it cannot be scaled to the entire network due to an extensive human-in-the-loop effort, iii) it is difficult to interpret polysemous neurons that acquire multiple roles in different contexts, iv) it is ineffective in identifying group neurons, and lastly and v) not all neurons are visually interpretable. Visualization nevertheless remains a useful tool when applied in combination to other interpretation methods that are discussed below.", "filtered_refids": [["b11", "b22", "b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1216, "num_references": 3}
{"corpusid_sectionid": "237353268-s7", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Linear Classifiers", "section": "The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.\n\nLimitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.\n\nGaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.\n\nLimitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.", "filtered_refids": [["b21", "b38"], ["b15", "b55"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2535, "num_references": 4}
{"corpusid_sectionid": "237353268-s8", "title": "Neuron-level Interpretation of Deep NLP Models: A Survey", "date": "2021-08-30", "section_title": "Causation-based methods", "section": "The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts. They do not inherently reflect their importance towards the model's performance. Causation-based methods identify neurons with respect to model's prediction.\n\nAblation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.\n\nLimitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007). Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation . Nevertheless all these approaches are approximations and may incur search errors.\n\nKnowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.\n\nLimitation The attribution-based methods highlight salient neurons with respect to a prediction. What concepts these salient neurons have learned is unknown. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.", "filtered_refids": [[], ["b21", "b22"], [null, "b9", "b56"], ["b26", "b46", "b49", null, "b1"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2745, "num_references": 11}
{"corpusid_sectionid": "263835243-s3", "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances", "date": "2023-10-11", "section_title": "Knowledge Editing", "section": "Since tuning LLMs to learn new knowledge can be prohibitively expensive (Patterson et al., 2021), researchers seek efficient methods to directly update more specific, localized, or fine-grained knowledge that is preserved in LLMs (Mitchell et al., 2022a). Knowledge editing (KE) is an arising and promising research area that aims to alter the parameters of some specific knowledge stored in pre-trained models so that the model can make new predictions on those revised instances while keeping other irrelevant knowledge unchanged (Sinitsin et al., 2020;De Cao et al., 2021;Mitchell et al., 2022a;Meng et al., 2022a;Hase et al., 2023b;.\n\nIn this section, we categorize existing methods into meta-learning, hypernetwork, and locate-and-edit -based methods.\n\nMeta-learning. This line of work generally focuses on the intrinsic editability of the model itself, aiming to modify the model parameters so that they can be easily updated during inference (De Cao et al., 2021;Mitchell et al., 2022a  concept and propose a gradient-based knowledge attribution method to identify these knowledge neurons in FFNs. Further, without fine-tuning, they directly modify the corresponding value slots (e.g., embeddings) in the located knowledge neurons and successfully update or delete knowledge, demonstrating a preliminary potential to edit knowledge in LMs. Other.  propose an evaluation framework and dataset for measuring the effectiveness of knowledge editing of LLMs, as well as the ability to reason with the altered knowledge and cross-lingual knowledge transfer. Similarly, Cohen et al. (2023) evaluate the implications of an edit on related facts and show that existing methods fail to introduce consistent changes in the model's knowledge. Ju and Zhang (2023) propose an evaluation benchmark for locate-and-edit-based methods, aiming to reassess the validity of the locality hypothesis of factual knowledge.  and  take multilingual into account and extend existing knowledge editing methods into cross-lingual scenarios.", "filtered_refids": [[null, "b31"], [], [null, "b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 2018, "num_references": 4}
{"corpusid_sectionid": "263835243-s4", "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances", "date": "2023-10-11", "section_title": "Continual Learning", "section": "Continual learning (CL) aims to enable a model to learn from a continuous data stream across time while reducing catastrophic forgetting of previously acquired knowledge (Biesialska et al., 2020). With CL, a deployed LLM has the potential to adapt to the changing world without costly re-training from scratch (Bubeck et al., 2023). In this section, we introduce approaches that employ CL for aligning LLMs with the current world knowledge, including continual pre-training and continual knowledge editing.\n\nContinual Pre-training. Unlike traditional continual learning, which sequentially fine-tunes a pre-trained LM on some specific downstream tasks (e.g., QA, text classification), continual pretraining is used to further pre-train an LM to acquire new knowledge, where the data corpus is usually unsupervised (Gururangan et al., 2020;Ke and Liu, 2023). Since our target is the versatile foundation LLMs (e.g., GPT-4) that can be applied to many different use cases rather than a fine-tuned model designed for a specific task, we focus on the literature on continual pre-training.\n\nEarly works (Gururangan et al., 2020;R\u00f6ttger and Pierrehumbert, 2021;Lazaridou et al., 2021;Dhingra et al., 2022)  1 Regularization. To mitigate forgetting, regularization-based methods apply regulations to penalize the changes of the critical parameters learned from previous data. Chen et al. (2020) improve the traditional EWC (Kirkpatrick et al., 2017) by recalling previously learned knowledge through the pre-trained parameters, and the method continually learns new information using a multitask learning objective.  compute the importance of each unit (i.e., attention head and neuron) to the general knowledge in the LM using a proxy based on model robustness to preserve learned knowledge. When continually learning new domains, the approach prevents catastrophic forgetting of the general and domain knowledge and encourages knowledge transfer via soft-masking and contrastive loss.\n\n2 Replay. These methods generally reduce forgetting by replaying previous training data when learning new data. Assuming that the initial pretraining corpus is available, He et al. (2021b) use a gradual decay mix-ratio to adjust the quantity of the pre-training corpus mixed in the new data when learning sequentially. ELLE  and CT0 (Scialom et al., 2022) also mix the old data while learning new data. However, ELLE starts the pre-training from a newly initialized and relatively small BERT ( ", "filtered_refids": [[null], [null], [null, "b34", "b27"], [null]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 14, "num_chars": 2475, "num_references": 6}
{"corpusid_sectionid": "264490542-s3", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Paper Databases", "section": "We source papers from eminent databases in the fields of NLP, the rest of CS, and medicine, as these are integral knowledge areas in the study of mental health CA.These databases include the ACL Anthology (referred to as ACL throughout this paper)2 , AAAI3 , IEEE4 , ACM5 , and PubMed6 .ACL is recognized as a leading repository that highlights pioneering research in NLP.AAAI features cuttingedge studies in AI.IEEE, a leading community, embodies the forefront of engineering and technology research.ACM represents the latest trends in Human Computer Interaction (HCI) along with several other domains of CS.PubMed, the largest search engine for science and biomedical topics including psychology, psychiatry, and informatics among others provides extensive coverage of the medical spectrum.\n\nDrawing on insights from prior literature reviews (Valizadeh and Parde, 2022;Montenegro et al., 2019;Laranjo et al., 2018) and discussion with experts from both the CS and medical domains, we opt for a combination of specific keywords.These search terms represent both our areas of focus: conversational agents (\"conversational agent\", \"chatbot\") and mental health (\"mental health\", \"depression\").Furthermore, we limit our search criteria to the paper between 2017 to 2022 to cover the most recent articles.We also apply the \"research article\" filter on ACM search, and \"Free Full Text or Full Text\" for PubMed search.Moreover, we manually add 3 papers recommended by the domain experts (Fitzpatrick et al., 2017;Laranjo et al., 2018;Montenegro et al., 2019).This results in 534 papers.", "filtered_refids": [[], ["b69", "b132", "b44", "b88"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1580, "num_references": 4}
{"corpusid_sectionid": "264490542-s8", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Target Demographic", "section": "Most of the papers (>65%) do not specify the target demographic of users for their CAs.The target demographic distribution is shown in Table 4.An advantage of the models proposed in these papers is that they could potentially offer support to a broad group of users irrespective of the underlying mental health condition.Papers without a target demographic and a target mental health category focus on proposing methods such as using generative language models for psychotherapy (Das et al., 2022a), or to address specific modules of the CAs such as leveraging reinforcement learning for response generation (Saha et al., 2022b).\n\nOn the other hand, 31% papers focus on one specific user group such as young individuals, students, women, older adults, etc, to give advanced assistance.Young individuals, including adolescents and teenagers, received the maximum attention (Rahman et al., 2021).Several papers also focus on the mental health care of women, for instance in prenatal and postpartum women (Green et al., 2019;Chung et al., 2021) and sexual abuse survivors (Maeng and Lee, 2022;Park and Lee, 2021).Papers targeting older adults are mainly designed for companionship and supporting isolated elders (Sidner et al., 2018;Razavi et al., 2022).", "filtered_refids": [["b114", "b29"], ["b79", "b21", null, "b123", "b110", "b106"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 5, "num_chars": 1251, "num_references": 8}
{"corpusid_sectionid": "264490542-s9", "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives", "date": "2023-10-25", "section_title": "Model Technique", "section": "Development of Large Language Models such as GPT-series (Radford et al., 2019;Brown et al., 2020) greatly enhanced the performance of generative models, which in turn made a significant impact on the development of CAs (Das et al., 2022b;Nie et al., 2022).However, as shown in Table 5, LLMs are yet to be utilized in the development of mental health CAs (as of the papers reviewed in this study), especially in medicine.No paper from PubMed in our final list dealt with generative models, with the primary focus being rule-based and retrieval-based CAs.\n\nRule-based models operate on predefined rules and patterns such as if-then statements or decision trees to match user inputs with predefined responses.The execution of Rule-based CAs can be straightforward and inexpensive, but developing and maintaining a comprehensive set of rules can be challenging.Retrieval-based models rely on a predefined database of responses to generate replies.They use techniques like keyword matching (Daley et al., 2020), similarity measures (Collins et al., 2022), or information retrieval (Morris et al., 2018)  and Eliza (Weizenbaum, 1966).\n\ntures.While they can often generate more diverse and contextually relevant responses compared to rule-based or retrieval-based models, they could suffer from hallucination and inaccuracies (Azaria and Mitchell, 2023).", "filtered_refids": [["b105", "b30", "b16", "b93"], ["b22", "b139", "b26", "b89"], ["b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 7, "num_chars": 1347, "num_references": 9}
{"corpusid_sectionid": "264305746-s5", "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis", "date": "2023-10-18", "section_title": "Affect or Feeling", "section": "Author's disposition towards a specific theme (Birjali et al., 2021) 3-D polarity Framework with 3 dimensions of polarities: Subjective\\Objective, Positive\\Negative, Strength (Sebastiani and Esuli, 2006) Emoticons Emoticons as sentiment indicators (Lou et al., 2020) Object's orientation Measure of the attitude towards individual aspects of an entity (Mowlaei et al., 2020) Implicit Emotional tendencies implied by commonsense knowledge of the effect of concepts or events (Zhang and Liu, 2011) Human Annotation Sentiment ratings collected from experts or crowd-sourced data collection (Kenyon-Dean et al., 2018) Table 2: Frameworks of Sentiment and corresponding definitions in Sentiment Analysis (2002) experimented with using the semantic orientation of words to find whether product reviews are positive or negative.Readily available data in the form of product reviews on e-commerce websites influenced early SA works and firmly established it to almost exclusively mean opinion mining, with sentiment defined as: 'overall opinion towards the subject matter' (Pang et al., 2002).Following this, Read (2005) proposed the use of emoticons as a proxy for ground truth data to measure sentiment in text.They defined SA as the method to 'identify a piece of text according to its author's general feeling toward their subject, be it positive or negative.'This marked a stark deviation of SA from 'opinion mining.'This expansion of the meaning of sentiment can also be seen in the work of Wilson et al. (2005b) where they defined SA as 'the task of identifying positive and negative opinions, emotions, and evaluations '. Subsequently, Sebastiani and Esuli (2006) proposed that SA consists of three dimensions: subjective-objective polarity, positive-negative polarity, and strength of polarity.\n\nThe first use of SA as a sociotechnical system is marked by Go et al. (2009)'s approach to train a SA model using data from a social media platform, namely Twitter.While most prior work still treated SA as a method to extract an author's subjective or objective opinion regarding an entity or an object, Go et al. (2009) defined sentiment from the perspective of a general feeling or emotion in text.Their definition of sentiment as 'a personal positive or negative feeling or opinion', is a marked deviation that influenced much of the literature in SA.Maas et al. (2011)'s work recognized sentiment as a 'complex, multi-dimensional concept' and attempted to operationalize it through a vector representation.Similarly, Zhang and Liu (2011) defined sentiment as an 'emotional tendency im-plied by commonsense knowledge of the effect of concepts or events' to define an implicit form of sentiment.To quantify sentiment from a 'human perspective', Kenyon-Dean et al. (2018) used human annotation, as a methodology to define and measure sentiment, using crowd-sourced data.\n\nTable 2 tabulates the multifarious frameworks encountered in SA.Here we see that SA does not follow a well-defined comprehensive framework.With the evolution of the field, different researchers adapted SA in dissimilar ways while not making a clear distinction between concepts such as emotions, opinions, and attitudes.We posit that there is a need for a nuanced, socially informed, and theoretically motivated framework for sentiment in SA.\n\nTo understand sentiment from an interdisciplinary perspective and draw out an interdisciplinary framework, we examine its meaning from a sociological perspective.", "filtered_refids": [["b115", "b140", "b92", "b107", "b179", "b258", "b80", null, "b125", "b169"], ["b80", "b54", "b179", "b234"], [null], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 3476, "num_references": 15}
{"corpusid_sectionid": "264305746-s6", "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis", "date": "2023-10-18", "section_title": "The Social Perception of Sentiment", "section": "A notable distinction exists between computational and psycho-linguistic perspectives on sentiment.In psychology, sentiment is often defined as \"socially constructed patterns of sensations, expressive gestures, and cultural meanings organized around a relationship to a social object, usually another person or group such as a family.\"(Gordon, 1981).While sentiment is most commonly categorized as positive, negative, or neutral in computational literature, it encompasses a broader spectrum, ranging from mild to intense (Taboada, 2016;Jo et al., 2017).Furthermore, sentiment (in psychology) is captured through physiological indicators, like facial expressions and heart rate variability (Wiebe et al., 2005;Plutchik, 2001).\n\nPsychological research widely recognizes that a simplistic positive-negative dichotomy is inade-quate for capturing the intricate range of human emotions (Hoffmann, 2018).This is evident in the distinction between seemingly negative emotions such as sadness and fear, which exhibit significant differences in their physiological and psychological effects (Plutchik, 2001).\n\nWe have seen that three primary and interrelated themes are commonly linked to sentiment: opinions, emotions/feelings, and subjectivity.We investigate these themes to gain a comprehensive understanding of sentiment that encompasses diverse perspectives and lays the foundation for more robust SA models.\n\nOpinions: From a psychological perspective, opinion is an individual's stance regarding an object or issue, formed after an evaluation through their own lens or perspective (Vaidis and Bran, 2019).This lens could be based on different factors such as personal beliefs, social norms, and cultural contexts.Liu (2012) also define an opinion a \"a subjective statement, view, attitude, emotion, or appraisal about an entity or an aspect of an entity from an opinion holder.\"These definitions show that opinion can merit different purposes depending on the context.\n\nFeelings/Emotions: Izard (2010) posit that the word emotion has both a descriptive definition i.e. based on its use in everyday life and a prescriptive definition i.e. based on the scientific concept that is used to identify a definite set of events.Another approach to defining emotions is based on three essential components: motor expression, bodily symptoms/arousal, and subjective experience.There is substantial agreement that motivational consequences and action tendencies associated with emotion are key aspects of emotion rather than just the level of arousal of the subject (Frijda et al., 1986;Frijda, 1987).\n\nSubjectivity: Banfield (2014) referred to sentences that take a character's psychological point of view as subjective, contrasted against sentences that narrate an event in a definite but yielding manner.Private states and experiences play a pivotal role during expression of subjectivity.Here private states could refer to intellectual factors, such as believing, wondering, knowing; or emotive factors, such as hating, being afraid; and perceptual ones, such as seeing or hearing something (Wiebe, 1994).Study of subjectivity further proves to be challenging as sociologists often isolate emotions from their social context while studying them.\n\nTerms like opinion, emotion, and subjectivity hold distinct meanings and are studied separately.Therefore, they are not synonymous with sentiment.Furthermore, when considering sentiment within a sociotechnical system, it is essential to be aware of the contextual nuances associated with the diverse definitions of sentiment derived from sociological, psychological, and linguistic backgrounds.Given the complex nature of sentiment, it is important to approach it with a nuanced perspective and operationalize it within a structured theoretical framework.Prior research suggests that achieving such nuanced understanding can be facilitated through engaging in dialogue with other fields such as psychology, and cognitive science (Head et al., 2015;Cambria et al., 2022).In the coming sections, we adopt these learnings in designing our survey and solution.", "filtered_refids": [["b57", "b78", "b149", "b235", "b117"], ["b65", "b117"], [], ["b155", null], ["b48", "b47"], ["b167"], ["b64", "b25"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 4095, "num_references": 14}
{"corpusid_sectionid": "264305746-s14", "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis", "date": "2023-10-18", "section_title": "Finance", "section": "Applications developed to comprehend the patterns and dynamics of financial management, creation, and investment analysis.\n\nTable 3: List of applications, defined through thematic analysis, their corresponding definitions, and frequency of papers categorized to the groups.\n\nanalysis (Vaismoradi et al., 2013) to uncover the various applications of SA.Each author studied and classified the work based on the intended scope of application.To ensure accuracy and prevent misclassification, this recursive process was employed.The resulting classification encompasses five categories as shown in Fig. 2 and Table 3 1 .Notably, the Health and Medicine domain emerged as the most prominent application area for SA where studies leverage SA to understand individual reactions in diverse medical scenarios (Rodrigues et al., 2016).Following closely, Government and Policy Making emerged as the second most prevalent category, where sentiment analysis plays a pivotal role in comprehending human behavior in governance solutions (Joyce and Deng, 2017).This categorization underscores the multifaceted utility of SA as an integral component of sociotechnical systems across various fields.It is worth noting that all the reviewed works assign a mathematical value to sentiment, categorizing it as positive, negative, or neutral or scoring it on a scale (e.g., -1 to +1).Most of the reviewed works lack clear definitions of sentiment or SA.Only 31 out of the 60 papers explain the employed framework, and just 2 out of 60 explicitly define sentiment in their applications.Only one takes an interdisciplinary perspective, defining sentiment in the context of finance for understanding market behavior (Kraaijeveld and De Smedt, 2020).Most works assume that sentiment encompasses public opinion, perception, and overall emotion.Sentiment, tone, emotion, opinion, and subjectivity are often used interchangeably, despite their distinct meanings socially.\n\nThe lack of precise sentiment definitions can result in misrepresented measurements.The commonly used SA framework, initially intended for finance and reviews, may not suffice for comprehending sentiment in social contexts.Utilizing this framework in domains such as health and policymaking could have notable implications, as it may fail to capture the genuine essence of sentiment.", "filtered_refids": [[], [], ["b79", "b128", "b197", "b156"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 2327, "num_references": 4}
{"corpusid_sectionid": "258832362-s3", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Word Representations", "section": "A well-known challenge in NLP is creating continuous dense vector representations of words in high-dimensional spaces to capture their semantic and syntactic meaning. The most widely used algorithm for creating word embeddings is Word2Vec (Mikolov et al., 2013). Traditional approaches to representing words before Word2Vec, like one-hot encoding or bag-ofwords, have a number of drawbacks: They require a lot of memory to hold sparse vectors and fail to capture the links between words or their meaning. By using a neural network to learn word embeddings, Word2Vec solved these issues. The model trains neural networks using a large corpus of text as input to predict the likelihood of a word given its context or vice versa. The weights of the network are changed during training to reduce the discrepancy between the expected and actual probabilities. The network weights are employed as the word embeddings after training is finished. It has been widely used and inspired other models such as GloVe (Pennington et al., 2014) and fastText (Joulin et al., 2016).", "filtered_refids": [["b15", "b8", "b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1064, "num_references": 3}
{"corpusid_sectionid": "258832362-s7", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Natural Language Inference", "section": "Natural Language Inference (NLI) is the process of determining the logical relationship between a premise (an assumed true sentence) and a hypothesis (a possibly true sentence). The objective of NLI is to determine whether the hypothesis can be logically inferred from the premise (entailment), contradicts the premise (contradiction), or is neutral with respect to it (Dagan et al., 2013). NLI serves as a proxy for evaluating natural language understanding. According to Conneau et al. (2017), learning sentence representations using NLI data can be effectively transferred to other NLP tasks, demonstrating the generality of this approach.\n\nIn \u00a7 2.3, we discussed Siamese-BERT networks as presented in Reimers and Gurevych (2019). There are two noteworthy components to this model. First, processing inputs individually without promoting interaction between words; second, using an encoder like BERT that is not generative as its backbone model. The first component is computationally efficient but has been found to result in poorer performance compared to methods that promote interaction between words (Reimers and Gurevych, 2019). This lack of interaction can limit the network's ability to capture the nuances of language, and may result in less accurate sentence embeddings. In order to solve this, Cheng (2021) incorporate word-level interaction features into the sentence embedding while maintaining the efficiency of Siamese-BERT networks. Their approach makes use of ideas from knowledge distillation (Hinton et al., 2015): using the rich knowledge in pretrained cross-encoders and significantly improving the performance of Siamese-BERT.\n\nMeanwhile, generative models have been pretrained on huge amounts of text data, and can perform a myriad of tasks. Ni et al. (2022) examined the use of generative models as backbone for extracting sentence embeddings. They ex-plore three methods using pre-trained T5 encoderdecoder models: using the representation of the first token of the encoder, using the representation of the first generated token of the decoder, or using the mean of the representations from the encoder. This is one of the first studies that shows the utility of generative models for obtaining sentence representations.", "filtered_refids": [[null], ["b22", "b4"], ["b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 15, "num_chars": 2248, "num_references": 4}
{"corpusid_sectionid": "258832362-s11", "title": "Beyond Words: A Comprehensive Survey of Sentence Representations", "date": "2023-05-22", "section_title": "Surface Level", "section": "To create a sentence that carries the same meaning as another, one can modify the words or characters in a way that retains the sentence's semantic value. Recent research Wu et al., 2022d) suggests certain transformations that preserve the semantic meaning.  propose randomly flipping the case of some tokens, while  mask spans of tokens to get positive instances and Wu et al. (2022d) suggest to repeat certain words or subwords. Representations generated by transformer networks are biased towards the frequency of tokens, the case of words and subwords, and the length of the sentence (Wu et al., 2022d). For example, researchers found that avoiding to use high-frequency tokens can result in better sentence representations . These transformation help in overcoming such biases.\n\nHowever altering the surface characteristics of sentences can lead to models relying on shortcuts rather than learning the semantics of the sentences (Du et al., 2021). To address this issue, Wu et al. (2022a) propose the use of multiple augmentation strategies rather than a single transformation. They use shuffling, repeating, and dropping words as transformation strategies to improve model robustness. Additionally, they implement mechanisms to enhance learning from multiple positive examples.", "filtered_refids": [["b37"], [null, "b34"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 10, "num_chars": 1283, "num_references": 3}
{"corpusid_sectionid": "3254638-s1", "title": "A Critical Survey of the Methodology for IE Evaluation", "date": 2004, "section_title": "IE Evaluation Methodology", "section": "The MUC conferences can be considered the starting point of the IE evaluation methodology as currently defined. The MUC participants borrowed the Information Retrieval concepts of precision and recall for scoring filled templates. Given a system response and an answer key prepared by a human, the system's precision was defined as the number of slots it filled correctly, divided by the number of fills it attempted. Recall was defined as the number of slots it filled correctly, divided by the number of possible correct fills, taken from the human-prepared key. All slots were given the same weight. F-measure, a weighted combination of precision and recall, was also introduced to provide a single figure to compare different systems' performances.\n\nApart from the definition of precise evaluation measures, the MUC conferences made other important contributions to the IE field: the availability of large amount of annotated data (which have made possible the development of Machine Learning based approaches), along with the evaluation software (i.e., the MUC scorer (Douthat, 1998)), the emphasis on domain-independence and portability, and the identification of a number of different tasks which can be evaluated separately (Hirschman, 1998).\n\nIt should be noticed that MUC evaluation concentrated mainly on IE from relatively unrestricted text, i.e. newswire articles. In independent efforts, other researchers developed and made available annotated corpora developed from somewhat more constrained texts. Califf compiled and annotated a set of 300 job postings from the Internet (Califf, 1998), and Freitag compiled corpora of seminar announcements and university web pages, as well as a corporate acquisitions corpus from newswire texts (Freitag, 1998). Several of these corpora are available from the RISE repository (RISE, 1998) where a number of tagged corpora have been made available by researchers in Machine Learning for IE. Freitag (1998) uses the term Information Extraction in a more restricted sense than MUC. In the Seminar Announcement collection, the templates are simple and include slots for the seminar speaker, location, start time, and end time. This is in strong contrast with what happened in MUC where templates might be nested (i.e., the slot of a template may take another template as its value), or there might be several templates from which to choose, depending on the type of document encountered. In addition, MUC domains include irrelevant documents which a correctly behaving extraction system must discard. A template slot may be filled with a lower-level template, a set of strings from the text, a single string, or an arbitrary categorical value that depends on the text in some way (a so-called \"set fill\"). Califf (1988) takes an approach that is somewhat inbetween Freitag's approach and more complex MUC extraction tasks. All of the documents are relevant to the task, and the assumption is that there is precisely one template per document, but that many of the slots in the template can have multiple fillers.\n\nAlthough the tasks to be accomplished are different, the methodology adopted by (Freitag, 1998) and (Califf, 1998) is similar to the one used in the MUC competition: precision, recall, and F-measure are employed as measures of the performances of the systems.", "filtered_refids": [[], ["b12", "b8"], [null, "b9", "b3"], ["b9", "b3"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3322, "num_references": 7}
{"corpusid_sectionid": "3254638-s2", "title": "A Critical Survey of the Methodology for IE Evaluation", "date": 2004, "section_title": "Problematic Issues in IE Evaluation", "section": "In Section 2. we have summarized the current status of the methodology adopted in IE. However, the definition of an evaluation methodology and the availability of standard annotated corpora do not guarantee that the experiments performed with different approaches and algorithms proposed in the literature can be reliably compared. Some of the problems are common to other NLP tasks (e.g., see (Daelemans and Hoste, 2002)): the difficulty of exactly identifying the effects on performances of the data used (the sample selection and the sample size), of the information sources used (the features selected), and of the algorithm parameter settings.\n\nOne of the most relevant issues is that of the exact split between training set and test set, considering both the numerical proportions between the two sets (e.g., a 50/50 vs. a 80/20 split) and the procedure adopted to partition the documents (e.g., n repeated random splits vs. n-fold crossvalidation).\n\nFurthermore, the question of how to formalize the learning-curve sampling method and its associated costbenefit trade-off may cloud comparison further. For example, the following two approaches have been used: (1) For each point on the learning curve, train on some fraction of the available data and test on the remaining fraction; or (2) Hold out some fixed test set to be used for all points on the learning curve. The second approach is generally preferable: with the first procedure, points on the \"high\" end of the learning curve will have a larger variance than points on the \"low\" end.\n\nAnother important issue concerns the features used by the algorithm and their contribution to the performances of the algorithm. In IE, for instance, it would be relevant to extensively investigate the effectiveness of the use of simple orthographic features with respect to the use of more complex linguistic features such as PoS tags or semantic labels extracted from gazetteers (Ciravegna, 2001b).\n\nApart from those problematic issues mentioned above, there are some others that are specific to IE evaluation. A first issue concerns how to deal with issues related to tokenization, which is often considered something obvious and non problematic but it is not so and can affect the performance of the IE algorithms.\n\nA second issue is related to how to evaluate an extracted fragment -e.g., if an extra comma is extracted should it count as correct, partial or wrong? This issue is related to the question of how relevant is the exact identification of the boundaries of the extracted items. (Freitag, 1998) proposes three different criteria for matching reference instances and extracted instances:\n\nExact The predicted instance matches exactly an actual instance.", "filtered_refids": [["b6"], [], [], ["b5"], [], ["b9"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2720, "num_references": 3}
{"corpusid_sectionid": "3254638-s3", "title": "A Critical Survey of the Methodology for IE Evaluation", "date": 2004, "section_title": "Contains", "section": "The predicted instance strictly contains an actual instance, and at most k neighboring tokens.\n\nOverlap The predicted instance overlaps an actual instance.\n\nEach of these criteria can be useful, depending on the situation, and it can be interesting to observe how performance varies with changing criteria. (De Sitter and Daelemans, 2003) mention such criteria and present the results of their algorithm for all of them.\n\nA third issue concerns which software has been used for the evaluation. The only publicly available tool for such aim is the MUC scorer. Usually IE researchers have implemented their own scorer, relying on a number of implicit assumptions that have a strong influence on performance's evaluation.\n\nWhen multiple fillers are possible for a single slot, there is an additional ambiguity -usually glossed over in papers -that can influence performance. For example, (Califf and Mooney, 2003) remark that there are differences in counting between RAPIER (Califf, 1998), SRV (Freitag, 1998), and WHISK (Soderland, 1999). In his test on Job Postings (Soderland, 1999) does not eliminate duplicate values. When applied to Seminar Announcements SRV and RAPIER behave differently: SRV assumes only one possible answer per slot, while RAPIER makes no such assumption since it allows for the possibility of needing to extract multiple independent strings.\n\nDe Sitter and Daelemans (2003) also discuss this question and claim that in such cases there are two different ways of evaluating performance in extracting slot fillers: to find all occurrences (AO) of an entity (e.g. every mention of the job title in the posting) or only one occurrence for each template slot (one best per document, OBD). The choice of one alternative over the other may have an impact on the performance of the algorithm. (De Sitter and Daelemans, 2003) provide results for the two alternative ways of evaluating performances. This issue is often left underspecified in papers and, given the lack of a common software for evaluation, this further amplifies the uncertainty about the reported results.\n\nNote that there are actually three ways to count:\n\none answer per slot (where \"2pm\" and \"2:00\" are considered one correct answer)\n\none answer per occurrence in the document (each individual appearance of a string to be extracted in the document where two separate occurrences of \"2pm\" would be counted separately)\n\none answer per different string (where two separate occurrences of \"2pm\" are considered one answer, but \"2:00\" is yet another answer)\n\nFreitag takes the first approach, Soderland takes the second, and Califf takes the third.\n\nTo summarize, an information extraction task should specify all of the following: 1. A set of fields to extract.\n\n2. The legal numbers of fillers for each field, such as \"exactly one value\", \"zero or one values\", \"zero or more values\", or \"one or more values\". For example, in Seminar Announcements, the fields stime, etime and location are \"0-1\", speaker is \"1+\"; for Job Postings, title is \"0-1 or 0+\", required programming languages is \"0+\", etc. Thus, in the following seminar announcement:\n\nSpeakers will be Joel S. Birnbaum and Mary E.S. Loomis.\n\nif the task specifies that there should be one or more speaker, then to be 100% correct the algorithm must extract both names, while if the task specifies that zero or more speakers are allowed, then extracting either name would result in 100% correct performance.\n\n3. The possibility of multiple varying occurrences of any particular filler. For example, a seminar announcement with 2 speakers might refer to them each twice, but slightly differently: ", "filtered_refids": [[], [], ["b7"], [], ["b14", "b9", "b2", "b3"], ["b7"], [], [], [], [], [], [], [], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3637, "num_references": 6}
{"corpusid_sectionid": "260063224-s7", "title": "How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques", "date": 2023, "section_title": "Complete Training", "section": "Betti et al. (2020) propose a text GAN composed of one generator and two discriminators. The generator is a Relational Memory with self-attention (Santoro et al., 2018) with the objective to generate text consistent with the specified control attribute. The syntax discriminator distinguishes between real and generated sentences, while the semantic discriminator assesses whether the generated sentence expresses the control attribute, e.g. positive sentiment. To solve the well-known problem of differentiation in GANs applied to text, the Gumbel-softmax trick (Jang et al., 2016) is applied. This approach enables control only for one attribute at a time and it has been evaluated on sentiment and topic control.\n\nIn order to enable multi-attribute control, Qiao et al. (2020) propose a Sentiment-Controllable topic-to-essay generator that deploys a Conditional Variational Auto-Encoder in adversarial training. The model simultaneously controls the topics of the essay and the sentiment of each sentence composing the essay. The topic control is achieved using a Topic Graph Attention, which includes a topic knowledge graph in the generation process. Sentiment control is achieved by injecting the sentiment representation both in the encoder and the decoder.\n\nIn a different direction, Xie et al. (2022) propose a psychology-guided story generation method that controls storytelling as the protagonist's psychological state changes. This technique enables multi-attribute control considering the protagonist of the story (Character), their chain of emotions (Emotion), and chain of needs (Need) representing the evolution of the psychological state of the protagonist. The model is an encoder-decoder architecture with the addition of psychology controllers designed to integrate the local and global psychological state into the story context representation.", "filtered_refids": [["b13"], ["b8"], ["b20"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 12, "num_chars": 1865, "num_references": 3}
{"corpusid_sectionid": "260063224-s10", "title": "How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques", "date": 2023, "section_title": "Modification of Token Distribution", "section": "Dathathri et al. (2019) propose a Plug and Play Language Model (PPLM) which uses external attribute classifiers to guide text generation without requiring any training of the PLM. The PLM is used to obtain the next token distribution, which is fed to external classifiers, called Attribute Models, to assess whether the token correctly expresses the desired attributes. The internal latent representations of the LM are updated with a backward pass using the gradients of the attribute models to increase the likelihood of the desired attributes. Finally, the next token distribution is recomputed taking into account the updated latent representations. This model allows control of multiple attributes at a time, such as sentiment and topic.\n\nInspired by this work, Madotto et al. (2020) propose a variation of PPLMs in which the backward pass is executed n times depending on the desired intensity of the control attribute. Furthermore, they add Residual Adapters (Houlsby et al., 2019) on top of each transformer layer to steer the PLM output distribution without changing its parameters.\n\nGoswamy et al. (2020) propose a different variation of PPLMs based on GPT-2, in which a modified loss is considered to take into account the intensity of the controlled sentiment. Furthermore, instead of considering only positive/negative sentiment, control over 8 emotion categories is enabled.\n\nStarting from PPLMs, Gu et al. (2022a) observe that using a controller alone leads to the trade-off problem, i.e. the controller used to modify the token distribution only focuses on how to make the prefix related to the desired attribute without taking into account the original distribution of the LM. In this way, the controller takes over the LM's control for the next token distribution. In order to alleviate  this problem, they propose a weighted decoding method that adds a regulator module that permits fine-grained adjustment of a bias signal from the controller. At every step, the regulator detects differences between the PLM distribution and the target attribute and it determines whether to suppress or amplify the bias signal. This method is model agnostic and has been evaluated with sentiment, topic, and toxicity attributes. The last two methods propose sampling procedures that can be applied to any LM. Landsman et al. (2022) propose to modify beam search by reweighing the token candidate likelihoods to control different attributes. Diverse beam search (Vijayakumar et al., 2016) is used to decode k candidates, which are then scored using an attribute model. The obtained scores are used to reweigh the original likelihoods to produce a reweighed candidate distribution that considers both fluency and attribute characteristics. The resulting distribution is used to sample the next token.\n\nLastly, Kumar et al. (2022) propose a sampling method combining LM log-likelihoods with arbitrary constraints in a single energy function generating samples in a non-autoregressive manner. The idea is to use a PLM without changing its distribution but sampling from it considering different constraints, i.e. control attributes. The constraints are discriminative classifiers trained from scratch or fine-tuned. This method allows multi-attribute control (sentiment and toxicity).", "filtered_refids": [[], [], [], [null, "b18"], ["b24"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3285, "num_references": 3}
{"corpusid_sectionid": "260063224-s11", "title": "How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques", "date": 2023, "section_title": "Hybrid", "section": "Hybrid techniques combine two or more Control Implementation techniques. One possibility is to combine Complete Training and Fine-Tuning, for example, designing a model composed of different modules in which some modules are trained from scratch and some are fine-tuned models. In this context, Tian et al. (2022) propose a conversa-tion model that generates empathetic responses and guides the mood of the conversation in a positive direction while acknowledging the user's emotion. The idea is to extract the sentiment from the conversation context using a fine-tuned sentiment evaluator and use both the context and the extracted sentiment to steer the generation of the next response by generating a responding strategy that will be used by the Conditional Conversation model to generate the final response. The proposed method enables only single-attribute control (of sentiment).\n\nAnother way to enable controllability using a hybrid technique is to combine Fine-Tuning and Modification of Token Distribution.  propose a technique to control Story Generation by fine-tuning an encoder that learns the representation of new special tokens identifying the control attributes, thus allowing the model to properly include this information in the generation process. The next token distribution is obtained by combining the decoder distribution and the attention distribution, which allows the model to copy important information from the specified control attributes. The model allows fine-grained control taking into account the characters of the story with their actions and emotions.\n\nIn contrast to  who learn the representation of special tokens during fine-tuning, Liu et al. (2021) propose to modify an LM's token distribution including two fine-tuned versions of the PLM: an expert, focused on the desired attribute, and an anti-expert, focused on the opposite of the desired attribute. The next token distribution is obtained by subtracting the anti-expert distribution from the expert one and combining the result with the distribution of the frozen PLM to maintain fluency. This method enables the control only of one control attribute at a time and it has been tested on sentiment and toxicity attributes.\n\nSimilarly, Krause et al. (2021) propose to con-  trast the desired control attribute and its opposite. Instead of fine-tuning specialised LMs for each attribute, GPT-2 is fine-tuned with control codes to obtain a Class-Conditional LM (CCLM). At each time step, the generation is guided by computing classification probabilities for all possible next tokens via the Bayes rule by normalizing two classconditional distributions: conditioned on the desired attribute and conditioned on the undesired attribute. Like the previous method, it allows the control of one attribute at a time and has been evaluated using sentiment, topic, and toxicity attributes. Liu et al. (2022) also use a CCLM which is finetuned using an external discriminator to generate texts with the desired attributes, supporting multiattribute control. The token distribution is modified based on a contrastive generator that learns effective representations by bringing together positive samples, i.e. samples with desired attributes, and separating negative samples, i.e. samples without desired attributes. The obtained distribution is combined with the distribution of a PLM to maintain the fluency of the generated text. The generated text is fed to the external discriminator to assess whether it contains the desired attributes or not. The model has been tested on the joint control of sentiment and topic.  explore the contrast between desired and undesired attributes proposing a fine-tuned LM incorporating the attribute knowledge of a discriminator, similarly to Liu et al. (2022), to optimize continuous virtual tokens called control-prompts. The learned control-prompts are used as prefixes to steer a fixed conditional LM to generate attribute-specific texts. The LM is finetuned using (i) likelihood training, encouraging the LM to generate tokens with higher probability as scored by the discriminator assessing the desired attribute, and (ii) unlikelihood training, keeping the generated tokens away from lower-probability candidates.", "filtered_refids": [["b16"], [], ["b21"], [null, "b5"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 24, "num_chars": 4241, "num_references": 4}
{"corpusid_sectionid": "264832783-s1", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Background and Notation", "section": "We refine the definition in Wei et al. (2022b), and specify the emergent abilities as the abilities that LLMs can leverage to achieve satisfactory results2 across diverse tasks, with only a few examples or chain-of-thought demonstrations, and without the need for re-training.\n\nFormally, let's define some key variables.D \u2208 T train represents a subset of demonstrations selected from the training set.Q \u2208 T test is the query taken from the test set, and Y stands for the label associated with each query.M represents the LLM with its parameters frozen as \u0398, and F denotes the evaluation metric function.For example, F is typically used to measure accuracy or F1 score in classification tasks, such as sentiment classification, and is often used to represent metrics like ROUGE (Lin, 2004) or BLEU (Papineni et al., 2002) in text generation tasks, such as summarization and machine translation.The concept of emergent ability can be formally expressed using the equation:\n\nwhere M is usually considered to exhibit emergent abilities if the computed value using F exceeds a pre-defined threshold.Under this definition, we can group similar concepts within the few-shot prompting paradigm.CoT can be viewed as a variant of ICL, with the primary distinction being the format of the demonstration.Specifically, ICL demonstrations typically rely on a standard prompt with optional demonstration examples, whereas CoT prompting incorporates an additional textual reasoning process.\n\nAccording to our definition, we organize existing literature (summarized in Table 1) on interpreting emergent capabilities into macro and micro perspectives.Researchers in the macro category focus on factors such as overall loss or the model architecture.Their goal is to establish a connection between the outcome of F and the behavior of M. Conversely, those in the micro category primarily centre their attention on the relationship between the outcome of F and the characteristics of the demonstration set D.", "filtered_refids": [["b62"], ["b43", "b31"], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 1988, "num_references": 3}
{"corpusid_sectionid": "264832783-s3", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Mechanistic Interpretability", "section": "With the goal of reverse-engineering components of frontier models into more understandable algorithms, Elhage et al. (2021) developed a mathematical framework for decomposing operations within transformers (Vaswani et al., 2017).They initially introduced the concept of \"induction heads\" in a two-layer attention-only model to explain the functioning of ICL within transformers with Circuits (Cammarata et al., 2020).They found that one-layer attention-only models perform relatively basic ICL in a crude manner, whereas two-layer models perform very general ICL using very different algorithms.Specifically, they discovered that one-layer models essentially function as an ensemble of bigram and \"skip-trigram\" models that can be accessed directly from the model weights without running the entire model.Most attention heads in these models allocate significant capacity to copying mechanisms, resulting in very simple ICL.In contrast, the two-layer models manifest a significantly powerful mechanism that employs more advanced, qualitative algorithms at inference time, referred to as \"induction heads\".This allows them to perform ICL in a manner that resembles a computer program executing an algorithm, rather than merely referencing skip-trigrams.Building on this foundation, Olsson et al. (2022) later investigated the internal structures responsible for ICL by extending the concept of \"induction head\" (Elhage et al., 2021).They implemented circuits consist of two attention heads: the \"previous token head\", which copies information from one token to its successor, and the actual \"induction head\", which uses this information to target tokens that precede the current one.Their study revealed a phase change occurring early in the training of LLMs of various sizes.This phase change involves circuits that perform \"fuzzy\" or \"nearest neighbor\" pattern completion in a mechanism similar to the two-layer induction heads.These circuits play a crucial role in implementating most ICL in large models.One pivotal insight from (Olsson et al., 2022) presented six arguments supporting their hypothesis that induction heads may serve as the primary mechanistic source of ICL in a significant portion of LLMs, particularly those based on transformer architectures.\n\nWhile Elhage et al. (2021) and Olsson et al. (2022) contribute to our understanding of ICL by probing the internal architecture of LLMs, it is important to note that their findings represent initial steps towards the comprehensive reverseengineering of LLMs.It becomes particularly intricate when dealing with LLMs characterized by complex structures comprising hundreds of layers and spanning billions to trillions of parameters.This complexity introduces significant challenges.Moreover, a substantial portion of their conclusions relies primarily on empirical correlations, which might be susceptible to confounding from various factors, thereby introducing potential vulnerabilities into their findings.", "filtered_refids": [[null, "b57", "b9", "b40"], [null, "b40"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 13, "num_chars": 2976, "num_references": 6}
{"corpusid_sectionid": "264832783-s4", "title": "The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities", "date": "2023-11-01", "section_title": "Regression Function Learning", "section": "Several research studies posited that the emergence of LLMs' competence in ICL can be attributed to their intrinsic capability to approximate regression functions for a novel query Q based on the demonstrations D. Garg et al. (2022) first formally de-fined ICL as a problem of learning functions and explored whether LLMs can be trained from scratch to learn simple and well-defined function classes, such as linear regression functions.To achieve this, they generated examples D using these functions, and trained models to predict the function value for the corresponding query Q.Their empirical findings revealed that trained Transformers exhibited ICL abilities, as they manifested to \"learn\" previously unseen linear functions from examples, achieving an average error comparable to that of the optimal least squares estimator.Furthermore, Garg et al. (2022) demonstrated that ICL can be applied to more complex function classes, including sparse linear functions, decision trees, and two-layer neural networks, and posited that the capability to learn a function class through ICL is an inherent property of the model M \u0398 , irrespective of its training methodology.\n\nLater, Li et al. (2023b) extended Garg et al. (2022) to interpret ICL from a statistical perspective.They derived generalization bounds for ICL, considering two types of input examples: sequences that are independently and identically distributed (i.i.d.) and trajectories originating from a dynamical system.They established a multitask generalization rate of 1/", "filtered_refids": [["b18"], [null, "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1536, "num_references": 3}
{"corpusid_sectionid": "222124957-s2", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Transfer: The Pretrain-Finetune Framework", "section": "While text representations can be learned in any manner, ultimately, they are evaluated using specific target tasks. Historically, the learned representations (e.g. word vectors) were used as initialization for task-specific models. Dai and Le (2015) are credited with using pretrained language model outputs as initialization, McCann et al. (2017) use pretrained outputs from translation as frozen word embeddings, and Howard and Ruder (2018) and Radford et al. (2018) demonstrate the effectiveness of finetuning to different target tasks by updating the full (pretrained) model for each task. We refer to the embeddings produced by the pretrained models (or encoders) as contextualized text representations. As our goal is to discuss the encoders and their representations, we do not cover the innovations in finetuning (Liu et al., 2015;Ruder et al., 2019;Phang et al., 2018;Liu et al., 2019c;Zhu et al., 2020, inter alia).\n\nEvaluation Widely adopted evaluations of text representations relate them to downstream natural language understanding (NLU) benchmarks. This full-stack process necessarily conflates representation power with finetuning strategies. Common language understanding benchmarks include (1) a diverse suite of sentence-level tasks covering paraphrasing, natural language inference, sentiment, and linguistic acceptability (GLUE) and its more challenging counterpart with additional commonsense and linguistic reasoning tasks (Super-GLUE) (Wang et al., 2019c,b;Clark et al., 2019a;De Marneffe et al., 2019;Roemmele et al., 2011;Khashabi et al., 2018;Zhang et al., 2018;Dagan et al., 2006;Bar Haim et al., 2006;Giampiccolo et al., 2007;Bentivogli et al., 2009;Pilehvar and Camacho-Collados, 2019;Poliak et al., 2018;Levesque et al., 2011); (2) crowdsourced questions derived from Wikipedia articles (Rajpurkar et al., 2016, 2018; and (3) multiple-choice reading comprehension (Lai et al., 2017, RACE).", "filtered_refids": [["b63", "b6", "b57", "b40", "b41", "b45", null, "b72"], ["b58", "b60", "b28", null, "b19", "b123", "b70", "b33", "b68"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 8, "num_chars": 1921, "num_references": 17}
{"corpusid_sectionid": "222124957-s4", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Token Prediction", "section": "Predicting (or generating) the next word has historically been equivalent to the task of language modeling. Large language models perform impressively on a variety of language understanding tasks while maintaining their generative capabilities (Radford et al., 2018(Radford et al., , 2019Keskar et al., 2019;Brown et al., 2020), often outperforming contemporaneous models that use additional training objectives.\n\nELMo (Peters et al., 2018) is a BiLSTM model with a language modeling objective for the next (or previous) token given the forward (or backward) history. This idea of looking at the full context was further refined as a cloze 3 task (Baevski et al., 2019), or as a denoising Masked Language Modeling (MLM) objective (Devlin et al., 2019, BERT). MLM replaces some tokens with a [mask] symbol and provides both right and left contexts (bidirectional context) for predicting the masked tokens. The bidirectionality is key to outperforming a unidirectional language model on a large suite of natural language understanding benchmarks (Devlin et al., 2019;Raffel et al., 2019).\n\nThe MLM objective is far from perfect, as the use of [mask] introduces a pretrain/finetune vo-cabulary discrepancy. Devlin et al. (2019) look to mitigate this issue by occasionally replacing [mask] with the original token or sampling from the vocabulary. Yang et al. (2019) convert the discriminative objective into an autoregressive one, which allows the [mask] token to be discarded entirely. Naively, this would result in unidirectional context. By sampling permutations of the factorization order of the joint probability of the sequence, they preserve bidirectional context. Similar ideas for permutation language modeling (PLM) have also been studied for sequence generation (Stern et al., 2019;Chan et al., 2019;Gu et al., 2019). The MLM and PLM objectives have since been unified architecturally Bao et al., 2020) and mathematically (Kong et al., 2020).\n\nELECTRA (Clark et al., 2020) replaces [mask] through the use of a small generator (trained with MLM) to sample a real token from the vocabulary. The main encoder, a discriminator, then determines whether each token was replaced.\n\nA natural extension would mask units that are more linguistically meaningful, such as rarer words, 4 whole words, or named entities (Devlin et al., 2019;Sun et al., 2019b). This idea can be simplified to random spans of texts (Yang et al., 2019;Song et al., 2019). Specifically, Joshi et al. (2020) add a reconstruction objective which predicts the masked tokens using only the span boundaries. They find that masking random spans is more effective than masking linguistic units.\n\nAn alternative architecture uses an encoderdecoder framework (or denoising autoencoder) where the input is a corrupted (masked) sequence the output is the full original sequence (Wang et al., 2019d;Lewis et al., 2020;Raffel et al., 2019).", "filtered_refids": [["b15", "b64", "b63", "b17"], ["b65", null, "b56"], [null, "b23", "b119", "b84"], [null], ["b90", "b13", "b82", null, "b119"], ["b29", "b104", "b65"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 20, "num_chars": 2900, "num_references": 20}
{"corpusid_sectionid": "222124957-s5", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "date": "2020-10-02", "section_title": "Nontoken Prediction", "section": "Bender and Koller (2020) argue that for the goal of natural language understanding, we cannot rely purely on a language modeling objective; there must be some grounding or external information that relates the text to each other or to the world. One solution is to introduce a secondary objective to directly learn these biases.\n\nSelf-supervised discourse structure objectives, such as text order, has garnered significant attention. To capture relationships between two sentences, 5 Devlin et al. (2019) introduce the next 4 Clark et al. (2020) report negative results for rarer words. 5 Sentence unfortunately refers to a text segment containing sentence prediction (NSP) objective. In this task, either sentence B follows sentence A or B is a random negative sample. Subsequent works showed that this was not effective, suggesting the model simply learned topic (Yang et al., 2019;. Jernite et al. (2017) propose a sentence order task of predicting whether A is before, after, or unrelated to B, and Wang et al. (2020b) and Lan et al. (2020) use it for pretraining encoders. They report that (1) understanding text order does contribute to improved language understanding; and (2) harder-to-learn pretraining objectives are more powerful, as both modified tasks have lower intrinsic performance than NSP. It is still unclear, however, if this is the best way to incorporate discourse structure, especially since these works do not use real sentences.\n\nAdditional work has focused on effectively incorporating multiple pretraining objectives. Sun et al. (2020a) use multi-task learning with continual pretraining (Hashimoto et al., 2017), which incrementally introduces newer tasks into the set of pretraining tasks from word to sentence to document level tasks. Encoders using visual features (and evaluated only on visual tasks) jointly optimize multiple different masking objectives over both token sequences and regions of interests in the image (Tan and Bansal, 2019). 6 Prior to token prediction, discourse information has been used in training sentence representations. Conneau et al. (2017Conneau et al. ( , 2018a use natural language inference sentence pairs, Jernite et al. (2017) use discourse-based objectives of sentence order, conjunction classifier, and next sentence selection, and  use discourse markers. While there is weak evidence suggesting that these types of objectives are less effective than language modeling (Wang et al., 2019a), we lack fair studies comparing the relative influence between the two categories of objectives.", "filtered_refids": [[], ["b11", "b26", "b119", "b106"], ["b93", "b91", null, "b11"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2554, "num_references": 8}
{"corpusid_sectionid": "52011136-s1", "title": "A review of Spanish corpora annotated with negation", "date": "2018-08-01", "section_title": "Negation in Spanish", "section": "Processing negation is not as easy as using a list of negation markers and applying look-up methods. They can be used to find out potential negation cues but they are not adequate because the presence of a cue does not imply that it acts as a negation. In the sentence \"You bought the car to use it, didn't you?\" the cue \"not\" is not used as a negation but it is used to reinforce the first part of the sentence. Moreover, it is also necessary to identify the scope or part of the sentence affected by the negation and its focus, the part more prominently negated. If we want to advance in the study of this phenomenon, as for most of NLP tasks, the availability of annotated corpora is essential to train algorithms. According to existing resources for English, annotating negation involves the annotation of the following aspects:\n\n\u2022 Negation cue: lexical item(s) that modify the truth value of the propositions that are within its scope.\n\nThere are different types of negation according to the type of the negation cue used: . It is also known as affixal negation.\n\n\u2022 Scope: the part of the sentence affected by the negation cue . The scope can be continuous or discontinuous.\n\n\u2022 Focus: the part of the scope that is most prominently or explicitly negated (Blanco and Moldovan, 2011).\n\n\u2022 Negated event: the event that is directly negated by the negation cue, usually a verb, a noun or an adjective (Kim et al., 2008). This is just a list of the main aspects that have been annotated for negation. However, each language has specific linguistic resources to express negation and specific negation structures, which should also be reflected in the information annotated in corpora. As we will show in Section 4, most existing annotation schemes for Spanish do not account for the complexity of the linguistic structures used to express negation that are present in texts. This happens mainly because of two reasons: first, annotation of negation started with the annotation of clinical reports in English (Chapman et al., 2001;Goldin and Chapman, 2003;Mutalik et al., 2001a), where there is not too much variation of negation structures. Second, corpora have been created for specific purposes, such as extracting negated clinical events, and not with the intention of accounting for all the linguistic complexity of the negation phenomenon.\n\nAn exception to this is the SFU Review SP -NEG corpus (Jim\u00e9nez-Zafra et al., 2018;. The guidelines specify a great variety of negation patterns at the syntactic level that we summarize below. Additionally, the guidelines also specify expressions that involve a negation cue but do not express negation.\n\nOn the one hand, patterns that express negation can be divided into three categories:\n\n1. Simple negation markers, if they are composed of only one single negation marker (i.e. no ['no/not'], nunca ['never']).\n\n3. Negation markers in contrastive constructions, if negation markers are used to counterpose different ideas, to correct something, to introduce new information or to express obligation, rather than to express negation (i.e. No hay m\u00e1s soluci\u00f3n que comprar una lavadora ['There is no other solution than to buy a washing machine']). 4. Negation markers in comparative constructions, if negation markers are used to compare some property with something, that is, negation is used to place an entity below or above another entity on a scale (i.e. No es tan grande como me lo imaginaba ['It is not as big as I imagined']).", "filtered_refids": [[], [], [], [], ["b1"], ["b15", "b9", "b5", "b3"], ["b8"], [], [], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 23, "num_chars": 3479, "num_references": 6}
{"corpusid_sectionid": "52011136-s3", "title": "A review of Spanish corpora annotated with negation", "date": "2018-08-01", "section_title": "UAM Spanish Treebank", "section": "The first Spanish corpus annotated with negation that we are aware of is the UAM Spanish Treebank (Moreno et al., 2003), which was enriched with the annotation of negation cues and their scopes (Sandoval and Salazar, 2013). The initial UAM Spanish Treebank consisted of 1,500 sentences extracted from newspaper articles (El Pa\u00eds Digital and Compra Maestra) that were annotated syntactically. Trees were encoded in a nested structure, including syntactic category, syntactic and semantic features, and constituent nodes, following the Penn Treebank model. Later, this version of the corpus was extended with the annotation of negation and 10.67% of the sentences were found to contain negations (160 sentences).\n\nIn this corpus, syntactic negation was annotated but not lexical nor morphological negation. It was annotated by two experts in corpus linguistics who followed similar guidelines to those of Bioscope corpus Vincze, 2010). They included negation cues within the scope as in Bioscope and NegDDI-DrugBank (Bokharaeian et al., 2014). All the arguments of the negated events were also included in the scope of negation, including the subject, which was excluded from the scope in active sentences in Bioscope. There is no information about inter-annotator agreement.\n\nThe UAM Spanish Treebank corpus is freely available at http://www.lllf.uam.es/ESP/ Treebank.html. It is in XML format, negation cues are tagged with the label Type=\"NEG\" and the scope of negation is tagged with the label Neg=\"YES\" in the syntactic constituent on which negation acts.", "filtered_refids": [["b14"], ["b23", "b2"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 11, "num_chars": 1558, "num_references": 3}
{"corpusid_sectionid": "52011136-s5", "title": "A review of Spanish corpora annotated with negation", "date": "2018-08-01", "section_title": "SFU Review SP -NEG", "section": "The SFU Review SP -NEG 9 (Jim\u00e9nez-Zafra et al., 2018) is the first Spanish corpus that includes the event in the annotation of negation and that takes into account discontinuous negation markers. Moreover, it is the first corpus where the effect of the negation on the words that are within its scope is annotated, that is, whether there is a change in the polarity or an increment or reduction of its value. It is an extension of the Spanish part of the SFU Review corpus (Taboada et al., 2006) and it could be considered as the counterpart of the SFU Review Corpus with negation and speculation annotations 10 (Konstantinova et al., 2012).\n\nThe Spanish SFU Review corpus consists of 400 reviews extracted from the website Ciao.es that belong to 8 different domains: cars, hotels, washing machines, books, cell phones, music, computers, and movies. For each domain there are 50 positive and 50 negative reviews, defined as positive or negative based on the number of stars given by the reviewer (1-2=negative; 4-5=positive; 3-star review were not included). Later, it was extended to the SFU Review SP -NEG corpus in which each review was automatically annotated at the token level with POS-tags and lemmas, and manually annotated at the sentence level with negation cues and their corresponding scopes and events. It is composed of 9,455 sentences, out of which 3,022 sentences (31.97%) contain at least one negation marker.\n\nIn this corpus, syntactic negation was annotated but not lexical nor morphological negation, as in the UAM Spanish Treebank corpus. Unlike this one, annotations on the event and on how negation affects the polarity of the words within its scope were included. The annotations were performed by two senior researchers with in-depth experience in corpus annotation who supervised the whole process and two trained annotators who carried out the annotation task. The Kappa coefficient for inter-annotator agreement was of 0.97 for negation cues, 0.95 for negated events and 0.94 for scopes. 11 A detailed discussion of the main sources of disagreements can be found in (Jim\u00e9nez-Zafra et al., 2016).\n\nThe guidelines of the Bioscope corpus were taken into account but after a thorough analysis of negation in Spanish, a typology of Spanish negation patterns was defined . As in Bioscope, NegDDI-DrugBank and UAM Spanish Treebank, negation markers were included within the scope. Moreover, the subject was also included within the scope when the word directly affected by negation is the verb of the sentence, as in ConanDoyle-neg corpus (Morante and Daelemans, 2012). The event was also included in the scope of negation as in ConanDoyle-neg corpus.\n\nThe SFU Review SP -NEG is publicly available and can be downloaded at http://sinai.ujaen. es/sfu-review-sp-neg-2/.", "filtered_refids": [["b21", "b10"], [], ["b7"], ["b13"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 17, "num_chars": 2788, "num_references": 4}
{"corpusid_sectionid": "47019063-s1", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "The concept of semantic shifts", "section": "Human languages change over time, due to a variety of linguistic and non-linguistic factors and at all levels of linguistic analysis. In the field of theoretical (diachronic) linguistics, much attention has been devoted to expressing regularities of linguistic change. For instance, laws of phonological change have been formulated (e.g., Grimm's law or the great vowel shift) to account for changes in the linguistic sound system. When it comes to lexical semantics, linguists have studied the evolution of word meaning over time, describing so-called lexical semantic shifts or semantic change, which Bloomfield (1933) defines as \"innovations which change the lexical meaning rather than the grammatical function of a form.\" Historically, much of the theoretical work on semantic shifts has been devoted to documenting and categorizing various types of semantic shifts (Br\u00e9al, 1899;Stern, 1931;Bloomfield, 1933). The categorization found in Bloomfield (1933) is arguably the most used and has inspired a number of more recent studies (Blank and Koch, 1999;Geeraerts, 1997;Traugott and Dasher, 2001). Bloomfield (1933) originally proposed nine classes of semantic shifts, six of which are complimentary pairs along a dimension. For instance, the pair 'narrowing' -'broadening' describes the observation that word meaning often changes to become either more specific or more general, e.g. Old English mete 'food' becomes English meat 'edible flesh,' or that the more general English word dog is derived from Middle English dogge which described a dog of a particular breed. Bloomfield (1933) also describes change along the spectrum from positive to negative, describing the speaker's attitude as one of either degeneration or elevation, e.g. from Old English cniht 'boy, servant' to the more elevated knight.\n\nThe driving forces of semantic shifts are varied, but include linguistic, psychological, sociocultural or cultural/encyclopedic causes (Blank and Koch, 1999;Grzega and Schoener, 2007). Linguistic processes that cause semantic shifts generally involve the interaction between words of the vocabulary and their meanings. This may be illustrated by the process of ellipsis, whereby the meaning of one word is transferred to a word with which it frequently co-occurs, or by the need for discrimination of synonyms caused by lexical borrowings from other languages. Semantic shifts may be also be caused by changes in the attitudes of speakers or in the general environment of the speakers. Thus, semantic shifts are naturally separated into two important classes: linguistic drifts (slow and regular changes in core meaning of words) and cultural shifts (culturally determined changes in associations of a given word). Researchers studying semantic shifts from a computational point of view have shown the existence of this division empirically (Hamilton et al., 2016c). In the traditional classification of Stern (1931), the semantic shift category of substitution describes a change that has a non-linguistic cause, namely that of technologi-cal progress. This may be exemplified by the word car which shifted its meaning from non-motorized vehicles after the introduction of the automobile.\n\nThe availability of large corpora have enabled the development of new methodologies for the study of lexical semantic shifts within general linguistics (Traugott, 2017). A key assumption in much of this work is that changes in a word's collocational patterns reflect changes in word meaning (Hilpert, 2008), thus providing a usage-based account of semantics (Gries, 1999). For instance, Kerremans et al. (2010) study the very recent neologism detweet, showing the development of two separate usages/meanings for this word ('to delete from twitter,' vs 'to avoid tweeting') based on large amounts of web-crawled data. The usage-based view of lexical semantics aligns well with the assumptions underlying the distributional semantic approach (Firth, 1957) often employed in NLP . Here, the time spans studied are often considerably shorter (decades, rather than centuries) and we find that these distributional methods seem well suited for monitoring the gradual process of meaning change. Gulordava and Baroni (2011), for instance, showed that distributional models capture cultural shifts, like the word sleep acquiring more negative connotations related to sleep disorders, when comparing its 1960s contexts to its 1990s contexts.\n\nTo sum up, semantic shifts are often reflected in large corpora through change in the context of the word which is undergoing a shift, as measured by co-occurring words. It is thus natural to try to detect semantic shifts automatically, in a 'data-driven' way. This vein of research is what we cover in the present survey. In the following sections, we overview the methods currently used for the automatic detection of semantic shifts and the recent academic achievements related to this problem.", "filtered_refids": [["b59", null, "b7", "b62", "b5", "b17"], ["b59", "b19", "b23", "b5"], ["b15", "b63", "b18", "b28", "b20", "b33"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 28, "num_chars": 4932, "num_references": 16}
{"corpusid_sectionid": "47019063-s4", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "Training data", "section": "The time unit (the granularity of the temporal dimension) can be chosen before slicing the text collection into subcorpora. Earlier works dealt mainly with long-term semantic shifts (spanning decades or even centuries), as they are easier to trace. One of the early examples is Sagi et al. (2011) who studied differences between Early Middle, Late Middle and Early Modern English, using the Helsinki Corpus (Rissanen and others, 1993).\n\nThe release of the Google Books Ngrams corpus 1 played an important role in the development of the field and spurred work on the new discipline of 'culturomics,' studying human culture through digital media (Michel et al., 2011). Mihalcea and Nastase (2012) used this dataset to detect differences in word usage and meaning across 50-years time spans, while Gulordava and Baroni (2011) compared word meanings in the 1960s and in the 1990s, achieving good correlation with human judgments. Unfortunately, Google Ngrams is inherently limited in that it does not contain full texts. However, for many cases, this corpus was enough, and its usage as the source of diachronic data continued in Mitra et al. (2014) (employing syntactic ngrams), who detected word sense changes over several different time periods spanning from 3 to 200 years.\n\nIn more recent work, time spans tend to decrease in size and become more granular. In general, corpora with smaller time spans are useful for analyzing socio-cultural semantic shifts, while corpora with longer spans are necessary for the study of linguistically motivated semantic shifts. As researchers are attempting to trace increasingly subtle cultural semantic shifts (more relevant for practical tasks), the granularity of time spans is decreasing: for example, Kim et al. (2014) and Liao and Cheng (2016) analyzed the yearly changes of words. Note that, instead of using granular 'bins', time can also be represented as a continuous differentiable value (Rosenfeld and Erk, 2018).\n\nIn addition to the Google Ngrams dataset (with granularity of 5 years), Kulkarni et al. (2015) used Amazon Movie Reviews (with granularity of 1 year) and Twitter data (with granularity of 1 month). Their results indicated that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collection, the New-York Times Annotated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA 2 ), with time slices equal to one decade. Hamilton et al. (2016a) continued the usage of COHA (along with the Google Ngrams corpus). Kutuzov et al. (2017b) started to employ the yearly slices of the English Gigaword corpus (Parker et al., 2011) in the analysis of cultural semantic drift related to armed conflicts.", "filtered_refids": [[null, "b57"], ["b45", "b20", "b44", "b48"], ["b34", "b41", "b55"], ["b39", "b58", "b14", "b50", "b35", "b21", "b69", "b68"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 18, "num_chars": 3171, "num_references": 17}
{"corpusid_sectionid": "47019063-s5", "title": "Diachronic word embeddings and semantic shifts: a survey", "date": "2018-06-09", "section_title": "Test sets", "section": "Diachronic corpora are needed not only as a source of training data for developing semantic shift detection systems, but also as a source of test sets to evaluate such systems. In this case, however, the situation is more complicated. Ideally, diachronic approaches should be evaluated on human-annotated lists of semantically shifted words (ranked by the degree of the shift). However, such gold standard data is difficult to obtain, even for English, let alone for other languages. General linguistics research on language change like that of Traugott and Dasher (2001) and others usually contain only a small number of hand-picked examples, which is not sufficient to properly evaluate an automatic unsupervised system.\n\nVarious ways of overcoming this problem have been proposed. For example, Mihalcea and Nastase (2012) evaluated the ability of a system to detect the time span that specific contexts of a word undergoing a shift belong to (word epoch disambiguation). A similar problem was offered as SemEval-2015 Task 7: 'Diachronic Text Evaluation' (Popescu and Strapparava, 2015). Another possible evaluation method is so-called cross-time alignment, where a system has to find equivalents for certain words in different time periods (for example, 'Obama' in 2015 corresponds to 'Trump' in 2017). There exist several datasets containing such temporal equivalents for English (Yao et al., 2018). Yet another evaluation strategy is to use the detected diachronic semantic shifts to trace or predict real-world events like armed conflicts (Kutuzov et al., 2017b). Unfortunately, all these evaluation methods still require the existence of large manually annotated semantic shift datasets. The work to properly create and curate such datasets is in its infancy.\n\nOne reported approach to avoid this requirement is borrowed from research on word sense disambiguation and consists of making a synthetic task by merging two real words together and then modifying the training and test data according to a predefined sense-shifting function. Rosenfeld and Erk (2018) successfully employed this approach to evaluate their system; however, it still operates on synthetic words, limiting the ability of this evaluation scheme to measure the models' performance with regards to real semantic shift data. Thus, the problem of evaluating semantic shift detection approaches is far from being solved, and practitioners often rely on self-created test sets, or even simply manually inspecting the results.", "filtered_refids": [["b62"], ["b53", "b39", "b68"], ["b55"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 16, "num_chars": 2498, "num_references": 5}
{"corpusid_sectionid": "49587276-s3", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models", "date": "2018-08-01", "section_title": "NER datasets", "section": "Since the first shared task on NER (Grishman and Sundheim, 1996) 1 , many shared tasks and datasets for NER have been created. CoNLL 2002 (Tjong Kim Sang, 2002) 2 and CoNLL 2003(Tjong Kim Sang and De Meulder, 2003 3 were created from newswire articles in four different languages (Spanish, Dutch, English, and German) and focused on 4 entities -PER (person), LOC (location), ORG (organization) and MISC (miscellaneous including all other types of entities).\n\nNER shared tasks have also been organized for a variety of other languages, including Indian languages (Rajeev Sangal and Singh, 2008), Arabic (Shaalan, 2014), German (Benikova et al., 2014), and slavic languages (Piskorski et al., 2017). The named entity types vary widely by source of dataset and language. For example, Rajeev Sangal and Singh (2008)'s southeast Asian language data has named entity types person, designation, temporal expressions, abbreviations, object number, brand, etc. Benikova et al. (2014)'s data, which is based on German wikipedia and online news, has named entity types similar to that of CoNLL 2002 and 2003: PERson, ORGanization, LOCation and OTHer. The shared task 4 or-ganized by Piskorski et al. (2017) covering 7 slavic languages (Croatian, Czech, Polish, Russian, Slovak, Slovene, Ukrainian) also has person, location, organization and miscellaneous as named entity types.\n\nIn the biomedical domain, Kim et al. (2004) organized a BioNER task on MedLine abstracts, focusing on protien, DNA, RNA and cell attribute entity types. Uzuner et al. (2007) presented a clinical note de-identification task that required NER to locate personal patient data phrases to be anonymized. The 2010 I2B2 NER task 5 (Uzuner et al., 2011), which considered clinical data, focused on clinical problem, test and treatment entity types. Segura Bedmar et al. (2013) organized a Drug NER shared task 6 as part of SemEval 2013 Task 9, which focused on drug, brand, group and drug n (unapproved or new drugs) entity types. (Krallinger et al., 2015) introduced the similar CHEMDNER task 7 focusing more on chemical and drug entities like trivial, systematic, abbreviation, formula, family, identifier, etc. Biology and microbiology NER datasets 8 (Hirschman et al., 2005;Bossy et al., 2013;Del\u0117ger et al., 2016) have been collected from PubMed and biology websites, and focus mostly on bacteria, habitat and geolocation entities. In biomedical NER systems, segmentation of clinical and drug entities is considered to be a difficult task because of complex orthographic structures of named entities (Liu et al., 2015).\n\nNER tasks have also been organized on social media data, e.g., Twitter, where the performance of classic NER systems degrades due to issues like variability in orthography and presence of grammatically incomplete sentences (Baldwin et al., 2015). Entity types on Twitter are also more variable (person, company, facility, band, sportsteam, movie, TV show, etc.) as they are based on user behavior on Twitter.\n\nThough most named entity annotations are flat, some datasets include more complex structures. Ohta et al. (2002) constructed a dataset of nested named entities, where one named entity can contain another. Strassel et al. (2003) highlighted both entity and entity head phrases. And discontinuous entities are common in chemical and clinical NER datasets (Krallinger et al., 2015). Eltyeb and Salim (2014) presented an survey of various NER systems developed for such NER datasets with a focus on chemical NER. Grishman and Sundheim (1996) scored NER performance based on type, whether the predicted label was correct regardless of entity boundaries, and text, whether the predicted entity boundaries were correct regardless of the label. For each score category, precision was defined as the number of entities a system predicted correctly divided by the number that the system predicted, recall was defined as the number of entities a system predicted correctly divided by the number that were identified by the human annotators, and (micro) F-score was defined as the harmonic mean of precision and recall from both type and text.", "filtered_refids": [["b72", null, "b25", "b71"], [null, "b67", "b56", "b5"], ["b28", "b66", "b30", "b74", "b42", "b33", "b8", "b73", "b17"], ["b4"], ["b52", "b25", "b20", "b69", "b33"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 21, "num_chars": 4128, "num_references": 23}
{"corpusid_sectionid": "49587276-s4", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models", "date": "2018-08-01", "section_title": "NER evaluation metrics", "section": "The exact match metrics introduced by CoNLL (Tjong Kim Sang and De Meulder, 2003;Tjong Kim Sang, 2002) considers a prediction to be correct only when the predicted label for the complete entity is matched to exactly the same words as the gold label of that entity. CoNLL also used (micro) F-score, taking the harmonic mean of the exact match precision and recall.\n\nThe relaxed F1 and strict F1 metrics have been used in many NER shared tasks (Segura Bedmar et al., 2013;Krallinger et al., 2015;Bossy et al., 2013;Del\u0117ger et al., 2016). Relaxed F1 considers a prediction to be correct as long as part of the named entity is identified correctly. Strict F1 requires the character offsets of a prediction and the human annotation to match exactly. In these data, unlike CoNLL, word offsets are not given, so relaxed F1 is intended to allow comparison despite different systems having different word boundaries due to different segmentation techniques (Liu et al., 2015).", "filtered_refids": [["b72", "b71"], ["b66", "b42", "b33", "b8", "b17"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 967, "num_references": 7}
{"corpusid_sectionid": "49587276-s6", "title": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models", "date": "2018-08-01", "section_title": "Unsupervised and bootstrapped systems", "section": "Some of the earliest systems required very minimal training data. Collins and Singer (1999) used only labeled seeds, and 7 features including orthography (e.g., capitalization), context of the entity, words contained within named entities, etc. for classifying and extracting named entities. Etzioni et al. (2005) proposed an unsupervised system to improve the recall of NER systems applying 8 generic pattern extractors to open web text, e.g., NP is a <class1>, NP1 such as NPList2. Nadeau et al. (2006) presented an unsupervised system for gazetteer building and named entity ambiguity resolution based on Etzioni et al. (2005) and Collins and Singer (1999) that combined an extracted gazetteer with commonly available gazetteers to achieve F-scores of 88%, 61%, and 59% on MUC-7 (Chinchor and Robinson, 1997) location, person, and organization entities, respectively.\n\nZhang and Elhadad (2013) used shallow syntactic knowledge and inverse document frequency (IDF) for an unsupervised NER system on biology (Kim et al., 2004) and medical (Uzuner et al., 2011) data, achieving 53.8% and 69.5% accuracy, respectively. Their model uses seeds to discover text having potential named entities, detects noun phrases and filters any with low IDF values, and feeds the filtered list to a classifier (Alfonseca and Manandhar, 2002) to predict named entity tags.", "filtered_refids": [["b11", "b22", "b13", "b50"], ["b30", "b1", "b74"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 6, "num_chars": 1354, "num_references": 7}
{"corpusid_sectionid": "49215220-s2", "title": "A Survey on Open Information Extraction", "date": "2018-06-14", "section_title": "Learning-based Systems", "section": "The line of work on Open IE begins with TEXTRUNNER (Banko et al., 2007), a self-supervised learning approach consisting of three modules. First, given a small sample of sentences from the Penn Treebank, the learner applies a dependency parser to heuristically identify and label a set of extractions as positive and negative training examples. This data is then used as input to a Naive Bayes classifier which learns a model of trustworthy relations using unlexicalized POS and noun phrase (NP) chunk features. The selfsupervised nature mitigates the need for hand-labeled training data, and unlexicalized features help scale to the multitudes of relations found on the Web. The second component, the extractor, then generates candidate tuples by first identifying pairs of NP arguments and then heuristically designating each word in between as part of a relation phrase or not. Next, each candidate extraction is presented to the classifier, whereupon only those labeled as trustworthy are kept. Restricting to the use of shallow features in this step makes TEXTRUNNER highly efficient. Finally, a redundancy-based assessor assigns a probability to each retained tuple based on the number of sentences from which each extraction was found, thus exploiting the redundancy of information in Web text and assigning higher confidence to extractions that occur multiple times. Figure 2: OLLIE's system architecture (Mausam et al., 2012). OLLIE begins with seed tuples from REVERB, uses them to build a bootstrap learning set, and learns open pattern templates. These are applied to individual sentences at extraction time.\n\nWOE (Wu and Weld, 2010) also learns an open information extractor without direct supervision. It makes use of Wikipedia as a source of training data by bootstrapping from entries in Wikipedia infoboxes, i.e. by heuristically matching infobox attribute-value pairs with corresponding sentences in the article. This data is then used to learn extraction patterns on both POS tags (WOE pos ) and dependency parses (WOE parse ). Former extractor utilizes a linear-chain Conditional Random Field (CRF) to train a model of relations on shallow features which outputs certain text between two NPs when it denotes a relation. Latter approach, in contrast, makes use of dependency trees to build a classifier that decides whether the shortest dependency path between two NPs indicates a semantic relation. By operating over dependency parses, even long-range dependencies can be captured. Accordingly, when comparing their two approaches, Wu and Weld (2010) show that the use of dependency features results in an increase in precision and recall over shallow linguistic features, though, at the cost of extraction speed, hence negatively affecting the scalability of the system. OLLIE (Mausam et al., 2012) follows the idea of bootstrap learning of patterns based on dependency parse paths. However, while WOE relies on Wikipedia-based bootstrapping, OLLIE applies a set of high precision seed tuples from its predecessor system REVERB (see section 2.2) to bootstrap a large training set over which it learns a set of extraction pattern templates using dependency parses (see Figure 2). In contrast to previously presented systems that fully ignore the context of a tuple and thus extract propositions that are not asserted as factual, but are only hypothetical or conditionally true, OLLIE includes a context-analysis step in which contextual information from the input sentence around an extraction is analyzed to expand the output representation by adding attribution and clausal modifiers, if necessary, and thus increasing the precision of the system (see extractions (1) and (2) in Figure 1; for details, see section 2.4). Moreover, OLLIE is the first Open IE approach to identify not only verb-based relations, but also relationships mediated by nouns and adjectives (see extractions (3) and (4) in Figure 1). In that way, it expands the syntactic scope of relational phrases to cover a wider range of relation expressions, resulting in a much higher yield (at comparable precision) as compared to previous systems.\n\nMore recently, Yahya et al. (2014) proposed ReNoun, an Open IE system that entirely focuses on the extraction of noun-mediated relations. Starting with a small set of high-precision seed facts relying on manually specified lexical patterns that are specifically tailored for NPs, a set of dependency parse patterns for the extraction of noun-based relations is learned with the help of distant supervision (Mintz et al., 2009). These patterns are then applied to generate a set of candidate extractions which are assigned a confidence score based on the frequency and coherence of the patterns producing them.", "filtered_refids": [["b20", "b3"], ["b20", "b40"], ["b24", "b42"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 26, "num_chars": 4745, "num_references": 6}
{"corpusid_sectionid": "49215220-s3", "title": "A Survey on Open Information Extraction", "date": "2018-06-14", "section_title": "Rule-based Systems", "section": "The second category of Open IE systems make use of hand-crafted extraction rules. This includes RE-VERB (Fader et al., 2011), a shallow extractor that addresses three common errors of hitherto existing Open IE systems: the output of such systems frequently contains a great many of uninformative extractions (i.e. extractions that omit critical information), incoherent extractions (i.e. extractions where the relational phrase has no meaningful interpretation) and overly-specific relations that convey too much information to be useful in further downstream semantic tasks. REVERB improves over those approaches by introducing a syntactic constraint that is expressed in terms of a simple POS-based regular expres-sion (see Figure 3), covering about 85% of verb-based relational phrases in English text, as a linguistic analysis has revealed. In that way, the amount of incoherent and uninformative extractions is reduced. Moreover, in order to avoid overspecified relational phrases, a lexical constraint is presented which is based on the idea that a valid relational phrase should take many distinct arguments in a large corpus. Besides, while formerly proposed approaches start with the identification of candidate argument pairs, REVERB follows a relation-centric approach by first determining relational phrases that satisfy abovementioned constraints, and then finding a pair of NP arguments for each such phrase. An example output produced by ReVerb can be seen in Figure 1 (6-7). Whereas previously mentioned Open IE systems focus on the extraction of binary relations, commonly leading to extraction errors such as incomplete, uninformative or erroneous propositions, KRAKEN (Akbik and L\u00f6ser, 2012) is the first approach to be specifically built for capturing complete facts from sentences by gathering the full set of arguments for each relational phrase within a sentence, thus producing tuples of arbitrary arity. The identification of relational phrases and their corresponding arguments is based on hand-written extraction rules over typed dependency parses.\n\nEXEMPLAR (Mesquita et al., 2013) applies a similar approach for extracting n-ary relations, using hand-crafted patterns based on dependency parse trees to detect a relation trigger and the arguments connected to it. Based on the task of Semantic Role Labeling (SRL), whose key idea is to classify semantic constituents into different semantic roles (Christensen et al., 2010), it assigns each argument its corresponding role (such as subject, direct object or prepositional object).\n\nA more abstract approach, PROPS, was suggested by , who argue that it is hard to read out from a dependency parse the complete structure of a sentence's propositions, since, amongst others, different predications are represented in a non-uniform manner and proposition boundaries are not easy to detect. Therefore, they introduce a more semantically-oriented sentence representation that is generated by transforming a dependency parse tree into a directed graph which is tailored to directly represent the proposition structure of an input sentence. Consequently, extracting propositions from this novel output format is straightforward. The conversion of the dependency tree into the proposition structure is carried out by a rule-based converter.\n\nPredPatt (White et al., 2016) follows a similar approach. It employs a set of non-lexicalized rules defined over Universal Dependency (UD) parses (Marneffe et al., 2014) to extract predicate-argument structures. In doing so, PredPatt constructs a directed graph, where a special dependency ARG is built between the head token of a predicate and the head tokens of its arguments, while the original UD relations are preserved within predicate and argument phrases. As PredPatt uses language-agnostic patterns on UD structures, it is one of the few Open IE systems that work across different languages.", "filtered_refids": [["b11", "b1"], ["b8", "b23"], [], ["b19"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 19, "num_chars": 3912, "num_references": 5}
{"corpusid_sectionid": "49215220-s5", "title": "A Survey on Open Information Extraction", "date": "2018-06-14", "section_title": "Systems Capturing Inter-Proposition Relationships", "section": "Aforementioned Open IE systems lack the expressiveness needed for a proper interpretation of complex assertions, since they ignore the context under which a proposition is complete and correct. Thus, they do not distinguish between information asserted in a sentence and information that is only hypothetical or conditionally true. For example, extracting the relational tuple the earth; is the center of ; the universe from the sentence \"Early scientists believed that the earth is the center of the universe.\" would be inappropriate, since the input is not asserting it, but only noting that is was believed by early scientists (Mausam, 2016). To properly handle such cases, OLLIE attempts a first solution by additionally extracting an attribution context, denoting a proposition that is reported or claimed by some entity:\n\n( the earth; be the center of ; the university ; AttributedTo believe; Early astronomers)\n\nIn that way, it extends the default Open IE representation of arg 1 , rel, arg 2 with an extra field. Besides, OLLIE pays attention to clausal modifiers, such as: ( Romney; will be elected; President ; ClausalModifier if ; he wins five key states)\n\nBoth types of modifiers are identified by matching patterns with the dependency parse of the sentence. Clausal modifiers are determined by an adverbial-clause edge and filtered lexically (the first word of the clause must match a list of cue terms, e.g. if, when, or although), while attribution modifiers are identified by a clausal-complement edge whose context verb must match one of the terms given in VerbNet's list of common verbs of communication and cognition (Mausam et al., 2012). A similar output is produced by OLLIES's successor OPENIE4 (Mausam, 2016), which combines SRLIE (Christensen et al., 2010) and RELNOUN (Pal and Mausam, 2016). Former is a system that converts the output of a SRL system into an Open IE extraction by treating the verb as the relational phrase, while taking its role-labeled arguments as the Open IE argument phrases related to the relation. Latter, in contrast, represents a rule-based Open IE system that extracts noun-mediated relations, thereby paying special attention to demonyms and compound relational nouns. In addition, OPENIE4 marks temporal and spatial arguments by assigning them a T or S label, respectively. Lately, its successor OPENIE 5.0 was released 1 . It integrates BONIE (Saha et al., 2017) and OpenIEListExtractor 2 . While the former focuses on extracting tuples where one of the arguments is a number or a quantity-unit phrase, the latter targets the extraction of propositions from conjunctive sentences.\n\nSimilar to OLLIE, Bast and Haussmann (2013), who explore the use of contextual sentence decomposition (CSD) for Open IE, advocate to further specify propositions with information on which they depend. Their system CSD-IE is based on the idea of paraphrasing-based approaches described in section 2.3. Using a set of hand-crafted rules over the output of a constituent parser, a sentence is first split into sub-sequences that semantically belong together, forming so-called \"contexts\". Each such context now contains a separate fact, yet it is often dependent on surrounding contexts. In order to preserve such inter-proposition relationships, tuples may contain references to other propositions. However, as opposed to OLLIE, where additional contextual modifiers are directly assigned to the corresponding relational tuples, Bast and Haussmann (2013) represent contextual information in the form of separate, linked propositions. To do so, each extraction is given a unique identifier that can be used in the argument position of an extraction for a later substitution with the corresponding fact by a downstream application. An example for an attribution is shown below (Bast and Haussmann, 2013): #1: The Embassy; said; that #2 #2: 6,700 Americans; were; in Pakistan.\n\nAnother current approach that captures inter-proposition relationships is proposed by Bhutani et al. (2016), who present a nested representation for Open IE that is able to capture high-level dependencies, allowing for a more accurate representation of the meaning of an input sentence. Their system NESTIE uses bootstrapping over a dataset for textual entailment to learn both binary and nested triple representations for n-ary relations over dependency parse trees. These patterns can take on the form of binary triples arg 1 ; rel; arg 2 or nested triples such as arg 1 ; rel; arg 2 ; rel 2 ; arg 3 for n-ary relations. Using a set of manually defined rules, contextual links between extracted propositions are inferred from the dependency parse in order to generate a nested representation of assertions that are complete and closer in meaning to the original statement. Similar to OLLIE, contextual links are identified as clausal complements, conditionals and relation modifiers. Linked propositions are represented by arguments that refer to the corresponding propositions using identifiers, e.g. (Bhutani et al., 2016) (Gashteovski et al., 2017), another recent Open IE system, is built on top of ClausIE, a system that was found to often produce overly specific extractions. Such overly specific constituents that combine multiple, potentially semantically unrelated propositions in a single relational or argument phrase generally hurt the performance of downstream semantic applications, such as question answering or textual entailment. In fact, those approaches benefit from extractions that are as compact as possible. Therefore, MinIE aims to minimize both relational and argument phrases by identifying and removing parts that are considered overly specific. For this purpose, MinIE provides four different minimization modes which differ in their aggressiveness, thus allowing to control the trade-off between precision and recall. Moreover, it semantically annotates extractions with information about polarity, modality, attribution and quantities instead of directly representing it in the actual extractions, as the following example shows (Gashteovski et al., 2017):\n\n\"Pinocchio believes that the hero Superman was not actually born on beautiful Krypton.\" with + and -signifying positive and negative polarity, respectively.\n\nIn that way, the output generated by MinIE is further reduced to its core constituents, producing maximally shortened, semantically enriched extractions.\n\nTo further enhance the expressiveness of extracted propositions and sustain their interpretability in downstream artificial intelligence tasks, Cetto et al. (2018) propose Graphene, an Open IE framework that uses a set of hand-crafted simplification rules to transform complex natural language sentences into clean, compact structures by removing clauses and phrases that present no central information from the input and converting them into stand-alone sentences. In that way, a source sentence is transformed into a hierarchical representation in the form of core facts and accompanying contexts (Niklaus et al., 2016). In addition, inspired by the work on Rhetorical Structure Theory (Mann and Thompson, 1988), a set of syntactic and lexical patterns is used to identify the rhetorical relations by which core sentences and their associated contexts are connected in order to preserve their semantic relationships and return a set of semantically typed and interconnected relational tuples (see extractions (15-17) in Figure 1). Graphene's extraction workflow is illustrated in Figure 5.", "filtered_refids": [["b21"], [], [], ["b20", "b27", "b21", "b30", "b8"], ["b4"], ["b14", "b5"], [], [], ["b26", "b7", "b18"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 38, "num_chars": 7506, "num_references": 12}
{"corpusid_sectionid": "237099284-s3", "title": "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey", "date": 2021, "section_title": "Dialog State Tracking Challenge (DSTC)", "section": "The dialog state tracking challenge (DSTC) is a series of dialogue related challenges that serves as a common test and evaluation suite for dialogue state tracking (Williams and Young, 2007;Williams et al., 2013Williams et al., , 2016b. The challenge was later renamed as dialog system technology challenge to accommodate various other dialogue related tasks. The most widely used datasets in the context of the DST challenge are DSTC2 and DSTC3.\n\nDSTC2 and DSTC3. The dialog state tracking challenges 2 (DSTC2 - (Henderson et al., 2014a)) and 3 (DSTC3 - (Henderson et al., 2014b)) are human-machine conversation dialogue datasets collected using Amazon Mechanical Turk, respectively for the restaurant and the tourist domain.\n\nDSTC2 is a spoken dialogue dataset consisting of automatic speech recognition (ASR) hypotheses and turn-level semantic labels along with the transcriptions. The dataset consists of 1,612 dialogues for training, 506 dialogues for development, and 1,117 dialogues for testing. DSTC3 aims to evaluate DST models on their ability to track unseen slot values and on their adaptability to a new domain. For this purpose, the dataset does not contain training dialogues and consists of 2,265 dialogues for testing. Typically, the models trained on the DSTC2 dataset were evaluated with the DSTC3 dataset to estimate their performance.", "filtered_refids": [["b35", "b32", "b36"], ["b6", "b7"], []], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 9, "num_chars": 1355, "num_references": 5}
{"corpusid_sectionid": "237099284-s10", "title": "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey", "date": 2021, "section_title": "Static Ontology DST Models", "section": "The main distinguishing characteristic of DST models, in our opinion, is their capacity to predict dialogue states either from a fixed set of slot-values (i.e., from a static ontology) or from a possible open set of slot-values (i.e., from a dynamic ontology).\n\nStatic ontology models rely on a fixed ontology to predict the dialogue state. This means that the set of slot-values is predefined, and that a model can only predict for those predefined values. These models typically consist of an input layer that transforms each input token into an embedding, of an encoder layer that encodes the input to a hidden state h t , and of an output layer that predicts the slot value based on h t . Considering that the set of possible slot-values is predefined, there are two approaches used for the output layer: i) a feed-forward layer, which receives the input representation and produces scores equal to the # of slot-values; ii) an output layer that receives both the input and the slot-value representations and compares them with each of the slot-value representations providing a score for each slot-value. The obtained score can then be normalized using a non-linear activation function, either softmax, to get a probability distribution over all the slot-value pairs, or sigmoid, to get the individual probability for each slot-value pair. Figure 2 shows the standard architecture of the two approaches.\n\nWe now review few challenges that have been addressed in static ontology models, including delexicalization, data-driven DST, parameter sharing, latency in prediction, and the use of pre-trained language models. Performances of the systems are all reported in Table 2.\n\nDelexicalization. Delexicalization is an effective approach adopted to counter imbalanced training data for slot-values. In this regard, the slot values in the input are replaced with labels corresponding to slot names. For instance, I want Chinese food is delexicalised as I want F.VALUE F.SLOT. It has to be noted that replacing slot-values needs a semantic dictionary listing the possible values for each slot. (Henderson et al., 2014c; has proposed a word-based DST with recurrent neural networks that uses delexicalization on top of an input representation based on Automatic Speech Recognition. This allows to improve the system robustness with respect to the user expressions mentioning slot values.\n\nData-driven DST. Although delexicalization showed to be effective, it requires additional manual feature engineering. An alternative, data-driven methodology, was proposed by the neural belief tracker (NBT) (Mrk\u0161i\u0107 et al., 2017a). Instead of delexicalizing the input, a separate module was learned to represent the slot-value pairs. Then, the slot-value representation and the input representation are passed through a binary decision maker before applying softmax activation. Similarly, a fully statistical NBT was proposed by (Mrk\u0161i\u0107 and Vuli\u0107, 2018), where a statistical update function replaces the rule-based update mechanism in NBT. The experimental results showed the statistical update function to outperform the rule-based update.\n\nParameter sharing. While the previous models consist of a separate encoder for each slot whose values have to be predicted, the DST efficiency crucially depends on the number of model parameters. In this direction, (Ren et al., 2018) proposed   StateNet, a DST sharing the parameters for all slots, thus reducing the number of model parameters.\n\nStateNet combines a n-gram input feature representation with a slot representation, and uses long short term memory (LSTM) to encode them into a single vector. The value representation is then compared with the encoded vector to obtain the score for each slot-value. A semantically specialised Paragram-SL999 (Wieting et al., 2015) was used to encode the tokens. Compared with fully statistical NBT, StateNet achieves high performance even with a rule-based update function.", "filtered_refids": [[], [], [], ["b8"], ["b19", "b18"], ["b26"], ["b31"]], "passed_title_criteria": true, "passed_length_criteria": true, "passed_reference_criteria": true, "passed_all_criteria": true, "num_sentences": 27, "num_chars": 3949, "num_references": 5}