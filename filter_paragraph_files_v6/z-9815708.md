# LITERATURE REVIEW OF ATTRIBUTE LEVEL AND STRUCTURE LEVEL DATA LINKAGE TECHNIQUES

CorpusID: 9815708 - [https://www.semanticscholar.org/paper/2ee3a601957528b0d50b68bc145c7086a14c15a6](https://www.semanticscholar.org/paper/2ee3a601957528b0d50b68bc145c7086a14c15a6)

Fields: Computer Science

## (s7) Rule/Regular expression
Number of References: 2

(p7.0) The Rule / Regular expression [40] approach uses rules or set of predefined regular expressions and perform matching on tuples. Regular Expression Pattern as proposed in [40] is more flexible than regular expression alone, which is built from alphabetical elements. This is also because the Regular Expression Pattern is built from patterns over a data element, allowing the use of constructs such as "wildcards" or pattern variables. Regular Expression Pattern is quite useful when manipulating strings, and can be used in conjunction with basic pattern matching. However, the problem with this approach lies in the fact that it is relatively domain specific and tends to only work well on strings.
## (s9) String distance
Number of References: 3

(p9.0) String distance methods, also known as character-based similarity metrics [34] are used to perform data linkage based on the cost associated within the comparing strings. The cost is estimated on the number of characters which needs to be inserted, replaced or deleted for a possible string match. For example, Fig. 3 shows the cost associated in editing string "Aussie" to "Australian" (the "+" sign shows addition, the "-" sign shows deletion, and the "x" sign shows replacement). Experimental results in [34] have shown that the different distance based methodologies discovered so far are efficient under different circumstances. Some of the commonly recommended distance based metrics include Levenstein distance, Needleman-Wunsch distance, Smith-Waterman distance, Affine-gap distance, Jaro metric, Jaro and Jaro-Winkler metric, Qgram distance, and positional Q-grams distance. Through the various methods, costs are assigned to compensate for pitfalls in the system. Yet, overall, string distance pattern is most effective for typographical errors, but is hardly useful outside of this area [34].
## (s10) Term frequency
Number of References: 4

(p10.0) Term frequency [43] approach determines the frequency of strings in relation and to favour matches of less common strings, and penalizes more common strings. The Term frequency methods allow for more commonly used strings to be left out of the similarity equation. TF-IDF [43] (Term Frequency-Inverse Document Frequency) is a method using the commonality of the term (TF) along with the overall importance of the term (IDF). TF-IDF is commonly used in conjunction with cosine similarity in the vector space model. Soft TF-IDG [44] adds similar token pairs to the cosine similarity computation. According to the researchers in [44], TF-IDF can be useful for similarity computations due to its ability to give proportionate token weights. However, this approach fails to make distinctions between the similarity level of two records with the same token or weight, and is essentially unable to determine which record is more relevant.
## (s16) Blocking
Number of References: 4

(p16.0) Blocking [46] techniques separate tuple values into set of blocks/groups. Within each of these blocks, comparisons are made. Sorted Neighborhood is a blocking method which first sorts and then slides a "window" over the data to make comparisons [46]. BigMatch [51] used by the U.S. Census Bureau, is another blocking technique. BigMatch identifies pairs for further processing through a more sophisticated means. The blocking function assigns a category for each record and identical records are given the same category. The disadvantage of the blocking method is that it will not work for records which have not been given the same category [18, 25, and 34].
## (s38) Knowledge integration
Number of References: 6

(p38.0) Knowledge integration techniques are used to enhance the functioning of structure level matching by integrating knowledge between data relationships to form a stronger concept base for performing data linkage [75]. Knowledge integration enhances query formulation when the information structure and data sources are not known, as highlighted in [76], and is becoming increasingly important in data matching processes as various data structures conceptualise the same concept in different ways, with resulting inconsistencies and overlapping material. Integration can be based on extensions or concepts, and is aimed at indemnifying inconsistencies and mismatches in the concepts. For example, the COIN technique [77] addresses data-level heterogeneities among data sources expressed in terms of context axioms and provides a comprehensive approach to knowledge integration. An extension of COIN is ECOIN, which improves upon COIN through its ability to handle both data-level and ontological heterogeneities in a single framework [77]. Knowledge integration is highly useful in medicine, to integrate concepts and information within various medical data sources. Knowledge integration involves the introduction of a dictionary to fill knowledge gaps, such as using distance-based weight measurement through Google [68]. For example, the Foundational Model of Anatomy is used as a concept roadmap to better integrate various medical data sources into unique anatomy concepts [68].
