# Deep Personality Trait Recognition: A Survey

CorpusID: 248530069 - [https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc](https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc)

Fields: Psychology, Medicine, Computer Science

## (s5) ChaLearn First Impression V1-V2
Number of References: 2

(p5.0) The ChaLearn First Impression data set has been developed into two versions: the ChaLearn First Impression V1 (Ponce-López et al., 2016), and the ChaLearn First Impression V2 : The ChaLearn First Impression V1 contains 10,000 short video clips, collected from about 2,762 YouTube highdefinition videos of persons who are facing and speaking to the camera in English. Each video has a resolution of 1,280 × 720, and a length of 15 s. The involved persons have different genders, ages, nationalities, and races. In this case, the task of predicting apparent personality traits becomes more difficult and challenging. The ChaLearn First Impression V2  is an extension of the ChaLearn First Impression V1 (Ponce-López et al., 2016). In this data set, the new variable of "job interview" is added for prediction. The manual transcriptions associated with the corresponding audio in videos are also provided.
## (s10) Convolutional Neural Networks
Number of References: 2

(p10.0) Convolutional neural networks (CNNs) were originally proposed by LeCun et al. (1998) in 1998, and initially developed as an advanced version of deep CNNs, such as AlexNet (Krizhevsky et al., 2012) in 2012. The basic structure of CNNs comprises of convolutional layers, pooling layers, as well as fully connected (FC) layers. CNNs usually have multiple convolutional and pooling layers, in which pooling layers are frequently followed by convolutional layers. The convolutional layer adopts a number of learnable filters to perform convolution operation on the whole input image, thereby yielding the corresponding activation feature maps. The pooling layer is employed to reduce the spatial size of produced feature maps by using non-linear down-sampling methods for translation invariance. Two wellknown used pooling strategies are average pooling and max-pooling. The FC layer, in which all neurons are fully connected, is often placed at the end of the CNN network.
