# A survey of Object Classification and Detection based on 2D/3D data

CorpusID: 170078919 - [https://www.semanticscholar.org/paper/b2c768b7ddc9a038111a1361ef25828adfdcbdf8](https://www.semanticscholar.org/paper/b2c768b7ddc9a038111a1361ef25828adfdcbdf8)

Fields: Engineering, Computer Science

## (s3) Object detection
Number of References: 5

(p3.0) In the object detection research area, just like in the image classification where the ImageNet dataset is available to train the algorithm, in the object detection, COCO(Common Objects in Context) [4] and VOC dataset can be used to train the algorithms. For the COCO dataset, the bounding box and the mask of the objects are provided. The first important contribution of using deep neural networks to solve the object detection problem is the R-CNN [20]. The region of interest(ROI) of an image is proposed by the selective search(SS) [21] algorithm, and the ROI is cropped to feed a CNN to do the detection. However, as every proposed interesting area will be calculated to predict whether that specified region is an object, the speed of this algorithm is very slow. In order to address this problem, a fast R-CNN algorithm is proposed in [22] by improving the feature map generation efficiency. In another paper which is short for faster RCNN [23], instead of using the SS algorithm to generate ROI, the ROI is proposed by using deep neural network structure. By the combination, the performance of the algorithm can also be improved itself. Figure 11: The 2D center offset box encoding method.
## (s8) Semantic segmentation
Number of References: 10

(p8.0) For semantic segmentation, a pixel level detection of an object is provided. One important paper in this area is Fully Convolutional Network(FCN) [36]. It upsamples the feature map to make sure that a more accurate location information can be preserved. In addition, the data in the previous layers are combined with the deeper layer to preserve more information and thus improve the accuracy of the semantic segmentation. After this paper, FCN became mainstream in semantic segmentation. DeepLab [37], FCIS [38] and mask-RCNN [28] are using FCN. For the DeepLab [37], the CRF is used to further improve the result by benefiting of the redundant information of nearby pixels. The CRF approach is firstly introduced in [39]. For FCIS [38], location sensitive feature maps are generated to improve the pixel level prediction. The FCIS [38] is the improved version of [30]. So far, the FCN and the CRF approach became the standard method in the semantic segmentation area. Besides those papers, [40] [41] are also using deep neural network approaches to improve the performance of the semantic segmentation.    The Mask R-CNN framework is shown in Figure 23. One important contribution of Mask R-CNN is RoIAlign as shown in Figure 24. The alignment of the ROI is not so sensitive in the object detection, however, in the semantic segmentation it is sensitive as it has to do the detection in pixel level and by employing the alignment, the performance is improved a lot. Another important fact for the improvement of the performance of the Mask R-CNN is using the more powerful backbone networks as observed from the results shown in Figure 25. The performance of RoIAlign is shown in Figure 26.
## (s16) Novel view point models
Number of References: 2

(p16.0) RotationNet is an extension of MVCNN [60]. In this paper, multiple views from different angles are explored. Three models of camera views are proposed as shown in Figure 37. The performance of case(i) (the same view points model as MVCNN [60]) and case(ii) are compared. Case (ii) achieves a better performance based on the ModelNet40 task. For the ModelNet40, the case(iii) model is not used.
## (s27) The 3D bounding box encoding methods
Number of References: 5

(p27.0) In the following section we will focus on the 3D only or 2D+3D detection systems.  A comprehensive comparison of the input data, the feature representation of input data and the bounding box encoding methods for both the proposals and the final 3D bounding box detection is provided in Table 11. DeepSliding [71] and VoxelNet [74] are using the 3D convolutional neural network to do the proposals generation and the bounding box detection. MV3D [72], AVOD [73] are projecting the depth or LiDAR data to a 2D similar images and are using a 2D CNN to do the proposal generation and bounding box detection. The relationship between MV3D and AVOD is explained later. F-PointNet [75] is using 2D RGB images to help generate the proposals and it is a special framework. At the same time, the different proposal generation method will have an influence on the application scenario of those frameworks, to be discussed later.
## (s34) BEV features for MV3D
Number of References: 5

(p34.0) The bird's eye view representation is encoded by height, intensity and density. The point cloud is projected into a 2D grid with resolution of 0.1m. For each cell, the height feature is computed as the maximum height of the points in the cell. To encode more detailed height information, the point cloud is divided equally into M slices. A height map is computed for each slice. The intensity feature is the reflectance value of the point which has the maximum height in each cell. The point cloud density indicates the number of points in each cell. So it has (M + 2) channel features [72] . MV3D uses point cloud in the range of [0, 70.4] × [− 40,40] meters in the X and Y dimensions. The size of the input features is 704 × 800 × (M + 2). The value of M is not provided in [72]. The length of Z dimension is also not provided. If we suppose that for the length of Z dimension is 2.5 meters as in AVOD [73] and the resolution of the Z dimension is also 0.1 meters, then the size of the input feature for the BEV will be 704 × 800 × 27.
## (s35) FV features for MV3D
Number of References: 2

(p35.0) MV3D projects the FV into a cylinder plane to generate a dense front view map as in VeloFCN [84]. The front view map is encoded with threechannel features, which are height, distance and intensity as shown in Figure  64. Since KITTI uses a 64-beam Velodyne laser scanner, the size of map for the front view is 64 × 512.  [72] The performance of MV3D is evaluated based on the outdoor KITTI dataset. The performance of 3D object detection based on the test set can be found from the leaderboard. The performance of 3D object detection based on validation dataset is shown in Figures 65 and 66. It only provides the car detection results. Detection results for the pedestrians and cyclists are not provided.   The framework of AVOD is shown in Figure 68. AVOD is using the same encoding method as MV3D for the BEV. In AVOD, the value of M is set as 5 and the range of the LiDAR is [0, 70] × [−40, 40] × [0, 2.5] meters. So the size of the input feature for the BEV is 700 × 800 × 7. AVOD is using both the BEV and image to do the region proposals which is the main difference to the MV3D work.  VoxelNet architecture is shown in Figure 71. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information. The space is represented as a sparse 4D tensor. The convolutional middle layers processes the 4D tensor to aggregate spatial context. Finally, a RPN generates the 3D detection. A fixed number, T , of points from voxels containing more than T points are randomly sampled. For each point, a 7-feature is used which is (x, y, z, r, x− v x , y − v y , z − v z ) where x, y, z are the XY Z coordinates for each point. r is the received reflectance and (v x , v y , v z ) is the centroid of points in the voxel. Voxel Feature Encoding is proposed in VoxelNet. The 7-feature for each point is fed into the Voxel feature encoding layer as shown in Figure 72. Fully connected networks are used in the VFE network with element-wise MaxPooling for each point and concatenation between each point and the element-wise MaxPooling output. The input of the VFE is T × 7 and the output will be T × C where C depends on the FC layers of the VFE itself and depends on the whole VFE layers network used. Finally, an element-wise MaxPooling is used again and change the dimension of the output to 1 × C. Then for each voxel we have a one vector with C elements as shown in Figure 71. For the whole framework, we will have an input data with shape of C ×D ×H ×W .
