# A Comprehensive Analytical Survey on Unsupervised and Semi-Supervised Graph Representation Learning Methods

CorpusID: 245335105 - [https://www.semanticscholar.org/paper/b415ecb687941e1e9ef68e04a4a1c68c73483d51](https://www.semanticscholar.org/paper/b415ecb687941e1e9ef68e04a4a1c68c73483d51)

Fields: Computer Science, Mathematics

## (s8) Shallow Networks
Number of References: 4

(p8.0) In this type of model, a random walk is performed on a graph and a path of a set of vertices is sampled for each random walk. Now, considering each vertex in the path as a word in a sentence, we can model this problem in a similar way to that of word representation in the NLP domain [69,68]. The basic task in word representation is to find the representation of words in vector space that preserves both syntactic and semantic meanings. This type of representation also becomes helpful for downstream analysis. In the graph embedding problem, we want to find the representation of vertices that preserves structural and neighborhood relationships in a graph. A popular model in the word embedding domain is word2vec that uses the skip-gram model [69] for optimizing the prediction of source context words from a target word. A pioneering graph embedding method, called DeepWalk [79], infers the analogy between word embedding and graph embedding, and uses the skip-gram model for training. In this model, target vertices are fed to the input layer and the weights of the network are updated to learn the semantics of the graph based on the source context vertices.
## (s34) Effect of Dimensions
Number of References: 4

(p34.0) Some previous studies have shown that the performance on the prediction task may vary if we choose different values for hyper-parameters [79,34,100,80]. For example, after reaching a certain value for dimensionality, the accuracy of prediction starts to drop when we increase it further. Most of the previous studies suggest using dimensional embedding. To summarize the results, we conduct experiments varying the dimensions of the output embedding for some shallow network-based methods. We set different parameters as described in Section 4.3 and take 20% of the dataset to train the logistic regression model while the rest of the samples in the dataset are used for the classification. We report the results of the F1-micro scores for the Pubmed dataset in Figs. 21 (a). We observe that Force2Vec, DeepWalk, and HARP perform better than other methods for various dimensional embedding. We also notice that, for lower dimensions, the F1-micro scores are not that much less compared to higher dimensions. In fact, the VERSE tool shows better performance for 16-dimensional embedding for the Pubmed dataset. RolX shows high sensitivity for different dimensions. It shows the lowest performance for 16-dimensional embedding. Then, with the increase of dimension, the F1-micro score also increases until 128-dimension. Then, it falls a little for 256-dimensional embedding. The LINE method shows similar sensitivity to the VERSE method though its F1-micro scores are lower than the VERSE.
