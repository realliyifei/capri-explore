# Video Generative Adversarial Networks: A Review

CorpusID: 226246346 - [https://www.semanticscholar.org/paper/5c1b1ab734ebc6ba56ea5d8af9d01a9fe8e0ba8b](https://www.semanticscholar.org/paper/5c1b1ab734ebc6ba56ea5d8af9d01a9fe8e0ba8b)

Fields: Engineering, Computer Science

## (s41) VIDEO GANS
Number of References: 6

(p41.0) Video GANs models vary depending on the condition settings. While at one end, there are video GANs frameworks that are not supplied with conditional signals, discussed in section 4.1, other models are conditioned on audios, texts, semantic maps, images and videos, as discussed in sections 4.2.1, 4.2.2, 4.2.3, and 4.2.4 respectively. In all these categories, the overall architecture can be divided roughly into three subcategories. First are the GANs frameworks that use RNN architectures to address the time-series nature of video data [39,40]. The second are progressive video GANs models [41,42] in which initial frames are first generated, and the generated data is fed into another generator to produce an enhanced result. The third are the two-stream architecture video GANs [6,43], where each stream considers a different aspect of the video. The following sections review the video GANs models based on the presence or absence of input condition and the variations within each category.
