# A Survey on Temporal Sentence Grounding in Videos

CorpusID: 237532483 - [https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453](https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453)

Fields: Computer Science

## (s7) Weakly supervised method
Number of References: 17

(p7.0) For the annotation of groundtruth data in TSGV, the annotators should read the query and watch the video first, and then determine the start and end points of the query-indicated segment in the video. Such a human-labored process is very time-consuming. Therefore, due to the labor-intensive groundtruth annotation procedure, some works start to extend TSGV to a weakly supervised scenario where the locations of groundtruth segments (i.e., the start and end timestamps) are unavailable in the training stage. This is formally named as weakly supervised TSGV. The typical methods include WSDEC [14], TGA [43], WSLLN [17], SCN [34], Chen et al. [12], VLANet [40], MARN [54], BAR [64], RTBPN [85], CCL [86], EC-SL [11], LoGAN [55] and CRM [26]. In general, weakly supervised methods for TSGV can be grouped into two categories (i.e., MIL-based and reconstruction-based). One representative work will be illustrated in detail for each category, after which we will introduce the remaining.

(p7.1) Some works [12,17,43,55] adopt multi-instance learning (MIL) to address the weakly TSGV task. When temporal annotations are not available, the whole video is treated as a bag of instances with bag-level annotations, and the predictions for instances (video segment proposals) are aggregated as the bag-level prediction.
## (s16) 4.2.1
Number of References: 8

(p16.0) Large-scale video corpus moment retrieval. Large-scale video corpus moment retrieval (VCMR) is a research direction extended from TSGV that has been explored over the past few years [15,32,77,79]. It has more application value since it can retrieve the target segment semantically corresponding to a given text query from a large-scale video corpus (i.e., a collection of untrimmed and unsegmented videos) rather than from a single video. As compared with TSGV, VCMR has higher efficiency requirements since it not only needs to retrieve a specific segment from one single video but also locates the target video from a video corpus. Escorcia et al. [15] first extend TSGV to VCMR, introducing a model named Clip Alignment with Language (CAL) to align the query feature with a sequence of uniformly partitioned clips for moment composing. Lei et al. [32] introduce a new dataset for VCMR called TVR, which is comprised of videos and their associated subtitle texts. A Cross-modal Moment Localization (XML) network with a novel convolutional start-end detector module is also proposed to produce moment predictions in a late fusion manner. Zhang et al. [77] present a hierarchical multi-modal encoder (HAMMER) to capture both coarse-and fine-grained semantic information from the videos and train the model with three sub-tasks (i.e., video retrieval, segment temporal localization, and masked language modeling). Zhang et al. [79] introduce contrastive learning for VCMR, designing a retrieval and localization network with contrastive learning (ReLoCLNet).
## (s17) Spatio-temporal localization.
Number of References: 3

(p17.0) Spatial-temporal sentence grounding in videos is another extension from TSGV which mainly localizes the referring object/instance as a continuing spatialtemporal tube (i.e., a sequence of bounding boxes) extracted from an untrimmed video via a natural language description. Since fine-grained labeling process of localizing a tube (i.e., annotate a spatial region for each frame in videos) for STSGV is labor-intensive and complicated, Chen et al. [13] propose to solve this task in a weakly-supervised manner which only needs video-level descriptions, with a newly-constructed VID-sentence dataset. Besides, VOGNet [50] commits to address the task of video object grounding, which grounds objects in videos referred to the natural language descriptions, and constructs a new dataset called ActivityNet-SRL. Tang et al. [56] employ visual transformer to solve a similar task which aims to localize a spatio-temporal tube of the target person from an untrimmed video based on a given textural description with a newly-constructed HC-STVG dataset.
