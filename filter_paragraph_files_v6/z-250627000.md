# Rank-based Decomposable Losses in Machine Learning: A Survey

CorpusID: 250627000 - [https://www.semanticscholar.org/paper/59ceacd001ee4a0e7984207040275e5e9e657d9b](https://www.semanticscholar.org/paper/59ceacd001ee4a0e7984207040275e5e9e657d9b)

Fields: Computer Science, Medicine, Mathematics

## (s4) Notation
Number of References: 3

(p4.0) We introduce some necessary notations that will be used in the following sections. Let R be the real domain and R + is the nonnegative domain of R. Denote N n as the set {1, · · · , n}, N C as the set {1, · · · , C}, and I a as an indicator function with I a = 1 if a is true and 0 otherwise. A hinge function is defined as [x] + = max{0, x}. Let x 1 , x 2 , x ∞ be the l 1 , l 2 , and l ∞ norms of a vector x, respectively. For a set S = {s 1 , · · · , s n } ⊂ R n , we define its ranked version as an ordered set (ranked list) S = s [1] , · · · , s [n] , with s [1] ≥ · · · ≥ s [n] , obtained by sorting elements of S in the descending order. |S| is the cardinality of a set S. s [k] denotes the top-k element, which is the k-th largest element in S (as well as S ). Without loss of generality, we only consider S with no ties since ties can be broken in any consistent way.
## (s46) Hyperparameter Tuning
Number of References: 3

(p46.0) Rank-based losses rely heavily on important hyperparameters that can greatly affect the performance of the final model. For instance, in the case of AoRR aggregate loss, the hyperparameter m determines the number of possible outliers that are ignored during training. If m is set lower than the actual number of outliers, the model will be adversely affected by the outliers. Conversely, if m is set too high, some essential samples will be removed during training, resulting in a suboptimal final model. Several works have attempted to develop new learning strategies to determine the hyperparameters in model training. For example, Kawaguchi et al. used the AT k aggregator to design ordered SGD optimization methods in [96]. They employed an adaptive setting to decrease k when the model performance achieved preset-specific criteria. In [13], a similar method was applied for learning the AoRR aggregate loss. They used greedy search to determine the hyperparameter m. However, these methods may not be efficient for large-scale datasets. Therefore, in [14], Hu et al. proposed an auxiliary learning framework to determine the hyperparameters using a clean dataset, which is extracted from the original dataset that may contain outliers. However, this method may not work when a clean dataset is unavailable. Therefore, exploring methods for determining hyperparameters in rank-based decomposable losses may be a promising future direction.
