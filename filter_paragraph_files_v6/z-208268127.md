# Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms

CorpusID: 208268127 - [https://www.semanticscholar.org/paper/54d4a221db5a91a2487b1610374843fafff5a23d](https://www.semanticscholar.org/paper/54d4a221db5a91a2487b1610374843fafff5a23d)

Fields: Computer Science, Mathematics

## (s32) Policy-Based Methods
Number of References: 14

(p32.0) For continuous games, due to the general negative results therein, Mazumdar and Ratliff (2018) introduces a new class of games, Morse-Smale games, for which the gradient dynamics correspond to gradient-like flows. Then, definitive statements on almost sure convergence of PG methods to either limit cycles, Nash equilibria, or non-Nash fixed points can be made, using tools from dynamical systems theory. Moreover, Balduzzi et al. (2018); Letcher et al. (2019) have studied the second-order structure of game dynamics, by decomposing it into two components.The first one, named symmetric component, relates to potential games, which yields gradient descent on some implicit function; the second one, named antisymmetric component, relates to Hamiltonian games that follows some conservation law, motivated by classical mechanical systems analysis. The fact that gradient descent converges to the Nash equilibrium of both types of games motivates the development of the Symplectic Gradient Adjustment (SGA) algorithm that finds stable fixed points of the game, which constitute all local Nash equilibria for zero-sum games, and only a subset of local NE for general-sum games. Chasnov et al. (2019) provides finitetime local convergence guarantees to a neighborhood of a stable local Nash equilibrium of continuous games, in both deterministic setting, with exact PG, and stochastic setting, with unbiased PG estimates. Additionally, Chasnov et al. (2019) has also explored the effects of non-uniform learning rates on the learning dynamics and convergence rates. Fiez et al. (2019) has also considered general-sum Stackelberg games, and shown that the same two-timescale algorithm update as in the zero-sum case now converges almost surely to the stable attractors only. It has also established finite-time performance for local convergence to a neighborhood of a stable Stackelberg equilibrium. In complete analogy to the zero-sum class, these convergence results for continuous games do not apply to MARL in Markov games directly, as they are built upon the differentiability/smoothness of the long-term return, which may not hold for general MGs, for example, LQ games (Mazum-dar et al., 2019).
## (s40) Other Applications
Number of References: 12

(p40.0) Furthermore, another popular testbed of MARL is the StarCraft II (Vinyals et al., 2017), which is an immensely popular multi-player real-strategy computer game. This game can be formulated as a multi-agent Markov game with partial observation, where each player has only limited information of the game state. Designing reinforcement learning systems for StarCraft II is extremely challenging due to the needs to make decisions under uncertainty and incomplete information, to consider the optimal strategy in the long-run, and to design good reward functions that elicits learning. Since released, both the full-game and sub-game version of StarCraft II have gained tremendous research interest. A breakthrough in this game was achieved by AlphaStar, recently proposed in , which has demonstrated superhuman performances in zero-sum two-player full-game StarCraft II. Its reinforcement learning algorithm combines LSTM for the parametrization of policy and value functions, asynchronous actor-critic (Mnih et al., 2016) for policy updates, and neural fictitious self-play (Heinrich and Silver, 2016) for equilibrium finding.

(p40.1) Furthermore, another popular testbed of MARL is the StarCraft II (Vinyals et al., 2017), which is an immensely popular multi-player real-strategy computer game. This game can be formulated as a multi-agent Markov game with partial observation, where each player has only limited information of the game state. Designing reinforcement learning systems for StarCraft II is extremely challenging due to the needs to make decisions under uncertainty and incomplete information, to consider the optimal strategy in the long-run, and to design good reward functions that elicits learning. Since released, both the full-game and sub-game version of StarCraft II have gained tremendous research interest. A breakthrough in this game was achieved by AlphaStar, recently proposed in , which has demonstrated superhuman performances in zero-sum two-player full-game StarCraft II. Its reinforcement learning algorithm combines LSTM for the parametrization of policy and value functions, asynchronous actor-critic (Mnih et al., 2016) for policy updates, and neural fictitious self-play (Heinrich and Silver, 2016) for equilibrium finding.
## (s41) Mixed Settings
Number of References: 6

(p41.0) Compared to the cooperative and competitive settings, research on MARL under the mixed setting is rather less explored. One application in this setting is multi-player poker. As we have mentioned in ยง5.2, Pluribus introduced in Brown and Sandholm (2019) has demonstrated superhuman performances in six-player no-limit Texas hold'em. In addition, as an extension of the problem of learning to communicate, introduced in ยง5.1, there is a line of research that aims to apply MARL to tackle learning social dilemmas, which is usually formulated as a multi-agent stochastic game with partial information. Thus, most of the algorithms proposed under these settings incorporate RNN or LSTM for learning representations of the histories experience by the agent, and the performances of these algorithms are usually exhibited using experimental results; see, e.g., Leibo et al. (2017); Lerer and Peysakhovich (2017); Hughes et al. (2018), and the references therein.
## (s77) Policy-Based Methods
Number of References: 14

(p77.0) For continuous games, due to the general negative results therein, Mazumdar and Ratliff (2018) introduces a new class of games, Morse-Smale games, for which the gradient dynamics correspond to gradient-like flows. Then, definitive statements on almost sure convergence of PG methods to either limit cycles, Nash equilibria, or non-Nash fixed points can be made, using tools from dynamical systems theory. Moreover, Balduzzi et al. (2018); Letcher et al. (2019) have studied the second-order structure of game dynamics, by decomposing it into two components.The first one, named symmetric component, relates to potential games, which yields gradient descent on some implicit function; the second one, named antisymmetric component, relates to Hamiltonian games that follows some conservation law, motivated by classical mechanical systems analysis. The fact that gradient descent converges to the Nash equilibrium of both types of games motivates the development of the Symplectic Gradient Adjustment (SGA) algorithm that finds stable fixed points of the game, which constitute all local Nash equilibria for zero-sum games, and only a subset of local NE for general-sum games. Chasnov et al. (2019) provides finitetime local convergence guarantees to a neighborhood of a stable local Nash equilibrium of continuous games, in both deterministic setting, with exact PG, and stochastic setting, with unbiased PG estimates. Additionally, Chasnov et al. (2019) has also explored the effects of non-uniform learning rates on the learning dynamics and convergence rates. Fiez et al. (2019) has also considered general-sum Stackelberg games, and shown that the same two-timescale algorithm update as in the zero-sum case now converges almost surely to the stable attractors only. It has also established finite-time performance for local convergence to a neighborhood of a stable Stackelberg equilibrium. In complete analogy to the zero-sum class, these convergence results for continuous games do not apply to MARL in Markov games directly, as they are built upon the differentiability/smoothness of the long-term return, which may not hold for general MGs, for example, LQ games (Mazum-dar et al., 2019).
## (s85) Other Applications
Number of References: 12

(p85.0) Furthermore, another popular testbed of MARL is the StarCraft II (Vinyals et al., 2017), which is an immensely popular multi-player real-strategy computer game. This game can be formulated as a multi-agent Markov game with partial observation, where each player has only limited information of the game state. Designing reinforcement learning systems for StarCraft II is extremely challenging due to the needs to make decisions under uncertainty and incomplete information, to consider the optimal strategy in the long-run, and to design good reward functions that elicits learning. Since released, both the full-game and sub-game version of StarCraft II have gained tremendous research interest. A breakthrough in this game was achieved by AlphaStar, recently proposed in , which has demonstrated superhuman performances in zero-sum two-player full-game StarCraft II. Its reinforcement learning algorithm combines LSTM for the parametrization of policy and value functions, asynchronous actor-critic (Mnih et al., 2016) for policy updates, and neural fictitious self-play (Heinrich and Silver, 2016) for equilibrium finding.

(p85.1) Furthermore, another popular testbed of MARL is the StarCraft II (Vinyals et al., 2017), which is an immensely popular multi-player real-strategy computer game. This game can be formulated as a multi-agent Markov game with partial observation, where each player has only limited information of the game state. Designing reinforcement learning systems for StarCraft II is extremely challenging due to the needs to make decisions under uncertainty and incomplete information, to consider the optimal strategy in the long-run, and to design good reward functions that elicits learning. Since released, both the full-game and sub-game version of StarCraft II have gained tremendous research interest. A breakthrough in this game was achieved by AlphaStar, recently proposed in , which has demonstrated superhuman performances in zero-sum two-player full-game StarCraft II. Its reinforcement learning algorithm combines LSTM for the parametrization of policy and value functions, asynchronous actor-critic (Mnih et al., 2016) for policy updates, and neural fictitious self-play (Heinrich and Silver, 2016) for equilibrium finding.
## (s86) Mixed Settings
Number of References: 6

(p86.0) Compared to the cooperative and competitive settings, research on MARL under the mixed setting is rather less explored. One application in this setting is multi-player poker. As we have mentioned in ยง5.2, Pluribus introduced in Brown and Sandholm (2019) has demonstrated superhuman performances in six-player no-limit Texas hold'em. In addition, as an extension of the problem of learning to communicate, introduced in ยง5.1, there is a line of research that aims to apply MARL to tackle learning social dilemmas, which is usually formulated as a multi-agent stochastic game with partial information. Thus, most of the algorithms proposed under these settings incorporate RNN or LSTM for learning representations of the histories experience by the agent, and the performances of these algorithms are usually exhibited using experimental results; see, e.g., Leibo et al. (2017); Lerer and Peysakhovich (2017); Hughes et al. (2018), and the references therein.
