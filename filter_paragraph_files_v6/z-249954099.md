# Synthetic Data-Based Simulators for Recommender Systems: A Survey

CorpusID: 249954099 - [https://www.semanticscholar.org/paper/74aa80b9c39724146682a55642de59002ed2460c](https://www.semanticscholar.org/paper/74aa80b9c39724146682a55642de59002ed2460c)

Fields: Computer Science

## (s3) Simulator's approbation
Number of References: 4

(p3.0) This block of criteria is motivated by modern requirements on the reproducibility of research in the field of RSs development. In particular, we include these criteria in order to follow the general line of Dacrema et al. (2021); Winecoff et al. (2021) about the need for careful analysis of the interpretability and reproducibility in the field. In this regard, we distinguish simulators by that they have:
## (s23) Bias and other negative effects studied by simulation
Number of References: 2

(p23.0) Let us consider a typical situation in an RS lifecycle. An e-commerce company offers some products to users and exploits an RS for that. Meanwhile, data scientists in the company work on improving the existing RS. They do offline development as well as evaluation and devise a new RS that outperforms the old one in some offline accuracy-based Parapar and Radlinski (2021) metric like nDCG 10 . Then it is a pretty common situation Huzhang et al. (2021) that once the new RS is deployed in production, the online performance of the new RS is worse than the performance of the old one. This is the example of offline-online inconsistency so ,in this section, we explore its origins and the solutions simulators may offer to it.
## (s25) Unrealistic metrics
Number of References: 2

(p25.0) To evaluate an RS offline, a typical procedure consists of dividing the available data into training and test sets. The RS is trained on the training set, part of which is usually used for validation. After training, RS is used for making predictions on the test set and comparing predictions with real data using some metrics. These metrics are called accuracy-based since they are computed on a test set using the true responses. The most frequent ones are Precision@K, Recall@K, AUC@K, nDCG@K, MAP@K. However, it has been noticed that the users' behavior is more complex than these metrics would imply. In particular, users would like to discover new content, and value diverse and surprising recommendations. Hence, there are beyond accuracy metrics proposed to evaluate an RS output: Novelty, Serendipity, Unexpectedness, Diversity and others Silveira et al. (2019).
## (s58) Simulator's approbation
Number of References: 4

(p58.0) This block of criteria is motivated by modern requirements on the reproducibility of research in the field of RSs development. In particular, we include these criteria in order to follow the general line of Dacrema et al. (2021); Winecoff et al. (2021) about the need for careful analysis of the interpretability and reproducibility in the field. In this regard, we distinguish simulators by that they have:
## (s78) Bias and other negative effects studied by simulation
Number of References: 2

(p78.0) Let us consider a typical situation in an RS lifecycle. An e-commerce company offers some products to users and exploits an RS for that. Meanwhile, data scientists in the company work on improving the existing RS. They do offline development as well as evaluation and devise a new RS that outperforms the old one in some offline accuracy-based Parapar and Radlinski (2021) metric like nDCG 10 . Then it is a pretty common situation Huzhang et al. (2021) that once the new RS is deployed in production, the online performance of the new RS is worse than the performance of the old one. This is the example of offline-online inconsistency so ,in this section, we explore its origins and the solutions simulators may offer to it.
## (s80) Unrealistic metrics
Number of References: 2

(p80.0) To evaluate an RS offline, a typical procedure consists of dividing the available data into training and test sets. The RS is trained on the training set, part of which is usually used for validation. After training, RS is used for making predictions on the test set and comparing predictions with real data using some metrics. These metrics are called accuracy-based since they are computed on a test set using the true responses. The most frequent ones are Precision@K, Recall@K, AUC@K, nDCG@K, MAP@K. However, it has been noticed that the users' behavior is more complex than these metrics would imply. In particular, users would like to discover new content, and value diverse and surprising recommendations. Hence, there are beyond accuracy metrics proposed to evaluate an RS output: Novelty, Serendipity, Unexpectedness, Diversity and others Silveira et al. (2019).
