# False Information on Web and Social Media: A Survey

CorpusID: 5058880 - [https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452](https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452)

Fields: Computer Science

## (s8) 3.2.2
Number of References: 6

(p8.0) Formation of echo-chambers. Given the advent of improved recommendation algorithms which promote personalized content for easy user access and exposure, social media platforms are often party to an "echo-chamber" effect [22]. This effect primarily refers to the self-selective polarizing effect of content where people immerse themselves in social circles in such a way that they are primarily exposed to content that agree with their beliefs. For example, a political liberal might friend more liberals on Facebook, thumbs-up liberal-minded content, and thus constantly be exposed to posts and news which aligns with his worldview. Figure 4 visualizes this echo-chamber effect on Twitter on a controversial topic of #beefban, where red and blue nodes represent users with opposing beliefs and edges represent who-retweets-whom, as shown by Garimella et al. [31]. Notice that both groups are mostly disconnected with few messages between nodes of different types. The echo-chamber effect in social networks is substantiated by Nikolov et al. [67] by demonstrating that the diversity of sources (links) clicked by users is significantly lower on social media platforms than in general search engines. Several studies have studied the effects and causes of echo-chambers. Quattrociocchi et al. [78] demonstrated that such resulting echo-chambers can serve to polarize the user's viewpoints by means of confirmation bias and lead to less diverse exposure and discussion between unaligned users. The resulting echo-chambers can contribute to the spread of false information by lowering the bar for critical fact-checking. Moreover, Trilling et al. [102] and Zajonc [116] posited that the perceived accuracy of false information increases linearly with the frequency of exposure of a participant to the same false information. This suggests that familiarity with repeatedly shared content (highly common and expected in echo-chambers) increases the perceived accuracy of the content, irrespective of its credibility. This calls for research on how to create effective techniques to break echo-chambers and slow down false information spread.
## (s14) Textual characteristics.
Number of References: 2

(p14.0) Since most reviews include textual content, researchers have extensively studied textual and linguistic features for discerning review fraud. Several works have posited that review fraudsters minimize effort by repeating the same reviews. Jindal et al. [43] provided the first well-known characterizations of review fraud, in which the authors characterized duplicate reviews (according to Jaccard similarity) across Amazon data as cases of fraud. The authors showed that many of these fraudulent duplicate reviews were from the same user on different products, rather than different users on the same product or different products. Figure 6 shows the distribution of maximum similarity between two reviewers' reviews. At the higher similarity end, 6% of the reviewers with more than one review have a maximum similarity score of 1, which is a sudden jump indicating that many reviewers copy reviews. Furthermore, Sandulescu et al. [82] showed that many review fraudsters adjust their reviews slightly so as not to post near or exactly similar reviews and be easily caught-instead, these sophisticated fraudsters tend to post semantically similar text (i.e. instead of duplicating "the hotel room had an excellent view, " the fraudster might post "the hotel room had a superb view" instead).
## (s23) 5.2.5
Number of References: 6

(p23.0) Debunking characteristics. Once false information spreads, attempts are made to debunk it and limit its spread. Recent research has shown that there is a significant time delay between the spread and its debunking. Zubiaga et al. [121] found that true information tends to be resolved faster than false information, which tends to take about 14 hours to be debunked. Shao et al. [86] came to a similar conclusion-they found a delay of 10-20 hours between the start of a rumor and sharing of its fact-checking contents.

(p23.1) But once debunking information reaches the rumor spreaders, do they stop spreading it or does it 'back-fire', as observed in in-lab settings [68] where corrections led to an increase in misperception? Several empirical studies on web-based false information suggest that debunking rumors is in fact effective, and people start deleting and questioning the rumor when presented with corrective information. Frigerri et al. [30] studied the spread of thousands of rumor reshare cascades on Facebook, and found that false information is more likely to be linked to debunking articles than true information. Moreover, once it is linked, it leads to a 4.4 times increase in deletion probability of false information than when it is not, and the probability is even higher if the link is made shortly after the post is created. Moreover, Zubiaga et al. [121] found that there are more tweets denying a rumor than supporting it after it is debunked, while prior to debunking, more tweets support the rumor. Furthermore, Vosoughi et al. [105] showed that there is a striking difference between replies on tweet containing false information than those containing true information-while people express fear, disgust, and surprise in replies, true information generates anticipation, sadness, joy, and trust. These differences can potentially be used to create early detection and debunking tools.
