# Computation-efficient Deep Learning for Computer Vision: A Survey

CorpusID: 261243890
 
tags: #Engineering, #Computer_Science

URL: [https://www.semanticscholar.org/paper/a87947f88519dba980d0f16cdfb78ed09a8e02f0](https://www.semanticscholar.org/paper/a87947f88519dba980d0f16cdfb78ed09a8e02f0)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

Computation-efficient Deep Learning for Computer Vision: A Survey


Yulin Wang 
Department of Automation
Tsinghua University ‡ Huawei Inc
BNRist

Yizeng Han 
Department of Automation
Tsinghua University ‡ Huawei Inc
BNRist

Chaofei Wang 
Department of Automation
Tsinghua University ‡ Huawei Inc
BNRist

Shiji Song 
Department of Automation
Tsinghua University ‡ Huawei Inc
BNRist

Qi Tian 
Department of Automation
Tsinghua University ‡ Huawei Inc
BNRist

Gao Huang gaohuang@tsinghua.edu.cn 
Department of Automation
Tsinghua University ‡ Huawei Inc
BNRist

Computation-efficient Deep Learning for Computer Vision: A Survey

Over the past decade, deep learning models have exhibited considerable advancements, reaching or even exceeding human-level performance in a range of visual perception tasks. This remarkable progress has sparked interest in applying deep networks to real-world applications, such as autonomous vehicles, mobile devices, robotics, and edge computing. However, the challenge remains that state-of-the-art models usually demand significant computational resources, leading to impractical power consumption, latency, or carbon emissions in real-world scenarios. This trade-off between effectiveness and efficiency has catalyzed the emergence of a new research focus: computationally efficient deep learning, which strives to achieve satisfactory performance while minimizing the computational cost during inference. This review offers an extensive analysis of this rapidly evolving field by examining four key areas: 1) the development of static or dynamic light-weighted backbone models for the efficient extraction of discriminative deep representations; 2) the specialized network architectures or algorithms tailored for specific computer vision tasks; 3) the techniques employed for compressing deep learning models; and 4) the strategies for deploying efficient deep networks on hardware platforms. Additionally, we provide a systematic discussion on the critical challenges faced in this domain, such as network architecture design, training schemes, practical efficiency, and more realistic model compression approaches, as well as potential future research directions.

# Introduction

Over the past decade, the field of computer vision has experienced significant advancements in deep learning. Innovations in model architectures and learning algorithms [1,2,3,4,5,6,7] have allowed deep networks to approach or even exceed human-level performance on benchmark competition datasets for a wide range of visual tasks, such as image recognition [6,7], object detection [8], image segmentation [9,10], video understanding [11,12], and 3D perception [13]. This considerable progress has stimulated interest in deploying deep models in practical applications, including self-driving cars, mobile devices, robotics, unmanned aerial vehicles, and internet of things devices [14,15,16].

However, the demands of real-world applications are distinct from those of competitions. Models achieving state-of-the-art accuracy in competitions often exhibit computational intensity and resource requirements during inference. In contrast, computation is typically equivalent to practical latency, power consumption, and carbon emissions. Low-latency or real-time inference is crucial for ensuring security and enhancing user experience [17,18,19,20]. Deep learning systems must prioritize low power consumption to improve battery life or reduce energy costs [21,22,22,23]. Minimizing carbon emissions is also essential for environmental considerations [24,25]. Motivated by these practical challenges, a substantial portion of recent literature focuses on achieving a balance between effectiveness and computational efficiency. Ideally, deep Introduction Figure 1: An overview of the survey. We first review the design of backbone networks, which are divided into static and dynamic models. Then we discuss the design of task-specialized algorithms and network architectures. Finally, we summarize the model compression approaches and the efficient hardware deployment techniques. learning models should yield accurate predictions while minimizing the computational cost during inference. This topic has given rise to numerous intriguing research questions and garnered significant attention from both academic and industrial sectors.

In light of these developments, this survey presents a comprehensive and systematic review of the exploration towards computationally efficient deep learning. Our aim is to provide an overview of this rapidly evolving field, summarize recent advances, and identify important challenges and potential directions for future research. Specifically, we will discuss existing works from the perspective of the following five directions:

1) Efficient Backbone Models. Designing light-weighted backbone networks that effectively extract discriminative deep representations from images, videos or 3D scenes with minimal computation by optimizing both efficient network micro-architectures (e.g., operators, modules, and layers) [26,27,28] and improving the system-level organization of micro-architectures [29,30]. Recent advances in neural architecture search (NAS) [31,32] have further enabled the automatic design of backbones.

2) Dynamic Deep Networks. Developing dynamic networks is an important emerging research direction for improving computational efficiency. These networks break the limits of static computational graphs and propose adapting their structures or parameters to the input during inference [33]. For example, the model can selectively activate certain model components (e.g., layers [34], channels [35], and sub-networks [36]) based on each test input or allocate less computation to less informative spatial/temporal regions [37,38,39]  The "split-transform-merge" architecture in representative computationally efficient deep networks. These blocks are typically adopted as basic components to build models. Here "Conv" refers to a convolutional layer. ResNet 3) Task-specialized Efficient Models. Numerous works focus on building task-specific heads on top of the features from light-weighted static/dynamic backbones to efficiently accomplish specific computer vision tasks. Examples include fast one-stage models for real-time object detection [40,41,42,43], the efficient multi-branch architecture for semantic segmentation [44], and end-to-end instance segmentation frameworks [45,46].


## 4) Model Compression Techniques.

Orthogonal to network architecture design, many algorithms have been proposed to compress relatively large models with minimal accuracy loss. This can be achieved by pruning less important network components [47,48], quantizing parameters [49,50], or distilling knowledge from large models to smaller models of interest [51,52].


## 5) Efficient Deployment on Hardware.

To achieve high practical efficiency, it is necessary to consider hardware requirements when developing deep learning applications. Reducing latency on specific hardware devices is usually treated as an objective in network design [53,54] or algorithm-hardware co-design [55,56]. Additionally, several acceleration tools have been developed for efficient deployment of deep learning models [57,58,59].

While some relevant surveys exist [14,60], our survey is more up-to-date and comprehensive in several crucial aspects: 1) we systematically review model design techniques for images, videos, and 3D vision; 2) we summarize the recent works on designing dynamic deep neural networks for efficient inference; and 3) we thoroughly discuss the specialized models for accomplishing the most common and challenging computer vision tasks, e.g., object detection and image segmentation.

The rest of this survey is organized as follows (see Figure 1 for the overview). In Sec. 2 and 3, we introduce the design of efficient static and dynamic backbone networks, respectively. In Sec. 4, the methodology for designing task-specialized efficient models is reviewed. The techniques for compressing deep learning models are investigated in Sec. 5. Efficient hardware deployment approaches are summarized in Sec. 6. Lastly, we discuss existing challenges and future directions in Sec. 7.


# Architecture Design of Backbone Networks

Typically, deep learning models for computation vision tasks incorporate two components, i.e., 1) a backbone network that extracts deep representations from the raw inputs (e.g., images, video frames, and point clouds); and 2) a task-specific head that is designed specialized for the task of interest. The deep features obtained from backbone networks are fed into the head to accomplish the corresponding task. The outputs of backbones (i.e., the inputs of the head) are usually assumed to have similar formats, while the outputs of the head are tailored for the tasks of interest.

In this section, we focus on how to design a computational-efficient general-purpose backbone network. Our discussions will start from processing the most fundamental data form, 2D images, where a lightweighted network may be obtained by either manual design (Sec. 2.1) or automatic searching approaches (Sec. 2.2). Then we will discuss the backbones for processing videos (Sec. 2.3) and understanding 3D scenes (Sec. 2.4).


## Efficient Models by Manual Design

A considerable number of efficient backbone networks are designed manually based on theoretical derivations, empirical observations, or heuristics. Existing works can be categorized into two levels according the granularity of modifying the network: micro-architecture (Sec. 2.1.1) and macro-architecture (Sec. 2.1.2)


### Micro-architecture

The micro-architecture refers to the individual layers, modules, and neural operators of backbones. These basic components are the foundation for constructing deep networks. Many works seek to attain higher computational efficiency by improving them. Notably, these works usually serve as off-the-shelf plug-in components that can be employed together with other techniques.

1) Split-transform-merge Strategy. Typically, deep networks consist of multiple successively stacked layers with dense connection, where all the input neurons is connected to every output neuron. Formally, ℓ-th layer f ℓ with inputs x ℓ−1 and outputs x ℓ can be expressed by
x ℓ = f ℓ (x ℓ−1 ).(1)
However, such dense layers tend to computationally intensive. To address this issue, researchers have proposed to replace the dense connection with particularly designed topologies [1,3,61], which dramatically reduces the computational complexity, yet yields a competitive or stronger representation ability. Among existing works, one of the most popular designs is the split-transform-merge strategy, as shown in the following (as a fundamental component, a residual connection [4] is added here):
{x ℓ−1 1 , . . . , x ℓ−1 C } = f ℓ split (x ℓ−1 ), x ℓ = x ℓ−1 + f ℓ merge (f ℓ 1 (x ℓ−1 1 ), . . . , f ℓ C (x ℓ−1 C )),(2)where x ℓ−1 is split into C embeddings x ℓ−1 1 , . . . , x ℓ−1 C
with lower dimensions by a cheap operator f ℓ split (·). The low-dimensional embeddings are processed by the transform functions f ℓ 1 (·), . . . , f ℓ C (·), whose input/output dimensions are the same. Notably, each of these functions corresponds to a dense layer, but this procedure is efficient due to the reduced input feature dimension. The processed embeddings are merged by f ℓ merge (·). In the following, we will first discuss the design of the transform and split/merge functions respectively, and then introduce recent improvements over the "split-transform-merge" paradigm. Table 1 summarizes some representative split-transform-merge architectures in popular computationally efficient deep networks. a) "Transform" -homogeneous multi-branch architecture. A straightforward choice is to let f ℓ c (·), c = 1, . . . , C have the same architecture, and differentiate each other only in the values of the learnable parameters. This design is named as grouped convolution [1,61] when f ℓ c (·) corresponds to a convolutional layer, and is adopted in many efficient ConvNets [61,66,67]. IGCV [68,69,70] further introduces a permutation operation to facilitate the interaction of different groups of embeddings (i.e., x ℓ−1 c , c = 1, . . . , C). In addition to convolution, this design is also widely adopted in the self-attention layers of vision Transformers (ViTs) [6,65]. In ViTs, it is name as multi-head self-attention, where f ℓ c (·) is the scaled dot-product attention [71]. In particular, the grouped convolution is named as depth-wise separable convolution when C is equal to the channel number of x ℓ−1 . Here f ℓ c (·) typically corresponds to a single convolution operation, where a large kernel size with sufficient receptive fields can be used without dramatically increasing the computational cost. This highly efficient component is first proposed in MobileNet [26], and adopted adopted in a wide variety of follow-up models [27,28,29,63,64,72,73,74,75,76]. b) "Transform" -heterogenous multi-branch architecture. Another line of works focus on developing nonequivalent branches, where each f ℓ c (·), c = 1, . . . , C is assigned with a specialized architecture or task. For example, the Inception architectures [3,77,78,79,80] adopt a varying receptive fields for different branches (e.g., by changing the convolution kernel size), aiming to aggregate the discriminative information at multiple levels. Recent works further extend this idea by feeding the outputs of f ℓ c (·) to f ℓ c+1 (·) [62,73,81], and thus integrating multi-scale features into the outputs. c) "Split/merge" functions f ℓ split (·) and f ℓ merge (·) are designed to map the features into or back from lowdimension embeddings with minimal cost. Most works adopt similar architectures for these two components: f ℓ split (·) corresponds to 1×1 convolution, while f ℓ merge (·) is accomplished by concatenation or concatenation + 1×1 convolution. Representative examples include ResNeXt [61], MobileNets [26,27,28] and Inception networks [3,77,78,79,80]. In particular, ShuffleNet [63] presents a more efficient design by combining 1×1 grouped convolution with channel shuffle. d) Improved paradigms over "split-transform-merge". More recently, some works start to rethink the limitations of the split-transform-merge paradigm, and find that more efficient deep networks can be obtained by breaking this design principle. For example, motivated by the success of ViTs, ConvNeXt [74] explicitly introduces an multilayer perceptron (MLP) at each layer by reverse f ℓ split (·) and the depth-wise separable convolution (i.e., f ℓ c (·)). EfficientNetV2 [82] replace f ℓ c (·) and f ℓ split (·) with a regular dense convolutional layer at earlier layers, achieving higher practical efficiency on GPU devices. MobileNeXt [83] moves the depth-wise convolution layers to the two ends of the residual path to encode more expressive spatial information.

2) Inverted Bottleneck. Bottleneck [4] is a widely-used efficient component in ConvNets. Its basic architecture can be understood on top of Eq. (2): the total channel number of {x ℓ−1 1 , . . . , x ℓ−1 C } will be reduced compared to x ℓ−1 (e.g., by 4× in ResNet [4]). Consequently, the computationally intensive operations f ℓ 1 (·), . . . , f ℓ C (·) are performed on the low-dimensional embeddings, and the overall cost is saved. The effectiveness of this bottleneck is validated in both dense layers (C = 1) [4] and grouped convolution [61]. However, it may be sub-optimal in depth-wise separable convolution, where its low-dimensional transform results in information loss [27]. Inspired by this observation, MobileNetV2 [27] achieves an improved efficiency by proposing an inverted bottleneck, i.e., {x ℓ−1 1 , . . . , x ℓ−1 C } have more dimensions than x ℓ−1 (e.g., 6× [27]). This designed is further adopted by a number of recent works [70,74,83].

3) Feature Reusing. Conventionally, the successive linear connection is the dominant topology for network design. The inputs are fed into a layer and transformed to obtained the inputs of the next layer. Any feature will be utilized for only a single time. Although being straightforward, this design is usually suboptimal from the lens of computational efficiency. An important idea for lighted-weighted models is to reuse the have-been-used features.

a) Inter-layer feature reusing. A basic idea is to reuse the features from previous layers. The skip-layer residual connection [4,84] adds the inputs of each layer to the outputs, contributing the effective training of very deep and computationally more efficient networks. A more general formulation is established by dense connection [5,85], where all the previous features are fed into a next layer. CondenseNets [86,87] extend this architecture by automatically learning the inter-layer connection topology. In contrast, other works like ShuffleNetV2 [64] and G-GhostNet [88] focus on manually designing inter-layer interaction mechanisms. b) Intra-layer feature reusing. The idea of feature reusing can also be leveraged within each network layers. For example, GhostNets [89,90] demonstrate that there exist considerable redundancy in the outputs of each layer. They first obtain a small set of intrinsic output features, which are not only used as the inputs of the next layer, but also reused to generating other output features using cheap operations like linear transformations.

4) Feature Down-sampling. Extracting deep representations from image-based data typically yields feature maps, which inherently have spatial sizes (i.e., height and weight). This property can be leveraged to reduce the computational cost of models e.g., introducing properly configured feature down-sampling modules.

a) Processing feature maps efficiently. The cost of processing feature maps grows quadratically with respect to their height/weight. OctConv [91] finds that processing all the features with the same resolution is not an optimal design. They propose to process a group of features at a down-sampled scale to capture only the low-frequency information, while the remaining features are designed to recognize high-frequency patterns, and the two groups exchange information after each layer. Consequently, the overall computational cost is reduced. This idea is also effective in ViTs [92]. Similarly, HRNets [93,94] and HRFormer [95] maintain multi-resolution features at each layer, aiming to efficiently extract multi-scale discriminative representations for various computer vision tasks in the meantime.

b) Facilitating efficient self-attention. Particularly, feature down-sampling can be embedded into self-attention operations in ViTs to improve its efficiency. For example, PVTs [96,97] and ShuntedViT [98] propose to compute attention maps efficiently with down-sampled feature maps. Twins [99] perform selfattention on low-resolution features to aggregate global information efficiently. 5) Efficient Self-attention. ViTs [6] have achieved remarkable success in the fields of computer vision. Their self-attention mechanisms enable adaptively aggregating information across the entire image, yielding excellent scalability with the growing dataset scale or model size. However, vanilla self-attention suffers from high computational cost. A considerable number of recent visual backbones focus on developing more efficient self-attention modules without sacrificing their performance. a) Locality-inspired Self-attention. In this direction, an important idea is drawn from the success of Con-vNets: exploiting the locality of images, i.e., encouraging the models to aggregate more information from adjacent spatial regions. Swin Transformers [7,100] achieve this by performing self-attention only within a square windows. Some other works extending this idea by designing different shapes of attention windows [101,102,103,104,105,106] or introducing soft local constraints to attention maps [107,108]. An important challenge faced by these works is how to model the interaction of different windows effectively. Possible solutions to address this issue include changing window positions [7,100], shuffling the channels [109], designing specialized window shapes [101,103,105,106], or further introducing window-level global self-attention modules [99,110,111].

b) SoftMax-free Self-attention. To reduce the inherent high computation complexity of self-attention, another line of research proposes to replace the SoftMax function in self-attention with separate kernel functions, yielding linear attention [112]. As representative examples, Performer [113] approximates SoftMax with orthogonal random features, while Nyströmformer [114] and SOFT [115] attain this goal through matrix decomposition. Castling-ViT [116] measures the spectral similarity between tokens with linear angular kernels. EfficientViT [117] further leverages depth-wise convolution to improve the local feature extraction ability of linear attention. FLatten Transformer proposes a focused linear attention module to achieve high expressiveness. [118].


### Macro-architecture

The macro-architecture refers to the system-level methodology of organizing micro-architectures (e.g., operators, modules and layers) and constructing the whole deep networks. Existing literature has revealed that, even with the same efficient micro-architectures, the approaches and configurations for combining them will significantly affect the computational efficiency of the resulting models. In the following, we will discuss the works and design principles relevant to this topic.


## 1) Marrying Convolution and Attention Modules.

Convolution and self-attention are both important modules with their own strengths. A considerable amount of literature has been published to study how to combine them for a higher overall computational efficiency. At the per-layer level, convolution can be leveraged to generate the inputs of self-attention, e.g., queries/keys/values [75,76] or position embeddings [119]. In addition, some works simultaneously utilize self-attention and a convolutional layer, and fuse their outputs [120,121], which facilitates the learning of local features. Another promising idea is to integrate convolution into the feed-forward network after the self-attention module [76,122,123].

At the network level, many existing works focus on the placing order of self-attention and depth-wise convolution blocks. In particular, leveraging convolution at earlier layers is proven beneficial [124,125,126,127,128], which enables the efficient extraction of local representations. Besides, convolutional blocks are usually adopted as light-weighted down-sample layers [103,127,129]. Another line of works parallelizes both a self-attention path and a convolution path in a single model [130,131,132,133,134,135], where the two paths typically interact in a layer-wise fashion.

2) Depth-width Relationship. In the context of ConvNets and hierarchical ViTs, the backbone models consist of multiple stages with progressively reduced feature resolution. The layers within each stage usually have the same width, while later stages are wider. The stage-wise width growing rule is an important configuration, where it is popular to adopt an exponential growth with base two [4,5,7]. In contrast, RegNets [30,136] further propose a more detailed principle: widths and depths of good networks can be explained by a quantized linear function.


## 3) Model Scaling.

On top of designing a single efficient model, it is also important to obtain a family of models that can adapt to varying computational budgets. An important principle for addressing this issue is compound scaling [29,82], which indicates that simultaneously increasing the depth, width and input resolution of a given base model will yield a family of efficient network architectures. Dollár et al. [137] further study how to design a proper model scaling rule in terms of the actual runtime. In addition, TinyNets [138] extend this idea to the shrinking of the model size.


## Automatic Architecture Design

Compared to manually designing backbones, another appealing idea is to find proper network architectures automatically, which is usually referred to as neural architecture search (NAS). In recent years, a number of existing works have investigated this idea through the lens of computational efficiency. In the following, we will discuss the basic computation-aware formulation of NAS (Sec. 2.2.1) and how the practical speed is considered in NAS (Sec. 2.2.2).


### Computation-aware NAS

Typically, NAS consists of two components: a searching space that contain a number of candidate architectures, and an algorithm to search for an optimal architecture. The computational cost for inferring the model is usually treated as a constraint, which is either inherently controlled by the searching space or strictly restricted by a pre-defined rule. The optimization objective is to maximize the validation accuracy.

1) Early Works. Early NAS methods propose to formulate a discrete searching space [31,139,140]. The network is viewed as a graph with a number of nodes connected by edges, where each edge corresponds to an operation and one needs to find the optimal operation for each edge. Such a problem can be solved with discrete optimization algorithms. For example, by viewing the validation performance as the rewards, one can leveraged off-the-shelf reinforcement learning methods [31,139,140]. Moreover, evolutionary algorithms also achieve favorable performance for discrete NAS [141,142,143].

2) Efficient Searching Algorithms. The aforementioned NAS methods are able to find computationally more efficient network architectures than human design. However, their searching cost is a notable limitation, since their search procedure usually incorporates training many candidate networks from scratch to convergence to evaluate their validation accuracy. Motivated by this issue, a large number of works focus on developing low cost NAS algorithms. A basic idea in this direction is to reuse the previous candidates, e.g., adding/deleting layers [117,144] and paths [145] on top of currently found architectures or adopting existing architectures as network components [146].

Driven by these preliminary explorations, ENAS [147] and DARTS [32] propose a parameter-sharing paradigm. They propose to construct a large computational graph that contains all possible connections and operations, such that each subgraph within it corresponds to a network architecture. The large graph is named as a super-net, while all possible candidate networks share the same super-net parameters. Hence, one can train the super-net, and directly sample architectures from it without retraining any specific candidate network. The network selection process is usually formulated to be differentiable and accomplished efficiently via gradient-based optimization methods [148,149,150,151,152]. Besides, some recent works focus on improving this procedure by introducing progressive searching mechanisms [153,154], introducing hyper-networks [155,156] or training more proper super-nets for NAS [157,158].


### Latency-aware Neural Architecture Search

From the lens of practical efficiency, an important challenge faced by NAS is the inference speed on real hardware (e.g., GPUs or CPUs). Since NAS usually leads to irregular network architectures, the obtained model with low theoretical computational cost may not be efficient in practice. To address this issue, recent NAS methods explicitly incorporate real latency into the optimization objective to achieve a good trade-off between real speed and accuracy [53,54,159]. As representative examples, MobileNetV3 [28] leverages hardware-aware NAS to obtain the basic architecture, and modifies it manually. Once-for-all [24] proposes to train a shared general super-nets, and perform NAS on top of it conditioned on the specific hardwares, yielding a state-of-the-art efficiency.


## Efficient Backbones for Video Understanding

In this subsection, we will focus on the efficient backbones for processing videos. Notably, videos consist of a series of frames, each of which is an image. In general, the aforementioned techniques for processing images are typically compatible with videos. Hence, here we mainly review the efficient modeling of the temporal relationships of video frames, including ConvNet-based (Sec. 2.3.1) and Transformer-based (Sec. 2.3.2) approaches.


### Efficient 3D ConvNets

The most straightforward approach to modeling temporal relationships may be introducing 3D convolutional layers [160,161], such that one can directly perform convolution in the space formed by frame height, width, and video duration. However, 3D convolution is computationally expensive, and many efficient backbones have been proposed to alleviate this problem.


## 1) Marrying 2D and 3D Convolution.

A basic idea is to avoid designing a pure 3D ConvNets, i.e., most of the feature extraction process may be accomplished by the efficient 2D convolution, while 3D convolution is only introduced at several particular positions. From the lens of macro-architecture, this goal can be attained by sequentially mixing 2D and 3D blocks, either first using 3D and later 2D or first 2D and later 3D [162,163]. At the micro-architecture level, the group-wise or depth-width 3D convolution can be integrated in to the transform module of 2D split-transform-merge architecture (Eq. (2)) [164,165].

2) (2+1)D Networks. Another elegant idea is to decompose 3D convolution into two components: a 2D convolution that extract representation from video frames, and a temporal operation that only focuses on learning the temporal relationships. The former can directly adopt 2D neural operators, while the latter can be implemented using 1D temporal convolution [166,167,168], adaptive 1D convolution [169], and MLPs [170].

3) 2D Networks. In addition to the aforementioned approaches, the models with only 2D convolution may also be able to model temporal relationships. This is typically achieved by designing zero-parameter operations. For example, subtracting the features of adjacent frames to extract the motion information [171,172]. The temporal-shift-based models [173,174,175] propose to shift part of the channels of 2D features along the temporal dimension, performing information exchange among neighboring frames efficiently.


## 4) Long/Short-term Separable Networks.

Another important idea is modeling long/short-term temporal dynamics with separate network architectures. An representative work in this direction is SlowFast [176], which incorporate a lower temporal resolution slow pathway and a higher temporal resolution fast pathway. Many recent works [172,177] further extend this idea.


### Transformer-based Video Backbones

Driven by the success of ViTs [6], a considerable number of recent works focus on facilitating efficient video understanding with self-attention-based models. In general, most of these works extend the aforementioned design ideas (including both image-based and video-based backbones) in the context of ViTs, e.g., performing spatial-temporal local self-attention [178,179,180], combining self-attention and convolution [181], and performing 1D temporal attention in (2+1)D designs [182,183,184,185].


## Efficient Backbones for 3D Vision

The perception and understanding of 3D scenes is not only a key ability of human intelligence, but also an important task for computer vision which are ubiquitous in real-world applications. In this subsection, we will review the backbones designed for processing 3D information efficiently. In general, the works in this direction can be categorized by the forms of model inputs, i.e., 3D 


### Point-based Models

A fundamental type of 3D geometric data structure is the cloud of 3D points, where each point is represented by its three coordinates. PointNet [186] is the pioneering work that leveraging deep learning to process 3D point clouds. It adopts point-wise feature extraction with shared MLPs to maintain the permutation invariance. PointNet++ [187] improves PointNet by facilitating capturing local geometric structures. On top of them, a number of works focus on how to aggregating local information effectively without increasing computational cost significantly. Representative approaches include introducing graph neural networks [188,189], projecting 3D points to regular grids to perform convolution [190,191,192,193], aggregating the features of adjacent points using the weights determined by the local geometric structure [194,195,196], and self-attention [197,198]. In particular, recent works have revealed that point-based models can achieve state-of-the-art computational efficiency with proper training and model scaling techniques [199].


### Voxel-based Models

The 3D point clouds can be further transformed to voxels, which are regular and can be directly processed with 3D convolution [200]. Typically, the 3D space is divided into cubic voxel grids, while the features of the points in each grid will be averaged. The side length of the grid is named as the voxel resolution. An important technique for processing voxels efficiently is sparse convolution [201,202,203], i.e., only performing convolution on the voxels with 3D points in them. Many works design backbone networks with this mechanism conditioned on the vision task of interest [204,205,206] for an optimal efficiency-accuracy trade-off. In addition, the point-based and voxel-based models can be combined to reduce the memory and computational cost [207]. Some recent works have explored the automatic backbone design using NAS [208] 


### Multi-view-based Models

Multi-view projective analysis is another effective solution for understanding 3D shapes, where the 3D objects are projected into 2D images from varying visual angles and processed by 2D backbone networks [209]. This idea can be implemented for recognition [210,211], retrieval [212,213] and pose estimation [214]. An important challenge for these methods is how to fuse the multi-view features. Existing works have proposed to leverage LSTM [213] or graph convolutional network [210].


# Dynamic Backbone Networks

Although the advanced architectures introduced in Sec. 2 have achieved significant progress in improving the inference efficiency of deep models, they generally have an intrinsic limitation: the computational graphs are kept the same during inference when processing different inputs with varying complexity. Such a static inference paradigm inevitably brings redundant computation on some "easy" samples. To address this issue, dynamic neural networks [33] have attracted great research interest in recent years due to their favorable efficiency, representation power, and adaptiveness [33].

Researchers have proposed various types of dynamic networks which can adapt their architectures/parameters to different inputs. Based on the granularity of adaptive inference, we categorize related works into sample-wise (Sec. 3.1), spatial-wise (Sec. 3.2), and temporal-wise (Sec. 3.3) dynamic networks. Compared to the previous work [33] which contains both vision and language models, we mainly focus on the computational efficient models for vision tasks in this survey. Moreover, more up-to-date works are included.


## Sample-wise Dynamic Networks

The most common adaptive inference paradigm is processing each input sample (e.g. an image) dynamically. There are mainly two lines of work in this direction: one aims at reducing the computation with decent network performance via dynamic architectures, and the other adjusts network parameters to boost the representation power with minor computational overhead. In this survey, we focus on the former line which typically reduces redundant computation for improving efficiency. Popular approaches include three types: 1) dynamic depth, 2) dynamic width, and 3) dynamic routing in a super network (SuperNet). Figure 2: Dynamic early exiting. When the prediction at an early exit satisfies some criterion (the green tick), the inference procedure terminates, and the later computation will be skipped.
Conv 1 Input … Conv ℓ … ✅ Inactivated

### Dynamic Depth

The inference procedure of a traditional (static) network can be written as
y = f (x) = f L • f L−1 • · · · • f 1 (x),(3)
where f ℓ , ℓ = 1, 2, . . . , L is the ℓ-th layer, and L is the network depth. In contrast, networks with dynamic depth process each sample x i with an adaptive number of layers:
y i = f (x) = f Li • f Li−1 • · · · • f 1 (x),(4)
where 1 ≤ L i ≤ L is decided based on x i itself. There are mainly two common implementations to realize dynamic depth. The first is early exiting, which means that the network predictions for some "easy" samples can be output at an intermediate layer without activating the deeper layers [215,216] (Figure 2). Researchers have found that multiple classifiers in a deep model may interfere with each other and degrade the performance by forcing early layers to capture semantic-level features [34]. To address this issue, multi-scale feature representation is adopted [34,217] to quickly produce coarse-scale features with rich semantic information. Instead of constructing intermediate exits in convolutional networks, the recent Dynamic Vision Transformer (DVT) [218] realizes early exiting in cascaded vision Transformers which process images with different token numbers. Dynamic Perceiver [219] proposes to integrate intermediate features and perform early-exit by introducing an addition attention-based path. Apart from architectural design, researchers have also proposed specialized techniques [220,221] for training early-exiting models. The aforementioned early-exiting methods dynamically terminate the forward propagation at a certain layer. An alternative approach to dynamic depth is skipping intermediate layers in models with skip connection such as ResNets [4] and vision Transformers [6] (Figure 3). Let x ℓ and f ℓ denote the feature and the computational unit at layer-ℓ, a typical implementation of layer skipping is using a gating module g ℓ (·) to dynamically decide whether to execute f ℓ [222,223,224]:
x ℓ+1 = x ℓ + g ℓ (x) · f ℓ (x), g ℓ (x) ∈ 0, 1.(5)

## Conv

Gating Module 0,1,0,1 Figure 4: Dynamic channel skipping, which uses a gating module to decide the computation of convolution channels.


### Dynamic Width

Instead of skipping an entire layer, a less aggressive approach is adjusting the network width to different inputs. In this direction, the most popular implementation is dynamically skipping the channels in convolutional blocks via a gating module [35,225,226,227] (Figure 4). Specifically, a gating module is first executed before conducting a convolution operation. The output of this gating module is a C-dimensional binary vector that decides whether to compute each channel, where C is the output channel number. This implementation is similar to that in the aforementioned layer-skipping scheme. The most prominent difference is that the output of the gating module in layer skipping is a scalar, and the gating module in channel-skipping is required to output a vector controlling the computation of different channels. Apart from convolution layers, the same idea can also be applied in vision Transformers to dynamically skip channels in multi-layer perceptron (MLP) blocks [224].


### Dynamic Routing in SuperNets

Instead of skipping the computation of layers or channels in conventional network architectures, one can also realize data-dependent inference via dynamic routing in super networks (SuperNets). A SuperNet usually contains various inference paths, and routing nodes are responsible for allocating each sample to the appropriate path. Let x ℓ i denote the i-th node in layer-ℓ, a general formulation of the computation for obtaining node-j in the next layer can be written as
x ℓ+1 j = i:α ℓ i→j >0 α ℓ i→j f ℓ i→j (x ℓ i ),(6)
where f ℓ i→j is the transformation between node i and j, and α ℓ i→j is the weight for this path which is calculated based on x ℓ i . If α ℓ i→j = 0, the transformation f ℓ i→j can be skipped. Extensive works have proposed different forms of SuperNets, such as tree structures [36,228,229], dynamic mixture-of-experts [230,231], and more general architectures [232,233].


## Spatial-wise Dynamic Networks

It has been found that different spatial locations in an image contribute unequally to the performance of vision tasks [234]. However, most existing deep models process different spatial locations with the same computation, leading to redundant computation on less important regions. To this end, spatial-wise dynamic networks are proposed to exploit the spatial redundancy in image data to achieve an improved efficiency. Based on the granularity of adaptive inference, we categorize relative works into pixel level, region level, and resolution level.


### Pixel-level Dynamic Networks

A typical approach to spatial-wise adaptive inference is dynamically deciding whether to compute each pixel in a convolution block based on a binary mask [235,236,237]. This form is similar to that in layer skipping and channel skipping (Sec. 3.1), except that the gating module is required to output a spatial mask. Each element of this spatial mask determines the computation of a feature pixel. In this way, the mask generators learn to locate the most discriminative regions in image features, and redundant computation on less informative pixels can be skipped.

The limitation of such pixel-level dynamic computation is that the acceleration is currently not supported by most deep learning libraries. The memory access cost can be heavier than static convolutions, and the computation parallelism is reduced due to sparse convolution. As a result, although the computation can be significantly reduced, the practical efficiency of these methods usually lags behind their theoretical efficiency. To this end, researchers have also proposed "coarse-grained" spatial-wise dynamic networks [39,238], which means that an element of a spatial mask can decide a patch rather than a pixel. In this way, more contiguous memory access is realized for realistic speedup. Moreover, the scheduling strategies are also proven to have a considerable effect on the inference latency [39]. It is also promising to co-design algorithm, scheduling, and hardware devices to better harvest the theoretical efficiency of spatial-wise dynamic networks.

Apart from skipping the computation of certain pixels, another line of work breaks the static reception field of traditional convolution and proposes deformable convolution [239,240,241]. Specifically, a lightweight module is used to learn the offsets for each feature pixel, and the convolution neighbors are sampled from arbitrary locations based on the predicted offsets. This idea has also been implemented in vision Transformers to enhance the performance of the local attention mechanism [242].


### Region-level Dynamic Networks

Instead of flexibly deciding which feature pixels to compute, another line of work aims at locating important regions (patches) in input images and cropping these patches for recognition tasks. For example, image recognition can be formulated as a sequential decision problem, in which an RNN is adopted to make predictions based on the cropped image patches [243,244]. A multi-scale CNN with multiple sub-networks could also be used to perform the classification task based on cropped salient image patches [245]. A lightweight module is placed between every two sub-networks to decide the coordinate and size of the salient patch.

Along this direction, the recent glance-and-focus network (GFNet) [246,247] proposes a general framework for region-level dynamic inference which is compatible with various visual backbones. It first "glances" a low-resolution input image, and then repeatedly "focus" on salient regions using reinforcement learning (RL) [248]. Moreover, early exiting (Sec. 3.1) is allowed, which means that the step number of "focus" can be dynamically adjusted for different input images.


### Resolution-level Dynamic Networks

Most existing vision models process different images with the same resolution. However, the input complexity could vary, and not all images require a high-resolution representation. Ideally, low-resolution representations should be sufficient for those "easy" samples with large objects and canonical features. The early work [249] proposes to adaptively zoom input images in the face detection task. The recent resolution adaptive network (RANet) [217] builds a multi-scale architecture, in which inputs are first processed with a low resolution and a small sub-network. Large sub-networks and high-resolution representations are conditionally activated based on early predictions. Instead of using a specialized structure, dynamic resolution network [250] rescales each image with the resolution predicted by a small model and feeds the rescaled image to common CNNs.

Note that different spatial locations are still processed equally in the aforementioned methods. We categorize the relative works in this section since they mainly utilize the spatial redundancy of image inputs for efficient inference.


## Temporal-wise Dynamic Networks

As video data can be viewed as a sequence of image data, adaptive computation could also be performed along the temporal dimension due to the considerable redundancy in video recognition tasks. Representative works can generally be divided into two lines: one processes video with recurrent models and dynamically save computation at certain time steps; the other aims at sampling key frames/clips and allocating the computation to these sampled frames.


### Dynamic Recurrent Models

Different video frames are unequally informative. To this end, extensive studies propose to dynamically activate computation when updating the hidden state in recurrent models. For example, LiteEval [251] establishes two different sized LSTM [252]. In each time step, a gating module is used to decide which LSTM should be executed for processing the current frame. AdaFuse [253] dynamically skips the computation of some convolution channels, and these channels are filled with the hidden state from the previous step. Moreover, the numerical precision [254] and image resolution [255] of different frames can also be dynamically decided.

The aforementioned works generally require a ConvNet for encoding each input frame before updating the hidden state. A more flexible solution is allowing the network to learn "where to see". In other words, networks can directly jump to an arbitrary temporal location in the video [37,256,257] or perform early exiting [258,259,260] instead of "watch" the entire video frame by frame.


### Dynamic Key Frame Sampling

An alternative to skipping computation in recurrent networks is sampling key frames and then feeding the sampled frames rather than the whole video to a standard model. Reinforcement learning is a popular technique for training frame samplers [261,262,263].

A recent trend is simultaneously achieving dynamic inference from multiple perspectives. For example, AdaFocus and its variants [38,264,265,266] makes use of both spatial and temporal redundancy in video data. Dynamic architecture with 3D convolution [267] is also an interesting topic.


# Efficient Models for Downstream Computer Vision Tasks

In this section, we assume that a light-weighted backbone network has already been obtained, and discuss how to design task-specific heads or algorithms on top of them. The general aim is to facilitate accomplishing real-world computer vision tasks efficiently or even in real time. To this end, we will focus on three representative tasks, namely object detection (Sec. 4.1), semantic segmentation (Sec. 4.2), and instance segmentation (Sec. 4.3), all of which have a strong need for accurate and real-time applications. Note that most of other more complex computer vision tasks (e.g. visual object tracking) are mainly based on the three tasks we consider.


## Object Detection

Object detection aims to answer two fundamental questions in computer vision: what visual objects are contained in the images, and where are them [8]? The classification and localization results obtained by object detection usually serve as the basis of other vision tasks, e.g., instance segmentation, image captioning, and object tracking. The algorithms for object detection can be roughly categorized into two-stage (Sec. 4.1.1) and one-stage (Sec. 4.1.2). In the following, we will discuss them respectively from the lens of computational efficiency.


### Two-stage Detectors

Object detection with deep learning starts from the two stage paradigm. The pioneer work, RCNN [268,269], proposes to first crop a set of object proposals from the images, and classify them with deep networks. On top of it, SPPNet [270] avoids repeatedly inferring the backbones by adaptively pooling the features of the regions of interest. Fast RCNN [271] simultaneously train a detector and a bounding box regressor in the same network, leading to more than 200 times of speedup than RCNN. Faster R-CNN [272,273] and its improvements [274,275] introduce a region proposal network that cheaply generates object proposals from the features, yielding the first nearly real-time deep learning detector. The feature pyramid networks further propose to leverage the feature maps at varying scales to detect the object with different sizes respectively, which improves the detection accuracy significantly without sacrificing the efficiency [276].


### One-stage Detectors

The major motivation behind the two-stage detects is the "coarse-to-fine" refining, i.e., first obtaining the coarse proposals, and then refining the localization and discrimination results on top of these proposals, such that an excellent detection performance can be achieved. Despite the aforementioned techniques proposed to improve the efficiency of this procedure, the speed and the complexity of two-stage detectors are usually not applicable to real-time applications. In contrast, the one-stage detectors directly output the detection results in a single step, yielding much faster inference speed with a decent accuracy.

1) Bounding-box-based Methods. The first deep-learning-based one-stage detector is YOLO [40]. YOLO divides the image into grid regions and simultaneously predicts the bounding boxes and the classification results conditioned on each region. The subsequent works of YOLO [43,277,278,279,280] focus on further improving the localization performance or classification accuracy without affecting the practical speed. The latest version, YOLOv7 [43], achieves a state-of-the-art effectiveness-efficiency trade-off.

In addition to YOLO, SSD [41] improves the accuracy of one-stage detectors by detecting the objects at different scales on different layers of the network. RetinaNet [281] proposes a focal loss to encourage the model to focus more on the difficult, misclassified examples, which boosting the accuracy of one-stage detectors effectively.

2) Point-based Methods. The aforementioned detection methods mostly learn to produce the groundtruth bounding boxes on top of pre-defined anchor boxes. Despite the effectiveness, this paradigm suffers from a lot of design hyper-parameters and an imbalance between positive/negative boxes during training. To address this issue, CornerNet [282] proposes to directly predict the top-left corner and bottom-right corner of candidate boxes. Many subsequent works extend this point-based setting. For example, FCOS [42] predicts the distances from each location in feature maps to the four sides of the bounding box. ExtremeNet [283] learns to detect the extreme points the center of bounding boxes. CenterNet [284] further considers each object to be a single center point and regresses all the attributes (2D/3D size, orientation, depth, locations, etc.) based on this point.


## 3) Transformer-based Methods. In recent years, N. Carion et al.

propose an end-to-end Transformerbased detection network, DETR [285]. DETR views detection as a set prediction problem, where the results are obtained based on several object queries. Deformable DETR [286] addresses the long convergence issue of DETR by introducing a deformable mechanism to self-attention.


## Semantic Segmentation

The aim of semantic segmentation is to predict the semantic label of each pixels [9,287] 


### Encoder-decoder

A popular approach is to first extract the low-resolution discriminative representations with a multi-stage backbone network, up-sample the deep features to the input resolution with a decoder, and then produce the pixel-wise predictions. This procedure is named as "encoder-decoder" [288,289]. To improve the efficiency of this paradigm, many works propose to design light-weighted decoders. Representative methods include introducing split-transform-merge architectures [73,290,291,292,293] (Eq. (2)), developing efficient approximations of the computationally intensive dilated convolution [294,295,296], and introducing dense connections [296,297]. In addition, it is efficient to simultaneously feed the low-level and high-level features into the decoder, i.e., comprehensively leveraging both of them improves the accuracy without introducing notable computational overhead [297,298,299,300,301].


### Multi-branch Models

Another popular efficient paradigm is designing multi-branch architectures. Typically, the model consist of two types of paths: 1) context paths with low-resolution feature maps and large receptive fields, aiming to extract discriminative information; and 2) spatial paths that preserve the low-level spatial information. These paths are fused in a parallel [44,298,302,303,304] or cascade [305] fashion, yielding high-resolution but semantically rich deep representations for segmentation.


### Others

In recent years, some new ideas have been proposed to facilitate efficient semantic segmentation. For example, processing deep features with self-attention layers [306,307,308], designing segmentation models with NAS [309,310,311], adjusting the architecture of the decoder conditioned on the inputs [233]. More recently, a considerable number of papers seek to design efficient semantic segmentation models on top of ViTs [312,313,314,315,316]. These works mainly focus on achieving a state-of-the-art performance with as less computational cost as possible.


## Instance Segmentation

Instance segmentation can be seen as a combination of object detection and semantic segmentation, where the model needs to detect the instances of objects, demarcate their boundaries and recognize their categories [9,317]. Existing works in this direction can be categorized into two-stage (Sec. 4.3.1) and End-to-end (Sec. 4.3.2).


### Two-stage Approaches

From the lens of efficiency, a notable milestone of deep-learning-based instance segmentation is the proposing of Mask R-CNN [318]. Mask R-CNN is developed by introducing mask segmentation branches on the basis of Faster R-CNN [272]. It enjoys high computational efficiency by directly obtaining the regions of interest from the feature maps. In contrast, MaskLab [319] improved Faster R-CNN by adding the semantic segmentation and direction prediction paths. To improve the accuracy of Mask R-CNN, MS R-CNN [320] predicts the quality of the predicted instance masks and prioritizes more accurate mask predictions during validation. PANet [321] introduces a path augmentation mechanism to facilitate the bottom-up information interaction of feature maps. HTC [322] proposes a hybrid task cascade framework to learn more discriminative features progressively while integrating complementary features in the meantime.


### End-to-end Approaches

Another liner of works focus on realizing efficient end-to-end instance segmentation. SOLO [45,46] achieves this by introducing the "instance categories", which assigns categories to each pixel within an instance according to the instance's location and size, thus converting instance segmentation into a pure dense classification problem. YOLACT [323] and BlendMask [324] propose to first generate a set of prototype masks, and then combines them with per-instance mask coefficients or attention scores. Inspired by SSD [41] and Reti-naNet [281], TensorMask [325] build an efficient sliding-window-based instance segmentation framework.


# Model Compression Techniques

Deep networks necessitate substantial resources, including energy, processing capacity, and storage. These resource requirements diminish the suitability of deep networks for resource-constrained devices [326]. Furthermore, the extensive resource requirements of deep networks become a bottleneck for real-time inference and executing deep networks on browser-based applications. To address these drawbacks of deep networks, various model compression techniques have been proposed in existing literature. Several comprehensive reviews on model compression techniques exist [327,328]. These reviews categorize model compression techniques, discuss challenges, provide overviews, solutions, and future directions of model compression techniques. We adopt their classification structure but place a greater emphasis on visionrelated works. Specifically, we categorize existing research into network pruning [329,330], network quantization [330,331], low-rank decomposition [332,333], knowledge distillation [51,334,335], and other techniques [336,337]. For readers interested in a particular category, we recommend consulting these more targeted reviews [52,329,330,331].  


## Network Pruning

Network pruning is one of the most prevalent techniques for reducing the size of a deep learning model by eliminating inadequate components, such as channels, filters, neurons, or layers, resulting in a lightweighted model. Network pruning techniques can be categorized into four types: channel pruning, filter pruning, connection pruning, and layer pruning. These techniques help decrease the storage and computation requirements of deep networks. A typical pruning algorithm consists of two stages: evaluating and pruning unimportant parameters, followed by fine-tuning the pruned model to restore accuracy. The steps and categories are illustrated in Figure 5.

In deep networks, the inputs provided to each layer are channeled. Channel pruning involves removing unimportant channels to reduce computation and storage requirements. Various channel pruning schemes have been proposed [47,338,339]. Convolutional operations in ConvNets incorporate a large number of filters to enhance performance. Increases in filter quantities result in a significant growth in the number of floating-point operations. Filter pruning eliminates unimportant filters, thus reducing computation [332,340,341]. The number of input and output connections to a layer in deep networks determines the number of parameters. These parameters can be used to estimate the storage and computation requirements of deep networks. Connection pruning is a direct approach to reduce parameters by removing unimportant connections [342,343,344]. Layer pruning involves selecting and deleting certain unimportant layers from the network, leading to ultra-high compression of the deep network. This is particularly useful for deploying deep networks on resource-constrained computing devices, where ultra-high compression is necessary. Some layer pruning approaches have been proposed to substantially reduce both storage and computation requirements [53,345]. However, layer pruning may result in a higher accuracy compromise due to the structural deterioration of deep networks.


## Network Quantization

Network quantization aims to compress the original network by reducing the storage requirements of weights. It can be categorized into linear quantization and nonlinear quantization. Linear quantization focuses on minimizing the number of bits needed to represent each weight, while nonlinear quantization involves dividing weights into several groups, with each group sharing a single weight.


### Linear Quantization

Utilizing 32-bit floating-point numbers to represent weights consumes a substantial amount of resources. Consequently, linear quantization employs low-bit number representation to approximate each weight. Suyog et al. contend that the weights of deep networks can be represented by 16-bit fixed-point numbers without significantly reducing classification accuracy [346]. Some studies further compress ConvNets to 8-bit [347,348]. In the extreme case of a 1-bit representation for each weight, binary weight neural networks emerge. The primary concept is to directly learn binary weights or activation during model training. Several works directly train ConvNets with binary weights, including BinaryConnect [49], BinaryNet [349], and XNOR [50].


### Nonlinear Quantization

Nonlinear quantization entails dividing weights into several groups, with each group sharing a single weight. Gong et al. initially employ the k-means algorithm to cluster weight parameters and replace the parameter values with the clustering center values, substantially reducing the network's storage space [350]. Wu et al. further quantize convolution filters, fully connected layers, and other parameters [351]. Chen et al. randomly assign weights to hash buckets, with each hash bucket sharing a single weight [352]. Han et al. combine network pruning, parameter quantization, and Huffman coding to achieve significant reductions in storage and memory [353].


## Knowledge Distillation

Knowledge distillation (KD) [51] is a widely adopted technique for transferring "dark knowledge" from a high-capacity model (teacher) to a more compact model (student) in order to achieve various types of efficiency. The two primary aspects of KD are knowledge representation and distillation schemes. In this section, we concentrate on existing research in these two technical areas and further summarize the theoretical exploration and application progress of KD in computer vision, as illustrated in Figure 6.


### Knowledge Representation

Drawing on [52], we examine different forms of knowledge in the following categories: response-based knowledge, feature-based knowledge, and relation-based knowledge. Response-based knowledge typically refers to the neural response of the teacher model's final output layer, with the main idea being to directly emulate the teacher model's final prediction. The most prevalent response-based knowledge for image classification is soft targets [51]. In object detection tasks, the response may include logits along with the bounding box offset [354]. For semantic landmark localization tasks, such as human pose estimation, the teacher model's response may consist of a heatmap for each landmark [355].

Feature-based knowledge pertains to the feature representation derived from intermediate layers. Fitnets [334] are the first to introduce intermediate representations, which subsequently inspire the development of various methods [356,357,358,359,360,361,362,363]. Relation-based knowledge further investigates the relationships between different feature layers [364,365,366] or data samples [335,367,368,369]. For instance, Yim et al. [364] propose calculating the relations between pairs of feature maps using the Gram matrix, while Liu et al. [367] suggest transferring the instance relationship graph, which defines instance features and relationships as vertices and edges, respectively.


### Distillation Schemes

The learning schemes of knowledge distillation can be classified into three main categories based on the synchronization of the teacher model's update with the student model: offline distillation, online distillation, and self-distillation.

In offline distillation, the teacher model is usually assumed to be pre-trained. The primary focus of offline methods is to enhance various aspects of knowledge transfer, including knowledge representation and the design of loss functions. Vanilla knowledge distillation [51] serves as a classic example of offline distillation methods. Most prior knowledge distillation methods operate in an offline manner.

In cases where a high-capacity, high-performance teacher model is unavailable, online distillation provides an alternative. In this approach, both the teacher model and the student model are updated simultaneously, allowing for an end-to-end trainable knowledge distillation framework. Deep mutual learning [370] introduced a method for training multiple neural networks collaboratively, where any given network can serve as the student model while the others act as teachers. Numerous online knowledge distillation methods have been proposed [371,372,373], with multi-branch architecture [371] and ensemble techniques [372,374] being widely adopted.

Self-distillation refers to a learning process in which the student model acquires knowledge independently, without the presence of teacher models, whether pre-trained or virtual. Several studies have explored this idea in various contexts. For instance, Zhang et al. [375] propose a method for distilling knowledge from deeper layers to shallower ones for image classification tasks. Similarly, Hou et al. [376] employ attention maps from deeper layers as distillation targets for lower layers in object detection tasks. In contrast, Yang et al. [377] introduce snapshot distillation, where checkpoints from earlier epochs are considered as teachers to distill knowledge for the models in later epochs. Additionally, Wang et al. [362] suggest constraining the outputs of the backbone network using target class activation maps.


### Theory and Applications

A wide range of knowledge distillation methods has been extensively employed in vision applications. Initially, most knowledge distillation methods were developed for image classification [51,364,378,379,380] and later extended to other vision tasks, including face recognition [361,381], action recognition [382,383], object detection [354,384,385], semantic segmentation [386,387,388,389], depth estimation [390,391,392,393,394,395], image retrieval [396,397], video captioning [398,399,400], and video classification [401,402], among others. Despite the significant practical success, relatively few works have focused on the theoretical or empirical understanding of knowledge distillation [379,403,404,405]. Hinton et al. [51] suggest that the success of KD could be attributed to learning similarities between categories. Yuan et al. [403] posited that dark knowledge not only encompasses category similarities but also imposes regularization on student training. They indicate that KD is a learned label smoothing regularization (LSR). Tang et al. [404] propose approach where, in addition to regularization and class relationships, another type of knowledge, instance-specific knowledge, is also used by the teacher to rescale the student model's per-instance gradients. Chen et al. [405] quantify the extraction of visual concepts from the intermediate layers of a deep learning model to explain knowledge distillation. Wang et al. [379] connect KD with the information bottleneck and empirically validate that preserving more mutual information between feature representation and input is more important than improving the teacher model's accuracy. Overall, theoretical research remains limited compared to the diverse and numerous applications.


## Low-rank Factorization

Convolution kernels can be viewed as 3D tensors. Ideas based on tensor decomposition are derived from the intuition that there is structural sparsity in the 3D tensor. In the case of fully connected layers, they can be viewed as 2D matrices (or 3D tensors), and low-rankness can also be helpful. The key idea of low-rank factorization is to find an approximate low-rank tensor that is close to the real tensor and easy to decompose. Low-rank factorization is beneficial for both tensors and matrices.

There are several typical low-rank methods for compressing 3D convolutional layers. Lebedev et al. [406] propose Canonical Polyadic (CP) decomposition for kernel tensors. They use nonlinear least squares to compute the CP decomposition for a better low-rank approximation. Since low-rank tensor decomposition is a non-convex problem and generally difficult to compute, Jaderberg et al. use iterative schemes to obtain an approximate local solution [333]. Then, Tai et al. find that the particular form of low-rank decomposition in [333] has an exact closed-form solution, which is the global optimum, and present a method for training low-rank constrained ConvNets from scratch [407].

Many classical works have exploited low-rankness in fully connected layers. Denil et al. reduce the number of dynamic parameters in deep models using the low-rank method [408]. Zhang et al. introduce a Tucker decomposition model to compress weight tensors in fully connected layers [409]. Lu et al. adopt truncated singular value decomposition to decompose the fully connected layer for designing compact multitask deep learning architectures [410]. Sainath et al. explore a low-rank matrix factorization of the final weight layer in deep networks for acoustic modeling [411].


## Hybrid Techniques

Apart from the four categories of mainstream techniques mentioned above, there are other techniques for network compression. Some studies have attempted to integrate orthogonal techniques to achieve more significant performance [353,412,413]. Some works have designed compact networks [26,64,89] or efficient convolutions [72,86], which have been discussed in Sec. 2.


# Efficient Deployment on Hardware

The aforementioned works mostly design network architectures based on their theoretical computation (e.g. floating operations, FLOPs). However, there is often a gap between theoretical computation and practical latency on hardware devices [27,64]. Realistic efficiency can be influenced by other factors such as hardware properties and scheduling strategies. Along this direction, we review relative works from the following perspectives: 1) hardware-aware neural architecture search (Sec. 6.1); 2) acceleration software libraries and hardware design (Sec. 6.2); and 3) algorithm-software codesign techniques.


## Hardware-aware Model Design

As the practical latency of models can be influenced by many factors other than theoretical computation, the commonly used FLOPs is an inaccurate proxy for network efficiency. Ideally, one should develop efficient models based on specific hardware properties. However, hand-designing networks for different hardware devices can be laborious. Therefore, automatically searching for efficient architectures is emerging as a promising direction. Compared to the traditional NAS methods [31,414], this line of works can generate appropriate models which satisfy different hardware constraints and gain realistic efficiency in practice. For example, ProxylessNAS [54] establishes a latency prediction function based on realistic tests on targeted hardware, and the predicted latency is then directly used as a regularization item in the NAS objective. A similar idea is also implemented by MnasNet [53] to search for efficient models on mobile devices. The following works FBNet [159], FBNet-v2 [415] and OFA [416] have improved NAS techniques.

Apart from the traditional static models, the hardware-aware design paradigm has also been applied to develop spatial-wise dynamic networks (Sec. 3.2) [39].

Note that we mainly give a brief introduction of basic ideas in this work due to the page limit. For more detailed techniques we refer the readers to the survey [417] which specifically focuses on this topic.


## Acceleration Tools

In addition to architectural design, the efficient deployment of algorithms on hardwares also requires acceleration software libraries or specific hardware accelerators.

1) Software Libraries. Extensive efforts have been made to accelerate model inference on different hardware platforms. For example, NVIDIA TensorRT [57] is widely used to deploy models for optimized inference on GPUs. NNPACK (https://github.com/Maratyszcza/NNPACK.), CoreML [58] and TinyEngine [418] are representative tools on multi-core CPUs, Apple silicons, and microcontrollers (MCUs), respectively. Cross-platform tools such as Tencent TNN (https://github.com/Tencent/TNN). and Apache TVM [59] have also emerged as popular development tools.

2) Hardware Accelerators. Apart from adapting neural architectures to given hardware devices, another line of works studies accelerators from the hardware perspective to enable fast inference of deep models. For example, DianNao [419] focuses on memory behavior and proposes an accelerator that simultaneously improves the inference speed and energy consumption of deep models. An FPGA-based accelerator is proposed quantitatively analyze the throughput of CNNs with the help of the classical roofline model [420]. In addition to the regular deep networks, researchers have also proposed accelerators to improve the inference efficiency of spatially sparse convolution [421,422].


## Algorithm-Hardware Co-design

The aforementioned methods typically improve the inference efficiency from the perspective of either algorithm or hardware. Ideally, one should expect algorithms and hardware can "cooperate" with each other to further push forward the Pareto frontier between accuracy and efficiency trade-off. Along this direction, extensive efforts have been made based on the highly flexible and versatile Field Programmable Gate Arrays (FPGA) platform, and NAS techniques (Sec. 6.1) are widely used to search for hardware-friendly network structures [55,56,423,424,425]. The recent MCUNet series [418,426,427] has enabled both inference and training on MCUs based on algorithm-hardware co-design with the help of their proposed tiny-Engine tool (Sec. 6.2).

The co-designing method has also been applied to the field of dynamic neural networks, especially for efficient spatially adaptive convolution [428,429,430] and attention [431] operations.


# Challenges and Future Directions

Despite the significant advances in the field of computationally efficient deep learning in recent years, numerous open challenges warrant further research. In this section, we summarize these challenges and discuss potential future directions.


## Designing General-purpose Backbones

The efficient extraction of discriminative representations from raw inputs has been established as a critical cornerstone for practical deep learning applications, as demonstrated in the existing literature. Lightweighted backbone networks are commonly employed to achieve this goal. As a result, a significant challenge lies in the design of efficient, general-purpose backbones. Potential avenues of investigation in this area encompass enhancing current convolution and self-attention operators via manual design [7,26], employing automated architecture search methodologies [24], and amalgamating these approaches to create comprehensive efficient modules [90]. Specifically, the exploration of innovative information aggregation methods beyond convolution and self-attention appears promising, for instance, clustering algorithms [432], LSTM [433], and graph convolution [434]. Moreover, an emerging area of interest involves enabling backbone networks to accommodate multi-modal inputs (e.g., text, images, and videos) and execute multiple visual tasks (e.g., retrieval, classification, and visual question answering) [435,436]. Consequently, the development of mobile-level multi-modal and multi-task visual foundation models could present an intriguing direction for future research.


## Developing Task-specialized Models

In addition to the architectural advancements in backbone models, tailoring deep learning methodologies to specific computer vision tasks of interest has been demonstrated as crucial. Two research challenges of particular significance in this domain can be identified. Firstly, the exploitation of representations extracted by backbones to efficiently obtain task-specific features is essential, for example, multi-scale features for object detection and multi-path fused features for semantic segmentation. A potential solution to this challenge could involve designing specialized, efficient decoders (e.g., utilizing NAS [311,437]). Secondly, it is important to streamline the multi-stage design of visual tasks (e.g., two-stage object detection [273] and instance segmentation [318] algorithms) to achieve end-to-end paradigms with minimal performance compromises. Additionally, the removal of time-consuming components, such as non-maximum suppression (NMS) [8], is crucial. A promising area for future research may involve the development of an efficient, unified, and end-to-end learnable interface for a majority of prevalent computer vision tasks [438].


## Deep Networks for Edge Computing

In practical applications, extant research predominantly focuses on conventional hardware, such as GPUs and CPUs. However, within the realm of edge computing, there is an increasing demand for the deployment of deep learning models on Internet of Things (IoT) devices and microcontrollers. These diminutive devices are characterized by their minimal size, low power consumption, affordability, and ubiquity [418]. The development of deep learning algorithms specifically adapted for such devices represents an exigent research direction. MCUNets [418,426,427] have provided an initial exploration by optimizing the design, inference, and training of ConvNets for these devices. Another prospective concept involves the creation of spiking neural networks [439], which, when co-designed with hardware, can yield energy-efficient solutions.


## Leveraging Large-scale Training Data

Contemporary large visual backbone models have exhibited remarkable scalability in response to the increasing volumes of training data [6], that is, the model's performance consistently enhances as more train-ing data becomes accessible. However, it is generally arduous for computationally efficient models with a reduced number of parameters to capitalize on this high-data regime to the same extent as their larger counterparts. For example, the improvements attained by pre-training light-weighted models on expansive ImageNet-22K/JFT datasets are typically inferior to those observed in larger models [6,7,74]. This challenge is similarly experienced by self-supervised learning algorithms, where the methods effective for larger models frequently produce limited gains for smaller models [440,441]. As a result, a propitious avenue of research involves the exploration of effective scalable supervised and unsupervised learning algorithms for light-weighted models, allowing them to reap the benefits of an unlimited amount of data without incurring the expense of acquiring annotations. Some recent works on novel training algorithms have started to preliminarily explore this direction [82,442,443,444,445].


## Practical Efficiency

While numerous extant studies have attained low theoretical computational costs, they may be hindered by the restricted practical efficiency. For example, certain irregular network architectures discovered through NAS may display considerable latency on GPUs/CPUs, and the models employing group or depth-wise convolution may exhibit reduced gains in actual speedup relative to their theoretical computational efficiency. To tackle this challenge, researchers might consider integrating the speed on practical hardwares as an objective in architecture design [24,53] or utilizing efficient implementation software [57,59]. From a hardware design standpoint, one potential direction involves the creation of model-specialized hardware platforms [55,56,423,424].


## Model Compression Approaches

Network compression algorithms, encompassing network pruning, quantization, and knowledge distillation, have exhibited a robust capacity to diminish the inference costs associated with deep networks. However, several avenues of investigation remain unexplored. For instance, while the overarching concept of model compression is not confined to a particular vision task, a majority of algorithms predominantly concentrate on image classification, rendering their extension to other tasks non-trivial. A significant research direction entails the development of general-purpose, task-agnostic model compression techniques. Furthermore, strategies such as network pruning may yield irregular architectural topologies, potentially impairing the practical efficiency of deep learning models. Consequently, the examination of practically efficient compression methodologies constitutes a propitious area for future research.

## Figure 3 :
3Dynamic layer skipping. A gating module is used to decide whether to execute the block.

## Figure 5 :
5The steps of network pruning.

## Figure 6 :
6Knowledge distillation. The section mainly contains knowledge representation, distillation schemes, theory and applications.

## Table 1 :
1


point clouds (Sec. 2.4.1), 3D voxels (Sec. 2.4.2) and multi-view images (Sec. 2.4.3).


, e.g., if a pixel belongs to a car, a bike, etc. Here we summarize existing efficient semantic segmentation methodologies based on their paradigms i.e., encoder-decoder (Sec. 4.2.1), multi-branch (Sec. 4.2.2) and others (Sec. 4.2.3).
AcknowledgmentsThis work is supported in part by the National Key R&D Program of China (2021ZD0140407), the National Natural Science Foundation of China (62022048, 62276150), Guoqiang Institute of Tsinghua University and Beijing Academy of Artificial Intelligence.
Imagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Communications of the ACM. 606Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu- tional neural networks. Communications of the ACM, 60(6):84-90, 2017.

Very deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, ICLR. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.

Going deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, CVPR. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, pages 1-9, 2015.

Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recogni- tion. In CVPR, pages 770-778, 2016.

Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, Kilian Q Weinberger, Convolutional networks with dense connectivity. TPAMI. 44Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Q Weinberger. Convolu- tional networks with dense connectivity. TPAMI, 44(12):8704-8716, 2019.

An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, ICLR. 2021Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un- terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.

Swin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, ICCV. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pages 10012-10022, 2021.

Object detection in 20 years: A survey. Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, Jieping Ye, Proceedings of the IEEE. the IEEEZhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object detection in 20 years: A survey. Proceedings of the IEEE, 2023.

Nasser Kehtarnavaz, and Demetri Terzopoulos. Image segmentation using deep learning: A survey. Shervin Minaee, Yuri Y Boykov, Fatih Porikli, J Antonio, Plaza, TPAMIShervin Minaee, Yuri Y Boykov, Fatih Porikli, Antonio J Plaza, Nasser Kehtarnavaz, and Demetri Ter- zopoulos. Image segmentation using deep learning: A survey. TPAMI, 2021.

A survey on deep learning technique for video segmentation. Tianfei Zhou, Fatih Porikli, J David, Luc Crandall, Wenguan Van Gool, Wang, TPAMITianfei Zhou, Fatih Porikli, David J Crandall, Luc Van Gool, and Wenguan Wang. A survey on deep learning technique for video segmentation. TPAMI, 2023.

Human action recognition and prediction: A survey. Yu Kong, Yun Fu, IJCV. 1305Yu Kong and Yun Fu. Human action recognition and prediction: A survey. IJCV, 130(5):1366-1401, 2022.

Human action recognition from various data modalities: A review. Zehua Sun, Qiuhong Ke, Hossein Rahmani, Mohammed Bennamoun, Gang Wang, Jun Liu, TPAMIZehua Sun, Qiuhong Ke, Hossein Rahmani, Mohammed Bennamoun, Gang Wang, and Jun Liu. Hu- man action recognition from various data modalities: A review. TPAMI, 2022.

Deep learning for 3d point clouds: A survey. Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, Mohammed Bennamoun, TPAMI. 4312Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep learning for 3d point clouds: A survey. TPAMI, 43(12):4338-4364, 2020.

Deep learning with edge computing: A review. Jiasi Chen, Xukan Ran, Proceedings of the IEEE. 1078Jiasi Chen and Xukan Ran. Deep learning with edge computing: A review. Proceedings of the IEEE, 107(8):1655-1674, 2019.

Deep learning on mobile devices: a review. Yunbin Deng, Mobile Multimedia/Image Processing, Security, and Applications. SPIE10993Yunbin Deng. Deep learning on mobile devices: a review. In Mobile Multimedia/Image Processing, Security, and Applications 2019, volume 10993, pages 52-66. SPIE, 2019.

Deep learning on computational-resource-limited platforms: a survey. Chunlei Chen, Peng Zhang, Huixiang Zhang, Jiangyan Dai, Yugen Yi, Huihui Zhang, Yonghui Zhang, Mobile Information Systems. 20204Chunlei Chen, Peng Zhang, Huixiang Zhang, Jiangyan Dai, Yugen Yi, Huihui Zhang, and Yonghui Zhang. Deep learning on computational-resource-limited platforms: a survey. Mobile Information Systems, 2020(4):1-19, 2020.

Deep learning for safe autonomous driving: Current challenges and future directions. Khan Muhammad, Amin Ullah, Jaime Lloret, Javier Del Ser, Victor Hugo C De Albuquerque, IEEE Transactions on Intelligent Transportation Systems. 227Khan Muhammad, Amin Ullah, Jaime Lloret, Javier Del Ser, and Victor Hugo C de Albuquerque. Deep learning for safe autonomous driving: Current challenges and future directions. IEEE Transactions on Intelligent Transportation Systems, 22(7):4316-4336, 2020.

End to end learning for selfdriving cars. Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, D Lawrence, Mathew Jackel, Urs Monfort, Jiakai Muller, Zhang, arXiv:1604.07316arXiv preprintMariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self- driving cars. arXiv preprint arXiv:1604.07316, 2016.

A survey of deep learning techniques for autonomous driving. Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, Gigel Macesanu, Journal of Field Robotics. 373Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu. A survey of deep learning techniques for autonomous driving. Journal of Field Robotics, 37(3):362-386, 2020.

Deep learning in mobile and wireless networking: A survey. Chaoyun Zhang, Paul Patras, Hamed Haddadi, IEEE Communications surveys & tutorials. 213Chaoyun Zhang, Paul Patras, and Hamed Haddadi. Deep learning in mobile and wireless networking: A survey. IEEE Communications surveys & tutorials, 21(3):2224-2287, 2019.

Deep learning for iot big data and streaming analytics: A survey. Mehdi Mohammadi, Ala Al-Fuqaha, Sameh Sorour, Mohsen Guizani, IEEE Communications Surveys & Tutorials. 204Mehdi Mohammadi, Ala Al-Fuqaha, Sameh Sorour, and Mohsen Guizani. Deep learning for iot big data and streaming analytics: A survey. IEEE Communications Surveys & Tutorials, 20(4):2923-2960, 2018.

Learning iot in edge: Deep learning for the internet of things with edge computing. He Li, Kaoru Ota, Mianxiong Dong, IEEE network. 321He Li, Kaoru Ota, and Mianxiong Dong. Learning iot in edge: Deep learning for the internet of things with edge computing. IEEE network, 32(1):96-101, 2018.

A review of deep learning methods and applications for unmanned aerial vehicles. Adrian Carrio, Carlos Sampedro, Alejandro Rodriguez-Ramos, Pascual Campoy, Journal of Sensors. Adrian Carrio, Carlos Sampedro, Alejandro Rodriguez-Ramos, and Pascual Campoy. A review of deep learning methods and applications for unmanned aerial vehicles. Journal of Sensors, 2017, 2017.

Once-for-all: Train one network and specialize it for efficient deployment. Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, Song Han, ICLR. Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and specialize it for efficient deployment. In ICLR, 2020.

Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, Lei Li, arXiv:2111.05193A survey on green deep learning. arXiv preprintJingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, and Lei Li. A survey on green deep learning. arXiv preprint arXiv:2111.05193, 2021.

Mobilenets: Efficient convolutional neural networks for mobile vision applications. G Andrew, Menglong Howard, Bo Zhu, Dmitry Chen, Weijun Kalenichenko, Tobias Wang, Marco Weyand, Hartwig Andreetto, Adam, arXiv:1704.04861arXiv preprintAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.

Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, Mo-bilenetv2: Inverted residuals and linear bottlenecks. CVPRMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo- bilenetv2: Inverted residuals and linear bottlenecks. In CVPR, pages 4510-4520, 2018.

Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, CVPR. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In CVPR, pages 1314- 1324, 2019.

Efficientnet: Rethinking model scaling for convolutional neural networks. Mingxing Tan, Quoc Le, ICML. PMLRMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, pages 6105-6114. PMLR, 2019.

Kaiming He, and Piotr Dollár. Designing network design spaces. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, CVPR. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Designing net- work design spaces. In CVPR, pages 10428-10436, 2020.

Neural architecture search with reinforcement learning. Barret Zoph, Quoc Le, ICLR. Barret Zoph and Quoc Le. Neural architecture search with reinforcement learning. In ICLR, 2017.

Darts: Differentiable architecture search. Hanxiao Liu, Karen Simonyan, Yiming Yang, ICLR. Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In ICLR, 2019.

Dynamic neural networks: A survey. Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, Yulin Wang, TPAMIYizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural networks: A survey. TPAMI, 2021.

Multi-scale dense networks for resource efficient image classification. Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens Van Der Maaten, Kilian Q Weinberger, Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens Van Der Maaten, and Kilian Q Weinberger. Multi-scale dense networks for resource efficient image classification. In ICLR, 2017.

Ji Lin, Yongming Rao, Jiwen Lu, Jie Zhou, Runtime neural pruning. NeurIPS. Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. NeurIPS, 2017.

Hd-cnn: hierarchical deep convolutional neural networks for large scale visual recognition. Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis Decoste, Wei Di, Yizhou Yu, ICCV. Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis DeCoste, Wei Di, and Yizhou Yu. Hd-cnn: hierarchical deep convolutional neural networks for large scale visual recognition. In ICCV, 2015.

Adaframe: Adaptive frame selection for fast video recognition. Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, Larry S Davis, CVPR. Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, and Larry S Davis. Adaframe: Adaptive frame selection for fast video recognition. In CVPR, 2019.

Adaptive focus for efficient video recognition. Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, Yizeng Han, Gao Huang, ICCV. 2021Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, Yizeng Han, and Gao Huang. Adaptive focus for efficient video recognition. In ICCV, 2021.

Latency-aware spatial-wise dynamic networks. Yizeng Han, Zhihang Yuan, Yifan Pu, Chenhao Xue, Shiji Song, Guangyu Sun, Gao Huang, NeurIPS. 2022Yizeng Han, Zhihang Yuan, Yifan Pu, Chenhao Xue, Shiji Song, Guangyu Sun, and Gao Huang. Latency-aware spatial-wise dynamic networks. In NeurIPS, 2022.

You only look once: Unified, realtime object detection. Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, CVPR. Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real- time object detection. In CVPR, pages 779-788, 2016.

Ssd: Single shot multibox detector. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C Berg, ECCV. SpringerWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In ECCV, pages 21-37. Springer, 2016.

Fcos: Fully convolutional one-stage object detection. Zhi Tian, Chunhua Shen, Hao Chen, Tong He, ICCV. Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detec- tion. In ICCV, pages 9627-9636, 2019.

Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. Chien-Yao Wang, Alexey Bochkovskiy, Hong-Yuan Mark Liao, arXiv:2207.02696arXiv preprintChien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. arXiv preprint arXiv:2207.02696, 2022.

Bisenet: Bilateral segmentation network for real-time semantic segmentation. Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang, ECCV. Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In ECCV, pages 325-341, 2018.

Solo: Segmenting objects by locations. Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, Lei Li, ECCV. SpringerXinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and Lei Li. Solo: Segmenting objects by locations. In ECCV, pages 649-665. Springer, 2020.

Solov2: Dynamic and fast instance segmentation. Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, Chunhua Shen, NeurIPS. 33Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast instance segmentation. In NeurIPS, volume 33, pages 17721-17732, 2020.

Learning efficient convolutional networks through network slimming. Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang, ICCV. Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In ICCV, pages 2736-2744, 2017.

The lottery ticket hypothesis: Finding sparse, trainable neural networks. Jonathan Frankle, Michael Carbin, ICLR. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In ICLR, 2019.

Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David, Binaryconnect: Training deep neural networks with binary weights during propagations. NeurIPS. Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. NeurIPS, 2015.

Xnor-net: Imagenet classification using binary convolutional neural networks. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi, ECCV. SpringerMohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classi- fication using binary convolutional neural networks. In ECCV, pages 525-542. Springer, 2016.

Distilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.025312arXiv preprintGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.

Knowledge distillation: A survey. Jianping Gou, Baosheng Yu, J Stephen, Dacheng Maybank, Tao, IJCV. 1296Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. IJCV, 129(6):1789-1819, 2021.

Mnasnet: Platform-aware neural architecture search for mobile. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V Le, CVPR. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In CVPR, pages 2820-2828, 2019.

Proxylessnas: Direct neural architecture search on target task and hardware. Han Cai, Ligeng Zhu, Song Han, ICLR. Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. In ICLR, 2019.

Fpga/dnn co-design: An efficient design methodology for iot. Cong Hao, Xiaofan Zhang, Yuhong Li, Sitao Huang, Jinjun Xiong, Kyle Rupnow, Wen-Mei Hwu, Deming Chen, DAC. Cong Hao, Xiaofan Zhang, Yuhong Li, Sitao Huang, Jinjun Xiong, Kyle Rupnow, Wen-mei Hwu, and Deming Chen. Fpga/dnn co-design: An efficient design methodology for iot intelligence on the edge. In DAC, 2019.

Hardware/software co-exploration of neural architectures. Weiwen Jiang, Lei Yang, Edwin Hsing-Mean, Qingfeng Sha, Shouzhen Zhuge, Sakyasingha Gu, Yiyu Dasgupta, Jingtong Shi, Hu, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. Weiwen Jiang, Lei Yang, Edwin Hsing-Mean Sha, Qingfeng Zhuge, Shouzhen Gu, Sakyasingha Das- gupta, Yiyu Shi, and Jingtong Hu. Hardware/software co-exploration of neural architectures. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2020.

Efficient inference with tensorrt. Han Vanholder, GPU Technology Conference. Han Vanholder. Efficient inference with tensorrt. In GPU Technology Conference, 2016.

Introduction to core ml framework. Beginning Machine Learning in iOS: CoreML Framework. Mohit Thakkar, Mohit Thakkar, Mohit Thakkar and Mohit Thakkar. Introduction to core ml framework. Beginning Machine Learning in iOS: CoreML Framework, 2019.

Tvm: end-to-end optimization stack for deep learning. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Haichen Shen, Eddie Q Yan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, Arvind Krishnamurthy, arXiv:1802.04799arXiv preprintTianqi Chen, Thierry Moreau, Ziheng Jiang, Haichen Shen, Eddie Q Yan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. Tvm: end-to-end optimization stack for deep learning. arXiv preprint arXiv:1802.04799, 2018.

Deep learning on mobile and embedded devices: State-of-the-art, challenges, and future directions. Yanjiao Chen, Baolin Zheng, Zihan Zhang, Qian Wang, Chao Shen, Qian Zhang, ACM Computing Surveys (CSUR). 534Yanjiao Chen, Baolin Zheng, Zihan Zhang, Qian Wang, Chao Shen, and Qian Zhang. Deep learning on mobile and embedded devices: State-of-the-art, challenges, and future directions. ACM Computing Surveys (CSUR), 53(4):1-37, 2020.

Aggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He, CVPR. Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transfor- mations for deep neural networks. In CVPR, pages 1492-1500, 2017.

Res2net: A new multi-scale backbone architecture. Shang-Hua, Ming-Ming Gao, Kai Cheng, Xin-Yu Zhao, Ming-Hsuan Zhang, Philip Yang, Torr, TPAMI. 432Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip Torr. Res2net: A new multi-scale backbone architecture. TPAMI, 43(2):652-662, 2019.

Shufflenet: An extremely efficient convolutional neural network for mobile devices. Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun, CVPR. Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convo- lutional neural network for mobile devices. In CVPR, pages 6848-6856, 2018.

Shufflenet v2: Practical guidelines for efficient cnn architecture design. Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun, ECCV. Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In ECCV, pages 116-131, 2018.

Training data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou, ICML. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In ICML, pages 10347-10357, 2021.

Selective kernel networks. Xiang Li, Wenhai Wang, Xiaolin Hu, Jian Yang, CVPR. Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In CVPR, pages 510- 519, 2019.

Resnest: Split-attention networks. Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, Manmatha, CVPR. Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. In CVPR, pages 2736-2746, 2022.

Interleaved group convolutions. Ting Zhang, Guo-Jun Qi, Bin Xiao, Jingdong Wang, ICCV. Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. Interleaved group convolutions. In ICCV, pages 4373-4382, 2017.

Interleaved structured sparse convolutional neural networks. Guotian Xie, Jingdong Wang, Ting Zhang, Jianhuang Lai, Richang Hong, Guo-Jun Qi, CVPR. Guotian Xie, Jingdong Wang, Ting Zhang, Jianhuang Lai, Richang Hong, and Guo-Jun Qi. Interleaved structured sparse convolutional neural networks. In CVPR, pages 8847-8856, 2018.

Igcv3: Interleaved low-rank group convolutions for efficient deep neural networks. Ke Sun, Mingjie Li, Dong Liu, Jingdong Wang, In BMVC. Ke Sun, Mingjie Li, Dong Liu, and Jingdong Wang. Igcv3: Interleaved low-rank group convolutions for efficient deep neural networks. In BMVC, 2018.

Attention is all you need. NeurIPS, 30. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017.

Xception: Deep learning with depthwise separable convolutions. François Chollet, CVPR. François Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, pages 1251-1258, 2017.

Espnetv2: A lightweight, power efficient, and general purpose convolutional neural network. Sachin Mehta, Mohammad Rastegari, Linda Shapiro, Hannaneh Hajishirzi, CVPR. Sachin Mehta, Mohammad Rastegari, Linda Shapiro, and Hannaneh Hajishirzi. Espnetv2: A light- weight, power efficient, and general purpose convolutional neural network. In CVPR, pages 9190- 9200, 2019.

A convnet for the 2020s. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie, CVPR. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In CVPR, pages 11976-11986, 2022.

Cvt: Introducing convolutions to vision transformers. Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang, ICCV. Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In ICCV, pages 22-31, 2021.

Cmt: Convolutional neural networks meet vision transformers. Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, Chang Xu, CVPR. Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. Cmt: Convolutional neural networks meet vision transformers. In CVPR, pages 12175-12185, 2022.

Batch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, ICML. PMLRSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, pages 448-456. PMLR, 2015.

Rethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, CVPR. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, pages 2818-2826, 2016.

Inception-v4, inceptionresnet and the impact of residual connections on learning. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alexander A Alemi, AAAI. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception- resnet and the impact of residual connections on learning. In AAAI, 2017.

Inception transformer. Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, Yan Shuicheng, NeurIPS. 2022Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, and Shuicheng YAN. Inception transformer. In NeurIPS, 2022.

Hornet: Efficient high-order spatial interactions with recursive gated convolutions. Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Nam Lim, Jiwen Lu, NeurIPS. 2022Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Nam Lim, and Jiwen Lu. Hornet: Effi- cient high-order spatial interactions with recursive gated convolutions. In NeurIPS, 2022.

Efficientnetv2: Smaller models and faster training. Mingxing Tan, Quoc Le, ICML. PMLRMingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In ICML, pages 10096-10106. PMLR, 2021.

Rethinking bottleneck structure for efficient mobile network design. Daquan Zhou, Qibin Hou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan, ECCV. SpringerDaquan Zhou, Qibin Hou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. Rethinking bottleneck structure for efficient mobile network design. In ECCV, pages 680-697. Springer, 2020.

Training very deep networks. K Rupesh, Klaus Srivastava, Jürgen Greff, Schmidhuber, NeurIPS. 28Rupesh K Srivastava, Klaus Greff, and Jürgen Schmidhuber. Training very deep networks. NeurIPS, 28, 2015.

Densely connected convolutional networks. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Q Weinberger, CVPR. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, pages 4700-4708, 2017.

Condensenet: An efficient densenet using learned group convolutions. Gao Huang, Shichen Liu, Laurens Van Der Maaten, Kilian Q Weinberger, CVPR. Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: An efficient densenet using learned group convolutions. In CVPR, pages 2752-2761, 2018.

Condensenet v2: Sparse feature reactivation for deep networks. Le Yang, Haojun Jiang, Ruojin Cai, Yulin Wang, Shiji Song, Gao Huang, Qi Tian, CVPR. Le Yang, Haojun Jiang, Ruojin Cai, Yulin Wang, Shiji Song, Gao Huang, and Qi Tian. Condensenet v2: Sparse feature reactivation for deep networks. In CVPR, pages 3569-3578, 2021.

Ghostnets on heterogeneous devices via cheap operations. Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chunjing Xu, Enhua Wu, Qi Tian, IJCV. 1304Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chunjing Xu, Enhua Wu, and Qi Tian. Ghostnets on heterogeneous devices via cheap operations. IJCV, 130(4):1050-1069, 2022.

Ghostnet: More features from cheap operations. Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, Chang Xu, CVPR. Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. Ghostnet: More features from cheap operations. In CVPR, pages 1580-1589, 2020.

Ghostnetv2: Enhance cheap operation with long-range attention. Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Chao Xu, Yunhe Wang, NeurIPS. 2022Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Chao Xu, and Yunhe Wang. Ghostnetv2: Enhance cheap operation with long-range attention. In NeurIPS, 2022.

Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution. Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus Rohrbach, Shuicheng Yan, Jiashi Feng, ICCV. Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus Rohrbach, Shuicheng Yan, and Jiashi Feng. Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution. In ICCV, pages 3435-3444, 2019.

Fast vision transformers with hilo attention. Zizheng Pan, Jianfei Cai, Bohan Zhuang, NeurIPS. 2022Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision transformers with hilo attention. In NeurIPS, 2022.

Deep high-resolution representation learning for human pose estimation. Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang, CVPR. Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In CVPR, pages 5693-5703, 2019.

Deep high-resolution representation learning for visual recognition. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, TPAMI. 4310Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. TPAMI, 43(10):3349-3364, 2020.

Hrformer: High-resolution vision transformer for dense predict. Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, Jingdong Wang, NeurIPS. 34Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and Jingdong Wang. Hrformer: High-resolution vision transformer for dense predict. NeurIPS, 34:7281-7293, 2021.

Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao, ICCV. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolu- tions. In ICCV, pages 568-578, 2021.

Pvt v2: Improved baselines with pyramid vision transformer. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao, Computational Visual Media. 83Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415-424, 2022.

Shunted self-attention via multi-scale token aggregation. Daquan Sucheng Ren, Shengfeng Zhou, Jiashi He, Xinchao Feng, Wang, CVPR. Sucheng Ren, Daquan Zhou, Shengfeng He, Jiashi Feng, and Xinchao Wang. Shunted self-attention via multi-scale token aggregation. In CVPR, pages 10853-10862, 2022.

Twins: Revisiting the design of spatial attention in vision transformers. Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, Chunhua Shen, NeurIPS. 34Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chun- hua Shen. Twins: Revisiting the design of spatial attention in vision transformers. NeurIPS, 34:9355- 9366, 2021.

Swin transformer v2: Scaling up capacity and resolution. Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, CVPR. Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In CVPR, pages 12009-12019, 2022.

Scaling local self-attention for parameter efficient visual backbones. Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, Jonathon Shlens, CVPR. Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient visual backbones. In CVPR, pages 12894- 12904, 2021.

Focal self-attention for local-global interactions in vision transformers. Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, Jianfeng Gao, arXiv:2107.00641arXiv preprintJianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.

Cswin transformer: A general vision transformer backbone with cross-shaped windows. Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, Baining Guo, CVPR. Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In CVPR, pages 12124-12134, 2022.

Volo: Vision outlooker for visual recognition. Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, Shuicheng Yan, TPAMILi Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. Volo: Vision outlooker for visual recognition. TPAMI, 2022.

V2x-vit: Vehicleto-everything cooperative perception with vision transformer. Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, Ming-Hsuan Yang, Jiaqi Ma, ECCV. SpringerRunsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, Ming-Hsuan Yang, and Jiaqi Ma. V2x-vit: Vehicle- to-everything cooperative perception with vision transformer. In ECCV, pages 107-124. Springer, 2022.

Maxvit: Multi-axis vision transformer. Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li, ECCV. SpringerZhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit: Multi-axis vision transformer. In ECCV, pages 459-479. Springer, 2022.

Convit: Improving vision transformers with soft convolutional inductive biases. Hugo Stéphane D&apos;ascoli, Touvron, L Matthew, Ari S Leavitt, Giulio Morcos, Levent Biroli, Sagun, ICML. PMLRStéphane d'Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In ICML, pages 2286- 2296. PMLR, 2021.

Locality guidance for improving vision transformers on tiny datasets. Kehan Li, Runyi Yu, Zhennan Wang, Li Yuan, Guoli Song, Jie Chen, ECCV. SpringerKehan Li, Runyi Yu, Zhennan Wang, Li Yuan, Guoli Song, and Jie Chen. Locality guidance for improv- ing vision transformers on tiny datasets. In ECCV, pages 110-127. Springer, 2022.

Msg-transformer: Exchanging local spatial information by manipulating messenger tokens. Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, Qi Tian, CVPR. Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, and Qi Tian. Msg-transformer: Exchanging local spatial information by manipulating messenger tokens. In CVPR, pages 12063- 12072, 2022.

Tokens-to-token vit: Training vision transformers from scratch on imagenet. Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, E H Francis, Jiashi Tay, Shuicheng Feng, Yan, ICCV. Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In ICCV, pages 558-567, 2021.

Transformer in transformer. Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang, NeurIPS. 34Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in trans- former. NeurIPS, 34:15908-15919, 2021.

Transformers are rnns: Fast autoregressive transformers with linear attention. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. PMLRICMLAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, pages 5156-5165. PMLR, 2020.

Rethinking attention with performers. Valerii Krzysztof Marcin Choromanski, David Likhosherstov, Xingyou Dohan, Andreea Song, Tamas Gane, Peter Sarlos, Jared Quincy Hawkins, Afroz Davis, Lukasz Mohiuddin, David Benjamin Kaiser, Lucy J Belanger, Adrian Colwell, Weller, ICLR. 2021Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In ICLR, 2021.

Nyströmformer: A nyström-based algorithm for approximating self-attention. Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh, AAAI. 2021Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention. In AAAI, 2021.

Soft: Softmax-free transformer with linear complexity. Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, Li Zhang, NeurIPS. 2021Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, and Li Zhang. Soft: Softmax-free transformer with linear complexity. In NeurIPS, 2021.

Castling-vit: Compressing self-attention via switching towards linear-angular attention during vision transformer inference. Haoran You, Yunyang Xiong, Xiaoliang Dai, Bichen Wu, Peizhao Zhang, Haoqi Fan, Peter Vajda, Yingyan Lin, CVPR. Haoran You, Yunyang Xiong, Xiaoliang Dai, Bichen Wu, Peizhao Zhang, Haoqi Fan, Peter Vajda, and Yingyan Lin. Castling-vit: Compressing self-attention via switching towards linear-angular attention during vision transformer inference. In CVPR, 2023.

Efficient architecture search by network transformation. Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, Jun Wang, AAAI. Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efficient architecture search by network transformation. In AAAI, 2018.

Flatten transformer: Vision transformer using focused linear attention. Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, Gao Huang, ICCV. Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, and Gao Huang. Flatten transformer: Vision transformer using focused linear attention. In ICCV, 2023.

Co-scale conv-attentional image transformers. Weijian Xu, Yifan Xu, Tyler Chang, Zhuowen Tu, ICCV. Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. In ICCV, pages 9981-9990, 2021.

Mpvit: Multi-path vision transformer for dense prediction. Youngwan Lee, Jonghee Kim, Jeffrey Willette, Sung Ju Hwang, CVPR. Youngwan Lee, Jonghee Kim, Jeffrey Willette, and Sung Ju Hwang. Mpvit: Multi-path vision trans- former for dense prediction. In CVPR, pages 7287-7296, 2022.

Glance-and-gaze vision transformer. Qihang Yu, Yingda Xia, Yutong Bai, Yongyi Lu, Alan L Yuille, Wei Shen, NeurIPS. 34Qihang Yu, Yingda Xia, Yutong Bai, Yongyi Lu, Alan L Yuille, and Wei Shen. Glance-and-gaze vision transformer. NeurIPS, 34:12992-13003, 2021.

Incorporating convolution designs into visual transformers. Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, Wei Wu, ICCV. Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. In ICCV, pages 579-588, 2021.

Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, Luc Van Gool, arXiv:2104.05707Localvit: Bringing locality to vision transformers. arXiv preprintYawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to vision transformers. arXiv preprint arXiv:2104.05707, 2021.

Coatnet: Marrying convolution and attention for all data sizes. Zihang Dai, Hanxiao Liu, V Quoc, Mingxing Le, Tan, NeurIPS. Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. In NeurIPS, pages 3965-3977, 2021.

Early convolutions help transformers see better. Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, Ross Girshick, NeurIPS. Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, and Ross Girshick. Early convolu- tions help transformers see better. In NeurIPS, pages 30392-30400, 2021.

Levit: a vision transformer in convnet's clothing for faster inference. Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze, ICCV. Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, and Matthijs Douze. Levit: a vision transformer in convnet's clothing for faster inference. In ICCV, pages 12259-12269, 2021.

Mobilevit: Light-weight, general-purpose, and mobilefriendly vision transformer. Sachin Mehta, Mohammad Rastegari, ICLR. 2022Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose, and mobile- friendly vision transformer. In ICLR, 2022.

Visformer: The vision-friendly transformer. Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, Qi Tian, ICCV. Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and Qi Tian. Visformer: The vision-friendly transformer. In ICCV, pages 589-598, 2021.

Rethinking spatial dimensions of vision transformers. Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, Seong Joon Oh, ICCV. Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial dimensions of vision transformers. In ICCV, pages 11936-11945, 2021.

Vitae: Vision transformer advanced by exploring intrinsic inductive bias. Yufei Xu, Qiming Zhang, Jing Zhang, Dacheng Tao, NeurIPS. 34Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Vitae: Vision transformer advanced by exploring intrinsic inductive bias. In NeurIPS, volume 34, pages 28522-28535, 2021.

Qiming Zhang, Yufei Xu, Jing Zhang, Dacheng Tao, Vision transformer advanced by exploring inductive bias for image recognition and beyond. IJCV. 2Qiming Zhang, Yufei Xu, Jing Zhang, and Dacheng Tao. Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond. IJCV, pages 1-22, 2023.

Mobile-former: Bridging mobilenet and transformer. Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, Zicheng Liu, CVPR. Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobile-former: Bridging mobilenet and transformer. In CVPR, pages 5270-5279, 2022.

Mixformer: Mixing features across windows and dimensions. Qiang Chen, Qiman Wu, Jian Wang, Qinghao Hu, Tao Hu, Errui Ding, Jian Cheng, Jingdong Wang, CVPR. Qiang Chen, Qiman Wu, Jian Wang, Qinghao Hu, Tao Hu, Errui Ding, Jian Cheng, and Jingdong Wang. Mixformer: Mixing features across windows and dimensions. In CVPR, pages 5249-5259, 2022.

Conformer: Local features coupling global representations for recognition and detection. Zhiliang Peng, Zonghao Guo, Wei Huang, Yaowei Wang, Lingxi Xie, Jianbin Jiao, Qi Tian, Qixiang Ye, TPAMIZhiliang Peng, Zonghao Guo, Wei Huang, Yaowei Wang, Lingxi Xie, Jianbin Jiao, Qi Tian, and Qixiang Ye. Conformer: Local features coupling global representations for recognition and detection. TPAMI, 2023.

Conformer: Local features coupling global representations for visual recognition. Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei Wang, Jianbin Jiao, Qixiang Ye, ICCV. Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei Wang, Jianbin Jiao, and Qixiang Ye. Con- former: Local features coupling global representations for visual recognition. In ICCV, pages 367-376, 2021.

On network design spaces for visual recognition. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, Piotr Dollár, ICCV. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Dollár. On network design spaces for visual recognition. In ICCV, pages 1882-1890, 2019.

Fast and accurate model scaling. Piotr Dollár, Mannat Singh, Ross Girshick, CVPR. Piotr Dollár, Mannat Singh, and Ross Girshick. Fast and accurate model scaling. In CVPR, pages 924-932, 2021.

Model rubik's cube: Twisting resolution, depth and width for tinynets. Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing Xu, Tong Zhang, NeurIPS. 33Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing Xu, and Tong Zhang. Model rubik's cube: Twisting resolution, depth and width for tinynets. NeurIPS, 33:19353-19364, 2020.

Designing neural network architectures using reinforcement learning. Bowen Baker, Otkrist Gupta, Nikhil Naik, Ramesh Raskar, Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using reinforcement learning. In ICLR, 2017.

Learning transferable architectures for scalable image recognition. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V Le, CVPR. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In CVPR, pages 8697-8710, 2018.

Large-scale evolution of image classifiers. Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, V Quoc, Alexey Le, Kurakin, ICML. PMLREsteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In ICML, pages 2902-2911. PMLR, 2017.

Genetic cnn. Lingxi Xie, Alan Yuille, ICCV. Lingxi Xie and Alan Yuille. Genetic cnn. In ICCV, pages 1379-1388, 2017.

Regularized evolution for image classifier architecture search. Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le, AAAI. Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In AAAI, pages 4780-4789, 2019.

Simple and efficient architecture search for convolutional neural networks. Thomas Elsken, Jan-Hendrik Metzen, Frank Hutter, In ICLR. Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter. Simple and efficient architecture search for convolutional neural networks. In ICLR, 2018.

Path-level network transformation for efficient architecture search. Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, Yong Yu, ICML. PMLRHan Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-level network transformation for efficient architecture search. In ICML, pages 678-687. PMLR, 2018.

Hierarchical representations for efficient architecture search. Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu, In ICLR. Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierar- chical representations for efficient architecture search. In ICLR, 2018.

Efficient neural architecture search via parameters sharing. Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, Jeff Dean, ICML. PMLRHieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. In ICML, pages 4095-4104. PMLR, 2018.

Understanding and simplifying one-shot architecture search. Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, Quoc Le, ICML. PMLRGabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. Understanding and simplifying one-shot architecture search. In ICML, pages 550-559. PMLR, 2018.

Snas: stochastic neural architecture search. Sirui Xie, Hehui Zheng, Chunxiao Liu, Liang Lin, ICLR. Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. Snas: stochastic neural architecture search. In ICLR, 2019.

Searching for a robust neural architecture in four gpu hours. Xuanyi Dong, Yi Yang, CVPR. Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four gpu hours. In CVPR, pages 1761-1770, 2019.

Understanding and robustifying differentiable architecture search. Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, Frank Hutter, ICLR. Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter. Un- derstanding and robustifying differentiable architecture search. In ICLR, 2020.

Network adjustment: Channel and block search guided by resource utilization ratio. Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, Qi Tian, IJCV. 1303Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and Qi Tian. Network adjustment: Channel and block search guided by resource utilization ratio. IJCV, 130(3):820-835, 2022.

Progressive differentiable architecture search: Bridging the depth gap between search and evaluation. Xin Chen, Lingxi Xie, Jun Wu, Qi Tian, ICCV. Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation. In ICCV, pages 1294-1303, 2019.

Greedynas: Towards fast one-shot nas with greedy supernet. Shan You, Tao Huang, Mingmin Yang, Fei Wang, Chen Qian, Changshui Zhang, CVPR. Shan You, Tao Huang, Mingmin Yang, Fei Wang, Chen Qian, and Changshui Zhang. Greedynas: Towards fast one-shot nas with greedy supernet. In CVPR, pages 1999-2008, 2020.

Smash: One-shot model architecture search through hypernetworks. Andrew Brock, Theo Lim, Nick Ritchie, Weston, In ICLR. Andrew Brock, Theo Lim, JM Ritchie, and Nick Weston. Smash: One-shot model architecture search through hypernetworks. In ICLR, 2018.

Graph hypernetworks for neural architecture search. Chris Zhang, Mengye Ren, Raquel Urtasun, ICLR. Chris Zhang, Mengye Ren, and Raquel Urtasun. Graph hypernetworks for neural architecture search. In ICLR, 2019.

Single path one-shot neural architecture search with uniform sampling. Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, Jian Sun, ECCV. SpringerZichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. In ECCV, pages 544-560. Springer, 2020.

Bignas: Scaling up neural architecture search with big single-stage models. Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, Quoc Le, ECCV. SpringerJiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with big single-stage models. In ECCV, pages 702-717. Springer, 2020.

Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, Kurt Keutzer, CVPR. Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In CVPR, pages 10734-10742, 2019.

Learning spatiotemporal features with 3d convolutional networks. Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri, ICCV. Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotem- poral features with 3d convolutional networks. In ICCV, pages 4489-4497, 2015.

Quo vadis, action recognition? a new model and the kinetics dataset. Joao Carreira, Andrew Zisserman, CVPR. Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR, pages 6299-6308, 2017.

Eco: Efficient convolutional network for online video understanding. Mohammadreza Zolfaghari, Kamaljeet Singh, Thomas Brox, ECCV. Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox. Eco: Efficient convolutional network for online video understanding. In ECCV, pages 695-712, 2018.

Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, Kevin Murphy, ECCV. Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In ECCV, pages 305-321, 2018.

Video classification with channel-separated convolutional networks. Du Tran, Heng Wang, Lorenzo Torresani, Matt Feiszli, ICCV. Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classification with channel-separated convolutional networks. In ICCV, pages 5552-5561, 2019.

X3d: Expanding architectures for efficient video recognition. Christoph Feichtenhofer, CVPR. Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In CVPR, pages 203-213, 2020.

A closer look at spatiotemporal convolutions for action recognition. Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann Lecun, Manohar Paluri, CVPR. Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In CVPR, pages 6450-6459, 2018.

Human action recognition using factorized spatiotemporal convolutional networks. Lin Sun, Kui Jia, Dit-Yan Yeung, Bertram E Shi, ICCV. Lin Sun, Kui Jia, Dit-Yan Yeung, and Bertram E Shi. Human action recognition using factorized spatio- temporal convolutional networks. In ICCV, pages 4597-4605, 2015.

Learning spatio-temporal representation with pseudo-3d residual networks. Zhaofan Qiu, Ting Yao, Tao Mei, ICCV. Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d residual networks. In ICCV, pages 5533-5541, 2017.

Temporal adaptive module for video recognition. Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, Tong Lu, Tam, ICCV. Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and Tong Lu. Tam: Temporal adaptive module for video recognition. In ICCV, pages 13708-13718, 2021.

Temporal relational reasoning in videos. Bolei Zhou, Alex Andonian, Aude Oliva, Antonio Torralba, ECCV. Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning in videos. In ECCV, pages 803-818, 2018.

Stm: Spatiotemporal and motion encoding for action recognition. Boyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, Junjie Yan, ICCV. Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal and motion encoding for action recognition. In ICCV, pages 2000-2009, 2019.

Tea: Temporal excitation and aggregation for action recognition. Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, Limin Wang, CVPR. Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and aggregation for action recognition. In CVPR, pages 909-918, 2020.

Tsm: Temporal shift module for efficient video understanding. Ji Lin, Chuang Gan, Song Han, ICCV. Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In ICCV, pages 7083-7093, 2019.

Token shift transformer for video classification. Hao Zhang, Yanbin Hao, Chong-Wah Ngo, ACM MM. Hao Zhang, Yanbin Hao, and Chong-Wah Ngo. Token shift transformer for video classification. In ACM MM, pages 917-925, 2021.

Gate-shift networks for video action recognition. Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz, CVPR. Swathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz. Gate-shift networks for video action recognition. In CVPR, pages 1102-1111, 2020.

Slowfast networks for video recognition. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He, ICCV. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In ICCV, pages 6202-6211, 2019.

Tdn: Temporal difference networks for efficient action recognition. Limin Wang, Zhan Tong, Bin Ji, Gangshan Wu, CVPR. Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu. Tdn: Temporal difference networks for efficient action recognition. In CVPR, pages 1895-1904, 2021.

Vivit: A video vision transformer. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid, ICCV. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and Cordelia Schmid. Vivit: A video vision transformer. In ICCV, pages 6836-6846, 2021.

Brais Martinez, and Georgios Tzimiropoulos. Space-time mixing attention for video transformer. Adrian Bulat, Juan Manuel Perez Rua, Swathikiran Sudhakaran, NeurIPS. 34Adrian Bulat, Juan Manuel Perez Rua, Swathikiran Sudhakaran, Brais Martinez, and Georgios Tz- imiropoulos. Space-time mixing attention for video transformer. NeurIPS, 34:19594-19607, 2021.

Video swin transformer. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han Hu, CVPR. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin trans- former. In CVPR, pages 3202-3211, 2022.

Uniformer: Unified transformer for efficient spatial-temporal representation learning. Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu, Hongsheng Li, Yu Qiao, ICLR. 2022Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatial-temporal representation learning. In ICLR, 2022.

Multiview transformers for video recognition. Xuehan Shen Yan, Anurag Xiong, Zhichao Arnab, Mi Lu, Chen Zhang, Cordelia Sun, Schmid, CVPR. Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun, and Cordelia Schmid. Multiview transformers for video recognition. In CVPR, pages 3333-3343, 2022.

Keeping your eye on the ball: Trajectory attention in video transformers. Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, João F Henriques, NeurIPS. 34Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and João F Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. NeurIPS, 34:12493-12506, 2021.

Is space-time attention all you need for video understanding? In ICML. Gedas Bertasius, Heng Wang, Lorenzo Torresani, Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, 2021.

Video transformer network. Daniel Neimark, Omri Bar, Maya Zohar, Dotan Asselmann, ICCV. Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network. In ICCV, pages 3163-3172, 2021.

Pointnet: Deep learning on point sets for 3d classification and segmentation. Hao Charles R Qi, Kaichun Su, Leonidas J Mo, Guibas, CVPR. Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR, pages 652-660, 2017.

Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Li Charles Ruizhongtai Qi, Hao Yi, Leonidas J Su, Guibas, NeurIPS. 30Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. NeurIPS, 30, 2017.

Graph attention convolution for point cloud semantic segmentation. Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, Jie Shan, CVPR. Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and Jie Shan. Graph attention convolution for point cloud semantic segmentation. In CVPR, pages 10296-10305, 2019.

Dynamic graph cnn for learning on point clouds. Yue Wang, Yongbin Sun, Ziwei Liu, E Sanjay, Sarma, Justin M Michael M Bronstein, Solomon, Acm Transactions On Graphics. 385Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics, 38(5):1-12, 2019.

Spidercnn: Deep learning on point sets with parameterized convolutional filters. Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, Yu Qiao, ECCV. Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep learning on point sets with parameterized convolutional filters. In ECCV, pages 87-102, 2018.

Tangent convolutions for dense prediction in 3d. Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, Qian-Yi Zhou, CVPR. Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-Yi Zhou. Tangent convolutions for dense prediction in 3d. In CVPR, pages 3887-3896, 2018.

Kpconv: Flexible and deformable convolution for point clouds. Hugues Thomas, R Charles, Jean-Emmanuel Qi, Beatriz Deschaud, François Marcotegui, Leonidas J Goulette, Guibas, ICCV. Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, François Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In ICCV, pages 6411-6420, 2019.

Pointcnn: Convolution on x-transformed points. Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, Baoquan Chen, NeurIPS. 31Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. NeurIPS, 31, 2018.

Relation-shape convolutional neural network for point cloud analysis. Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. CVPRYongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. In CVPR, pages 8895-8904, 2019.

Pointconv: Deep convolutional networks on 3d point clouds. Wenxuan Wu, Zhongang Qi, Li Fuxin, CVPR. Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In CVPR, pages 9621-9630, 2019.

A closer look at local aggregation operators in point cloud analysis. Ze Liu, Han Hu, Yue Cao, Zheng Zhang, Xin Tong, ECCV. SpringerZe Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong. A closer look at local aggregation operators in point cloud analysis. In ECCV, pages 326-342. Springer, 2020.

Stratified transformer for 3d point cloud segmentation. Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, Jiaya Jia, CVPR. Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified transformer for 3d point cloud segmentation. In CVPR, pages 8500-8509, 2022.

Point transformer. Hengshuang Zhao, Li Jiang, Jiaya Jia, H S Philip, Vladlen Torr, Koltun, ICCV. Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In ICCV, pages 16259-16268, 2021.

Pointnext: Revisiting pointnet++ with improved training and scaling strategies. Guocheng Qian, Yuchen Li, Houwen Peng, NeurIPS. Jinjie Mai, Hasan Abed Al Kader Hammoud, Mohamed Elhoseiny, and Bernard Ghanem2022Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Abed Al Kader Hammoud, Mohamed El- hoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. In NeurIPS, 2022.

Voxnet: A 3d convolutional neural network for real-time object recognition. Daniel Maturana, Sebastian Scherer, IROS. IEEEDaniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In IROS, pages 922-928. IEEE, 2015.

3d semantic segmentation with submanifold sparse convolutional networks. Benjamin Graham, Martin Engelcke, Laurens Van Der Maaten, CVPR. Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. In CVPR, pages 9224-9232, 2018.

Christopher Choy, Junyoung Gwak, Silvio Savarese, Minkowski convolutional neural networks. In CVPR. 4d spatio-temporal convnetsChristopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In CVPR, pages 3075-3084, 2019.

Focal sparse convolutional networks for 3d object detection. Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, Jiaya Jia, CVPR. Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, and Jiaya Jia. Focal sparse convolutional networks for 3d object detection. In CVPR, pages 5428-5437, 2022.

Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li, CVPR. Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. In CVPR, pages 10529-10538, 2020.

Pv-rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection. Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang, Hongsheng Li, IJCV. Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection. IJCV, pages 1-21, 2022.

Class-balanced grouping and sampling for point cloud 3d object detection. Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, Gang Yu, arXiv:1908.09492arXiv preprintBenjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced grouping and sampling for point cloud 3d object detection. arXiv preprint arXiv:1908.09492, 2019.

Point-voxel cnn for efficient 3d deep learning. Zhijian Liu, Haotian Tang, Yujun Lin, Song Han, NeurIPS. 32Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for efficient 3d deep learning. In NeurIPS, volume 32, 2019.

Searching efficient 3d architectures with sparse point-voxel convolution. Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, Song Han, ECCV. SpringerHaotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching efficient 3d architectures with sparse point-voxel convolution. In ECCV, pages 685-702. Springer, 2020.

Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. Hang Su, Subhransu Maji, ICCV. Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In ICCV, pages 945-953, 2015.

View-gcn: View-based graph convolutional network for 3d shape analysis. Xin Wei, Ruixuan Yu, Jian Sun, CVPR. Xin Wei, Ruixuan Yu, and Jian Sun. View-gcn: View-based graph convolutional network for 3d shape analysis. In CVPR, pages 1850-1859, 2020.

Mvtn: Multi-view transformation network for 3d shape recognition. Abdullah Hamdi, Silvio Giancola, Bernard Ghanem, ICCV. Abdullah Hamdi, Silvio Giancola, and Bernard Ghanem. Mvtn: Multi-view transformation network for 3d shape recognition. In ICCV, pages 1-11, 2021.

Gift: A real-time and scalable 3d shape search engine. Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, Longin Jan Latecki, CVPR. Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, and Longin Jan Latecki. Gift: A real-time and scalable 3d shape search engine. In CVPR, pages 5023-5032, 2016.

Mlvcnn: Multi-loop-view convolutional neural network for 3d shape retrieval. Jianwen Jiang, Di Bao, Ziqiang Chen, Xibin Zhao, Yue Gao, AAAI. Jianwen Jiang, Di Bao, Ziqiang Chen, Xibin Zhao, and Yue Gao. Mlvcnn: Multi-loop-view convolu- tional neural network for 3d shape retrieval. In AAAI, pages 8513-8520, 2019.

Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints. Asako Kanezaki, Yasuyuki Matsushita, Yoshifumi Nishida, CVPR. Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints. In CVPR, pages 5010-5019, 2018.

Surat Teerapittayanon, Bradley Mcdanel, Hsiang-Tsung Kung, Branchynet: Fast inference via early exiting from deep neural networks. In ICPR. Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early exiting from deep neural networks. In ICPR, 2016.

Adaptive neural networks for efficient inference. Tolga Bolukbasi, Joseph Wang, Ofer Dekel, Venkatesh Saligrama, ICML. Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive neural networks for efficient inference. In ICML, 2017.

Resolution adaptive networks for efficient inference. Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, Gao Huang, CVPR. Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution adaptive networks for efficient inference. In CVPR, 2020.

Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition. Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, Gao Huang, NeurIPSYulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao Huang. Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition. NeurIPS, 2021.

Dynamic perceiver for efficient visual recognition. Yizeng Han, Dongchen Han, Zeyu Liu, Yulin Wang, Xuran Pan, Yifan Pu, Chao Deng, Junlan Feng, Shiji Song, Gao Huang, ICCV. Yizeng Han, Dongchen Han, Zeyu Liu, Yulin Wang, Xuran Pan, Yifan Pu, Chao Deng, Junlan Feng, Shiji Song, and Gao Huang. Dynamic perceiver for efficient visual recognition. In ICCV, 2023.

Improved techniques for training adaptive deep networks. Hao Li, Hong Zhang, Xiaojuan Qi, Ruigang Yang, Gao Huang, ICCV. Hao Li, Hong Zhang, Xiaojuan Qi, Ruigang Yang, and Gao Huang. Improved techniques for training adaptive deep networks. In ICCV, 2019.

Learning to weight samples for dynamic early-exiting networks. Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfeng Cao, Wenhui Huang, Chao Deng, Gao Huang, ECCV. 2022Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfeng Cao, Wenhui Huang, Chao Deng, and Gao Huang. Learning to weight samples for dynamic early-exiting networks. In ECCV, 2022.

Skipnet: Learning dynamic routing in convolutional networks. Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, Joseph E Gonzalez, In ECCV. Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dynamic routing in convolutional networks. In ECCV, 2018.

Convolutional networks with adaptive inference graphs. Andreas Veit, Serge Belongie, ECCV. Andreas Veit and Serge Belongie. Convolutional networks with adaptive inference graphs. In ECCV, 2018.

Adavit: Adaptive vision transformers for efficient image recognition. Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, Ser-Nam Lim, CVPR. 2022Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. Adavit: Adaptive vision transformers for efficient image recognition. In CVPR, 2022.

Batch-shaping for learning conditional channel gated networks. Tijmen Babak Ehteshami Bejnordi, Max Blankevoort, Welling, ICLR. Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max Welling. Batch-shaping for learning condi- tional channel gated networks. In ICLR, 2019.

Channel selection using gumbel softmax. Charles Herrmann, Richard Strong Bowen, Ramin Zabih, ECCV. Charles Herrmann, Richard Strong Bowen, and Ramin Zabih. Channel selection using gumbel soft- max. In ECCV, 2020.

Dynamic slimmable network. Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang, Zhihui Li, Xiaojun Chang, CVPR. 2021Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang, Zhihui Li, and Xiaojun Chang. Dynamic slimmable network. In CVPR, 2021.

Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi, Aditya Nori, Adaptive neural trees. In ICML. Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi, and Aditya Nori. Adaptive neural trees. In ICML, 2019.

The tree ensemble layer: Differentiability meets conditional computation. Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, Rahul Mazumder, ICML. Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. The tree ensemble layer: Differentiability meets conditional computation. In ICML, 2020.

Condconv: Conditionally parameterized convolutions for efficient inference. Brandon Yang, Gabriel Bender, V Quoc, Jiquan Le, Ngiam, NeurIPS. Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolutions for efficient inference. In NeurIPS, 2019.

Adaptive rotated convolution for rotated object detection. Yifan Pu, Yiru Wang, Zhuofan Xia, Yizeng Han, Yulin Wang, Weihao Gan, Zidong Wang, Shiji Song, Gao Huang, ICCV. Yifan Pu, Yiru Wang, Zhuofan Xia, Yizeng Han, Yulin Wang, Weihao Gan, Zidong Wang, Shiji Song, and Gao Huang. Adaptive rotated convolution for rotated object detection. In ICCV, 2023.

Instanas: Instance-aware neural architecture search. An-Chieh Cheng, Chieh Hubert Lin, Da-Cheng Juan, Wei Wei, Min Sun, AAAI. An-Chieh Cheng, Chieh Hubert Lin, Da-Cheng Juan, Wei Wei, and Min Sun. Instanas: Instance-aware neural architecture search. In AAAI, 2020.

Learning dynamic routing for semantic segmentation. Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, Jian Sun, CVPR. Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, and Jian Sun. Learning dynamic routing for semantic segmentation. In CVPR, 2020.

Learning deep features for discriminative localization. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, CVPR. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In CVPR, 2016.

More is less: A more complicated network with less inference complexity. Xuanyi Dong, Junshi Huang, Yi Yang, Shuicheng Yan, CVPR. Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicated network with less inference complexity. In CVPR, 2017.

Dynamic convolutions: Exploiting spatial sparsity for faster inference. Thomas Verelst, Tinne Tuytelaars, CVPR. Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions: Exploiting spatial sparsity for faster inference. In CVPR, 2020.

Spatially adaptive inference with stochastic feature sampling and interpolation. Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, Stephen Lin, ECCV. Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially adaptive inference with stochastic feature sampling and interpolation. In ECCV, 2020.

Spatially adaptive feature refinement for efficient inference. Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, Haojun Jiang, TIP. Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, and Haojun Jiang. Spatially adaptive feature refinement for efficient inference. TIP, 2021.

Deformable convolutional networks. Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei, Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV, 2017.

Deformable convnets v2: More deformable, better results. Xizhou Zhu, Han Hu, Stephen Lin, Jifeng Dai, CVPR. Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable, better results. In CVPR, 2019.

Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, arXiv:2211.05778Exploring large-scale vision foundation models with deformable convolutions. arXiv preprintWenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778, 2022.

Vision transformer with deformable attention. Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, Gao Huang, CVPR. 2022Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao Huang. Vision transformer with deformable attention. In CVPR, 2022.

Recurrent models of visual attention. Volodymyr Mnih, Nicolas Heess, Alex Graves, NeurIPSVolodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. NeurIPS, 2014.

Dynamic computational time for visual attention. Zhichao Li, Yi Yang, Xiao Liu, Feng Zhou, Shilei Wen, Wei Xu, Zhichao Li, Yi Yang, Xiao Liu, Feng Zhou, Shilei Wen, and Wei Xu. Dynamic computational time for visual attention. In ICCVW, 2017.

Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition. Jianlong Fu, Heliang Zheng, Tao Mei, CVPR. Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition. In CVPR, 2017.

Glance and focus: a dynamic approach to reducing spatial redundancy in image classification. Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, Gao Huang, NeurIPS. Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. Glance and focus: a dynamic approach to reducing spatial redundancy in image classification. In NeurIPS, 2020.

Glance and focus networks for dynamic visual recognition. Gao Huang, Yulin Wang, Kangchen Lv, Haojun Jiang, Wenhui Huang, Pengfei Qi, Shiji Song, TPAMIGao Huang, Yulin Wang, Kangchen Lv, Haojun Jiang, Wenhui Huang, Pengfei Qi, and Shiji Song. Glance and focus networks for dynamic visual recognition. TPAMI, 2022.

Reinforcement learning: A survey. Leslie Pack Kaelbling, Andrew W Michael L Littman, Moore, Journal of artificial intelligence research. Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 1996.

Scale-aware face detection. Zekun Hao, Yu Liu, Hongwei Qin, Junjie Yan, Xiu Li, Xiaolin Hu, CVPR. Zekun Hao, Yu Liu, Hongwei Qin, Junjie Yan, Xiu Li, and Xiaolin Hu. Scale-aware face detection. In CVPR, 2017.

Dynamic resolution network. Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan, Yunhe Wang, NeurIPS. Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan, and Yunhe Wang. Dy- namic resolution network. NeurIPS, 2021.

Liteeval: A coarse-to-fine framework for resource efficient video recognition. Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang, Larry S Davis, NeurIPS. Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang, and Larry S Davis. Liteeval: A coarse-to-fine framework for resource efficient video recognition. NeurIPS, 2019.

Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 1997.

Adafuse: Adaptive temporal fusion network for efficient action recognition. Yue Meng, Rameswar Panda, Chung-Ching Lin, Prasanna Sattigeri, Leonid Karlinsky, Kate Saenko, Aude Oliva, Rogerio Feris, ICLR. 2021Yue Meng, Rameswar Panda, Chung-Ching Lin, Prasanna Sattigeri, Leonid Karlinsky, Kate Saenko, Aude Oliva, and Rogerio Feris. Adafuse: Adaptive temporal fusion network for efficient action recog- nition. In ICLR, 2021.

Dynamic network quantization for efficient video inference. Ximeng Sun, Rameswar Panda, Chun-Fu Richard Chen, Aude Oliva, Rogerio Feris, Kate Saenko, ICCV. 2021Ximeng Sun, Rameswar Panda, Chun-Fu Richard Chen, Aude Oliva, Rogerio Feris, and Kate Saenko. Dynamic network quantization for efficient video inference. In ICCV, 2021.

Ar-net: Adaptive frame resolution for efficient action recognition. Yue Meng, Chung-Ching Lin, Rameswar Panda, Prasanna Sattigeri, Leonid Karlinsky, Aude Oliva, Kate Saenko, Rogerio Feris, ECCV. Yue Meng, Chung-Ching Lin, Rameswar Panda, Prasanna Sattigeri, Leonid Karlinsky, Aude Oliva, Kate Saenko, and Rogerio Feris. Ar-net: Adaptive frame resolution for efficient action recognition. In ECCV, 2020.

End-to-end learning of action detection from frame glimpses in videos. Serena Yeung, Olga Russakovsky, Greg Mori, Li Fei-Fei, CVPR. Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei. End-to-end learning of action detection from frame glimpses in videos. In CVPR, 2016.

Action search: Spotting actions in videos and its application to temporal action localization. Humam Alwassel, Bernard Fabian Caba Heilbron, Ghanem, ECCV. Humam Alwassel, Fabian Caba Heilbron, and Bernard Ghanem. Action search: Spotting actions in videos and its application to temporal action localization. In ECCV, 2018.

Watching a small portion could be as good as watching all: Towards efficient video classification. Hehe Fan, Zhongwen Xu, Linchao Zhu, Chenggang Yan, Jianjun Ge, Yi Yang, In IJCAI. Hehe Fan, Zhongwen Xu, Linchao Zhu, Chenggang Yan, Jianjun Ge, and Yi Yang. Watching a small portion could be as good as watching all: Towards efficient video classification. In IJCAI, 2018.

Dynamic inference: A new approach toward efficient video action recognition. Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, Yi Yang, Shilei Wen, CVPRWWenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, Yi Yang, and Shilei Wen. Dynamic inference: A new approach toward efficient video action recognition. In CVPRW, 2020.

Frameexit: Conditional early exiting for efficient video recognition. Amir Ghodrati, Ehteshami Babak, Amirhossein Bejnordi, Habibian, CVPR. 2021Amir Ghodrati, Babak Ehteshami Bejnordi, and Amirhossein Habibian. Frameexit: Conditional early exiting for efficient video recognition. In CVPR, 2021.

Deep progressive reinforcement learning for skeleton-based action recognition. Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li, Jie Zhou, CVPR. Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li, and Jie Zhou. Deep progressive reinforcement learning for skeleton-based action recognition. In CVPR, 2018.

Multi-agent reinforcement learning based frame sampling for effective untrimmed video recognition. Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, Shilei Wen, ICCV. Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, and Shilei Wen. Multi-agent reinforcement learning based frame sampling for effective untrimmed video recognition. In ICCV, 2019.

Dynamic sampling networks for efficient action recognition in videos. Yin-Dong Zheng, Zhaoyang Liu, Tong Lu, Limin Wang, TIP. Yin-Dong Zheng, Zhaoyang Liu, Tong Lu, and Limin Wang. Dynamic sampling networks for efficient action recognition in videos. TIP, 2020.

Adafocus v2: End-to-end training of spatial dynamic networks for video recognition. Yulin Wang, Yang Yue, Yuanze Lin, Haojun Jiang, Zihang Lai, Victor Kulikov, Nikita Orlov, Humphrey Shi, Gao Huang, CVPR. 2022Yulin Wang, Yang Yue, Yuanze Lin, Haojun Jiang, Zihang Lai, Victor Kulikov, Nikita Orlov, Humphrey Shi, and Gao Huang. Adafocus v2: End-to-end training of spatial dynamic networks for video recog- nition. In CVPR, 2022.

Adafocusv3: On unified spatial-temporal dynamic video recognition. Yulin Wang, Yang Yue, Xinhong Xu, Ali Hassani, Victor Kulikov, Nikita Orlov, Shiji Song, Humphrey Shi, Gao Huang, ECCV. 2022Yulin Wang, Yang Yue, Xinhong Xu, Ali Hassani, Victor Kulikov, Nikita Orlov, Shiji Song, Humphrey Shi, and Gao Huang. Adafocusv3: On unified spatial-temporal dynamic video recognition. In ECCV, 2022.

Dynamic spatial focus for efficient compressed video action recognition. Ziwei Zheng, Le Yang, Yulin Wang, Miao Zhang, Lijun He, Gao Huang, Fan Li, IEEE TCSVT. Ziwei Zheng, Le Yang, Yulin Wang, Miao Zhang, Lijun He, Gao Huang, and Fan Li. Dynamic spatial focus for efficient compressed video action recognition. IEEE TCSVT, 2023.

2d or not 2d? adaptive 3d convolution selection for efficient video recognition. Hengduo Li, Zuxuan Wu, Abhinav Shrivastava, Larry S Davis, CVPR. 2021Hengduo Li, Zuxuan Wu, Abhinav Shrivastava, and Larry S Davis. 2d or not 2d? adaptive 3d convo- lution selection for efficient video recognition. In CVPR, 2021.

Rich feature hierarchies for accurate object detection and semantic segmentation. Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, CVPR. Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, pages 580-587, 2014.

Region-based convolutional networks for accurate object detection and segmentation. Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, TPAMI38Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Region-based convolutional networks for accurate object detection and segmentation. TPAMI, 38(1):142-158, 2015.

Spatial pyramid pooling in deep convolutional networks for visual recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, TPAMI. 379Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convolu- tional networks for visual recognition. TPAMI, 37(9):1904-1916, 2015.

Fast r-cnn. Ross Girshick, ICCV. Ross Girshick. Fast r-cnn. In ICCV, pages 1440-1448, 2015.

Faster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, NeurIPS. 28Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. NeurIPS, 28, 2015.

Faster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, TPAMI. 3906Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. TPAMI, 39(06):1137-1149, 2017.

R-fcn: Object detection via region-based fully convolutional networks. Jifeng Dai, Yi Li, Kaiming He, Jian Sun, NeurIPS. 29Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolu- tional networks. NeurIPS, 29, 2016.

Light-head r-cnn: In defense of two-stage object detector. Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, Jian Sun, arXiv:1711.07264arXiv preprintZeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, and Jian Sun. Light-head r-cnn: In defense of two-stage object detector. arXiv preprint arXiv:1711.07264, 2017.

Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. Tsung-Yi Lin, Piotr Dollár, Ross Girshick, CVPR. Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, pages 2117-2125, 2017.

Joseph Redmon, Ali Farhadi, arXiv:1804.02767Yolov3: An incremental improvement. arXiv preprintJoseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.

Yolov4: Optimal speed and accuracy of object detection. Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao, arXiv:2004.10934arXiv preprintAlexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020.

Yolo9000: better, faster, stronger. Joseph Redmon, Ali Farhadi, CVPR. Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In CVPR, pages 7263-7271, 2017.

You only learn one representation: Unified network for multiple tasks. Chien-Yao Wang, I-Hau Yeh, Hong-Yuan Mark Liao, arXiv:2105.04206arXiv preprintChien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. You only learn one representation: Unified network for multiple tasks. arXiv preprint arXiv:2105.04206, 2021.

Kaiming He, and Piotr Dollár. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, ICCV. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In ICCV, pages 2980-2988, 2017.

Cornernet: Detecting objects as paired keypoints. Hei Law, Jia Deng, ECCV. Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints. In ECCV, pages 734-750, 2018.

Bottom-up object detection by grouping extreme and center points. Xingyi Zhou, Jiacheng Zhuo, Philipp Krahenbuhl, CVPR. Xingyi Zhou, Jiacheng Zhuo, and Philipp Krahenbuhl. Bottom-up object detection by grouping ex- treme and center points. In CVPR, pages 850-859, 2019.

. Xingyi Zhou, Dequan Wang, Philipp Krähenbühl, arXiv:1904.07850Objects as points. arXiv preprintXingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points. arXiv preprint arXiv:1904.07850, 2019.

End-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, ECCV. SpringerNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, pages 213-229. Springer, 2020.

Deformable detr: Deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, ICLR. 2021Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: De- formable transformers for end-to-end object detection. In ICLR, 2021.

On efficient real-time semantic segmentation: a survey. J Christopher, Muhammad Holder, Shafique, arXiv:2206.08605arXiv preprintChristopher J Holder and Muhammad Shafique. On efficient real-time semantic segmentation: a survey. arXiv preprint arXiv:2206.08605, 2022.

Pyramid scene parsing network. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia, CVPR. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, pages 2881-2890, 2017.

Segnet: A deep convolutional encoderdecoder architecture for image segmentation. Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla, TPAMI. 3912Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder- decoder architecture for image segmentation. TPAMI, 39(12):2481-2495, 2017.

Enet: A deep neural network architecture for real-time semantic segmentation. Adam Paszke, Abhishek Chaurasia, Sangpil Kim, Eugenio Culurciello, arXiv:1606.02147arXiv preprintAdam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello. Enet: A deep neural net- work architecture for real-time semantic segmentation. arXiv preprint arXiv:1606.02147, 2016.

Linknet: Exploiting encoder representations for efficient semantic segmentation. Abhishek Chaurasia, Eugenio Culurciello, 2017 IEEE visual communications and image processing (VCIP). IEEEAbhishek Chaurasia and Eugenio Culurciello. Linknet: Exploiting encoder representations for efficient semantic segmentation. In 2017 IEEE visual communications and image processing (VCIP), pages 1-4. IEEE, 2017.

Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation. Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, Hannaneh Hajishirzi, ECCV. Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, and Hannaneh Hajishirzi. Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation. In ECCV, pages 552-568, 2018.

Segnext: Rethinking convolutional attention design for semantic segmentation. Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, Shi-Min Hu, NeurIPS. 2022Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, and Shi-Min Hu. Seg- next: Rethinking convolutional attention design for semantic segmentation. In NeurIPS, 2022.

Erfnet: Efficient residual factorized convnet for real-time semantic segmentation. Eduardo Romera, M José, Alvarez, M Luis, Roberto Bergasa, Arroyo, IEEE Transactions on Intelligent Transportation Systems. 191Eduardo Romera, José M Alvarez, Luis M Bergasa, and Roberto Arroyo. Erfnet: Efficient residual fac- torized convnet for real-time semantic segmentation. IEEE Transactions on Intelligent Transportation Systems, 19(1):263-272, 2017.

Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation. Gen Li, Joongkyu Kim, BMVC. Gen Li and Joongkyu Kim. Dabnet: Depth-wise asymmetric bottleneck for real-time semantic seg- mentation. In BMVC, 2019.

Efficient dense modules of asymmetric convolution for real-time semantic segmentation. Hsueh-Ming Shao-Yuan Lo, Sheng-Wei Hang, Jing-Jhih Chan, Lin, ACM MM Asia. Shao-Yuan Lo, Hsueh-Ming Hang, Sheng-Wei Chan, and Jing-Jhih Lin. Efficient dense modules of asymmetric convolution for real-time semantic segmentation. In ACM MM Asia, 2019.

Efficient ladder-style densenets for semantic segmentation of large images. Ivan Krešo, Josip Krapac, Sinišašegvić , IEEE Transactions on Intelligent Transportation Systems. 228Ivan Krešo, Josip Krapac, and SinišaŠegvić. Efficient ladder-style densenets for semantic segmen- tation of large images. IEEE Transactions on Intelligent Transportation Systems, 22(8):4951-4961, 2020.

Encoderdecoder with atrous separable convolution for semantic image segmentation. Yukun Liang-Chieh Chen, George Zhu, Florian Papandreou, Hartwig Schroff, Adam, ECCV. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder- decoder with atrous separable convolution for semantic image segmentation. In ECCV, pages 801- 818, 2018.

Fast-scnn: Fast semantic segmentation network. P K Rudra, Stephan Poudel, Roberto Liwicki, Cipolla, BMVC. Rudra PK Poudel, Stephan Liwicki, and Roberto Cipolla. Fast-scnn: Fast semantic segmentation net- work. In BMVC, 2019.

Shelfnet for fast semantic segmentation. Juntang Zhuang, Junlin Yang, Lin Gu, Nicha Dvornek, ICCVW. Juntang Zhuang, Junlin Yang, Lin Gu, and Nicha Dvornek. Shelfnet for fast semantic segmentation. In ICCVW, pages 0-0, 2019.

In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images. Marin Orsic, Ivan Kreso, Petra Bevandic, Sinisa Segvic, CVPR. Marin Orsic, Ivan Kreso, Petra Bevandic, and Sinisa Segvic. In defense of pre-trained imagenet archi- tectures for real-time semantic segmentation of road-driving images. In CVPR, pages 12607-12616, 2019.

Contextnet: Exploring context and detail for semantic segmentation in real-time. P K Rudra, Ujwal Poudel, Stephan Bonde, Christopher Liwicki, Zach, BMVC. Rudra PK Poudel, Ujwal Bonde, Stephan Liwicki, and Christopher Zach. Contextnet: Exploring context and detail for semantic segmentation in real-time. In BMVC, 2018.

Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation. Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, Chunhua Shen, Nong Sang, IJCV. 129Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, Chunhua Shen, and Nong Sang. Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation. IJCV, 129:3051- 3068, 2021.

Rethinking bisenet for real-time semantic segmentation. Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng Luo, Xiaolin Wei, CVPR. Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng Luo, and Xiaolin Wei. Rethinking bisenet for real-time semantic segmentation. In CVPR, pages 9716-9725, 2021.

Icnet for real-time semantic segmentation on high-resolution images. Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia, ECCV. Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic segmentation on high-resolution images. In ECCV, pages 405-420, 2018.

Lednet: A lightweight encoder-decoder network for real-time semantic segmentation. Yu Wang, Quan Zhou, Jia Liu, Jian Xiong, Guangwei Gao, Xiaofu Wu, Longin Jan Latecki, ICIP. IEEEYu Wang, Quan Zhou, Jia Liu, Jian Xiong, Guangwei Gao, Xiaofu Wu, and Longin Jan Latecki. Lednet: A lightweight encoder-decoder network for real-time semantic segmentation. In ICIP, pages 1860- 1864. IEEE, 2019.

Dfanet: Deep feature aggregation for realtime semantic segmentation. Hanchao Li, Pengfei Xiong, Haoqiang Fan, Jian Sun, CVPR. Hanchao Li, Pengfei Xiong, Haoqiang Fan, and Jian Sun. Dfanet: Deep feature aggregation for real- time semantic segmentation. In CVPR, pages 9522-9531, 2019.

Real-time semantic segmentation with fast attention. Ping Hu, Federico Perazzi, Fabian Caba Heilbron, Oliver Wang, Zhe Lin, Kate Saenko, Stan Sclaroff, IEEE Robotics and Automation Letters. 61Ping Hu, Federico Perazzi, Fabian Caba Heilbron, Oliver Wang, Zhe Lin, Kate Saenko, and Stan Sclaroff. Real-time semantic segmentation with fast attention. IEEE Robotics and Automation Let- ters, 6(1):263-270, 2020.

Squeezenas: Fast neural architecture search for faster semantic segmentation. Albert Shaw, Daniel Hunter, Forrest Landola, Sammy Sidhu, ICCVW. Albert Shaw, Daniel Hunter, Forrest Landola, and Sammy Sidhu. Squeezenas: Fast neural architecture search for faster semantic segmentation. In ICCVW, pages 0-0, 2019.

Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation. Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, Li Fei-Fei, CVPR. Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, and Li Fei-Fei. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation. In CVPR, pages 82-92, 2019.

Fasterseg: Searching for faster real-time semantic segmentation. Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, Zhangyang Wang, ICLR. Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, and Zhangyang Wang. Fasterseg: Searching for faster real-time semantic segmentation. In ICLR, 2020.

Per-pixel classification is not all you need for semantic segmentation. Bowen Cheng, Alex Schwing, Alexander Kirillov, NeurIPS. 34Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. NeurIPS, 34:17864-17875, 2021.

Rethinking semantic segmentation from a sequenceto-sequence perspective with transformers. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, H S Philip, Torr, CVPR. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence- to-sequence perspective with transformers. In CVPR, pages 6881-6890, 2021.

Segformer: Simple and efficient design for semantic segmentation with transformers. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, M Jose, Ping Alvarez, Luo, NeurIPS. 34Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. NeurIPS, 34:12077-12090, 2021.

Semantic segmentation by early region proxy. Yifan Zhang, Bo Pang, Cewu Lu, CVPR. Yifan Zhang, Bo Pang, and Cewu Lu. Semantic segmentation by early region proxy. In CVPR, pages 1258-1268, 2022.

Maskedattention mask transformer for universal image segmentation. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, Rohit Girdhar, CVPR. Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked- attention mask transformer for universal image segmentation. In CVPR, pages 1290-1299, 2022.

A survey on instance segmentation: Recent advances and challenges. Huiyan Zhang, Hao Sun, Wengang Ao, Georgi Dimirovski, Int. J. Innov. Comput. Inf. Control. 17Huiyan Zhang, Hao Sun, Wengang Ao, and Georgi Dimirovski. A survey on instance segmentation: Recent advances and challenges. Int. J. Innov. Comput. Inf. Control, 17:1041-1053, 2021.

Mask r-cnn. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick, ICCV. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV, pages 2961-2969, 2017.

Masklab: Instance segmentation by refining object detection with semantic and direction features. Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng Wang, Hartwig Adam, CVPR. Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng Wang, and Hartwig Adam. Masklab: Instance segmentation by refining object detection with semantic and direction features. In CVPR, pages 4013-4022, 2018.

Mask scoring r-cnn. Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang, Xinggang Wang, CVPR. Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang, and Xinggang Wang. Mask scoring r-cnn. In CVPR, pages 6409-6418, 2019.

Path aggregation network for instance segmentation. Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, Jiaya Jia, CVPR. Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In CVPR, pages 8759-8768, 2018.

Hybrid task cascade for instance segmentation. Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, CVPR. Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In CVPR, pages 4974-4983, 2019.

Yolact: Real-time instance segmentation. Daniel Bolya, Chong Zhou, Fanyi Xiao, Yong Jae Lee, ICCV. Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In ICCV, pages 9157-9166, 2019.

Blendmask: Top-down meets bottom-up for instance segmentation. Kunyang Hao Chen, Zhi Sun, Chunhua Tian, Yongming Shen, Youliang Huang, Yan, CVPR. Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yongming Huang, and Youliang Yan. Blendmask: Top-down meets bottom-up for instance segmentation. In CVPR, pages 8573-8581, 2020.

Tensormask: A foundation for dense object segmentation. Xinlei Chen, Ross Girshick, Kaiming He, Piotr Dollár, ICCV. Xinlei Chen, Ross Girshick, Kaiming He, and Piotr Dollár. Tensormask: A foundation for dense object segmentation. In ICCV, pages 2061-2069, 2019.

Smart, secure, yet energy-efficient, internet-ofthings sensors. Yin Ayten Ozge Akmandor, Niraj K Hongxu, Jha, IEEE Transactions on Multi-Scale Computing Systems. 44Ayten Ozge Akmandor, YIN Hongxu, and Niraj K Jha. Smart, secure, yet energy-efficient, internet-of- things sensors. IEEE Transactions on Multi-Scale Computing Systems, 4(4):914-930, 2018.

Rahul Mishra, Hari Prabhat Gupta, Tanima Dutta, arXiv:2010.03954A survey on deep neural network compression: Challenges, overview, and solutions. arXiv preprintRahul Mishra, Hari Prabhat Gupta, and Tanima Dutta. A survey on deep neural network compression: Challenges, overview, and solutions. arXiv preprint arXiv:2010.03954, 2020.

A survey of model compression and acceleration for deep neural networks. Yu Cheng, Duo Wang, Pan Zhou, Tao Zhang, arXiv:1710.09282arXiv preprintYu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.

Convolutional neural network pruning: A survey. Sheng Xu, Anran Huang, Lei Chen, Baochang Zhang, 2020 39th Chinese Control Conference (CCC). IEEESheng Xu, Anran Huang, Lei Chen, and Baochang Zhang. Convolutional neural network pruning: A survey. In 2020 39th Chinese Control Conference (CCC), pages 7458-7463. IEEE, 2020.

Pruning and quantization for deep neural network acceleration: A survey. Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, Xiaotong Zhang, Neurocomputing. 461Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. Pruning and quantization for deep neural network acceleration: A survey. Neurocomputing, 461:370-403, 2021.

A survey of quantization methods for efficient neural network inference. Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, W Michael, Kurt Mahoney, Keutzer, Low-Power Computer Vision. Chapman and Hall/CRCAmir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. In Low-Power Computer Vision, pages 291-326. Chapman and Hall/CRC.

Exploiting linear structure within convolutional networks for efficient evaluation. Wojciech Emily L Denton, Joan Zaremba, Yann Bruna, Rob Lecun, Fergus, NeurIPS. 27Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. NeurIPS, 27, 2014.

Max Jaderberg, Andrea Vedaldi, Andrew Zisserman, arXiv:1405.3866Speeding up convolutional neural networks with low rank expansions. arXiv preprintMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.

Fitnets: Hints for thin deep nets. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio, ICLR. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.

Correlation congruence for knowledge distillation. Baoyun Peng, Xiao Jin, Dongsheng Li, Shunfeng Zhou, Yichao Wu, Jiaheng Liu, Zhaoning Zhang, Yu Liu, ICCV. Baoyun Peng, Xiao Jin, Dongsheng Li, Shunfeng Zhou, Yichao Wu, Jiaheng Liu, Zhaoning Zhang, and Yu Liu. Correlation congruence for knowledge distillation. In ICCV, pages 5007-5016, 2019.

Weight sharing and deep learning for spectral data. Jacob Søgaard Larsen, Line Clemmensen, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEJacob Søgaard Larsen and Line Clemmensen. Weight sharing and deep learning for spectral data. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4227-4231. IEEE, 2020.

Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, Minje Park, Pvanet, arXiv:1608.08021Deep but lightweight neural networks for real-time object detection. arXiv preprintKye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, and Minje Park. Pvanet: Deep but lightweight neural networks for real-time object detection. arXiv preprint arXiv:1608.08021, 2016.

Channel pruning for accelerating very deep neural networks. Yihui He, Xiangyu Zhang, Jian Sun, ICCV. Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In ICCV, pages 1389-1397, 2017.

Variational convolutional neural network pruning. Chenglong Zhao, Bingbing Ni, Jian Zhang, Qiwei Zhao, Wenjun Zhang, Qi Tian, CVPR. Chenglong Zhao, Bingbing Ni, Jian Zhang, Qiwei Zhao, Wenjun Zhang, and Qi Tian. Variational convolutional neural network pruning. In CVPR, pages 2780-2789, 2019.

Thinet: A filter level pruning method for deep neural network compression. Jian-Hao Luo, Jianxin Wu, Weiyao Lin, ICCV. Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In ICCV, pages 5058-5066, 2017.

Deepmon: Mobile gpu-based deep learning framework for continuous vision applications. N Loc, Youngki Huynh, Rajesh Krishna Lee, Balan, Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services. the 15th Annual International Conference on Mobile Systems, Applications, and ServicesLoc N Huynh, Youngki Lee, and Rajesh Krishna Balan. Deepmon: Mobile gpu-based deep learning framework for continuous vision applications. In Proceedings of the 15th Annual International Confer- ence on Mobile Systems, Applications, and Services, pages 82-95, 2017.

Eie: Efficient inference engine on compressed deep neural network. Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, A Mark, William J Horowitz, Dally, ACM SIGARCH Computer Architecture News. 443Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William J Dally. Eie: Efficient inference engine on compressed deep neural network. ACM SIGARCH Computer Archi- tecture News, 44(3):243-254, 2016.

Bayesian compression for deep learning. Christos Louizos, Karen Ullrich, Max Welling, Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. NeurIPS, 30, 2017.

Scnn: An accelerator for compressed-sparse convolutional neural networks. Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan Venkatesan, Brucek Khailany, Joel Emer, W Stephen, William J Keckler, Dally, ACM SIGARCH computer architecture news. 452Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan Venkatesan, Brucek Khailany, Joel Emer, Stephen W Keckler, and William J Dally. Scnn: An accelerator for compressed-sparse convolutional neural networks. ACM SIGARCH computer architecture news, 45(2):27-40, 2017.

Multi-task zipping via layer-wise neuron sharing. Xiaoxi He, Zimu Zhou, Lothar Thiele, NeurIPS. 31Xiaoxi He, Zimu Zhou, and Lothar Thiele. Multi-task zipping via layer-wise neuron sharing. NeurIPS, 31, 2018.

Deep learning with limited numerical precision. Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan, ICML. PMLRSuyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In ICML, pages 1737-1746. PMLR, 2015.

Hardware-oriented approximation of convolutional neural networks. Philipp Gysel, Mohammad Motamedi, Soheil Ghiasi, arXiv:1604.03168arXiv preprintPhilipp Gysel, Mohammad Motamedi, and Soheil Ghiasi. Hardware-oriented approximation of con- volutional neural networks. arXiv preprint arXiv:1604.03168, 2016.

Pramod Kumar Swami, and Soyeb Nagori. Sparse, quantized, full frame cnn for low power embedded devices. Manu Mathew, Kumar Desappan, CVPR Workshops. Manu Mathew, Kumar Desappan, Pramod Kumar Swami, and Soyeb Nagori. Sparse, quantized, full frame cnn for low power embedded devices. In CVPR Workshops, pages 11-19, 2017.

Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio, arXiv:1602.02830Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprintMatthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.

Compressing deep convolutional networks using vector quantization. Yunchao Gong, Liu Liu, Ming Yang, Lubomir Bourdev, arXiv:1412.6115arXiv preprintYunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.

Quantized convolutional neural networks for mobile devices. Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng, CVPR. Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized convolutional neural networks for mobile devices. In CVPR, pages 4820-4828, 2016.

Compressing neural networks with the hashing trick. Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, Yixin Chen, ICML. PMLRWenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. Compressing neural networks with the hashing trick. In ICML, pages 2285-2294. PMLR, 2015.

Song Han, Huizi Mao, William J Dally, arXiv:1510.00149Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprintSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.

Learning efficient object detection models with knowledge distillation. NeurIPS, 30. Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, Manmohan Chandraker, Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efficient object detection models with knowledge distillation. NeurIPS, 30, 2017.

Fast human pose estimation. Feng Zhang, Xiatian Zhu, Mao Ye, CVPR. Feng Zhang, Xiatian Zhu, and Mao Ye. Fast human pose estimation. In CVPR, pages 3517-3526, 2019.

Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Sergey Zagoruyko, Nikos Komodakis, Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the perfor- mance of convolutional neural networks via attention transfer. In ICLR, 2017.

Learning to specialize with knowledge distillation for visual question answering. Jonghwan Mun, Kimin Lee, Jinwoo Shin, Bohyung Han, NeurIPS. 31Jonghwan Mun, Kimin Lee, Jinwoo Shin, and Bohyung Han. Learning to specialize with knowledge distillation for visual question answering. NeurIPS, 31, 2018.

Paraphrasing complex network: Network compression via factor transfer. Jangho Kim, Seonguk Park, Nojun Kwak, NeurIPS. 31Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphrasing complex network: Network compression via factor transfer. NeurIPS, 31, 2018.

Knowledge distillation via route constrained optimization. Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan, Xiaolin Hu, ICCV. Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan, and Xiaolin Hu. Knowledge distillation via route constrained optimization. In ICCV, pages 1345-1354, 2019.

Feature normalized knowledge distillation for image classification. Kunran Xu, Lai Rui, Yishi Li, Lin Gu, ECCV. SpringerKunran Xu, Lai Rui, Yishi Li, and Lin Gu. Feature normalized knowledge distillation for image classi- fication. In ECCV, pages 664-680. Springer, 2020.

Exclusivity-consistency regularized knowledge distillation for face recognition. Xiaobo Wang, Tianyu Fu, Shengcai Liao, Shuo Wang, Zhen Lei, Tao Mei, ECCV. SpringerXiaobo Wang, Tianyu Fu, Shengcai Liao, Shuo Wang, Zhen Lei, and Tao Mei. Exclusivity-consistency regularized knowledge distillation for face recognition. In ECCV, pages 325-342. Springer, 2020.

Towards learning spatially discriminative feature representations. Chaofei Wang, Jiayu Xiao, Yizeng Han, Qisen Yang, Shiji Song, Gao Huang, ICCV. Chaofei Wang, Jiayu Xiao, Yizeng Han, Qisen Yang, Shiji Song, and Gao Huang. Towards learning spatially discriminative feature representations. In ICCV, pages 1326-1335, 2021.

Learn from the past: Experience ensemble knowledge distillation. Chaofei Wang, Shaowei Zhang, Shiji Song, Gao Huang, ICPR. 2022Chaofei Wang, Shaowei Zhang, Shiji Song, and Gao Huang. Learn from the past: Experience ensemble knowledge distillation. In ICPR, 2022.

A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. Junho Yim, Donggyu Joo, Ji-Hoon Bae, Junmo Kim, CVPR. Junho Yim, Donggyu Joo, Ji-Hoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In CVPR, 2017.

Self-supervised knowledge distillation using singular value decomposition. Dae Ha Seung Hyun Lee, Byung Cheol Kim, Song, ECCV. Seung Hyun Lee, Dae Ha Kim, and Byung Cheol Song. Self-supervised knowledge distillation using singular value decomposition. In ECCV, pages 335-350, 2018.

Heterogeneous knowledge distillation using information flow modeling. Nikolaos Passalis, Maria Tzelepi, Anastasios Tefas, CVPR. Nikolaos Passalis, Maria Tzelepi, and Anastasios Tefas. Heterogeneous knowledge distillation using information flow modeling. In CVPR, pages 2339-2348, 2020.

Knowledge distillation via instance relationship graph. Yufan Liu, Jiajiong Cao, Bing Li, Chunfeng Yuan, Weiming Hu, Yangxi Li, Yunqiang Duan, CVPR. Yufan Liu, Jiajiong Cao, Bing Li, Chunfeng Yuan, Weiming Hu, Yangxi Li, and Yunqiang Duan. Knowl- edge distillation via instance relationship graph. In CVPR, pages 7096-7104, 2019.

Similarity-preserving knowledge distillation. Frederick Tung, Greg Mori, ICCV. Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In ICCV, pages 1365- 1374, 2019.

Relational knowledge distillation. Wonpyo Park, Dongju Kim, Yan Lu, Minsu Cho, CVPR. Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In CVPR, pages 3967-3976, 2019.

Deep mutual learning. Ying Zhang, Tao Xiang, Timothy M Hospedales, Huchuan Lu, CVPR. Ying Zhang, Tao Xiang, Timothy M. Hospedales, and Huchuan Lu. Deep mutual learning. In CVPR, 2018.

Knowledge distillation by on-the-fly native ensemble. Xiatian Zhu, Shaogang Gong, NeurIPS. 31Xiatian Zhu, Shaogang Gong, et al. Knowledge distillation by on-the-fly native ensemble. NeurIPS, 31, 2018.

Online knowledge distillation via collaborative learning. Qiushan Guo, Xinjiang Wang, Yichao Wu, Zhipeng Yu, Ding Liang, Xiaolin Hu, Ping Luo, CVPR. Qiushan Guo, Xinjiang Wang, Yichao Wu, Zhipeng Yu, Ding Liang, Xiaolin Hu, and Ping Luo. Online knowledge distillation via collaborative learning. In CVPR, pages 11020-11029, 2020.

Online ensemble model compression using knowledge distillation. Devesh Walawalkar, Zhiqiang Shen, Marios Savvides, ECCV. SpringerDevesh Walawalkar, Zhiqiang Shen, and Marios Savvides. Online ensemble model compression using knowledge distillation. In ECCV, pages 18-35. Springer, 2020.

Online knowledge distillation with diverse peers. Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, Chun Chen, AAAI. Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, and Chun Chen. Online knowledge distillation with diverse peers. In AAAI, 2020.

Be your own teacher: Improve the performance of convolutional neural networks via self distillation. Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, Kaisheng Ma, ICCV. Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In ICCV, pages 3713-3722, 2019.

Learning lightweight lane detection cnns by self attention distillation. Yuenan Hou, Zheng Ma, Chunxiao Liu, Chen Change Loy, ICCV. Yuenan Hou, Zheng Ma, Chunxiao Liu, and Chen Change Loy. Learning lightweight lane detection cnns by self attention distillation. In ICCV, pages 1013-1021, 2019.

Snapshot distillation: Teacher-student optimization in one generation. Chenglin Yang, Lingxi Xie, Chi Su, Alan L Yuille, CVPR. Chenglin Yang, Lingxi Xie, Chi Su, and Alan L Yuille. Snapshot distillation: Teacher-student optimiza- tion in one generation. In CVPR, pages 2859-2868, 2019.

Few-shot image recognition with knowledge transfer. Zhimao Peng, Zechao Li, Junge Zhang, Yan Li, Guo-Jun Qi, Jinhui Tang, ICCV. Zhimao Peng, Zechao Li, Junge Zhang, Yan Li, Guo-Jun Qi, and Jinhui Tang. Few-shot image recog- nition with knowledge transfer. In ICCV, pages 441-449, 2019.

Efficient knowledge distillation from model checkpoints. Chaofei Wang, Qisen Yang, Rui Huang, Shiji Song, Gao Huang, NeurIPS. 2022Chaofei Wang, Qisen Yang, Rui Huang, Shiji Song, and Gao Huang. Efficient knowledge distillation from model checkpoints. In NeurIPS, 2022.

Tc3kd: Knowledge distillation via teacher-student cooperative curriculum customization. Chaofei Wang, Ke Yang, Shaowei Zhang, Gao Huang, Shiji Song, Neurocomputing. 508Chaofei Wang, Ke Yang, Shaowei Zhang, Gao Huang, and Shiji Song. Tc3kd: Knowledge distillation via teacher-student cooperative curriculum customization. Neurocomputing, 508:284-292, 2022.

Triplet distillation for deep face recognition. Yushu Feng, Huan Wang, Haoji Roland Hu, Lu Yu, Wei Wang, Shiyan Wang, ICIP. IEEEYushu Feng, Huan Wang, Haoji Roland Hu, Lu Yu, Wei Wang, and Shiyan Wang. Triplet distillation for deep face recognition. In ICIP, pages 808-812. IEEE, 2020.

Modality distillation with multiple stream networks for action recognition. C Nuno, Pietro Garcia, Vittorio Morerio, Murino, ECCV. Nuno C Garcia, Pietro Morerio, and Vittorio Murino. Modality distillation with multiple stream net- works for action recognition. In ECCV, pages 103-118, 2018.

D3d: Distilled 3d networks for video action recognition. Jonathan Stroud, David Ross, Chen Sun, Jia Deng, Rahul Sukthankar, WACV. Jonathan Stroud, David Ross, Chen Sun, Jia Deng, and Rahul Sukthankar. D3d: Distilled 3d networks for video action recognition. In WACV, pages 625-634, 2020.

Relation distillation networks for video object detection. Jiajun Deng, Yingwei Pan, Ting Yao, Wengang Zhou, Houqiang Li, Tao Mei, ICCV. Jiajun Deng, Yingwei Pan, Ting Yao, Wengang Zhou, Houqiang Li, and Tao Mei. Relation distillation networks for video object detection. In ICCV, pages 7023-7032, 2019.

General instance distillation for object detection. Xing Dai, Zeren Jiang, Zhao Wu, Yiping Bao, Zhicheng Wang, Si Liu, Erjin Zhou, CVPR. Xing Dai, Zeren Jiang, Zhao Wu, Yiping Bao, Zhicheng Wang, Si Liu, and Erjin Zhou. General instance distillation for object detection. In CVPR, pages 7842-7851, 2021.

Structured knowledge distillation for semantic segmentation. Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, Jingdong Wang, CVPR. Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, and Jingdong Wang. Structured knowl- edge distillation for semantic segmentation. In CVPR, pages 2604-2613, 2019.

Geometryaware distillation for indoor semantic segmentation. Jianbo Jiao, Yunchao Wei, Zequn Jie, Honghui Shi, W H Rynson, Thomas S Lau, Huang, CVPR. Jianbo Jiao, Yunchao Wei, Zequn Jie, Honghui Shi, Rynson WH Lau, and Thomas S Huang. Geometry- aware distillation for indoor semantic segmentation. In CVPR, pages 2869-2878, 2019.

Intra-class feature variation distillation for semantic segmentation. Yukang Wang, Wei Zhou, Tao Jiang, Xiang Bai, Yongchao Xu, ECCV. SpringerYukang Wang, Wei Zhou, Tao Jiang, Xiang Bai, and Yongchao Xu. Intra-class feature variation distil- lation for semantic segmentation. In ECCV, pages 346-362. Springer, 2020.

Inter-region affinity distillation for road marking segmentation. Yuenan Hou, Zheng Ma, Chunxiao Liu, Tak-Wai Hui, Chen Change Loy, CVPR. Yuenan Hou, Zheng Ma, Chunxiao Liu, Tak-Wai Hui, and Chen Change Loy. Inter-region affinity distillation for road marking segmentation. In CVPR, pages 12486-12495, 2020.

Learning monocular depth by distilling cross-domain stereo networks. Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, Xiaogang Wang, ECCV. Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, and Xiaogang Wang. Learning monocular depth by distilling cross-domain stereo networks. In ECCV, pages 484-500, 2018.

Pad-net: Multi-tasks guided prediction-anddistillation network for simultaneous depth estimation and scene parsing. Dan Xu, Wanli Ouyang, Xiaogang Wang, Nicu Sebe, CVPR. Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe. Pad-net: Multi-tasks guided prediction-and- distillation network for simultaneous depth estimation and scene parsing. In CVPR, pages 675-684, 2018.

Learning monocular depth estimation infusing traditional stereo knowledge. Fabio Tosi, Filippo Aleotti, Matteo Poggi, Stefano Mattoccia, CVPR. Fabio Tosi, Filippo Aleotti, Matteo Poggi, and Stefano Mattoccia. Learning monocular depth estimation infusing traditional stereo knowledge. In CVPR, pages 9799-9809, 2019.

Refine and distill: Exploiting cycleinconsistency and knowledge distillation for unsupervised monocular depth estimation. Andrea Pilzer, Stephane Lathuiliere, Nicu Sebe, Elisa Ricci, CVPR. Andrea Pilzer, Stephane Lathuiliere, Nicu Sebe, and Elisa Ricci. Refine and distill: Exploiting cycle- inconsistency and knowledge distillation for unsupervised monocular depth estimation. In CVPR, pages 9768-9777, 2019.

Rahul Sukthankar, and Marius Leordeanu. Depth distillation: unsupervised metric depth estimation for uavs by finding consensus between kinematics, optical flow and deep learning. Mihai Pirvu, Victor Robu, Vlad Licaret, Dragos Costea, Alina Marcu, Emil Slusanschi, CVPR. Mihai Pirvu, Victor Robu, Vlad Licaret, Dragos Costea, Alina Marcu, Emil Slusanschi, Rahul Suk- thankar, and Marius Leordeanu. Depth distillation: unsupervised metric depth estimation for uavs by finding consensus between kinematics, optical flow and deep learning. In CVPR, pages 3215-3223, 2021.

Knowledge distillation for fast and accurate monocular depth estimation on mobile devices. Yiran Wang, Xingyi Li, Min Shi, Ke Xian, Zhiguo Cao, CVPR. Yiran Wang, Xingyi Li, Min Shi, Ke Xian, and Zhiguo Cao. Knowledge distillation for fast and accurate monocular depth estimation on mobile devices. In CVPR, pages 2457-2465, 2021.

Feature estimations based correlation distillation for incremental image retrieval. Wei Chen, Yu Liu, Nan Pu, Weiping Wang, Li Liu, Michael S Lew, TMM. 24Wei Chen, Yu Liu, Nan Pu, Weiping Wang, Li Liu, and Michael S Lew. Feature estimations based correlation distillation for incremental image retrieval. TMM, 24:1844-1856, 2021.

Deep hash distillation for image retrieval. Geonmo Young Kyun Jang, Byungsoo Gu, Isaac Ko, Nam Ik Kang, Cho, ECCV. SpringerYoung Kyun Jang, Geonmo Gu, Byungsoo Ko, Isaac Kang, and Nam Ik Cho. Deep hash distillation for image retrieval. In ECCV, pages 354-371. Springer, 2022.

Object relational graph with teacher-recommended learning for video captioning. Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, Zheng-Jun Zha, CVPR. Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, and Zheng-Jun Zha. Object relational graph with teacher-recommended learning for video captioning. In CVPR, pages 13278- 13288, 2020.

Spatio-temporal graph for video captioning with knowledge distillation. Haoye Boxiao Pan, De-An Cai, Kuan-Hui Huang, Adrien Lee, Ehsan Gaidon, Juan Carlos Adeli, Niebles, CVPR. Boxiao Pan, Haoye Cai, De-An Huang, Kuan-Hui Lee, Adrien Gaidon, Ehsan Adeli, and Juan Carlos Niebles. Spatio-temporal graph for video captioning with knowledge distillation. In CVPR, pages 10870-10879, 2020.

Revisiting knowledge distillation for image captioning. Jingjing Dong, Zhenzhen Hu, Yuanen Zhou, CAAI International Conference on Artificial Intelligence. SpringerJingjing Dong, Zhenzhen Hu, and Yuanen Zhou. Revisiting knowledge distillation for image caption- ing. In CAAI International Conference on Artificial Intelligence, pages 613-625. Springer, 2021.

Better and faster: Knowledge transfer from multiple self-supervised learning tasks via graph distillation for video classification. Chenrui Zhang, Yuxin Peng, In IJCAI. Chenrui Zhang and Yuxin Peng. Better and faster: Knowledge transfer from multiple self-supervised learning tasks via graph distillation for video classification. In IJCAI, 2018.

Efficient video classification using fewer frames. Shweta Bhardwaj, Mukundhan Srinivasan, Mitesh M Khapra, CVPR. Shweta Bhardwaj, Mukundhan Srinivasan, and Mitesh M Khapra. Efficient video classification using fewer frames. In CVPR, pages 354-363, 2019.

Revisit knowledge distillation: a teacher-free framework. Li Yuan, E H Francis, Guilin Tay, Tao Li, Jiashi Wang, Feng, Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisit knowledge distillation: a teacher-free framework. 2019.

Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H Chi, Sagar Jain, arXiv:2002.03532Understanding and improving knowledge distillation. arXiv preprintJiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H Chi, and Sagar Jain. Under- standing and improving knowledge distillation. arXiv preprint arXiv:2002.03532, 2020.

Explaining knowledge distillation by quantifying the knowledge. Xu Cheng, Zhefan Rao, Yilan Chen, Quanshi Zhang, CVPR. Xu Cheng, Zhefan Rao, Yilan Chen, and Quanshi Zhang. Explaining knowledge distillation by quan- tifying the knowledge. In CVPR, pages 12925-12935, 2020.

Speeding-up convolutional neural networks using fine-tuned cp-decomposition. V Lebedev, M Ganin, Rakhuba, V Oseledets, Lempitsky, ICLR. V Lebedev, Y Ganin, M Rakhuba, I Oseledets, and V Lempitsky. Speeding-up convolutional neural networks using fine-tuned cp-decomposition. In ICLR, 2015.

Convolutional neural networks with low-rank regularization. Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, E Weinan, ICLR. Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, and E Weinan. Convolutional neural networks with low-rank regularization. In ICLR, 2016.

Predicting parameters in deep learning. Misha Denil, Babak Shakibi, Laurent Dinh, Marc&apos;aurelio Ranzato, Nando De Freitas, NeurIPS. 26Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, and Nando De Freitas. Predicting parameters in deep learning. NeurIPS, 26, 2013.

A tucker deep computation model for mobile multimedia feature learning. Qingchen Zhang, T Laurence, Xingang Yang, Zhikui Liu, Peng Chen, Li, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM). 13Qingchen Zhang, Laurence T Yang, Xingang Liu, Zhikui Chen, and Peng Li. A tucker deep compu- tation model for mobile multimedia feature learning. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 13(3s):1-18, 2017.

Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification. Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, Rogerio Feris, CVPR. Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, and Rogerio Feris. Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification. In CVPR, pages 5334-5343, 2017.

Lowrank matrix factorization for deep neural network training with high-dimensional output targets. N Tara, Brian Sainath, Vikas Kingsbury, Ebru Sindhwani, Bhuvana Arisoy, Ramabhadran, 2013 IEEE international conference on acoustics, speech and signal processing. IEEETara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low- rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 6655-6659. IEEE, 2013.

Clip-q: Deep network compression learning by in-parallel pruningquantization. Frederick Tung, Greg Mori, CVPR. Frederick Tung and Greg Mori. Clip-q: Deep network compression learning by in-parallel pruning- quantization. In CVPR, pages 7873-7882, 2018.

Model compression via distillation and quantization. Antonio Polino, Razvan Pascanu, Dan Alistarh, ICLR. Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantiza- tion. In ICLR, 2018.

DARTS: Differentiable architecture search. Hanxiao Liu, Karen Simonyan, Yiming Yang, ICLR. Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In ICLR, 2019.

Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions. Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining Xie, Bichen Wu, Matthew Yu, Tao Xu, Kan Chen, CVPR. Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining Xie, Bichen Wu, Matthew Yu, Tao Xu, Kan Chen, et al. Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions. In CVPR, 2020.

Once-for-all: Train one network and specialize it for efficient deployment. Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, Song Han, ICLR. Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and specialize it for efficient deployment. In ICLR, 2020.

Neural architecture search survey: A hardware perspective. Krishna Teja Chitty-Venkata, Arun K Somani, ACM Computing Surveys. Krishna Teja Chitty-Venkata and Arun K Somani. Neural architecture search survey: A hardware perspective. ACM Computing Surveys, 2022.

Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, Tiny deep learning on iot devices. NeurIPS. Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, et al. Mcunet: Tiny deep learning on iot devices. NeurIPS, 2020.

Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning. Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, Olivier Temam, ACM SIGARCH Computer Architecture News. Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, and Olivier Temam. Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning. ACM SIGARCH Computer Architecture News, 2014.

Roofline: an insightful visual performance model for multicore architectures. Samuel Williams, Andrew Waterman, David Patterson, Communications of the ACM. Samuel Williams, Andrew Waterman, and David Patterson. Roofline: an insightful visual performance model for multicore architectures. Communications of the ACM, 2009.

Cnvlutin: Ineffectual-neuron-free deep neural network computing. Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright Jerger, Andreas Moshovos, ACM SIGARCH Computer Architecture NewsJorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright Jerger, and Andreas Moshovos. Cnvlutin: Ineffectual-neuron-free deep neural network computing. ACM SIGARCH Com- puter Architecture News, 2016.

An efficient hardware accelerator for sparse convolutional neural networks on fpgas. Liqiang Lu, Jiaming Xie, Ruirui Huang, Jiansong Zhang, Wei Lin, Yun Liang, FCCM. Liqiang Lu, Jiaming Xie, Ruirui Huang, Jiansong Zhang, Wei Lin, and Yun Liang. An efficient hardware accelerator for sparse convolutional neural networks on fpgas. In FCCM, 2019.

When neural architecture search meets hardware implementation: from hardware awareness to co-design. Xinyi Zhang, Weiwen Jiang, Yiyu Shi, Jingtong Hu, ISVLSI. Xinyi Zhang, Weiwen Jiang, Yiyu Shi, and Jingtong Hu. When neural architecture search meets hardware implementation: from hardware awareness to co-design. In ISVLSI, 2019.

Standing on the shoulders of giants: Hardware and neural architecture co-search with hot start. Weiwen Jiang, Lei Yang, Sakyasingha Dasgupta, Jingtong Hu, Yiyu Shi, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. Weiwen Jiang, Lei Yang, Sakyasingha Dasgupta, Jingtong Hu, and Yiyu Shi. Standing on the shoulders of giants: Hardware and neural architecture co-search with hot start. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems, 2020.

Best of both worlds: Automl codesign of a cnn and its hardware accelerator. Łukasz Mohamed S Abdelfattah, Thomas Dudziak, Royson Chau, Hyeji Lee, Nicholas D Kim, Lane, DAC. Mohamed S Abdelfattah, Łukasz Dudziak, Thomas Chau, Royson Lee, Hyeji Kim, and Nicholas D Lane. Best of both worlds: Automl codesign of a cnn and its hardware accelerator. In DAC, 2020.

Mcunetv2: Memory-efficient patchbased inference for tiny deep learning. Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, Song Han, NeurIPS. 2021Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, and Song Han. Mcunetv2: Memory-efficient patch- based inference for tiny deep learning. In NeurIPS, 2021.

On-device training under 256kb memory. Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, Song Han, NeurIPS. 2022Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song Han. On-device training under 256kb memory. In NeurIPS, 2022.

Boosting the performance of cnn accelerators with dynamic fine-grained channel gating. Weizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang, G Edward Suh, MICROWeizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang, and G Edward Suh. Boosting the perfor- mance of cnn accelerators with dynamic fine-grained channel gating. In MICRO, 2019.

Drq: dynamic region-based quantization for deep neural network acceleration. Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li Jiang, Naifeng Jing, Xiaoyao Liang, ISCA. Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li Jiang, Naifeng Jing, and Xiaoyao Liang. Drq: dynamic region-based quantization for deep neural network acceleration. In ISCA, 2020.

Processor architecture optimization for spatially dynamic neural networks. Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars, Marian Verhelst, VLSI-SoC. Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars, and Marian Verhelst. Processor architecture optimization for spatially dynamic neural networks. In VLSI-SoC, 2021.

Energon: Toward efficient acceleration of transformers using dynamic sparse attention. Zhe Zhou, Junlin Liu, Zhenyu Gu, Guangyu Sun, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. Zhe Zhou, Junlin Liu, Zhenyu Gu, and Guangyu Sun. Energon: Toward efficient acceleration of transformers using dynamic sparse attention. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2022.

Image as set of points. Xu Ma, Yuqian Zhou, Huan Wang, Can Qin, Bin Sun, Chang Liu, Yun Fu, ICLR. Xu Ma, Yuqian Zhou, Huan Wang, Can Qin, Bin Sun, Chang Liu, and Yun Fu. Image as set of points. In ICLR, 2023.

Sequencer: Deep LSTM for image classification. Yuki Tatsunami, Masato Taki, NeurIPS. 2022Yuki Tatsunami and Masato Taki. Sequencer: Deep LSTM for image classification. In NeurIPS, 2022.

Vision GNN: An image is worth graph of nodes. Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang, Enhua Wu, NeurIPS2022Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang, and Enhua Wu. Vision GNN: An image is worth graph of nodes. In NeurIPS, 2022.

Image as a foreign language: Beit pretraining for all vision and vision-language tasks. Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Saksham Owais Khan Mohammed, Subhojit Singhal, Som, arXiv:2208.10442arXiv preprintWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.

Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, arXiv:2111.11432A new foundation model for computer vision. arXiv preprintLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xue- dong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.

Nas-fpn: Learning scalable feature pyramid architecture for object detection. Golnaz Ghiasi, Tsung-Yi Lin, Quoc V Le, CVPR. Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In CVPR, pages 7036-7045, 2019.

A unified sequence interface for vision tasks. Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, Geoffrey Hinton, NeurIPS. 2022Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J. Fleet, and Geoffrey Hinton. A unified sequence interface for vision tasks. In NeurIPS, 2022.

Saeed Reza Kheradpisheh, Timothée Masquelier, and Anthony Maida. Deep learning in spiking neural networks. Amirhossein Tavanaei, Masoud Ghodrati, Neural networks. 111Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh, Timothée Masquelier, and An- thony Maida. Deep learning in spiking neural networks. Neural networks, 111:47-63, 2019.

Momentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, CVPR. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsuper- vised visual representation learning. In CVPR, pages 9729-9738, 2020.

Masked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, CVPR. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoen- coders are scalable vision learners. In CVPR, pages 16000-16009, 2022.

Revisiting locally supervised learning: an alternative to end-to-end training. Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, Gao Huang, ICLR. 2021Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, and Gao Huang. Revisiting locally supervised learning: an alternative to end-to-end training. In ICLR, 2021.

Automated progressive learning for efficient training of vision transformers. Changlin Li, Bohan Zhuang, Guangrun Wang, Xiaodan Liang, Xiaojun Chang, Yi Yang, CVPR. 2022Changlin Li, Bohan Zhuang, Guangrun Wang, Xiaodan Liang, Xiaojun Chang, and Yi Yang. Automated progressive learning for efficient training of vision transformers. In CVPR, 2022.

Deep incubation: Training large models by divide-and-conquering. Zanlin Ni, Yulin Wang, Jiangwei Yu, Haojun Jiang, Yue Cao, Gao Huang, ICCV. 2022Zanlin Ni, Yulin Wang, Jiangwei Yu, Haojun Jiang, Yue Cao, and Gao Huang. Deep incubation: Training large models by divide-and-conquering. In ICCV, 2022.

Efficienttrain: Exploring generalized curriculum learning for training visual backbones. Yulin Wang, Yang Yue, Rui Lu, Tianjiao Liu, Zhao Zhong, Shiji Song, Gao Huang, ICCV. Yulin Wang, Yang Yue, Rui Lu, Tianjiao Liu, Zhao Zhong, Shiji Song, and Gao Huang. Efficienttrain: Exploring generalized curriculum learning for training visual backbones. In ICCV, 2023.