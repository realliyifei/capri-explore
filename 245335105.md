# A Comprehensive Analytical Survey on Unsupervised and Semi-Supervised Graph Representation Learning Methods

CorpusID: 245335105
 
tags: #Mathematics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/b415ecb687941e1e9ef68e04a4a1c68c73483d51](https://www.semanticscholar.org/paper/b415ecb687941e1e9ef68e04a4a1c68c73483d51)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

A Comprehensive Analytical Survey on Unsupervised and Semi-Supervised Graph Representation Learning Methods


Md Khaledur Rahman 
Computer Science
Indiana University
47408BloomingtonINUSA

Ariful Azad 
Intelligent Systems Engineering
Indiana University
47408BloomingtonINUSA

A Comprehensive Analytical Survey on Unsupervised and Semi-Supervised Graph Representation Learning Methods
Graph Representation LearningGraph Neural Network (GNN)ClassificationLink PredictionClustering
Graph representation learning is a fast-growing field where one of the main objectives is to generate meaningful representations of graphs in lower-dimensional spaces. The learned embeddings have been successfully applied to perform various prediction tasks, such as link prediction, node classification, clustering, and visualization. The collective effort of the graph learning community has delivered hundreds of methods, but no single method excels under all evaluation metrics such as prediction accuracy, running time, scalability, etc. This survey aims to evaluate all major classes of graph embedding methods by considering algorithmic variations, parameter selections, scalability, hardware and software platforms, downstream ML tasks, and diverse datasets. We organized graph embedding techniques using a taxonomy that includes methods from manual feature engineering, matrix factorization, shallow neural networks, and deep graph convolutional networks. We evaluated these classes of algorithms for node classification, link prediction, clustering, and visualization tasks using widely used benchmark graphs. We designed our experiments on top of PyTorch Geometric and DGL libraries and run experiments on different multicore CPU and GPU platforms. We rigorously scrutinize the performance of embedding methods under various performance metrics and summarize the results. Thus, this paper may serve as a comparative guide to help users select methods that are most suitable for their tasks.

## Introduction

A graph or network is a widely-used representation of relation data observed in a variety of applications, such as Online Social Network (OSN) analysis [106], scientific literature analysis [95], and biomedical data analysis [78]c. In a graph, a set of vertices represents entities, such as persons in an OSN, articles in scientific literature, and brain in neurons biomedical data. Similarly, a set of edges represents relationships among entities, such as friendship, citations, and neuron synapses in those networks, respectively. By analyzing graphs, we can extract and infer various structural and feature-based information from graph-structured data that can leverage many graph learning tasks, e.g., detection of communities, recommendation of friends or events, and prediction of online traffic.

Machine learning (ML) methods have been successfully applied to many research areas for solving prediction or recommendation tasks [65]. In particular, ML techniques for graphs have enjoyed tremendous success in many downstream tasks, such as node classification, link prediction, and clustering. The central task in graph ML is to learn the representation or embedding of graphs in a lower-dimensional space preserving the intrinsic properties of the original graphs. Then, the lower-dimensional embedding can be used for various prediction tasks. Over the last decade, researchers have developed a plethora of methods to learn the representation of graphs in both unsupervised and semi-supervised fashion. Early graph embedding methods extracted information from graphs by handcrafted feature engineering and then use them to prediction tasks [41,3,28]. Then, various unsupervised methods such as DeepWalk [79] and node2vec [34] were introduced to propel the graph embedding field to a new level. Finally, recent semisupervised graph embedding techniques can automatically extract a meaningful representation of graphs for the downstream ML tasks [50,37]. The diversity of these graph embedding methods often makes it difficult to select the best method given an application and computing platform. The aim of this survey is to experimentally evaluate all major classes of graph embedding methods so that application and algorithm developers are able to make informed decisions based on the comparative performance of embedding algorithms.

In addition to algorithmic diversity, evaluations of graph embedding methods are significantly influenced by the structure and size of graphs, software and hardware platforms where evaluations are performed, and various implementation details. Most real-world graphs are sparse with irregular sparsity structures such as scale-free graphs [8]. The irregular and often unpredictable sparsity of graphs makes it difficult to predict the performance of an algorithm on a graph without an experimental evaluation. In addition to the property of irregular structure, graphs can be very large; e.g., the Facebook network has around 2 billion vertices and over a trillion edges [58]. Many existing methods may fail to process such large graphs because  of memory and scalability limitations. Parallel graph embedding methods such as VERSE [100], Force2Vec [82] and parallelized DeepWalk [79] attempted to address the scalability problem by using multi-core processors and GPUs. A handful of methods [58,2] employed distributed-memory parallelism to process graphs with hundreds of millions vertices. Thus, to understand the performance (runtime, memory usage, scalability, etc.) of graph embedding methods, we need to run an extensive evaluation with a diverse collection of both small and large graphs on modern parallel computing systems. This survey aims to serve this evaluation purpose. Through our experimental evaluations, we shed light on efficiency, effectiveness, and scalability for graph representation learning. Several survey papers in the literature summarized existing methods, discussing applications, and in some cases, reporting their performance measures for different benchmark graphs [13,33,38,107,19]. However, these surveys do not provide in-depth analyses of running time, scalability, and memory consumption, which are the bottlenecks for large scale graph embedding. We cover all these issues in this survey in addition to the adverse effect of the unconscious negative sampling approach used by several methods upon downstream prediction tasks. Thus, this paper provides performance-centric guidance about existing graph embedding methods. We also discuss several future research directions that may advance this rapidly evolving field.


### Our Contributions

One of the main contributions of this this survey is to provide a comprehensive review of various experimental aspects related to unsupervised and semi-supervised graph embedding methods. Table 1 provides a summary of various experimental aspects surveyed this paper. We cover empirical performances (runtime, memory usage, scalability) on various hardware and software platforms for diverse algorithms and datasets. We also compare the performance of these algorithms in node classification, link prediction, clustering, and visualization tasks. Additionally, we survey the sensitivity of various methods with respect to their parameters. Hence, this paper complements other published surveys that extensively discussed existing graph embedding methods and applications. Below, we summarize our key contributions in this survey:

• Taxonomy of the embedding methods: We provide a taxonomy of existing graph representation learning methods and discuss the relation in the respective category. Our discussion focuses on the categories of the corresponding methods. We briefly outline how some methods differ from others in the same category.

• In depth running time and memory usage analysis:

We provide an enhanced discussion about the runtime and memory consumption which are mostly ignored in the existing survey papers. This may outline a new insight to develop scalable methods which will run fast and consume less memory.

• Parameter sensitivity and negative sampling: We show parameter sensitivity for several methods, such as the effect of choosing various dimensional embeddings, number of walks in random-walk-based method, etc. We also analyze the effect of choosing different numbers of negative samples. This analysis is significant since the results of prediction may significantly drop if the fraction of false negatives is high in the set of negative samples.

• Comprehensive experimental evaluation: We rigorously evaluate all embeddings and show comparative performance among several methods for several prediction tasks such as link prediction, multi-label classification, and graph reconstruction. We also compare methods based on their modularity score for clustering and visualizations.

• Challenges and limitations: We discuss some challenges that are commonly faced in the graph embedding domain. Then, we provide a list of limitations present in the existing graph embedding methods.

The rest of the survey is organized as follows: In Section 2, we provide some preliminary definitions and notations that are used throughout the paper. We formally define the graph embedding problem in Section 2.1. We discuss the taxonomy of existing graph embedding methods in Section 3. We discuss benchmark datasets and experimental analyses in Section 4. Finally, we discuss the challenges and limitations of the existing methods in Section 6.


## Preliminaries

In this section, we define several terminologies that are used throughout the rest of the paper. For the sake of simplicity, we assume that our input graph is undirected and unweighted.

We represent a graph by G = (V, E), where, V is the set of vertices, E is the set of edges and each z i of a set Z represents 1-dimensional embedding of i th vertex. We represent neighbors and the degree of vertex u in G by N(u) and deg(u), respectively. An adjacency matrix A of graph G is defined as follows:
A i j =       
1, if vertex i and vertex j are connected by an edge 0, otherwise Definition 2.1. (One-hot encoded representation) Given a graph G with |V| = n, one-hot encoded representation of V is denoted by a n × n dimensional matrix V such that V i j = 1, when i = j, otherwise V i j = 0 and i th row is a 1×n dimensional binary vector representation for i th vertex.

Definition 2.2. (Node embedding) Given a graph G with |V| = n and |E| = m, a node embedding is a mapping function f : u → z u ∈ IR d ∀u ∈ V such that d n and the function f preserves some similarity measure defined on graph G. Here, d is the dimension of the embedding. Definition 2.3. (Similarity function) We define a similarity function over Z to be any pairwise function σ :
z u × z v → [0, 1].
This similarity function is symmetric, i.e., for any z u and z v ,
σ(z u , z v ) = σ(z v , z u ).
Definition 2.4. (Identity matrix) We define a matrix I to be an indentity when all of its diagonal entries are 1's and all other entries in the matrix are 0's, i.e., ∀i I ii = 1, and I i j = 0, when i j.

Definition 2.5. (Degree matrix) We represent the degree matrix D of a graph G by filling all diagonal entries by degree of corresponding vertex and all other entries as zeros, i.e., ∀i D ii = deg(i), and D i j = 0, when i j.

We represent a differentiable non-linear function by φ which has different gradients at different points. The graph Laplacian matrix [67] is represented by L which is obtained from the degree matrix and adjacency matrix of the graph as follows:
L = D − A.
As "graph" is also referred to as "network" in the literature [7], we use these two terms interchangeably throughout the rest of our discussion. Similar to this, we also use other corresponding terminologies interchangeably like vertex vs. node, edge vs. link, and graph embedding vs. graph representation learning.


### Problem Definition

We can formally define the problem as follows: Given a graph G, we map it onto a d-dimensional space Z ∈ IR n×d such that the intrinsic properties of the original graph are preserved as much as possible. In other words, if we pick |N(u)| number of vertices for vertex u based on the similarity function σ in Figure 1: A conceptual figure to show that a mapping function preserves the notion of similarity of the original graph in embedding space. f is the mapping function and z i is the representation of i th vertex in the embedding space. the embedding space, then we should get all the neighbors of the original graph. Precisely, this represents the embedding of nodes in a network. In Fig. 1, we show pictorially that the embedding of vertices 3 and 5 preserves some notion of similarity in the embedding space as shown by z 3 and z 5 , respectively. So, if we find similarities among all other vertices with respect to vertex 3 in the embedding space, then the similarity between z 3 and z 5 will be higher than the similarity between z 3 and z 1 . A mapping or encoding function f is applied to generate such embedding of vertices.

Node embedding can be further extended to edge representation or even full representation of the graph. Simply, we can create an edge embedding of a graph by concatenating the embedding of two incident vertices of an edge. Similarly, a full representation of the graph can be deduced by taking the average contributions of all vertices for a given dimension. We describe several other kinds of edge embedding in Section 5.6.2. Most of the existing methods follow a general framework to generate and validate their graph embedding. We can summarize this precisely by Fig. 2. In the embedding generation step, a method is employed, and in the validation step, several prediction tasks are carried out using the learned embedding. Sometimes, embedding is pre-processed for performing latter tasks. For example, a portion of a dataset is used for training and the remaining portion of the dataset is used for testing. Generally, logistic regression is used for link prediction or node classification tasks, with the K-means algorithm being applied to cluster the graph, and the t-SNE [64] dimensionality reduction tool being used to visualize the embedding data.


## Methodologies

The methods for generating graph embedding can be classified into several categories based on the procedures, objective functions, and types of optimizations used. We propose a taxonomy of these categories as shown in Fig. 3. The early methods (during 2008 to 2011) of graph embedding were based upon the feature engineering of graphs. Then, with the increasing popularity of neural networks (shallow as well as deep), some methods were introduced (e.g., DeepWalk in 2014) and evolved which borrowed the idea from language modeling. At the same time, some matrix factorization methods were introduced and evolved alongside the shallow networks-based methods. In 2017, the paper on graph convolutional neural networks [50] was published and gained much popularity. In the following, we describe each of these categories elaborately. 


### Feature Engineering

Early methods on graph mining tasks were mostly based on supervised (hand-crafted) feature engineering [41,3,28]. Those hand-crafted features are designed with respect to some common intuitions that are supposed to infer some meaningful information from graphs. For example, ReFeX by Henderson et al. (2011) extracts a predetermined set of features such as the degree of a vertex (in/out-degree in case of directed graph and weights in case of a weighted graph), number of edges in the egonet (number of incoming/outgoing edges in case of a directed graph) and recursive features by summing up or averaging two types of previous features. Gallagher and Eliassi-Rad (2008), on the other hand, focuses on extracting features such as the average degree of egonet, the number of incident links, betweenness centrality, and clustering coefficient. These extracted features are then arranged in a vector format with their corresponding label and fed to a standard machine learning classifier for making predictions. The prediction tasks based on the extracted features are conducted in the following way: First feature vectors with corresponding labels are partitioned into two sets, e.g., T % of the samples are used for training and the rest of the datasets are used for testing. Then, logistic regression or any other standard machine learning method can be trained to learn the parameters of the model using the training dataset. Finally, the prediction task is performed using the test dataset based on the trained model. These types of methods do not always perform well on graph mining tasks because a predefined set of features is not always enough to capture the latent characteristics of the graph. Sometimes, they are found to be difficult to generalize across different types of graphs due to the highly irregular structure. Thus, more advanced methods have been evolved over time which is briefly discussed in the following sections.


### Matrix Factorization

Matrix Factorization is an effective technique to decompose a matrix into two lower-dimensional rectangular matrices [53]. This technique has been successfully applied to recommender systems [53], data compression [113], and spectral clustering [22]. It was first applied to graph factorization in large scale by Ahmed et al. (2013). Matrix A of dimension M × N can be decomposed into two lower-dimensional rectangular matrices B and C having dimensions M × R and R × N, respectively. Generally, R is much less than M and N. In the case of factorizing an adjacency matrix, we have M = N and C = B , where B is a transpose matrix of B. The loss function for this problem is defined by the following Equation:
f (A, B, λ) = 1 2 (i, j)∈E (A i j − B i .B j ) 2 + λ 2 i B i 2(1)
In Equation 1, λ is a regularization parameter. To optimize the loss function, we need to calculate the gradient of Equation 1. Doing so, we get the following:
∂ f (A, B, λ) ∂B i = − j∈N(i) (A i j − B i .B j )B j + λB i(2)
Now we can iteratively optimize the loss function by updating B using Stochastic Gradient Descent (SGD) [87]. Ahmed et al. (2013) implemented this method in distributed systems to optimize the loss function in both synchronous and asynchronous ways. Luo et al. (2011) propose a graph embedding technique that uses Cauchy distribution 1 to preserve the local topology of the graph in the embedding space [63]. Cauchy graph embedding optimizes the following objective function:
max z uv w uv (z u − z v ) 2 + γ 2 , s.t. u z 2 u = 1, u z u = 0.(3)
Here, γ is the scaling parameter, w uv is the edge weight between vertices u and v, and (z u − z v ) 2 represents the distance between vertices u and v in the embedding space. Thus the whole term indicates a similarity distribution that we want to maximize. Note that z u = 0 will maximize the objective function but collapse all vertices to a single one, which is not expected i.e., we do not want to maximize similarity where two vertices are located far away in terms of graph-theoretic distance. Thus, the authors add an extra term z 2 u = 1 in the constraints which 1 https://en.wikipedia.org/wiki/Cauchy_distribution prevent the embedding of all the vertices from collapsing into the origin. GraRep method, introduced by Cao et al. (2015), first constructs a transition probability matrix from an adjacent matrix to generate the embedding of the graph. The transition probability of a vertex u to another vertex v can be obtained from the adjacent matrix by normalizing the u th row dividing by deg(u) and then taking (u, v) entry of the normalized matrix. In particular, they denote the probability of transition from vertex u to v after k steps as p k (v|u) = A k u,v , where A k is a k th -order transition probability matrix. The reason for constructing such a matrix is to capture high-order similarity among vertices that have been found effective in random walk-based methods [79]. Then they deduce the loss function using the following equation:
L k (u, v) = A k u,v . log(σ(u.v)) + λ n u A u ,v . log(σ(−u.v)) (4)
In Equation 4, σ(u.v) generally represents a sigmoid function defined as σ(u.v) = 1 1+e −u.v . Authors of GraRep mostly follow the work of Levy and Goldberg (2014) in neural word embedding. They calculate the gradient of Equation 4 and formulate the problem as a matrix factorization where each entry of the original matrix is computed in the following way:
x = u.v = log A k u,v u A k u ,v − log( λ n )(5)
Cao et al. (2015) create a matrix X for all vertices using Equation 5 and replace all negative values by 0's. Then, they factorize X into UΣV using Singualar Value Decomposition (SVD) [31], where Σ represents the singular values in a diagonal matrix. Finally, they take U(Σ) 1 2 as the representation of the graph.

HOPE [75] is another matrix factorization-based method that preserves asymmetric transitivity of the graph by creating a higher-order proximity matrix similar to GraRep. A generalized proximity matrix S is constructed using a similarity metric and an adjacent matrix of the graph. Then, the objective function becomes to minimize S − U s .U t 2 F , where U s and U t are two embeddings for the source and the target, respectively. They formulate the proximity matrix S from the local and global proximity as follows:
S = M −1 g .M l(6)
Here, M g represents the global proximity matrix and M l represents the local proximity matrix. Generally, the global proximity has a recursive definition that can iterate over the whole graph to find global transitivity, whereas the local proximity has no recursive definition and tends to find local transitivity from neighborhoods. Katz Index [49] measures the relative influence of a node within a network, which can be used as a proximity measure. Thus, S can be defined by its Katz Index as follows:
S = ∞ l=1 β.A l = β.A.S + β.A S = (I − β.A) −1 .β.A(7)
In Equation 7, A is the adjacency matrix, β is a decay parameter, M g = I − β.A where I is the identity matrix, and M l = β.A. Specifically, authors incorporated the Katz Index [49] and the Rooted PageRank to construct global proximity matrices, and Common Neighbors and Adamic-Adar [1] to construct local proximity matrices. They also choose SVD to generate the final embedding.

Other techniques use a similar formulation, such as structurepreserving embedding [93], which maps graphs onto euclidean space which is also effective for visualization. In summary, SVD and graph laplacian have a strong influence on matrix factorization-based methods. Technically, these types of models have higher computation and memory consumption costs.


### Neural Networks

Neural networks have been widely used to solve prediction tasks in various research domains. In a neural networksbased method, there are three layers stacked in cascaded style, namely, (i) Input layer, (ii) Hidden layer(s), and (iii) Output layer. Generally, one hot-encoded vector of a vertex or input feature is fed to the network as input. The output layer contains target vertices or label(s) that are also one-hot encoded vectors. There can be multiple hidden layers in a neural network-based model. Hence, we can divide the neural network-based methods into two categories: (1) Shallow networks having one hidden layer and (2) Deep networks having more than one hidden layer. We describe each of these categories below: 


#### Shallow Networks

In this type of model, a random walk is performed on a graph and a path of a set of vertices is sampled for each random walk. Now, considering each vertex in the path as a word in a sentence, we can model this problem in a similar way to that of word representation in the NLP domain [69,68]. The basic task in word representation is to find the representation of words in vector space that preserves both syntactic and semantic meanings. This type of representation also becomes helpful for downstream analysis. In the graph embedding problem, we want to find the representation of vertices that preserves structural and neighborhood relationships in a graph. A popular model in the word embedding domain is word2vec that uses the skip-gram model [69] for optimizing the prediction of source context words from a target word. A pioneering graph embedding method, called DeepWalk [79], infers the analogy between word embedding and graph embedding, and uses the skip-gram model for training. In this model, target vertices are fed to the input layer and the weights of the network are updated to learn the semantics of the graph based on the source context vertices.

Suppose a random walk has the path v 1 , v 2 , v 3 , . . . v l such that 1 ≤ k ≤ l, where v k is the target vertex and all others are context vertices. To compute likelihood, a fixed window size of w slides over the path. Then, the likelihood of the target vertex based on the skip-gram model would be P((v k−w , . . . , v k−1 , v k+1 , . . . , v k+w )|σ(v k )), where σ represents a similarity function. The skip-gram model assumes that context vertices are conditionally independent. Hence, using negative log-likelihood, we can minimize the following objective function:
− log k+w i=k−w P(v i |σ(v k )(8)
The probability P(v i |σ(v k ) in Equation 8 is calculated by a normalized softmax function which can defined as follows:
P(v i |σ(v k ) = e z k .z i i e z k .z i(9)
Note that the normalizing factor i e z k .z i is computationally expensive which is asymptotically O(n). A more efficient way is to use hierarchical softmax [71] which takes O(log n) time to approximate Equation 9. In Fig. 4 (a), we have shown a toy graph with its representation. Let us sample a random walk from the graph as 1 → 4 → 2 → 3 → 5 → 4 and window size is 2. In Fig.  4 (b), we have shown the skip-gram model for the sampled random walk where 2 is the target vertex and {1, 4, 3, 5} are context vertices in the current window as shown in Fig.4 (a). The input vertex is a N dimensional one-hot-encoded vector. The hidden layer is a M dimensional vector representing activation units. Z and Z are two weight matrices that represent the embedding of the network and are learned by optimizing the objective function. A popular choice for optimizing the objective function is to use Stochastic Gradient Descent [12]. Thus the runtime for DeepWalk is O(dn log n), where d is the dimension of embedding for a graph with n vertices.

After the reputation of DeepWalk, many other methods have been introduced in the literature that is based upon a similar optimization model.  introduces LINE that optimizes an objective function based on first order and second order proximities. Instead of taking a random walk-based path sampling, LINE finds similarity considering all neighbors that are in the 1-hop distance as the first order proximity and 2-hop distance as the second-order proximity. For unweighted graph, first-order proximity is optimized by following the reduced KLdivergence Equation 10 whereas second-order proximity optimization follows Equation 8:
O 1 = − (i, j)∈E log p 1 (v i , v j )(10)
where, Fig. 5 (a), we represent the first order proximity vertices (which are A, B, E, and D) of C by blue circles which are in 1-hop distance from C. Similarly, second order proximity vertices of C are K, F, G, and H (shown by green circles in Fig. 5 (a)). It further improves running time by using negative sampling instead of hierarchical softmax. Thus the overall runtime for LINE becomes O(sdn), where s is the number of negative samples and generally, s < log n.
p 1 (v i , v j ) = 1 1+e −z i .z j . In
Grover and Leskovec (2016) propose node2vec which is another popular work that follows similar optimization function to the DeepWalk and negative sampling like LINE. It samples biased random walks from the network. Basically, it introduces two controlling parameters which can simultaneously discover homophily and structural equivalence in the network. A homophily relation determines the similarity among vertices that are close to one another or belong to the same clusters or community. On the other hand, structural equivalence characterizes vertices having a similar role in the graph. Authors of node2vec state that Breadth-First Search (BFS) based random walks are likely to contribute to structural equivalence discovery in the graph whereas Depth First Search (DFS) based random walks are likely to contribute to homophily discovery in the network. Though it is laborious to find optimal values for each controlling parameter, this method introduces some notion of universality in the tandem discovery of different latent characteristics. The running time for this method is the same as LINE. In Fig.  5(b), we show two types of walks represented by colored arrows where crimson-colored arrows to direct DFS search and violet-colored arrows direct BFS search. It is easy to infer from Fig. 5 (b) that C is structurally equivalent to H which is a hub in the network. Similarly, E and F are structurally equivalent which represent bridges in the network. Tsitsulin et al. (2018) introduce VERSE, a more recent work based on random walk, which can instantiate the embedding using several similarity functions such as personalized PageRank [76], adjacency similarity [100] and simrank [47]. Authors optimize the objective function using cross-entropy as it is proportional to KL-divergence. They also claim that the stationary distribution of random walk eventually converges to personalized PageRank. Rahman et al. (2020a) introduce a parallel force-directed graph embedding method, called Force2Vec, that can efficiently utilize the multi-core architecture and effectively generate embedding of graphs.

Most of the previously discussed shallow networks work better for homophily prediction. Ribeiro et al. (2017) propose struc2vec to the solve structural equivalence problem in networks. To begin, it creates a multi-layer weighted graph from the original graph where each layer contains an equal number of vertices and edges to the original graph. Weights in each layer are calculated as inversely proportional to structural distance as follows: If there is an edge between vertices u and v in G, then
weight w(u, v) = e − f (u,v) , where f (u, v)
is the structural distance between u and v which is determined by the recursive Equation 11.
f k (u, v) = f k−1 (u, v) + g(D k (u), D k (v))(11)
Here, k represents k-hop neighborhoods, f −1 = 0, D k (u) represents the ordered degree sequence of all vertices that are k-hop distant from u, and g measures the distance between degree sequences using Dynamic Time Warping (DTW) [86]. There are directed edges between two consecutive layers that connect corresponding vertices u's with an edge weight determined by the logarithmic number of edges incident to u having a larger weight than the average weight of the complete graph in that respective layer. Then, struc2vec samples random-walks from the multi-layered graph based on the normalized weight as the probability. The rest of the procedures are similar to DeepWalk, as described previously. Ribeiro et al. (2017) apply struc2vec to three novel airport datasets and empirically show that their method achieves better performance over previously existing methods. Some other works use the random-walk based model for graph embedding such as GEMSEC [90] which generates embeddings that are more useful for evaluating clusters and SINE [117] which generates embeddings that are more effective for incomplete graphs. In summary, the skip-gram model, introduced in natural language modeling, has a huge influence on shallow network-based graph embedding techniques.


#### Deep Networks

Deep learning [57] has become a very powerful framework which has been successfully applied in many research domains such as image classification [61,81], bioinformatics [70], speech recognition [5], etc. The idea of a convolutional operator [56] has made deep learning more flexible to use. It can extract features from structured input by itself which lessens the tiresome handcrafted feature extraction and automates the classification process. In deep convolutional neural networks, gen- erally, a filter 2 slides over the image in each convolutional layer, performs matrix multiplication, and pools information from it as features. This convolutional operator works well for regularly structured data as it can extract spatial features by matrix multiplication and pooling. For example, we show an image in Fig. 6 (a) where it can be represented as a grid graph. Then a filter slides over it from left to right and then top to bottom to capture spatial information from the image. Similarly, texts or speech signals (see Fig. 6(b)) can be represented as a line graph which is easy to design a convolutional operator for. However, it is very challenging to design a convolutional operator for graphs because of their highly irregular structure. In Fig. 6 (c), we show such a graph where different vertices have different degrees.

Sometimes, preliminary knowledge becomes available for a graph, such as users' profiles, images in social networks or, gene expressions profiles in biological networks. We may also want to use such information that may carry meaning about the graph. Unfortunately, other approaches, like matrixfactorization or shallow networks, can not deal with such information. In addition to this, embedding for dynamic graphs is also essential as new nodes (i.e., users) are coming to social networks daily, and we may not want to generate embeddings for new nodes by running the whole procedure again. This is termed as transductive problem settings i.e., a graph embedding model can generalize the network so that it can generate embeddings for new incoming nodes. On the other hand, an inductive setting can generalize embeddings for completely new graphs of similar type e.g., embedding of Protein-Protein Interaction (PPI) networks will be similar, and as a result, knowledge of generating embedding for PPI networks can be applied to future PPI networks. Previously discussed graph embedding methods lag behind these characteristics. Planetoid covers different settings with the help of random-walk-based methods discussed previously, but its performance is not impressive [110]. Thus, efforts have been made to develop methods that can apply convolutional operators on graphs using preliminary knowledge of the graph while also supporting transductive and inductive set-tings. In the following, we briefly discuss some existing graph convolutional network-based methods.

There are several works on graph neural networks [32,92,29]. Kipf and Welling (2016a) were the first to introduce the idea of Graph Convolutional Network (GCN). They designed spectral convolutional operators to capture structural information from neighbors. They define spectral convolution as a multiplication of a signal x ∈ IR n with a filter g w = diag(w) parameterized by w ∈ IR n in the Fourier domain and further approximate it as the following:
g w × x = UΣU T x ≈ w(I − D − 1 2 AD − 1 2 )x(12)
Here, U is the matrix of the eigenvectors of the normalized graph Laplacian, Σ represents a diagonal matrix of eigenvalues, and I −D −1 2 AD −1 2 is the symmetric normalized form of the graph laplacian [102]. The approximation term is derived by Kipf and Welling (2016a) from [39]. They generalize Equation 12 as the output of a convolution layer in GCN by setting this normalized form toÂ. Their layer-wise propagation function becomes the following:
H l+1 = φ(ÂH l W l )(13)
Here, H l is the activation of l th hidden layer and H 0 = X, where X is the input feature of dimension n × f . The φ is a non-linear activation function in Equation 13 and a popular choice, also used in GCN, are Rectified Linear Units (ReLU) [72].Â is the adjacency matrix of dimension n × n and W l is the weight matrix of the l th layer having dimension f × d. Thus, our final output embedding Z comes from H l+1 whose dimension is n × d. Sometimes, a bias term is added, but to keep it simple we skip any bias in Equation 13. In GCN, authors in particular show results varying across several layers in the network. They define a softmax activation function in the output layer, compute cross-entropy loss, and update weights using a batch gradient descent approach. In the semi-supervised settings, authors train their model using a subset of vertices of the graph and then test performance based on the remaining subset of vertices.

In Equation 13, notice that when we multiply H by W and then byÂ, we sum-up contributions from neighbors. In Fig. 7, we summarize this procedure. When we compute the activation of a layer by Equation 13, we take contributions from neighbors e.g., activation of vertex A takes contributions from its neighbors B, E, and F (see Fig. 7(b)). Similarly, the activation of vertex D considers contributions from its neighbors C, and E (see Fig. 7(c)). Note that the weight of a layer W is shared among all vertices while computing their activations.

GraphSAGE is an extension of GCN that carefully explores some neighborhood aggregation procedures [37]. Authors of GraphSAGE represent the convolutional mechanism in the following way:  of a node and then takes the arithmetic mean (2) Max-pool aggregator -it takes the maximum of all contributions from neighbors, and (3) Long Short-Term Memory (LSTM) -it is a variant of vanilla Recurrent Neural Network (RNN) that is normally useful for sequential or time-series data. Mean and Max-pool aggregators are order-invariant whereas LSTM is order dependent [44]. In GraphSAGE, authors show superior performance over previous methods in both transductive and inductive settings. Graph Attention Networks GAT [103] is another popular work that shows better performance over previous methods. It takes weighted contributions from neighbors in an aggregation function. Recent work by [109] generalizes characteristics of graph neural network and proposes a Graph Isomorphism Network (GIN) that is as powerful as the Weisfeiler-Lehman approach [94] for graph isomorphism testing. Authors claim that an aggregation function should be injective and must not map two different neighborhoods to the same representation. They propose a Multi-Layer Perceptrons (MLP) based convolution mechanism and summation as an aggregation function. Notably, MLP is a universal approximator [45] and thus, decoupled from the summation-based aggregation function, it can capture different graph structures very well. Authors also show the limitations of several aggregation functions used in Graph-SAGE which can be summarized by Fig. 8 (the same color represents the same contribution). Between two graphs in Fig.  8 (a), vertices A and a, both get the same contributions from neighbors using Mean or Max-pooling aggregation functions, though graphs have different structures. Similarly, Max-pooling aggregation fails for the case of Fig. 8 (b) as both A and a will get the same maximum contribution from one of its neighbors. For the same reason, both Mean and Max-pooling fail for the scenario shown in Fig. 8 (c). They suggest that summation is an injective aggregation function that can distinguish such structures among different graphs.
H l+1 N(u) = AGGREGAT E l+1 ({H l v , ∀v ∈ N(u)}) H l+1 u = φ W l .CONCAT (H l u , H l+1 N(u))(14)
Earlier methods of graph convolution networks run more slowly and consume a significant amount of memory even for a very small graph. Thus, researchers have made efforts to improve runtime for training to effectively operate on large graphs. Of course, there does not exist any such method that gains maximum performance, consumes minimum memory, and runs in optimal time. There is always a trade-off between sacrificing one option and gaining better performance in another option. Fast-GCN [17] is such a method that sacrifices performance in terms of accuracy measures but runs significantly faster than previous methods. Sometimes, contributions from neighbors take a long time and consume more memory, in the case of densely connected or scale-free graphs. Thus, traditional batch processing does not help. They assume that the vertices of a graph have an independent and identical sampling distribution and evaluate the activation of a layer through a Monte Carlo approximation [48]. Their batch update procedure runs faster and shows comparable performance. It is common practice to shuffle the mini-batch while training a model using SGD, as it leads to quicker convergence. However, random shuffling of the mini-batch does not help while working with graphs if the sequential ordering of vertices is already grouped based on some properties. Thus, it is a good idea to cluster the graph before mini-batch training. Cluster-GCN [18] is such an approach that clusters the graph first and then takes a subset of vertices as a mini-batch. It intentionally removes all inter-cluster edges from the graph and can train each batch independently. Authors of Cluster-GCN show that their approach converges faster than other methods and can train very large graphs within a reasonable time. Zeng et al. (2019) propose a similar technique where they sample a subgraph from G using the Frontier Sampling approach [88] and then use the subgraph as a mini-batch for training. Authors empirically show that their highly optimized parallel implementation achieves a significant speed-up over GraphSAGE without sacrificing accuracy.

The embedding generation of a graph is basically treated as an encoding scheme to reduce the dimensions to vector space. Thus, researchers also apply an autoencoder [43] based technique to solve this particular problem in this area. There are also some other methods that are based on Recurrent Neural Networks (RNN) [91] or its variants. We mention some of these methods in the following Section 3.4.


### Other Methods

There are some social networks where each edge may have either a positive weight or negative weight, indicating trust or distrust between two users in the network. General-purpose network embedding methods can not solve this problem efficiently. Thus, efforts have been made to analyze signed networks [105,46]. Regular equivalence is a relaxation of structural equivalence where the embeddings of two vertices will be similar if they have neighbors which are also similar. Tu et al. (2018) propose a Deep Recursive Network Embedding (DRNE) method which uses LSTM to solve this problem and predicts different centrality measures 3 very well. Some other methods also use variants of RNN for generating graph embeddings [112] or artificial graph generation [111]. Apart from this, autoencoder based methods have been applied to graph embedding [51,77] or molecular graph (such as SMILES 4 ) reconstruction problems [96,54,20]. We see few methods in the literature that introduce a common framework to generate an embedding for large graphs using a multi-level approach where other unsupervised methods such as DeepWalk or LINE can be used for underlying computations [60,16]. There are some other methods that are developed for a specific purpose or to solve a specific problem. For example, edge2vec 5 has been proposed for three biomedical prediction tasks [30], namely, (i) biomedical entity classification, (ii) compound-gene bioactivity prediction, and (iii) biomedical information retrieval. Authors empirically show that edge2vec outperforms general-purpose graph embedding methods for their heterogeneous biomedical dataset. OhmNet has been proposed for multi-cellular function prediction [120] that uses node2vec as an underlying model. BioENV 6 has been developed recently by combining several general-purpose existing methods to solve biomedical link prediction task [114]. Another recent method is MeSHHead-ing2vec that converts Medical Subject Headings (MeSH) tree structure into a relationship network and applies five existing graph embedding methods to perform several downstream analyses tasks [35]. We provide a shortlist of popular generalpurpose methods for graph embedding in Table 2.


## Experiment Setup


### Experimental Coverage

The primary objective of this survey is to compare and contrast the performance of unsupervised and semi-supervised graph embedding methods via experiments with a diverse collection of graphs. We aim to compare the running time, memory utilization, and scalability of graph embedding algorithms. We also compare the performance of these algorithms in node classification, link prediction, clustering, and visualization tasks. Additionally, we survey the sensitivity of various methods with respect to their parameters. Table 3 shows the experimental converge considered in this survey. By considering aspect of systems, performance with ML tasks, and parameter sensitivity, this survey will help users select methods based on the requirements of their respective applications.


### Computing Platform Coverage

The choice of a computing platform does not influence the performance of algorithms in various ML tasks nor it impacts   Experiment type Experiment details Time and memory running time (CPU and GPU), memory requirement Scalability scaling with processors, scaling with graphs Classification node classification (multiclass and multilabel), link prediction Interpretation clustering, visualization Algorithm-specific parameters number of layers, random walk length Algorithm-independent parameters embedding dimension, sampling, convergence   


### Coverage of Methods and Setup

Over the last few years, researchers have developed hundreds of new methods for graph representation learning. A representative subset of available methods is shows in Table 2 where we selected methods from different categories (Fig. 3). For each method, we report its computational complexity and a link to the publicly available software. For comparative experiments, we select a subset of methods based on their popularity and citations. More specifically, we choose RolX from the feature engineering based methods, HOPE from the matrix factorization based methods, a set of methods {DeepWalk, LINE, struc2vec, VERSE, Force2Vec} from shallow embedding methods, and a set of methods {GCN, GraphSAGE, FastGCN, GAT, Cluster-GCN, GraphSAINT} from the GNN methods.

We briefly discuss each of these methods below with their hyper-parameter set-up. We use default values for other parameters when not explicitly mentioned.

• RolX 7 [40]: We set all parameters to the same as described in the paper. Specifically, we set 5 as the number of binarization bins, 32 as the batch size, 0.9 as the pruning cutoff, 128 as the dimension of the output embedding, and 3 as the depth of recursion.

• HOPE 8 [33]: The original version is implemented in MATLAB. However, a more generalized and Python implementation is available in the GEM [33] package. We set β to 0.01, as shown in Equation 7. We also set the output embedding dimension to 128 to match with other methods.

We explicitly mention whenever we use different parameters other than these.

• DeepWalk 9 [79]: This is a pioneering work for the graph embedding problem which generates random walks for each vertex in the graph and then formulates the problem in a similar way as word2vec. In fact, DeepWalk uses the word2vec framework directly in its programming model. • struc2vec 11 [89]: This method generates an embedding based on the structural equivalence of the graph. It uses a hierarchy to measure vertex similarity and constructs a multilayer graph. In the following step, it generates random walks from the multilayer graph and generates an embedding using a similar optimization model to DeepWalk.

To conduct experiments using struc2vec, we set the number of walks to 20, walk length to 80, window size to 5, size of embedding dimension to 128, layers to 6 and use all options available for running time optimization.

• VERSE 12 [100]: This method embeds graphs based on several similarity measures of vertices. The full version of VERSE has higher running time and memory consumption. Thus, authors also provide a negative sampling based technique which has comparatively lower running time and memory consumption. To run VERSE, we set several of its hyper-parameters as follows: size of dimension = 128, number of negative samples = 5 and number of threads = 48.

• Force2Vec 13 [82,84]: This method embeds graphs forcedirected graph layout method. This method use negative sampling and effective parallelization techniques that significantly boost-up the runtime performance. There are three versions of the algorithms where either sigmoid or t-distribution can be used as a similarity function. To run Force2Vec, we set several of its hyper-parameters as follows: size of dimension = 128, number of negative samples = 5 and number of threads = 48.

• GCN 14 [50], GraphSAGE 15 [37] and FastGCN 16 [17]: For all experimental datasets, we use 1000 vertices for testing, 500 vertices for validation and the rest of the vertices for training. To increase the usability of GraphSAGE, authors implemented this method in Tensorflow and Py-Torch. For all of our experiments, we report results based on the PyTorch implementation of GraphSAGE using the Mean aggregation function.

• GAT [103], ClusterGCN [18] and GraphSAINT [116]: For all experimental datasets, we use 1000 vertices for testing, 500 vertices for validation and the rest of the vertices for training. We use the implementations available in PyG framework.


### Datasets Coverage

To conduct experiments, we choose both homogeneous and heterogeneous benchmark networks that are commonly used to assess the performance of state-of-the-art methods. We briefly discuss the properties of the datasets as follows.


#### Homogeneous Networks

We define a network to be homogeneous when each node and each edge is assigned to a single label. Thus, this type of networks contains only structural information which is more local to a vertex or an edge. In the following, we describe three such homogeneous networks which have been widely used for benchmarking in the literature [62,103,52,73].

• Cora: This dataset consists of 2708 machine learning papers which are classified into one of following seven topic classes: Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, and Theory. This set of papers is selected in such a way that they have at least one one-way mutual citation. There are 5429 such citations and we term these as edges in the graph.

• Citeseer: This dataset consists of 3312 scientific papers that are classified into one of the following six classes: Agents, Artificial Intelligence, Database, Human Computer Interaction, Machine Learning, and Information Retrieval. This set of papers is selected in a similar way to Cora. After processing the dataset, there are 4732 citations which we term as edges in the network.

• Pubmed: This is another widely used diabetes dataset which has 19717 scientific papers classified into one of three diabetes type: Diabetes Mellitus Experimental, Diabetes Mellitus Type 1, Diagebes Mellitus Type 2. There are 44338 edges in the graph which represent links or citations between a pair of papers. We process this dataset to make a consistent format with the above two networks.

Software implementations of different methods often process graphs in an edgelist, CSV [25], or CSR [100] format. We preprocess the aforementioned networks in different formats required by various software. As each vertex in a homogeneous network has one label, the classification problem solved on such networks is a multi-class classification problem. All these networks are available on the UCSC 17 website.


#### Heterogeneous Networks

We define a network to be heterogeneous when its vertices can have more than one label. The vertices are likely to contain global information about the network. Predictions in such network are harder than in homogeneous networks. According to Cai et al. (2018), the main sources of such networks are community based questions answering sites [26], multimedia networks [15,118] and knowledge graphs [10]. In the following, we describe two such heterogeneous networks which have been used for benchmarking performance in the literature [79,34,99]. • Youtube: This is a social network dataset curated from the popular video sharing website Youtube 19 . Each vertex in the network represents a user and each edge represents friendship between two users. According to Perozzi et al. (2014), the labels of vertices indicate groups of viewers who enjoy common type of videos. This is a relatively large dataset which can be used for scalability purpose. It has 1138499 vertices, 2990443 edges and 47 labels.

We also preprocess these datasets so that we can run all methods in their respective input formats. As each vertex in a heterogeneous network can have more than one label, the classification problem solved on such networks is a multi-label classification problem. All these datasets are available on the ASU 20 website. Besides these, we have also used three airtraffic datasets (namely, brazilian air-traffic, european air-traffic and US air-traffice) introduced in struc2vec which are mainly used for structural equivalence discovery. Properties of these datasets are given in Table 6. Brazilian  131  1038  5  European  399  5995  5  USA  1190  13599  8   Table 6: Air-traffic datasets of three regional airports which are mostly used for structural identity.


## Air-traffic # vertices # edges Diameter


### Performance Metrics

A key focus of this survey is to understand the practical performance such as runtime, memory utilization, and scalability of various graph embedding and GNN methods on different graphs and hardware platforms. We also measure the accuracy of different methods when performing various machine learning tasks. We use F1-micro and F1-macro scores for the multilabel classification task, accuracy for the binary prediction task, and the modularity score to compare the results of clustering. We give a brief description of these measures below:

• Accuracy: When the predicted class is positive (negative) and the ground truth class is also positive (negative), it is termed as a true positive (negative). But when the predicted class is positive (negative) whereas ground truth class is negative (positive) then we term this as a false positive (negative). We represent an absolute number of true positive, false positive, true negative and false negative as TP, FP, TN and FN, respectively. In any type of binary classification or link prediction, we define accuracy as the following:
Accuracy = T P + T N T P + T N + FP + FN(15)
• F1-micro: For the multi-class classification task, the F1micro score aggregates the contributions of all classes to calculate the average value of final F1 score. In terms of precision-recall we can define this for a set of classes C as follows:
P = c∈C T P c c∈C (T P c + FP c ) , R = c∈C T P c c∈C (T P c + FN c ) , F1-micro = 2 * P * R P + R(16)
• F1-macro: Unlike the F1-micro score, the F1-macro score will compute the score independently for each class and finally take their average F1 score. In terms of 20 http://socialcomputing.asu.edu/pages/datasets precision-recall, we can define this for a set of classes C as follows:
P = T P (T P + FP) , R = T P (T P + FN) , F1-macro = 1 |C| c∈C 2 * P c * R c P c + R c(17)
• Modularity: The modularity score is a well-known measure to evaluate the effectiveness of any graph clustering technique. It computes the fraction of the edges that are within a given cluster minus the expected fraction, if edges are distributed randomly [74]. We can compute this score by using following Equation:
1 2m i j A i j − k i k j 2m δ(c i , c j )(18)
Here, A is the adjacency matrix of the graph, m is the number of edges, k i is the degree of the vertex v i . The membership of vertex v i belonging to a cluster is represented by c i , and δ(c i , c j ) = 1, if i and j are in the same cluster; otherwise, δ(c i , c j ) = 0.


## Runtime and Scalability

We measure the average time of several runs under the same experimental setup for all methods in the corresponding server machine. Since no other programs run on those servers, we get almost consistent time for all runs. Thus, the standard deviation of runtimes is insignificant that has been ignored in the rest of our discussion.


### Training Time for Unsupervised Embedding Algorithms

To conduct experiments for runtime, we run each of the methods with their parameters discussed in Section 4.3. If a method has a parallel implementation, we use the maximum number of cores available in each processor. Table 7 reports the runtime (in seconds) of different graph-embedding algorithms on the Intel Skylake server for both homogeneous and heterogeneous networks. We observe that the Force2Vec algorithm runs the fastest for most of the graphs. This runtime is expected for Force2Vec due to the simplicity in its underlying model, efficient shared memory-based parallelism, and effective memory utilization. One reason behind the superior performance of Force2Vec on the Intel processor is Force2Vec's ability to utilize Single Instruction Multiple Data (SIMD) vectorization units. The RolX method shows better results for the small graph Cora; however, it shows relatively high runtime for large graphs. We also observe that popular DeepWalk and LINE methods may take several hours for some of our largescale graphs. Their high running times are expected from their higher computational complexity as shown in Table 2. We can see in Table 7 that struc2vec takes higher runtime than other methods due to its higher computational complexity. It creates a multi-layer graph from the original graph which increases its   runtime significantly. We could not run this method on Intel server for the Youtube graph due to the higher memory consumption. Table 8 reports the runtime of all methods on the ARM server. Since the ARM ThunderX processor has 64GB memory (as opposed to 256GB in the Intel server), all methods failed to process the Youtube graph on the ARM server. The relative performance of different methods on the ARM processor is similar to that of the Intel processor (Table 7), except for the Blogcatalog graph where VERSE runs faster than Force2Vec. The average degree of nodes in Blogcatalog is comparatively high where VERSE's sampling strategy has significant benefits, and Force2Vec does not support SIMD vectorization with load-balancing for the ARM processor. These two reasons may have contributed to the observed performance for Blogcatalog.

In Table 9, we report the runtime of different methods using the AMD Epyc server. When running smaller graphs such as Cora and Citeseer on the 64-core AMD processor, parallel methods such as Force2Vec and VERSE do not have enough work to keep all cores busy. Consequently, the overheads of thread creation and scheduling overshadow the benefit of parallel executions, making HOPE the fastest method for small graph on the AMD processor. By contrast, VERSE and Force2Vec outperform other methods for large graphs such Pubmed and Youtube.

Summary: For small graphs, all graph embedding methods run in a reasonable time on all processors considered. However, highly parallel algorithms such as Force2Vec and multilevel methods such as HARP are needed to generate embeddings of large graphs such as Pubmed, Flickr7, and Youtube. Computationally expensive methods such as RolX and struc2vec are impractical for large graphs (for example, struc2vec takes more than two weeks to generate the embedding for the Youtube graph on the AMD processor).


### Scalability of Unsupervised Embedding Algorithms

We analyze three types of scalability, namely, (i) strong scaling, where the number of threads are increased for a fixed input graph, (ii) weak scaling, where the size of the graph and the number of threads are increased while keeping the work per thread constant, and (iii) graph scaling where the size of the graph is increased for a fixed number of threads. Strong scaling and weak scaling studies are commonly used by highperformance researchers to demonstrate the performance of algorithms with increasing computing resources. For our analyses, we use a real-world graph and several synthetic graphs. Some of the unsupervised methods such as DeepWalk, VERSE, LINE, HARP, and Force2Vec have parallel implementations for multicore processors. Thus, we pick these methods for the scalability study. We show strong scaling, weak scaling, and graph scaling results in Figs. 9 (a), 9 (b), and 9 (c), respectively. Fig. 9 (a) shows the strong scaling results for the Flickr dataset where the x-axis denotes different number of threads and the y-axis denotes the runtime in log-scale. We see that the runtime of Force2Vec and VERSE decrease almost linearly with the increase of the number of threads. The better scalability of Force2Vec and VERSE make them suitable to run experiments on modern multicore processors. By contrast, HARP  Table 9: Runtime (in seconds) of different unsupervised methods for different benchmark graphs on the AMD server.All methods use the same computing resources. For each graph, the fastest runtime is shown in a green shade. '×' indicates that the method failed to run or it went out of available memory. and DeepWalk exhibit limited scalability as they do not run significantly faster even when they use 32 threads.

To conduct a weak scaling experiment, we create five synthetic networks using a benchmark dataset generator by Lancichinetti et al. (2008). The number of vertices in these five networks are 2K, 4K, 8K, 16K and 32K, respectively. Then, we run all methods using 2, 4, 8, 16, and 32 threads for graphs having 2K, 4K, 8K, 16K, and 32K vertices, respectively. In an idead weak scaling scenario, the runtime should remain the same for different numbers of threads as the problem size assigned per thread does not increase. In Fig. 9 (b), we observe that LINE, VERSE, and Force2Vec keep the runtime stable, whereas Deep-Walk shows poor weak scaling performance.

In Fig. 9 (c), we show the graph scaling results for Force2Vec, VERSE and DeepWalk. We ran all methods for a fixed 48 threads using different graphs. We increase the number of nodes by a factor of 2 starting from a graph having 2K nodes generated similar to weak scaling Lancichinetti et al. (2008). All these methods show similar graph scaling results which is almost linear in the log-scale.

Summary. Our experiments suggest that Force2Vec, LINE and VERSE show desirable strong and weak scaling behavior. Hence, these methods can exploit parallel processors to run faster (relative to their sequential runtime) and process largescale graphs quickly.


### Training time for GNN methods

In the recent years, GNNs emerge as a popular option for graph representation learning, especially when nodes are partially labeled to facilitate semi-supervised predictions. In this section, we analyze the training time of some popular GNN methods. We measure the training time of GCN, GraphSAGE and FastGCN using their original source codes which are publicly available (Category 1). To measure the runtime of GAT, ClusterGCN and GraphSAINT, we use source codes available in the PyTorch Geometric (PyG) framework (Category 2) [27]. We ran all these methods on the Intel Skylake server and reported their training time in Table 10. We observe that GCN is the fastest GNN method for smaller graphs, but FastGCN runs faster than other methods for bigger graphs. The benefit of FastGCN stems from a sampling approach that is used to accelerate the training process while keeping the accuracy competitive to GCN and GraphSAGE. ClusterGCN and GraphSAINT apply a clustering technique and a sampling technique, respectively, as a pre-processing step before starting the training procedure. These expensive reprocessing steps make ClusterGCN and GraphSAINT slower than some of their peers.

Summary. GCN and FastGCN are generally the fastest GNN methods. They have comparatively simpler computational overheads compared to other GNN methods.


### Impact of Graph Learning Frameworks

Over the last few years, several frameworks have been develop to simplify the development and testing of GNN and graph embedding methods. Among them, PyG [27] and Deep Graph Library (DGL) [104] have become popular. PyG is built on top of PyTorch whereas DGL supports PyTorch, MXNet, and Tensorflow backends. Here, we analyze the runtime performance of PyG and DGL frameworks using three benchmark datasets. For PyG and DGL experiments, we use GNN models with identical hyper-parameters. Table 11 shows the training and validation time of two popular GNN models, GAT and GCN, using the PyG and DGL frameworks. We ran the same experiments on an Intel CPU and   Table 11: Training and validation time (in seconds) of different semi-supervised methods in PyG and DGL frameworks with respect to GPU and CPU. We run all methods using same computing resources and experimental setup. Better runtime of each category is shown in bold face for each dataset.

an NVIDIA GPU (see Table 5). We observed that PyG runs GNN models faster than DGL on our GPU. However, there is no clear winner when their performances are compared on CPUs. For example, GCN implemented with DGL ran faster than its implementation with PyG. By contrast, PyG's CPU implementation of GAT runs faster for most graphs considered in this paper. Both of these frameworks are under active development and their performance will continue to improve over time.

Summary. At the time of this survey, PyG ran faster than DGL on our GPU, but their CPU performances were mixed. Since the performance gap is not significant, users can use wither frameworks based on their preferences.


### Memory Consumption Analysis

Memory requirement is an important property of graph embedding and GNN algorithms, and it often determines whether a method can successfully run on various GPUs and CPUs. We analyze memory consumption of a method using a Python package called memory-profiler and report the measurement in Megabytes in Fig. 10. Among unsupervised graph embedding algorithms in Fig. 10 (a), VERSE consumes the least memory. The memory profile of Force2Vec is also similar to VERSE. Both of these methods only allocate necessary data structures for the input sparse graph and output embedding. Random-walk based methods, on the other hand, consume more memory because they need to store a set of random walks generated in the preprocessing step. For example, DeepWalk samples a set of random walks for each vertex and stores those in arrays before passing them to the word2vec model. DeepWalk stores random walks in disk when the generated walks do not fit in memory, which can be significantly more expensive. For the Pubmed graph in Fig. 10 (a), VERSE and Force2Vec consume around 12MB and 14.63MB of memory, respectively whereas struc2vec, LINE and DeepWalk consume 240MB, 560MB and 1182MB, respectively. Similarly, for Flickr, VERSE consumes around 50MB of memory whereas DeepWalk consumes more than 4GB. In Fig. 10 (b), we show the memory consumption by HOPE and RolX which are chosen as the representative methods from the matrix factorization and feature engineering based methods, respectively. We see that HOPE consumes more than 17GB of memory for the Pubmed dataset which has only 19K vertices. The high memory requirements of matrix factorization methods arise from their use of dense decomposition techniques (e.g., singular value decomposition). Fig. 10 reports the memory consumption of various GNN methods. We observe that GNNs consume a significant amount of memory even for small graphs such as Cora. Most GNNs store weight matrices, biases and hidden representations in all intermediate layers, which resulted in their high memory demands. In particular, GraphSAGE consumes more memory for the Pubmed dataset than GCN and FastGCN because Graph-SAGE concatenates hidden vector in successive layers whereas other methods generate fixed length hidden vector for each node.

Summary. Among all graph embedding methods, VERSE and Force2Vec are more memory efficient than others. GNN methods typically require more memory than unsupervised graph embedding methods.


### Applications


#### Node Classification

Node classification is one of the most celebrated applications of graph representation learning. In this task, labels of some vertices in the test set are predicted based on the structure of the graph and other information available for training data [33]. For a homogeneous graph where each vertex belongs to a unique class, node classification is formulated as a multiclass classification problem. By contrast, node classification on heterogeneous graphs where a vertex could belong to more than one class is formulated as a multi-label classification task. In both cases, the label of a vertex can be inferred by using its embedding. For the unsupervised methods, node embeddings are generated based on the graph structure and node features. A classifier such as a logistic regression model is trained using a fraction of vertices with known labels and then the trained classifier is used to predict labels of other nodes whose labels are not known. For semi-supervised methods such as GNNs, the classification problem is incorporated in the loss function and solved in an end-to-end training of GNNs.

Unsupervised Methods. We report the results of F1-micro scores for six different datasets in Fig 11, where the x-axis denotes the percentage of vertices used to train the logistic regression model. The remaining vertices are used for testing when computing the F1-micro scores shown on the y-axis in Fig 11. Generally, the increase of training data also boosts the F1-micro score for all methods. For a given training-testing split, no single method performs the best for all the datasets. Force2Vec performs better for Cora, Citeseer, and Pubmed, whereas Deep-Walk performs better for Flickr, Blogcatalog and Youtube. Due to the high memory requirement, we could not run HOPE on larger graphs. We also report F1-macro scores for different datasets in Figs. 12 (a), (b), and (c) which show results similar to Figs. 11 (a), (b), and (c).

Graphs shown in Figs. 11 and 12 exhibit strong homophilic relations (nodes with same labels are more likely to be connected by edges). It is well understood that most graph embedding and GNN methods perform reasonably well on homophilic graphs, but do not perform well for heterophilic graphs. There are a few methods such as struc2vec that are especially designed for this type of graphs. struc2vec discovers structural equivalence instead of node proximity and thus performs well for heterophilic graphs. We collect three airport datasets from [89] to conduct experiments on structural equivalence under the same settings used in previous classification experiments and report the results in Fig. 13. We observe that struc2vec indeed outperforms DeepWalk and VERSE in the node classification task for all these airport datasets.

Graph Neural Networks. Fig. 14 shows the classification accuracy (F1-micro scores) for three GNN methods using Cora and Pubmed datasets. For this experiment, we divide all vertices in a graph into three sets: (1) training set, (2) validation set, and (3) testing set. For both graphs, we choose 1000 random vertices without replacement in the testing set, 500 random vertices without replacement in the validation dataset, and  20 vertices per-class in the training set. In Fig. 14 Fig. 14 (b), we report the results for the Pubmed dataset. We oberserve that GraphSAGE performs poorly in both training and validation cases. FastGCN achieves a high F1-micro score in the training set, as usual, whereas it shows poor performance in the validation set but not as bad as GraphSAGE. GCN performs better in the validation set. In test cases of the Pubmed dataset, GCN, GraphSAGE and FastGCN achieve F1micro scores of 0.78, 0.44 and 0.72, respectively. Notice that GraphSAGE or GCN achieves higher or similar accuracy compared to other types of methods for the Cora dataset, but using the same experimental setting, they could not reach the same level of accuracy as DeepWalk or Force2Vec for the Pubmed dataset.

We also conduct experiments using the GAT, ClusterGCN and GraphSAINT methods available in the PyG framework. We report the resutls in Figs. 15 (a), and (b) for Cora and Pubmed datasets, respectively. We observe that the ClusterGCN and GraphSAINT method show better training accuracy over GAT method; however, their validation accuracy is slightly less than the GAT method for Cora dataset. For the Pubmed dataset, ClusterGCN shows better validation curve than other methods.

Nowadays, graph neural network has become a popular strategy for the graph mining. In the recent years, GNN methods are mainly proposed for node classification or graph classification tasks; however, other graph learning tasks such as link prediction, clustering, and visualization have been heavily studied using the unsupervised methods. Thus, without loss of generality, we focus on discussing unsupervised methods for these tasks in the rest of the experimental analyses.  


#### Link Predictions

Link prediction itself is a broadened area of research in social network analysis [4]. It has many applications in social and biological networks, such as the recommendation of new friends in Facebook [97] and drug-disease association prediction in biomedical networks [114]. For link prediction, a good embedding method should capture local information from the network and preserve it in the embedding space. Thus, we can easily apply graph embedding to predict possible links in a graph. We show an example in Fig. 16 (a), where blue colored edges represent existing links in the graph when the embedding is generated and dotted grey lines with question marks indicate links to be predicted. For the sake of simplicity, we can assume that there is a high probability to have a link between one vertex to another if one's neighbors are connected to another vertex. In Fig. 16 (b), we show predicted links with green colored lines. For such link prediction tasks, we create a dataset from the embedding where positive samples are constructed from pairs of vertices with existing edges in the graph and negative samples are constructed from pairs of vertices having no edges. Thus, it becomes a binary classification problem which is mostly tackled by logistic regression [34,100]. When we take a pair of vertices, we can choose the Hadamard [34] vector operator to construct a new vector from embeddings of two vertices. This has been found effective in the literature, whereas some other vector operators exist, such as weighted L1, weighted L2, average 21 , etc. Finally, we choose 50% of the edges to train the model and the rest of the edges is used for testing. 21 Several studies have found that average operator does not perform well compared to other operators [34,100].


## Operator

Notation Definition Hadamard We show notational definitions of three vector operators in Table 12. All these operators have been used previously for link prediction tasks [34]. We show the experimental results of unsupervised methods on this task in Figs. 17 (a), 17 (b), and 17 (c). As link prediction is a binary classification problem, we only report accuracy. We can see while using the Hadamard operator, Force2Vec and VERSE show competitive performance and outperform other methods. They achieve almost 99% accuracy in all datasets. For weighted L1 and L2 operators, DeepWalk performs better than other methods. For all cases, struc2vec is the worst performer as this tool is mainly designed to capture structural equivalence in the network. We also conducted experiments for link prediction task using HOPE and RolX. We observe that HOPE achieves accuracy of 79.8% and 80% for the Cora and the Pubmed datasets, respectively, using the Hadamard vector operator. On the other hand, RolX achieves accuracy of 79.7% and 74.8% for the Cora and the Pubmed datasets, respectively, using the Hadamard vector operator. These values of accuracy are lower than that of Force2Vec, VERSE and DeepWalk.
z i z j = z i * z j Weighted-L1 . 1 z i .z j 1 = |z i − z j | Weighted-L2 . 2 z i .z j 2 = |z i − z j | 2

#### Clustering

The clustering of vertices is an important task in graph mining where common/similar vertices tend to form a cluster. High quality embedding can be helpful to detect a community in large scale social networks. A good graph clustering has a higher number of intra-cluster edges and a lower number of inter-cluster edges. Generally, the Louvain algorithm [9] is widely used to find clusters in a graph which focuses on maximizing modularity. However, we can not apply it on embedding as we do not have any structural information about the graph. Instead, we apply the k-means 22 algorithm which can effectively detect clusters in the embedding space of the graph. The common practice is to set a value for k in a range and find the clusters that show the highest modularity score [100]. We report modularity scores of Force2Vec, DeepWalk, VERSE, HARP and LINE across different datasets in Fig. 18 (a). We consider Louvain algorithm as the baseline method. It has the modularity scores of 0.81, 0.88, 0.73, and 0.49 for Cora, Citeseer, Pubmed and Flickr datasets, respectively. We observe that Force2Vec achieves higher modulartiy scores for Cora, Citeseer, and Pubmed datasets. DeepWalk achieves higher modularity scores than other methods for Flickr dataset and VERSE shows better modularity score for the Youtube dataset. Notably these results are very competitive to the baseline method. For large graphs, such as Flickr and Youtube, the modularity scores of Force2Vec, VERSE and DeepWalk are comparative. As the LINE from authors' repository shows poor performance as usual, we run another implementation 23 . This version of LINE shows better performance compared to the previous version. In particular, this version of LINE achieves the modularity scores of 0.61, 0.51, and 0.51 for the Cora, Citeseer, and Pubmed datasets, respectively. Note that we do not show the results of other methods such as struc2vec or HOPE due to their poor performance in graph clustering task using the k-means algorithm.


#### Graph Reconstruction

The graph reconstruction task is related to the construction of a graph from the generated embedding such that it can create 23 https://github.com/shenweichen/GraphEmbedding edges as of the original graph with high accuracy. If a method generates a high quality embedding, then it can accurately reconstruct the original graph. However, it requires n(n−1) 2 comparisons to find the nearest neighbors for a graph of n vertices which is highly expensive for large graphs. Thus, we select 1000 vertices randomly to reconstruct a sub-graph and check the accuracy of finding neighborhoods. Generally, the degree of each vertex is used to find the nearest neighborhoods and these are compared with the neighbors of that vertex in the original graph. We compute the accuracy to explore how many vertices can accurately capture their neighborhoods based on their consine similarity 24 scores.

We report the results of the graph reconstruction accuracy in Fig. 18 (b). We observe that both Force2Vec and VERSE show similar performance on the graph re-construction task. Specifically, Force2Vec (sigmoid) achieves a higher reconstruction accuracy than DeepWalk on the Citeseer and Pubmed datasets. LINE and HARP, on the other hand, show very poor reconstruction ability on the Pubmed dataset. Notice that this task has an inherent relation with link prediction. Here, we find the top-most degree number of neighborhoods based on their cosine similarity and the methods that perform well for the link prediction task, as reported in Section 5.6.2, also show competitive performance for graph reconstruction task.


### Visualization

Graph visualization is an enriched research field which has diversified applications in data mining and bioinformatics [21]. The common practice of graph visualization is to generate two dimensional coordinates for each vertex of the graph and then plot in the Cartesian coordinate system connecting all the edges. A plethora of studies in graph drawing community have focused on different types of network visualization [108,42,85]. However, when we generate an embedding, we basically create a d-dimensional projection of a graph where graph visualization methods are not directly applicable. Thus, generic data visualization technique such as t-SNE [64] or UMAP [66] are used to visualize the graphs in 2D using its embedding. This type of visualization can help to identify clusters or latent characteristics of the graph. To conduct this experiment, we use t-SNE by setting several of its hyper-parameters to default values and generate two dimensional coordinates for visualization in the Cartesian space. In Fig. 19, we show two dimensional visualizations for the Cora dataset applying t-SNE on 128 dimensional embeddings generated by different methods. We color the vertices of distinct class with distinct colors i.e., vertices of the same class have the same color. We observe that the visualization of the Force2Vec method preservers more local and global clustering structure. The DeepWalk method, on the other hand, also shows local clustering information but some vertices from different classes form cluster in the left side. The VERSE method also shows local clustering, but they are high in numbers and have scattered placement. HARP method shows similar visualization to the VERSE method. We also show two dimensional visualizations for the Pubmed dataset in Fig. 20. We observe similar results that the Force2Vec and DeepWalk methods show better visualizations than other methods. Force2Vec (t-distribution) can gain more information to separate clusters and has better local and global clustering information.


### Parameter Sensitivity


#### Effect of Dimensions

Some previous studies have shown that the performance on the prediction task may vary if we choose different values for hyper-parameters [79,34,100,80]. For example, after reaching a certain value for dimensionality, the accuracy of prediction starts to drop when we increase it further. Most of the previous studies suggest using dimensional embedding. To summarize the results, we conduct experiments varying the dimensions of the output embedding for some shallow network-based methods. We set different parameters as described in Section 4.3 and take 20% of the dataset to train the logistic regression model while the rest of the samples in the dataset are used for the classification. We report the results of the F1-micro scores for the Pubmed dataset in Figs. 21 (a). We observe that Force2Vec, DeepWalk, and HARP perform better than other methods for various dimensional embedding. We also notice that, for lower dimensions, the F1-micro scores are not that much less compared to higher dimensions. In fact, the VERSE tool shows better performance for 16-dimensional embedding for the Pubmed dataset. RolX shows high sensitivity for different dimensions. It shows the lowest performance for 16-dimensional embedding. Then, with the increase of dimension, the F1-micro score also increases until 128-dimension. Then, it falls a little for 256-dimensional embedding. The LINE method shows similar sensitivity to the VERSE method though its F1-micro scores are lower than the VERSE.


#### Effect of Various Negative Samples

Noise-contrastive estimation [36] is a popular technique used by most of the shallow graph embedding models [100,117,90]. Using this technique, a subset of vertices are randomly selected from a uniform distribution as negative samples which are used alongside positive samples (i.e., k-hop neighbors) to optimize the objective function. However, randomly generated negative samples may hurt the optimization function due to the selection of false negative samples and this become vital if the number of negative samples is very large. Armandpour et al. (2019) have made efforts to analyze this issue theoretically and proposed robust negative sampling techniques for graph embedding problem. NSCaching [119] is another interesting work that generates efficient negative samples for knowledge graphs. In Fig. 21 (b), we empirically show the effect on performance measures varying the number of negative samples. To conduct experiment, we choose Force2Vec, VERSE and LINE methods as they support random negative sampling approach. We take 25% of the vertices in the training set and report the F1-micro score for the rest of the testing dataset. We observe that performance score drops when we use more than 5 negative samples for the Pubmed dataset and continues to decrease for more negative samples. VERSE method is more sensitive to higher negative samples than others as its performance score significantly drops after using more than 15 negative samples for the Pubmed dataset. Note that the size of this dataset is relatively small as it has only around 19K vertices. For smaller graphs than Pubmed, the number of randomly selected negative samples will show more sensitivity than bigger graphs than the Pubmed dataset. The reason is that for bigger graphs, the probability of selecting false negative is lower compared to the smaller graphs. Thus, care must be taken to choose an effective number of negative samples rather than random selection. 


#### Effect of the Number of Walks and Walk-length

The random-walk based method such as DeepWalk, shows sensitivity on the number of walks as well as the length of the walk. We analyze these aspects using DeepWalk and report the results on Figs. 22 (a) and (b), respectively for Pubmed and Blogcatalog graphs. In Fig. 22 (a), we observe that DeepWalk shows less sensitivity for the number of walks on the Pubmed dataset for a fixed walk-length of 40 since this is a homogeneous network. However, for the heterogenous network Blogcatalog, it shows reasonable sensitivity i.e., F1-micro score increases with the increase of the number of walks. We also see similar sensitivity for different walk-length for a fixed 10 number of walks on Fig. 22 i.e., Blogcatalog shows higher sensitivity for different length of walks since this is a heterogeneous network.


## Discussions and Limitations

Graph representation learning is a challenging problem as it is hard to capture different latent characteristics of the original graph in the embedding space. If the output embedding can not capture the intrinsic properties well, then it performs poorly on several prediction tasks. A single tool can not generalize the embedding well for all prediction tasks. For example, struc2vec performs well for structural equivalence prediction whereas DeepWalk performs well for homophily prediction. In addition, we have shown in our experimental analyses that some methods may consume a high amount of memory but run fast, whereas some methods may take less memory but run slow. Some methods can generate a high quality embedding but have higher runtime and consume more memory. We have summarized the results of some embedding methods except GNN models in Table 13, based on several characteristics. We have evaluated all of the tested methods in this survey scoring from L, M, H, and VH denoting Low, Medium, High, and Very High values, respectively. Note that we want low values (L) for memory consumption, running time and number of hyper-parameters. On the other hand, we want very high values (VH) representing a better score for scalability, robustness and performance. We put 'M/H' for struc2vec in performance as it performs well for networks having structural equivalence properties, however shows moderate performance for networks having homophily characteristics. H* represents the variable running time for the LINE tool as it has a high running time for small networks but takes comparatively less time for large networks with respect to other methods. From our analyses, we can presume that an optimal embedding tool will have less runtime and memory consumption cost; furthermore, it will generate a high quality embedding that will show superior performance in various prediction tasks. In reality, there is always a trade-off between our expectation and available resources. From our analyses, we observe that the existing methods face the following challenges and limitations.

• Runtime, Memory Cost and Scalability: Runtime and memory requirement are two bottlenecks of random-walk based embedding methods, though they show superior performance, e.g., DeepWalk achieves superior performance for most of the prediction tasks, though it consumes significant amount of memory and runs very slow. Besides these, most of the real-world social networks are dynamic  Table 13: Summary of graph embedding methods except GNN methods based on different characteristics. We score each tool from L, M, H, and VH which represent Low, Medium, High, and Very High, respectively. Note that L represents better score for memory consumption, running time and number of hyperparameters whereas VH represents better score for scalability, robustness and performance.

in nature i.e., they are changing (in size) rapidly. For example., Facebook has around 2.5 billion users and this number is increasing day by day. This large graph makes matrix factorization methods obsolete. Thus, scalability is an import issue for future research in developing graph embedding methods.

• Bottleneck in Optimization: Some embedding methods support multi-core implementations in shared memory architecture and employ asynchronous SGD to optimize the objective function. It makes the output embedding nondeterministic which is not expected. It also incurs the false sharing problem, which is an unexpected phenomenon in shared memory programming due to poor optimization [11]. Thus, efforts can be made for new embedding methods so that the optimization process becomes deterministic and free of unexpected bottlenecks [83].

• Visualization: The current methods embed graphs in vector space. Thus, they need another dimensionality reduction tool (e.g., t-SNE or UMAP) to be embedded in euclidean space for visualization. We have seen that the embedding of shallow networks or matrix factorization based methods can be used for several prediction tasks. A future direction can be to explore whether we can generate an embedding which can be directly used for visualization. Another interesting idea for dynamic graph visualization might be to use graph convolutional neural networks. The current visualization techniques mostly visualize static graph. If new vertices or edges arrive in the network, then we need to run the visualization method again. So, if we can learn weights for a graph using an inductive approach, as of graph convolutional neural networks, we can easily generate coordinates for new nodes which will facilitate visualization of dynamic graphs.

• Interpretability of Representation: Considering the irregular connections and structures of graphs, it is often found hard to generalize a model so that it can characterize different types of graphs well. Most of the methods assume that neighboring vertices will have a similar representation. This strategy works well for link prediction or even multi-class classification problem. If we go beyond that e.g., multi-label classification, random-walk or graph convolutional networks based methods become viable approaches. However, there are a few works on the interpretability aspects of graph embedding. In this direction, there is scope to explore more on the applicability of different complex network motifs and their theoretical aspects.

In addition to the above challenges, the selection of effective negative samples from a graph can be a future work. We have seen that false negative in randomly selected negative sampling approach plays an important role in optimization. Thus, special care must be taken while selecting negative samples from a graph. An illustrative theoretical work can give new insight about effective negative sampling in graph embedding domain.


## Conclusions

In this paper, we have formally discuss the graph representation learning problem and several existing methodologies to solve it effectively. We have shown a new taxonomy of existing methods that are basically deduced from its evolution over time. We have also discussed some benchmark datasets that are commonly used to compare experimental results. We have conducted an extensive set of experiments to show comparative results of different methods for different prediction tasks such as link prediction, node classification, graph reconstruction, and clustering. Our empirical results show that some embedding methods can achieve better accuracy in prediction tasks but they may have high runtime and memory cost whereas some methods can perform reasonably well with moderate runtime and memory. Thus this paper provides a guideline to choose an embedding method focusing on a trade-off between better performance and computing resources. We have analyzed the sensitivity on hyper-parameters, e.g., a higher value of embedding dimension and the number of negative samples may not be important for making effective predictions. Finally, we have provided a list of challenges that can be considered as future directions. We hope that this paper shows a new insight into the evolution of graph embedding methods and their comparative analysis which will help future researchers to incorporate some challenging issues in the field.


feature-engineering based, (b) matrix-factorization based, (c) random-walk-based embedding, (d) GNNs Hardware platforms (a) Intel, AMD, and ARM CPUs (b) NVIDIA GPUs Software platforms (a) PyTorch Geometric (PyG) (b) Deep Graph Library (DGL) Dataset coverage (a) homogeneous and heterogeneous networks, (b) scale-free networks, (c) small and large networks

## Figure 2 :
2A general framework that most of the existing methods follow.

## Figure 3 :
3A taxonomy of graph embedding methods.

## Figure 4 :
4(a) An example graph with one-hot encoded representation and sliding window. (b) Skip-gram model used in random-walk based methods such as DeepWalk[79] and node2vec[34] 

## Figure 5 :
5(a) First order proximity vertices of C are A, B, D, and E. Second order proximity vertices of C are K, F, G, and H. (b) A DFS traversal walk C → E → F → H contributes to homophily characteristics discovery in the graph whereas a BFS traversal from C to {A, B, D, E} contributes to structural discovery.

## Figure 6 :
6(a) An image can be converted to grid graph. (b) Text or speech signal can be converted to line graph. (c) An example of graph having irregular structure such as different vertices have different degrees.


Hamilton et al. (2017a) use three aggregation functions: (1) Mean aggregator -it computes contributions from all neighbors

## Figure 7 :
7(a) An example graph. (b) Contributions from the neighbors of vertex A are being aggregated towards it through an activation function. (c) Contributions from the neighbors of vertex D are being aggregated towards it through an activation function. Note that the weight matrix W l is shared in the 1 th layer of the graph convolutional networks.

## Figure 8 :
8Examples where aggregation function fails to distinguish graph structures (same color indicates same contribution). (a) Mean and Max-pooling aggregation fail. (b) Max-pooling aggregation fails. (c) Mean and Max-pooling aggregation fail.


2: A list of popular graph representation learning methods are tabulated with corresponding programming languages, their publication years and venues, theoretical running time and publicly available repositories (red, violet, orange and blue colors represent feature engineering, matrix factorization, shallow neural network and graph neural networks based methods, respectively). n -number of vertices, m -number of edges, r -number of roles, d -embedding dimension or size of representation, s -number of negative samples, p -order of Chebyshev polynomial approximation, c -a constant factor which is a product of number of walks, walk length, and window size for Skip-gram model, l -number of layer in graph convolutional networks, r -number of sampled neighbors per vertex.

## •
BlogCatalog: This dataset contains bloggers of the Blog-Catalog 18 website as vertices and friendship between two bloggers as edges. Each blogger submits a blog to the website and mentions some metadata about the blog. Based on this, bloggers can label their blogs in a prespecified set of categories i.e., each blogger can choose multiple categories. This dataset has 10312 vertices, 333983 edges and 39 categories. According toTang and Liu (2009), each blogger includes his/her own blog under 1.6 categories on average.

## Figure 9 :
9(a) Strong scaling results using the Flickr graph. (b) Weak scaling results using synthetic graphs. (c) Graph scaling results using synthetic graphs. Experiments were run on the Intel server.

## Figure 10 :Figure 11 :
1011(a) Memory consumption by unsupervised graph embedding methods for different datasets. (b) Memory consumption by matrix factorization and feature engineering based methods for two benchmark datasets. (c) Memory consumption by GNN methods for two benchmark datasets. Results of F1-micro scores for node classification tasks for (a) Cora, (b) Citeseer, (c) Pubmed, (d) Flickr, (e) BlogCatalog, and (f) Youtube datasets.

## Figure 12 :Figure 13 :
1213Results of F1-macro scores on multi-label classification task for (a) Cora, (b) Citeseer, Results of F1-micro scores on multi-label classification task for (a) USA airport, (b) European airport, and (c) Brazilian airport datasets.

## Figure 14 :
14Training and validation accuracy for GNN methods on (a) Cora and (b) Pubmed datasets.

## Figure 15 :
15Training and validation accuracy for GNN methods in PyG framework using (a) Cora and (b) Pubmed datasets.

## Figure 16 :
16(a) An example graph having some missing links to be predicted (shown in dotted color with question marks). (b) Graph with predicted missing links shown in green colored.

## Figure 17 :
17(a) Link prediction based on Hadamard vector operator. (b) Link prediction based on weighted-L1 based vector operator. (c) Link prediction based on weighted-L2 based vector operator.

## Figure 18 :
18(a) Modularity scores of different methods for different benchmark datasets. (b) Results of graph reconstruction accuracy for different methods.

## Figure 19 :Figure 20 :
19202D visualization of Cora dataset applying t-SNE tool on embeddings generated by (a) Force2Vec, (b) DeepWalk, (c) HARP, and (d) VERSE. Different colors represents different class labels of vertices. 2D visualization of Pubmed dataset applying t-SNE tool on embeddings generated by (a) Force2Vec, (b) DeepWalk, (c) HARP, and (d) VERSE. Different colors represents different class labels of vertices.

## Figure 21 :
21(a) Effect on classification score for Pubmed dataset using various dimensional embedding. (b) Effect of different number of negative samples on Cora and Pubmed datasets.

## Figure 22 :
22(a) Effect of the number of walks and (b) Effect of the walk length in DeepWalk for PubMed and BlogCatalog datasets.

## Table 1 :
1The left column shows various experimental aspects related to graph representation learning and the right column shows a comprehensive set of tasks covered in this survey.

## Table


## Table 3 :
3The experimental coverage in this survey. Horizontal lines separate different aspects of experiments, from top to bottom: complexity and scalability, performance in downstream tasks, and parameter sensitivity.Graphs 
Vertices 
Edges 
#Labels Avg. Degree 
Cora 
2,708 
5,429 
7 
3.89 
Citeseer 
3,327 
4,732 
6 
2.736 
Pubmed 
19,717 
44,338 
3 
4.49 
Flickr 
89,250 
899,756 
7 
20.16 
BlogCatalog 
10312 
333983 
39 
64.77 
Youtube 
1,138,499 2,990,443 
47 
5.253 



## Table 4 :
4Datasets used for different experiments. Graphs are available at https://linqs.soe.ucsc.edu/data and https://sparse.tamu.edu/[Last ac-


## Table 5 :
5Hardware configurations of different CPU and GPU architecture for our experiments. Numbers in the parenthesis represent GPU specification.Titan RTX 



In this work, authors use a KL-divergence based optimization function where first-order and secondorder proximity of vertices are considered. They assume that neighbors of neighbors can capture a similar kind of inherent information. This method works for directed/undirected graphs as well as for weighted graphs.7 https://github.com/benedekrozemberczki/RolX 
8 https://github.com/palash1992/GEM 
9 https://github.com/phanein/deepwalk 

Recently, authors have updated their repository with a new 
version of word2vec. To conduct the experiment, we set 
several of its hyper-parameters as follows: walk length to 
80, number of walks per vertex to 10, size of dimension to 
128, and number of workers to 48. 

• LINE 10 [98]: We set several of its hyper-parameters as follows: size 
of dimension = 128, order of proximity = 2, number of 
negative samples = 5, total number of training samples 
= 10000 millions, starting value of learning rate = 0.025 
and number of threads = 48. 



## Table 7 :
7Runtime (in seconds) of different unsupervised methods for different benchmark graphs on the Intel server. We run all methods using the same computing resources. For each graph, the fastest runtime is shown in a green shade. '×' indicates that the method failed to run or it went out of available memory.Methods 
Runtime in seconds 
Cora 
Citeseer Blogcatalog Pubmed 
Flickr7 
Youtube 
HOPE 
7.21 
11.12 
165.19 
985.43 
× 
× 
DeepWalk 
83.10 
90.69 
351.63 
626.02 
3,506.17 
× 
struct2vec 
891.08 
1,045.26 
2,817.38 
8,589.17 32,169.06 
× 
LINE 
1,437.14 1,295.16 
1,621.78 
1,327.12 
2,300.31 
× 
VERSE 
36.63 
45.07 
146.15 
265.04 
1,676.94 
× 
HARP 
26.25 
27.27 
892.32 
128.60 
1,295.83 
× 
Force2Vec 
6.64 
6.87 
253.52 
50.68 
455.93 
× 



## Table 8 :
8Runtime (in seconds) of different unsupervised methods for different benchmark graphs on the ARM server. All methods use the same computing resources. For each graph, the fastest runtime is shown in a green shade. '×' indicates that the method failed to run because it went out of available memory.

## Table 10 :
10Training and validation time (in seconds) for different semi-supervised methods on the Intel server. The same hardware configurations were used in all 
experiments. 

Graphs 

GPU 
CPU 
GAT 
GCN 
GAT 
GCN 
PyG DGL PyG DGL 
PyG 
DGL 
PyG 
DGL 
Cora 
1.158 1.630 0.680 1.358 4.762 
8.161 
4.340 2.799 
Citeseer 1.220 1.980 0.685 1.021 6.890 14.772 4.713 3.700 
Pubmed 1.267 2.668 0.826 1.300 43.951 40.873 63.101 4.590 



## Table 12 :
12Vector operators used for link prediction. These notational definitions are borrowed from node2vec[34].


CharacteristicsRolX HOPE DeepWalk struc2vec LINE HARP VERSE Force2VecMemory 
consumption 
M 
VH 
H 
M 
H 
H 
L 
L 

Runtime 
H 
L 
VH 
VH 
H  *  
H 
L 
L 
Number of 
Hyper-parameters 
L 
L 
H 
H 
M 
H 
M 
M 

Scalability 
L 
L 
L 
L 
H 
M 
H 
VH 
Robustness 
M 
L 
H 
L 
L 
H 
H 
H 
Performance 
L 
L 
H 
M/H 
M 
H 
H 
H 


A smaller two-dimensional grid and the dimension of a filter is less than the dimension of the image.
https://en.wikipedia.org/wiki/Centrality 4 "The Simplified Molecular-Input Line-Entry System (SMILES) is a specification in the form of a line notation for describing the structure of chemical species using short ASCII strings". (Wikipedia) 5 https://github.com/RoyZhengGao/edge2vec 6 https://github.com/xiangyue9607/BioNEV
https://github.com/tangjianpku/LINE 11 https://github.com/leoribeiro/struc2vec 12 https://github.com/xgfs/verse
https://github.com/HipGraph/Force2Vec 14 https://github.com/tkipf/gcn 15 https://github.com/williamleif/graphsage-simple 16 https://github.com/matenure/FastGCN
https://linqs.soe.ucsc.edu/data 18 https://www.blogcatalog.com/ 19 https://www.youtube.com/
https://en.wikipedia.org/wiki/K-means_clustering
https://en.wikipedia.org/wiki/Cosine_similarity

Friends and neighbors on the web. A Lada, Eytan Adamic, Adar, Social networks. 25Lada A Adamic and Eytan Adar. 2003. Friends and neighbors on the web. Social networks 25, 3 (2003), 211-230.

Distributed large-scale natural graph factorization. Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josifovski, Alexander J Smola, Proceedings of the 22nd international conference on World Wide Web. the 22nd international conference on World Wide WebAmr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josi- fovski, and Alexander J Smola. 2013. Distributed large-scale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web. 37-48.

Oddball: Spotting anomalies in weighted graphs. Leman Akoglu, Mary Mcglohon, Christos Faloutsos , Pacific-Asia Conference on Knowledge Discovery and Data Mining. SpringerLeman Akoglu, Mary McGlohon, and Christos Faloutsos. 2010. Odd- ball: Spotting anomalies in weighted graphs. In Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 410-421.

A survey of link prediction in social networks. Al Mohammad, Mohammed J Hasan, Zaki, Social network data analytics. SpringerMohammad Al Hasan and Mohammed J Zaki. 2011. A survey of link prediction in social networks. In Social network data analytics. Springer, 243-275.

Deep speech 2: End-toend speech recognition in english and mandarin. Dario Amodei, Rishita Sundaram Ananthanarayanan, Jingliang Anubhai, Eric Bai, Carl Battenberg, Jared Case, Bryan Casper, Qiang Catanzaro, Guoliang Cheng, Chen, International conference on machine learning. Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catan- zaro, Qiang Cheng, Guoliang Chen, et al. 2016. Deep speech 2: End-to- end speech recognition in english and mandarin. In International con- ference on machine learning. 173-182.

Robust negative sampling for network embedding. Mohammadreza Armandpour, Patrick Ding, Jianhua Huang, Xia Hu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Mohammadreza Armandpour, Patrick Ding, Jianhua Huang, and Xia Hu. 2019. Robust negative sampling for network embedding. In Pro- ceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 3191-3198.

Network science. Cambridge university press. Albert-László Barabási, Albert-László Barabási et al. 2016. Network science. Cambridge uni- versity press.

Scale-free networks. Albert-László Barabási, Eric Bonabeau, Scientific american. 288Albert-László Barabási and Eric Bonabeau. 2003. Scale-free networks. Scientific american 288, 5 (2003), 60-69.

Fast unfolding of communities in large networks. D Vincent, Jean-Loup Blondel, Renaud Guillaume, Etienne Lambiotte, Lefebvre, Journal of statistical mechanics: theory and experiment. 1010008Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Eti- enne Lefebvre. 2008. Fast unfolding of communities in large net- works. Journal of statistical mechanics: theory and experiment 2008, 10 (2008), P10008.

Freebase: a collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, Proceedings of the 2008 ACM SIG-MOD international conference on Management of data. the 2008 ACM SIG-MOD international conference on Management of dataKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIG- MOD international conference on Management of data. 1247-1250.

False sharing and its effect on shared memory performance. J William, Michael L Bolosky, Scott, 4th Symposium on Experimental Distributed and Multiprocessor Systems. William J Bolosky and Michael L Scott. 1993. False sharing and its ef- fect on shared memory performance. In 4th Symposium on Experimental Distributed and Multiprocessor Systems. 57-71.

Large-scale machine learning with stochastic gradient descent. Léon Bottou, Proceedings of COMPSTAT'2010. COMPSTAT'2010SpringerLéon Bottou. 2010. Large-scale machine learning with stochastic gradi- ent descent. In Proceedings of COMPSTAT'2010. Springer, 177-186.

A comprehensive survey of graph embedding: Problems, techniques, and applications. Hongyun Cai, W Vincent, Kevin Chen-Chuan Zheng, Chang, IEEE Transactions on Knowledge and Data Engineering. 30Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. 2018. A comprehensive survey of graph embedding: Problems, techniques, and applications. IEEE Transactions on Knowledge and Data Engineer- ing 30, 9 (2018), 1616-1637.

Grarep: Learning graph representations with global structural information. Shaosheng Cao, Wei Lu, Qiongkai Xu, Proceedings of the 24th ACM international on conference on information. the 24th ACM international on conference on informationShaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM international on conference on information and knowl- edge management. 891-900.

Heterogeneous network embedding via deep architectures. Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, C Charu, Thomas S Aggarwal, Huang, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningShiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C Aggarwal, and Thomas S Huang. 2015. Heterogeneous network embedding via deep architectures. In Proceedings of the 21th ACM SIGKDD Interna- tional Conference on Knowledge Discovery and Data Mining. 119-128.

Harp: Hierarchical representation learning for networks. Haochen Chen, Bryan Perozzi, Yifan Hu, Steven Skiena, Thirty-Second AAAI Conference on Artificial Intelligence. Haochen Chen, Bryan Perozzi, Yifan Hu, and Steven Skiena. 2018. Harp: Hierarchical representation learning for networks. In Thirty- Second AAAI Conference on Artificial Intelligence.

Fastgcn: fast learning with graph convolutional networks via importance sampling. Jie Chen, Tengfei Ma, Cao Xiao, arXiv:1801.10247arXiv preprintJie Chen, Tengfei Ma, and Cao Xiao. 2018. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247 (2018).

Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, Cho-Jui Hsieh, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningWei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho- Jui Hsieh. 2019. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 257-266.

A survey on network embedding. Peng Cui, Xiao Wang, Jian Pei, Wenwu Zhu, IEEE Transactions on Knowledge and Data Engineering. 31Peng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. 2018. A survey on network embedding. IEEE Transactions on Knowledge and Data Engi- neering 31, 5 (2018), 833-852.

Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, Le Song, arXiv:1802.08786Syntax-directed variational autoencoder for structured data. arXiv preprintHanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. 2018. Syntax-directed variational autoencoder for structured data. arXiv preprint arXiv:1802.08786 (2018).

From visual data exploration to visual data mining: A survey. De Mc Ferreira, Haim Oliveira, Levkowitz, IEEE Transactions on Visualization and Computer Graphics. 9MC Ferreira De Oliveira and Haim Levkowitz. 2003. From visual data exploration to visual data mining: A survey. IEEE Transactions on Vi- sualization and Computer Graphics 9, 3 (2003), 378-394.

On the equivalence of nonnegative matrix factorization and spectral clustering. Chris Ding, Xiaofeng He, Horst D Simon, Proceedings of the 2005 SIAM international conference on data mining. SIAM. the 2005 SIAM international conference on data mining. SIAMChris Ding, Xiaofeng He, and Horst D Simon. 2005. On the equivalence of nonnegative matrix factorization and spectral clustering. In Proceed- ings of the 2005 SIAM international conference on data mining. SIAM, 606-610.

meta-path2vec: Scalable representation learning for heterogeneous networks. Yuxiao Dong, V Nitesh, Ananthram Chawla, Swami, Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. the 23rd ACM SIGKDD international conference on knowledge discovery and data miningYuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017. meta- path2vec: Scalable representation learning for heterogeneous networks. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. 135-144.

Learning structural node embeddings via diffusion wavelets. Claire Donnat, Marinka Zitnik, David Hallac, Jure Leskovec, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningClaire Donnat, Marinka Zitnik, David Hallac, and Jure Leskovec. 2018. Learning structural node embeddings via diffusion wavelets. In Proceed- ings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1320-1329.

Is a single embedding enough? learning node representations that capture multiple social contexts. Alessandro Epasto, Bryan Perozzi, The World Wide Web Conference. Alessandro Epasto and Bryan Perozzi. 2019. Is a single embedding enough? learning node representations that capture multiple social con- texts. In The World Wide Web Conference. 394-404.

Community-based question answering via heterogeneous social network learning. Hanyin Fang, Fei Wu, Zhou Zhao, Xinyu Duan, Yueting Zhuang, Martin Ester, Thirtieth AAAI Conference on Artificial Intelligence. Hanyin Fang, Fei Wu, Zhou Zhao, Xinyu Duan, Yueting Zhuang, and Martin Ester. 2016. Community-based question answering via hetero- geneous social network learning. In Thirtieth AAAI Conference on Arti- ficial Intelligence.

Fast Graph Representation Learning with PyTorch Geometric. Matthias Fey, Jan E Lenssen, ICLR Workshop on Representation Learning on Graphs and Manifolds. Matthias Fey and Jan E. Lenssen. 2019. Fast Graph Representation Learning with PyTorch Geometric. In ICLR Workshop on Representa- tion Learning on Graphs and Manifolds.

Leveraging labelindependent features for classification in sparsely labeled networks: An empirical study. Brian Gallagher, Tina Eliassi-Rad, International Workshop on Social Network Mining and Analysis. SpringerBrian Gallagher and Tina Eliassi-Rad. 2008. Leveraging label- independent features for classification in sparsely labeled networks: An empirical study. In International Workshop on Social Network Mining and Analysis. Springer, 1-19.

Graph echo state networks. Claudio Gallicchio, Alessio Micheli, The 2010 International Joint Conference on Neural Networks (IJCNN). IEEE. Claudio Gallicchio and Alessio Micheli. 2010. Graph echo state net- works. In The 2010 International Joint Conference on Neural Networks (IJCNN). IEEE, 1-8.

Zheng Gao, Gang Fu, Chunping Ouyang, Satoshi Tsutsui, Xiaozhong Liu, Ying Ding, arXiv:1809.02269edge2vec: Learning node representation using edge semantics. arXiv preprintZheng Gao, Gang Fu, Chunping Ouyang, Satoshi Tsutsui, Xiaozhong Liu, and Ying Ding. 2018. edge2vec: Learning node representation us- ing edge semantics. arXiv preprint arXiv:1809.02269 (2018).

Singular value decomposition and least squares solutions. H Gene, Christian Golub, Reinsch, Linear Algebra. SpringerGene H Golub and Christian Reinsch. 1971. Singular value decomposi- tion and least squares solutions. In Linear Algebra. Springer, 134-151.

A new model for learning in graph domains. Marco Gori, Gabriele Monfardini, Franco Scarselli, Proceedings. 2005 IEEE International Joint Conference on Neural Networks. 2005 IEEE International Joint Conference on Neural NetworksIEEE2Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005. A new model for learning in graph domains. In Proceedings. 2005 IEEE In- ternational Joint Conference on Neural Networks, 2005., Vol. 2. IEEE, 729-734.

Graph embedding techniques, applications, and performance: A survey. Knowledge-Based Systems. Palash Goyal, Emilio Ferrara, 151Palash Goyal and Emilio Ferrara. 2018. Graph embedding techniques, applications, and performance: A survey. Knowledge-Based Systems 151 (2018), 78-94.

2016. node2vec: Scalable feature learning for networks. Aditya Grover, Jure Leskovec, Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on Knowledge discovery and data miningAditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD in- ternational conference on Knowledge discovery and data mining. 855- 864.

MeSHHead-ing2vec: a new method for representing MeSH headings as vectors based on graph embedding algorithm. Zhu-Hong Zhen-Hao Guo, De-Shuang You, Hai-Cheng Huang, Kai Yi, Zhan-Heng Zheng, Yan-Bin Chen, Wang, Briefings in Bioinformatics. Zhen-Hao Guo, Zhu-Hong You, De-Shuang Huang, Hai-Cheng Yi, Kai Zheng, Zhan-Heng Chen, and Yan-Bin Wang. 2020. MeSHHead- ing2vec: a new method for representing MeSH headings as vectors based on graph embedding algorithm. Briefings in Bioinformatics (2020).

Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. Michael Gutmann, Aapo Hyvärinen, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. the Thirteenth International Conference on Artificial Intelligence and StatisticsMichael Gutmann and Aapo Hyvärinen. 2010. Noise-contrastive esti- mation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. 297-304.

Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, Advances in neural information processing systems. Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive rep- resentation learning on large graphs. In Advances in neural information processing systems. 1024-1034.

Rex William L Hamilton, Jure Ying, Leskovec, arXiv:1709.05584Representation learning on graphs: Methods and applications. arXiv preprintWilliam L Hamilton, Rex Ying, and Jure Leskovec. 2017. Represen- tation learning on graphs: Methods and applications. arXiv preprint arXiv:1709.05584 (2017).

Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis. K David, Pierre Hammond, Rémi Vandergheynst, Gribonval, 30David K Hammond, Pierre Vandergheynst, and Rémi Gribonval. 2011. Wavelets on graphs via spectral graph theory. Applied and Computa- tional Harmonic Analysis 30, 2 (2011), 129-150.

Rolx: structural role extraction & mining in large graphs. Keith Henderson, Brian Gallagher, Tina Eliassi-Rad, Hanghang Tong, Sugato Basu, Leman Akoglu, Danai Koutra, Christos Faloutsos, Lei Li, Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. the 18th ACM SIGKDD international conference on Knowledge discovery and data miningKeith Henderson, Brian Gallagher, Tina Eliassi-Rad, Hanghang Tong, Sugato Basu, Leman Akoglu, Danai Koutra, Christos Faloutsos, and Lei Li. 2012. Rolx: structural role extraction & mining in large graphs. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. 1231-1239.

It's who you know: graph mining using recursive structural features. Keith Henderson, Brian Gallagher, Lei Li, Leman Akoglu, Tina Eliassi-Rad, Hanghang Tong, Christos Faloutsos, Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. the 17th ACM SIGKDD international conference on Knowledge discovery and data miningKeith Henderson, Brian Gallagher, Lei Li, Leman Akoglu, Tina Eliassi- Rad, Hanghang Tong, and Christos Faloutsos. 2011. It's who you know: graph mining using recursive structural features. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. 663-671.

Graph visualization and navigation in information visualization: A survey. Ivan Herman, Guy Melançon, M Scott Marshall, IEEE Transactions on visualization and computer graphics. 6Ivan Herman, Guy Melançon, and M Scott Marshall. 2000. Graph vi- sualization and navigation in information visualization: A survey. IEEE Transactions on visualization and computer graphics 6, 1 (2000), 24- 43.

Reducing the dimensionality of data with neural networks. E Geoffrey, Hinton, R Ruslan, Salakhutdinov, science. 313Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. science 313, 5786 (2006), 504-507.

Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. 9Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term mem- ory. Neural computation 9, 8 (1997), 1735-1780.

Multilayer feedforward networks are universal approximators. Kurt Hornik, Maxwell Stinchcombe, Halbert White, Neural networks. 2Kurt Hornik, Maxwell Stinchcombe, Halbert White, et al. 1989. Mul- tilayer feedforward networks are universal approximators. Neural net- works 2, 5 (1989), 359-366.

. Aditya Mohammad Raihanul Islam, Naren Prakash, Ramakrishnan, arXiv:1702.06819Distributed Representations of Signed Networks. arXiv preprintMohammad Raihanul Islam, B Aditya Prakash, and Naren Ramakrish- nan. 2017. Distributed Representations of Signed Networks. arXiv preprint arXiv:1702.06819 (2017).

SimRank: a measure of structuralcontext similarity. Glen Jeh, Jennifer Widom, Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. the eighth ACM SIGKDD international conference on Knowledge discovery and data miningGlen Jeh and Jennifer Widom. 2002. SimRank: a measure of structural- context similarity. In Proceedings of the eighth ACM SIGKDD interna- tional conference on Knowledge discovery and data mining. 538-543.

Monte-Carlo approximation algorithms for enumeration problems. M Richard, Michael Karp, Neal Luby, Madras, Journal of algorithms. 10Richard M Karp, Michael Luby, and Neal Madras. 1989. Monte-Carlo approximation algorithms for enumeration problems. Journal of algo- rithms 10, 3 (1989), 429-448.

A new status index derived from sociometric analysis. Leo Katz, Psychometrika. 18Leo Katz. 1953. A new status index derived from sociometric analysis. Psychometrika 18, 1 (1953), 39-43.

Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.02907arXiv preprintThomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016).

. N Thomas, Max Kipf, Welling, arXiv:1611.07308Variational graph autoencoders. arXiv preprintThomas N Kipf and Max Welling. 2016. Variational graph auto- encoders. arXiv preprint arXiv:1611.07308 (2016).

Diffusion Improves Graph Learning. Johannes Klicpera, Stefan Weißenberger, Stephan Günnemann, Conference on Neural Information Processing Systems (NeurIPS). Johannes Klicpera, Stefan Weißenberger, and Stephan Günnemann. 2019. Diffusion Improves Graph Learning. In Conference on Neural Information Processing Systems (NeurIPS).

Matrix factorization techniques for recommender systems. Yehuda Koren, Robert Bell, Chris Volinsky, Computer. 42Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factor- ization techniques for recommender systems. Computer 42, 8 (2009), 30-37.

Grammar variational autoencoder. J Matt, Brooks Kusner, José Miguel Hernández-Lobato Paige, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Matt J Kusner, Brooks Paige, and José Miguel Hernández-Lobato. 2017. Grammar variational autoencoder. In Proceedings of the 34th Interna- tional Conference on Machine Learning-Volume 70. JMLR. org, 1945- 1954.

Benchmark graphs for testing community detection algorithms. Andrea Lancichinetti, Santo Fortunato, Filippo Radicchi, Physical review E. 7846110Andrea Lancichinetti, Santo Fortunato, and Filippo Radicchi. 2008. Benchmark graphs for testing community detection algorithms. Phys- ical review E 78, 4 (2008), 046110.

Face recognition: A convolutional neural-network approach. Steve Lawrence, Lee Giles, Ah Chung Tsoi, Andrew D Back, IEEE transactions on neural networks. 8Steve Lawrence, C Lee Giles, Ah Chung Tsoi, and Andrew D Back. 1997. Face recognition: A convolutional neural-network approach. IEEE transactions on neural networks 8, 1 (1997), 98-113.

Deep learning. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, nature. 521Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learn- ing. nature 521, 7553 (2015), 436-444.

Pytorch-biggraph: A large-scale graph embedding system. Adam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix, Luca Wehrstedt, Abhijit Bose, Alex Peysakhovich, arXiv:1903.12287arXiv preprintAdam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix, Luca Wehrst- edt, Abhijit Bose, and Alex Peysakhovich. 2019. Pytorch-biggraph: A large-scale graph embedding system. arXiv preprint arXiv:1903.12287 (2019).

Neural word embedding as implicit matrix factorization. Omer Levy, Yoav Goldberg, Advances in neural information processing systems. Omer Levy and Yoav Goldberg. 2014. Neural word embedding as im- plicit matrix factorization. In Advances in neural information processing systems. 2177-2185.

Mile: A multi-level framework for scalable graph embedding. Jiongqian Liang, Saket Gurukar, Srinivasan Parthasarathy, arXiv:1802.09612arXiv preprintJiongqian Liang, Saket Gurukar, and Srinivasan Parthasarathy. 2018. Mile: A multi-level framework for scalable graph embedding. arXiv preprint arXiv:1802.09612 (2018).

Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I Sánchez. 2017. A survey on deep learning in medical image analysis. Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Medical image analysis. 42Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I Sánchez. 2017. A survey on deep learning in medical image analysis. Medical image analysis 42 (2017), 60-88.

Link-based classification. Qing Lu, Lise Getoor, Proceedings of the 20th International Conference on Machine Learning (ICML-03. the 20th International Conference on Machine Learning (ICML-03Qing Lu and Lise Getoor. 2003. Link-based classification. In Proceed- ings of the 20th International Conference on Machine Learning (ICML- 03). 496-503.

Cauchy graph embedding. Dijun Luo, Feiping Nie, Heng Huang, Chris H Ding, Proceedings of the 28th International Conference on Machine Learning. the 28th International Conference on Machine LearningDijun Luo, Feiping Nie, Heng Huang, and Chris H Ding. 2011. Cauchy graph embedding. In Proceedings of the 28th International Conference on Machine Learning (ICML-11). 553-560.

Visualizing data using t-SNE. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data us- ing t-SNE. Journal of machine learning research 9, Nov (2008), 2579- 2605.

An investigation of machine learning based prediction systems. Carolyn Mair, Gada Kadoda, Martin Lefley, Keith Phalp, Chris Schofield, Martin Shepperd, Steve Webster, Journal of systems and software. 53Carolyn Mair, Gada Kadoda, Martin Lefley, Keith Phalp, Chris Schofield, Martin Shepperd, and Steve Webster. 2000. An investiga- tion of machine learning based prediction systems. Journal of systems and software 53, 1 (2000), 23-29.

Umap: Uniform manifold approximation and projection for dimension reduction. Leland Mcinnes, John Healy, James Melville, arXiv:1802.03426arXiv preprintLeland McInnes, John Healy, and James Melville. 2018. Umap: Uni- form manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426 (2018).

Laplacian matrices of graphs: a survey. Russell Merris, Linear algebra and its applications. 197Russell Merris. 1994. Laplacian matrices of graphs: a survey. Linear algebra and its applications 197 (1994), 143-176.

Efficient estimation of word representations in vector space. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, arXiv:1301.3781arXiv preprintTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Effi- cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013).

Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. 3111-3119.

Deep learning in bioinformatics. Byunghan Seonwoo Min, Sungroh Lee, Yoon, Briefings in bioinformatics. 18Seonwoo Min, Byunghan Lee, and Sungroh Yoon. 2017. Deep learning in bioinformatics. Briefings in bioinformatics 18, 5 (2017), 851-869.

Hierarchical probabilistic neural network language model. Frederic Morin, Yoshua Bengio, Aistats. Citeseer5Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model.. In Aistats, Vol. 5. Citeseer, 246-252.

Rectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Proceedings of the 27th international conference on machine learning. the 27th international conference on machine learningVinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10). 807-814.

Query-driven active surveying for collective classification. Galileo Namata, Ben London, Lise Getoor, Bert Huang, Umd Edu, 10th International Workshop on Mining and Learning with Graphs. 8Galileo Namata, Ben London, Lise Getoor, Bert Huang, and UMD EDU. 2012. Query-driven active surveying for collective classification. In 10th International Workshop on Mining and Learning with Graphs, Vol. 8.

Finding and evaluating community structure in networks. E J Mark, Michelle Newman, Girvan, Physical review E. 6926113Mark EJ Newman and Michelle Girvan. 2004. Finding and evaluat- ing community structure in networks. Physical review E 69, 2 (2004), 026113.

Asymmetric transitivity preserving graph embedding. Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, Wenwu Zhu, Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on Knowledge discovery and data miningMingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asymmetric transitivity preserving graph embedding. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discov- ery and data mining. 1105-1114.

The pagerank citation ranking: Bringing order to the web. Lawrence Page, Sergey Brin, Rajeev Motwani, Terry Winograd, Technical ReportStanford InfoLabLawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bringing order to the web. Tech- nical Report. Stanford InfoLab.

Adversarially regularized graph autoencoder for graph embedding. Ruiqi Shirui Pan, Guodong Hu, Jing Long, Lina Jiang, Chengqi Yao, Zhang, arXiv:1802.04407arXiv preprintShirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. 2018. Adversarially regularized graph autoencoder for graph embedding. arXiv preprint arXiv:1802.04407 (2018).

A survey of visualization tools for biological network analysis. Anna-Lynn Georgios A Pavlopoulos, Reinhard Wegener, Schneider, Biodata mining. 112Georgios A Pavlopoulos, Anna-Lynn Wegener, and Reinhard Schneider. 2008. A survey of visualization tools for biological network analysis. Biodata mining 1, 1 (2008), 12.

Deepwalk: Online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data miningBryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 701-710.

Training Sensitivity in Graph Isomorphism Network. Md Khaledur Rahman, Proceedings of the 29th ACM International Conference on Information & Knowledge Management. the 29th ACM International Conference on Information & Knowledge ManagementMd Khaledur Rahman. 2020. Training Sensitivity in Graph Isomor- phism Network. In Proceedings of the 29th ACM International Confer- ence on Information & Knowledge Management. 2181-2184.

Evaluating the Community Structures from Network Images Using Neural Networks. Ariful Md Khaledur Rahman, Azad, International Conference on Complex Networks and Their Applications. SpringerMd Khaledur Rahman and Ariful Azad. 2019. Evaluating the Com- munity Structures from Network Images Using Neural Networks. In In- ternational Conference on Complex Networks and Their Applications. Springer, 866-878.

Majedul Haque Sujon, and Ariful Azad. 2020. Force2Vec: Parallel force-directed graph embedding. Md Khaledur Rahman, 2020 IEEE International Conference on Data Mining (ICDM). IEEEMd Khaledur Rahman, Majedul Haque Sujon, and Ariful Azad. 2020. Force2Vec: Parallel force-directed graph embedding. In 2020 IEEE In- ternational Conference on Data Mining (ICDM). IEEE, 442-451.

FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph Neural Networks. Majedul Md Khaledur Rahman, Ariful Haque Sujon, Azad, 2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS). IEEE. Md Khaledur Rahman, Majedul Haque Sujon, and Ariful Azad. 2021. FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph Neural Networks. In 2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS). IEEE, 256-266.

Majedul Haque Sujon, and Ariful Azad. 2022. Scalable Force-Directed Graph Representation Learning and Visualization. Md Khaledur Rahman, Knowledge and Information Systems. Md Khaledur Rahman, Majedul Haque Sujon, and Ariful Azad. 2022. Scalable Force-Directed Graph Representation Learning and Visualiza- tion. Knowledge and Information Systems (2022).

BatchLayout: A Batch-Parallel Force-Directed Graph Layout Algorithm in Shared Memory. Majedul Md Khaledur Rahman, Ariful Haque Sujon, Azad, arXiv:2002.08233arXiv preprintMd Khaledur Rahman, Majedul Haque Sujon, Ariful Azad, et al. 2020. BatchLayout: A Batch-Parallel Force-Directed Graph Layout Algorithm in Shared Memory. arXiv preprint arXiv:2002.08233 (2020).

Addressing big data time series: Mining trillions of time series subsequences under dynamic time warping. Bilson Thanawin Rakthanmanon, Abdullah Campana, Gustavo Mueen, Brandon Batista, Qiang Westover, Jesin Zhu, Eamonn Zakaria, Keogh, ACM Transactions on Knowledge Discovery from Data (TKDD). 7Thanawin Rakthanmanon, Bilson Campana, Abdullah Mueen, Gustavo Batista, Brandon Westover, Qiang Zhu, Jesin Zakaria, and Eamonn Keogh. 2013. Addressing big data time series: Mining trillions of time series subsequences under dynamic time warping. ACM Transactions on Knowledge Discovery from Data (TKDD) 7, 3 (2013), 1-31.

Hogwild: A lock-free approach to parallelizing stochastic gradient descent. Benjamin Recht, Christopher Re, Stephen Wright, Feng Niu, Advances in neural information processing systems. Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild: A lock-free approach to parallelizing stochastic gradient de- scent. In Advances in neural information processing systems. 693-701.

Estimating and sampling graphs with multidimensional random walks. Bruno Ribeiro, Don Towsley, Proceedings of the 10th ACM SIGCOMM conference on Internet measurement. the 10th ACM SIGCOMM conference on Internet measurementBruno Ribeiro and Don Towsley. 2010. Estimating and sampling graphs with multidimensional random walks. In Proceedings of the 10th ACM SIGCOMM conference on Internet measurement. 390-403.

Learning node representations from structural identity. Leonardo Fr Ribeiro, H P Pedro, Daniel R Saverese, Figueiredo, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining2Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. 2017. struc2vec: Learning node representations from structural identity. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 385-394.

Gemsec: Graph embedding with self clustering. Ryan Benedek Rozemberczki, Rik Davies, Charles Sarkar, Sutton, Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and MiningBenedek Rozemberczki, Ryan Davies, Rik Sarkar, and Charles Sutton. 2019. Gemsec: Graph embedding with self clustering. In Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. 65-72.

Learning representations by back-propagating errors. Geoffrey E David E Rumelhart, Ronald J Hinton, Williams, nature. 323David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by back-propagating errors. nature 323, 6088 (1986), 533-536.

The graph neural network model. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, Gabriele Monfardini, IEEE Transactions on Neural Networks. 20Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2008. The graph neural network model. IEEE Transactions on Neural Networks 20, 1 (2008), 61-80.

Structure preserving embedding. Blake Shaw, Tony Jebara, Proceedings of the 26th Annual International Conference on Machine Learning. the 26th Annual International Conference on Machine LearningBlake Shaw and Tony Jebara. 2009. Structure preserving embedding. In Proceedings of the 26th Annual International Conference on Machine Learning. 937-944.

Weisfeiler-lehman graph kernels. Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, Karsten M Borgwardt, Journal of Machine Learning Research. 12Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. 2011. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research 12, Sep (2011), 2539- 2561.

Mapping knowledge domains. M Richard, Katy Shiffrin, Börner, Richard M Shiffrin and Katy Börner. 2004. Mapping knowledge do- mains.

GraphVAE: Towards generation of small graphs using variational autoencoders. Martin Simonovsky, Nikos Komodakis, International Conference on Artificial Neural Networks. SpringerMartin Simonovsky and Nikos Komodakis. 2018. GraphVAE: Towards generation of small graphs using variational autoencoders. In Interna- tional Conference on Artificial Neural Networks. Springer, 412-422.

Link prediction in multi-modal social networks. Panagiotis Symeonidis, Christos Perentis, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. SpringerPanagiotis Symeonidis and Christos Perentis. 2014. Link prediction in multi-modal social networks. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 147-162.

Line: Large-scale information network embedding. Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei, Proceedings of the 24th international conference on world wide web. the 24th international conference on world wide webJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th international conference on world wide web. 1067-1077.

Relational learning via latent social dimensions. Lei Tang, Huan Liu, Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. the 15th ACM SIGKDD international conference on Knowledge discovery and data miningLei Tang and Huan Liu. 2009. Relational learning via latent social di- mensions. In Proceedings of the 15th ACM SIGKDD international con- ference on Knowledge discovery and data mining. 817-826.

Verse: Versatile graph embeddings from similarity measures. Anton Tsitsulin, Davide Mottin, Panagiotis Karras, Emmanuel Müller, Proceedings of the 2018 World Wide Web Conference. the 2018 World Wide Web ConferenceAnton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel Müller. 2018. Verse: Versatile graph embeddings from similarity mea- sures. In Proceedings of the 2018 World Wide Web Conference. 539- 548.

Deep recursive network embedding with regular equivalence. Ke Tu, Peng Cui, Xiao Wang, S Philip, Wenwu Yu, Zhu, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningKe Tu, Peng Cui, Xiao Wang, Philip S Yu, and Wenwu Zhu. 2018. Deep recursive network embedding with regular equivalence. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Dis- covery & Data Mining. 2357-2366.

Graphs whose normalized Laplacian has three eigenvalues. G R Edwin R Van Dam, Omidi, Linear algebra and its applications. 435Edwin R van Dam and GR Omidi. 2011. Graphs whose normalized Laplacian has three eigenvalues. Linear algebra and its applications 435, 10 (2011), 2560-2569.

Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arXiv:1710.10903Graph attention networks. arXiv preprintPetar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention net- works. arXiv preprint arXiv:1710.10903 (2017).

Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, Zheng Zhang, arXiv:1909.01315Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks. arXiv preprintMinjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang. 2019. Deep Graph Li- brary: A Graph-Centric, Highly-Performant Package for Graph Neural Networks. arXiv preprint arXiv:1909.01315 (2019).

Attributed signed network embedding. Suhang Wang, Charu Aggarwal, Jiliang Tang, Huan Liu, Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. the 2017 ACM on Conference on Information and Knowledge ManagementSuhang Wang, Charu Aggarwal, Jiliang Tang, and Huan Liu. 2017. At- tributed signed network embedding. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. 137-146.

Social network analysis: Methods and applications. Stanley Wasserman, Katherine Faust, Cambridge university press8Stanley Wasserman, Katherine Faust, et al. 1994. Social network analy- sis: Methods and applications. Vol. 8. Cambridge university press.

Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip S Yu, arXiv:1901.00596A comprehensive survey on graph neural networks. arXiv preprintZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. 2019. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596 (2019).

Graphscape: integrated multivariate network visualization. Kai Xu, Andrew Cunningham, Seok-Hee Hong, Bruce H Thomas, 6th International Asia-Pacific Symposium on Visualization. IEEE. Kai Xu, Andrew Cunningham, Seok-Hee Hong, and Bruce H Thomas. 2007. Graphscape: integrated multivariate network visualization. In 2007 6th International Asia-Pacific Symposium on Visualization. IEEE, 33-40.

. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, arXiv:1810.00826arXiv preprintKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).

Revisiting semi-supervised learning with graph embeddings. Zhilin Yang, W William, Ruslan Cohen, Salakhutdinov, arXiv:1603.08861arXiv preprintZhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2016. Revis- iting semi-supervised learning with graph embeddings. arXiv preprint arXiv:1603.08861 (2016).

Jiaxuan You, Rex Ying, Xiang Ren, L William, Jure Hamilton, Leskovec, arXiv:1802.08773Graphrnn: Generating realistic graphs with deep autoregressive models. arXiv preprintJiaxuan You, Rex Ying, Xiang Ren, William L Hamilton, and Jure Leskovec. 2018. Graphrnn: Generating realistic graphs with deep auto- regressive models. arXiv preprint arXiv:1802.08773 (2018).

Learning deep network representations with adversarially regularized autoencoders. Wenchao Yu, Cheng Zheng, Wei Cheng, C Charu, Dongjin Aggarwal, Bo Song, Haifeng Zong, Wei Chen, Wang, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningWenchao Yu, Cheng Zheng, Wei Cheng, Charu C Aggarwal, Dongjin Song, Bo Zong, Haifeng Chen, and Wei Wang. 2018. Learning deep network representations with adversarially regularized autoencoders. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2663-2671.

Projective nonnegative matrix factorization for image compression and feature extraction. Zhijian Yuan, Erkki Oja, Scandinavian Conference on Image Analysis. SpringerZhijian Yuan and Erkki Oja. 2005. Projective nonnegative matrix factor- ization for image compression and feature extraction. In Scandinavian Conference on Image Analysis. Springer, 333-342.

Xiang Yue, Zhen Wang, Jingong Huang, Srinivasan Parthasarathy, Soheil Moosavinasab, Yungui Huang, M Simon, Wen Lin, Ping Zhang, Huan Zhang, Sun, arXiv:1906.05017Graph Embedding on Biomedical Networks: Methods, Applications, and Evaluations. arXiv preprintXiang Yue, Zhen Wang, Jingong Huang, Srinivasan Parthasarathy, So- heil Moosavinasab, Yungui Huang, Simon M Lin, Wen Zhang, Ping Zhang, and Huan Sun. 2019. Graph Embedding on Biomedical Networks: Methods, Applications, and Evaluations. arXiv preprint arXiv:1906.05017 (2019).

Rajgopal Kannan, and Viktor Prasanna. 2019. Accurate, efficient and scalable graph embedding. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, 2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS). IEEE. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. 2019. Accurate, efficient and scalable graph em- bedding. In 2019 IEEE International Parallel and Distributed Process- ing Symposium (IPDPS). IEEE, 462-471.

Graphsaint: Graph sampling based inductive learning method. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, Viktor Prasanna, Proceedings of ICLR. ICLRHanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. 2020. Graphsaint: Graph sampling based inductive learning method. Proceedings of ICLR (2020).

SINE: Scalable Incomplete Network Embedding. Daokun Zhang, Jie Yin, Xingquan Zhu, Chengqi Zhang, 2018 IEEE International Conference on Data Mining (ICDM). IEEEDaokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang. 2018. SINE: Scalable Incomplete Network Embedding. In 2018 IEEE Inter- national Conference on Data Mining (ICDM). IEEE, 737-746.

Learning from collective intelligence: Feature learning using social images and tags. Hanwang Zhang, Xindi Shang, Huanbo Luan, Meng Wang, Tat-Seng Chua, ACM transactions on multimedia computing, communications, and applications (TOMM). 13Hanwang Zhang, Xindi Shang, Huanbo Luan, Meng Wang, and Tat- Seng Chua. 2016. Learning from collective intelligence: Feature learn- ing using social images and tags. ACM transactions on multimedia com- puting, communications, and applications (TOMM) 13, 1 (2016), 1-23.

Nscaching: Simple and efficient negative sampling for knowledge graph embedding. Yongqi Zhang, Quanming Yao, Yingxia Shao, Lei Chen, 2019 IEEE 35th International Conference on Data Engineering (ICDE). IEEEYongqi Zhang, Quanming Yao, Yingxia Shao, and Lei Chen. 2019. Nscaching: Simple and efficient negative sampling for knowledge graph embedding. In 2019 IEEE 35th International Conference on Data Engi- neering (ICDE). IEEE, 614-625.

Predicting multicellular function through multi-layer tissue networks. Marinka Zitnik, Jure Leskovec, Bioinformatics. 33Marinka Zitnik and Jure Leskovec. 2017. Predicting multicellular func- tion through multi-layer tissue networks. Bioinformatics 33, 14 (2017), i190-i198.