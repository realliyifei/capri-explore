# Pretraining in Deep Reinforcement Learning: A Survey

CorpusID: 253397510
 
tags: #Computer_Science

URL: [https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096](https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

Pretraining in Deep Reinforcement Learning: A Survey


Zhihui Xie 
ZICHUANLIN@TENCENT.COM
JUNYOULI@TENCENT.COM
DERICYE@TENCENT.COM Tencent
Shanghai Jiao Tong University
Shanghai Jiao Tong University


Zichuan Lin 
ZICHUANLIN@TENCENT.COM
JUNYOULI@TENCENT.COM
DERICYE@TENCENT.COM Tencent
Shanghai Jiao Tong University
Shanghai Jiao Tong University


Tencent Junyou Li 
ZICHUANLIN@TENCENT.COM
JUNYOULI@TENCENT.COM
DERICYE@TENCENT.COM Tencent
Shanghai Jiao Tong University
Shanghai Jiao Tong University


Tencent Shuai Li 
ZICHUANLIN@TENCENT.COM
JUNYOULI@TENCENT.COM
DERICYE@TENCENT.COM Tencent
Shanghai Jiao Tong University
Shanghai Jiao Tong University


Deheng Ye 
ZICHUANLIN@TENCENT.COM
JUNYOULI@TENCENT.COM
DERICYE@TENCENT.COM Tencent
Shanghai Jiao Tong University
Shanghai Jiao Tong University


Pretraining in Deep Reinforcement Learning: A Survey

The past few years have seen rapid progress in combining reinforcement learning (RL) with deep learning. Various breakthroughs ranging from games to robotics have spurred the interest in designing sophisticated RL algorithms and systems. However, the prevailing workflow in RL is to learn tabula rasa, which may incur computational inefficiency. This precludes continuous deployment of RL algorithms and potentially excludes researchers without large-scale computing resources. In many other areas of machine learning, the pretraining paradigm has shown to be effective in acquiring transferable knowledge, which can be utilized for a variety of downstream tasks. Recently, we saw a surge of interest in Pretraining for Deep RL with promising results. However, much of the research has been based on different experimental settings. Due to the nature of RL, pretraining in this field is faced with unique challenges and hence requires new design principles. In this survey, we seek to systematically review existing works in pretraining for deep reinforcement learning, provide a taxonomy of these methods, discuss each sub-field, and bring attention to open problems and future directions.

## Introduction

Reinforcement learning (RL) provides a general-purpose mathematical formalism for sequential decision-making (Sutton & Barto, 2018). By utilizing RL algorithms together with deep neural networks, various milestones in different domains have achieved superhuman performances via optimizing user-specified reward functions in a data-driven manner (Silver et al., 2016;Akkaya et al., 2019;Vinyals et al., 2019;Ye et al., , 2022Chen et al., 2021b). As such, we have seen a growing interest recently in this research direction.

However, while RL has been proven effective at solving well-specified tasks, the issue of sample efficiency (Jin et al., 2021) and generalization (Kirk et al., 2021) still hinder its application to realworld problems. In RL research, a standard paradigm is to let the agent learn from its own or others' collected experience, usually on a single task, and to optimize neural networks tabula rasa with random initializations. For humans, in contrast, prior knowledge about the world contributes greatly to the decision-making process. If the task is related to previously seen tasks, humans tend to reuse what has been learned to quickly adapt to a new task, without learning from exhaustive  et al., 2020), researchers further consider how to leverage unlabeled and sub-optimal offline data for pretraining (Stooke et al., 2021;Schwarzer et al., 2021b), which we term offline pretraining. The offline paradigm with task-irrelevant data further paves the way towards generalist pretraining, where diverse datasets from different tasks and modalities as well as general-purpose models with great scaling properties are combined to build generalist models (Reed et al., 2022;. Pretraining has the potential to play a big role for RL and this survey could serve as a starting point for those interested in this direction. In this paper, we seek to provide a systematic review of existing works in pretraining for deep reinforcement learning. To the best of our knowledge, it is one of the pioneering efforts to systematically study pretraining in deep RL.

Following the development trend of pretraining in RL, we organize the paper as follows. After going through the preliminaries of reinforcement learning and pretraining (Section 2), we start with online pretraining in which an agent learns from interacting with the environment without reward signals (Section 3). And then, we consider offline pretraining, the scenario where unlabeled training data is collected once with any policy (Section 4). In Section 5, we discuss recent advances in developing generalist agents for a variety of orthogonal tasks. We further discuss how to adapt to downstream RL tasks (Section 6). Finally, we conclude this survey together with a few prospects (Section 7).


## Preliminaries


## Reinforcement learning

Reinforcement learning considers the problem of finding a policy that interacts with the environment under uncertainty to maximize its collected reward. Mathematically, this problem can be formulated via a Markov Decision Process (MDP) defined by tuple (S, A, T , ρ 0 , r, γ), with a state space S, an action space A, a state transition distribution T : S × A × S → [0, 1], an initial state distribution ρ 0 : S → [0, 1], a reward function r : S × A → R, and a discount factor γ ∈ (0, 1). The objective is to find such a policy π θ (a|s) parameterized by θ that maximizes J(π θ ) = E π θ ,T ,ρ 0 ∞ t=0 γ t r (s t , a t ) , known as the discounted returns. The notation used in the paper is summarized in Table 1.


## Pretraining

Pretraining aims at obtaining transferable knowledge from large-scale training data to facilitate downstream tasks. In the context of RL, transferable knowledge typically includes good representations that facilitate the agent to perceive the world (i.e., a better state space) and reusable skills from which the agent can quickly build complex behaviors given task descriptions (i.e., a better action space). Training data can be one bottleneck for effective RL pretraining. Unlike what we have witnessed in fields like computer vision and natural language processing where a wealth of unlabeled data can be collected with minimal supervision, RL usually requires highly task-specific reward design, which hinders scaling up pretraining for large-scale applications.

Therefore, the focus of this survey is unsupervised pretraining, in which task-specific rewards are unavailable during pretraining but it is still allowed to learn from online interaction, unlabeled logged data, or task-irrelevant data from other modalities. We omit supervised pretraining given that with task-specific rewards this scenario roughly degenerates to existing RL settings . Figure 1 demonstrates an overview of the pretraining and adaptation process.

The objective is to acquire useful prior knowledge in various forms like good visual representations, exploratory policies π θ (a|s), latent-conditioned policies π (a|s, z), or simply logged datasets. Depending on what data is available during pretraining, it requires different considerations to obtain useful knowledge (Section 3-5) and adapt it accordingly to downstream tasks (Section 6).


## Online Pretraining

Most of the previous successes in RL have been achieved given dense and well-designed reward functions. Despite its primacy in providing excel performances for a specific task, the traditional RL paradigm faces two critical challenges when scaling it up to large-scale pretraining. Firstly, it is notoriously easy for an RL agent to overfit . As a result, a pretrained agent trained with sophisticated task rewards can hardly generalize to unseen task specifications. Furthermore, it remains a practical challenge to design reward functions which is usually costly and requires expert knowledge.

Online pretraining without these reward signals can potentially be a good solution to learning generic skills and eliminate the supervision requirement. Online pretraining aims at acquiring prior knowledge by interacting with the environment in the absence of human supervision. During the pretraining phase, the agent is allowed to interact with the environment for a long period without access to extrinsic rewards. When the environment is accessible, playing with it facilitates skill learning that will be useful later when a task is assigned to the agent. This solution, also known as Type Algorithm

Intrinsic Reward Visual


## Curiosity-driven Exploration

ICM (Pathak et al., 2017) (Burda et al., 2019b) rt ∝ f (φ (st) , at) − φ (s t+1 ) 2 Disagreement  rt ∝ Var (f (φ (st) , at)) Plan2Explore (Sekar et al., 2020) rt ∝ Var (f (φ (st) , at)) Skill Discovery VIC (Gregor et al., 2016) r ∝ log q (z | φ(s H )) − log p(z) VALOR (Achiam et al., 2018) r ∝ log q (z | s 1:H ) − log p(z) DIAYN (Eysenbach et al., 2019) rt ∝ log q (z | st) − log p(z) VISR (Hansen et al., 2020) rt ∝ log q (z | φ(st)) − log p(z) DADS (Sharma et al., 2020) rt ∝ log q (s t+1 | st, z) − log q (s t+1 | st) EDL (Campos et al., 2020) rt ∝ log q (st | z) APS (Liu & Abbeel, 2021a) rt ∝ log q (st | z) + i∈I random log φ(st) − h i HIDIO (Zhang et al., 2021c) rt ∝ log q(z | a t−k+1:t , s t−k:t ) UPSIDE (Kamienny et al., 2022) rt ∝ log q(z | st) − log p(z) LSD (Park et al., 2022) rt
rt ∝ f (φ (st) , at) − φ (s t+1 ) 2 RND∝ (φ (s t+1 ) − φ (st)) z

## Data Coverage Maximization

CBB (Bellemare et al., 2016) rt  rt ∝ i∈Iprototype log φ(st) − h i RE3 (Seo et al., 2021) rt ∝ log ( φ(st) − KNN (φ(st)) + 1) unsupervised RL, has been actively studied in recent years (Burda et al., 2019a;Srinivas & Abbeel, 2021).
∝N (st) − 1 2 MaxEnt (Hazan et al., 2019) rt ∝ ∇R d π t SMM (Lee et al., 2019) rt ∝ logp(st) − log pπ(st) APT (Liu & Abbeel, 2021b) rt ∝ i∈I random log φ(st) − h i Proto-RL
To encourage the agent to build its own knowledge without any supervision, we need principled mechanisms to provide the agent with intrinsic drives. Psychologists found that babies can discover both the tasks to be learned and the solution to those tasks through interacting with the environment (Smith & Gasser, 2005). With experiences accumulated, they are capable of more difficult tasks later on. This motivates a wealth of research that studies how to build self-taught agents with intrinsic rewards (Schmidhuber, 1991;Singh et al., 2004;Oudeyer et al., 2007). Intrinsic rewards, in contrast to task-specifying extrinsic rewards, refer to general learning signals that encourage the agent either to collect diverse experiences or to develop useful skills. It has been shown that pretraining an agent with intrinsic rewards and standard RL algorithms can lead to fast adaptation once the downstream task is given .

Based on how to design intrinsic rewards, we classify existing approaches of unsupervised RL into three categories 1 : curiosity-driven exploration, skill discovery, and maximal data coverage. Table 2 presents a categorization of representative online pretraining algorithms together with their used intrinsic rewards.


## Curiosity-driven Exploration

In the psychology of motivation, curiosity represents motivation to reduce uncertainty about the world (Silvia, 2012). Inspired by this line of psychological theory, similar ideas have been studied to build curiosity-driven approaches for online pretraining. Curiosity-driven approaches seek to explore interesting states that can possibly bring knowledge about the environment. Intuitively, if the agent falls short of accurately predicting the environment, it gains knowledge by interacting and then reducing this part of the uncertainty. The defining characteristic of a curiosity-driven agent 1. This taxonomy of unsupervised RL was originally proposed by Srinivas and Abbeel (2021). Figure 2: The process of computing intrinsic rewards using curiosity-driven exploration approaches.
ℎ ℎ +1 s +1 ℎ +1 = | ℎ +1 − ℎ +1 | 2 2
is how to compute the degree of curiosity to these interesting states, which directly serves as the intrinsic reward for learning. A concrete example is ICM (Pathak et al., 2017), which applies the intrinsic reward proportional to the prediction error as shown in Figure 2:
r t ∝ f (φ (s t ) , a t ) − φ (s t+1 ) 2 2 ,
where f and φ represent the learned forward dynamics model and feature encoder, respectively. To measure curiosity, a broad class of approaches (Pathak et al., 2017;Haber et al., 2018) leverages this kind of learned dynamics models to predict future states in an auxiliary feature space. there are mainly two kinds of estimation: prediction error and prediction uncertainty. Despite that these dynamics-based approaches perform quite well across common scenarios, they usually suffer from action-dependent noisy TVs (Burda et al., 2019a), which will be discussed later in Section 3.1.1. This deficiency encourages the following work to design dynamics-free curiosity estimation (Burda et al., 2019b) and more sophisticated uncertainty estimation methods Sekar et al., 2020).

Another important design choice is associated with the feature encoder φ, especially for highdimensional observations. A proper feature encoder can make the prediction task more tractable and filter out irrelevant aspects so that the agent can only focus on the informative ones. Early studies (Stadie et al., 2015;Burda et al., 2019a) leverage auto-encoding embeddings to recover the original high-dimensional inputs, but the induced feature space is usually too informative about irrelevant details and hence susceptible to noise. To address this issue, Pathak et al. (2017) utilize an inverse dynamics model for feature encoding to make sure that the agent is unaffected by nuisance factors in the environment. The proposed ICM shows impressive zero-shot performance in playing video games. Burda et al. (2019b) further relax the design burden by simply replacing the feature model with a fixed randomly initialized neural network, which is proven effective by a following large-scale empirical study (Burda et al., 2019a). Despite that random feature encoders are sufficient for good performance at training, learned features (e.g., based on inverse dynamics) generalize better (Burda et al., 2019a). Inspired by recent advances in representation learning, Du et al. Figure 3: The process of computing intrinsic rewards using skill discovery approaches.
(2021) = ℎ +1 − +1 ℎ +1 ℎ +1 ( )
directly link curiosity and representation learning loss by formulating a minimax game between a generic representation learning algorithm and a reinforcement learning policy.


### CHALLENGES & FUTURE DIRECTIONS

This kind of approach has several deficiencies. One of the most important issues is how to distinguish epistemic and aleatoric uncertainty. Epistemic uncertainty refers to uncertainty caused by a lack of knowledge. Aleatoric uncertainty, in contrast, refers to the variability in the outcome due to inherently random effects. A concrete phenomenon in RL is the noisy TV problem (Mavor-Parker et al., 2022), which refers to the cases where the agent gets trapped by its curiosity in highly stochastic environments. To mitigate this issue, some work attempts to use intrinsic rewards proportional to a reduction in uncertainty (Houthooft et al., 2016;Pathak et al., 2019). However, tractable epistemic uncertainty estimation in high dimension remains challenging (Hüllermeier & Waegeman, 2021) due to its sensitivity to imperfect data.

Another issue with the above approaches is that they only receive retrospective signals after the agent has achieved epistemic uncertainty, which might cause inefficiency in exploration. Based on this intuition, Sekar et al. (2020) design a model-based method that can prospectively look for uncertainty in the environment.


## Skill Discovery

Apart from curiosity-driven approaches that tackle unsupervised RL in a model-based perspective, one can also consider model-free learning of primitive skills 2 that can be composed to solve downstream tasks. This kind of approach is usually referred to as skill discovery approach. The main intuition behind this is that the learned skill should control which states the agent visits, which can be seen as a notion of empowerment.

Generally speaking, the objective for skill discovery can be formalized as maximizing the mutual information (MI) between skill latent variable z and state s:
I(s; z) = H(z) − H(z | s) = H(s) − H(s | z),(1)
where we define skills or options as the policies conditioned on z. There are two components for a skill discovery agent to determine: 1) a skill distribution p(z); 2) a skill policy π (a|s, z). Before each episode, skill latent z is sampled from distribution p(z), followed by skill π (a|s, z) to interact 2. In this work, we use skill, option, and behavior prior interchangeably.

with the environment. Learning skills that maximize MI is a challenging optimization problem, upon which a variety of approaches sharing the same spirit have been applied. Among the existing MI-based skill discovery methods, the majority (Gregor et al., 2016;Eysenbach et al., 2019;Hansen et al., 2020) apply the former form of Equation 1 with the following variational lower bound (Agakov, 2004):
I(s; z) = E s,z∼p(s,z) [log p(z | s)] − E z∼p(z) [log p(z)] ≥ E s,z∼p(s,z) [log q(z | s)] − E z∼p(z) [log p(z)].
In this case, a parametric model q(z | s) is trained together with other variables to estimate the conditional distribution p(z | s). Maximizing H(z) can be achieved by sampling z from a learned distribution (Gregor et al., 2016) or directly from a fixed uniform distribution (Eysenbach et al., 2019). As shown in Figure 3, the intrinsic reward is given by r t = log q (z | s t ) − log p(z), upon which one can apply standard RL algorithms to learn skills.

Another line of research (Sharma et al., 2020;Campos et al., 2020;Liu & Abbeel, 2021a;Laskin et al., 2022) considers the latter form and similarly derives a lower bound:
I(s; z) = E s,z∼p(s,z) [log p(s | z)] − E s∼p(s) [log p(s)] ≥ E s,z∼p(s,z) [log q(s | z)] − E s∼p(s) [log p(s)].
In this formulation, maximizing the state entropy H(s) encourages exploration while minimizing the conditional entropy results in directed behaviors. The difficulty lies in the density estimation of s, especially for high-dimensional state spaces. A common practice is to maximize H(s) via maximum entropy estimation (Hazan et al., 2019;Lee et al., 2019;Liu & Abbeel, 2021b), which will be elaborated more in Section 3.3.

Although different work uses slightly different approaches to optimize Equation 1, it could be more important to decide other design factors when using skill discovery for online pretraining. For instance, while most studies consider the episodic setting, some efforts have been made to extend MI-based skill discovery to non-episodic settings (Xu et al., 2020;Lu et al., 2021b). It is also promising to consider a curriculum with an increasing number of skills to learn (Achiam et al., 2018). Several other factors are also worth mentioning, such as whether skill latent z is discrete (Achiam et al., 2018)  Skill discovery can be also reinterpreted as goal-conditioned policy learning, where z as selfgenerated and abstract goal is sampled from a distribution instead of provided by the task. One can also consider generating concrete goals in a self-supervised manner (Warde-Farley et al., 2019;Pong et al., 2020) and derive a goal-conditioned reward function similarly from MI maximization. DISCERN (Warde-Farley et al., 2019) designs a non-parametric approach for goal sampling, maintaining a buffer of past observations that drifts as the agent collects new experiences. Skew-Fit (Pong et al., 2020) instead learns a maximum entropy goal distribution by increasing the entropy of a generative model in an iterative manner. Choi et al. (2021) provide a more formal connection mainly from the perspective of goal-conditioned RL. We refer the interested reader to Colas et al. (2022) for further discussion.


### CHALLENGES & FUTURE DIRECTIONS

A major issue for MI-based skill discovery approaches is that the objective does not necessarily lead to strong state coverage as one can maximize I(s; z) even with the smallest state variations (Campos et al., 2020;Park et al., 2022). This lack of coverage can greatly limit their applicability to downstream tasks with complex environments (Campos et al., 2020). To resolve this issue, some existing work explicitly uses x-y coordinates as features to enforce state coverage induced by skills (Eysenbach et al., 2019;Sharma et al., 2020). It is also explored to separate the learning process to first maximize H(s) via maximum entropy estimation, followed by behavior learning (Campos et al., 2020;Liu & Abbeel, 2021a).

Moreover, it is empirically shown that skill discovery methods underperform other kinds of online pretraining methods, which may be due to restricted skill spaces . This calls attention to dissecting what skills are learned. In order to live up to their full potential, the discovered skills must strike a balance between generality (i.e., the applicability to a large variety of downstream tasks) and specificity (i.e., the quality of being useful to induce specific behaviors) (Gehring et al., 2021). It is also desired to avoid learning trivial skills (Sharma et al., 2020;Baumli et al., 2021).


## Data Coverage Maximization

Previously we have discussed how to obtain knowledge or skills, measured by the agent's own capability, from unsupervised interaction. Albeit indirectly related to the agent's ability, data diversity induced by online pretraining plays an essential role in deciding how well the agent obtains prior knowledge. In the field of supervised learning, recent advances have shown that diverse data can enhance out-of-distribution generalization (Hendrycks et al., 2020b) and robustness (Hendrycks et al., 2020a). Another supporting evidence is that most of the famed datasets are large and diverse (Deng et al., 2009;Wang et al., 2019). Motivated by the above considerations, it is desired to use data coverage maximization, usually measured by state visitation, as an objective to stimulate unsupervised learning.


### COUNT-BASED EXPLORATION

The first category of data coverage maximization is count-based exploration. Count-based exploration methods directly use visit counts to guide the agent towards underexplored states (Bellemare et al., 2016;Ostrovski et al., 2017). For tabular MDPs, Model-based Interval Estimation with Exploration Bonuses (Strehl & Littman, 2008) provably turn state-action N (s, a) counts into an exploration bonus reward:
r t ∝ N (s t , a t ) −1/2 .(2)
Built on Equation 2, a series of work has studied how to tractably generalize count bonuses to high-dimensional state spaces (Bellemare et al., 2016;Ostrovski et al., 2017;Tang et al., 2017). To approximate these counts in high dimensions, Bellemare et al. (2016) introduce pseudo-counts derived from a density model. Specifically, the pseudo-count is defined as:
N (s) = ρ t (s) (1 − ρ t (s)) ρ t (s) − ρ t (s) ,
where ρ is a density model over state space S, ρ t (s) is the density assigned to s after training on a sequence of states s 1 , . . . , s t , and ρ t (s) is the density of s if ρ were to be trained on s one additional time. Based on similar ideas, it has been shown that a better density model (Ostrovski et al., 2017) or a hash function (Tang et al., 2017;Rashid et al., 2020) for computing state statistics can further improve performance. Besides, a self-supervised inverse dynamics model as discussed in Section 3.1 can also be used to bias the count-based bonuses towards what the agent can control (Badia et al., 2020).


### ENTROPY MAXIMIZATION

To encourage novel state visitation, an alternative objective is to directly maximize the entropy of state visitation distribution d π induced by policy π θ (a|s):
π * ∈ arg max π∈Π H (d π ) ,
where H(·) can be Shannon entropy (Hazan et al., 2019;Lee et al., 2019;Seo et al., 2021), Rényi entropy , or geometry-aware entropy (Guo et al., 2021). The state distribution d π can either be a discounted distribution (Hazan et al., 2019), a marginal distribution , or a stationary distribution (Tarbouriech & Lazaric, 2019). Albeit compelling, the objective relies on maximizing state entropy, which is notoriously hard to estimate and optimize. Hazan et al. (2019) contribute a provably efficient algorithm in the tabular setting using the conditional gradient method (Frank & Wolfe, 1956) to avoid direct optimization. Lee et al. (2019) propose a similar approach that can be viewed from the perspective of state marginal matching between the state distribution and a given target distribution (e.g., a uniform distribution). Both Hazan et al. (2019) and Lee et al. (2019) propose to learn a mixture of policies that maximizes the induced state entropy in an iterative manner. While impressive, these parametric approaches struggle to scale up to high dimensional spaces. To address this issue, Mutti et al. (2020) instead optimize a non-parametric, particle-based estimate of state distribution entropy (Kontoyiannis et al., 1998), but restrict its use to state-based tasks.

For unsupervised online pretraining with visual observations, entropy maximization becomes more tricky as exploration is now inextricably intertwined with representation learning. This leads to a chicken-and-egg problem Tam et al., 2022), where learning useful representations requires diverse data, while effective exploration can only be achieved with good representations. Based on particle-based entropy estimators, several approaches successfully apply entropy maximization in image-based tasks with self-supervised representations learned by inverse dynamics prediction (Seo et al., 2021), contrastive learning (Liu & Abbeel, 2021b;Yarats et al., 2021), or the information bottleneck (Tao et al., 2020).


### CHALLENGES & FUTURE DIRECTIONS

Although count-based approaches are shown effective for exploration, it has been shown in previous work (Ecoffet et al., 2021) that they usually suffer from detachment, in which the agent loses track of interesting areas to explore, and derailment, in which the exploratory mechanism prevents it from returning to previously visited states. Count-based approaches also tend to be short-sighted, driving the agent to get stuck in local minima (Burda et al., 2019b).


## Type Algorithm

Objective Visual Expert Data


## Skill Extraction

SPiRL (Pertsch et al., 2021a) Variational Auto-encoder OPAL (Ajay et al., 2021) Variational Auto-encoder Parrot (Singh et al., 2021) Normalizing Flow SkiLD (Pertsch et al., 2021b) Variational Auto-encoder TRIAL  Energy-based Model FIST (Hakhamaneshi et al., 2022) Variational Auto-encoder


## Representation Learning

World Model (Ha & Schmidhuber, 2018) Reconstruction ST-DIM (Anand et al., 2019) Forward Pixel Prediction ATC (Stooke et al., 2021) Forward Dynamics Modeling SGI (Schwarzer et al., 2021b) Forward Dynamics Modeling Markov (Allen et al., 2021) Inverse Dynamics Modeling Table 3: Categorization of representative offline pretraining approaches.

When applying state entropy maximization approaches for pretraining, it is worth pointing out that many of them aim at maximizing the entropy of all states visited during the process, and hence the final policy is not necessarily exploratory . It has also been shown theoretically that the class of Markovian policies is insufficient for the maximum state entropy objective, while non-Markovian policies are essential to guarantee good exploration.

Instead of learning an exploratory policy, another line of research considers collecting unlabeled records as a prerequisite for offline RL Lambert et al., 2022), which is an interesting direction for understanding and utilizing task-agnostic agents.


## Offline Pretraining

Despite its attractive effectiveness of learning without human supervision, online pretraining is still limited for large-scale applications. Eventually, it is difficult to reconcile online interaction with the need to train on large and diverse datasets (Levine, 2021). To address this issue, it is desired to decouple data collection and pretraining and directly leverage historical data collected from other agents or humans.

A feasible solution is offline RL (Lange et al., 2012;Levine et al., 2020), which has been gaining attention recently. Offline RL aims to obtain a reward-maximizing policy purely from offline data. A fundamental challenge of offline RL is the distributional shift, which refers to the distribution discrepancy between training data and those seen during testing. Existing offline RL approaches focus on how to address this challenge when using function approximation. For instance, policy constraint approaches Siegel et al., 2020) explicitly require the learned policy to avoid taking unseen actions in the dataset. Value regularization methods  alleviate the overestimation problem of value functions by fitting them to some forms of lower bounds. However, it remains under-explored whether policies trained offline can generalize to new contexts unseen in the offline dataset (Kirk et al., 2021).

Another scenario is offline-to-online RL Lu et al., 2022;Kostrikov et al., 2022), where offline RL is used for pretraining, followed by online finetuning. It has been shown in this scenario that offline RL can accelerate online RL . However, both offline RL and offline-to-online RL require the offline experience to be annotated with rewards, which are challenging to provide for large real-world datasets (Pertsch et al., 2021a).

A compelling alternative direction for leveraging offline data is to sidestep policy learning, but instead learn prior knowledge that is beneficial for downstream tasks in terms of convergence speed or final performances. What is more intriguing, if our model were able to utilize data without human supervision, it could potentially benefit from web-scale data for decision-making. We refer to this setting as offline pretraining, where the agent can extract important information (e.g., good representations and behavior priors) from offline data. In Table 3, we categorize existing offline pretraining approaches as well as summarize each approach's key properties.


## Skill Extraction

Learning useful behaviors from offline data has a long history (Pomerleau, 1988;Argall et al., 2009). When the offline data comes from expert demonstrations, it is straightforward to pretrain policies via imitation learning (Silver et al., 2016;Rajeswaran et al., 2018;Gupta et al., 2020), which is often used in real-world applications like robotic manipulation (Zhu et al., 2018; and self-driving (Codevilla et al., 2019). However, imitation learning approaches often assume that the training data contains complete solutions. They therefore usually fall short of obtaining good policies when demonstrations are collected from a series of sources.

An alternative solution is to learn useful behavior priors from offline data Chebotar et al., 2021), similar to what we have discussed in Section 3.2. Compared with its online counterpart, offline skill extraction assumes a fixed set of trajectories. These approaches learn a spectrum of behavior policies conditioned on latent z, which provide a more compact action space for learning high-level policies that can quickly adapt to downstream tasks. Specifically, temporal skill extraction  for few-shot imitation (Ajay et al., 2021) and RL (Ajay et al., 2021;Pertsch et al., 2021aPertsch et al., , 2021b considers how to distill offline trajectories into primitive policies π (a|s, z), where z ∈ Z denotes a skill latent learned via unsupervised learning. By leveraging stochastic latent variable models, we aim at learning a skill latent z i ∈ Z for a sequence of state-action pairs {s t , a t , . . . , s t+H−1 , a t+H−1 }, where H is a fixed horizon or a variable one (Kipf et al., 2019;. For example, Ajay et al. (2021) propose the following auto-encoding objective to learn primitive skills:
min θ,φ,ω J(θ, φ, ω) = E τ ∼D,z∼q φ (z|τ ) − H−1 t=0 log π θ (a t | s t , z) s.t. E τ ∼D [D KL (q φ (z | τ ) ρ ω (z | s 0 ))] ≤ KL ,
where q φ (z | τ ) encodes the trajectory τ into skill latent z and skill policy π (a|s, z) serves as a decoder to translate skill latent z into action sequences. To transfer skills into downstream tasks, it is feasible to learn a hierarchical policy that generates high-level behaviors with π(z | s) trained on downstream tasks (Pertsch et al., 2021a), which will be elaborated in Secition 6.2.

Various latent variable models have been used for pretraining behavior priors. For instance, variational auto-encoders (Kingma & Welling, 2014) are widely considered (Pertsch et al., 2021a;Lynch et al., 2020;Ajay et al., 2021). Following work (Singh et al., 2021;Florence et al., 2022) also explores normalizing flow (Dinh et al., 2017) and energy-based models (LeCun et al., 2006) to learn action priors.

The scenario of pretraining behavior priors also bears resemblance to few-shot imitation learning (Dance et al., 2021;Hakhamaneshi et al., 2022). However, for few-shot imitation learning, it is often assumed that expert data is collected from a single behavior policy. Furthermore, due to error accumulation (Ross et al., 2011), few-shot imitation learning is often limited to short-horizon  problems (Hakhamaneshi et al., 2022). In this regard, learning behavior priors from diverse and sub-optimal data appears to be a promising direction.


### CHALLENGES & FUTURE DIRECTIONS

Despite its potential to extract useful primitive skills, it is still challenging to pretrain on highly sub-optimal offline data containing random actions (Ajay et al., 2021). Besides, RL with learned skills does not usually generalize to downstream tasks efficiently, requiring millions of online interactions to converge (Hakhamaneshi et al., 2022). A possible solution is to combine with successor features (Barreto et al., 2017;Hansen et al., 2020) for fast task inference. However, strategies that directly use the pretrained policies for exploitation may result in sub-optimal solutions in such a scenario (Campos et al., 2021).


## Representation Learning

While pretraining behavior priors focus on reducing the complexity of the action space, there exists another line of work that aims to pretrain good state representations from offline data to promote transfer. If the agent effectively reduces the representation gap between the learned state representations and the ground-truth endogenous states, it can better focus on factors that are essential for control. Table 4 compares different kinds of representation learning objectives in terms of sufficiency (i.e., whether the representations contain sufficient state information) and compactness (i.e., whether the representations discard irrelevant information).

Learning good state representations for RL is a mature research area with a range of tools van den Oord et al., 2018;Zhang et al., 2022b). Traditionally, the problem is formulated to group states into clusters based on certain properties (Li et al., 2006). Existing representation learning approaches generally propose some predictive properties that the desired representations have, with regard to states, actions, and rewards across different time-steps. One of the most representative concepts is bisimulation (Larsen & Skou, 1991;Givan et al., 2003), which originally requires two equivalent states to have the same reward and equivalent distributions over the next bisimilar states. The objective turns out to be very restrictive and is further relaxed by following work (Ferns et al., 2004;Castro & Precup, 2010) with a defined pseudo-metric space to measure behavioral similarity. Despite their recent advances (Gelada et al., 2019;Zhang et al., 2021a; in effective representation learning using deep neural networks, bisimulation methods fail to provide good abstraction when the rewards are sparse or even absent. In this case, solely relying on a forward model can lead to representation collapse (Allen et al., 2021).

To alleviate representation collapse, one can instead set the targets to pixel observations. This includes reconstruction-based approaches (Lange & Riedmiller, 2010;Ha & Schmidhuber, 2018) and those based on pixel prediction (Watter et al., 2015;Kaiser et al., 2020). Reconstruction-based approaches typically train an auto-encoder on image observations to learn a low-dimensional representation, using which a policy is learned subsequently. Approaches based on pixel prediction force the representations to contain sufficient information about future pixel observations. Despite these learned representations preserving sufficient information about the observation, it lacks compactness and does not guarantee to capture of useful information for the control task.

Instead of predicting the future, it is also beneficial to model the inverse dynamics of the system (Christiano et al., 2016;Pathak et al., 2017). Inverse dynamics modeling learns a representation that is predictive of the action taken between a pair of consecutive states. It has been shown that the learned representation can filter out all uncontrollable aspects of the observations (Efroni et al., 2021). However, it can also wrongly ignore controllable information and cause over-abstraction over the state space (Rakelly et al., 2021;Efroni et al., 2021).

With the rise of self-supervised learning developed for CV and NLP, a natural direction is to adapt these task-agnostic techniques to RL. For instance, a large body of works has explored contrastive learning (Gutmann & Hyvärinen, 2010) as an effective framework to learn good representations Laskin et al., 2020;Mazoure et al., 2020;Zhan et al., 2020). Contrastive learning typically uses the InfoNCE loss (Poole et al., 2019) to maximize mutual information between two variables:
L InfoNCE = E log exp (f (x i , y i )) 1 K K j=1 exp (f (x i , y j )) , where f is a bilinear function f (x i , y i ) = φ(x i ) W φ(y i )
with learned parameter W ∈ R n×n and K is the number of negative samples. These approaches usually incorporate temporal information, aiming to distinguish between sequential and non-sequential states (Anand et al., 2019;Stooke et al., 2021). Following works (Schwarzer et al., 2021a(Schwarzer et al., , 2021b further consider bootstrapped latent representations (Grill et al., 2020) that get rid of negative samples. Aside from the above representation learning objectives, some other work considers imposing Lipschitz smoothness (Gelada et al., 2019;Zhang et al., 2021a), kinematic inseparability (Misra et al., 2020), or the Markov property (Allen et al., 2021). It has also been shown that a combined objective can also lead to better performance (Schwarzer et al., 2021b).


### CHALLENGES & FUTURE DIRECTIONS

While unsupervised representations have been shown to bring significant improvements to downstream tasks, the absence of reward signals typically leads the pretrained encoder to focus on taskirrelevant features instead of task-relevant ones in visually complex environments . To alleviate this issue, one might incorporate additional inductive bias (Janny et al., 2022) or labeled data that are cheaper to obtain. We will discuss the latter solution in Section 5.

Another challenge for unsupervised representation learning is how to measure its effectiveness without access to downstream tasks. Such evaluation is beneficial because it can provide a proxy metric to predict performance and promote a deeper understanding of the semantic meanings of pretrained representations. To achieve this, it is desired to analyze these representations with probing techniques and determine which properties they encode. Although previous work has made efforts in this direction , it remains unclear what properties are most indispensable for pretrained representations.


## Towards Generalist Agents with RL

So far we have discussed online and offline scenarios that are generally restricted to a single modality and single environment. Recently, there is a surge of interest in building a single generalist model (Reed et al., 2022;Fan et al., 2022) to handle tasks in different environments across different modalities. To enable the agent to learn from and adapt to various open-ended tasks, it is desired to leverage considerable prior knowledge in different forms such as visual perception and language understanding. Intuitively, the aim is to bridge the worlds of RL and other fields of machine learning, combining previous success together to build a large decision-making model capable of a diverse set of tasks. In this section, we look at various considerations for handling data and tasks from different modalities to acquire useful prior knowledge.


## Visual Pretraining

Perception is an unavoidable prerequisite for real-world applications. With an increased number of image-based decision-making tasks, pretrained visual encoders that were exposed to a wide distribution of images can provide RL agents with robust and resilient representations as a basis to learn optimal policies.

The field of computer vision has seen tremendous progress in pretraining visual encoders from large-scale image datasets (Deng et al., 2009) and video corpora (Abu-El-Haija et al., 2016). Given that these data are cheap to access, several works have explored the use of pretrained visual encoders on large-scale image datasets as means to improve the generalization and sample efficiency of RL agents. Shah and Kumar (2021) equip standard deep RL algorithms with ResNet encoders pretrained on ImageNet and observe that the pretrained representations lead to impressive performances in Adroit (Rajeswaran et al., 2018) but struggle in the DeepMind control suite (Tassa et al., 2018) due to large domain gap. Parisi et al. (2022) further investigate various design choices including datasets, augmentations, and layers, and report positive results on all four considered control tasks. Träuble et al. (2022) conduct a large-scale study on how different properties of pretrained VAEbased embeddings affect out-of-distribution generalization, concluding that some of them (e.g., the GS metric (Dittadi et al., 2021)) can be good proxy metrics to predict generalization performance.

Instead of extracting visual information from static image datasets, another intriguing direction is to capture temporal relations from unlabeled videos. Sermanet et al. (2018) design a selfsupervised approach to learning temporal variance and multi-view invariance on multi-view video data. Xiao et al. (2022) empirically find that, without exploiting temporal information, in-the-wild images collected from YouTube or Egocentric videos lead to better self-supervised representations for manipulation tasks that ImageNet images. Seo et al. (2022) introduce a two-phase learning framework, which first learns useful representations via generative pretraining on videos and then uses the pretrained model for learning action-conditional world models. Baker et al. (2022) successfully extract behavioral priors from internet-scale videos with an inverse dynamics model to uncover the underlying actions followed by behavior cloning, finding that the pretrained model exhibits impressive zero-shot capabilities and finetuning results for playing Minecraft. Zhang et al. (2022) also leverage inverse dynamics models to predict action labels from action-free videos, upon which a new contrastive learning framework is proposed to pretrain action-conditioned policies.


## Natural Language Pretraining

Human beings are not only able to perceive the visual world through their eyes, but understand high-level natural language instructions and ground the rich knowledge from texts to complete tasks. In this vein, there has been a long history of how to connect language and actions (Kollar et al., 2010;Tellex et al., 2011). Especially due to the rapid development of large language models (LLMs) Chowdhery et al., 2022) that exhibit great capability of encoding semantic knowledge, it appears to be a promising direction to leverage advanced LLMs as generic computation engines to facilitate decision making .


### LANGUAGE-CONDITIONED POLICY LEARNING

To extract and harness the knowledge of well-informed pretrained LLMs, a feasible solution is to condition the policies on text descriptions processed by LLMs. This kind of language-conditioned policy learning could be extremely useful for robotic tasks where high-level language instructions are available. For example, Ahn et al. (2022) use pretrained LLMs to split high-level instructions into sub-tasks via prompt engineering for grounding value functions in real-world robotic tasks. Huang et al. (2022b) further enable grounded closed-loop feedback generated by additional perception models as the source of corrections for LLMs' predictions. Tam et al. (2022) instead consider effective exploration in 3D environments, showing that pretrained representations from vision-language models (Radford et al., 2021) form a semantically meaningful state space for curiosity-driven intrinsic rewards. Mahmoudieh et al. (2022) also connect reward specification to vision-language supervision, introducing a framework that leverages text descriptions and pixel observations to produce reward signals.


### POLICY INITIALIZATION

Recent advances bridge the gap between reinforcement learning and sequential modeling (Chen et al., 2021a;Janner et al., 2021;Zheng et al., 2022;Furuta et al., 2022), opening up opportunities to borrow sequential models to RL tasks. Despite the clear distinction, pretrained LLMs could arguably provide reusable knowledge via weight initialization. Reid et al. (2022) investigate whether pretrained LLMs can provide good weight initialization for Transformer-based offline RL models, and conclude with very positive results. Li et al. (2022b) also demonstrate that pretrained LLMs can be used to initialize policies and facilitate behavior cloning as well as online reinforcement learning for embodied tasks. They also suggest using sequential input representations and fintuning the pretrained weights for better generalization.


## Multi-task and Multi-modal Pretraining

With recent advances in building powerful sequence models to handle different modalities and tasks (Lu et al., 2019;Jaegle et al., 2022;Wang et al., 2022), the wave of using large general-propose models (Bommasani et al., 2021) has been sweeping through the field of supervised learning. The key ingredient is Transformer (Vaswani et al., 2017), a highly capable neural architecture built on the self-attention mechanism (Bahdanau et al., 2015) that excels at capturing long-range dependencies in sequential data. Due to its strong generality where various tasks in different domains can be formulated as sequence modeling, Transformer is believed to be a unified architecture for developing foundation models (Bommasani et al., 2021).

Recently, Transformer-based architectures have also been extended to the field of offline RL (Chen et al., 2021a;Janner et al., 2021) and then online RL (Zheng et al., 2022), in which the agent is trained auto-regressively in a supervised manner via likelihood maximization. This opens up the possibility of replicating previous success achieved with Transformer in the field of supervised learning. Specifically, it is expected that by combining large-scale data, open-ended objectives, and Transformer-based architectures, we are ready to build general-purpose decision-making agents that are capable of various downstream tasks in different environments.

Pioneering work in this direction is Gato (Reed et al., 2022), a generalist agent trained on various tasks from control environments, vision datasets, and language datasets in a supervised manner. To handle multi-task and multi-modal data, Gato uses demonstrations as prompt sequences  at inference time.  extend Decision Transformer (Chen et al., 2021a) to train a generalist agent called Multi-Game DT that can play 41 Atari games simultaneously. Both Gato and Multi-Game DT show impressive scaling law properties. Fan et al. (2022) make use of large-scale multi-modal data from YouTube videos, Wikipedia pages, and Reddit posts to train an agent able to solve various tasks in Minecraft. To provide dense reward signals, a pretrained vision-language model based on CLIP (Radford et al., 2021) is introduced as a proxy of human evaluation.


## Challenges & Future Directions

In spite of some promising results, how generalist models benefit from multi-modal and multitask data remains unclear. More specifically, these models might suffer from detrimental gradient interference  between modalities and tasks due to the incurred optimization challenges. To mitigate this issue, it is desired to incorporate more analysis tools for optimization landscapes (Goodfellow & Vinyals, 2015) and gradients  to tease out the precise principles.

Another compelling direction is to compose separate pretrained models (e.g., GPT-3 (Brown et al., 2020) and CLIP (Radford et al., 2021)) together. By leveraging expert knowledge from different models, this kind of framework can solve complex multi-modal tasks (Li et al., 2022a).


## Task Adaptation

While pretraining on unsupervised experiences can result in rich transferable knowledge, it remains challenging to adapt the knowledge to downstream tasks in which reward signals are exposed. In this section, we discuss briefly various considerations for downstream task adaptation. We limit the scope to online adaptation, while adaptation with offline RL or imitation learning is also feasible (Yang & Nachum, 2021).

In online task adaptation, a pretrained model is given, which can be composed of various components such as policies and representations, together with a target MDP that can interact with. Given that pretraining could result in different forms of knowledge, it brings difficulties to designing principled adaptation techniques. Nevertheless, considerable efforts have been made to study this aspect.


## Representation Transfer

In the field of supervised learning, recent advances (Devlin et al., 2019;He et al., 2020;Chen et al., 2020) have demonstrated that good representations can be pretrained on large-scale unlabeled dataset, as evidenced by their impressive downstream performances. The most common practice is to freeze the weights of the pretrained feature encoder and train a randomly initialized task-specific network on top of that during adaptation. The success of this paradigm is essentially based on the promise that related tasks can usually be solved using similar representations.

For RL, it has been shown that directly reusing pretrained task-agnostic representations can significantly improve sample efficiency on downstream tasks. For instance, Schwarzer et al. (2021b) conduct experiments on the Atari 100K benchmark and find that frozen representations pretrained on exploratory offline data already form a basis of data-efficient RL. This success also extends to the cases where domain discrepancy exists between upstream and downstream tasks (Shah & Kumar, 2021;Parisi et al., 2022). However, the issue of negative transfer in the face of domain discrepancy might be exacerbated for RL due to its complexity (Shah & Kumar, 2021).

When adapting to tasks that have the same environment dynamics as that of the upstream task(s), successor features (Barreto et al., 2017) can be a powerful tool to aid task adaptation. The framework of successor features is based on the following decomposition of reward functions: r s, a, s = φ s, a, s w,

where φ (s, a, s ) ∈ R d represents features of transition (s, a, s ) and w ∈ R d encodes rewardspecifying weights. This leads to a representation of the value function that decouples the dynamics of the environment from the rewards:
Q π (s, a) = E st=s,at=a ∞ i=t γ i−t φ s i+1 , a i+1 , s i+1 w = ψ π (s, a) w,
where we call ψ π (s, a) the successor features of (s, a) under π. Intuitively, ψ π summarizes the dynamics induced by π and has been studied within the framework of online pretraining (Hansen et al., 2020;Liu & Abbeel, 2021a) by combining with skill discovery approaches to implicitly learn controllable successor features ψ π (s, a). Given a learned ψ π (s, a), the problem of task adaptation reduces to a linear regression derived from Equation 3.


## Policy Transfer

A compelling alternative for task adaptation is to transfer learned behaviors. As discussed in previous sections, existing work has explored how to pretrain primitive skills that can be reused to face new tasks or a single exploratory policy that facilitates exploration at the beginning of task adaptation. The differences in pretrained behaviors result in different adaptation strategies. To achieve high rewards on the downstream task with skill-conditioned policy π (a|s, z), a straightforward strategy is to simply choose the skill z with the best outcome and further enhance it with finetuning. However, a single best-performing skill can not fulfill its potential. To better combine diverse skills for task solving, one can view them from the perspective of hierarchical RL (Barto & Mahadevan, 2003;Kulkarni et al., 2016). In hierarchical RL, the decision-making task is typically decomposed into a two-level hierarchy, where a meta-controller π(z | s) decides which low-level policy to use for task solving, depending on the current state. This hierarchical scheme is agnostic to how the low-level policies are learned. Therefore, it is sufficient to train a meta-controller on top of the discovered skills, which has been proven effective for few-shot adaptation (Hakhamaneshi et al., 2022) and zero-shot adaptation (Sharma et al., 2020).

Exploratory policies, as another form of prior knowledge, benefit downstream tasks in a different way. Due to the importance of exploration, exploratory policies can provide good initialization for the agent to gather diverse experiences and reach high-rewarding states. For example, Campos et al. (2021) validate the effectiveness of transferring exploratory policies trained by curiosity-driven approaches, in particular for domains that require structured exploration.

While it is always feasible to finetune pretrained policies, considerations should be taken in order to prevent catastrophic forgetting when learning in the downstream task. Catastrophic forgetting refers to the tendency of neural networks to disregard their previously obtained knowledge when new information is acquired. To mitigate this issue, one might apply knowledge distillationlike regularization together with RL objectives (Schmitt et al., 2018):
L KD = H (π(a | s) π θ (a | s)) ,
where H is cross entropy andπ is the teacher policy. We refer the reader to Khetarpal et al. (2020) for more discussions on catastrophic forgetting in reinforcement learning.


## Challenges & Future Directions

Parameter Efficiency. Despite that existing pretrained models for RL have much fewer parameters as compared with those in the field of supervised learning, the issue of parameter efficiency is still important with the ever-increasing number of model parameters. More concretely, it is desired to design parameter-efficient transfer learning that updates only a small fraction of parameters while keeping most of the pretrained parameters intact. It has been actively studied in natural language processing (He et al., 2022) with solutions like adding small neural modules as adapters (Houlsby et al., 2019) and prepending learnable prefix tokens as soft prompts (Lester et al., 2021). Built on these techniques, several efforts have been made to enable parameter-efficient transfer with prompting Reed et al., 2022), which we believe has a large room to improve with tailored methods.

Domain adaptation. In this section, we mainly consider task adaptation where unseen tasks are given in the same environment. A more challenging but practical scenario is domain adaptation. In domain adaptation, there exist environmental shifts between the upstream and downstream tasks. Despite that these environmental shifts are commonly seen in real-world applications, it remains a challenging problem to transfer across different domains (Eysenbach et al., 2021;Huang et al., 2022a). However, we believe that this direction will rapidly evolve by bringing related techniques from supervised learning to reinforcement learning.

Continually-developed models. For practical applications, we can take a step forward and consider building large pretrained models continually to support added features (e.g., a modified action space, more powerful architectures, etc). While such consideration was already underway during the development of large-scale RL models (Berner et al., 2019), it requires a more principled way of combining updates into RL models. We refer the reader to recent work in this direction in the field of supervised learning (Raffel, 2021) and reinforcement learning (Campos et al., 2021;.


## Conclusions and Future Perspectives

In this section, we conclude this survey and highlight several future directions which we believe will be important topics for future work.

This paper introduces pretraining in deep RL by discussing recent trends to obtain general prior knowledge for decision-making. In contrast to its supervised learning counterpart, pretraining faces a variety of challenges unique to RL. In this survey, we present several promising research directions to tackle these challenges and we believe this field will evolve rapidly in the coming years.

There are still several open questions that are important and remain to be addressed.

Benchmarks and evaluation metrics. Evaluation serves as a means for comparing various methods and driving further improvements. In the field of natural language processing, GLUE (Wang et al., 2019) is a widely used benchmark to evaluate the performance of models across various natural language understanding tasks. Recently, there has been a surge of research on improving evaluation for RL in terms of evaluation metrics  and benchmark datasets (Cobbe et al., 2020). To the best of our knowledge, URLB  is the only benchmark for pretraining in deep RL. It presents a unified evaluation protocol for online pretraining based on the DeepMind control suite (Tassa et al., 2018). However, a principled evaluation framework for offline pretraining and generalist pretraining is still missing. We expect existing offline RL benchmarks like D4RL  and RL Unplugged (Gulcehre et al., 2020) can serve as the basis for developing pretraining benchmarks, but more challenging tasks should better illustrate the value of pretraining.

Architecture. As discussed in previous sections, there has been a surge of leveraging large transformers for RL tasks. We expect other recent advances in model architecture can bring more improvements. For example, Mustafa et al. (2022) learn large sparse models with a mixture of experts that simultaneously handle images and text with modality-agnostic routing. This has the promise to solve complex tasks at scale. Besides, one can also rethink existing architectures that have the potential to support large-scale pretraining (e.g., Progress Neural Networks (Rusu et al., 2016)).

Multi-agent RL. Multi-agent RL  is an important sub-field of RL. Extending existing pretraining techniques to the multi-agent scenario is non-trivial. Multi-agent RL typically requires socially desirable behaviors (Ndousse et al., 2021) and representations .

To the best of our knowledge, Meng et al. (2021) present the only effort in pretraining multi-agent RL with supervision. How to enable unsupervised pretraining for multi-agent RL remains unclear, which we believe is a promising research direction.

Theoretical results. For RL, the significant gap between theory and practice has been a longstanding problem, and bringing large-scale pretraining to RL may exacerbate this even further. Fortunately, recent theoretical studies have made efforts in terms of representation transfer  and skill-conditioned policy transfer (Eysenbach et al., 2022). Increasing focus on theoretical results is likely to have profound effects on the development of more advanced pretraining methods.' 


or continuous (Warde-Farley et al., 2019), whether the reward signals are dense (Eysenbach et al., 2019) or sparse (Gregor et al., 2016), and whether it works for imagebased observations (Warde-Farley et al., 2019).


Table 1: Notations used in the survey.Notation Description 
M 
Markov decision process 
S 
State space 
A 
Action space 
T 
Transition function 
ρ 0 
Initial state distribution 
r 
Reward function 
γ 
Discount factor 
D 
Offline dataset 
τ 
Trajectory 
Q 
Q function 
J 
Expected total discounted reward function 
θ 
Neural network parameters 
φ 
Feature encoder 
z 
Skill latent vector 
Z 
Skill latent space 
H 
Entropy 
I 
Mutual information 



## Table 2 :
2Categorization of representative online pretraining approaches.

## Table 4 :
4Comparison between different representation learning approaches.


Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics. DOI: 10.18653/v1/N19-1423. Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2017). Density estimation using real NVP. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. https://openreview. net/forum?id=HkpbnH9lx. Dittadi, A., Träuble, F., Locatello, F., Wuthrich, M., Agrawal, V., Winther, O., Bauer, S., & Schölkopf, B. (2021). On the transfer of disentangled representations in realistic settings. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id= 8VXvj1QNRl1. Du, Y., Gan, C., & Isola, P. (2021). Curious representation learning for embodied intelligence. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10408-10417. Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., & Clune, J. (2021). First return, then explore. Nature, 590(7847), 580-586. Efroni, Y., Misra, D., Krishnamurthy, A., Agarwal, A., & Langford, J. (2021). Provable RL with exogenous distractors via multistep inverse dynamics. CoRR, abs/2110.08847. https: //arxiv.org/abs/2110.08847. Eysenbach, B., Chaudhari, S., Asawa, S., Levine, S., & Salakhutdinov, R. (2021). Off-dynamics reinforcement learning: Training for transfer with domain classifiers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=eqBwg3AcIAK. Eysenbach, B., Gupta, A., Ibarz, J., & Levine, S. (2019). Diversity is all you need: Learning skills without a reward function. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https: //openreview.net/forum?id=SJx63jRqFm. Eysenbach, B., Salakhutdinov, R., & Levine, S. (2022). The information geometry of unsupervised reinforcement learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview. net/forum?id=3wU2UX0voE. Fan, L.,Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y.,  & Anandkumar, A. (2022). Minedojo: Building open-ended embodied agents with internetscale knowledge. ArXiv preprint, abs/2206.08853. https://arxiv.org/abs/2206. 08853. Ferns, N., Panangaden, P., & Precup, D. (2004). Metrics for finite markov decision processes.. In UAI, Vol. 4, pp. 162-169. Florence, P., Lynch, C., Zeng, A., Ramirez, O. A., Wahid, A., Downs, L., Wong, A., Lee, J., Mordatch, I., & Tompson, J. (2022). Implicit behavioral cloning. In Conference on Robot Learning, pp. 158-168. PMLR. Frank, M., & Wolfe, P. (1956). An algorithm for quadratic programming. Naval Research Logistics Quarterly, 3(1-2), 95-110. DOI: https://doi.org/10.1002/nav.3800030109.

Youtube-8m: A large-scale video classification benchmark. S Abu-El-Haija, N Kothari, J Lee, P Natsev, G Toderici, B Varadarajan, S Vijayanarasimhan, abs/1609.08675CoRRAbu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., & Vijaya- narasimhan, S. (2016). Youtube-8m: A large-scale video classification benchmark. CoRR, abs/1609.08675. http://arxiv.org/abs/1609.08675.

Variational option discovery algorithms. J Achiam, H Edwards, D Amodei, P Abbeel, abs/1807.10299CoRRAchiam, J., Edwards, H., Amodei, D., & Abbeel, P. (2018). Variational option discovery algorithms. CoRR, abs/1807.10299. http://arxiv.org/abs/1807.10299.

The im algorithm: a variational approach to information maximization. D B F Agakov, Advances in neural information processing systems. 16201Agakov, D. B. F. (2004). The im algorithm: a variational approach to information maximization. Advances in neural information processing systems, 16(320), 201.

Provable benefits of representational transfer in reinforcement learning. A Agarwal, Y Song, W Sun, K Wang, M Wang, X Zhang, 10.48550/arXiv.2205.14571CoRRAgarwal, A., Song, Y., Sun, W., Wang, K., Wang, M., & Zhang, X. (2022). Provable benefits of representational transfer in reinforcement learning. CoRR, abs/2205.14571. DOI: 10.48550/arXiv.2205.14571.

Contrastive behavioral similarity embeddings for generalization in reinforcement learning. R Agarwal, M C Machado, P S Castro, M G Bellemare, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaAgarwal, R., Machado, M. C., Castro, P. S., & Bellemare, M. G. (2021). Contrastive behavioral similarity embeddings for generalization in reinforcement learning. In 9th International Con- ference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=qda7-sVg84.

Reincarnating reinforcement learning: Reusing prior computation to accelerate progress. R Agarwal, M Schwarzer, P S Castro, A Courville, M G Bellemare, 10.48550/ARXIV.2206.01626Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A., & Bellemare, M. G. (2022). Rein- carnating reinforcement learning: Reusing prior computation to accelerate progress.. DOI: 10.48550/ARXIV.2206.01626.

Deep reinforcement learning at the edge of the statistical precipice. R Agarwal, M Schwarzer, P S Castro, A C Courville, M ; M Bellemare, A Beygelzimer, Y Dauphin, P Liang, Vaughan, Advances in Neural Information Processing Systems. J. W.34Curran AssociatesAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., & Bellemare, M. (2021). Deep reinforcement learning at the edge of the statistical precipice. In Ran- zato, M., Beygelzimer, A., Dauphin, Y., Liang, P., & Vaughan, J. W. (Eds.), Ad- vances in Neural Information Processing Systems, Vol. 34, pp. 29304-29320. Cur- ran Associates, Inc. https://proceedings.neurips.cc/paper/2021/file/ f514cec81cb148559cf475e7426eed5e-Paper.pdf.

Do as i can, not as i say: Grounding language in robotic affordances. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, abs/2204.01691ArXiv preprintAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., et al. (2022). Do as i can, not as i say: Grounding language in robotic affordances. ArXiv preprint, abs/2204.01691. https://arxiv.org/abs/ 2204.01691.

OPAL: offline primitive discovery for accelerating offline reinforcement learning. A Ajay, A Kumar, P Agrawal, S Levine, O Nachum, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaOpenReviewAjay, A., Kumar, A., Agrawal, P., Levine, S., & Nachum, O. (2021). OPAL: offline primitive discovery for accelerating offline reinforcement learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe- view.net. https://openreview.net/forum?id=V69LGwJ0lIN.

Solving rubik's cube with a robot hand. I Akkaya, M Andrychowicz, M Chociej, M Litwin, B Mcgrew, A Petron, A Paino, M Plappert, G Powell, R Ribas, abs/1910.07113ArXiv preprintAkkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plap- pert, M., Powell, G., Ribas, R., et al. (2019). Solving rubik's cube with a robot hand. ArXiv preprint, abs/1910.07113. https://arxiv.org/abs/1910.07113.

Learning markov state abstractions for deep reinforcement learning. C Allen, N Parikh, O Gottesman, G Konidaris, Advances in Neural Information Processing Systems. 34Allen, C., Parikh, N., Gottesman, O., & Konidaris, G. (2021). Learning markov state abstractions for deep reinforcement learning. Advances in Neural Information Processing Systems, 34, 8229-8241.

Unsupervised state representation learning in atari. A Anand, E Racah, S Ozair, Y Bengio, M Côté, R D Hjelm, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E. B., & Garnett, R.NeurIPS; Vancouver, BC, CanadaAnand, A., Racah, E., Ozair, S., Bengio, Y., Côté, M., & Hjelm, R. D. (2019). Unsu- pervised state representation learning in atari. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E. B., & Garnett, R. (Eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Informa- tion Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8766-8779. https://proceedings.neurips.cc/paper/2019/ hash/6fb52e71b837628ac16539c1ff911667-Abstract.html.

A survey of robot learning from demonstration. B D Argall, S Chernova, M Veloso, B Browning, Robotics and Autonomous Systems. 575Argall, B. D., Chernova, S., Veloso, M., & Browning, B. (2009). A survey of robot learning from demonstration. Robotics and Autonomous Systems, 57(5), 469-483.

. 10.1016/j.robot.2008.10.024DOI: https://doi.org/10.1016/j.robot.2008.10.024.

Never give up: Learning directed exploration strategies. A P Badia, P Sprechmann, A Vitvitskyi, D Guo, B Piot, S Kapturowski, O Tieleman, M Arjovsky, A Pritzel, A Bolt, C Blundell, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Badia, A. P., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot, B., Kapturowski, S., Tieleman, O., Arjovsky, M., Pritzel, A., Bolt, A., & Blundell, C. (2020). Never give up: Learning di- rected exploration strategies. In 8th International Conference on Learning Representa- tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https: //openreview.net/forum?id=Sye57xStvB.

Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, 3rd International Conference on Learning Representations. Bengio, Y., & LeCun, Y.San Diego, CA, USAConference Track ProceedingsBahdanau, D., Cho, K., & Bengio, Y. (2015). Neural machine translation by jointly learning to align and translate. In Bengio, Y., & LeCun, Y. (Eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Pro- ceedings. http://arxiv.org/abs/1409.0473.

Video pretraining (vpt): Learning to act by watching unlabeled online videos. B Baker, I Akkaya, P Zhokhov, J Huizinga, J Tang, A Ecoffet, B Houghton, R Sampedro, J Clune, abs/2206.11795ArXiv preprintBaker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., & Clune, J. (2022). Video pretraining (vpt): Learning to act by watching unlabeled online videos. ArXiv preprint, abs/2206.11795. https://arxiv.org/abs/2206.11795.

Successor features for transfer in reinforcement learning. A Barreto, W Dabney, R Munos, J J Hunt, T Schaul, D Silver, H Van Hasselt, I Guyon, U Von Luxburg, S Bengio, H M Wallach, R Fergus, S V N Vishwanathan, Gar, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. nett, R.Long Beach, CA, USABarreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., Silver, D., & van Hasselt, H. (2017). Successor features for transfer in reinforcement learning. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., & Gar- nett, R. (Eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 4055-4065. https://proceedings.neurips.cc/paper/2017/hash/ 350db081a661525235354dd3e19b8c05-Abstract.html.

Recent advances in hierarchical reinforcement learning. A G Barto, S Mahadevan, 10.1023/A:1022140919877Discret. Event Dyn. Syst. 131-2Barto, A. G., & Mahadevan, S. (2003). Recent advances in hierarchical reinforcement learning. Discret. Event Dyn. Syst., 13(1-2), 41-77. DOI: 10.1023/A:1022140919877.

Relative variational intrinsic control. K Baumli, D Warde-Farley, S Hansen, V Mnih, 10.1609/aaai.v35i8.16832Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Baumli, K., Warde-Farley, D., Hansen, S., & Mnih, V. (2021). Relative variational intrinsic con- trol. Proceedings of the AAAI Conference on Artificial Intelligence, 35(8), 6732-6740. DOI: 10.1609/aaai.v35i8.16832.

Unifying count-based exploration and intrinsic motivation. M G Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems. Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., & Garnett, R.Barcelona, SpainBellemare, M. G., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., & Munos, R. (2016). Unifying count-based exploration and intrinsic motivation. In Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., & Garnett, R. (Eds.), Ad- vances in Neural Information Processing Systems 29: Annual Conference on Neu- ral Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 1471-1479. https://proceedings.neurips.cc/paper/2016/hash/ afda332245e2af431fb7b672a68b659d-Abstract.html.

Dota 2 with large scale deep reinforcement learning. C Berner, G Brockman, B Chan, V Cheung, P Debiak, C Dennison, D Farhi, Q Fischer, S Hashme, C Hesse, R Józefowicz, S Gray, C Olsson, J Pachocki, M Petrov, H P De Oliveira Pinto, J Raiman, T Salimans, J Schlatter, J Schneider, S Sidor, I Sutskever, J Tang, F Wolski, S Zhang, abs/1912.06680CoRRBerner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., Józefowicz, R., Gray, S., Olsson, C., Pachocki, J., Petrov, M., de Oliveira Pinto, H. P., Raiman, J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J., Wolski, F., & Zhang, S. (2019). Dota 2 with large scale deep rein- forcement learning. CoRR, abs/1912.06680. http://arxiv.org/abs/1912.06680.

On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, abs/2108.07258ArXiv preprintBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. (2021). On the opportunities and risks of foundation models. ArXiv preprint, abs/2108.07258. https://arxiv.org/abs/2108.07258.

Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H.2020Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D. (2020). Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H. (Eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. https://proceedings.neurips.cc/paper/ 2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.

Largescale study of curiosity-driven learning. Y Burda, H Edwards, D Pathak, A J Storkey, T Darrell, A A Efros, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USABurda, Y., Edwards, H., Pathak, D., Storkey, A. J., Darrell, T., & Efros, A. A. (2019a). Large- scale study of curiosity-driven learning. In 7th International Conference on Learning Repre- sentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https: //openreview.net/forum?id=rJNwDjAqYX.

Exploration by random network distillation. Y Burda, H Edwards, A J Storkey, O Klimov, 7th International Conference on Learning Representations, ICLR 2019, New Orleans. LA, USABurda, Y., Edwards, H., Storkey, A. J., & Klimov, O. (2019b). Exploration by random network dis- tillation. In 7th International Conference on Learning Representations, ICLR 2019, New Or- leans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum? id=H1lJJnR5Ym.

Beyond fine-tuning: Transferring behavior in reinforcement learning. V Campos, P Sprechmann, S S Hansen, A Barreto, S Kapturowski, A Vitvitskyi, A P Badia, C Blundell, ICML 2021 Workshop on Unsupervised Reinforcement Learning. Campos, V., Sprechmann, P., Hansen, S. S., Barreto, A., Kapturowski, S., Vitvitskyi, A., Badia, A. P., & Blundell, C. (2021). Beyond fine-tuning: Transferring behavior in reinforcement learning. In ICML 2021 Workshop on Unsupervised Reinforcement Learning. https:// openreview.net/forum?id=4NUhTHom2HZ.

Explore, discover and learn: Unsupervised discovery of state-covering skills. V Campos, A Trott, C Xiong, R Socher, X Giró-I-Nieto, J Torres, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020of Proceedings of Machine Learning ResearchCampos, V., Trott, A., Xiong, C., Socher, R., Giró-i-Nieto, X., & Torres, J. (2020). Explore, discover and learn: Unsupervised discovery of state-covering skills. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Vir- tual Event, Vol. 119 of Proceedings of Machine Learning Research, pp. 1317-1327. PMLR. http://proceedings.mlr.press/v119/campos20a.html.

Using bisimulation for policy transfer in mdps. P S Castro, D Precup, Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010. Fox, M., & Poole, D.the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010Atlanta, Georgia, USAAAAI PressCastro, P. S., & Precup, D. (2010). Using bisimulation for policy transfer in mdps. In Fox, M., & Poole, D. (Eds.), Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010. AAAI Press. http: //www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1907.

Actionable models: Unsupervised offline reinforcement learning of robotic skills. Y Chebotar, K Hausman, Y Lu, T Xiao, D Kalashnikov, J Varley, A Irpan, B Eysenbach, R Julian, C Finn, S Levine, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Meila, M., & Zhang, T.the 38th International Conference on Machine Learning, ICML 2021139of Proceedings of Machine Learning ResearchChebotar, Y., Hausman, K., Lu, Y., Xiao, T., Kalashnikov, D., Varley, J., Irpan, A., Eysenbach, B., Julian, R., Finn, C., & Levine, S. (2021). Actionable models: Unsupervised offline re- inforcement learning of robotic skills. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Vir- tual Event, Vol. 139 of Proceedings of Machine Learning Research, pp. 1518-1528. PMLR. http://proceedings.mlr.press/v139/chebotar21a.html.

Decision transformer: Reinforcement learning via sequence modeling. L Chen, K Lu, A Rajeswaran, K Lee, A Grover, M Laskin, P Abbeel, A Srinivas, I Mordatch, Advances in neural information processing systems. 34Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., & Mordatch, I. (2021a). Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 15084-15097.

Which heroes to pick? learning to draft in moba games with neural networks and tree search. S Chen, M Zhu, D Ye, W Zhang, Q Fu, W Yang, IEEE Transactions on Games. 134Chen, S., Zhu, M., Ye, D., Zhang, W., Fu, Q., & Yang, W. (2021b). Which heroes to pick? learning to draft in moba games with neural networks and tree search. IEEE Transactions on Games, 13(4), 410-421.

A simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G E Hinton, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020of Proceedings of Machine Learning ResearchChen, T., Kornblith, S., Norouzi, M., & Hinton, G. E. (2020). A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, Vol. 119 of Proceedings of Machine Learning Research, pp. 1597-1607. PMLR. http://proceedings.mlr. press/v119/chen20j.html.

Variational empowerment as representation learning for goal-conditioned reinforcement learning. J Choi, A Sharma, H Lee, S Levine, S S Gu, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Meila, M., & Zhang, T.the 38th International Conference on Machine Learning, ICML 2021139of Proceedings of Machine Learning ResearchChoi, J., Sharma, A., Lee, H., Levine, S., & Gu, S. S. (2021). Variational empowerment as repre- sentation learning for goal-conditioned reinforcement learning. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, Vol. 139 of Proceedings of Machine Learning Research, pp. 1953-1963. PMLR. http://proceedings.mlr.press/v139/choi21b.html.

Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, abs/2204.02311ArXiv preprintChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. (2022). Palm: Scaling language modeling with path- ways. ArXiv preprint, abs/2204.02311. https://arxiv.org/abs/2204.02311.

Transfer from simulation to real world through learning deep inverse dynamics model. P F Christiano, Z Shah, I Mordatch, J Schneider, T Blackwell, J Tobin, P Abbeel, W Zaremba, abs/1610.03518CoRRChristiano, P. F., Shah, Z., Mordatch, I., Schneider, J., Blackwell, T., Tobin, J., Abbeel, P., & Zaremba, W. (2016). Transfer from simulation to real world through learning deep inverse dynamics model. CoRR, abs/1610.03518. http://arxiv.org/abs/1610.03518.

Leveraging procedural generation to benchmark reinforcement learning. K Cobbe, C Hesse, J Hilton, J Schulman, Proceedings of the 37th International Conference on Machine Learning. III, H. D., & Singh, A.the 37th International Conference on Machine Learning119of Proceedings of Machine Learning ResearchCobbe, K., Hesse, C., Hilton, J., & Schulman, J. (2020). Leveraging procedural generation to bench- mark reinforcement learning. In III, H. D., & Singh, A. (Eds.), Proceedings of the 37th International Conference on Machine Learning, Vol. 119 of Proceedings of Machine Learn- ing Research, pp. 2048-2056. PMLR. https://proceedings.mlr.press/v119/ cobbe20a.html.

Exploring the limitations of behavior cloning for autonomous driving. F Codevilla, E Santana, A M Lopez, A Gaidon, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Codevilla, F., Santana, E., Lopez, A. M., & Gaidon, A. (2019). Exploring the limitations of behavior cloning for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).

Autotelic agents with intrinsically motivated goal-conditioned reinforcement learning: A short survey. C Colas, T Karch, O Sigaud, P.-Y Oudeyer, 10.1613/jair.1.13554J. Artif. Int. Res. Colas, C., Karch, T., Sigaud, O., & Oudeyer, P.-Y. (2022). Autotelic agents with intrinsically motivated goal-conditioned reinforcement learning: A short survey. J. Artif. Int. Res., 74. DOI: 10.1613/jair.1.13554.

Demonstration-conditioned reinforcement learning for few-shot imitation. C R Dance, J Perez, T Cachet, Proceedings of the 38th International Conference on Machine Learning. Meila, M., & Zhang, T.the 38th International Conference on Machine Learning139of Proceedings of Machine Learning ResearchDance, C. R., Perez, J., & Cachet, T. (2021). Demonstration-conditioned reinforcement learning for few-shot imitation. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th International Conference on Machine Learning, Vol. 139 of Proceedings of Machine Learning Research, pp. 2376-2387. PMLR. https://proceedings.mlr.press/v139/dance21a. html.

Imagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L Li, K Li, F Li, 10.1109/CVPR.2009.5206848IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Miami, Florida, USADeng, J., Dong, W., Socher, R., Li, L., Li, K., & Li, F. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pat- tern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pp. 248-255. IEEE Computer Society. DOI: 10.1109/CVPR.2009.5206848.

BERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human LanguageDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirec- tional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language

D4RL: datasets for deep data-driven reinforcement learning. J Fu, A Kumar, O Nachum, G Tucker, S Levine, abs/2004.07219CoRRFu, J., Kumar, A., Nachum, O., Tucker, G., & Levine, S. (2020). D4RL: datasets for deep data-driven reinforcement learning. CoRR, abs/2004.07219. https://arxiv.org/abs/2004. 07219.

Generalized decision transformer for offline hindsight information matching. H Furuta, Y Matsuo, S S Gu, International Conference on Learning Representations. Furuta, H., Matsuo, Y., & Gu, S. S. (2022). Generalized decision transformer for offline hindsight information matching. In International Conference on Learning Representations. https: //openreview.net/forum?id=CAjxVodl_v.

Hierarchical skills for efficient exploration. J Gehring, G Synnaeve, A Krause, N Usunier, Advances in Neural Information Processing Systems. 34Gehring, J., Synnaeve, G., Krause, A., & Usunier, N. (2021). Hierarchical skills for efficient explo- ration. Advances in Neural Information Processing Systems, 34, 11553-11564.

Deepmdp: Learning continuous latent space models for representation learning. C Gelada, S Kumar, J Buckman, O Nachum, M G Bellemare, Proceedings of the 36th International Conference on Machine Learning, ICML 2019. Chaudhuri, K., & Salakhutdinov, R.the 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97of Proceedings of Machine Learning ResearchGelada, C., Kumar, S., Buckman, J., Nachum, O., & Bellemare, M. G. (2019). Deepmdp: Learning continuous latent space models for representation learning. In Chaudhuri, K., & Salakhut- dinov, R. (Eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, Vol. 97 of Proceedings of Machine Learning Research, pp. 2170-2179. PMLR. http://proceedings.mlr. press/v97/gelada19a.html.

Equivalence notions and model minimization in markov decision processes. R Givan, T Dean, M Greig, Artificial Intelligence. 1471-2Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions and model minimization in markov decision processes. Artificial Intelligence, 147(1-2), 163-223.

Qualitatively characterizing neural network optimization problems. I J Goodfellow, O Vinyals, 3rd International Conference on Learning Representations. Bengio, Y., & LeCun, Y.San Diego, CA, USAConference Track ProceedingsGoodfellow, I. J., & Vinyals, O. (2015). Qualitatively characterizing neural network optimization problems. In Bengio, Y., & LeCun, Y. (Eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Pro- ceedings. http://arxiv.org/abs/1412.6544.

Variational intrinsic control. ArXiv preprint. K Gregor, D J Rezende, D Wierstra, abs/1611.07507Gregor, K., Rezende, D. J., & Wierstra, D. (2016). Variational intrinsic control. ArXiv preprint, abs/1611.07507. https://arxiv.org/abs/1611.07507.

. J Grill, F Strub, F Altché, C Tallec, P H Richemond, E Buchatskaya, C Doersch, B Á Pires, Z Guo, M G Azar, B Piot, K Kavukcuoglu, R Munos, M Valko, Grill, J., Strub, F., Altché, F., Tallec, C., Richemond, P. H., Buchatskaya, E., Doer- sch, C., Pires, B.Á., Guo, Z., Azar, M. G., Piot, B., Kavukcuoglu, K., Munos, R., & Valko, M. (2020).

Bootstrap your own latent -A new approach to selfsupervised learning. H Larochelle, M Ranzato, R Hadsell, M Balcan, Bootstrap your own latent -A new approach to self- supervised learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., &

Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Lin, H.2020Lin, H. (Eds.), Advances in Neural Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual. https://proceedings.neurips.cc/paper/2020/hash/ f3ada80d5c4ee70142b17b8192b2958e-Abstract.html.

Rl unplugged: A suite of benchmarks for offline reinforcement learning. C Gulcehre, Z Wang, A Novikov, T Paine, S Gómez, K Zolna, R Agarwal, J S Merel, D J Mankowitz, C Paduraru, G Dulac-Arnold, J Li, M Norouzi, M Hoffman, N Heess, N De Freitas, Advances in Neural Information Processing Systems. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H.Curran Associates, Inc33Gulcehre, C., Wang, Z., Novikov, A., Paine, T., Gómez, S., Zolna, K., Agarwal, R., Merel, J. S., Mankowitz, D. J., Paduraru, C., Dulac-Arnold, G., Li, J., Norouzi, M., Hoffman, M., Heess, N., & de Freitas, N. (2020). Rl unplugged: A suite of benchmarks for offline re- inforcement learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H. (Eds.), Advances in Neural Information Processing Systems, Vol. 33, pp. 7248-7259. Cur- ran Associates, Inc. https://proceedings.neurips.cc/paper/2020/file/ 51200d29d1fc15f5a71c1dab4bb54f7c-Paper.pdf.

Geometric entropic exploration. Z D Guo, M G Azar, A Saade, S Thakoor, B Piot, B A Pires, M Valko, T Mesnard, T Lattimore, R Munos, abs/2101.02055ArXiv preprintGuo, Z. D., Azar, M. G., Saade, A., Thakoor, S., Piot, B., Pires, B. A., Valko, M., Mesnard, T., Lattimore, T., & Munos, R. (2021). Geometric entropic exploration. ArXiv preprint, abs/2101.02055. https://arxiv.org/abs/2101.02055.

Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. A Gupta, V Kumar, C Lynch, S Levine, K Hausman, Proceedings of the Conference on Robot Learning. Kaelbling, L. P., Kragic, D., & Sugiura, K.the Conference on Robot Learning100of Proceedings of Machine Learning ResearchGupta, A., Kumar, V., Lynch, C., Levine, S., & Hausman, K. (2020). Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. In Kaelbling, L. P., Kragic, D., & Sugiura, K. (Eds.), Proceedings of the Conference on Robot Learning, Vol. 100 of Proceedings of Machine Learning Research, pp. 1025-1037. PMLR. https: //proceedings.mlr.press/v100/gupta20a.html.

Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. M Gutmann, A Hyvärinen, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. Teh, Y. W., & Titterington, M.the Thirteenth International Conference on Artificial Intelligence and StatisticsSardinia, Italy9Chia Laguna ResortProceedings of Machine Learning ResearchGutmann, M., & Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Teh, Y. W., & Titterington, M. (Eds.), Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, Vol. 9 of Proceedings of Machine Learning Research, pp. 297-304, Chia Laguna Resort, Sardinia, Italy. PMLR. https://proceedings.mlr.press/v9/gutmann10a.html.

World models. CoRR. D Ha, J Schmidhuber, abs/1803.10122Ha, D., & Schmidhuber, J. (2018). World models. CoRR, abs/1803.10122. http://arxiv. org/abs/1803.10122.

Learning to play with intrinsically-motivated, self-aware agents. N Haber, D Mrowca, S Wang, F Li, D L Yamins, S Bengio, H M Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing. & Garnett, R.NeurIPS; Montréal, CanadaHaber, N., Mrowca, D., Wang, S., Li, F., & Yamins, D. L. (2018). Learning to play with intrinsically-motivated, self-aware agents. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., & Garnett, R. (Eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Informa- tion Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 8398-8409. https://proceedings.neurips.cc/paper/2018/hash/ 71e63ef5b7249cfc60852f0e0f5bf4c8-Abstract.html.

Hierarchical few-shot imitation with skill transition models. K Hakhamaneshi, R Zhao, A Zhan, P Abbeel, M Laskin, International Conference on Learning Representations. Hakhamaneshi, K., Zhao, R., Zhan, A., Abbeel, P., & Laskin, M. (2022). Hierarchical few-shot imi- tation with skill transition models. In International Conference on Learning Representations. https://openreview.net/forum?id=xKZ4K0lTj_.

Fast task inference with variational intrinsic successor features. S Hansen, W Dabney, A Barreto, D Warde-Farley, T V De Wiele, V Mnih, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Hansen, S., Dabney, W., Barreto, A., Warde-Farley, D., de Wiele, T. V., & Mnih, V. (2020). Fast task inference with variational intrinsic successor features. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. Open- Review.net. https://openreview.net/forum?id=BJeAHkrYDS.

Provably efficient maximum entropy exploration. E Hazan, S M Kakade, K Singh, A V Soest, Proceedings of the 36th International Conference on Machine Learning, ICML 2019. Chaudhuri, K., & Salakhutdinov, R.the 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97of Proceedings of Machine Learning ResearchHazan, E., Kakade, S. M., Singh, K., & Soest, A. V. (2019). Provably efficient maximum entropy exploration. In Chaudhuri, K., & Salakhutdinov, R. (Eds.), Proceedings of the 36th Inter- national Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali- fornia, USA, Vol. 97 of Proceedings of Machine Learning Research, pp. 2681-2691. PMLR. http://proceedings.mlr.press/v97/hazan19a.html.

Towards a unified view of parameter-efficient transfer learning. J He, C Zhou, X Ma, T Berg-Kirkpatrick, G Neubig, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., & Neubig, G. (2022). Towards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https: //openreview.net/forum?id=0RDcd5Axok.

Momentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R B Girshick, 10.1109/CVPR42600.2020.009752020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. B. (2020). Momentum contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 9726-9735. IEEE. DOI: 10.1109/CVPR42600.2020.00975.

Pretrained transformers improve out-of-distribution robustness. D Hendrycks, X Liu, E Wallace, A Dziedzic, R Krishnan, D Song, 10.18653/v1/2020.acl-main.244Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsHendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan, R., & Song, D. (2020a). Pretrained transformers improve out-of-distribution robustness. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pp. 2744-2751, Online. Association for Computational Linguistics. DOI: 10.18653/v1/2020.acl-main.244.

Augmix: A simple data processing method to improve robustness and uncertainty. D Hendrycks, N Mu, E D Cubuk, B Zoph, J Gilmer, B Lakshminarayanan, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Hendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J., & Lakshminarayanan, B. (2020b). Aug- mix: A simple data processing method to improve robustness and uncertainty. In 8th Interna- tional Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26- 30, 2020. OpenReview.net. https://openreview.net/forum?id=S1gmrxHFvB.

Parameter-efficient transfer learning for NLP. N Houlsby, A Giurgiu, S Jastrzebski, B Morrone, Q De Laroussilhe, A Gesmundo, M Attariyan, S Gelly, Proceedings of the 36th International Conference on Machine Learning. Chaudhuri, K., & Salakhutdinov, R.the 36th International Conference on Machine Learning97of Proceedings of Machine Learning ResearchHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., & Gelly, S. (2019). Parameter-efficient transfer learning for NLP. In Chaudhuri, K., & Salakhutdinov, R. (Eds.), Proceedings of the 36th International Conference on Machine Learning, Vol. 97 of Proceedings of Machine Learning Research, pp. 2790-2799. PMLR. https://proceedings.mlr.press/v97/houlsby19a.html.

VIME: variational information maximizing exploration. R Houthooft, X Chen, Y Duan, J Schulman, F D Turck, P Abbeel, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems. Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., & Garnett, R.Barcelona, SpainHouthooft, R., Chen, X., Duan, Y., Schulman, J., Turck, F. D., & Abbeel, P. (2016). VIME: vari- ational information maximizing exploration. In Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., & Garnett, R. (Eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 1109-1117. https://proceedings.neurips.cc/paper/ 2016/hash/abd815286ba1007abfbb8415b83ae2cf-Abstract.html.

Adarl: What, where, and how to adapt in transfer reinforcement learning. B Huang, F Feng, C Lu, S Magliacane, K Zhang, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. Huang, B., Feng, F., Lu, C., Magliacane, S., & Zhang, K. (2022a). Adarl: What, where, and how to adapt in transfer reinforcement learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https: //openreview.net/forum?id=8H5bpVwvt5.

Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, abs/2207.05608ArXiv preprintHuang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., et al. (2022b). Inner monologue: Embodied reasoning through planning with language models. ArXiv preprint, abs/2207.05608. https://arxiv.org/abs/2207. 05608.

Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. E Hüllermeier, W Waegeman, Machine Learning. 110Hüllermeier, E., & Waegeman, W. (2021). Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. Machine Learning, 110(3), 457-506.

Perceiver IO: A general architecture for structured inputs & outputs. A Jaegle, S Borgeaud, J.-B Alayrac, C Doersch, C Ionescu, D Ding, S Koppula, D Zoran, A Brock, E Shelhamer, O J Henaff, M Botvinick, A Zisserman, O Vinyals, J Carreira, International Conference on Learning Representations. Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ding, D., Koppula, S., Zoran, D., Brock, A., Shelhamer, E., Henaff, O. J., Botvinick, M., Zisserman, A., Vinyals, O., & Carreira, J. (2022). Perceiver IO: A general architecture for structured inputs & outputs. In International Conference on Learning Representations. https://openreview.net/ forum?id=fILj7WpI-g.

Offline reinforcement learning as one big sequence modeling problem. M Janner, Q Li, S Levine, Advances in neural information processing systems. 34Janner, M., Li, Q., & Levine, S. (2021). Offline reinforcement learning as one big sequence model- ing problem. Advances in neural information processing systems, 34, 1273-1286.

Filtered-cophy: Unsupervised learning of counterfactual physics in pixel space. S Janny, F Baradel, N Neverova, M Nadri, G Mori, C Wolf, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. Janny, S., Baradel, F., Neverova, N., Nadri, M., Mori, G., & Wolf, C. (2022). Filtered-cophy: Unsupervised learning of counterfactual physics in pixel space. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. Open- Review.net. https://openreview.net/forum?id=1L0C5ROtFp.

Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. C Jin, Q Liu, S Miryoosefi, A Beygelzimer, Y Dauphin, P Liang, Vaughan, Advances in Neural Information Processing Systems. J. W.Curran Associates, Inc34Jin, C., Liu, Q., & Miryoosefi, S. (2021). Bellman eluder dimension: New rich classes of rl prob- lems, and sample-efficient algorithms. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., & Vaughan, J. W. (Eds.), Advances in Neural Information Processing Systems, Vol. 34, pp. 13406-13418. Curran Associates, Inc. https://proceedings.neurips.cc/ paper/2021/file/6f5e4e86a87220e5d361ad82f1ebc335-Paper.pdf.

Model based reinforcement learning for atari. L Kaiser, M Babaeizadeh, P Milos, B Osinski, R H Campbell, K Czechowski, D Erhan, C Finn, P Kozakowski, S Levine, A Mohiuddin, R Sepassi, G Tucker, H Michalewski, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020OpenReviewKaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., Mohiuddin, A., Sepassi, R., Tucker, G., & Michalewski, H. (2020). Model based reinforcement learning for atari. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe- view.net. https://openreview.net/forum?id=S1xCPJHtDB.

Direct then diffuse: Incremental unsupervised skill discovery for state covering and goal reaching. P.-A Kamienny, J Tarbouriech, A Lazaric, L Denoyer, International Conference on Learning Representations. Kamienny, P.-A., Tarbouriech, J., Lazaric, A., & Denoyer, L. (2022). Direct then diffuse: Incre- mental unsupervised skill discovery for state covering and goal reaching. In International Conference on Learning Representations. https://openreview.net/forum?id= 25kzAhUB1lz.

Scaling laws for neural language models. J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, abs/2001.08361CoRRKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Rad- ford, A., Wu, J., & Amodei, D. (2020). Scaling laws for neural language models. CoRR, abs/2001.08361. https://arxiv.org/abs/2001.08361.

Human-level atari 200x faster. S Kapturowski, V Campos, R Jiang, N Rakićević, H Van Hasselt, C Blundell, A P Badia, 10.48550/ARXIV.2209.07550Kapturowski, S., Campos, V., Jiang, R., Rakićević, N., van Hasselt, H., Blundell, C., & Badia, A. P. (2022). Human-level atari 200x faster.. DOI: 10.48550/ARXIV.2209.07550.

Towards continual reinforcement learning: A review and perspectives. K Khetarpal, M Riemer, I Rish, D Precup, abs/2012.13490CoRRKhetarpal, K., Riemer, M., Rish, I., & Precup, D. (2020). Towards continual reinforcement learning: A review and perspectives. CoRR, abs/2012.13490. https://arxiv.org/abs/2012. 13490.

Auto-encoding variational bayes. D P Kingma, M Welling, 2nd International Conference on Learning Representations. Bengio, Y., & LeCun, Y.Banff, AB, CanadaConference Track ProceedingsKingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. In Bengio, Y., & LeCun, Y. (Eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings. http://arxiv.org/abs/ 1312.6114.

Compile: Compositional imitation learning and execution. T Kipf, Y Li, H Dai, V F Zambaldi, A Sanchez-Gonzalez, E Grefenstette, P Kohli, P W Battaglia, Proceedings of the 36th International Conference on Machine Learning, ICML 2019. Chaudhuri, K., & Salakhutdinov, R.the 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97of Proceedings of Machine Learning ResearchKipf, T., Li, Y., Dai, H., Zambaldi, V. F., Sanchez-Gonzalez, A., Grefenstette, E., Kohli, P., & Battaglia, P. W. (2019). Compile: Compositional imitation learning and execution. In Chaudhuri, K., & Salakhutdinov, R. (Eds.), Proceedings of the 36th International Confer- ence on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, Vol. 97 of Proceedings of Machine Learning Research, pp. 3418-3428. PMLR. http: //proceedings.mlr.press/v97/kipf19a.html.

A survey of generalisation in deep reinforcement learning. R Kirk, A Zhang, E Grefenstette, T Rocktäschel, abs/2111.09794CoRRKirk, R., Zhang, A., Grefenstette, E., & Rocktäschel, T. (2021). A survey of generalisation in deep reinforcement learning. CoRR, abs/2111.09794. https://arxiv.org/abs/2111. 09794.

Toward understanding natural language directions. T Kollar, S Tellex, D Roy, N Roy, 10.1109/HRI.2010.54531865th ACM/IEEE International Conference on Human-Robot Interaction (HRI). Kollar, T., Tellex, S., Roy, D., & Roy, N. (2010). Toward understanding natural language directions. In 2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 259-266. DOI: 10.1109/HRI.2010.5453186.

Nonparametric entropy estimation for stationary processes and random fields, with applications to english text. I Kontoyiannis, P Algoet, Y Suhov, A Wyner, 10.1109/18.669425IEEE Transactions on Information Theory. 443Kontoyiannis, I., Algoet, P., Suhov, Y., & Wyner, A. (1998). Nonparametric entropy estimation for stationary processes and random fields, with applications to english text. IEEE Transactions on Information Theory, 44(3), 1319-1327. DOI: 10.1109/18.669425.

Offline reinforcement learning with implicit q-learning. I Kostrikov, A Nair, S Levine, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. Kostrikov, I., Nair, A., & Levine, S. (2022). Offline reinforcement learning with implicit q-learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.net/forum?id= 68n2s9ZJWF8.

Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. T D Kulkarni, K Narasimhan, A Saeedi, J Tenenbaum, Advances in Neural Information Processing Systems. Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., & Garnett, R.Curran Associates, Inc29Kulkarni, T. D., Narasimhan, K., Saeedi, A., & Tenenbaum, J. (2016). Hierar- chical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., & Garnett, R. (Eds.), Advances in Neural Information Processing Systems, Vol. 29. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2016/file/ f442d33fa06832082290ad8544a8da27-Paper.pdf.

Stabilizing off-policy q-learning via bootstrapping error reduction. A Kumar, J Fu, M Soh, G Tucker, S Levine, Advances in Neural Information Processing Systems. Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., & Garnett, R.Curran Associates, Inc32Kumar, A., Fu, J., Soh, M., Tucker, G., & Levine, S. (2019). Stabilizing off-policy q-learning via bootstrapping error reduction. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché- Buc, F., Fox, E., & Garnett, R. (Eds.), Advances in Neural Information Processing Systems, Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/ 2019/file/c2073ffa77b5357a498057413bb09d3a-Paper.pdf.

Conservative q-learning for offline reinforcement learning. A Kumar, A Zhou, G Tucker, S Levine, H Larochelle, M Ranzato, R Hadsell, M Balcan, Advances in Neural Information Processing Systems. & Lin, H.Curran Associates, Inc33Kumar, A., Zhou, A., Tucker, G., & Levine, S. (2020). Conservative q-learning for offline re- inforcement learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H. (Eds.), Advances in Neural Information Processing Systems, Vol. 33, pp. 1179-1191. Cur- ran Associates, Inc. https://proceedings.neurips.cc/paper/2020/file/ 0d2b2061826a5df3221116a5085a6052-Paper.pdf.

The challenges of exploration for offline reinforcement learning. N Lambert, M Wulfmeier, W Whitney, A Byravan, M Bloesch, V Dasagi, T Hertweck, M Riedmiller, abs/2201.11861ArXiv preprintLambert, N., Wulfmeier, M., Whitney, W., Byravan, A., Bloesch, M., Dasagi, V., Hertweck, T., & Riedmiller, M. (2022). The challenges of exploration for offline reinforcement learning. ArXiv preprint, abs/2201.11861. https://arxiv.org/abs/2201.11861.

Batch reinforcement learning. S Lange, T Gabel, M Riedmiller, Reinforcement learning. SpringerLange, S., Gabel, T., & Riedmiller, M. (2012). Batch reinforcement learning. In Reinforcement learning, pp. 45-73. Springer.

Deep auto-encoder neural networks in reinforcement learning. S Lange, M A Riedmiller, 10.1109/IJCNN.2010.5596468International Joint Conference on Neural Networks, IJCNN 2010. Barcelona, SpainIEEELange, S., & Riedmiller, M. A. (2010). Deep auto-encoder neural networks in reinforcement learn- ing. In International Joint Conference on Neural Networks, IJCNN 2010, Barcelona, Spain, 18-23 July, 2010, pp. 1-8. IEEE. DOI: 10.1109/IJCNN.2010.5596468.

Bisimulation through probabilistic testing. Information and computation. K G Larsen, A Skou, 94Larsen, K. G., & Skou, A. (1991). Bisimulation through probabilistic testing. Information and computation, 94(1), 1-28.

CIC: contrastive intrinsic control for unsupervised skill discovery. M Laskin, H Liu, X B Peng, D Yarats, A Rajeswaran, P Abbeel, abs/2202.00161CoRRLaskin, M., Liu, H., Peng, X. B., Yarats, D., Rajeswaran, A., & Abbeel, P. (2022). CIC: con- trastive intrinsic control for unsupervised skill discovery. CoRR, abs/2202.00161. https: //arxiv.org/abs/2202.00161.

CURL: contrastive unsupervised representations for reinforcement learning. M Laskin, A Srinivas, P Abbeel, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020of Proceedings of Machine Learning ResearchLaskin, M., Srinivas, A., & Abbeel, P. (2020). CURL: contrastive unsupervised representations for reinforcement learning. In Proceedings of the 37th International Conference on Ma- chine Learning, ICML 2020, 13-18 July 2020, Virtual Event, Vol. 119 of Proceedings of Machine Learning Research, pp. 5639-5650. PMLR. http://proceedings.mlr. press/v119/laskin20a.html.

Urlb: Unsupervised reinforcement learning benchmark. M Laskin, D Yarats, H Liu, K Lee, A Zhan, K Lu, C Cang, L Pinto, P Abbeel, abs/2110.15191ArXiv preprintLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., & Abbeel, P. (2021). Urlb: Unsupervised reinforcement learning benchmark. ArXiv preprint, abs/2110.15191. https://arxiv.org/abs/2110.15191.

A tutorial on energy-based learning. Y Lecun, S Chopra, R Hadsell, M Ranzato, F Huang, Predicting structured data. 01LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). A tutorial on energy-based learning. Predicting structured data, 1(0).

Multi-game decision transformers. K.-H Lee, O Nachum, M Yang, L Lee, D Freeman, W Xu, S Guadarrama, I Fischer, E Jang, H Michalewski, abs/2205.15241ArXiv preprintLee, K.-H., Nachum, O., Yang, M., Lee, L., Freeman, D., Xu, W., Guadarrama, S., Fischer, I., Jang, E., Michalewski, H., et al. (2022). Multi-game decision transformers. ArXiv preprint, abs/2205.15241. https://arxiv.org/abs/2205.15241.

Efficient exploration via state marginal matching. ArXiv preprint, abs/1906. L Lee, B Eysenbach, E Parisotto, E Xing, S Levine, R Salakhutdinov, Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., & Salakhutdinov, R. (2019). Effi- cient exploration via state marginal matching. ArXiv preprint, abs/1906.05274. https: //arxiv.org/abs/1906.05274.

Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble. S Lee, Y Seo, K Lee, P Abbeel, J Shin, PMLRConference on Robot Learning. Lee, S., Seo, Y., Lee, K., Abbeel, P., & Shin, J. (2022). Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble. In Conference on Robot Learning, pp. 1702- 1712. PMLR.

The power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, 10.18653/v1/2021.emnlp-main.243Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican RepublicAssociation for Computational LinguisticsLester, B., Al-Rfou, R., & Constant, N. (2021). The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045-3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. DOI: 10.18653/v1/2021.emnlp-main.243.

Understanding the world through action. S Levine, 5th Annual Conference on Robot Learning, Blue Sky Submission Track. Levine, S. (2021). Understanding the world through action. In 5th Annual Conference on Robot Learning, Blue Sky Submission Track. https://openreview.net/forum?id= L55-yn1iwrm.

Offline reinforcement learning: Tutorial, review, and perspectives on open problems. S Levine, A Kumar, G Tucker, J Fu, abs/2005.01643ArXiv preprintLevine, S., Kumar, A., Tucker, G., & Fu, J. (2020). Offline reinforcement learning: Tutorial, review, and perspectives on open problems. ArXiv preprint, abs/2005.01643. https://arxiv. org/abs/2005.01643.

Celebrating diversity in shared multi-agent reinforcement learning. C Li, T Wang, C Wu, Q Zhao, J Yang, C Zhang, Advances in Neural Information Processing Systems. Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., & Vaughan, J. W.Curran Associates, Inc34Li, C., Wang, T., Wu, C., Zhao, Q., Yang, J., & Zhang, C. (2021). Celebrating diversity in shared multi-agent reinforcement learning. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., & Vaughan, J. W. (Eds.), Advances in Neural Information Processing Systems, Vol. 34, pp. 3991-4002. Curran Associates, Inc. https://proceedings.neurips.cc/paper/ 2021/file/20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf.

Towards a unified theory of state abstraction for mdps. L Li, T J Walsh, M L Littman, AI&M. Li, L., Walsh, T. J., & Littman, M. L. (2006). Towards a unified theory of state abstraction for mdps.. In AI&M.

Composing ensembles of pre-trained models via iterative consensus. S Li, Y Du, J B Tenenbaum, A Torralba, I Mordatch, 10.48550/arXiv.2210.11522CoRRLi, S., Du, Y., Tenenbaum, J. B., Torralba, A., & Mordatch, I. (2022a). Compos- ing ensembles of pre-trained models via iterative consensus. CoRR, abs/2210.11522. DOI: 10.48550/arXiv.2210.11522.

Pre-trained language models for interactive decision-making. S Li, X Puig, Y Du, C Wang, E Akyurek, A Torralba, J Andreas, I Mordatch, abs/2202.01771ArXiv preprintLi, S., Puig, X., Du, Y., Wang, C., Akyurek, E., Torralba, A., Andreas, J., & Mordatch, I. (2022b). Pre-trained language models for interactive decision-making. ArXiv preprint, abs/2202.01771. https://arxiv.org/abs/2202.01771.

APS: active pretraining with successor features. H Liu, P Abbeel, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Meila, M., & Zhang, T.the 38th International Conference on Machine Learning, ICML 2021139of Proceedings of Machine Learning ResearchLiu, H., & Abbeel, P. (2021a). APS: active pretraining with successor features. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, Vol. 139 of Proceedings of Machine Learn- ing Research, pp. 6736-6747. PMLR. http://proceedings.mlr.press/v139/ liu21b.html.

Behavior from the void: Unsupervised active pre-training. H Liu, P Abbeel, Advances in Neural Information Processing Systems. 34Liu, H., & Abbeel, P. (2021b). Behavior from the void: Unsupervised active pre-training. Advances in Neural Information Processing Systems, 34, 18459-18473.

Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, Advances in Neural Information Processing Systems. Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., & Garnett, R.Curran Associates, Inc32Lu, J., Batra, D., Parikh, D., & Lee, S. (2019). Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., & Garnett, R. (Eds.), Advances in Neural Information Process- ing Systems, Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/ paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf.

Pretrained transformers as universal computation engines. K Lu, A Grover, P Abbeel, I Mordatch, abs/2103.05247ArXiv preprintLu, K., Grover, A., Abbeel, P., & Mordatch, I. (2021a). Pretrained transformers as universal com- putation engines. ArXiv preprint, abs/2103.05247. https://arxiv.org/abs/2103.

Reset-free lifelong learning with skillspace planning. K Lu, A Grover, P Abbeel, I Mordatch, International Conference on Learning Representations. Lu, K., Grover, A., Abbeel, P., & Mordatch, I. (2021b). Reset-free lifelong learning with skill- space planning. In International Conference on Learning Representations. https:// openreview.net/forum?id=HIGSa_3kOx3.

Aw-opt: Learning robotic skills with imitation and reinforcement at scale. Y Lu, K Hausman, Y Chebotar, M Yan, E Jang, A Herzog, T Xiao, A Irpan, M Khansari, D Kalashnikov, S Levine, Proceedings of the 5th Conference on Robot Learning. Faust, A., Hsu, D., & Neumann, G.the 5th Conference on Robot Learning164of Proceedings of Machine Learning ResearchLu, Y., Hausman, K., Chebotar, Y., Yan, M., Jang, E., Herzog, A., Xiao, T., Irpan, A., Khansari, M., Kalashnikov, D., & Levine, S. (2022). Aw-opt: Learning robotic skills with imitation and reinforcement at scale. In Faust, A., Hsu, D., & Neumann, G. (Eds.), Proceedings of the 5th Conference on Robot Learning, Vol. 164 of Proceedings of Machine Learning Research, pp. 1078-1088. PMLR. https://proceedings.mlr.press/v164/lu22a.html.

Learning latent plans from play. C Lynch, M Khansari, T Xiao, V Kumar, J Tompson, S Levine, P Sermanet, PMLRConference on robot learning. Lynch, C., Khansari, M., Xiao, T., Kumar, V., Tompson, J., Levine, S., & Sermanet, P. (2020). Learning latent plans from play. In Conference on robot learning, pp. 1113-1132. PMLR.

Zero-shot reward specification via grounded natural language. P Mahmoudieh, D Pathak, T ; K Darrell, S Jegelka, L Song, C Szepesvari, G Niu, Proceedings of the 39th International Conference on Machine Learning. & Sabato, S.the 39th International Conference on Machine Learning162of Proceedings of Machine Learning ResearchMahmoudieh, P., Pathak, D., & Darrell, T. (2022). Zero-shot reward specification via grounded natural language. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., & Sabato, S. (Eds.), Proceedings of the 39th International Conference on Machine Learn- ing, Vol. 162 of Proceedings of Machine Learning Research, pp. 14743-14752. PMLR. https://proceedings.mlr.press/v162/mahmoudieh22a.html.

How to stay curious while avoiding noisy tvs using aleatoric uncertainty estimation. A Mavor-Parker, K Young, C Barry, L Griffin, PMLRInternational Conference on Machine Learning. Mavor-Parker, A., Young, K., Barry, C., & Griffin, L. (2022). How to stay curious while avoiding noisy tvs using aleatoric uncertainty estimation. In International Conference on Machine Learning, pp. 15220-15240. PMLR.

Deep reinforcement and infomax learning. B Mazoure, R T Des Combes, T Doan, P Bachman, R D Hjelm, H Larochelle, M Ranzato, R Hadsell, M Balcan, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. & Lin, H.2020Mazoure, B., des Combes, R. T., Doan, T., Bachman, P., & Hjelm, R. D. (2020). Deep re- inforcement and infomax learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H. (Eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. https://proceedings.neurips.cc/paper/2020/hash/ 26588e932c7ccfa1df309280702fe1b5-Abstract.html.

Offline pre-trained multi-agent decision transformer: One big sequence model tackles all SMAC tasks. L Meng, M Wen, Y Yang, C Le, X Li, W Zhang, Y Wen, H Zhang, J Wang, B Xu, abs/2112.02845CoRRMeng, L., Wen, M., Yang, Y., Le, C., Li, X., Zhang, W., Wen, Y., Zhang, H., Wang, J., & Xu, B. (2021). Offline pre-trained multi-agent decision transformer: One big sequence model tackles all SMAC tasks. CoRR, abs/2112.02845. https://arxiv.org/abs/2112.02845.

Kinematic state abstraction and provably efficient rich-observation reinforcement learning. D Misra, M Henaff, A Krishnamurthy, J Langford, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020of Proceedings of Machine Learning ResearchMisra, D., Henaff, M., Krishnamurthy, A., & Langford, J. (2020). Kinematic state abstrac- tion and provably efficient rich-observation reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Vir- tual Event, Vol. 119 of Proceedings of Machine Learning Research, pp. 6961-6971. PMLR. http://proceedings.mlr.press/v119/misra20a.html.

Multimodal contrastive learning with limoe: the language-image mixture of experts. B Mustafa, C Riquelme, J Puigcerver, R Jenatton, N Houlsby, 10.48550/arXiv.2206.02770CoRRMustafa, B., Riquelme, C., Puigcerver, J., Jenatton, R., & Houlsby, N. (2022). Multimodal con- trastive learning with limoe: the language-image mixture of experts. CoRR, abs/2206.02770. DOI: 10.48550/arXiv.2206.02770.

A policy gradient method for task-agnostic exploration. M Mutti, L Pratissoli, M Restelli, 4th Lifelong Machine Learning Workshop at ICML 2020. Mutti, M., Pratissoli, L., & Restelli, M. (2020). A policy gradient method for task-agnostic exploration. In 4th Lifelong Machine Learning Workshop at ICML 2020. https:// openreview.net/forum?id=d9j_RNHtQEo.

Near-optimal representation learning for hierarchical reinforcement learning. O Nachum, S Gu, H Lee, S Levine, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USANachum, O., Gu, S., Lee, H., & Levine, S. (2019). Near-optimal representation learning for hi- erarchical reinforcement learning. In 7th International Conference on Learning Represen- tations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https: //openreview.net/forum?id=H1emus0qF7.

Accelerating online reinforcement learning with offline datasets. A Nair, M Dalal, A Gupta, S Levine, abs/2006.09359ArXiv preprintNair, A., Dalal, M., Gupta, A., & Levine, S. (2020). Accelerating online reinforcement learning with offline datasets. ArXiv preprint, abs/2006.09359. https://arxiv.org/abs/2006.

Emergent social learning via multiagent reinforcement learning. K K Ndousse, D Eck, S Levine, N Jaques, Proceedings of the 38th International Conference on Machine Learning. Meila, M., & Zhang, T.the 38th International Conference on Machine Learning139of Proceedings of Machine Learning ResearchNdousse, K. K., Eck, D., Levine, S., & Jaques, N. (2021). Emergent social learning via multi- agent reinforcement learning. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th International Conference on Machine Learning, Vol. 139 of Proceedings of Machine Learn- ing Research, pp. 7991-8004. PMLR. https://proceedings.mlr.press/v139/ ndousse21a.html.

Count-based exploration with neural density models. G Ostrovski, M G Bellemare, A Van Den Oord, R Munos, Proceedings of the 34th International Conference on Machine Learning. Precup, D., & Teh, Y. W.the 34th International Conference on Machine LearningSydney, NSW, Australia70of Proceedings of Machine Learning ResearchOstrovski, G., Bellemare, M. G., van den Oord, A., & Munos, R. (2017). Count-based exploration with neural density models. In Precup, D., & Teh, Y. W. (Eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, Vol. 70 of Proceedings of Machine Learning Research, pp. 2721-2730. PMLR. http://proceedings.mlr.press/v70/ostrovski17a.html.

Intrinsic motivation systems for autonomous mental development. P.-Y Oudeyer, F Kaplan, V V Hafner, IEEE transactions on evolutionary computation. 112Oudeyer, P.-Y., Kaplan, F., & Hafner, V. V. (2007). Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 11(2), 265-286.

The unsurprising effectiveness of pre-trained vision models for control. S Parisi, A Rajeswaran, S Purushwalkam, A Gupta, K Chaudhuri, S Jegelka, L Song, C Szepesvari, G Niu, Proceedings of the 39th International Conference on Machine Learning. & Sabato, S.the 39th International Conference on Machine Learning162of Proceedings of Machine Learning ResearchParisi, S., Rajeswaran, A., Purushwalkam, S., & Gupta, A. (2022). The unsurprising effectiveness of pre-trained vision models for control. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., & Sabato, S. (Eds.), Proceedings of the 39th International Conference on Machine Learning, Vol. 162 of Proceedings of Machine Learning Research, pp. 17359-17371. PMLR. https://proceedings.mlr.press/v162/parisi22a.html.

Lipschitz-constrained unsupervised skill discovery. S Park, J Choi, J Kim, H Lee, G Kim, International Conference on Learning Representations. Park, S., Choi, J., Kim, J., Lee, H., & Kim, G. (2022). Lipschitz-constrained unsupervised skill discovery. In International Conference on Learning Representations. https:// openreview.net/forum?id=BGvt0ghNgA.

Curiosity-driven exploration by selfsupervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, Proceedings of the 34th International Conference on Machine Learning. Precup, D., & Teh, Y. W.the 34th International Conference on Machine LearningSydney, NSW, Australia, 6-11 Au70of Proceedings of Machine Learning ResearchPathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017). Curiosity-driven exploration by self- supervised prediction. In Precup, D., & Teh, Y. W. (Eds.), Proceedings of the 34th Inter- national Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 Au- gust 2017, Vol. 70 of Proceedings of Machine Learning Research, pp. 2778-2787. PMLR. http://proceedings.mlr.press/v70/pathak17a.html.

Self-supervised exploration via disagreement. D Pathak, D Gandhi, A Gupta, Proceedings of the 36th International Conference on Machine Learning, ICML 2019. Chaudhuri, K., & Salakhutdinov, R.the 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97of Proceedings of Machine Learning ResearchPathak, D., Gandhi, D., & Gupta, A. (2019). Self-supervised exploration via disagreement. In Chaudhuri, K., & Salakhutdinov, R. (Eds.), Proceedings of the 36th International Confer- ence on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, Vol. 97 of Proceedings of Machine Learning Research, pp. 5062-5071. PMLR. http: //proceedings.mlr.press/v97/pathak19a.html.

Accelerating reinforcement learning with learned skill priors. K Pertsch, Y Lee, J Lim, Proceedings of the 2020 Conference on Robot Learning. Kober, J., Ramos, F., & Tomlin, C.the 2020 Conference on Robot Learning155of Proceedings of Machine Learning ResearchPertsch, K., Lee, Y., & Lim, J. (2021a). Accelerating reinforcement learning with learned skill priors. In Kober, J., Ramos, F., & Tomlin, C. (Eds.), Proceedings of the 2020 Conference on Robot Learning, Vol. 155 of Proceedings of Machine Learning Research, pp. 188-204. PMLR. https://proceedings.mlr.press/v155/pertsch21a.html.

Demonstration-guided reinforcement learning with learned skills. K Pertsch, Y Lee, Y Wu, J J Lim, 5th Annual Conference on Robot Learning. Pertsch, K., Lee, Y., Wu, Y., & Lim, J. J. (2021b). Demonstration-guided reinforcement learn- ing with learned skills. In 5th Annual Conference on Robot Learning. https:// openreview.net/forum?id=JSC4KMlENqF.

Alvinn: An autonomous land vehicle in a neural network. D A Pomerleau, Advances in neural information processing systems. 1Pomerleau, D. A. (1988). Alvinn: An autonomous land vehicle in a neural network. Advances in neural information processing systems, 1.

Skew-fit: State-covering self-supervised reinforcement learning. V Pong, M Dalal, S Lin, A Nair, S Bahl, S Levine, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020of Proceedings of Machine Learning ResearchPong, V., Dalal, M., Lin, S., Nair, A., Bahl, S., & Levine, S. (2020). Skew-fit: State-covering self-supervised reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, Vol. 119 of Proceedings of Machine Learning Research, pp. 7783-7792. PMLR. http://proceedings.mlr. press/v119/pong20a.html.

On variational bounds of mutual information. B Poole, S Ozair, A Van Den Oord, A Alemi, G Tucker, Proceedings of the 36th International Conference on Machine Learning. Chaudhuri, K., & Salakhutdinov, R.the 36th International Conference on Machine Learning97of Proceedings of Machine Learning ResearchPoole, B., Ozair, S., Van Den Oord, A., Alemi, A., & Tucker, G. (2019). On variational bounds of mutual information. In Chaudhuri, K., & Salakhutdinov, R. (Eds.), Proceedings of the 36th International Conference on Machine Learning, Vol. 97 of Proceedings of Machine Learn- ing Research, pp. 5171-5180. PMLR. https://proceedings.mlr.press/v97/ poole19a.html.

Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Meila, M., & Zhang, T.the 38th International Conference on Machine Learning, ICML 2021139of Proceedings of Machine Learning ResearchRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning transferable visual models from natural language supervision. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, Vol. 139 of Proceedings of Machine Learning Research, pp. 8748-8763. PMLR. http: //proceedings.mlr.press/v139/radford21a.html.

A call to build models like we build opensource software. C Raffel, Raffel, C. (2021). A call to build models like we build open- source software..

Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. A Rajeswaran, V Kumar, A Gupta, G Vezzani, J Schulman, E Todorov, S Levine, 10.15607/RSS.2018.XIV.049Robotics: Science and Systems XIV. Kress-Gazit, H., Srinivasa, S. S., Howard, T., & Atanasov, N.Pittsburgh, Pennsylvania, USACarnegie Mellon UniversityRajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E., & Levine, S. (2018). Learning complex dexterous manipulation with deep reinforcement learning and demonstra- tions. In Kress-Gazit, H., Srinivasa, S. S., Howard, T., & Atanasov, N. (Eds.), Robotics: Science and Systems XIV, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA, June 26-30, 2018. DOI: 10.15607/RSS.2018.XIV.049.

Which mutual-information representation learning objectives are sufficient for control. K Rakelly, A Gupta, C Florensa, S Levine, Advances in Neural Information Processing Systems. 34Rakelly, K., Gupta, A., Florensa, C., & Levine, S. (2021). Which mutual-information representation learning objectives are sufficient for control?. Advances in Neural Information Processing Systems, 34, 26345-26357.

Optimistic exploration even with a pessimistic initialisation. T Rashid, B Peng, W Boehmer, S Whiteson, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Rashid, T., Peng, B., Boehmer, W., & Whiteson, S. (2020). Optimistic exploration even with a pessimistic initialisation. In 8th International Conference on Learning Representa- tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https: //openreview.net/forum?id=r1xGP6VYwH.

. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, abs/2205.06175A generalist agent. ArXiv preprintReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al. (2022). A generalist agent. ArXiv preprint, abs/2205.06175. https://arxiv.org/abs/2205.06175.

Can wikipedia help offline reinforcement learning. M Reid, Y Yamada, S S Gu, abs/2201.12122ArXiv preprint. Reid, M., Yamada, Y., & Gu, S. S. (2022). Can wikipedia help offline reinforcement learning?. ArXiv preprint, abs/2201.12122. https://arxiv.org/abs/2201.12122.

A reduction of imitation learning and structured prediction to no-regret online learning. S Ross, G Gordon, D Bagnell, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. Gordon, G., Dunson, D., & Dudík, M.the Fourteenth International Conference on Artificial Intelligence and StatisticsFort Lauderdale, FL, USA15of Proceedings of Machine Learning ResearchRoss, S., Gordon, G., & Bagnell, D. (2011). A reduction of imitation learning and structured predic- tion to no-regret online learning. In Gordon, G., Dunson, D., & Dudík, M. (Eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, Vol. 15 of Proceedings of Machine Learning Research, pp. 627-635, Fort Lauderdale, FL, USA. PMLR. https://proceedings.mlr.press/v15/ross11a.html.

A A Rusu, N C Rabinowitz, G Desjardins, H Soyer, J Kirkpatrick, K Kavukcuoglu, R Pascanu, R Hadsell, Progressive neural networks. CoRR, abs/1606.04671. Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pas- canu, R., & Hadsell, R. (2016). Progressive neural networks. CoRR, abs/1606.04671. http://arxiv.org/abs/1606.04671.

Curious model-building control systems. J Schmidhuber, Proc. international joint conference on neural networks. international joint conference on neural networksSchmidhuber, J. (1991). Curious model-building control systems. In Proc. international joint con- ference on neural networks, pp. 1458-1463.

Kickstarting deep reinforcement learning. S Schmitt, J J Hudson, A Zídek, S Osindero, C Doersch, W M Czarnecki, J Z Leibo, H Küttler, A Zisserman, K Simonyan, S M A Eslami, abs/1803.03835CoRRSchmitt, S., Hudson, J. J., Zídek, A., Osindero, S., Doersch, C., Czarnecki, W. M., Leibo, J. Z., Küttler, H., Zisserman, A., Simonyan, K., & Eslami, S. M. A. (2018). Kickstarting deep rein- forcement learning. CoRR, abs/1803.03835. http://arxiv.org/abs/1803.03835.

Dataefficient reinforcement learning with self-predictive representations. M Schwarzer, A Anand, R Goel, R D Hjelm, A C Courville, P Bachman, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaSchwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville, A. C., & Bachman, P. (2021a). Data- efficient reinforcement learning with self-predictive representations. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=uCQfPZwRaUu.

Pretraining representations for data-efficient reinforcement learning. M Schwarzer, N Rajkumar, M Noukhovitch, A Anand, L Charlin, R D Hjelm, P Bachman, A C Courville, Advances in Neural Information Processing Systems. 34Schwarzer, M., Rajkumar, N., Noukhovitch, M., Anand, A., Charlin, L., Hjelm, R. D., Bachman, P., & Courville, A. C. (2021b). Pretraining representations for data-efficient reinforcement learning. Advances in Neural Information Processing Systems, 34, 12686-12699.

Planning to explore via self-supervised world models. R Sekar, O Rybkin, K Daniilidis, P Abbeel, D Hafner, D Pathak, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020of Proceedings of Machine Learning ResearchSekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., & Pathak, D. (2020). Planning to ex- plore via self-supervised world models. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, Vol. 119 of Proceedings of Machine Learning Research, pp. 8583-8592. PMLR. http://proceedings.mlr. press/v119/sekar20a.html.

State entropy maximization with random encoders for efficient exploration. Y Seo, L Chen, J Shin, H Lee, P Abbeel, K Lee, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Meila, M., & Zhang, T.the 38th International Conference on Machine Learning, ICML 2021139of Proceedings of Machine Learning ResearchSeo, Y., Chen, L., Shin, J., Lee, H., Abbeel, P., & Lee, K. (2021). State entropy maximization with random encoders for efficient exploration. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Vir- tual Event, Vol. 139 of Proceedings of Machine Learning Research, pp. 9443-9454. PMLR. http://proceedings.mlr.press/v139/seo21a.html.

Reinforcement learning with action-free pretraining from videos. ArXiv preprint. Y Seo, K Lee, S James, P Abbeel, abs/2203.13880Seo, Y., Lee, K., James, S., & Abbeel, P. (2022). Reinforcement learning with action-free pre- training from videos. ArXiv preprint, abs/2203.13880. https://arxiv.org/abs/ 2203.13880.

Time-contrastive networks: Self-supervised learning from video. P Sermanet, C Lynch, Y Chebotar, J Hsu, E Jang, S Schaal, S Levine, G Brain, 2018 IEEE international conference on robotics and automation (ICRA). IEEESermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., Schaal, S., Levine, S., & Brain, G. (2018). Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE international conference on robotics and automation (ICRA), pp. 1134-1141. IEEE.

RRL: resnet as representation for reinforcement learning. R M Shah, V Kumar, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Meila, M., & Zhang, T.the 38th International Conference on Machine Learning, ICML 2021139of Proceedings of Machine Learning ResearchShah, R. M., & Kumar, V. (2021). RRL: resnet as representation for reinforcement learning. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th International Conference on Ma- chine Learning, ICML 2021, 18-24 July 2021, Virtual Event, Vol. 139 of Proceedings of Machine Learning Research, pp. 9465-9476. PMLR. http://proceedings.mlr. press/v139/shah21a.html.

Learning robot skills with temporal variational inference. T Shankar, A Gupta, Proceedings of the 37th International Conference on Machine Learning. III, H. D., & Singh, A.the 37th International Conference on Machine Learning119of Proceedings of Machine Learning ResearchShankar, T., & Gupta, A. (2020). Learning robot skills with temporal variational inference. In III, H. D., & Singh, A. (Eds.), Proceedings of the 37th International Conference on Machine Learning, Vol. 119 of Proceedings of Machine Learning Research, pp. 8624-8633. PMLR. https://proceedings.mlr.press/v119/shankar20b.html.

Discovering motor programs by recomposing demonstrations. T Shankar, S Tulsiani, L Pinto, A Gupta, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Shankar, T., Tulsiani, S., Pinto, L., & Gupta, A. (2020). Discovering motor programs by re- composing demonstrations. In 8th International Conference on Learning Representa- tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https: //openreview.net/forum?id=rkgHY0NYwr.

Dynamics-aware unsupervised discovery of skills. A Sharma, S Gu, S Levine, V Kumar, K Hausman, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Sharma, A., Gu, S., Levine, S., Kumar, V., & Hausman, K. (2020). Dynamics-aware unsupervised discovery of skills. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview. net/forum?id=HJgLZR4KvH.

Keep doing what worked: Behavior modelling priors for offline reinforcement learning. N Siegel, J T Springenberg, F Berkenkamp, A Abdolmaleki, M Neunert, T Lampe, R Hafner, N Heess, M Riedmiller, International Conference on Learning Representations. Siegel, N., Springenberg, J. T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., Hafner, R., Heess, N., & Riedmiller, M. (2020). Keep doing what worked: Behavior modelling priors for offline reinforcement learning. In International Conference on Learning Representations. https://openreview.net/forum?id=rke7geHtwH.

Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, nature. 5297587Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with deep neural networks and tree search. nature, 529(7587), 484-489.

Curiosity and Motivation. P J Silvia, 10.1093/oxfordhb/9780195399820.013.0010The Oxford Handbook of Human Motivation. Oxford University PressSilvia, P. J. (2012). Curiosity and Motivation. In The Oxford Handbook of Human Motivation. Oxford University Press. DOI: 10.1093/oxfordhb/9780195399820.013.0010.

Parrot: Data-driven behavioral priors for reinforcement learning. A Singh, H Liu, G Zhou, A Yu, N Rhinehart, S Levine, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaSingh, A., Liu, H., Zhou, G., Yu, A., Rhinehart, N., & Levine, S. (2021). Parrot: Data-driven behavioral priors for reinforcement learning. In 9th International Conference on Learn- ing Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=Ysuv-WOFeKR.

Intrinsically motivated reinforcement learning. S P Singh, A G Barto, N Chentanez, Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004. Vancouver, British Columbia, CanadaSingh, S. P., Barto, A. G., & Chentanez, N. (2004). Intrinsically motivated reinforcement learning. In Advances in Neural Information Processing Systems 17 [Neural Informa- tion Processing Systems, NIPS 2004, December 13-18, 2004, Vancouver, British Columbia, Canada], pp. 1281-1288. https://proceedings.neurips.cc/paper/2004/ hash/4be5a36cbaca8ab9d2066debfe4e65c1-Abstract.html.

The development of embodied cognition: Six lessons from babies. L Smith, M Gasser, Artificial life. 111-2Smith, L., & Gasser, M. (2005). The development of embodied cognition: Six lessons from babies. Artificial life, 11(1-2), 13-29.

Unsupervised learning for reinforcement learning. A Srinivas, P Abbeel, Srinivas, A., & Abbeel, P. (2021). Unsupervised learning for reinforcement learning.. https: //icml.cc/media/icml-2021/Slides/10843_QHaHBNU.pdf.

Incentivizing exploration in reinforcement learning with deep predictive models. B C Stadie, S Levine, P Abbeel, abs/1507.00814ArXiv preprintStadie, B. C., Levine, S., & Abbeel, P. (2015). Incentivizing exploration in reinforcement learning with deep predictive models. ArXiv preprint, abs/1507.00814. https://arxiv.org/ abs/1507.00814.

Decoupling representation learning from reinforcement learning. A Stooke, K Lee, P Abbeel, M Laskin, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Meila, M., & Zhang, T.the 38th International Conference on Machine Learning, ICML 2021139of Proceedings of Machine Learning ResearchStooke, A., Lee, K., Abbeel, P., & Laskin, M. (2021). Decoupling representation learning from reinforcement learning. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th Inter- national Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, Vol. 139 of Proceedings of Machine Learning Research, pp. 9870-9879. PMLR. http: //proceedings.mlr.press/v139/stooke21a.html.

An analysis of model-based interval estimation for markov decision processes. A L Strehl, M L Littman, Journal of Computer and System Sciences. 748Strehl, A. L., & Littman, M. L. (2008). An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8), 1309-1331.

R Sutton, The bitter lesson. Incomplete Ideas (blog). 1312Sutton, R. (2019). The bitter lesson. Incomplete Ideas (blog), 13, 12.

Reinforcement learning: An introduction. R S Sutton, A G Barto, MIT pressSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.

Semantic exploration from language abstractions and pretrained representations. A C Tam, N C Rabinowitz, A K Lampinen, N A Roy, S C Chan, D Strouse, J X Wang, A Banino, F Hill, abs/2204.05080ArXiv preprintTam, A. C., Rabinowitz, N. C., Lampinen, A. K., Roy, N. A., Chan, S. C., Strouse, D., Wang, J. X., Banino, A., & Hill, F. (2022). Semantic exploration from language abstractions and pretrained representations. ArXiv preprint, abs/2204.05080. https://arxiv.org/abs/2204.

#exploration: A study of count-based exploration for deep reinforcement learning. H Tang, R Houthooft, D Foote, A Stooke, X Chen, Y Duan, J Schulman, F D Turck, P Abbeel, I Guyon, U Von Luxburg, S Bengio, H M Wallach, R Fergus, S V N Vishwanathan, Garnett, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. R.Long Beach, CA, USATang, H., Houthooft, R., Foote, D., Stooke, A., Chen, X., Duan, Y., Schulman, J., Turck, F. D., & Abbeel, P. (2017). #exploration: A study of count-based explo- ration for deep reinforcement learning. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., & Garnett, R. (Eds.), Ad- vances in Neural Information Processing Systems 30: Annual Conference on Neu- ral Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 2753-2762. https://proceedings.neurips.cc/paper/2017/hash/ 3a20f62a0af1aa152670bab3c602feed-Abstract.html.

Novelty search in representational space for sample efficient exploration. R Y Tao, V Francois-Lavet, J Pineau, H Larochelle, M Ranzato, R Hadsell, M Balcan, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Lin, H.NeurIPSTao, R. Y., Francois-Lavet, V., & Pineau, J. (2020). Novelty search in representational space for sample efficient exploration. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H. (Eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. https://proceedings.neurips.cc/paper/2020/hash/ 5ca41a86596a5ed567d15af0be224952-Abstract.html.

Active exploration in markov decision processes. J Tarbouriech, A Lazaric, The 22nd International Conference on Artificial Intelligence and Statistics. Chaudhuri, K., & Sugiyama, M.Naha, Okinawa, Japan2019of Proceedings of Machine Learning ResearchTarbouriech, J., & Lazaric, A. (2019). Active exploration in markov decision processes. In Chaud- huri, K., & Sugiyama, M. (Eds.), The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, Vol. 89 of Proceed- ings of Machine Learning Research, pp. 974-982. PMLR. http://proceedings.mlr. press/v89/tarbouriech19a.html.

. Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D De Las Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, T P Lillicrap, M A Riedmiller, Deepmind control suite. CoRR, abs/1801.00690Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., de Las Casas, D., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T. P., & Riedmiller, M. A. (2018). Deepmind control suite. CoRR, abs/1801.00690. http://arxiv.org/abs/1801.00690.

Understanding natural language commands for robotic navigation and mobile manipulation. S Tellex, T Kollar, S Dickerson, M Walter, A Banerjee, S Teller, N Roy, 10.1609/aaai.v25i1.7979Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence25Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., & Roy, N. (2011). Understanding natural language commands for robotic navigation and mobile manipula- tion. Proceedings of the AAAI Conference on Artificial Intelligence, 25(1), 1507-1514. DOI: 10.1609/aaai.v25i1.7979.

The role of pretrained representations for the OOD generalization of RL agents. F Träuble, A Dittadi, M Wuthrich, F Widmaier, P V Gehler, O Winther, F Locatello, O Bachem, B Schölkopf, S Bauer, International Conference on Learning Representations. Träuble, F., Dittadi, A., Wuthrich, M., Widmaier, F., Gehler, P. V., Winther, O., Locatello, F., Bachem, O., Schölkopf, B., & Bauer, S. (2022). The role of pretrained representations for the OOD generalization of RL agents. In International Conference on Learning Representa- tions. https://openreview.net/forum?id=8eb12UQYxrG.

Representation learning with contrastive predictive coding. A Van Den Oord, Y Li, O Vinyals, abs/1807.03748CoRRvan den Oord, A., Li, Y., & Vinyals, O. (2018). Representation learning with contrastive predictive coding. CoRR, abs/1807.03748. http://arxiv.org/abs/1807.03748.

Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L U Kaiser, I ; Polosukhin, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, Advances in Neural Information Processing Systems. & Garnett, R.Curran Associates, Inc30Guyon, IVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., & Polo- sukhin, I. (2017). Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., & Garnett, R. (Eds.), Advances in Neural Information Pro- cessing Systems, Vol. 30. Curran Associates, Inc. https://proceedings.neurips. cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.

Grandmaster level in starcraft ii using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, M Mathieu, A Dudzik, J Chung, D H Choi, R Powell, T Ewalds, P Georgiev, Nature. 5757782Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782), 350-354.

GLUE: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAWang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R. (2019). GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=rJ4km2R5t7.

Image as a foreign language: Beit pretraining for all vision and vision-language tasks. W Wang, H Bao, L Dong, J Bjorck, Z Peng, Q Liu, K Aggarwal, O K Mohammed, S Singhal, S Som, F Wei, 10.48550/arXiv.2208.10442CoRRWang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O. K., Singhal, S., Som, S., & Wei, F. (2022). Image as a foreign language: Beit pretraining for all vision and vision-language tasks. CoRR, abs/2208.10442. DOI: 10.48550/arXiv.2208.10442.

Unsupervised control through non-parametric discriminative rewards. D Warde-Farley, T V De Wiele, T D Kulkarni, C Ionescu, S Hansen, V Mnih, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAWarde-Farley, D., de Wiele, T. V., Kulkarni, T. D., Ionescu, C., Hansen, S., & Mnih, V. (2019). Unsupervised control through non-parametric discriminative rewards. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=r1eVMnA9K7.

Embed to control: A locally linear latent dynamics model for control from raw images. M Watter, J Springenberg, J Boedecker, M Riedmiller, Advances in Neural Information Processing Systems. Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., & Garnett, R.Curran Associates, Inc28Watter, M., Springenberg, J., Boedecker, J., & Riedmiller, M. (2015). Embed to control: A lo- cally linear latent dynamics model for control from raw images. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., & Garnett, R. (Eds.), Advances in Neural Information Process- ing Systems, Vol. 28. Curran Associates, Inc. https://proceedings.neurips.cc/ paper/2015/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf.

Finetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. OpenReviewWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., & Le, Q. V. (2022). Finetuned language models are zero-shot learners. In The Tenth International Con- ference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenRe- view.net. https://openreview.net/forum?id=gEZrGCozdqR.

Masked visual pre-training for motor control. T Xiao, I Radosavovic, T Darrell, J Malik, abs/2203.06173ArXiv preprintXiao, T., Radosavovic, I., Darrell, T., & Malik, J. (2022). Masked visual pre-training for motor control. ArXiv preprint, abs/2203.06173. https://arxiv.org/abs/2203.06173.

Continual learning of control primitives : Skill discovery via reset-games. K Xu, S Verma, C Finn, S Levine, H Larochelle, M Ranzato, R Hadsell, M Balcan, Advances in Neural Information Processing Systems. & Lin, H.Curran Associates, Inc33Xu, K., Verma, S., Finn, C., & Levine, S. (2020). Continual learning of control primitives : Skill discovery via reset-games. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H. (Eds.), Advances in Neural Information Processing Systems, Vol. 33, pp. 4999-5010. Cur- ran Associates, Inc. https://proceedings.neurips.cc/paper/2020/file/ 3472ab80b6dff70c54758fd6dfc800c2-Paper.pdf.

Prompting decision transformer for few-shot policy generalization. M Xu, Y Shen, S Zhang, Y Lu, D Zhao, B J Tenenbaum, C Gan, Thirty-ninth International Conference on Machine Learning. Xu, M., Shen, Y., Zhang, S., Lu, Y., Zhao, D., Tenenbaum, B. J., & Gan, C. (2022). Prompting deci- sion transformer for few-shot policy generalization. In Thirty-ninth International Conference on Machine Learning.

Task-induced representation learning. J Yamada, K Pertsch, A Gunjal, J J Lim, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. Yamada, J., Pertsch, K., Gunjal, A., & Lim, J. J. (2022). Task-induced representation learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.net/forum?id= OzyXtIZAzFv.

TRAIL: Near-optimal imitation learning with suboptimal data. M Yang, S Levine, O Nachum, International Conference on Learning Representations. Yang, M., Levine, S., & Nachum, O. (2022). TRAIL: Near-optimal imitation learning with suboptimal data. In International Conference on Learning Representations. https: //openreview.net/forum?id=6q_2b6u0BnJ.

Representation matters: Offline pretraining for sequential decision making. M Yang, O Nachum, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Meila, M., & Zhang, T.the 38th International Conference on Machine Learning, ICML 2021139of Proceedings of Machine Learning ResearchYang, M., & Nachum, O. (2021). Representation matters: Offline pretraining for sequential decision making. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, Vol. 139 of Proceedings of Machine Learning Research, pp. 11784-11794. PMLR. http://proceedings.mlr. press/v139/yang21h.html.

. D Yarats, D Brandfonbrener, H Liu, M Laskin, P Abbeel, A Lazaric, L Pinto, Yarats, D., Brandfonbrener, D., Liu, H., Laskin, M., Abbeel, P., Lazaric, A., & Pinto, L. (2022).

Don't change the algorithm, change the data: Exploratory data for offline reinforcement learning. ICLR 2022 Workshop on Generalizable Policy Learning in Physical World. Don't change the algorithm, change the data: Exploratory data for offline reinforcement learning. In ICLR 2022 Workshop on Generalizable Policy Learning in Physical World. https://openreview.net/forum?id=Su-zh4a41Z5.

Reinforcement learning with prototypical representations. D Yarats, R Fergus, A Lazaric, L Pinto, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Meila, M., & Zhang, T.the 38th International Conference on Machine Learning, ICML 2021139of Proceedings of Machine Learning ResearchYarats, D., Fergus, R., Lazaric, A., & Pinto, L. (2021). Reinforcement learning with prototyp- ical representations. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th Interna- tional Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, Vol. 139 of Proceedings of Machine Learning Research, pp. 11920-11931. PMLR. http: //proceedings.mlr.press/v139/yarats21a.html.

Towards playing full MOBA games with deep reinforcement learning. D Ye, G Chen, W Zhang, S Chen, B Yuan, B Liu, J Chen, Z Liu, F Qiu, H Yu, Y Yin, B Shi, L Wang, T Shi, Q Fu, W Yang, L Huang, W Liu, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H.2020Ye, D., Chen, G., Zhang, W., Chen, S., Yuan, B., Liu, B., Chen, J., Liu, Z., Qiu, F., Yu, H., Yin, Y., Shi, B., Wang, L., Shi, T., Fu, Q., Yang, W., Huang, L., & Liu, W. (2020). Towards playing full MOBA games with deep reinforcement learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H. (Eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. https://proceedings.neurips.cc/paper/ 2020/hash/06d5ae105ea1bea4d800bc96491876e9-Abstract.html.

Supervised learning achieves human-level performance in MOBA games: A case study of honor of kings. D Ye, G Chen, P Zhao, F Qiu, B Yuan, W Zhang, S Chen, M Sun, X Li, S Li, J Liang, Z Lian, B Shi, L Wang, T Shi, Q Fu, W Yang, L Huang, 10.1109/TNNLS.2020.3029475IEEE Trans. Neural Networks Learn. Syst. 333Ye, D., Chen, G., Zhao, P., Qiu, F., Yuan, B., Zhang, W., Chen, S., Sun, M., Li, X., Li, S., Liang, J., Lian, Z., Shi, B., Wang, L., Shi, T., Fu, Q., Yang, W., & Huang, L. (2022). Supervised learning achieves human-level performance in MOBA games: A case study of honor of kings. IEEE Trans. Neural Networks Learn. Syst., 33(3), 908-918. DOI: 10.1109/TNNLS.2020.3029475.

Mastering complex control in MOBA games with deep reinforcement learning. D Ye, Z Liu, M Sun, B Shi, P Zhao, H Wu, H Yu, S Yang, X Wu, Q Guo, Q Chen, Y Yin, H Zhang, T Shi, L Wang, Q Fu, W Yang, L Huang, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceYe, D., Liu, Z., Sun, M., Shi, B., Zhao, P., Wu, H., Yu, H., Yang, S., Wu, X., Guo, Q., Chen, Q., Yin, Y., Zhang, H., Shi, T., Wang, L., Fu, Q., Yang, W., & Huang, L. (2020). Mastering com- plex control in MOBA games with deep reinforcement learning. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 6672-6679. AAAI Press. https://ojs.aaai.org/index.php/AAAI/article/ view/6144.

Gradient surgery for multi-task learning. T Yu, S Kumar, A Gupta, S Levine, K Hausman, C Finn, H Larochelle, M Ranzato, R Hadsell, M Balcan, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. & Lin, H.2020Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., & Finn, C. (2020). Gradient surgery for multi-task learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H. (Eds.), Advances in Neural Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual. https://proceedings.neurips.cc/paper/2020/hash/ 3fe78a8acf5fda99de95303940a2420c-Abstract.html.

A framework for efficient robotic manipulation. ArXiv preprint. A Zhan, P Zhao, L Pinto, P Abbeel, M Laskin, abs/2012.07975Zhan, A., Zhao, P., Pinto, L., Abbeel, P., & Laskin, M. (2020). A framework for efficient robotic ma- nipulation. ArXiv preprint, abs/2012.07975. https://arxiv.org/abs/2012.07975.

A dissection of overfitting and generalization in continuous reinforcement learning. A Zhang, N Ballas, J Pineau, abs/1806.07937CoRRZhang, A., Ballas, N., & Pineau, J. (2018). A dissection of overfitting and generalization in continu- ous reinforcement learning. CoRR, abs/1806.07937. http://arxiv.org/abs/1806.

Learning invariant representations for reinforcement learning without reconstruction. A Zhang, R T Mcallister, R Calandra, Y Gal, S Levine, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaZhang, A., McAllister, R. T., Calandra, R., Gal, Y., & Levine, S. (2021a). Learning invariant repre- sentations for reinforcement learning without reconstruction. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe- view.net. https://openreview.net/forum?id=-2FCwDKRREu.

Exploration by maximizing rényi entropy for reward-free rl framework. C Zhang, Y Cai, L Huang, J Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceZhang, C., Cai, Y., Huang, L., & Li, J. (2021b). Exploration by maximizing rényi entropy for reward-free rl framework. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 10859-10867.

Hierarchical reinforcement learning by discovering intrinsic options. J Zhang, H Yu, W Xu, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaZhang, J., Yu, H., & Xu, W. (2021c). Hierarchical reinforcement learning by discovering intrinsic options. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum? id=r-gPPHEjpmw.

Multi-agent reinforcement learning: A selective overview of theories and algorithms. K Zhang, Z Yang, T Basar, abs/1911.10635CoRRZhang, K., Yang, Z., & Basar, T. (2019). Multi-agent reinforcement learning: A selective overview of theories and algorithms. CoRR, abs/1911.10635. http://arxiv.org/abs/1911. 10635.

Learning to drive by watching youtube videos: Actionconditioned contrastive policy pretraining. Q Zhang, Z Peng, B Zhou, 10.48550/ARXIV.2204.02393Zhang, Q., Peng, Z., & Zhou, B. (2022). Learning to drive by watching youtube videos: Action- conditioned contrastive policy pretraining.. DOI: 10.48550/ARXIV.2204.02393.

Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. T Zhang, Z Mccarthy, O Jow, D Lee, X Chen, K Goldberg, P Abbeel, 10.1109/ICRA.2018.84612492018 IEEE International Conference on Robotics and Automation (ICRA). Zhang, T., McCarthy, Z., Jow, O., Lee, D., Chen, X., Goldberg, K., & Abbeel, P. (2018). Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 5628-5635. DOI: 10.1109/ICRA.2018.8461249.

Light-weight probing of unsupervised representations for reinforcement learning. W Zhang, A Gx-Chen, V Sobal, Y Lecun, N Carion, 10.48550/arXiv.2208.12345CoRRZhang, W., GX-Chen, A., Sobal, V., LeCun, Y., & Carion, N. (2022a). Light-weight prob- ing of unsupervised representations for reinforcement learning. CoRR, abs/2208.12345. DOI: 10.48550/arXiv.2208.12345.

Efficient reinforcement learning in block MDPs: A model-free representation learning approach. X Zhang, Y Song, M Uehara, M Wang, A Agarwal, W Sun, K Chaudhuri, S Jegelka, L Song, C Szepesvari, G Niu, Proceedings of the 39th International Conference on Machine Learning. & Sabato, S.the 39th International Conference on Machine Learning162of Proceedings of Machine Learning ResearchZhang, X., Song, Y., Uehara, M., Wang, M., Agarwal, A., & Sun, W. (2022b). Efficient reinforce- ment learning in block MDPs: A model-free representation learning approach. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., & Sabato, S. (Eds.), Proceedings of the 39th International Conference on Machine Learning, Vol. 162 of Proceedings of Machine Learning Research, pp. 26517-26547. PMLR. https://proceedings.mlr.press/ v162/zhang22aa.html.

Online decision transformer. Q Zheng, A Zhang, A ; K Grover, S Jegelka, L Song, C Szepesvari, G Niu, Proceedings of the 39th International Conference on Machine Learning. & Sabato, S.the 39th International Conference on Machine Learning162of Proceedings of Machine Learning ResearchZheng, Q., Zhang, A., & Grover, A. (2022). Online decision transformer. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., & Sabato, S. (Eds.), Proceedings of the 39th Inter- national Conference on Machine Learning, Vol. 162 of Proceedings of Machine Learning Research, pp. 27042-27059. PMLR. https://proceedings.mlr.press/v162/ zheng22c.html.

Reinforcement and imitation learning for diverse visuomotor skills. Y Zhu, Z Wang, J Merel, A Rusu, T Erez, S Cabi, S Tunyasuvunakool, J Kramã¡r, R Hadsell, N De Freitas, N Heess, 10.15607/RSS.2018.XIV.009Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsPittsburgh, PennsylvaniaZhu, Y., Wang, Z., Merel, J., Rusu, A., Erez, T., Cabi, S., Tunyasuvunakool, S., KramÃ¡r, J., Hadsell, R., de Freitas, N., & Heess, N. (2018). Reinforcement and imitation learning for diverse vi- suomotor skills. In Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania. DOI: 10.15607/RSS.2018.XIV.009.