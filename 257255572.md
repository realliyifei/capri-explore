# Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation

CorpusID: 257255572
 
tags: #Medicine, #Engineering, #Computer_Science

URL: [https://www.semanticscholar.org/paper/564ec33ed2cbfae044e8c770f73e0044d305107f](https://www.semanticscholar.org/paper/564ec33ed2cbfae044e8c770f73e0044d305107f)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation
AUGUST 2021 1

Journal Of L A T E X Class 
Files 
Towards More Precise Automatic Analysis: A Comprehensive Survey of Deep Learning-based Multi-organ Segmentation
148AUGUST 2021 1
Accurate segmentation of multiple organs of the head, neck, chest, and abdomen from medical images is an essential step in computer-aided diagnosis, surgical navigation, and radiation therapy. In the past few years, with a data-driven feature extraction approach and end-to-end training, automatic deep learning-based multi-organ segmentation method has far outperformed traditional methods and become a new research topic. This review systematically summarizes the latest research in this field. For the first time, from the perspective of full and imperfect annotation, we comprehensively compile 161 studies on deep learning-based multi-organ segmentation in multiple regions such as the head and neck, chest, and abdomen, containing a total of 214 related references. The method based on full annotation summarizes the existing methods from four aspects: network architecture, network dimension, network dedicated modules, and network loss function. The method based on imperfect annotation summarizes the existing methods from two aspects: weak annotation-based methods and semi annotationbased methods. We also summarize frequently used datasets for multi-organ segmentation and discuss new challenges and new research trends in this field.Index Terms-abdomen multi-organ, chest multi-organ, deep learning, head and neck multi-organ, multi-organ segmentation.

Moreover, different physicians or hospitals will have different results of labelling [6]- [9]. Therefore, accurate automatic multi-organ segmentation method is urgently needed in clinical practice.

Multi-organ segmentation is a challenging task. First, the contour of the anatomical structure in image is highly variable, which is difficult to expressed by a unified mathematical rule. Second, the boundaries between different organs or tissue regions in an image are often blurred due to image noise and low intensity contrast, and these boundaries are difficult to identify using techniques of traditional digital image processing. Third, the use of different scanners, scanning protocols, and contrast agents will lead to different intensity distributions of organs in the obtained images, which poses a great challenge to the generalizability of the model. Finally, considering safety and ethical issues, many hospitals do not disclose their datasets. Many segmentation methods are trained and validated on private datasets, making it difficult to compare different methods. Therefore, designing accurate and robust multi-organ segmentation models is a very difficult and expensive task.

Traditional methods [10]- [13] usually utilize manually extracted image features for image segmentation, such as the threshold [14] method, graph cut [15] method, and region growth [16] method. Limited by a large number of manually extracted image features and the selection of non-robust thresholds or seeds, the segmentation results of these methods are usually unstable, and often yield only a rough segmentation result or only apply to specific organs. Knowledgebased methods can obtain anatomical information of different organs from labelled datasets, reduce the burden of manual feature extraction, and improve the robustness and accuracy of multi-organ segmentation, which commonly include multiatlas label fusion [17], [18] and statistical shape models [19], [20]. The method based on multi-atlas label fusion-based uses image alignment to align predefined structural contours to the image to be segmented, and this method typically includes multiple steps. Therefore, the performance of this method may be influenced by various relevant factors involved in each step. The atlas-based method is still very popular, but due to the use of fixed atlases, it is difficult to handle the anatomical variation of organs between patients. In addition, it is computationally intensive and takes a long time to complete an alignment task. The statistical shape model uses the positional relationships between different organs, and uses the shape of the organs in the statistical space as a constraint to regularize the segmentation results. However, the accuracy arXiv:2303.00232v2 [eess.IV] 2 Mar 2023 of this approach is largely dependent on the reliability and extensibility of the shape model, and the model based on normal anatomical structures has very limited effect in the segmentation of irregular structures.

Using data-driven feature extraction approach and end-toend training, the methods based on deep learning (DL) have been widely studied in the fields of image classification [21], object detection [22] and image segmentation [23], [24], image fusion [25], image registration [26], etc. The segmentation method based on deep learning has become a mainstream method in the field of medical image processing. However, there are two main difficulties in multi-organ deep learning segmentation tasks. First, as shown in the head and neck in Fig. 1, the abdomen in Fig. 2, the chest in Fig. 3, and the statistics of the multi-organ size in each part in Fig. 4, there are very large differences between the organs sizes, and the serious imbalances of different organs sizes will lead to a poor segmentation performance of the trained segmentation network for small organs. Second, due to the imaging principle of CT technology and the complex anatomical structure of the human body, the contrast between organs and their surrounding tissues is often low, which leads to the inaccurate segmentation of organ boundaries by segmentation networks. Therefore, it has become a new hot research topic to develop deep multi-organ segmentation methods that can accurately segment small and large organs at the same time.  (4) left lens, (5) right lens, (6) left optic nerve, (7) right optic nerve, (8) Optical chiasm, (9) left temporal lobe, (10) right temporal lobe, (11) pituitary gland, (12) left parotid gland, (13) right parotid gland, (14) left temporal bone rock, (15) right temporal bone rock, (16) left temporal bone, (17) right temporal bone, (18) left mandibular condyle, (19) right mandibular condyle, (20) spinal cord, (21) left mandible, (22) right mandible. Recently, a large number of deep learning-based multiorgan segmentation methods with significantly improved performance have been proposed [27]. Fu et al. [28] systematically reviewed the medical image multi-organ segmentation methods based on deep learning by 2020 according to the Fig. 3. Schematic diagram of the abdominal organs, where the numbers are arranged in order: (1) liver, (2) kidney, (3) spleen, (4) pancreas, (5) aorta, (6) inferior vena cava, (7) stomach, (8) gallbladder, (9) esophagus, (10) right adrenal gland, (11) left adrenal gland, and (12) celiac artery. network architecture. However, with the rapid development of deep learning technology, more representative new techniques and methods have been proposed, such as transformer-based multi-organ segmentation methods and imperfect annotationbased methods. A more comprehensive review and summary of these techniques and methods are very important for the development of this field. This paper reviews deep learning-based multi-organ segmentation method of the head, neck, chest and abdomen published from 2016 to 2022. On Google Scholar, a search using the keywords 'Multi Organ Segmentation' and 'Deep Learning' yielded an initial 287 articles, 73 articles were removed according to abstract and keywords, and 161 highly relevant studies containing a total of 214 relevant references were obtained. Fig. 5 summarizes all current state-of-the-art deep learning-based multi-organ segmentation methods according to full annotation and imperfect annotation architectures. In full annotation-based methods, we summarize the existing methods in four aspects: network architecture, network dimension, network dedicated modules, and network loss function. In imperfect annotation-based methods, we summarize the existing methods in two aspects, weak annotation and semi annotation, to investigate their innovation, contribution, and challenges. This article is organized as follows. Section II expounds the mathematical definition of multi-organ segmentation and the corresponding evaluation metrics. Section III summarizes the multi-organ segmentation datasets. Section IV describes the literature based on full annotation-based methods, involving four parts: network architecture (section IV.A), network dimension (section IV.B), network dedicated modules (section IV.C), and network loss function (section IV.D). Section V analyzes the articles based on imperfect annotation methods, including two parts: weak annotation-based methods (section V.A) and semi annotation-based methods (section V.B). We discuss the existing methods and their future outlooks in section VI, and conclude the whole paper in section VII.


## II. DEFINITION AND EVALUATION METRICS

Let X represent the union of multiple organ regions in the input images, G represent the union of ground truth labels of multiple organs in the input images, P represent the union of predicted labels of multiple organs in the output images, x c i ∈ X, g c i ∈ G, p c i ∈ P, i = 1, · · · N , and c = 1, · · · C, where N represents the number of pixel in the image, C represents the number of categories to which the pixels belong, f represents the neural network, and θ represents the parameters of the neural network optimization, where P = f (X; θ).

The loss function represents the gap between the predicted and true values. In the multi-organ segmentation task, common loss functions include the cross-entropy loss and Dice loss. Section IV-D provides specific details about the loss function.

Given a multi-organ segmentation task, {Ψ} represents the represents the class set of organs to be segmented. {x} * represents the set of organs annotated in x. According to the available annotations, multi-organ segmentation can be implemented according to three learning paradigms: full annotation-based learning, weak annotation-based learning, and semi annotation-based learning. The last two are called imperfect annotation-based methods, as shown in Fig. 6. Full annotation-based learning means that the labels of all organ are given, which indicates that ∀x ∈ X, {x} * = {Ψ} . Weak annotation often means that the data come from n different datasets. However, each dataset provides the annotations of one or more organs but not all organs, which means that X = X 1 ∪ X 2 ∪ · · · ∪ X n , ∀x k,i ∈ X k , k = 1, 2, . . . n,
{x k,i } * < {Ψ}, n k=1 {x k,i } * = {Ψ}.
Here, x k,i denotes the i-th image in X k . Semi annotationbased methods indicate that some of the training datasets are fully labelled and others are unlabelled, X = X l ∪ X u · X l . X l represents the fully labelled dataset, X u represents the unlabelled dataset, which indicates that ∀x l ∈ X l , {x l } * = {Ψ} and ∀x u ∈ X u , {x u } * = φ, and the size of X l is far less than the one of X u . Usually using the Dice similarity coefficient (DSC), 95% Hausdorff distance (HD95) and mean surface distance (MSD) to evaluate the performance of the segmentation methods. DSC is a measure of the volume overlap between the predicted labels and ground truth labels, HD95 and MSD are measures of the surface distance between the predicted labels and ground truth labels.
DSC = 2 × |P c ∩ G c | |P c | + |G c | (1) HD95 = max 95% d P c s , G c s , d G c s , P c s (2) M SD = 1 |P c s | + |G c s |   p c s ∈P c s d (p c s , G c s ) + g c s ∈G c s d (g c s , P c s )  (3)
where P c and G c represent the set of predicted pixels and the set of real pixels of the c class organ, respectively; P c s and G c s represent the set of predicted pixels and the set of real pixels of the surface of the c class organ, respectively; and d p c s , G c s = min c g c s ∈G c s p c s − g c s 2 represents the minimal distance from point p c s to surface G c s . The review reports various methods based on DSC values.


## III. MULTI-ORGAN SEGMENTATION DATASETS

To obtain high-quality organ segmentation datasets, many research teams have undertaken several collaborations with medical organizations. Table I summarizes the common head and neck, thorax, and abdomen datasets used for the development and validation of multi-organ segmentation method. Table I also shows that the quantity of annotated data available for deep learning studies is still very low.


## IV. FULL ANNOTATION-BASED METHODS

The method based on full annotation means that all organs of the multi-organ segmentation task are fully annotated. The existing methods can be analysed from four parts: network architecture, network dimension, network dedicated modules, and network loss function. Among these methods, the network architecture part summarizes the common neural network architectures and the combination or cascade of different architectures. In the network dimension part, the existing methods are classified into 2D, 3D, and multi-view methods according to the image dimension used. The part of network dedicated modules describes modules that are commonly used in multiorgan segmentation to improve the segmentation performance, The part of network loss function summarizes how common loss functions are innovated around multi-organ segmentation.


## A. Network Architecture

Based on the design of the network architecture, multi-organ segmentation methods can be classified according to singlestage and multistage implementations. Single-stage methods include those based on CNN (Convolutional Neural Network), GAN (Generative Adversarial Network), transformer or hybrid networks. Multistage approaches include coarse-to-fine methods, localization and segmentation methods, or other cascade approaches. Tables II-IV summarize the literature related to single-stage methods for the segmentation of multi-organ in the head and neck, abdomen and chest based on DSC metrics. Since there are too many organs in the head and neck as well as abdomen, this paper mainly reports 9 organs in the head and neck and 7 organs in the abdomen. Tables XI-XII in the  supplementary materials summarize the DSC values of other  organs. 1) CNN-Based Methods: Convolutional Neural Network (CNN) is a feedforward neural network which can automatically extract deep features of the image. Multiple neurons are connected to each neuron in next layer, where each layer can perform complex tasks such as convolution, pooling, or loss computation [37]. CNNs have been successfully applied to medical images, such as brain [38], [39] and pancreas [40] segmentation tasks. a) Early CNN-Based Methods: Earlier CNN-based methods mainly used convolutional layers to extract features and then went through pooling layers and fully connected layers to obtain the final prediction results. Ibragimov and Xing [41] used deep learning methods to segment OARs in head and neck CT images for the first time, training 13 CNNs for 13 OARs, and showed that the CNNs outperformed or were comparable to advanced algorithms in segmentation accuracy for organs such as the spinal cord, mandible, larynx, pharynx, eye, and optic nerve, but performed poorly in the segmentation of organs such as the parotid gland, submandibular gland, and optical chiasm. Fritscher et al. [42] combined the shape location as well as the intensity with CNN for segmentation of the parotid gland, submandibular gland and optic nerve. Moeskops et al. [43] investigated whether a single CNN can be used to segment six tissues in brain MR images, pectoral muscles in breast MR images, and coronary arteries in heart CTA images. The results showed that a single CNN can segment multiple organs not only on a single modality but also on multiple modalities. b) FCN-Based Methods: Early CNN-based methods made some improvements in segmentation accuracy compared to traditional methods. However, CNN involves multiple identical computations of overlapping voxels during the convolution operation, which may cause some performance loss. Moreover, the spatial information of the image is lost when the convolutional features are input into the final fully connected network layer. Thus, Shelhamer et al. [44] proposed the Fully Convolutional Network (FCN), which enables endto-end segmentation by using transposed convolutional layers that allow the size of the predicted image to match the size of the input image. Wang et al. [45] used FCN combined with a new sample selection strategy to segment 16 organs in the abdomen, and Trullo et al. [83] used a variant of FCN, SharpMask [46], to segment the esophagus, heart, trachea, and aorta in the thorax, which showed the segmentation results of all four organs were improved compared with the normal FCN.

c) U-Net-Based Methods: Based on FCN, Ronneberger et al. [47] proposed a classical U-Net architecture, which is consisted of an encoder for the down sampling layer and a decoder for the up-sampling layer, and connects them layer by layer with skip connections, so that the features extracted from the down sampling layer can be directly transmitted to the upsampling layer to fuse multiscale features for segmentation. U-Net has become one of the most commonly used architectures in the field of multi-organ segmentation [48]- [54]. Roth et al. [52] applied the U-Net architecture to segment the abdominal aorta, portal vein, liver, spleen, stomach, gallbladder, and pancreas. The advanced segmentation performance of multiple organs was achieved with an average Dice value of 0.893 for seven organs. Lambert et al. [55] proposed a simplified U-Net for segmenting the heart, trachea, aorta, and esophagus of the chest. The results showed that adding dropout and using bilinear interpolation can significantly improve the segmentation performance of the heart, aorta, and esophagus compared with the ordinary U-Net. In addition to U-Net, V-Net [56] proposes 3D image segmentation method based on volumetric, fully convolutional neural network. This method can directly  [58] proposed a new probabilistic V-Net model which combines a conditional variational autoencoder (cVAE) and hierarchical spatial feature transform (HSPT) for abdominal multi-organ segmentation. nnU-Net [101] is a novel framework based on U-Net architecture with the addition of adaptive preprocessing, data enhancement, and postprocessing techniques, and has shown state-of-the-art results on many publicly available datasets for different biomedical segmentation challenges [59]- [62]. Podobnik et al. [59] reported the results of segmentation of 31 OARs of the head and neck using the nnU-Net architecture combined with CT and MR images.

2) GAN-Based Methods: A typical Generative Adversarial Network (GAN) [63] includes a pair of competitive networks, which are generators and discriminators. The generator attempts to deceive the discriminator by generating the artificial data, and the discriminator strives to discriminate the artificial data without being deceived by the generator; after alternate optimization training, the performance of both networks can eventually be improved. In recent years, many GAN-based multi-organ segmentation methods have been proposed and achieved high segmentation accuracy [64]- [70].

Dong et al. [66] jointly trained GAN with a set of U-Nets as a generator and a set of FCNs as a discriminator for segmenting the left lung, right lung, spinal cord, esophagus and heart from chest CT images. The results showed that the segmentation performance of most of the organs were improved with the help of adversarial networks, and the average DSC values of the above five OARs were finally obtained as 0.970, 0.970, 0.900, 0.750 and 0.870. Tong et al. [64] proposed a Shape-constraint GAN for automatic head and neck OARs segmentation (SC-GAN) from CT and lowfield MRI images. It uses DenseNet, a deep supervised fully convolutional network to segment organs for prediction, and uses a CNN as discriminator network to correct the error of prediction. The results show that the combination of GAN and DenseNet can further improve the segmentation performance of CNN based on the original shape constraints.

GAN can improve accuracy with its adversarial losses. However, the training of GAN network is difficult and timeconsuming since the generator needs to achieve Nash equilibrium with the discriminator. And its adversarial loss as a shape modifier can only achieve higher segmentation accuracy when segmenting organs with regular and unique shapes (such as liver and heart), but may not work well for irregular or tubular structures (such as pancreas and aorta).

3) Transformer-Based Methods: CNN-based methods can perform well for segmenting multiple organs in many tasks, but the inherent shortcomings of the perceptual field of the convolutional layers lead to the inability of CNNs to model global relationships, hindering the performance of the models. The self-attentive mechanism of the transformer [71] can solve the long-term dependency problem well, achieving better results than CNNs in many tasks such as natural language processing (NLP) or computer vision [72]. The performance of the medical image segmentation networks using transformer is also close or even better than the one of current state-ofthe-art methods [73]- [76].

Cao et al. [77] integrated the transformer with a U-shaped architecture to explore the potential of the pure transformer model for abdominal multi-organ segmentation. The results showed that the method has good segmentation accuracy. However, the method needs to initialize the network encoder and decoder using the training weights of the Swin transformer on ImageNet. Huang et al. [78] introduced an efficient and powerful medical image segmentation architecture, MISS-Former, where the proposed enhanced mixed block can effectively overcome the feature recognition limitation problem caused by convolution. Moreover, compared with Swin-UNet, this model does not require pre-training on large-scale datasets to achieve comparable segmentation results.

Transformer-based approaches can capture long-range dependencies and achieve better performance than CNNs in many tasks. However, multi-organ segmentation problem involves the segmentation of many tiny organs, and the pure transformer network focuses on the global context modelling. This leads to the lack of detailed localization information of low-resolution features. Thus, a coarser segmentation result is usually obtained. 4) Hybrid Networks: CNN convolution operation can extract local features well, but it is difficult to obtain global features. The self-attentive mechanism of the transformer can effectively capture feature dependencies over long distances, but it loses local feature details, which may obtain poor results for the segmentation accuracy of small organs. Therefore, some researchers have combined the CNN and transformer to overcome the limitations of both architectures [74], [79]- [83].

Suo et al. [84] proposed an intra-scale and inter-scale collaborative learning network (I2-Net) by combining features extracted by the CNN and transformer to segment multiple organs of the abdomen, which improved the segmentation performance of small and medium-sized organs by 4.19% and 1.83%-3.8%, respectively. Kan et al. [85] proposed ITUnet, which adds the features extracted by the transformer to the output of each block of the CNN-based encoder, which can obtain segmentation results provided by both the local and global information of the image. ITUnet has better accuracy and robustness than other methods, especially on difficult organs such as the lens. Chen et al. [86] proposed a new network architecture, TransUNet, which uses a transformer to further encode CNN encoders to build stronger encoders and report competitive results for multi-organ segmentation of the head and neck. Hatamizadeh et al. [87] proposed a new architecture U-net transformer (UNETR) using a transformer as an encoder and the CNN as a decoder, which achieves better segmentation accuracy by capturing global and local dependencies.

In addition to the methods combining CNN and transformer, there are some other hybrid frames. For example, Chen et al. [88] combined U-Net and long short-term memory (LSTM) to realize the segmentation of five organs in the chest, and the DSC values of all five organs were above 0.8. Chakravarty et al. [89] proposed a hybrid architecture combining CNN and RNN to segment the optic disc, nucleus, and left atrium. The hybrid architecture-based approach can combine and utilize the advantages of the two architectures for the accurate segmentation of small and medium-sized organs, which is a key research direction for the future. 5) Cascade Networks: Due to most organs occupy only a small volume in images, the segmentation models are easy to segment large organs and ignore small organs, which prompted researchers to propose cascade multistage methods. Multistage methods can be divided into two main categories, depending on the information provided by the primary network to the secondary network. The first category is called coarse-to-fine multi-organ segmentation method, where the first network performs coarse segmentation, and its results are passed to another network to achieve fine segmentation. The second category is called multi-organ segmentation method based on localization and segmentation, where candidate boxes for the location of each organ are identified by registration methods or localization networks, and then input into the second-level network for fine segmentation. In addition, the first network can provide other information, such as the shape location or proportion, to better guide the segmentation of the second network. Tables V-X summarize the relevant literature of the cascade methods for head and neck, chest and abdomen based on DSC metrics, and tables VIII-IX in the supplementary materials summarize the DSC metrics of other organs.

a) Coarse-to-Fine-Based Methods: The coarse-to-finebased methods first inputs the original image and its corresponding labels into the first network. After training, the first-level network obtains the coarse segmentation probability map, which will be multiplied by the original image, and the results will be input into the second network to refine the rough segmentation. This process is shown in Fig. 7. In recent years, a number of coarse-to-fine methods have been proposed for multi-organ segmentation [90]- [99], and the references are shown in Tables VI-VIII. Trullo et al. [100] proposed two synergistic depth architectures to jointly segment all organs, including the esophagus, heart, aorta, and trachea. Probabilistic maps obtained in the first stage were passed to the second stage to learn anatomical constraints, and then four networks were trained for four structures in the second stage to distinguish the background from each target organ in separate refinements. Zhang et al. [94] proposed a new cascaded network model with Block Level Skip Connections (BLSC) between two cascaded networks. This architecture enabled the second-stage network to capture the features learned by each block in the first-stage network and accelerated the convergence of the second-stage network.

Xie et al. [95] proposed a new framework called the Recurrent Saliency Transformation Network (RSTN). This framework enabled coarse scale segmentation masks to be passed to the fine stage as spatial weights, while gradients can be backpropagated from the loss layer to the whole network, so as to realize the joint optimization of the two stages, thus improving the segmentation accuracy of small targets. Ma et al. [92] proposed a new end-to-end coarse-to-fine segmentation model to automatically segment multiple OARs in head and neck CT images. This model used a predetermined threshold to classify the initial results of the coarse stage into large and small OARs, and then designed different modules to refine the segmentation results.

This coarse-to-fine approach effectively reduces the complexity of the background and enhances the discriminative information of the target structures. Compared with the singlestage approach, this coarse-to-fine-based method improves the segmentation results for small organs, but there are limitations in memory and training time because at least two networks need to be trained.

b) Localization and Segmentation Based Methods: The localization and segmentation methods are also multistage cascade methods. Here, the first-level network provides location information, returns a candidate frame, and crops the region of interest of the image according to the location information, and uses it as the input of the second network. In this way, when the second network performs segmentation, one organ can be targeted, excluding the interference of other organs or background noise and improving the segmentation accuracy. The process is shown in Fig. 8. The organ location in the first stage can be obtained through registration or localization network. The relevant literature of multi-stage method based on location and segmentation are listed in Tables VIII-X, and the DSC values of other organs are listed in tables XV-XVI.  [105] proposed decomposing OARs segmentation into two stages of localization and segmentation. The first stage localizes the target OARs using the bounding box, the second stage segments the target OARs within the bounding box, and both stages use neural networks. Among them, Wang et al. [101] and Francis et al. [104] used a 3D U-net in both stages. Lei et al. [103] used Faster RCNN to automatically locate the ROI of organs in the first stage. Korte et al. [106] demonstrated that the CNN is a suitable method for automatically segmenting parotid and submandibular glands in MRI images of HNC patients. The segmentation accuracy of the parotid and submandibular glands can be improved by cascading localizing CNNs, cropping and segmenting highresolution CNNs. FocusNet [69], [107] presented a novel deep neural network to solve the class imbalance problem in the segmentation of head and neck OARs. The small organs are first localized by the organ localization network. Then, combined with the high-resolution information of each small organ, multiscale features are input to the segmentation network together to accurately segment the small organs.

The organ localization by Larsson et al. [108], Zhao et al. [109], Ren et al. [110] and Huang et al. [111] was obtained with registration method followed by the application of convolutional neural networks for segmentation. Among them, Ren et al. [110] designed interleaved cascades of 3D-CNNs to segment each structure of interest. Since adjacent tissues are usually highly correlated from a physiological and anatomical perspective, using the initial segmentation results of a specific tissue can help refine the segmentation of other neighbouring tissues. Zhao et al. [109] proposed a new flexible knowledgeassisted convolutional neural network which combine deep learning and traditional methods to improve the segmentation accuracy in the second stage.

The vast majority of approaches require to determine the target areas prior to segmentation network training by different localization methods. For example, Ren et al. [110] localized organ regions through a multi-atlas-based method. Wang et al. [101] used separate CNNs to localize candidate areas. That is, their target organ region localization is constructed independently of organ segmentation, which will hinder the transmission of information between these two related learning tasks. On this basis, Liang et al. [112] proposed a multiorgan segmentation framework based on multi view spatial aggregation, which combines the learning of the organ localization subnetwork and the segmentation subnetwork to reduce the influence of background regions and neighbouring similar structures in the input data. Additionally, the proposed finegrained representation based on ROIs can improve the segmentation accuracy of organs with different sizes, especially the segmentation results of small organs.

The type of multistage method improves the organ segmentation accuracy, especially for small organs, which largely reduces the interference of the background. However, this twostep process has certain requirements for memory and training time, and the segmentation accuracy also depends largely on the regional localization accuracy. Better localization of organs and improvement of segmentation accuracy are still directions to be investigated in the future. c) Other Cascade Methods: In addition to probability maps and localization information, the first network can also provide other types of information, such as scale information and shape priors. For example, Tong et al. [113] combines the FCNN and a shape representation model (SRM) for head and neck OARs segmentation. The first-level network is the SRM for learning highly representative shape features in head and neck organs. The direct comparison of the FCNN with and without SRM shows that the SRM significantly improves the segmentation accuracy of nine organs with different sizes, morphological complexity, and different CT contrasts. Roth et al. [114] proposed a multiscale 3D FCN approach which is accomplished by two cascaded FCNs, where low-resolution 3D FCN predictions are upsampled, cropped, and connected to higher-resolution 3D FCN inputs. In this case, the primary network provides scale information to the secondary network. And the method uses the scale space pyramid with automatic context to perform high-resolution semantic image segmentation, while considering large contextual information from the lower resolution levels.


## B. Network Dimension

Considering the dimensionality of input images and convolutional kernels, multi-organ segmentation neural networks can be classified into 2D, 2.5D and 3D architectures, as shown in Fig. 9, and the differences between the three architectures will be discussed in follows. [117] used 2D networks for multi-organ segmentation. 2D architectures can reduce the GPU memory burden, but CT or MRI images are inherently 3D. Moreover, slicing images into 2D tends to ignore the rich information in the entire image voxel, so 2D models are insufficient for analysing the complex 3D structures in medical images.

3D multi-organ segmentation neural network architectures use 3D convolutional kernels, which can directly extract feature information from 3D medical images. Roth et al. [52], Zhu et al. [48], Gou et al. [50], and Jain et al. [118] used 3D architectures for multi-organ segmentation. However, due to GPU memory limitations, 3D architectures may face computationally intensive and memory shortage problems, so the majority of 3D network methods use sliding windows acting on patches. Zhu et al. [48] proposed a deep learning model called AnatomyNet, which receives full-volume head and neck CT images as the inputs and generates masks of all organs to be segmented at once. AnatomyNet only uses a down sampling layer in the first encoding block to consider the trade-off between GPU memory usage and network learning capability, which can occupy less GPU memory than other network structures while preserving information about small anatomical structures.

2) Multi-View-Based Methods: In medical image segmentation, it is crucial to make good use of the spatial information between medical image slices. Directly input 3D images into the network, the 3D images will occupy huge memory, or convert 3D images to 2D images, the spatial information between medical image slices will be directly discarded. Thus, the idea of multiple views has appeared, which means using 2.5D neural networks with multiple 2D slices and combining 2D convolution and 3D convolution.

The 2.5D multi-organ segmentation neural network architecture still uses 2D convolutional kernels, but the input of the network is multiple slices, either a stack of adjacent slices using interslice information [119], [120], or slices along three orthogonal directions (axial, coronal, and sagittal) [41], [42], [91], [121]. This 2.5D approach saves computational resources and makes good use of spatial information. It is also widely used in semi supervised-based methods, which are reviewed in Section V-B. Zhou et al. [122] segmented each 2D slice using the FCN by sampling a 3D CT case on three orthogonally oriented slices (2D images) and then assembled the segmented output (i.e., 2D slice results) back into 3D. Chen et al. [117] developed a multi-view training method at the ratio of 4:1:1 on different views (axial, coronal, and sagittal) and applied a majority voting strategy to combine the three predictions into a final segmentation. The results show that the method can remove some wrong segmentation areas in the single-view output, especially for the small intestine and duodenum. Wang et al. [123] used a statistical fusion approach to combine segmentation results from three views and relate the structural similarity of 2D views to the original 3D image. Liang et al. [121] performed context-based iterative refinement training on each of the three views and aggregated all the predicted probability maps of the three orthogonal views in the last iteration to obtain the final segmentation results. Experiments show that this multi-view framework outperforms the segmentation results of the three separate views.

Tang et al. [124] proposed a new framework for combining 3D and 2D models, which implements segmentation through high-resolution 2D convolution and extracting spatial contextual information through low-resolution 3D convolution. The corresponding 3D features used to guide 2D segmentation are controlled by a self-attentive mechanism, and the results show that this method consistently outperforms existing 2D and 3D models. Chen et al. [116] proposed a hybrid convolutional neural network, OrganNet2.5D, which can make full use of 3D image information to process different planar and depth image resolutions. OrganNet2.5D integrates 2D convolution and 3D convolution to extract both clear underlying edge features and rich high-level semantic features.

Some current studies only deal with 2D image, which avoids memory and computation problems but does not make full use of 3D image information. 2.5D methods can make better use of information from multiple views and improve single-view segmentation compared to 2D networks, but the spatial contextual information they can extract is still limited. Moreover, the current 2.5D methods using in multi-organ segmentation are the aggregation of three perspectives at the outcome level, and the intermediate processes are independent of each other; better use of the intermediate learning process is also the direction to be investigated [125]- [127]. Some studies have performed 3D convolution, but local patches need to be processed. For example, Networks that process full-volume 3D CT images, similar to AnatomyNet, use only a down sampling layer to preserve information about small anatomical structures, so the receptive field of these networks is limited. To solve this problem, DenseASPP with four expansion rates (3,6,12,18) is introduced into FocusNet [107]; however, when the expansion rates of the cascaded expanded convolution have a common factor relationship, grid problems affecting the segmentation accuracy may occur. Pure 3D networks also face the problem of increased parameter and computational burden, which limits the depth and performance of the network. Therefore, considering the memory and computational burden, better combination of multi-view information for more accurate multi-organ segmentation is still the future research direction.


## C. Network Dedicated Modules

The network architecture is very important to improve the multi-organ segmentation accuracy, but its design process is complex. In multi-organ segmentation tasks, there are many special mechanisms to improve the accuracy of organ segmentation, such as the dilation convolution module, feature pyramid module, and attention module. They improve multiorgan segmentation accuracy by increasing the perceptual field, aggregating features of different scales, and focusing the network on the segmented region. Cheng et al. [128] studied the performance improvement of each module of the network compared with the basic U-Net network in the head and neck segmentation task.

1) Shape Prior Module: Shape prior is more suitable for medical images than natural images because the spatial relationships between internal structures in medical images are relatively fixed. Therefore, considering anatomical priors in a multi-organ segmentation task will significantly improve the performance of multi-organ segmentation.

The current methods using anatomical priors fall into two main categories. One category is based on the idea of statistics, which calculates the average distribution of organs in a fully labelled dataset so that the prediction results can be as close as possible to the average distribution of organs [40], [42], [66], [129], [130]. The other is to train a shape representation model, which pretrains the shape representation model using the annotation of the training dataset, and then uses it as a regularization term to constrain the predictions of the segmentation network during training [64], [113]. It has also been shown that generative models can learn anatomical priors [131]. Therefore, it is a future research direction to consider using generative models (e.g., diffusion models, which are popular in the last two years [132], [133]) to better obtain anatomical prior knowledge to improve segmentation performance.

2) Dilated Convolutional Module: In traditional CNNs, down sampling and pooling operation are usually used several times to reduce the computation and expand the field of perception, which will lose the spatial information and make image reconstruction difficult. Dilated convolution (also known as "Atrous") introduces another parameter to the convolution layer, namely, the expansion rate, which can expand the field of perception to extract features across a larger spatial range without increasing the computational cost. Dilated convolution is a commonly used method in multi-organ segmentation tasks [40], [53], [120], [134], [135] that increases the size of the sampling space, allowing the neural network to extract features in a larger receptive field that captures multiscale contextual information. These contextual features can capture finer structural information, which is important for pinpointing organ location. Gibson et al. [40] used CNN networks with dilated convolution to accurately segment the liver, pancreas, stomach, and esophagus from abdominal CT. Men et al. [115] proposed a new method based on deep extended convolutional neural network (DDCNN) for fast and consistent automatic segmentation of clinical target volumes (CTVs) and OARs. Vesal et al. [135] introduced dilated convolution to 2D U-Net for segmenting the esophagus, heart, aorta, and thoracic trachea.

3) Multiscale Module: Neural networks extract the features of the target layer by layer. The lower layer networks have smaller perceptual fields and stronger representation of geometric detail information, and they have higher resolution but weaker representation of semantic information. The higher layer networks have larger perceptual fields and stronger representation of semantic information, but they have lower resolution of feature maps and weaker representation of geometric information, leading to the information loss of small targets. Common multiscale fusion modules include bottom-up, topdown, and laterally connected feature pyramids (FPNs) [136], spatial pooling pyramids (ASPPs) [137] combining dilated convolution and multiscale fusion, and others. In multi-organ segmentation tasks, multiscale feature fusion has been widely used in multi-organ segmentation due to the different sizes of the organs of interest. Jia and Wei [53] introduced the feature pyramid into the multi-organ segmentation network using two opposite feature pyramids, top-down and bottom-up forms, which can effectively handle multiscale changes and improve the segmentation accuracy of small targets. Shi et al. [120] used the pyramidal structure of lateral connections between encoders and decoders to capture contextual information at multiple scales. Srivastava et al. [138] proposed a new segmentation architecture named OARFocalFuseNet, which uses a focal modulation scheme to aggregate multiscale contexts in a specific resolution stream when performing multiscale fusion.


## 4) Attention Module:

The attention module can highlight important features by dynamically weighting them. This novel attention mechanism allows exploring the inherent selfattentiveness of the network and is essential for multi-organ segmentation tasks [65], [139]. Common attention mechanisms include channel attention, spatial attention, and self-attention.

Squeeze-and-excitation (SE) module [140] is a typical channel attention module which can focus on key parts of an image by generating a channel attention tensor. AnatomyNet [48] uses 3D SE residual blocks to segment the OARs of the head and neck, enabling the extraction of 3D features directly from CT images and adaptively calibrating the mapping of residual features within each feature channel. Liu et al. [141] proposed a new cross-layer spatial attentional map fusion network (CSAF-CNN) to segment multiple organs in the chest, which can effectively integrate the weights of different spatial attentional maps in the network, thus obtain more useful attentional maps. The average DSC of 22 organs in the head and neck was 72.50%, which was significantly better than U-Net (63.9%) and SE-UNet (67.9%). Gou et al. [50] designed a self-channel-spatial-attention neural network (SCSA-Net) for 3D head and neck OARs segmentation, which can adaptively enhance both channel and spatial features. Compared with SE-Res-Net and SE-Net, SCSA-Net improved the DSC of the optic nerve and submandibular gland by 0.06 and 0.03 and 0.05 and 0.04, respectively. Lin et al. [142] suggested to embed the variance uncertainty into the attention architecture and proposed a variance-aware attention U-Net network to improve the attention to error-prone regions (e.g., boundary regions) in multi-organ segmentation. Compared with existing methods, the segmentation results of small organs and organs with irregular structures (e.g., duodenum, esophagus, gallbladder, and pancreas) are significantly improved. Zhang et al. [51] proposed a new hybrid network (Weaving attention U-Net, WAU-Net) with a U-Net++ [143] structure that uses CNNs to extract the underlying features, and uses axial attention blocks to efficiently model global relationships at different levels of the network, which achieve competitive performance in the head and neck multi-organ segmentation task.


## 5) Other Modules:

The dense block [144] can efficiently use the information of the intermediate layer, and the residual block [145] can prevent gradient disappearance during backpropagation. These two modules are often embedded in the basic segmentation framework. The convolution kernel of the deformable convolution [146] can adapt itself to the actual situation and better extract the input features. Heinrich et al. [147] proposed a 3D abdominal multi-organ segmentation architecture with sparse deformable convolutions (OBELISK-Net) and showed that the combination with conventional CNNs can further improve the segmentation of small organs with large shape variations (e.g., pancreas, esophagus). The deformable convolutional block proposed by Shen et al. [148] can handle variations in the shape and size of different organs by generating reasonable receptive fields for different organs with additional trainable offsets. The strip pooling (strip pooling) [149] module can target long strip structures (e.g., esophagus and spinal cord) by using long pooling instead of traditional square pooling to avoid merging contaminated information from unrelated regions and better capture anisotropic and remote contextual information. For example, Zhang et al. [150] used a pool of anisotropic strips with three different directional receptive fields to capture the spatial relationships between multiple organs in the abdomen. Compared to network architectures, network modules have been widely utilized because of their relatively simple design process and the relative ease of embedding them into various architectures.


## D. Network Loss Function

As we all known, in addition to the network architecture or network modules, the segmentation accuracy also depends on the selected loss function. In multi-organ segmentation tasks, selecting a suitable loss function can reduce the class imbalance in deep learning and improve the segmentation accuracy of small organs.

Jadon [151] summarized the commonly used loss functions in semantic segmentation, which are classified into distribution-based loss functions, region-based loss functions, boundary-based loss functions, and compound-based loss functions. Common loss functions used for multi-organ segmentation include CE loss [152], Dice loss [153], Tversky loss [154], focal loss [155] and their combined loss functions.

1) CE Loss: The CE loss (cross-entropy loss function) [152] is an information theoretic measure that calculates the difference between the prediction of the network and the ground truth. Men et al. [115], Moeskops et al. [43], Zhang et al. [51] used CE loss for multi-organ segmentation. However, when the number of foreground pixels is much smaller than the background, CE loss will heavily bias the model towards the background, resulting in poor segmentation results. The weighted CE loss [156] adds weight parameters to each category based on CE loss. so that it can obtain better results in the case of unbalanced sample sizes compared to the original CE loss. Since there is a significant class imbalance problem in multi-organ segmentation, i.e., a very large difference in the number of voxels in different organs, using weighted CE loss will achieve better results than using only the CE loss. Trullo et al. [100] used a weighted CE loss to segment the heart, esophagus, trachea, and aorta in thechest image; Roth et al. [52] applied a weighted CE loss to abdomen multi-organ segmentation.

2) Dice Loss: Milletari et al. [56] proposed the Dice loss as a volume-based overlap measure, converting the voxel measure to the semantic label overlap measure, and becoming a commonly loss function in the segmentation task. Ibragimov and Xing [41] used the Dice loss to segment multiple organs of the head and neck. However, the use of the Dice loss alone does not eliminate the problem that the inherent nature of neural networks is beneficial to large volume organs. Inspired by the weighted CE loss, Sudre et al. [153] introduced the weighted Dice score (GDSC), which adaptively weighed its Dice values according to the current class size. Shen et al. [157] investigated three different types of GDSC based on class label frequencies (uniform, simple, and square) and evaluated their effects on segmentation accuracy. Gou et al. [50] used GDSC for head and neck multi-organ segmentation. Tappeiner et al. [158] introduced the class adaptive Dice loss to further compensate for high imbalances based on nnU-Net, and the results showed that the method could improve the performance of class imbalance segmentation tasks.

3) Other Losses: The Tversky loss [154] is a generalization of the Dice loss and can be optimized by adjusting the parameters to control the balance between false positives and false negatives. The focal loss [155] was proposed in the field of object detection to enhance the attention on samples that are difficult to segment. Similar to the focal loss, the focal Tversky loss [159] focuses on segmenting difficult samples by reducing the weights of simple sample losses. Berzoini et al. [54] used the focal Tversky loss on smaller organs, thus balancing the indices between organs of different sizes, increasing the weights of small samples that are difficult to segment and finally solving the class imbalance problem caused by the kidney and bladder. Inspired by the exponential logarithmic loss (ELD-Loss) [160], Liu et al. [141] introduced the top-k exponential logarithmic loss (TELD-Loss) to solve the class imbalance problem in the head and neck. The results showed that using this loss function has a strong ability to handle mislabelling. 4) Combined Loss: Each type of loss function has its own advantages and disadvantages. Combining multiple functions can be used for multi-organ segmentation. A more common method is the weighted sum of the Dice loss and CE loss, which attempts to solve the class imbalance problem with the Dice loss while using the CE loss for curve smoothing. Isensee et al. [101] proposed combining the Dice loss and CE loss to measure the overlap of voxel-like predicted outcomes and ground truth. Isler et al. [134], Srivastava et al. [138], Xu et al. [58], Lin et al. [142], and Song et al. [161] used the weighted combination of the Dice loss and CE loss for multi-organ segmentation. When small objects are involved, using only the Dice loss leads to a lower accuracy; when the predicted region does not overlap with the labelled region, using the CE loss allows the prediction to be as close to the label as possible. Zhu et al. [48] specifically studied different loss functions for the unbalanced head and neck region, and pointed out that the combination of the Dice loss and focal loss was superior to the ordinary Dice loss. Both Cheng et al. [128] and Chen et al. [116] used this combined loss function.

The conventional Dice loss is detrimental for smaller structures because a small amount of voxel misclassification leads to a large decrease in the Dice score. Applying the exponential logarithmic loss or combining the focal loss with the Dice loss can solve this problem. Using this kind of loss function does not require much adjustment to the network, however, it reduces the segmentation accuracy of the hard voxels in the region. On this basis, Lei et al. [162] proposed a new hardnessaware loss function that can focus more on hard voxels to achieve accurate segmentation. The ultimate goal of neural network optimization is the loss function, and designing a suitable loss function so that the network can improve the segmentation accuracy of various organs is still a research direction.


## V. IMPERFECT ANNOTATION-BASED METHODS

Currently, most of the methods in the multi-organ segmentation field are based on fully annotated methods. However, medical image data is usually hard to acquire and annotate. In particular, for multi-organ segmentation tasks, obtaining fully annotated datasets is quite difficult, which inspired the idea of using imperfect annotation. In this paper, imperfect annotations are classified into two categories. The first category is weak annotation-based methods, where weak annotation indicates that the data annotation is incomplete or imprecise in each case. For example, in multi-organ segmentation, each image has only one kind of organ annotated; each image has no pixel-level annotation but only category annotation; or the annotation is scribbled or contains noise. Another category is semi supervised-based methods, where semi supervision indicates that only a small portion of the total data is annotated and most of the remaining is unannotated. In the following, we introduce the application of these two types of methods in multi-organ segmentation.


## A. Weak Annotation-Based Methods

In medical image segmentation, it is a difficult task to obtain the annotation of multiple organs simultaneously on the same set of images. For example, many existing singleorgan datasets, such as LiTS [163], KiTS [164] (p19), and pancreas datasets [165], can only provide annotations for a single organ. However, multi-organ segmentation networks cannot be effectively trained solely based on these singleorgan annotated datasets. Therefore, many studies have started to explore learning unified multi-organ segmentation networks from partially labelled datasets. Based on the implementation methods, we divide the current studies into model-based approaches and pseudo label-based approaches.


## 1) Model-Based Methods:

The idea of the model-based approach is to realize a unified network for multiple partially labelled organs. Chen et al. [166] introduced a multi-branch decoder structure with a shared encoder and eight decoders to address the partial labelling problem. However, this structure is not flexible enough to be extended to new classes. Dmitriev and Kaufman [167] proposed conditional CNNs for learning multi-organ segmentation models, which integrate information of organ categories into the segmentation network. Zhang and Xie et al. [168], [169] proposed the idea of DoDNet. Similar to conditional CNN, they spliced the task encoding with the features extracted by the encoder, and introduced a dynamic parameter mechanism in the segmentation head. Zhang et al. [103] used the leading framework nn-UNet [170] as the backbone model, adding task encoding as supporting information to the decoder of nn-UNet, and combined the deep supervision mechanism to further refine the output of organs of different sizes. Wu et al. [171] proposed TGNet composed of task-guided attention module and task-guided residual block, which can highlight task-relevant features while suppressing task-irrelevant information during feature extraction. Liu et al. [172] first introduced incremental learning (IL) to aggregate partially labelled datasets in stages, and verified that the distribution of different partially labelled datasets misleads the process of IL. Xu and Yan [173] proposed a new federated multi-encoding U-Net (Fed-MENU) method that can effectively use independent datasets with different partial labels to train a unified model for multi-organ segmentation. The model outperformed any model trained on a single dataset as well as the model trained on all datasets combined. Fang and Yan [174] and Shi et al. [175] trained uniform models on partially labelled datasets by designing new network and proposing specific loss function.

2) Pseudo Label-Based Methods: The pseudo label-based methods generate pseudo labels of unlabelled organs by using partial-organ segmentation models trained in partially labelled datasets, which can be converted to fully supervised methods. Zhou et al. [129] proposed an a Prior-aware Neural Network (PaNN), which utilized prior statistics obtained from a fully labelled dataset to guide the training process based on partially labelled datasets. Huang et al. [176] proposed a weightaveraging joint training framework, which can correct the noise in the pseudo labels, so as to learn a more robust model. Zhang et al. [177] proposed a multi-teacher knowledge distillation framework that utilizes pseudo labels predicted by teacher models trained on partially labelled datasets to train student models for multi-organ segmentation. Lian et al. [130] proposed a multi-organ segmentation model (PRIMP) based on single and multiple organs anatomical priors. The model first generates pseudo labels for each partially labelled dataset so as to obtain a set of multi-organ datasets with pseudo label. Then the multi-organ segmentation model is trained on this dataset, and tested on another new dataset. For the first time, this method considers the domain discrepancy between partially labelled datasets and the tested multi-organ datasets.

In addition to partial annotation, weak annotation also includes image-level annotation, sparse annotation, and noisy annotation [178]. Regarding multi-organ segmentation tasks, Kanavati et al. [179] proposed a weakly supervised organ segmentation method based on classification forests for the liver, spleen, and kidney, in which the labels are scribbled on the organs.


## B. Semi Supervised-Based Methods

Semi supervised multi-organ segmentation methods make full use of unlabelled data to improve the segmentation performance, thus reducing the need for extensive annotation. In recent years, semi supervised learning has been widely used in medical image segmentation, such as heart segmentation [180]- [182], pancreas segmentation [183], and tumour target region segmentation [184]. A detailed review of semi supervised learning in medical images was presented by Jiao et al. [185], who classified semi supervised medical image segmentation methods into three paradigms: pseudo labelbased methods, consistency regularization-based methods, and knowledge prior-based methods. In this review, we focus on semi supervised multi-organ segmentation methods.

Ma et al. [36] established a new benchmark for semi supervised abdominal multi-organ segmentation, which developed a method based on pseudo labelling. The teacher model was first trained on the labelled data, and generated the pseudo labels for the unlabelled data. Then, the student model was trained on both the real labelled and pseudo labelled data. Finally, the teacher model was substituted with the student model to complete the training. The results on the liver, kidney, spleen, and pancreas show that using unlabelled data can improve the performance of multi-organ segmentation.

Multi-view methods are also widely used in semi supervised multi-organ segmentation, where the model is made to learn in a collaborative training manner to extract useful information from multiple planes (e.g., sagittal, coronal, and axial planes), and then use multi-plane fusion to generate more reliable pseudo labels, and thus train better segmentation networks. Zhou et al. [186] designed a system framework, DMPCT, for multi-organ segmentation of abdominal CT scans by fusing multi-planar information on unlabelled data during training. The framework uses a multi-planar fusion module to synthesize inferences and iteratively update pseudo labels for multiple configurations of unlabelled data. Xia et al. [187] proposed an uncertainty-aware multi-view collaborative training (UMCT) method based on uncertainty perception, which first obtains multiple views by spatial transformations such as rotation and alignment, then trains a 3D deep segmentation network on each view, and performs joint training by implementing multi-view consistency on unlabelled data.

In addition to the collaborative training approach, multiorgan segmentation is also suitable for consistency-based learning due to the large number of prospect categories and dense distribution of organs. Consistency learning encourages consistent output through networks with different parameters. Lai et al. [188] developed a semi supervised learning-based DLUNet for abdominal multi-organ segmentation, which consists of two lightweight U-Nets in the training phase. Moreover, regarding unlabelled data, the outputs obtained from two networks are used to supervise each other, which can improve the accuracy of these unlabelled data. It eventually achieves an average DSC of 0.8718 for 13 organs in the abdomen.

In addition, there are other semi supervised multi-organ segmentation-based methods. Lee et al. [189] proposed a discriminator module based on human-in-the-loop quality assurance (QA) to supervise the learning of unlabelled data. They used QA scores as a loss function for unlabelled data. Raju Cheng et al. [190] proposed a powerful semi supervised organ segmentation method, CHASe, for liver and lesion segmentation. It integrates co-training and heteromodality learning into a co-heterogeneous training framework. The framework is trained on a small single-phase dataset and can be adapted to label-free multicentre and multiphase clinical data.


## VI. DISCUSSION AND FUTURE TRENDS

In this paper, a systematic review of deep learning methods for multi-organ segmentation is presented from the perspectives of both full annotation and imperfect annotation. The main innovations of the full annotation method focus on the design of network architectures, the combination of network dimensions, the innovation of network modules and the proposal of new loss functions. In terms of the network architecture design, with the development of the transformer [75] architectures, better utilization of these advanced architectures for multi-organ segmentation is a promising direction, as well as the automatic search for the optimal architecture for each organ through neural network architecture search (NAS) [191]. In the network dimension, optimally combining 2D and 3D architectures is a worthwhile research direction. In terms of network module, more dedicated modules need to design to improve the segmentation accuracy according to the multi-organ segmentation task. In terms of the loss functions, targeting the class imbalance, geometric prior or introducing adversarial learning loss will have great potential for designing more comprehensive and diverse loss functions.

Full annotation methods rely on fully annotated and highquality datasets. Many imperfect annotation-based methods have been proposed for medical image segmentation in the last two years, including the aforementioned multi-organ segmentation based on weak annotation-based methods and semi annotation-based methods. However, compared to full annotation-based methods, the imperfect annotation-based methods have been less studied. It is a future research focus if imperfect annotation-based methods can be used more adequately to achieve the performance close to that of the full annotation-based methods.

Deep learning has already played a significant role in multiorgan segmentation task, but many challenges remain to be explored in the future, which are summarized in follows:


## A. Higher Segmentation Accuracy

The current multi-organ segmentation method is more effective in solving the segmentation of large organs and organs with standard contours, such as the brainstem and mandible in the head and neck; the left and right lungs and heart in the chest; and the liver, spleen, and stomach in the abdomen. Moreover, the DSC of various methods can basically reach 0.8 or higher, while for small organs, such as the optical chiasm in the head and neck (see Fig. 1(8)), the left and right optic nerves (see Fig. 1(6 and 7)), the DSC can only reach about 0.7; irregular organs such as the pancreas in the abdomen (Fig. 2(4)), and long striped organs such as the spinal cord ( Fig. 2(6)), the segmentation results are also not very satisfactory. The future research direction is to enhance the segmentation accuracy of these types of organs using more advanced automatic segmentation frameworks.


## B. More Comprehensive Public Datasets

Currently, public datasets covering multiple organs are not sufficient. And the vast majority of methods are validated on their private datasets, making it difficult to verify the generalizability of the models. Therefore, there is a need to establish multicentre public datasets of multi-organ segmentation with large data volumes, wide coverage, and strong clinical relevance in the future.


## C. Better Use of Imperfect Annotations

The vast majority of current methods are based on full annotation methods. Since medical image data are usually not easy to collect and annotating all the organs on the same image is a time-consuming and laborious work. Further studies can be performed to better utilize imperfect annotations [192], [193], including the use of weakly annotated datasets and semi annotated datasets.


## D. Study of Transfer Learning Models

Existing deep learning models usually trained on one part of the body, which usually tend to obtain poor results when migrated to other datasets or applied to other parts of the body. Therefore, transfer learning models need to be explored in the future. For example, Fu et al. [194] proposed a new method called domain adaptive relational reasoning (DARR). It is used to generalize 3D multi-organ segmentation models to medical data from different domains. In addition, a very significant problem with medical images compared to other natural images is that many private datasets are not publicly available, and many hospitals only release trained models. Therefore, source free domain adaptation problem will be a very important research direction in the future. For example, Hong et al. [195] proposed a source free unsupervised domain adaptive cross-modal approach for abdomen multi-organ segmentation.


## VII. CONCLUSION

In this paper, we systematically review 214 deep learningbased multi-organ segmentation studies in two broad categories, namely full annotation-based methods and imperfect annotation-based methods for multiple parts, including the head and neck, thorax and abdomen. In the fully labelled methods, we summarize the existing methods according to network architectures, network modules, network dimensions, and loss functions. In the imperfect annotation-based methods, we summarize both weak annotation-based methods and semi annotated-based methods. On this basis, we also put forward tailored solutions for some current difficulties and shortcomings in this field, and illustrate the future trends. The comprehensive survey shows that multi-organ segmentation algorithm based on deep learning is rapidly developing towards a new era of more accurate, more detailed and more automated analysis.            

## Fig. 1 .
1Schematic diagram of the organs of the head and neck, where the numbers are arranged in order: (1) brainstem, (2) left eye, (3) right eye,

## Fig. 2 .
2Schematic diagram of the thoracic organs, where the numbers are arranged in order: (1) left lung, (2) right lung, (3) heart, (4) esophagus, (5) trachea, and (6) spinal cord.

## Fig. 4 .
4Illustration of the percentage of voxels in each organ of the head and neck (a), chest (b), and abdomen (c), respectively.

## Fig. 5 .
5Framework diagram of the overview.

## Fig. 6 .
6General overview of the learning paradigms reviewed in this paper.

## Fig. 7 .
7Coarse-to-fine-based segmentation method.

## Fig. 8 .
8Localization and segmentation based method. Wang et al. [101], Men et al. [102], Lei et al. [103], Francis et al. [104], Tang et al.

## Fig. 9 .
9Different network dimensions. 1) 2D-& 3D-Based Methods: The input of the 2D multiorgan segmentation neural network is slices from a threedimensional medical image, and the convolution kernel is also two-dimensional. Men et al. [115], Trullo et al. [100], Gibson et al. [57], Chen et al. [116], Zhang et al. [51], Chen et al.


ACKNOWLEDGMENTS This work was supported by the National Natural Science Foundation of China under grant 82072021. This work was also supported by the medical-industrial integration project of Fudan University under grant XM03211181. SUPPLEMENTARY MATERIALS

## TABLE I FREQUENTLY
IUSED DATASET FOR MULTI-ORGAN SEGMENTATIONYear 
Dataset 
Modality Part 
Number of organs (specific or-
gans) 
Quantity 
Labelling status Image size 

2015 

MICCAI Multi-
Atlas Labelling 
Beyond 
the 
Cranial 
Vault 
(BTCV) [29] 

CT 
Abdomen 

13 (spleen, right kidney, left 
kidney, gallbladder, esophagus, 
liver, stomach, aorta, inferior 
vena cava, portal and splenic 
veins, pancreas, right adrenal 
gland, left adrenal gland) 

50 (30 training 
and 20 testing) 

The training set 
are labelled, the 
test set are not 
labelled 

512 × 512 × [85∼198] 

2015 

MICCAI 
head 
and neck Auto 
Segmentation 
Challenge (HNC) 
[30] 

CT 
Head and neck 

9 (brainstem, mandible, chi-
asm, left optic nerves, right op-
tic nerves, left parotid glands, 
right parotid glands, left sub-
mandibular glands, right sub-
mandibular glands) 

35 (25 training, 
10 off-site tests, 
5 on-site tests) 

Labelled 
512 × 512 × [110∼190] 

2015 
Synapse multi-
organ segmentation 
dataset (Synapse) 

CT 
Abdomen 

13 (spleen, right kidney, left 
kidney, gallbladder, esophagus, 
liver, stomach, aorta, inferior 
vena cava, portal vein and 
splenic vein, pancreas, right 
adrenal gland, left adrenal 
gland) 

50 (30 training, 
20 testing) 
Labelled 
512 × 512 × [85∼198] 

2015 

Public Domain 
Database 
for 
Computational 
Anatomy 
(PDDCA) [30] 

CT 
Head and neck 

9 (brainstem, mandible, chi-
asm, left optic nerves, right op-
tic nerves, left parotid glands, 
right parotid glands, left sub-
mandibular glands, right sub-
mandibular glands) 

48 (25 training, 
8 
additional 
training, 
10 
off-site and 5 
on-site tests) 

Labelled 
512 × 512 × [110∼190] 

2017 

Thoracic Auto-
segmentation 
Challenge 
(AAPM) [31] 

CT 
Thorax 
5 (left lung, right lung, heart, 
Esophagus, spinal cord) 

60 (36 training, 
12 off-site tests, 
12 on-site tests) 

Labelled 
512 × 512 × [103∼279] 

2019 
Combined (CT-MR) 
Healthy Abdominal 
Organ Segmentation 
(CHAOS) [32] 

CT 
Abdomen 
4 (left kidney, right kidney, liver, 
spleen) 

40 (20 training 
and 20 testing) 

Labelled train-
ing set and un-
labeled test set 

512 × 512 × [78∼294] 

MR 

40 (20 training, 
20 testing) × 3 
sequences 

Labelled train-
ing set and un-
labeled test set 

256 × 256 × [26∼50] 

2019 

SegTHOR 
Challenge: 
Segmentation 
of 
Thoracic 
Organs at Risk 
in CT Images 
(SegTHOR) [33] 

CT 
Thorax 
5 (left and right lungs, heart, 
esophagus, spinal cord) 

60 (36 training, 
12 off-site tests, 
12 on-site tests) 

Labelled 
512 × 512 × N 

2019 

Annotations for 
Body 
Organ 
Localization 
based 
on 
MICCAI LITS 
Dataset [34] 

CT 
Thorax 

11 (heart, left lung, right lung, 
liver, spleen, pancreas, left kid-
ney, right kidney, bladder, left 
femoral head, right femoral 
head) 

201 (131 train-
ing and 70 test-
ing) 

Bounding 
boxes labelled 
512 × 512 × N 

2019 
Automatic Structure 
Segmentation for 
Radiotherapy Planning 
Challenge 2019 
(StructSeg) 

CT 
Head and neck 

22 (left eye, right eye, left lens, 
right lens, left optical nerve, 
right optical nerve, chiasm, pi-
tuitary, brainstem, left tempo-
ral lobes, right temporal lobes, 
spinal cord, left parotid gland, 
right parotid gland, left inner 
ear, right inner ear, left mid-
dle ear, right middle ear, left 
temporomandibular joint, right 
temporomandibular joint, left 
mandible, right mandible) 

60 (50 training, 
10 testing) 
Labelled 
training 
set and 
unlabeled 
test set 

512 × 512 × [98∼140] 

Thorax 
6 (left lung, right lung, spinal 
cord, esophagus, heart, trachea) 

60 (50 training, 
10 testing) 

2020 

OpenKBP: The 
open-access 
knowledge-based 
planning grand 
challenge 
and 
dataset [35] 

CT 
Head and neck 

7 (brainstem, spinal cord, right 
parotid, left parotid, larynx, 
esophagus, mandible) 

340 (200 train-
ing, 40 validat-
ing, 100 test-
ing) 

Labelled 
128×128×128 

2021 
Abdomenct-1k 
[36] 
CT 
Abdomen 
5 (liver, right and left kidneys, 
spleen, pancreas) 
1112 
Labelled 
512 × 512 × N 
process 3D medical data by introducing residual connections 
and using convolutional layers instead of pooling layers in 
the original U-Net. Gibson et al. [57] used dense V-Networks 
to segment the pancreas, esophagus, stomach, liver, spleen, 
gallbladder, left kidney, and duodenum of the abdomen. Xu 
et al. 

## TABLE II DSC
II-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN SINGLE-STATE SEGMENTATION METHODS FOR THE HEAD AND NECKRef 
Backbone 
Datasets 
Quantity Organ type Brainstem 
Mandible 
Parotid gland 
Submandibular gland Optic nerve 
Chiasm 
Left 
Right 
Left 
Right 
Left 
Right 

Ibragimov and Xing [41] 2.5D CNN 
Private (CT) 
50 
13 
-
0.895 
0.766 0.779 0.697 
0.730 
0.639 
0.645 0.374 
Fritscher et al. [42] 
2.5D CNN 
HNC (CT) [30] 
30 
3 
-
-
0.810 -
0.650 -
-
-
0.520 
Zhu et al. [48] 
3D U-Net 
Private (CT) 
261 
9 
0.867 
0.925 
0.881 0.874 0.814 0.813 
0.721 0.706 0.532 
Van Rooij et al. [49] 
3D U-Net 
Private (CT) 
157 
11 
0.640 
-
0.830 0.830 
0.820 0.810 
-
-
-
Tong et al. [64] 
3D GAN 
PDDCA (CT) [30] 
48 
9 
0.867 
0.939 
0.855 0.858 0.807 0.819 
0.664 0.699 
0.592 
Tong et al. [64] 
3D GAN 
Private (MRI) 
25 
9 
0.916 
0.816 
0.865 
0.825 -
-
0.717 
0.693 0.589 
Gou et al. [50] 
3D U-Net 
HNC (CT) [30] 
48 
9 
0.880 
0.940 
0.870 
0.860 0.780 0.810 
0.720 0.710 0.610 
Liu et al. [196] 
3D U-Net 
Private (MRI & CT) 
45 
19 
0.880 
0.890 
0.890 
0.880 -
-
0.720 0.720 0.760 
Liu et al. [196] 
3D U-Net 
HNC (CT) [30] 
48 
9 
0.910 
0.960 
0.880 
0.880 0.860 0.850 
0.780 0.780 0.730 
Chen et al. [116] 
2.5D U-Net 
Private (CT) 
307 
24 
-
-
-
-
-
-
0.711 
0.712 0.598 
Chen et al. [116] 
2.5D U-Net 
HNC (CT) [30] 
48 
9 
0.872 
0.922 
0.867 
0.858 0.821 
0.821 
0.750 
0.741 0.663 
Liu et al. [141] 
2D U-Net 
StructSeg (CT) 
50 
22 
0.864 
0.906 
0.802 
0.826 -
-
0.770 0.647 0.712 
Cros et al. [197] 
3D U-Net 
Private (CT) 
200 
12 
-
0.900 
0.760 0.760 
0.740 -
-
-
-
Lei et al. [162] 
2.5D U-Net 
StructSeg (CT) 
50 
22 
0.897 
0.914 
0.857 
0.873 -
-
0.680 
0.663 0.566 
Lei et al. [162] 
2.5D U-Net 
Hybrid HAN (CT) 
165 
7 
0.874 
0.900 
0.847 
0.846 -
-
0.624 
0.621 0.290 
Zhang et al. [51] 
2D U-Net 
HNC (CT) [30] 
48 
9 
0.840 
0.900 
0.820 0.830 
0.820 0.810 
0.670 0.710 0.660 
Srivastava et al. [138] 
3D U-Net 
OpenKBP (CT) [35] 
188 
5 
0.803 
0.883 
0.799 
0.773 -
-
-
-
-
Podobnik et al. [59] 
2D nnU-net 
Private (CT&MR) 
56 
31 
0.836 
0.898 
0.817 0.765 
0.716 0.670 
0.572 0.604 0.387 
Kan et al. [85] 
3D Transformer and U-Net Private (CT) 
94 
18 
0.871 
0.925 
0.821 
0.844 -
-
0.717 
0.679 0.328 
Jiang et al. [198] 
2D U-Net 
PDDCA (CT) [30] 
16 
6 
0.920 
0.950 
0.880 
0.880 0.820 
0.830 

Francis et al. [199] 
3D U-Net 
Private (CT) 
232 
7 
0.890 
0.932 
0.852 0.870 
0.744 0.764 0.635 
HNC (CT) [30] 
48 
7 
0.862 
0.940 
0.885 0.885 
0.728 0.723 0.620 

Isler et al. [134] 
2D U-Net 
HNC (CT) [30] 
48 
6 
0.830 
-
0.790 0.760 
-
-
0.580 0.540 0.520 
OpenKBP (CT) [35] 
188 
5 
0.800 
0.860 
0.750 0.760 
-
-
-
-
-



## TABLE III DSC
III-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN SINGLE-STAGE SEGMENTATION METHODS FOR THE ABDOMENRef 
Backbone 
Datasets 
Quantity 
Organ type Liver 
Spleen 
Kidney 
Pancreas 
Gallbladder Stomach 
Left 
Right 

Gibson et al. [40] 
3D CNN 
TCIA [200], [201] & 
BTCV [29] (CT) 
72 
4 
0.920 -
-
-
0.660 
-
0.830 

Men et al. [115] 
2D CNN 
Private (CT) 
278 
5 
-
-
-
-
-
-
-
Shen et al. [157] 
3D U-Net 
Private (CT) 
377 
7 
0.965 0.947 
-
-
0.847 
0.808 
0.963 

Gibson et al. [57] 
3D V-Net 
TCIA [200], [201] & 
BTCV [29] (CT) 
90 
8 
0.960 0.960 
0.950 -
0.780 
0.840 
0.900 

Roth et al. [52] 
3D U-Net 
Private (CT) 
377 
7 
0.971 0.977 
-
-
0.849 
0.851 
0.961 
Cai et al. [65] 
2D FCN 
Private (CT) 
120 
16 
0.96 
0.951 
0.956 0.954 0.785 
0.797 
0.909 

Cai et al. [65] 
3D GAN 
Private (CT) 
131(liver)+281(spleen) 
+41(pancreas) 
3 
0.944 0.960 
-
-
0.743 
-
-

Heinrich et al. [147] 
3D U-Net 
TCIA (CT) [200], [201] 
43 
8 
0.954 0.944 
-
-
0.702 
0.753 
0.868 
Private (CT) 
10 
7 

Ahn et al. [119] 
2.5D CNN 
Private (CT) 
813+150 
2 
0.973 0.974 
-
-
-
-
-
Private (CT) 
813+50 
2 
0.983 0.968 
-
-
-
-
-

Fu et al. [194] 
3D V-Net 
Synapse (CT) [2] 
90 
8 
Aorta, gallbladder, left kidney, right kidney, liver, pancreas, spleen and 
stomach average DSC: 0.698 

Hatamizadeh et al. [87] 
3D Transformer 
and U-Net 
BTCV (CT) [29] 
30 
13 
0.983 0.972 
0.954 0.942 0.799 
0.825 
0.945 

Chen et al. [117] 
2.5D U-Net 
Private (MR) 
102 
10 
0.963 0.946 
0.954 0.954 0.880 
0.732 
0.923 
Tang et al. [124] 
2.5D U-Net 
ABD-110 (CT) [124] 
110 
11 
0.964 0.959 
0.960 0.957 0.821 
0.822 
0.875 
Jia and Wei [53] 
3D U-Net 
CHAOS (CT [32] 
20 
4 
0.934 0.896 
0.937 0.949 -
-
-

Lin et al. [142] 
3D U-Net 
TCIA [200], [201] & 
BTCV [29] (CT) 
90 
8 
0.953 0.920 
0.902 -
0.742 
0.760 
0.862 

Cao et al. [77] 
2D Transformer Synapse (CT) [2] 
30 
8 
0.943 0.907 
0.833 0.796 0.566 
0.665 
0.766 

Chen et al. [86] 
2D Transformer 
And U-Net 
Synapse (CT) [2] 
30 
8 
0.941 0.851 
0.819 0.770 0.559 
0.631 
0.756 

Song et al. [161] 
2D CNN 
Synapse (CT) [2] 
30 
8 
0.959 0.926 
0.906 0.892 0.687 
0.671 
0.839 
Kumar et al. [202] 
2D GAN 
CHAOS (CT) [32] 
40 
4 
Liver, left kidney, right kidney and spleen average DSC: 0.970 
Huang et al. [78] 
2D Transformer 
Synapse (CT) [2] 
30 
8 
0.944 0.919 
0.852 0.820 0.657 
0.687 
0.808 

Suo et al. [84] 
2D Transformer 
And U-Net 
Synapse (CT) [2] 
30 
8 
0.951 0.914 
0.890 0.851 0.699 
0.720 
0.826 

Xu et al. [58] 
3D V-Net 

AbdomenCT-1K [36] 
+Private (CT) 
1112+100 
4 
0.953 0.920 
0.914 
0.747 
-
-

AbdomenCT-1K [36] 
+TCIA [200], [201] & 
BTCV (CT) [29] 

1112+90 
4 
0.961 0.954 
0.918 -
0.784 
-
-

Berzoini et al. [54] 
2D U-Net 
Open-source CT-org 
dataset [203] 
140 
5 
0.922 -
0.837 
-
-
-

Shen et al. [148] 
2D U-Net 
TCIA (CT) [200], [201] 
42 
5 
0.960 -
-
-
0.754 
0.805 
0.889 

Hong et al. [195] 
2D U-Net 
BTCV [29] & CHAOS 
[32] (CT) 
30+20 
4 
0.884 0.911 
0.864 0.891 -
-
-

Xie et al. [79] 
3D CNN And 
Transformer 
BTCV [29] (CT) 
30 
11 
0.971 0.963 
0.939 
0.831 
0.666 
0.882 

Srivastava et al. [138] 
3D U-Net 
Synapse (CT) [2] 
30 
8 
0.950 0.870 
0.842 0.824 0.681 
0.675 
0.760 
Jiang et al. [198] 
2D U-Net 
BTCV (CT) [29] 
30 
12 
0.969 0.958 
0.943 0.921 0.798 
0.786 
0.906 


## TABLE IV DSC
IV-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN SINGLE-STATE SEGMENTATION METHODS FOR THE THORAX BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN COARSE-TO-FINE SEGMENTATION METHODS FOR THE HEAD AND NECKRef 
Backbone 
Dataset 
Quantity 
Organ type Heart 
Esophagus 
Trachea 
Aorta 
Spinal cord Stomach 
Left 
Right 

Trullo et al. [100] 
2D FCN 
Private (CT) 
30 
4 
0.900 0.670 
0.820 -
-
0.860 
-
Dong et al. [66] 
3D GAN 
AAPM (CT) [31] 
35 
4 
0.870 0.750 
-
0.970 0.970 
-
0.900 
Vu et al. [204] 
2D U-Net 
Private (CT) 
22411(2D) 5 
0.910 0.630 
-
0.960 0.960 
-
0.710 
Lambert et al. [55] 
Gali et al. [205] 

2D U-Net 
2D U-Net 

SegTHOR (CT) [33] 
SegTHOR (CT) [33] 

60 
60 

4 
4 

0.930 
0.860 

0.820 
0.469 

0.850 
0.643 

-
-

-
-

0.910 
0.854 

-
-
Vesal et al. [135] 
2D U-Net 
SegTHOR (CT) [33] 60 
4 
0.941 0.858 
0.926 -
-
0.938 
-
Shi et al. [120] 
2.5D U-Net 
StructSeg (CT) [1] 
50 
5 
0.941 0.821 
0.882 0.968 0.971 
-
0.902 
Mahmood et al. [206] 2D U-Net 
AAPM (CT) [31] 
60 
5 
0.880 0.660 
-
0.970 0.970 
-
0.800 
Zhang et al. [207] 
2D FCN 
Private (CT) 
36 
6 
0.860 0.670 
0.910 0.950 0.960 
-
0.890 

TABLE V 
DSC-Ref 
Coarse 
Fine 
Datasets 
Quantity 
Organ type Brainstem 
Mandible 
Parotid gland 
Submandibular gland 
Optic Nerve 
Chiasm 
Left 
Right 
Left 
Right 
Left 
Right 

Ren et al. [110] 
3D CNN 
3D CNN 
HNC (CT) [30] 
48 
3 
-
-
-
-
-
-
0.720 0.700 0.580 

Tappeiner et al. [90] 
3D CNN 
3D CNN 
HNC (CT) [30] 
40 
7 
0.820 
0.910 
0.800 
0.810 
-
-
0.640 0.630 0.420 

Pu et al. [91] 
2.5D U-Net 
3D U-Net 
HNC (CT) [30] 48 
9 
0.880 
0.940 
0.860 0.865 
0.788 
0.802 
0.743 0.768 0.612 

Ma et al. [92] 
3D U-Net 
HNC (CT) [30] 
48 
9 
0.879 
0.945 
0.892 
0.884 
0.829 
0.815 
0.753 0.747 0.659 

Fang et al. [70] 
2D FCN 
3D U-Net 
HNC (CT) [30] 
32 
9 
0.849 
0.924 
0.842 
0.849 
0.734 0.782 
0.676 0.684 0.547 
Private (CT) 
56 
14 
0.863 
0.905 
0.582 0.687 
0.668 
0.575 



## TABLE VI DSC
VI-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN COARSE-TO-FINE SEGMENTATION METHODS FOR THE ABDOMENTABLE VII DSC-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN COARSE-TO-FINE SEGMENTATION METHODS FOR THE THORAXRef 
Coarse 
Fine 
Datasets 
Quantity 
Category 
Liver 
Spleen 
Kidney 
Pancreas Gallbladder Stomach 
Left 
Right 

Hu et al. [93] 
3D FCN 
Refinement Model 
Private (CT) 
140 
4 
0.960 
0.942 
0.954 
-
-
-
Roth et al. [114] 
3D FCN 
3D FCN 
Private (CT) 
331 
3 
0.932 
0.906 
-
-
0.631 
0.706 
0.843 
Wang et al. [123] 
2.5D FCN 2.5D FCN 
Private (CT) 
236 
13 
0.980 0.971 
0.968 
0.984 0.878 
0.905 
0.952 
Zhang et al. [94] 
3D V-Net 
3D V-Net 
BTCV (CT) [29] 30 
13 
0.945 0.915 
0.909 0.919 0.694 
0.682 
0.784 
Xie et al. [95] 
2.5D FCN 
2.5D FCN 
Private (CT) 
200 
16 
0.969 0.968 
0.962 0.960 0.877 
0.894 
0.951 

Zhang et al. [150] 
3D U-Net 
3D U-Net 
FLARE 
2021 (CT) [208] 
511 
4 
0.954 0.942 
0.936 
0.753 
-
-

Lee et al. [96] 
3D U-Net 
3D U-Net 
Private (CT) 
100 
13 
0.960 0.965 
0.945 0.920 0.766 
0.793 
0.833 
Kakeya et al. [209] 3D U-Net 
3D U-Net 
Private (CT) 
47 
8 
0.971 
0.969 
0.984 0.975 0.861 
0.918 
-

Ref 
Coarse 
Fine 
Datasets 
Quantity Category Heart 
Esophagus Trachea 
Lung 
Aorta 
Spinal cord 
Left 
Right 

Trullo et al. [210] 
2D FCN 
2D FCN 
Private (CT) 
30 
4 
0.900 
0.690 
0.870 
-
-
0.89 
-
Cao et al. [211] 
2D U-Net 
2D U-Net SegTHOR (CT) [33] 50 
6 
0.945 
0.850 
0.807 
0.97 0.966 -
0.91 
Zhang et al. [94] 
3D V-Net 
3D V-Net 
SegTHOR (CT) [33] 
50 
4 
0.930 0.785 
0.890 
-
-
0.916 
-


## TABLE VIII
VIIIDSC-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN LOCALIZATION AND SEGMENTATION METHODS FOR THE HEAD AND NECKRef 
Localization 
Segmentation 
Datasets 
Quantity Category Brainstem 
Mandible 
Parotid gland 
Submandibular gland Optic nerve 
Chiasm 
Left 
Right 
Left 
Right 
Left 
Right 

Wang et al. [101] 
3D U-Net 
3D U-Net 
HNC (CT) [30] 
48 
9 
0.875 
0.930 
0.864 0.848 0.758 0.733 
0.737 0.736 0.451 
Men et al. [102] 
3D U-Net 
3D U-Net 
TCIA (CT) [200], [201] 100 
7 
0.900 
0.920 
0.860 0.860 -
-
-
-
-
Tang et al. [105] 
3D U-Net 
3D U-Net 
Private (CT) 
215 
28 
0.863 
0.931 
0.849 0.849 0.807 0.825 
0.757 0.761 0.642 
Tang et al. [105] 
3D U-Net 
3D U-Net 
PDDCA (CT) [30] 
48 
9 
0.875 
0.95 
0.887 0.875 0.823 0.815 
0.748 0.723 0.615 
Yang et al. [212] 
3D CNN 
2D U-Net 
Private (CT) 
88 
17 
0.831 
0.875 
0.807 0.811 -
-
0.638 0.675 -

Liang et al. [112] 
2D CNN 
2D CNN 
Private (CT) 
185 
18 
0.896 
left: 0.914; 
right: 0.912 
0.852 0.85 
-
-
0.661 0.717 -

Gao et al. [107] 
3D CNN 
3D CNN 
Private (CT) 
50 
18 
0.858 
-
0.772 0.800 -
-
0.639 0.617 0.638 
Gao et al. [107] 
3D CNN 
3D CNN 
HNC (CT) [30] 
48 
9 
0.875 
0.935 
0.863 0.879 0.798 0.801 
0.735 0.744 0.596 
Liang et al. [121] 
2.5D CNN 
2.5D CNN 
HNC (CT) [30] 
48 
9 
0.923 
0.941 
0.876 
0.808 
0.736 
0.713 

Liang et al. [121] 
2.5D CNN 
2.5D CNN 
Private (CT) 
96 
11 
-
Left: 0.911; 
right: 0.914 
0.883 0.868 -
-
0.871 0.874 -

Lei et al. [103] 
3D CNN 
3D U-Net 
Private (CT) 
15 
8 
-
0.850 
0.820 0.810 -
-
-
-
-
Huang et al. [111] 3D CNN 
3D CNN 
HNC (CT) [30] 
48 
9 
0.879 
0.916 
0.884 0.878 0.801 0.776 
0.677 0.706 0.643 
Huang et al. [111] 3D CNN 
3D CNN 
StructSeg (CT) [1] 
15 
7 
0.769 
0.807 
0.802 0.802 -
-
0.499 0.534 0.211 
Huang et al. [111] 3D CNN 
3D CNN 
Private (CT) 
15 
9 
0.957 
0.848 
0.962 0.946 0.846 0.808 
0.824 0.843 0.434 

Korte et al. [106] 
3D U-Net 
3D U-Net 
Public RT-MAC dataset 
(MRI) [213] 
31 
8 
-
-
0.860 0.857 0.830 0.785 
-
-
-

Korte et al. [106] 
3D U-Net 
3D U-Net 
Private (MRI) 
10 
8 
-
-
0.730 0.775 0.537 0.435 
-
-
-

Gao et al. [69] 
3D CNN 
3D CNN 
Private (CT) 
1164 
22 
0.891 
Left: 0.924; 
right: 0.925 
0.846 0.87 
-
-
0.713 0.753 0.612 

Gao et al. [69] 
3D CNN 
3D CNN 
HNC (CT) [30] 
48 
9 
0.882 
0.947 
0.898 0.881 0.840 0.838 
0.790 0.817 0.713 



## TABLE IX DSC
IX-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN LOCALIZATION AND SEGMENTATION METHODS FOR THE ABDOMENTABLE X DSC-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN LOCALIZATION AND SEGMENTATION METHODS FOR THE THORAX TABLE XI DSC-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN SINGLE-STATE SEGMENTATION METHODS FOR THE HEAD AND NECK-SUPPLEMENTARY MATERIAL Spinal cord: 0.897; pituitary gland: 0.608; oral cavity: 0.908; left eye: 0.907; right eye: 0.902; left lens: 0.724; right lens: 0.689; left TMJ: 0.789; right TMJ: 0.778; left temporal lobe: 0.803; right temporal lobe: 0.802 Isler et al. [134]Ref 
Localization 
Segmentation Dataset 
Quantity 
Category Liver 
Spleen 
Kidney 
Pancreas Gallbladder Stomach 
Left 
Right 

Larsson et al. [108] 
Multi-Atlas 
3D FCN 
BTCV (CT) [29] 
30 
13 
0.949 0.936 
0.911 0.897 0.646 
0.613 
0.764 

Zhao et al. [109] 
Registration 
2D U-Net 
VISCERAL challenge dataset Nonenhanced 
CT (CTwb) [214] 
20 
4 
-
-
-
-
0.583 
0.473 
-

Zhao et al. [109] 
Registration 
2D U-Net 
VISCERAL challenge dataset enhanced 
CT (CTce) [214] 
20 
4 
-
-
-
-
0.588 
0.624 
-

Ref 
Localization 
Segmentation 
Dataset 
Quantity Category Heart 
Esophagus Trachea 
Lung 
Aorta Spinal cord 
Left 
Right 

Francis et al. [211] 3D U-Net 
Two 3D U-Net AAPM (CT) [31] 
60 
5 
0.941 0.738 
-
0.979 0.973 -
0.899 
Feng et al. [104] 
3D U-Net 
3D U-Net 
AAPM (CT) [31] 
60 
5 
0.925 
0.726 
-
0.979 0.972 -
0.893 
Feng et al. [104] 
3D U-Net 
3D U-Net 
Private (CT) 
30 
5 
0.86 
0.685 
-
0.976 0.977 -
0.852 

Ref 
Backbone 
Datasets 
Quantity Organ type Other organs 

Ibragimov and Xing [41] 
2.5D CNN 
Private (CT) 
50 
13 
Pharynx: 0.856; Left eyeball: 0.884; Right eyeball: 0.877; Spinal cord: 0.870; Larynx: 0.693 

Van Rooij et al. [49] 
3D U-Net 
Private (CT) 
157 
11 
Larynx: 0.780; Pharyngeal Constrictor: 0.680; Cricopharynx: 0.730; Upper esophageal sphinc-
ter: 0.810; esophagus: 0.600; Oral Cavity: 0.780 
Tong et al. [64] 
3D GAN 
Private (MRI) 
25 
9 
Pharynx: 0.706; Larynx: 0.799 

Liu et al. [196] 
3D U-Net 
Private (MRI & CT) 45 
19 

Pharynx: 0.740; spinal cord: 0.840; left cochlea: 0.760; right cochlea: 0.750; esophagus: 0.850; 
oral cavity: 0.900; left eye: 0.890; right eye: 0.870; left lens: 0.730; right lens: 0.730; larynx: 
0.900; brain: 0.950 

Chen et al. [116] 
2.5D U-Net 
Private (CT) 
307 
24 
Pituitary: 0.756; left middle ear: 0.869; right middle ear: 0.859; left lens: 0.844; right lens: 
0.839; left temporomandibular joint: 0.838; right temporomandibular joint: 0.829 

Liu et al. [141] 
2D U-Net 
StructSeg (CT) 
50 
22 

Left eye: 0.858; right eye: 0.882; spinal cord: 0.804; pituitary: 0.503; left middle ear: 0.825; 
right middle ear: 0.717; left lens: 0.898; right lens: 0.786; left temporomandibular joint: 0.723; 
right temporomandibular joint: 0.824 

Cros et al. [197] 
3D U-Net 
Private (CT) 
200 
12 

Medullary canal: 0.870; Outer medullary canal: 0.860; oral cavity: 0.660; esophagus: 0.600; 
trachea: 0.670; trunk: 0.670; outer trunk: 0.700; inner ears: 0.710; eyes: 0.770; sub-maxillary 
glands: 0.740 

Lei et al. [162] 
3D U-Net 
StructSeg (CT) 
50 
22 

Left eye: 0.886; right eye: 0.873; spinal cord: 0.830; pituitary: 0.661; left middle ear: 0.826; 
right middle ear: 0.783; left lens: 0.815; right lens: 0.754; left temporomandibular joint: 0.757; 
right temporomandibular joint: 0.772 
Srivastava et al. [138] 
3D U-Net 
OpenKBP (CT) [36] 
188 
5 
Spinal cord: 0.740 

Podobnik et al. [59] 
2D nnU-net 
Private (CT & MRI) 56 
31 

Spinal cord: 0.812; Pharyngeal constrictor muscles: 0.617; oral cavity: 0.845; Larynx-
supraglottis: 0.728; Larynx-glottis:0.615; Lips:0.728; Thyroid: 0.721; pituitary gland: 0.658; 
Lacrimal glands (left): 0.621; Lacrimal glands (right): 0.636; left eye: 0.887; right eye: 0.884; 
left lens: 0.723; right lens: 0.763; Cervical esophagus: 0.559; Cricopharyngeal inlet: 0.517; 
Cochleae (left):0.558; Cochleae (right):0.514; Carotid arteries (left):0.624; Carotid arteries 
(right):0.618; Buccal mucosa: 0.661; Arytenoids:0.474 

Kan et al. [85] 
3D Transformer and U-Net 
Private (CT) 
94 
18 

2D U-Net 
OpenKBP (CT) [36] 
188 
5 
Spinal cord: 0.750 


## TABLE XII DSC
XII-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN SINGLE-STATE SEGMENTATION METHODS FOR THE ABDOMEN-SUPPLEMENTARY MATERIALRef 
Network 
Datasets 
Quantity 
Organ type Other organs 

Gibson et al. [40] 
3D CNN 
TCIA [200], [201] &BTCV [29] (CT) 
72 
4 
Esophagus: 0.730 

Men et al. [115] 
2D CNN 
Private (CT) 
278 
5 
Bladder: 0.934; Intestine: 0.653; Left femoral head: 0.921; Right femoral head: 
0.923; Colon: 0.618 
Shen et al. [157] 
3D U-Net 
Private (CT) 
377 
7 
Artery: 0.892; Vein: 0.793 
Gibson et al. [57] 
3D V-Net 
TCIA [200], [201] &BTCV [29] (CT) 
90 
8 
Duodenum 0.630; Esophagus: 0.760 
Roth et al. [52] 
3D U-Net 
Private (CT) 
377 
7 
Artery: 0.835; Vein 0.805 

Cai et al. [65] 
2D FCN 
Private (CT) 
120 
16 

Aorta: 0.810; Adrenal gland: 0.368; Celiac AA: 0.385; Duodenum:0.649; Colon: 
0.776; Inferior vena cava: 0.786; Superior mesenteric artery: 0.496; Small 
bowel: 0.729; Veins: 0.651 
Heinrich et al. [147] 
3D U-Net 
TCIA (CT) [200], [201] 
43 
8 
Left adrenal gland: 0.942; duodenum: 0.538; Esophagus: 0.633 

Private (CT) 
10 
7 
Spleen, pancreas, kidney, gallbladder, esophagus, liver, stomach and duodenum 
average DSC: 0.823 
Hatamizadeh et al. [87] 3D Transformer And U-Net 
BTCV (CT) [29] 
30 
13 
Esophagus: 0.864; aorta: 0.948; Inferior vena cava: 0.890; vein: 0.858 

Chen et al. [117] 
2.5D U-Net 
Private (MR) 
102 
10 
Duodenum: 0.801; Small Intestine: 0.870; Spinal Cord: 0.904; Vertebral Body: 
0.900 
Tang et al. [124] 
2.5D U-Net 
ABD-110 (CT) [124] 
110 
11 
Large intestine: 0.825; small intestine 0.765; duodenum 0.707; spinal cord 0.908 
Jia and Wei [53] 
3D U-Net 
CHAOS (CT) [32] 
20 
4 
-
Lin et al. [142] 
3D U-Net 
TCIA [200], [201] & BTCV(CT) [29] 
90 
8 
Duodenum 0.637; esophagus 0.733 
Cao et al. [77] 
2D Transformer 
Synapse (CT) [2] 
30 
8 
Aorta 0.855 
Chen et al. [86] 
2D Transformer And U-Net Synapse (CT) [2] 
30 
8 
Aorta 0.872 
Song et al. [161] 
2D CNN 
Synapse (CT) [2] 
30 
8 
Aorta 0.903 
Huang et al. [78] 
2D Transformer 
Synapse (CT) [2] 
30 
8 
Aorta: 0.870 
Suo et al. [84] 
2D Transformer And U-Net Synapse (CT) [2] 
30 
8 
Aorta: 0.881 
Berzoini et al. [54] 
2D U-Net 
Open-source CT-org dataset [203] 
140 
6 
Lung: 0.967; bladder: 0.836; bone: 0.944 
Shen et al. [148] 
2D U-Net 
TCIA (CT) [200], [201] 
42 
5 
Duodenum: 0.615 

Xie et al. [79] 
3D CNN And Transformer 
BTCV (CT) [29] 
30 
11 
Esophagus: 0.780; aorta: 0.912; Inferior vena cava: 0.880; Portal vein and 
splenic vein: 0.781; 
Srivastava et al. [138] 
3D U-Net 
Synapse (CT) [2] 
30 
8 
Aorta: 0.909 

Jiang et al. [198] 
2D U-Net 
BTCV (CT) [29] 
30 
12 
Esophagus: 0.807; aorta: 0.913; Inferior vena cava: 0.850; Portal vein and 
splenic vein: 0.809; adrenal gland: 0.691 



## TABLE XIII DSC
XIII-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN COARSE-TO-FINE SEGMENTATION METHODS FOR THE HEAD AND NECK-SUPPLEMENTARY MATERIALRef 
Coarse 
Fine 
Datasets 
Quantity Organ type Other organs 

Fang et al. [70] 2D FCN 3D U-Net 
Private (CT) 56 
14 
Right eyeball: 0.634; Left eyeball: 0.636; Lips: 0.676; Oral Cavity: 0.829; throat: 0.389; 
Esophagus: 0.735; Thyroid gland: 0.642; spinal cord: 0.782 



## TABLE XIV DSC
XIV-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN COARSE-TO-FINE SEGMENTATION METHODS FOR THE ABDOMENRef 
Coarse network Fine network Dataset 
Quantity Category 
Other organs 

Roth [103] 
3D FCN 
3D FCN 
Private (CT) 
331 
3 
Artery: 0.796; vein: 0.731 

Wang [111] 
2.5D FCN 
2.5D FCN 
Private (CT) 
236 
13 
Aorta: 0.918; colon: 0.830; duodenum: 0.754; Inferior vena cava: 0.870; small intestine: 0.801; 
vein: 0.807 

Zhang [123] 
3D V-Net 
3D V-Net 
BTCV (CT) [29] 
30 
13 
Esophagus: 0.691; aorta: 0.877; Inferior vena cava: 0.865; Portal vein and splenic vein: 0.688; 
right adrenal gland: 0.651; left adrenal gland: 0.619 

Xie [94] 
2.5D FCN 
2.5D FCN 
Private (CT) 
200 
16 
Aorta: 0.937; adrenal gland: 0.630; abdominal cavity: 0.620; duodenum: 0.735; Inferior vena 
cava: 0.837; Vascular: 0.742; small intestine: 0.751; vein: 0.748; Colon: 0.800 

Lee [114] 
3D U-Net 
3D U-Net 
BTCV (CT) [29] 47 
8 
Esophagus: 0.783; aorta: 0.916; Inferior vena cava: 0.856; Portal vein and splenic vein: 0.762; 
RAD: 0.741; LAD: 0.746 
Kakeya [108] 
3D U-Net 
3D U-Net 
Private (CT) 
47 
8 
Inferior vena cava: 0.908; Aorta: 0.969 


## TABLE XV DSC
XV-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN LOCALIZATION AND SEGMENTATION METHODS FOR THE HEAD AND NECK-SUPPLEMENTARY MATERIAL Brachial plexus: 0.562; pharyngeal constrictor: 0.755; left ear: 0.773; right ear: 0.786; left eye: 0.925; right eye: 0.925; pituitary gland: 0.639; larynx: 0.893; left lens: 0.819; right lens: 0.830; oral cavity: 0.908; spinal cord: 0.856; sublingual gland: 0.460; left temporal lobe: 0.848; right temporal lobe: 0.841; thyroid: 0.856; left temporomandibular joint: 0.880; right temporomandibular joint: 0.869; trachea: 0.813 Left eye: 0.897; right eye: 0.895; left lens: 0.819; right lens: 0.825; pituitary gland: 0.722; left temporal lobe: 0.877; right temporal lobe: 0.883; spinal cord: 0.831; left inner ear: 0.864; right inner ear: 0.855; left middle ear: 0.857; right middle ear: 0.843; left temporomandibular joint: 0.764; right temporomandibular joint: 0.789;Ref 
Localization 
Segmentation 
Dataset 
Quantity Category Other organs 

Men et al. [102] 
3D U-Net 
3D U-Net 
TCIA (CT) [200], [201] 100 
7 
Spinal cord: 0.910; Left eye: 0.930; Right eye: 0.920 

Tang et al. [105] 
3D U-Net 
3D U-Net 
Private (CT) 
215 
28 

Yang et al. [212] 
3D CNN 
2D U-Net 
Private (CT) 
88 
17 

Left eye: 0.875; right eye: 0.889; left lens: 0.747; right lens: 0.698; cerebellum: 0.936; pituitary: 
0.672; thyroid: 0.844; Temporal lobe left: 0.762; Temporal lobe right: 0.784; brain: 0.976; head: 
0.943 

Liang et al. [112] 2D CNN 
2D CNN 
Private (CT) 
185 
18 

Left eye: 0.932; right eye: 0.936; left lens: 0.930; right lens: 0.842; larynx: 0.870; oral cavity: 
0.928; left mastoid: 0.821; right mastoid: 0.824; spinal cord: 0.884; left TMJ: 0.846; right TMJ: 
0.844; 

Gao et al. [107] 
3D CNN 
3D CNN 
Private (CT) 
50 
18 

Left eye: 0.876; right eye: 0.912; oral cavity: 0.792; larynx: 0.658; spinal cord: 0.874; left lens: 
0.808; right lens: 0.790; pituitary gland: 0.769; left middle ear: 0.567; right middle ear: 0.522; 
left TMJ: 0.584; right TMJ: 0.572 
Private (CT) 
96 
11 
Left eye: 0.930; right eye: 0.930; spinal cord: 0.900; left lens: 0.872; right lens: 0.883; 
Lei et al. [103] 
3D CNN 
3D U-Net 
Private (CT) 
15 
8 
Esophagus: 0.840; Throat: 0.790; Oral: 0.890; Pharynx: 0.850; spinal cord: 0.890 

Korte et al. [106] 
3D U-Net 
3D U-Net 

Public RT-MAC dataset 
(MRI) [213] 
43 
8 
Secondary lymph nodes (left): 0.708; secondary lymph nodes (right): 0.715; tertiary lymph 
nodes (left): 0.561; tertiary lymph nodes (right): 0.573; 

Private (MRI) 
10 
8 
Secondary lymph nodes (left): 0.553; Secondary lymph nodes (right): 0.525; Tertiary lymph 
nodes (left): 0.304; Tertiary lymph nodes (right): 0.189; 

Gao et al. [69] 
3D CNN 
3D CNN 
Private (CT) 
1164 
22 



## TABLE XVI DSC
XVI-BASED SUMMARY OF THE LITERATURE ON MULTI-ORGAN LOCALIZATION AND SEGMENTATION METHODS FOR THE ABDOMEN-SUPPLEMENTARY MATERIAL Esophagus: 0.588; aorta: 0.870; Inferior vena cava: 0.758; Portal vein and splenic vein: 0.715; Right adrenal gland: 0.630; Left adrenal gland: 0.631Ref 
Localization Segmentation Dataset 
Quantity 
Category 
Other organs 

Larsson et al. [108] 
Multi-Atlas 
3D FCN 
BTCV (CT) [29] 
30 
13 
Zhao et al. [109] 
Registration 
2D U-Net 

VISCERAL challenge dataset Nonenhanced 
CT (CTwb) [214] 
20 
4 
Left adrenal gland: 0.472; Right adrenal gland: 0.390 

VISCERAL challenge dataset enhanced 
CT (CTce) [214] 
20 
4 
Left adrenal gland: 0.403; Right adrenal gland: 0.434 


Computeraided diagnosis: how to move from the laboratory to the clinic. B Van Ginneken, C M Schaefer-Prokop, M Prokop, Radiology. 2613B. Van Ginneken, C. M. Schaefer-Prokop, and M. Prokop, "Computer- aided diagnosis: how to move from the laboratory to the clinic," Radiology, vol. 261, no. 3, pp. 719-732, 2011.

Reflections on the current status of commercial automated segmentation systems in clinical practice. J Sykes, J. Sykes, "Reflections on the current status of commercial automated segmentation systems in clinical practice," pp. 131-134, 2014.

Head and neck cancers, version 2.2020, nccn clinical practice guidelines in oncology. D G Pfister, S Spencer, D Adelstein, D Adkins, Y Anzai, D M Brizel, J Y Bruce, P M Busse, J J Caudell, A J Cmelak, Journal of the National Comprehensive Cancer Network. 187D. G. Pfister, S. Spencer, D. Adelstein, D. Adkins, Y. Anzai, D. M. Brizel, J. Y. Bruce, P. M. Busse, J. J. Caudell, A. J. Cmelak et al., "Head and neck cancers, version 2.2020, nccn clinical practice guidelines in oncology," Journal of the National Comprehensive Cancer Network, vol. 18, no. 7, pp. 873-898, 2020.

Advances in the use of motion management and image guidance in radiation therapy treatment for lung cancer. J K Molitoris, T Diwanji, J W Snider, Iii , S Mossahebi, S Samanta, S N Badiyan, C B Simone, P Mohindra, Journal of thoracic disease. 10Suppl 21J. K. Molitoris, T. Diwanji, J. W. Snider III, S. Mossahebi, S. Samanta, S. N. Badiyan, C. B. Simone, P. Mohindra et al., "Advances in the use of motion management and image guidance in radiation therapy treatment for lung cancer," Journal of thoracic disease, vol. 10, no. Suppl 21, pp. S2437-S2450, 2018.

Advances in proton therapy in lung cancer. M A Vyfhuis, N Onyeuku, T Diwanji, S Mossahebi, N P Amin, S N Badiyan, P Mohindra, C B Simone, Therapeutic advances in respiratory disease. 121753466618783878M. A. Vyfhuis, N. Onyeuku, T. Diwanji, S. Mossahebi, N. P. Amin, S. N. Badiyan, P. Mohindra, and C. B. Simone, "Advances in proton therapy in lung cancer," Therapeutic advances in respiratory disease, vol. 12, p. 1753466618783878, 2018.

Variability in target volume delineation on ct scans of the breast. C W Hurkmans, J H Borger, B R Pieters, N S Russell, E P Jansen, B J Mijnheer, International Journal of Radiation Oncology Biology Physics. 505C. W. Hurkmans, J. H. Borger, B. R. Pieters, N. S. Russell, E. P. Jansen, and B. J. Mijnheer, "Variability in target volume delineation on ct scans of the breast," International Journal of Radiation Oncology Biology Physics, vol. 50, no. 5, pp. 1366-1372, 2001.

Target definition in prostate, head, and neck. C Rasch, R Steenbakkers, M Van Herk, Seminars in radiation oncology. Elsevier15C. Rasch, R. Steenbakkers, and M. van Herk, "Target definition in prostate, head, and neck," in Seminars in radiation oncology, vol. 15, no. 3. Elsevier, 2005, pp. 136-145.

Definition of gross tumor volume in lung cancer: inter-observer variability. J Van De Steene, N Linthout, J De Mey, V Vinh-Hung, C Claassens, M Noppen, A Bel, G Storme, Radiotherapy and oncology. 621J. Van de Steene, N. Linthout, J. De Mey, V. Vinh-Hung, C. Claassens, M. Noppen, A. Bel, and G. Storme, "Definition of gross tumor volume in lung cancer: inter-observer variability," Radiotherapy and oncology, vol. 62, no. 1, pp. 37-49, 2002.

A system for continual quality improvement of normal tissue delineation for radiation therapy treatment planning. J Breunig, S Hernandez, J Lin, S Alsager, C Dumstorf, J Price, J Steber, R Garza, S Nagda, E Melian, International Journal of Radiation Oncology Biology Physics. 835J. Breunig, S. Hernandez, J. Lin, S. Alsager, C. Dumstorf, J. Price, J. Steber, R. Garza, S. Nagda, E. Melian et al., "A system for continual quality improvement of normal tissue delineation for radiation therapy treatment planning," International Journal of Radiation Oncology Biology Physics, vol. 83, no. 5, pp. e703-e708, 2012.

A survey of graph cuts/graph search based medical image segmentation. X Chen, L Pan, IEEE reviews in biomedical engineering. 11X. Chen and L. Pan, "A survey of graph cuts/graph search based medical image segmentation," IEEE reviews in biomedical engineering, vol. 11, pp. 112-124, 2018.

Concurrent multimodality image segmentation by active contours for radiotherapy treatment planning a. I El Naqa, D Yang, A Apte, D Khullar, S Mutic, J Zheng, J D Bradley, P Grigsby, J O Deasy, Medical physics. 3412I. El Naqa, D. Yang, A. Apte, D. Khullar, S. Mutic, J. Zheng, J. D. Bradley, P. Grigsby, and J. O. Deasy, "Concurrent multimodality image segmentation by active contours for radiotherapy treatment planning a," Medical physics, vol. 34, no. 12, pp. 4738-4749, 2007.

Robust edge-stop functions for edge-based active contour models in medical image segmentation. A Pratondo, C.-K Chui, S.-H Ong, IEEE Signal Processing Letters. 232A. Pratondo, C.-K. Chui, and S.-H. Ong, "Robust edge-stop functions for edge-based active contour models in medical image segmentation," IEEE Signal Processing Letters, vol. 23, no. 2, pp. 222-226, 2015.

A shape-based approach to the segmentation of medical imagery using level sets. A Tsai, A Yezzi, W Wells, C Tempany, D Tucker, A Fan, W E Grimson, A Willsky, IEEE transactions on medical imaging. 222A. Tsai, A. Yezzi, W. Wells, C. Tempany, D. Tucker, A. Fan, W. E. Grimson, and A. Willsky, "A shape-based approach to the segmentation of medical imagery using level sets," IEEE transactions on medical imaging, vol. 22, no. 2, pp. 137-154, 2003.

Threshold based segmentation method for hyperspectral images. A M Saranathan, M Parente, 2013 5Th workshop on hyperspectral image and signal processing: evolution in remote sensing (WHISPERS). GainesvilleIEEEA. M. Saranathan and M. Parente, "Threshold based segmentation method for hyperspectral images," in 2013 5Th workshop on hyper- spectral image and signal processing: evolution in remote sensing (WHISPERS). Gainesville: IEEE, 2013, pp. 1-4.

Normalized cuts and image segmentation. J Shi, J Malik, IEEE Transactions. 228J. Shi and J. Malik, "Normalized cuts and image segmentation," IEEE Transactions on pattern analysis and machine intelligence, vol. 22, no. 8, pp. 888-905, 2000.

Segmentation using region growing algorithm based on clahe for medical images. A J Vyavahare, R Thool, Fourth International Conference on Advances in Recent Technologies in Communication and Computing (ARTCom2012). Bangalore, IndiaIETA. J. Vyavahare and R. Thool, "Segmentation using region growing algorithm based on clahe for medical images," in Fourth International Conference on Advances in Recent Technologies in Communication and Computing (ARTCom2012). Bangalore, India: IET, 2012, pp. 182-185.

Multi-atlas-based segmentation with local decision fusion-application to cardiac and aortic segmentation in ct scans. I Isgum, M Staring, A Rutten, M Prokop, M A Viergever, B Van Ginneken, IEEE transactions on medical imaging. 287I. Isgum, M. Staring, A. Rutten, M. Prokop, M. A. Viergever, and B. Van Ginneken, "Multi-atlas-based segmentation with local decision fusion-application to cardiac and aortic segmentation in ct scans," IEEE transactions on medical imaging, vol. 28, no. 7, pp. 1000-1010, 2009.

Multi-atlas based segmentation of brain images: atlas selection and its effect on accuracy. P Aljabar, R A Heckemann, A Hammers, J V Hajnal, D Rueckert, Neuroimage. 463P. Aljabar, R. A. Heckemann, A. Hammers, J. V. Hajnal, and D. Rueck- ert, "Multi-atlas based segmentation of brain images: atlas selection and its effect on accuracy," Neuroimage, vol. 46, no. 3, pp. 726-738, 2009.

Automatic model-based segmentation of the heart in ct images. O Ecabert, J Peters, H Schramm, C Lorenz, J Berg, M J Walker, M Vembar, M E Olszewski, K Subramanyan, G Lavi, IEEE transactions on medical imaging. 279O. Ecabert, J. Peters, H. Schramm, C. Lorenz, J. von Berg, M. J. Walker, M. Vembar, M. E. Olszewski, K. Subramanyan, G. Lavi et al., "Automatic model-based segmentation of the heart in ct images," IEEE transactions on medical imaging, vol. 27, no. 9, pp. 1189-1201, 2008.

Auto-segmentation of normal and target structures in head and neck ct images: a feature-driven model-based approach. A A Qazi, V Pekar, J Kim, J Xie, S L Breen, D A Jaffray, Medical physics. 3811A. A. Qazi, V. Pekar, J. Kim, J. Xie, S. L. Breen, and D. A. Jaffray, "Auto-segmentation of normal and target structures in head and neck ct images: a feature-driven model-based approach," Medical physics, vol. 38, no. 11, pp. 6160-6170, 2011.

Comparison of regularization methods for imagenet classification with deep convolutional neural networks. E A Smirnov, D M Timoshenko, S N Andrianov, Aasri Procedia. 6E. A. Smirnov, D. M. Timoshenko, and S. N. Andrianov, "Comparison of regularization methods for imagenet classification with deep convo- lutional neural networks," Aasri Procedia, vol. 6, pp. 89-94, 2014.

Fast capsnet for lung cancer screening. A Mobiny, H Van Nguyen, 21st International Conference. Granada, Spain; ChamSpringerProceedings, Part II 11A. Mobiny and H. Van Nguyen, "Fast capsnet for lung cancer screening," in Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II 11. Cham: Springer, 2018, pp. 741-749.

Recurrent residual u-net for medical image segmentation. M Z Alom, C Yakopcic, M Hasan, T M Taha, V K Asari, Journal of Medical Imaging. 6114006M. Z. Alom, C. Yakopcic, M. Hasan, T. M. Taha, and V. K. Asari, "Recurrent residual u-net for medical image segmentation," Journal of Medical Imaging, vol. 6, no. 1, p. 014006, 2019.

Medical image segmentation using deep learning: A survey. R Wang, T Lei, R Cui, B Zhang, H Meng, A K Nandi, IET Image Processing. 165R. Wang, T. Lei, R. Cui, B. Zhang, H. Meng, and A. K. Nandi, "Medical image segmentation using deep learning: A survey," IET Image Processing, vol. 16, no. 5, pp. 1243-1267, 2022.

A review of multimodal medical image fusion techniques. B Huang, F Yang, M Yin, X Mo, C Zhong, Computational and mathematical methods in medicine. 20208279342B. Huang, F. Yang, M. Yin, X. Mo, and C. Zhong, "A review of multimodal medical image fusion techniques," Computational and mathematical methods in medicine, vol. 2020, p. 8279342, 2020.

Deep learning in medical image registration: a review. Y Fu, Y Lei, T Wang, W J Curran, T Liu, X Yang, Physics in Medicine & Biology. 6520Y. Fu, Y. Lei, T. Wang, W. J. Curran, T. Liu, and X. Yang, "Deep learning in medical image registration: a review," Physics in Medicine & Biology, vol. 65, no. 20, p. 20TR01, 2020.

Deep learning in multi-organ segmentation. Y Lei, Y Fu, T Wang, R L Qiu, W J Curran, T Liu, X Yang, arXiv:2001.10619arXiv preprintY. Lei, Y. Fu, T. Wang, R. L. Qiu, W. J. Curran, T. Liu, and X. Yang, "Deep learning in multi-organ segmentation," arXiv preprint arXiv:2001.10619, 2020.

A review of deep learning based methods for medical image multi-organ segmentation. Y Fu, Y Lei, T Wang, W J Curran, T Liu, X Yang, Physica Medica. 85Y. Fu, Y. Lei, T. Wang, W. J. Curran, T. Liu, and X. Yang, "A review of deep learning based methods for medical image multi-organ segmentation," Physica Medica, vol. 85, pp. 107-122, 2021.

Segmentation outside the cranial vault challenge. B Landman, Z Xu, J E Igelsias, M Styner, T Langerak, A Klein, Synapse. B. Landman, Z. Xu, J. E. Igelsias, M. Styner, T. Langerak, and A. Klein, "Segmentation outside the cranial vault challenge," Synapse, 2015.

Evaluation of segmentation methods on head and neck ct: auto-segmentation challenge. P F Raudaschl, P Zaffino, G C Sharp, M F Spadea, A Chen, B M Dawant, T Albrecht, T Gass, C Langguth, M Lüthi, Medical physics. 445P. F. Raudaschl, P. Zaffino, G. C. Sharp, M. F. Spadea, A. Chen, B. M. Dawant, T. Albrecht, T. Gass, C. Langguth, M. Lüthi et al., "Evaluation of segmentation methods on head and neck ct: auto-segmentation challenge 2015," Medical physics, vol. 44, no. 5, pp. 2020-2036, 2017.

Autosegmentation for thoracic radiation treatment planning: a grand challenge at aapm 2017. J Yang, H Veeraraghavan, S G Armato, Iii , K Farahani, J S Kirby, J Kalpathy-Kramer, W Van Elmpt, A Dekker, X Han, X Feng, Medical physics. 4510J. Yang, H. Veeraraghavan, S. G. Armato III, K. Farahani, J. S. Kirby, J. Kalpathy-Kramer, W. van Elmpt, A. Dekker, X. Han, X. Feng et al., "Autosegmentation for thoracic radiation treatment planning: a grand challenge at aapm 2017," Medical physics, vol. 45, no. 10, pp. 4568- 4581, 2018.

Chaos challengecombined (ct-mr) healthy abdominal organ segmentation. A E Kavur, N S Gezer, M Barış, S Aslan, P.-H Conze, V Groza, D D Pham, S Chatterjee, P Ernst, S Özkan, Medical Image Analysis. 69101950A. E. Kavur, N. S. Gezer, M. Barış, S. Aslan, P.-H. Conze, V. Groza, D. D. Pham, S. Chatterjee, P. Ernst, S.Özkan et al., "Chaos challenge- combined (ct-mr) healthy abdominal organ segmentation," Medical Image Analysis, vol. 69, p. 101950, 2021.

Deep convolutional neural network for segmentation of thoracic organs-atrisk using cropped 3d images. X Feng, K Qing, N J Tustison, C H Meyer, Q Chen, Medical physics. 465X. Feng, K. Qing, N. J. Tustison, C. H. Meyer, and Q. Chen, "Deep convolutional neural network for segmentation of thoracic organs-at- risk using cropped 3d images," Medical physics, vol. 46, no. 5, pp. 2169-2180, 2019.

Annotations for body organ localization based on miccai lits dataset. X Xu, F Zhou, B Liu, X Bai, IEEE Dataport. X. Xu, F. Zhou, B. Liu, and X. Bai, "Annotations for body organ localization based on miccai lits dataset," IEEE Dataport, 2018.

Openkbp: the open-access knowledgebased planning grand challenge and dataset. A Babier, B Zhang, R Mahmood, K L Moore, T G Purdie, A L Mcniven, T C Chan, Medical Physics. 489A. Babier, B. Zhang, R. Mahmood, K. L. Moore, T. G. Purdie, A. L. McNiven, and T. C. Chan, "Openkbp: the open-access knowledge- based planning grand challenge and dataset," Medical Physics, vol. 48, no. 9, pp. 5549-5561, 2021.

Abdomenct-1k: Is abdominal organ segmentation a solved problem?. J Ma, Y Zhang, S Gu, C Zhu, C Ge, Y Zhang, X An, C Wang, Q Wang, X Liu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4410J. Ma, Y. Zhang, S. Gu, C. Zhu, C. Ge, Y. Zhang, X. An, C. Wang, Q. Wang, X. Liu et al., "Abdomenct-1k: Is abdominal organ segmen- tation a solved problem?" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 10, pp. 6695-6714, 2021.

Backpropagation applied to handwritten zip code recognition. Y Lecun, B Boser, J S Denker, D Henderson, R E Howard, W Hubbard, L D , Neural computation. 14Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, "Backpropagation applied to handwritten zip code recognition," Neural computation, vol. 1, no. 4, pp. 541-551, 1989.

Neuroimaging and deep learning for brain stroke detection-a review of recent advancements and future prospects. R Karthik, R Menaka, A Johnson, S Anand, Computer Methods and Programs in Biomedicine. 197105728R. Karthik, R. Menaka, A. Johnson, and S. Anand, "Neuroimaging and deep learning for brain stroke detection-a review of recent ad- vancements and future prospects," Computer Methods and Programs in Biomedicine, vol. 197, p. 105728, 2020.

Deep learning shows good reliability for automatic segmentation and volume measurement of brain hemorrhage, intraventricular extension, and peripheral edema. X Zhao, K Chen, G Wu, G Zhang, X Zhou, C Lv, S Wu, Y Chen, G Xie, Z Yao, European radiology. 317X. Zhao, K. Chen, G. Wu, G. Zhang, X. Zhou, C. Lv, S. Wu, Y. Chen, G. Xie, and Z. Yao, "Deep learning shows good reliability for automatic segmentation and volume measurement of brain hemorrhage, intraventricular extension, and peripheral edema," European radiology, vol. 31, no. 7, pp. 5012-5020, 2021.

Towards image-guided pancreas and biliary endoscopy: automatic multiorgan segmentation on abdominal ct with dense dilated networks. E Gibson, F Giganti, Y Hu, E Bonmati, S Bandula, K Gurusamy, B R Davidson, S P Pereira, M J Clarkson, D C Barratt, Medical Image Computing and Computer Assisted Intervention-MIC-CAI 2017: 20th International Conference. Quebec City, QC, Canada; Cham, SwitzerlandSpringerProceedings, Part I 20E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B. R. Davidson, S. P. Pereira, M. J. Clarkson, and D. C. Barratt, "To- wards image-guided pancreas and biliary endoscopy: automatic multi- organ segmentation on abdominal ct with dense dilated networks," in Medical Image Computing and Computer Assisted Intervention-MIC- CAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20. Cham, Switzerland: Springer, 2017, pp. 728-736.

Segmentation of organs-at-risks in head and neck ct images using convolutional neural networks. B Ibragimov, L Xing, Medical physics. 442B. Ibragimov and L. Xing, "Segmentation of organs-at-risks in head and neck ct images using convolutional neural networks," Medical physics, vol. 44, no. 2, pp. 547-557, 2017.

Deep neural networks for fast segmentation of 3d medical images. K Fritscher, P Raudaschl, P Zaffino, M F Spadea, G C Sharp, R Schubert, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2016: 19th International Conference. Athens, Greece; Cham, SwitzerlandSpringerProceedings, Part II 19K. Fritscher, P. Raudaschl, P. Zaffino, M. F. Spadea, G. C. Sharp, and R. Schubert, "Deep neural networks for fast segmentation of 3d medical images," in Medical Image Computing and Computer- Assisted Intervention-MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19. Cham, Switzerland: Springer, 2016, pp. 158-165.

Deep learning for multitask medical image segmentation in multiple modalities. P Moeskops, J M Wolterink, B H Van Der Velden, K G Gilhuijs, T Leiner, M A Viergever, I Išgum, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2016: 19th International Conference. Athens, Greece; Cham, SwitzerlandSpringerProceedings, Part II 19P. Moeskops, J. M. Wolterink, B. H. Van Der Velden, K. G. Gilhuijs, T. Leiner, M. A. Viergever, and I. Išgum, "Deep learning for multi- task medical image segmentation in multiple modalities," in Medical Image Computing and Computer-Assisted Intervention-MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19. Cham, Switzerland: Springer, 2016, pp. 478- 486.

Fully convolutional networks for semantic segmentation. Jonathan Long, Shelhamer, Darrell Evan, Trevor , IEEE Transactions on Pattern Analysis & Machine Intelligence. 394Long, Jonathan, Shelhamer, Evan, Darrell, and Trevor, "Fully convo- lutional networks for semantic segmentation," IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 39, no. 4, pp. 640-651, 2017.

Training multi-organ segmentation networks with sample selection by relaxed upper confident bound. Y Wang, Y Zhou, P Tang, W Shen, E K Fishman, A L Yuille, Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference. Granada, Spain; Cham, SwitzerlandSpringerProceedings, Part IV 11Y. Wang, Y. Zhou, P. Tang, W. Shen, E. K. Fishman, and A. L. Yuille, "Training multi-organ segmentation networks with sample selection by relaxed upper confident bound," in Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part IV 11. Cham, Switzerland: Springer, 2018, pp. 434-442.

Learning to refine object segments. P O Pinheiro, T.-Y Lin, R Collobert, P Dollár, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The Netherlands; Cham, SwitzerlandSpringerProceedings, Part I 14P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollár, "Learning to refine object segments," in Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I 14. Cham, Switzerland: Springer, 2016, pp. 75-91.

U-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference. Munich, Germany; Cham, SwitzerlandSpringerProceedings, Part III 18O. Ronneberger, P. Fischer, and T. Brox, "U-net: Convolutional net- works for biomedical image segmentation," in Medical Image Comput- ing and Computer-Assisted Intervention-MICCAI 2015: 18th Interna- tional Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Cham, Switzerland: Springer, 2015, pp. 234-241.

Anatomynet: deep learning for fast and fully automated whole-volume segmentation of head and neck anatomy. W Zhu, Y Huang, L Zeng, X Chen, Y Liu, Z Qian, N Du, W Fan, X Xie, Medical physics. 462W. Zhu, Y. Huang, L. Zeng, X. Chen, Y. Liu, Z. Qian, N. Du, W. Fan, and X. Xie, "Anatomynet: deep learning for fast and fully automated whole-volume segmentation of head and neck anatomy," Medical physics, vol. 46, no. 2, pp. 576-589, 2019.

Deep learning-based delineation of head and neck organs at risk: geometric and dosimetric evaluation. W Van Rooij, M Dahele, H R Brandao, A R Delaney, B J Slotman, W F Verbakel, International Journal of Radiation Oncology Biology Physics. 1043W. van Rooij, M. Dahele, H. R. Brandao, A. R. Delaney, B. J. Slotman, and W. F. Verbakel, "Deep learning-based delineation of head and neck organs at risk: geometric and dosimetric evaluation," International Journal of Radiation Oncology Biology Physics, vol. 104, no. 3, pp. 677-684, 2019.

Selfchannel-and-spatial-attention neural network for automated multi-organ segmentation on head and neck ct images. S Gou, N Tong, S Qi, S Yang, R Chin, K Sheng, Physics in Medicine & Biology. 6524245034S. Gou, N. Tong, S. Qi, S. Yang, R. Chin, and K. Sheng, "Self- channel-and-spatial-attention neural network for automated multi-organ segmentation on head and neck ct images," Physics in Medicine & Biology, vol. 65, no. 24, p. 245034, 2020.

Weaving attention u-net: A novel hybrid cnn and attention-based method for organs-at-risk segmentation in head and neck ct images. Z Zhang, T Zhao, H Gay, W Zhang, B Sun, Medical physics. 4811Z. Zhang, T. Zhao, H. Gay, W. Zhang, and B. Sun, "Weaving attention u-net: A novel hybrid cnn and attention-based method for organs-at-risk segmentation in head and neck ct images," Medical physics, vol. 48, no. 11, pp. 7052-7062, 2021.

Deep learning and its application to medical image segmentation. H R Roth, C Shen, H Oda, M Oda, Y Hayashi, K Misawa, K Mori, Medical Imaging Technology. 362H. R. Roth, C. Shen, H. Oda, M. Oda, Y. Hayashi, K. Misawa, and K. Mori, "Deep learning and its application to medical image segmentation," Medical Imaging Technology, vol. 36, no. 2, pp. 63-71, 2018.

Amo-net: abdominal multi-organ segmentation in mri with a extend unet. C Jia, J Wei, 2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC). Chongqing, ChinaIEEE4C. Jia and J. Wei, "Amo-net: abdominal multi-organ segmentation in mri with a extend unet," in 2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Con- ference (IMCEC), vol. 4. Chongqing, China: IEEE, 2021, pp. 1770- 1775.

An optimized u-net for unbalanced multi-organ segmentation. R Berzoini, A A Colombo, S Bardini, A Conelli, E D&apos;arnese, M D Santambrogio, 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC). Glasgow, ScotlandIEEER. Berzoini, A. A. Colombo, S. Bardini, A. Conelli, E. D'Arnese, and M. D. Santambrogio, "An optimized u-net for unbalanced multi-organ segmentation," in 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC). Glasgow, Scotland: IEEE, 2022, pp. 3764-3767.

Segthor: Segmentation of thoracic organs at risk in ct images. Z Lambert, C Petitjean, B Dubray, S Kuan, 2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA). Paris, FranceIEEEZ. Lambert, C. Petitjean, B. Dubray, and S. Kuan, "Segthor: Seg- mentation of thoracic organs at risk in ct images," in 2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA). Paris, France: IEEE, 2020, pp. 1-6.

V-net: Fully convolutional neural networks for volumetric medical image segmentation. F Milletari, N Navab, S.-A Ahmadi, 2016 fourth international conference on 3D vision (3DV). Stanford, CAIEEEF. Milletari, N. Navab, and S.-A. Ahmadi, "V-net: Fully convolutional neural networks for volumetric medical image segmentation," in 2016 fourth international conference on 3D vision (3DV). Stanford, CA: IEEE, 2016, pp. 565-571.

Automatic multi-organ segmentation on abdominal ct with dense vnetworks. E Gibson, F Giganti, Y Hu, E Bonmati, S Bandula, K Gurusamy, B Davidson, S P Pereira, M J Clarkson, D C Barratt, IEEE transactions on medical imaging. 378E. Gibson, F. Giganti, Y. Hu, E. Bonmati, S. Bandula, K. Gurusamy, B. Davidson, S. P. Pereira, M. J. Clarkson, and D. C. Barratt, "Automatic multi-organ segmentation on abdominal ct with dense v- networks," IEEE transactions on medical imaging, vol. 37, no. 8, pp. 1822-1834, 2018.

A new probabilistic v-net model with hierarchical spatial feature transform for efficient abdominal multi-organ segmentation. M Xu, H Guo, J Zhang, K Yan, L Lu, arXiv:2208.01382arXiv preprintM. Xu, H. Guo, J. Zhang, K. Yan, and L. Lu, "A new probabilistic v-net model with hierarchical spatial feature transform for efficient ab- dominal multi-organ segmentation," arXiv preprint arXiv:2208.01382, 2022.

Segmentation of organs-at-risk from ct and mr images of the head and neck: Baseline results. G Podobnik, B Ibragimov, P Strojan, P Peterlin, T Vrtovec, 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI). Kolkata, IndiaIEEEG. Podobnik, B. Ibragimov, P. Strojan, P. Peterlin, and T. Vrtovec, "Segmentation of organs-at-risk from ct and mr images of the head and neck: Baseline results," in 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI). Kolkata, India: IEEE, 2022, pp. 1-4.

nnu-net for brain tumor segmentation. F Isensee, P F Jäger, P M Full, P Vollmuth, K H Maier-Hein, Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 6th International Workshop. Lima, Peru; Part II 6. Cham, SwitzerlandSpringer2020Conjunction with MICCAI 2020. Revised Selected PapersF. Isensee, P. F. Jäger, P. M. Full, P. Vollmuth, and K. H. Maier- Hein, "nnu-net for brain tumor segmentation," in Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 6th Interna- tional Workshop, BrainLes 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Revised Selected Papers, Part II 6. Cham, Switzerland: Springer, 2021, pp. 118-132.

Multiorgan segmentation from partially labeled datasets with conditional nnu-net. G Zhang, Z Yang, B Huo, S Chai, S Jiang, Computers in Biology and Medicine. 136104658G. Zhang, Z. Yang, B. Huo, S. Chai, and S. Jiang, "Multiorgan segmentation from partially labeled datasets with conditional nnu-net," Computers in Biology and Medicine, vol. 136, p. 104658, 2021.

A fusion biopsy framework for prostate cancer based on deformable superellipses and nnu-net. N Altini, A Brunetti, V P Napoletano, F Girardi, E Allegretti, S M Hussain, G Brunetti, V Triggiani, V Bevilacqua, D Buongiorno, Bioengineering. 98343N. Altini, A. Brunetti, V. P. Napoletano, F. Girardi, E. Allegretti, S. M. Hussain, G. Brunetti, V. Triggiani, V. Bevilacqua, and D. Buongiorno, "A fusion biopsy framework for prostate cancer based on deformable superellipses and nnu-net," Bioengineering, vol. 9, no. 8, p. 343, 2022.

I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, arXiv:1406.2661Generative adversarial networks. arXiv preprintI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial net- works," arXiv preprint arXiv:1406.2661, 2014.

Shape constrained fully convolutional densenet with adversarial training for multiorgan segmentation on head and neck ct and low-field mr images. N Tong, S Gou, S Yang, M Cao, K Sheng, Medical physics. 466N. Tong, S. Gou, S. Yang, M. Cao, and K. Sheng, "Shape constrained fully convolutional densenet with adversarial training for multiorgan segmentation on head and neck ct and low-field mr images," Medical physics, vol. 46, no. 6, pp. 2669-2682, 2019.

End-to-end adversarial shape learning for abdomen organ deep segmentation. J Cai, Y Xia, D Yang, D Xu, L Yang, H Roth, Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019. Shenzhen, China; Cham, SwitzerlandSpringerProceedings 10J. Cai, Y. Xia, D. Yang, D. Xu, L. Yang, and H. Roth, "End-to-end adversarial shape learning for abdomen organ deep segmentation," in Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings 10. Cham, Switzerland: Springer, 2019, pp. 124-132.

Automatic multiorgan segmentation in thorax ct images using u-net-gan. X Dong, Y Lei, T Wang, M Thomas, L Tang, W J Curran, T Liu, X Yang, Medical physics. 465X. Dong, Y. Lei, T. Wang, M. Thomas, L. Tang, W. J. Curran, T. Liu, and X. Yang, "Automatic multiorgan segmentation in thorax ct images using u-net-gan," Medical physics, vol. 46, no. 5, pp. 2157-2168, 2019.

Multiorgan segmentation using distance-aware adversarial networks. R Trullo, C Petitjean, B Dubray, S Ruan, Journal of Medical Imaging. 6114001R. Trullo, C. Petitjean, B. Dubray, and S. Ruan, "Multiorgan segmen- tation using distance-aware adversarial networks," Journal of Medical Imaging, vol. 6, no. 1, p. 014001, 2019.

Deep adversarial training for multi-organ nuclei segmentation in histopathology images. F Mahmood, D Borders, R J Chen, G N Mckay, K J Salimian, A Baras, N J Durr, IEEE transactions on medical imaging. 3911F. Mahmood, D. Borders, R. J. Chen, G. N. McKay, K. J. Salimian, A. Baras, and N. J. Durr, "Deep adversarial training for multi-organ nuclei segmentation in histopathology images," IEEE transactions on medical imaging, vol. 39, no. 11, pp. 3257-3267, 2019.

Focusnetv2: Imbalanced large and small organ segmentation with adversarial shape constraint for head and neck ct images. Y Gao, R Huang, Y Yang, J Zhang, K Shao, C Tao, Y Chen, D N Metaxas, H Li, M Chen, Medical Image Analysis. 67101831Y. Gao, R. Huang, Y. Yang, J. Zhang, K. Shao, C. Tao, Y. Chen, D. N. Metaxas, H. Li, and M. Chen, "Focusnetv2: Imbalanced large and small organ segmentation with adversarial shape constraint for head and neck ct images," Medical Image Analysis, vol. 67, p. 101831, 2021.

Multi-organ segmentation network with adversarial performance validator. H Fang, Y Fang, X Yang, arXiv:2204.07850arXiv preprintH. Fang, Y. Fang, and X. Yang, "Multi-organ segmentation network with adversarial performance validator," arXiv preprint arXiv:2204.07850, 2022.

Attention is all uou need. A Vaswani, N Shazeer, N Parmar, arXiv:170603762A. Vaswani, N. Shazeer, and N. Parmar, "Attention is all uou need," arXiv:170603762, 2021.

Lambdanetworks: Modeling long-range interactions without attention. I Bello, arXiv:2102.08602arXiv preprintI. Bello, "Lambdanetworks: Modeling long-range interactions without attention," arXiv preprint arXiv:2102.08602, 2021.

Utnet: a hybrid transformer architecture for medical image segmentation. Y Gao, M Zhou, D N Metaxas, Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference. Strasbourg, France; Cham, SwitzerlandSpringerProceedings, Part III 24Y. Gao, M. Zhou, and D. N. Metaxas, "Utnet: a hybrid transformer architecture for medical image segmentation," in Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part III 24. Cham, Switzerland: Springer, 2021, pp. 61-71.

Transclaw u-net: Claw unet with transformers for medical image segmentation. C Yao, M Hu, G Zhai, X Zhang, arXiv:2107.05188arXiv preprintC. Yao, M. Hu, G. Zhai, and X. Zhang, "Transclaw u-net: Claw u- net with transformers for medical image segmentation," arXiv preprint arXiv:2107.05188, 2021.

Medical transformer: Gated axial-attention for medical image segmentation. J M J Valanarasu, P Oza, I Hacihaliloglu, V M Patel, Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference. Strasbourg, FranceSpringerProceedings, Part I 24J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V. M. Patel, "Medical transformer: Gated axial-attention for medical image segmentation," in Medical Image Computing and Computer Assisted Intervention- MICCAI 2021: 24th International Conference, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part I 24. Springer, 2021, pp. 36-46.

Male pelvic multi-organ segmentation using token-based transformer vnet. S Pan, Y Lei, T Wang, J Wynne, C.-W Chang, J Roper, A B Jani, P Patel, J D Bradley, T Liu, Physics in Medicine & Biology. 6720205012S. Pan, Y. Lei, T. Wang, J. Wynne, C.-W. Chang, J. Roper, A. B. Jani, P. Patel, J. D. Bradley, T. Liu et al., "Male pelvic multi-organ segmentation using token-based transformer vnet," Physics in Medicine & Biology, vol. 67, no. 20, p. 205012, 2022.

Swin-unet: Unet-like pure transformer for medical image segmentation. H Cao, Y Wang, J Chen, D Jiang, X Zhang, Q Tian, M Wang, arXiv:2105.05537arXiv preprintH. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang, "Swin-unet: Unet-like pure transformer for medical image segmenta- tion," arXiv preprint arXiv:2105.05537, 2021.

Missformer: An effective medical image segmentation transformer. X Huang, Z Deng, D Li, X Yuan, arXiv:2109.07162arXiv preprintX. Huang, Z. Deng, D. Li, and X. Yuan, "Missformer: An ef- fective medical image segmentation transformer," arXiv preprint arXiv:2109.07162, 2021.

Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation. Y Xie, J Zhang, C Shen, Y Xia, Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference. Strasbourg, France; Cham, SwitzerlandSpringerProceedings, Part III 24Y. Xie, J. Zhang, C. Shen, and Y. Xia, "Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation," in Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part III 24. Cham, Switzerland: Springer, 2021, pp. 171-180.

Uctransnet: rethinking the skip connections in u-net from a channel-wise perspective with transformer. H Wang, P Cao, J Wang, O R Zaiane, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence36H. Wang, P. Cao, J. Wang, and O. R. Zaiane, "Uctransnet: rethinking the skip connections in u-net from a channel-wise perspective with transformer," in Proceedings of the AAAI conference on artificial intelligence, vol. 36, no. 3, 2022, pp. 2441-2449.

Mixed transformer u-net for medical image segmentation. H Wang, S Xie, L Lin, Y Iwamoto, X.-H Han, Y.-W Chen, R Tong, ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). SingaporeIEEEH. Wang, S. Xie, L. Lin, Y. Iwamoto, X.-H. Han, Y.-W. Chen, and R. Tong, "Mixed transformer u-net for medical image segmentation," in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Singapore: IEEE, 2022, pp. 2390-2394.

Levit-unet: Make faster encoders with transformer for medical image segmentation. G Xu, X Wu, X Zhang, X He, arXiv:2107.08623arXiv preprintG. Xu, X. Wu, X. Zhang, and X. He, "Levit-unet: Make faster encoders with transformer for medical image segmentation," arXiv preprint arXiv:2107.08623, 2021.

Transfuse: Fusing transformers and cnns for medical image segmentation. Y Zhang, H Liu, Q Hu, Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference. Strasbourg, France; Cham, SwitzerlandSpringerProceedings, Part I 24Y. Zhang, H. Liu, and Q. Hu, "Transfuse: Fusing transformers and cnns for medical image segmentation," in Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September 27-October 1, 2021, Pro- ceedings, Part I 24. Cham, Switzerland: Springer, 2021, pp. 14-24.

I2-net: Intra-and inter-scale collaborative learning network for abdominal multi-organ segmentation. C Suo, X Li, D Tan, Y Zhang, X Gao, Proceedings of the 2022 International Conference on Multimedia Retrieval. the 2022 International Conference on Multimedia RetrievalNew York, NY, 2022C. Suo, X. Li, D. Tan, Y. Zhang, and X. Gao, "I2-net: Intra-and inter-scale collaborative learning network for abdominal multi-organ segmentation," in Proceedings of the 2022 International Conference on Multimedia Retrieval, New York, NY, 2022, pp. 654-660.

Itunet: Integration of transformers and unet for organs-atrisk segmentation. H Kan, J Shi, M Zhao, Z Wang, W Han, H An, Z Wang, S Wang, 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC). IEEEH. Kan, J. Shi, M. Zhao, Z. Wang, W. Han, H. An, Z. Wang, and S. Wang, "Itunet: Integration of transformers and unet for organs-at- risk segmentation," in 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC). IEEE, 2022, pp. 2123-2127.

Transunet: Transformers make strong encoders for medical image segmentation. J Chen, Y Lu, Q Yu, X Luo, E Adeli, Y Wang, L Lu, A L Yuille, Y Zhou, arXiv:2102.04306arXiv preprintJ. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and Y. Zhou, "Transunet: Transformers make strong encoders for medical image segmentation," arXiv preprint arXiv:2102.04306, 2021.

Unetr: Transformers for 3d medical image segmentation. A Hatamizadeh, Y Tang, V Nath, D Yang, A Myronenko, B Landman, H R Roth, D Xu, Proceedings of the IEEE/CVF winter conference on applications of computer vision. the IEEE/CVF winter conference on applications of computer visionWaikoloa, HI, 2022A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Land- man, H. R. Roth, and D. Xu, "Unetr: Transformers for 3d medical im- age segmentation," in Proceedings of the IEEE/CVF winter conference on applications of computer vision, Waikoloa, HI, 2022, pp. 574-584.

Attention-lstm fused u-net architecture for organ segmentation in ct images. P.-H Chen, C.-H Huang, S.-K Hung, L.-C Chen, H.-L Hsieh, W.-Y Chiou, M.-S Lee, H.-Y. Lin, W.-M Liu, 2020 International Symposium on Computer, Consumer and Control (IS3C). Taichung City. TaiwanIEEEP.-H. Chen, C.-H. Huang, S.-K. Hung, L.-C. Chen, H.-L. Hsieh, W.-Y. Chiou, M.-S. Lee, H.-Y. Lin, and W.-M. Liu, "Attention-lstm fused u-net architecture for organ segmentation in ct images," in 2020 International Symposium on Computer, Consumer and Control (IS3C). Taichung City, Taiwan: IEEE, 2020, pp. 304-307.

Race-net: a recurrent neural network for biomedical image segmentation. A Chakravarty, J Sivaswamy, IEEE journal of biomedical and health informatics. 233A. Chakravarty and J. Sivaswamy, "Race-net: a recurrent neural net- work for biomedical image segmentation," IEEE journal of biomedical and health informatics, vol. 23, no. 3, pp. 1151-1162, 2018.

Multi-organ segmentation of the head and neck area: an efficient hierarchical neural networks approach. E Tappeiner, S Pröll, M Hönig, P F Raudaschl, P Zaffino, M F Spadea, G C Sharp, R Schubert, K Fritscher, International journal of computer assisted radiology and surgery. 145E. Tappeiner, S. Pröll, M. Hönig, P. F. Raudaschl, P. Zaffino, M. F. Spadea, G. C. Sharp, R. Schubert, and K. Fritscher, "Multi-organ segmentation of the head and neck area: an efficient hierarchical neural networks approach," International journal of computer assisted radiology and surgery, vol. 14, no. 5, pp. 745-754, 2019.

A coarse to fine framework for multi-organ segmentation in head and neck images. Y Pu, S.-I Kamata, Y Wang, 2020 Joint 9th International Conference on Informatics, Electronics & Vision (ICIEV) and 2020 4th International Conference on Imaging, Vision & Pattern Recognition (icIVPR). Kitakyushu, JapanIEEEY. Pu, S.-I. Kamata, and Y. Wang, "A coarse to fine framework for multi-organ segmentation in head and neck images," in 2020 Joint 9th International Conference on Informatics, Electronics & Vision (ICIEV) and 2020 4th International Conference on Imaging, Vision & Pattern Recognition (icIVPR). Kitakyushu, Japan: IEEE, 2020, pp. 1-6.

Coarse-to-fine segmentation of organs at risk in nasopharyngeal carcinoma radiotherapy. Q Ma, C Zu, X Wu, J Zhou, Y Wang, Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference. Strasbourg, France; Cham, SwitzerlandSpringerProceedings, Part I 24Q. Ma, C. Zu, X. Wu, J. Zhou, and Y. Wang, "Coarse-to-fine segmen- tation of organs at risk in nasopharyngeal carcinoma radiotherapy," in Medical Image Computing and Computer Assisted Intervention- MICCAI 2021: 24th International Conference, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part I 24. Cham, Switzerland: Springer, 2021, pp. 358-368.

Automatic abdominal multi-organ segmentation using deep convolutional neural network and time-implicit level sets. P Hu, F Wu, J Peng, Y Bao, F Chen, D Kong, International journal of computer assisted radiology and surgery. 123P. Hu, F. Wu, J. Peng, Y. Bao, F. Chen, and D. Kong, "Automatic abdominal multi-organ segmentation using deep convolutional neural network and time-implicit level sets," International journal of computer assisted radiology and surgery, vol. 12, no. 3, pp. 399-411, 2017.

Block level skip connections across cascaded v-net for multi-organ segmentation. L Zhang, J Zhang, P Shen, G Zhu, P Li, X Lu, H Zhang, S A Shah, M Bennamoun, IEEE Transactions on Medical Imaging. 399L. Zhang, J. Zhang, P. Shen, G. Zhu, P. Li, X. Lu, H. Zhang, S. A. Shah, and M. Bennamoun, "Block level skip connections across cascaded v-net for multi-organ segmentation," IEEE Transactions on Medical Imaging, vol. 39, no. 9, pp. 2782-2793, 2020.

Recurrent saliency transformation network for tiny target segmentation in abdominal ct scans. L Xie, Q Yu, Y Zhou, Y Wang, E K Fishman, A L Yuille, IEEE transactions on medical imaging. 392L. Xie, Q. Yu, Y. Zhou, Y. Wang, E. K. Fishman, and A. L. Yuille, "Recurrent saliency transformation network for tiny target segmentation in abdominal ct scans," IEEE transactions on medical imaging, vol. 39, no. 2, pp. 514-525, 2019.

Rap-net: Coarse-to-fine multi-organ segmentation with single random anatomical prior. H H Lee, Y Tang, S Bao, R G Abramson, Y Huo, B A Landman, 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). Nice, FranceIEEEH. H. Lee, Y. Tang, S. Bao, R. G. Abramson, Y. Huo, and B. A. Land- man, "Rap-net: Coarse-to-fine multi-organ segmentation with single random anatomical prior," in 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). Nice, France: IEEE, 2021, pp. 1491- 1494.

Automatic liver and lesion segmentation in ct using cascaded fully convolutional neural networks and 3d conditional random fields. P F Christ, M E A Elshaer, F Ettlinger, S Tatavarty, M Bickel, P Bilic, M Rempfler, M Armbruster, F Hofmann, M , International conference on medical image computing and computerassisted intervention. Cham, SwitzerlandSpringerP. F. Christ, M. E. A. Elshaer, F. Ettlinger, S. Tatavarty, M. Bickel, P. Bilic, M. Rempfler, M. Armbruster, F. Hofmann, M. D'Anastasi et al., "Automatic liver and lesion segmentation in ct using cascaded fully convolutional neural networks and 3d conditional random fields," in International conference on medical image computing and computer- assisted intervention. Cham, Switzerland: Springer, 2016, pp. 415- 423.

Glioma segmentation with cascaded unet," in Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes. D Lachinov, E Vasiliev, V Turlapov, Conjunction with MICCAI 2018. Granada, Spain; Cham, SwitzerlandSpringerRevised Selected Papers. Part II 4.D. Lachinov, E. Vasiliev, and V. Turlapov, "Glioma segmentation with cascaded unet," in Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part II 4. Cham, Switzerland: Springer, 2019, pp. 189-198.

Cascade dense-unet for prostate segmentation in mr images. S Li, Y Chen, S Yang, W Luo, Intelligent Computing Theories and Application: 15th International Conference, ICIC 2019. Nanchang, China; Cham, SwitzerlandSpringerProceedings, Part I 15S. Li, Y. Chen, S. Yang, and W. Luo, "Cascade dense-unet for prostate segmentation in mr images," in Intelligent Computing Theories and Application: 15th International Conference, ICIC 2019, Nanchang, China, August 3-6, 2019, Proceedings, Part I 15. Cham, Switzerland: Springer, 2019, pp. 481-490.

Segmentation of organs at risk in thoracic ct images using a sharpmask architecture and conditional random fields. R Trullo, C Petitjean, S Ruan, B Dubray, D Nie, D Shen, 2017 IEEE 14th international symposium on biomedical imaging. Melbourne, AustraliaIEEER. Trullo, C. Petitjean, S. Ruan, B. Dubray, D. Nie, and D. Shen, "Segmentation of organs at risk in thoracic ct images using a sharpmask architecture and conditional random fields," in 2017 IEEE 14th inter- national symposium on biomedical imaging (ISBI 2017). Melbourne, Australia: IEEE, 2017, pp. 1003-1006.

Organ at risk segmentation in head and neck ct images using a two-stage segmentation framework based on 3d u-net. Y Wang, L Zhao, M Wang, Z Song, IEEE Access. 7Y. Wang, L. Zhao, M. Wang, and Z. Song, "Organ at risk segmentation in head and neck ct images using a two-stage segmentation framework based on 3d u-net," IEEE Access, vol. 7, pp. 144 591-144 602, 2019.

More accurate and efficient segmentation of organs-at-risk in radiotherapy with convolutional neural networks cascades. K Men, H Geng, C Cheng, H Zhong, M Huang, Y Fan, J P Plastaras, A Lin, Y Xiao, Medical physics. 461K. Men, H. Geng, C. Cheng, H. Zhong, M. Huang, Y. Fan, J. P. Plas- taras, A. Lin, and Y. Xiao, "More accurate and efficient segmentation of organs-at-risk in radiotherapy with convolutional neural networks cascades," Medical physics, vol. 46, no. 1, pp. 286-292, 2019.

Multi-organ segmentation in head and neck mri using u-faster-rcnn. Y Lei, J Zhou, X Dong, T Wang, H Mao, M Mcdonald, W J Curran, T Liu, X Yang, Medical Imaging 2020: Image Processing. Houston, TXSPIE113133Y. Lei, J. Zhou, X. Dong, T. Wang, H. Mao, M. McDonald, W. J. Curran, T. Liu, and X. Yang, "Multi-organ segmentation in head and neck mri using u-faster-rcnn," in Medical Imaging 2020: Image Processing, vol. 113133A. Houston, TX: SPIE, 2020, pp. 826-831.

Thoraxnet: a 3d u-net based two-stage framework for oar segmentation on thoracic ct images. S Francis, P Jayaraj, P Pournami, M Thomas, A T Jose, A J Binu, N Puzhakkal, Physical and Engineering Sciences in Medicine. 451S. Francis, P. Jayaraj, P. Pournami, M. Thomas, A. T. Jose, A. J. Binu, and N. Puzhakkal, "Thoraxnet: a 3d u-net based two-stage framework for oar segmentation on thoracic ct images," Physical and Engineering Sciences in Medicine, vol. 45, no. 1, pp. 189-203, 2022.

Clinically applicable deep learning framework for organs at risk delineation in ct images. H Tang, X Chen, Y Liu, Z Lu, J You, M Yang, S Yao, G Zhao, Y Xu, T Chen, Nature Machine Intelligence. 110H. Tang, X. Chen, Y. Liu, Z. Lu, J. You, M. Yang, S. Yao, G. Zhao, Y. Xu, T. Chen et al., "Clinically applicable deep learning framework for organs at risk delineation in ct images," Nature Machine Intelli- gence, vol. 1, no. 10, pp. 480-491, 2019.

Cascaded deep learning-based auto-segmentation for head and neck cancer patients: Organs at risk on t2-weighted magnetic resonance imaging. J C Korte, N Hardcastle, S P Ng, B Clark, T Kron, P Jackson, Medical physics. 4812J. C. Korte, N. Hardcastle, S. P. Ng, B. Clark, T. Kron, and P. Jackson, "Cascaded deep learning-based auto-segmentation for head and neck cancer patients: Organs at risk on t2-weighted magnetic resonance imaging," Medical physics, vol. 48, no. 12, pp. 7757-7772, 2021.

Focusnet: Imbalanced large and small organ segmentation with an end-to-end deep neural network for head and neck ct images. Y Gao, R Huang, M Chen, Z Wang, J Deng, Y Chen, Y Yang, J Zhang, C Tao, H Li, Medical Image Computing and Computer Assisted Intervention-MICCAI 2019: 22nd International Conference. Shenzhen, China; Cham, SwitzerlandSpringerProceedings, Part III 22Y. Gao, R. Huang, M. Chen, Z. Wang, J. Deng, Y. Chen, Y. Yang, J. Zhang, C. Tao, and H. Li, "Focusnet: Imbalanced large and small organ segmentation with an end-to-end deep neural network for head and neck ct images," in Medical Image Computing and Computer Assisted Intervention-MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13-17, 2019, Proceedings, Part III 22. Cham, Switzerland: Springer, 2019, pp. 829-838.

Robust abdominal organ segmentation using regional convolutional neural networks. M Larsson, Y Zhang, F Kahl, Applied Soft Computing. 70M. Larsson, Y. Zhang, and F. Kahl, "Robust abdominal organ seg- mentation using regional convolutional neural networks," Applied Soft Computing, vol. 70, pp. 465-471, 2018.

Knowledge-aided convolutional neural network for small organ segmentation. Y Zhao, H Li, S Wan, A Sekuboyina, X Hu, G Tetteh, M Piraud, B Menze, IEEE journal of biomedical and health informatics. 234Y. Zhao, H. Li, S. Wan, A. Sekuboyina, X. Hu, G. Tetteh, M. Piraud, and B. Menze, "Knowledge-aided convolutional neural network for small organ segmentation," IEEE journal of biomedical and health informatics, vol. 23, no. 4, pp. 1363-1373, 2019.

Interleaved 3d-cnn s for joint segmentation of small-volume structures in head and neck ct images. X Ren, L Xiang, D Nie, Y Shao, H Zhang, D Shen, Q Wang, Medical physics. 455X. Ren, L. Xiang, D. Nie, Y. Shao, H. Zhang, D. Shen, and Q. Wang, "Interleaved 3d-cnn s for joint segmentation of small-volume structures in head and neck ct images," Medical physics, vol. 45, no. 5, pp. 2063- 2075, 2018.

3d lightweight network for simultaneous registration and segmentation of organs-at-risk in ct images of head and neck cancer. B Huang, Y Ye, Z Xu, Z Cai, Y He, Z Zhong, L Liu, X Chen, H Chen, B Huang, IEEE Transactions on Medical Imaging. 414B. Huang, Y. Ye, Z. Xu, Z. Cai, Y. He, Z. Zhong, L. Liu, X. Chen, H. Chen, and B. Huang, "3d lightweight network for simultaneous registration and segmentation of organs-at-risk in ct images of head and neck cancer," IEEE Transactions on Medical Imaging, vol. 41, no. 4, pp. 951-964, 2021.

Deep-learning-based detection and segmentation of organs at risk in nasopharyngeal carcinoma computed tomographic images for radiotherapy planning. S Liang, F Tang, X Huang, K Yang, T Zhong, R Hu, S Liu, X Yuan, Y Zhang, European radiology. 294S. Liang, F. Tang, X. Huang, K. Yang, T. Zhong, R. Hu, S. Liu, X. Yuan, and Y. Zhang, "Deep-learning-based detection and seg- mentation of organs at risk in nasopharyngeal carcinoma computed tomographic images for radiotherapy planning," European radiology, vol. 29, no. 4, pp. 1961-1967, 2019.

Fully automatic multi-organ segmentation for head and neck cancer radiotherapy using shape representation model constrained fully convolutional neural networks. N Tong, S Gou, S Yang, D Ruan, K Sheng, Medical physics. 4510N. Tong, S. Gou, S. Yang, D. Ruan, and K. Sheng, "Fully automatic multi-organ segmentation for head and neck cancer radiotherapy using shape representation model constrained fully convolutional neural networks," Medical physics, vol. 45, no. 10, pp. 4558-4567, 2018.

Hierarchical 3d fully convolutional networks for multi-organ segmentation. H R Roth, H Oda, Y Hayashi, M Oda, N Shimizu, M Fujiwara, K Misawa, K Mori, arXiv:1704.06382arXiv preprintH. R. Roth, H. Oda, Y. Hayashi, M. Oda, N. Shimizu, M. Fujiwara, K. Misawa, and K. Mori, "Hierarchical 3d fully convolutional networks for multi-organ segmentation," arXiv preprint arXiv:1704.06382, 2017.

Automatic segmentation of the clinical target volume and organs at risk in the planning ct for rectal cancer using deep dilated convolutional neural networks. K Men, J Dai, Y Li, Medical physics. 4412K. Men, J. Dai, and Y. Li, "Automatic segmentation of the clinical target volume and organs at risk in the planning ct for rectal cancer using deep dilated convolutional neural networks," Medical physics, vol. 44, no. 12, pp. 6377-6389, 2017.

A novel hybrid convolutional neural network for accurate organ segmentation in 3d head and neck ct images. Z Chen, C Li, J He, J Ye, D Song, S Wang, L Gu, Y Qiao, Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference. Strasbourg, France; Cham, SwitzerlandSpringerProceedings, Part I 24Z. Chen, C. Li, J. He, J. Ye, D. Song, S. Wang, L. Gu, and Y. Qiao, "A novel hybrid convolutional neural network for accurate organ segmentation in 3d head and neck ct images," in Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part I 24. Cham, Switzerland: Springer, 2021, pp. 569-578.

Fully automated multiorgan segmentation in abdominal magnetic resonance imaging with deep neural networks. Y Chen, D Ruan, J Xiao, L Wang, B Sun, R Saouaf, W Yang, D Li, Z Fan, Medical physics. 4710Y. Chen, D. Ruan, J. Xiao, L. Wang, B. Sun, R. Saouaf, W. Yang, D. Li, and Z. Fan, "Fully automated multiorgan segmentation in abdominal magnetic resonance imaging with deep neural networks," Medical physics, vol. 47, no. 10, pp. 4971-4982, 2020.

Automatic multiorgan segmentation on abdominal ct scans using deep u-net model. R Jain, A Sutradhar, A K Dash, S Das, 2021 19th OITS International Conference on Information Technology (OCIT). Bhubaneswar, IndiaIEEER. Jain, A. Sutradhar, A. K. Dash, and S. Das, "Automatic multi- organ segmentation on abdominal ct scans using deep u-net model," in 2021 19th OITS International Conference on Information Technology (OCIT). Bhubaneswar, India: IEEE, 2021, pp. 48-53.

Deep learning algorithm for automated segmentation and volume measurement of the liver and spleen using portal venous phase computed tomography images. Y Ahn, J S Yoon, S S Lee, H.-I Suk, J H Son, Y S Sung, Y Lee, B.-K Kang, H S Kim, Korean journal of radiology. 218Y. Ahn, J. S. Yoon, S. S. Lee, H.-I. Suk, J. H. Son, Y. S. Sung, Y. Lee, B.-K. Kang, and H. S. Kim, "Deep learning algorithm for automated segmentation and volume measurement of the liver and spleen using portal venous phase computed tomography images," Korean journal of radiology, vol. 21, no. 8, pp. 987-997, 2020.

A novel ulike network for the segmentation of thoracic organs. J Shi, K Wen, X Hao, X Xue, H An, H Zhang, 2020 IEEE 17th International Symposium on Biomedical Imaging Workshops (ISBI Workshops). Iowa City, IAIEEEJ. Shi, K. Wen, X. Hao, X. Xue, H. An, and H. Zhang, "A novel u- like network for the segmentation of thoracic organs," in 2020 IEEE 17th International Symposium on Biomedical Imaging Workshops (ISBI Workshops). Iowa City, IA: IEEE, 2020, pp. 1-4.

Multi-view spatial aggregation framework for joint localization and segmentation of organs at risk in head and neck ct images. S Liang, K.-H Thung, D Nie, Y Zhang, D Shen, IEEE Transactions on Medical Imaging. 399S. Liang, K.-H. Thung, D. Nie, Y. Zhang, and D. Shen, "Multi-view spatial aggregation framework for joint localization and segmentation of organs at risk in head and neck ct images," IEEE Transactions on Medical Imaging, vol. 39, no. 9, pp. 2794-2805, 2020.

Deep learning of the sectional appearances of 3d ct images for anatomical structure segmentation based on an fcn voting method. X Zhou, R Takayama, S Wang, T Hara, H Fujita, Medical physics. 4410X. Zhou, R. Takayama, S. Wang, T. Hara, and H. Fujita, "Deep learning of the sectional appearances of 3d ct images for anatomical structure segmentation based on an fcn voting method," Medical physics, vol. 44, no. 10, pp. 5221-5233, 2017.

Abdominal multi-organ segmentation with organ-attention networks and statistical fusion. Y Wang, Y Zhou, W Shen, S Park, E K Fishman, A L Yuille, Medical image analysis. 55Y. Wang, Y. Zhou, W. Shen, S. Park, E. K. Fishman, and A. L. Yuille, "Abdominal multi-organ segmentation with organ-attention networks and statistical fusion," Medical image analysis, vol. 55, pp. 88-102, 2019.

Spatial context-aware self-attention model for multi-organ segmentation. H Tang, X Liu, K Han, X Xie, X Chen, H Qian, Y Liu, S Sun, N Bai, Proceedings of the IEEE/CVF winter conference on applications of computer vision. the IEEE/CVF winter conference on applications of computer visionWaikoloa, HIH. Tang, X. Liu, K. Han, X. Xie, X. Chen, H. Qian, Y. Liu, S. Sun, and N. Bai, "Spatial context-aware self-attention model for multi-organ segmentation," in Proceedings of the IEEE/CVF winter conference on applications of computer vision, Waikoloa, HI, 2021, pp. 939-949.

M 3 net: A multi-scale multi-view framework for multiphase pancreas segmentation based on cross-phase non-local attention. T Qu, X Wang, C Fang, L Mao, J Li, P Li, J Qu, X Li, H Xue, Y Yu, Medical image analysis. 75102232T. Qu, X. Wang, C. Fang, L. Mao, J. Li, P. Li, J. Qu, X. Li, H. Xue, Y. Yu et al., "M 3 net: A multi-scale multi-view framework for multi- phase pancreas segmentation based on cross-phase non-local attention," Medical image analysis, vol. 75, p. 102232, 2022.

Mvfusfra: a multi-view dynamic fusion framework for multimodal brain tumor segmentation. Y Ding, W Zheng, J Geng, Z Qin, K.-K R Choo, Z Qin, X Hou, IEEE Journal of Biomedical and Health Informatics. 264Y. Ding, W. Zheng, J. Geng, Z. Qin, K.-K. R. Choo, Z. Qin, and X. Hou, "Mvfusfra: a multi-view dynamic fusion framework for multimodal brain tumor segmentation," IEEE Journal of Biomedical and Health Informatics, vol. 26, no. 4, pp. 1570-1581, 2021.

Multiview radar semantic segmentation. A Ouaknine, A Newson, P Pérez, F Tupin, J Rebut, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionMontreal, QC, 2021680A. Ouaknine, A. Newson, P. Pérez, F. Tupin, and J. Rebut, "Multi- view radar semantic segmentation," in Proceedings of the IEEE/CVF International Conference on Computer Vision, Montreal, QC, 2021, pp. 15 671-15 680.

A novel hybrid network for h&n organs at risk segmentation. Z S Cheng, T Y Zeng, S J Huang, X Yang, Proceedings of the 5th International Conference on Biomedical Signal and Image Processing. the 5th International Conference on Biomedical Signal and Image ProcessingSuzhou, ChinaZ. S. Cheng, T. Y. Zeng, S. J. Huang, and X. Yang, "A novel hybrid network for h&n organs at risk segmentation," in Proceedings of the 5th International Conference on Biomedical Signal and Image Processing, Suzhou, China, 2020, pp. 7-13.

Prior-aware neural network for partially-supervised multi-organ segmentation. Y Zhou, Z Li, S Bai, C Wang, X Chen, M Han, E Fishman, A L Yuille, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionSeoul, South Korea10Y. Zhou, Z. Li, S. Bai, C. Wang, X. Chen, M. Han, E. Fishman, and A. L. Yuille, "Prior-aware neural network for partially-supervised multi-organ segmentation," in Proceedings of the IEEE/CVF interna- tional conference on computer vision, Seoul, South Korea, 2019, pp. 10 672-10 681.

Learning multi-organ segmentation via partial-and mutual-prior from singleorgan datasets. S Lian, L Li, Z Luo, Z Zhong, B Wang, S Li, Biomedical Signal Processing and Control. 80104339S. Lian, L. Li, Z. Luo, Z. Zhong, B. Wang, and S. Li, "Learning multi-organ segmentation via partial-and mutual-prior from single- organ datasets," Biomedical Signal Processing and Control, vol. 80, p. 104339, 2023.

Anatomically constrained neural networks (acnns): application to cardiac image enhancement and segmentation. O Oktay, E Ferrante, K Kamnitsas, M Heinrich, W Bai, J Caballero, S A Cook, A De Marvao, T Dawes, D P O&apos;regan, IEEE transactions on medical imaging. 372O. Oktay, E. Ferrante, K. Kamnitsas, M. Heinrich, W. Bai, J. Ca- ballero, S. A. Cook, A. De Marvao, T. Dawes, D. P. O'Regan et al., "Anatomically constrained neural networks (acnns): application to cardiac image enhancement and segmentation," IEEE transactions on medical imaging, vol. 37, no. 2, pp. 384-395, 2017.

Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, Advances in Neural Information Processing Systems. 33J. Ho, A. Jain, and P. Abbeel, "Denoising diffusion probabilistic models," Advances in Neural Information Processing Systems, vol. 33, pp. 6840-6851, 2020.

Denoising diffusion implicit models. J Song, C Meng, S Ermon, arXiv:2010.02502arXiv preprintJ. Song, C. Meng, and S. Ermon, "Denoising diffusion implicit mod- els," arXiv preprint arXiv:2010.02502, 2020.

Enhancing organ at risk segmentation with improved deep neural networks. I Isler, C Lisle, J Rineer, P Kelly, D Turgut, J Ricci, U Bagci, Medical Imaging 2022: Image Processing. San Diego, CASPIE12032I. Isler, C. Lisle, J. Rineer, P. Kelly, D. Turgut, J. Ricci, and U. Bagci, "Enhancing organ at risk segmentation with improved deep neural networks," in Medical Imaging 2022: Image Processing, vol. 12032. San Diego, CA: SPIE, 2022, pp. 814-820.

A 2d dilated residual unet for multi-organ segmentation in thoracic ct. S Vesal, N Ravikumar, A Maier, arXiv:1905.07710arXiv preprintS. Vesal, N. Ravikumar, and A. Maier, "A 2d dilated residual u- net for multi-organ segmentation in thoracic ct," arXiv preprint arXiv:1905.07710, 2019.

Feature pyramid networks for object detection. T.-Y Lin, P Dollár, R Girshick, K He, B Hariharan, S Belongie, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHonolulu, HIT.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie, "Feature pyramid networks for object detection," in Proceedings of the IEEE conference on computer vision and pattern recognition, Honolulu, HI, 2017, pp. 2117-2125.

Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, IEEE transactions on pattern analysis and machine intelligence. 40L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs," IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 4, pp. 834-848, 2017.

An efficient multi-scale fusion network for 3d organ at risk (oar) segmentation. A Srivastava, D Jha, E Keles, B Aydogan, M Abazeed, U Bagci, arXiv:2208.07417arXiv preprintA. Srivastava, D. Jha, E. Keles, B. Aydogan, M. Abazeed, and U. Bagci, "An efficient multi-scale fusion network for 3d organ at risk (oar) segmentation," arXiv preprint arXiv:2208.07417, 2022.

O Oktay, J Schlemper, L L Folgoc, M Lee, M Heinrich, K Misawa, K Mori, S Mcdonagh, N Y Hammerla, B Kainz, arXiv:1804.03999Attention u-net: Learning where to look for the pancreas. arXiv preprintO. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz et al., "Atten- tion u-net: Learning where to look for the pancreas," arXiv preprint arXiv:1804.03999, 2018.

Hu j., shen l., albanie s., sun g. S Hu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 428Squeezeand-excitation networksS. Hu et al., "Hu j., shen l., albanie s., sun g., wu e," Squeeze- and-excitation networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 8, pp. 2011-2023, 2019.

Csaf-cnn: cross-layer spatial attention map fusion network for organ-at-risk segmentation in head and neck ct images. Z Liu, H Wang, W Lei, G Wang, 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). Iowa City, IAIEEEZ. Liu, H. Wang, W. Lei, and G. Wang, "Csaf-cnn: cross-layer spatial attention map fusion network for organ-at-risk segmentation in head and neck ct images," in 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). Iowa City, IA: IEEE, 2020, pp. 1522- 1525.

Variance-aware attention u-net for multi-organ segmentation. H Lin, Z Li, Z Yang, Y Wang, Medical Physics. 4812H. Lin, Z. Li, Z. Yang, and Y. Wang, "Variance-aware attention u-net for multi-organ segmentation," Medical Physics, vol. 48, no. 12, pp. 7864-7876, 2021.

Unet++: A nested u-net architecture for medical image segmentation. Z Zhou, M M Rahman Siddiquee, N Tajbakhsh, J Liang, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop. Granada, Spain; Cham, SwitzerlandSpringerProceedings 4.Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, "Unet++: A nested u-net architecture for medical image segmenta- tion," in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, Proceedings 4. Cham, Switzerland: Springer, 2018, pp. 3-11.

Densely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHonolulu, HIG. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, "Densely connected convolutional networks," in Proceedings of the IEEE confer- ence on computer vision and pattern recognition, Honolulu, HI, 2017, pp. 4700-4708.

Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSan Juan, PRK. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition, San Juan, PR, 2016, pp. 770-778.

Deformable convolutional networks. J Dai, H Qi, Y Xiong, Y Li, G Zhang, H Hu, Y Wei, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionVenice, ItalyJ. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, "Deformable convolutional networks," in Proceedings of the IEEE international conference on computer vision, Venice, Italy, 2017, pp. 764-773.

Obelisk-net: Fewer layers to solve 3d multi-organ segmentation with sparse deformable convolutions. M P Heinrich, O Oktay, N Bouteldja, Medical image analysis. 54M. P. Heinrich, O. Oktay, and N. Bouteldja, "Obelisk-net: Fewer layers to solve 3d multi-organ segmentation with sparse deformable convolutions," Medical image analysis, vol. 54, pp. 1-9, 2019.

Multi-organ segmentation network for abdominal ct images based on spatial attention and deformable convolution. N Shen, Z Wang, J Li, H Gao, W Lu, P Hu, L Feng, Expert Systems with Applications. 211118625N. Shen, Z. Wang, J. Li, H. Gao, W. Lu, P. Hu, and L. Feng, "Multi-organ segmentation network for abdominal ct images based on spatial attention and deformable convolution," Expert Systems with Applications, vol. 211, p. 118625, 2023.

Strip pooling: Rethinking spatial pooling for scene parsing. Q Hou, L Zhang, M.-M Cheng, J Feng, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionSeattle, WAQ. Hou, L. Zhang, M.-M. Cheng, and J. Feng, "Strip pooling: Rethink- ing spatial pooling for scene parsing," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, Seattle, WA, 2020, pp. 4003-4012.

Efficient context-aware network for abdominal multi-organ segmentation. F Zhang, Y Wang, H Yang, arXiv:2109.10601arXiv preprintF. Zhang, Y. Wang, and H. Yang, "Efficient context-aware net- work for abdominal multi-organ segmentation," arXiv preprint arXiv:2109.10601, 2021.

A survey of loss functions for semantic segmentation. S Jadon, 2020 IEEE conference on computational intelligence in bioinformatics and computational biology (CIBCB). Via del Mar. ChileIEEES. Jadon, "A survey of loss functions for semantic segmentation," in 2020 IEEE conference on computational intelligence in bioinformatics and computational biology (CIBCB). Via del Mar, Chile: IEEE, 2020, pp. 1-7.

Automated image segmentation using improved pcnn model based on cross-entropy. M Yi-De, L Qing, Q Zhi-Bai, Proceedings of 2004 International Symposium on Intelligent Multimedia, Video and Speech Processing. 2004 International Symposium on Intelligent Multimedia, Video and Speech ProcessingHong Kong, ChinaIEEEM. Yi-de, L. Qing, and Q. Zhi-Bai, "Automated image segmentation using improved pcnn model based on cross-entropy," in Proceedings of 2004 International Symposium on Intelligent Multimedia, Video and Speech Processing, 2004. Hong Kong, China: IEEE, 2004, pp. 743- 746.

Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations. C H Sudre, W Li, T Vercauteren, S Ourselin, M. Jorge Cardoso, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th International Workshop. Québec City, QC, Canada; Cham, SwitzerlandSpringer3Conjunction with MICCAI 2017C. H. Sudre, W. Li, T. Vercauteren, S. Ourselin, and M. Jorge Car- doso, "Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations," in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Québec City, QC, Canada, September 14, Proceedings 3. Cham, Switzerland: Springer, 2017, pp. 240-248.

Tversky loss function for image segmentation using 3d fully convolutional deep networks. S S M Salehi, D Erdogmus, A Gholipour, Machine Learning in Medical Imaging: 8th International Workshop, MLMI 2017, Held in Conjunction with MICCAI 2017. Quebec City, QC, Canada; Cham, SwitzerlandSpringer8S. S. M. Salehi, D. Erdogmus, and A. Gholipour, "Tversky loss function for image segmentation using 3d fully convolutional deep networks," in Machine Learning in Medical Imaging: 8th International Workshop, MLMI 2017, Held in Conjunction with MICCAI 2017, Quebec City, QC, Canada, September 10, 2017, Proceedings 8. Cham, Switzerland: Springer, 2017, pp. 379-387.

Focal loss for dense object detection. T.-Y Lin, P Goyal, R Girshick, K He, P Dollár, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionVenice, ItalyT.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, "Focal loss for dense object detection," in Proceedings of the IEEE international conference on computer vision, Venice, Italy, 2017, pp. 2980-2988.

Weighted rank aggregation of cluster validation measures: a monte carlo cross-entropy approach. V Pihur, S Datta, S Datta, Bioinformatics. 2313V. Pihur, S. Datta, and S. Datta, "Weighted rank aggregation of cluster validation measures: a monte carlo cross-entropy approach," Bioinformatics, vol. 23, no. 13, pp. 1607-1615, 2007.

On the influence of dice loss function in multi-class organ segmentation of abdominal ct using 3d fully convolutional networks. C Shen, H R Roth, H Oda, M Oda, Y Hayashi, K Misawa, K Mori, arXiv:1801.05912arXiv preprintC. Shen, H. R. Roth, H. Oda, M. Oda, Y. Hayashi, K. Misawa, and K. Mori, "On the influence of dice loss function in multi-class organ segmentation of abdominal ct using 3d fully convolutional networks," arXiv preprint arXiv:1801.05912, 2018.

Tackling the class imbalance problem of deep learning-based head and neck organ segmentation. E Tappeiner, M Welk, R Schubert, International Journal of Computer Assisted Radiology and Surgery. 1711E. Tappeiner, M. Welk, and R. Schubert, "Tackling the class imbalance problem of deep learning-based head and neck organ segmentation," International Journal of Computer Assisted Radiology and Surgery, vol. 17, no. 11, pp. 2103-2111, 2022.

A novel focal tversky loss function with improved attention u-net for lesion segmentation. N Abraham, N M Khan, 2019 IEEE 16th international symposium on biomedical imaging. Venice, ItalyIEEEN. Abraham and N. M. Khan, "A novel focal tversky loss function with improved attention u-net for lesion segmentation," in 2019 IEEE 16th international symposium on biomedical imaging (ISBI 2019). Venice, Italy: IEEE, 2019, pp. 683-687.

3d segmentation with exponential logarithmic loss for highly unbalanced object sizes. K C Wong, M Moradi, H Tang, T Syeda-Mahmood, 21st International Conference. Granada, Spain; Cham, SwitzerlandSpringerProceedings, Part III 11K. C. Wong, M. Moradi, H. Tang, and T. Syeda-Mahmood, "3d segmentation with exponential logarithmic loss for highly unbalanced object sizes," in Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part III 11. Cham, Switzerland: Springer, 2018, pp. 612-619.

Global and local feature reconstruction for medical image segmentation. J Song, X Chen, Q Zhu, F Shi, D Xiang, Z Chen, Y Fan, L Pan, W Zhu, IEEE Transactions on Medical Imaging. 419J. Song, X. Chen, Q. Zhu, F. Shi, D. Xiang, Z. Chen, Y. Fan, L. Pan, and W. Zhu, "Global and local feature reconstruction for medical image segmentation," IEEE Transactions on Medical Imaging, vol. 41, no. 9, pp. 2273-2284, 2022.

Automatic segmentation of organs-at-risk from head-and-neck ct using separable convolutional neural network with hard-region-weighted loss. W Lei, H Mei, Z Sun, S Ye, R Gu, H Wang, R Huang, S Zhang, S Zhang, G Wang, Neurocomputing. 442W. Lei, H. Mei, Z. Sun, S. Ye, R. Gu, H. Wang, R. Huang, S. Zhang, S. Zhang, and G. Wang, "Automatic segmentation of organs-at-risk from head-and-neck ct using separable convolutional neural network with hard-region-weighted loss," Neurocomputing, vol. 442, pp. 184- 199, 2021.

The liver tumor segmentation benchmark (lits). P Bilic, P Christ, H B Li, E Vorontsov, A Ben-Cohen, G Kaissis, A Szeskin, C Jacobs, G E H Mamani, G Chartrand, Medical Image Analysis. 84102680P. Bilic, P. Christ, H. B. Li, E. Vorontsov, A. Ben-Cohen, G. Kaissis, A. Szeskin, C. Jacobs, G. E. H. Mamani, G. Chartrand et al., "The liver tumor segmentation benchmark (lits)," Medical Image Analysis, vol. 84, p. 102680, 2023.

The kits19 challenge data: 300 kidney tumor cases with clinical context, ct semantic segmentations, and surgical outcomes. N Heller, N Sathianathen, A Kalapara, E Walczak, K Moore, H Kaluzniak, J Rosenberg, P Blake, Z Rengel, M Oestreich, arXiv:1904.00445arXiv preprintN. Heller, N. Sathianathen, A. Kalapara, E. Walczak, K. Moore, H. Kaluzniak, J. Rosenberg, P. Blake, Z. Rengel, M. Oestreich et al., "The kits19 challenge data: 300 kidney tumor cases with clinical context, ct semantic segmentations, and surgical outcomes," arXiv preprint arXiv:1904.00445, 2019.

A large annotated medical image dataset for the development and evaluation of segmentation algorithms. A L Simpson, M Antonelli, S Bakas, M Bilello, K Farahani, B Van Ginneken, A Kopp-Schneider, B A Landman, G Litjens, B Menze, arXiv:1902.09063arXiv preprintA. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. Van Ginneken, A. Kopp-Schneider, B. A. Landman, G. Litjens, B. Menze et al., "A large annotated medical image dataset for the de- velopment and evaluation of segmentation algorithms," arXiv preprint arXiv:1902.09063, 2019.

Med3d: Transfer learning for 3d medical image analysis. S Chen, K Ma, Y Zheng, arXiv:1904.00625arXiv preprintS. Chen, K. Ma, and Y. Zheng, "Med3d: Transfer learning for 3d medical image analysis," arXiv preprint arXiv:1904.00625, 2019.

Learning multi-class segmentations from single-class datasets. K Dmitriev, A E Kaufman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLong Beach, CAK. Dmitriev and A. E. Kaufman, "Learning multi-class segmentations from single-class datasets," in Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, Long Beach, CA, 2019, pp. 9501-9511.

Dodnet: Learning to segment multi-organ and tumors from multiple partially labeled datasets. J Zhang, Y Xie, Y Xia, C Shen, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionNashville, TNJ. Zhang, Y. Xie, Y. Xia, and C. Shen, "Dodnet: Learning to segment multi-organ and tumors from multiple partially labeled datasets," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, Nashville, TN, 2021, pp. 1195-1204.

Learning from partially labeled data for multi-organ and tumor segmentation. Y Xie, J Zhang, Y Xia, C Shen, arXiv:2211.06894arXiv preprintY. Xie, J. Zhang, Y. Xia, and C. Shen, "Learning from partially labeled data for multi-organ and tumor segmentation," arXiv preprint arXiv:2211.06894, 2022.

nnu-net: a self-configuring method for deep learning-based biomedical image segmentation. F Isensee, P F Jaeger, S A Kohl, J Petersen, K H Maier-Hein, Nature methods. 182F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein, "nnu-net: a self-configuring method for deep learning-based biomedical image segmentation," Nature methods, vol. 18, no. 2, pp. 203-211, 2021.

Tgnet: A task-guided network architecture for multi-organ and tumour segmentation from partially labelled datasets. H Wu, S Pang, A Sowmya, 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI). Kolkata, IndiaIEEEH. Wu, S. Pang, and A. Sowmya, "Tgnet: A task-guided network architecture for multi-organ and tumour segmentation from partially labelled datasets," in 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI). Kolkata, India: IEEE, 2022, pp. 1-5.

Incremental learning for multiorgan segmentation with partially labeled datasets. P Liu, L Xiao, S K Zhou, arXiv:2103.04526arXiv preprintP. Liu, L. Xiao, and S. K. Zhou, "Incremental learning for multi- organ segmentation with partially labeled datasets," arXiv preprint arXiv:2103.04526, 2021.

Federated multi-organ segmentation with partially labeled data. X Xu, P Yan, arXiv:2206.07156arXiv preprintX. Xu and P. Yan, "Federated multi-organ segmentation with partially labeled data," arXiv preprint arXiv:2206.07156, 2022.

Multi-organ segmentation over partially labeled datasets with multi-scale feature abstraction. X Fang, P Yan, IEEE Transactions on Medical Imaging. 3911X. Fang and P. Yan, "Multi-organ segmentation over partially labeled datasets with multi-scale feature abstraction," IEEE Transactions on Medical Imaging, vol. 39, no. 11, pp. 3619-3629, 2020.

Marginal loss and exclusion loss for partially supervised multi-organ segmentation. G Shi, L Xiao, Y Chen, S K Zhou, Medical Image Analysis. 70101979G. Shi, L. Xiao, Y. Chen, and S. K. Zhou, "Marginal loss and exclusion loss for partially supervised multi-organ segmentation," Medical Image Analysis, vol. 70, p. 101979, 2021.

Multi-organ segmentation via co-training weight-averaged models from few-organ datasets. R Huang, Y Zheng, Z Hu, S Zhang, H Li, Medical Image Computing and Computer Assisted Intervention-MICCAI 2020: 23rd International Conference. Lima, Peru; Cham, SwitzerlandSpringerProceedings, Part IV 23R. Huang, Y. Zheng, Z. Hu, S. Zhang, and H. Li, "Multi-organ segmentation via co-training weight-averaged models from few-organ datasets," in Medical Image Computing and Computer Assisted Intervention-MICCAI 2020: 23rd International Conference, Lima, Peru, October 4-8, 2020, Proceedings, Part IV 23. Cham, Switzerland: Springer, 2020, pp. 146-155.

Unsupervised ensemble distillation for multi-organ segmentation. L Zhang, S Feng, Y Wang, Y Wang, Y Zhang, X Chen, Q Tian, 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI). Kolkata, IndiaIEEEL. Zhang, S. Feng, Y. Wang, Y. Wang, Y. Zhang, X. Chen, and Q. Tian, "Unsupervised ensemble distillation for multi-organ segmentation," in 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI). Kolkata, India: IEEE, 2022, pp. 1-5.

Weakly-supervised cerebrovascular segmentation network with shape prior and model indicator. Q Wu, Y Chen, N Huang, X Yue, Proceedings of the 2022 International Conference on Multimedia Retrieval. the 2022 International Conference on Multimedia RetrievalNewark, NJ, 2022Q. Wu, Y. Chen, N. Huang, and X. Yue, "Weakly-supervised cere- brovascular segmentation network with shape prior and model in- dicator," in Proceedings of the 2022 International Conference on Multimedia Retrieval, Newark, NJ, 2022, pp. 668-676.

Joint supervoxel classification forest for weaklysupervised organ segmentation. F Kanavati, K Misawa, M Fujiwara, K Mori, D Rueckert, B Glocker, Machine Learning in Medical Imaging: 8th International Workshop. Quebec City, QC, Canada; Cham, SwitzerlandSpringer8Conjunction with MICCAI 2017F. Kanavati, K. Misawa, M. Fujiwara, K. Mori, D. Rueckert, and B. Glocker, "Joint supervoxel classification forest for weakly- supervised organ segmentation," in Machine Learning in Medical Imag- ing: 8th International Workshop, MLMI 2017, Held in Conjunction with MICCAI 2017, Quebec City, QC, Canada, September 10, 2017, Proceedings 8. Cham, Switzerland: Springer, 2017, pp. 79-87.

Semisupervised learning for network-based cardiac mr image segmentation. W Bai, O Oktay, M Sinclair, H Suzuki, M Rajchl, G Tarroni, B Glocker, A King, P M Matthews, D Rueckert, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2017: 20th International Conference. Quebec City, QC, Canada; Cham, SwitzerlandSpringerProceedings, Part II 20W. Bai, O. Oktay, M. Sinclair, H. Suzuki, M. Rajchl, G. Tarroni, B. Glocker, A. King, P. M. Matthews, and D. Rueckert, "Semi- supervised learning for network-based cardiac mr image segmentation," in Medical Image Computing and Computer-Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part II 20. Cham, Switzerland: Springer, 2017, pp. 253-260.

Semi-supervised medical image segmentation via cross teaching between cnn and transformer. X Luo, M Hu, T Song, G Wang, S Zhang, International Conference on Medical Imaging with Deep Learning. Zurich, SwitzerlandPMLRX. Luo, M. Hu, T. Song, G. Wang, and S. Zhang, "Semi-supervised medical image segmentation via cross teaching between cnn and transformer," in International Conference on Medical Imaging with Deep Learning. Zurich, Switzerland: PMLR, 2022, pp. 820-833.

Semi-supervised unpaired medical image segmentation through task-affinity consistency. J Chen, J Zhang, K Debattista, J Han, IEEE Transactions on Medical Imaging. J. Chen, J. Zhang, K. Debattista, and J. Han, "Semi-supervised unpaired medical image segmentation through task-affinity consistency," IEEE Transactions on Medical Imaging, 2022.

Mutual consistency learning for semi-supervised medical image segmentation. Y Wu, Z Ge, D Zhang, M Xu, L Zhang, Y Xia, J Cai, Medical Image Analysis. 81102530Y. Wu, Z. Ge, D. Zhang, M. Xu, L. Zhang, Y. Xia, and J. Cai, "Mutual consistency learning for semi-supervised medical image segmentation," Medical Image Analysis, vol. 81, p. 102530, 2022.

Efficient semi-supervised gross target volume of nasopharyngeal carcinoma segmentation via uncertainty rectified pyramid consistency. X Luo, W Liao, J Chen, T Song, Y Chen, S Zhang, N Chen, G Wang, S Zhang, Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference. Strasbourg, France; Cham, SwitzerlandSpringerProceedings, Part II 24X. Luo, W. Liao, J. Chen, T. Song, Y. Chen, S. Zhang, N. Chen, G. Wang, and S. Zhang, "Efficient semi-supervised gross target volume of nasopharyngeal carcinoma segmentation via uncertainty rectified pyramid consistency," in Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part II 24. Cham, Switzerland: Springer, 2021, pp. 318-329.

Learning with limited annotations: a survey on deep semi-supervised learning for medical image segmentation. R Jiao, Y Zhang, L Ding, R Cai, J Zhang, arXiv:2207.14191arXiv preprintR. Jiao, Y. Zhang, L. Ding, R. Cai, and J. Zhang, "Learning with limited annotations: a survey on deep semi-supervised learning for medical image segmentation," arXiv preprint arXiv:2207.14191, 2022.

Semi-supervised 3d abdominal multi-organ segmentation via deep multi-planar co-training. Y Zhou, Y Wang, P Tang, S Bai, W Shen, E Fishman, A Yuille, 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). Waikoloa, HIIEEEY. Zhou, Y. Wang, P. Tang, S. Bai, W. Shen, E. Fishman, and A. Yuille, "Semi-supervised 3d abdominal multi-organ segmentation via deep multi-planar co-training," in 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). Waikoloa, HI: IEEE, 2019, pp. 121-140.

Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation. Y Xia, D Yang, Z Yu, F Liu, J Cai, L Yu, Z Zhu, D Xu, A Yuille, H Roth, Medical image analysis. 65101766Y. Xia, D. Yang, Z. Yu, F. Liu, J. Cai, L. Yu, Z. Zhu, D. Xu, A. Yuille, and H. Roth, "Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation," Medical image analysis, vol. 65, p. 101766, 2020.

Dlunet: Semi-supervised learning based dual-light unet for multi-organ segmentation," in Fast and Low-Resource Semi-supervised Abdominal Organ Segmentation: MICCAI 2022 Challenge, FLARE 2022, Held in Conjunction with MICCAI 2022. H Lai, T Wang, S Zhou, Proceedings. SpringerH. Lai, T. Wang, and S. Zhou, "Dlunet: Semi-supervised learning based dual-light unet for multi-organ segmentation," in Fast and Low- Resource Semi-supervised Abdominal Organ Segmentation: MICCAI 2022 Challenge, FLARE 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings. Springer, 2023, pp. 64-73.

Semi-supervised multiorgan segmentation through quality assurance supervision. H H Lee, Y Tang, O Tang, Y Xu, Y Chen, D Gao, S Han, R Gao, M R Savona, R G Abramson, Medical Imaging 2020: Image Processing. Houston, TXSPIE11313H. H. Lee, Y. Tang, O. Tang, Y. Xu, Y. Chen, D. Gao, S. Han, R. Gao, M. R. Savona, R. G. Abramson et al., "Semi-supervised multi- organ segmentation through quality assurance supervision," in Medical Imaging 2020: Image Processing, vol. 11313. Houston, TX: SPIE, 2020, pp. 363-369.

Co-heterogeneous and adaptive segmentation from multi-source and multi-phase ct imaging data: A study on pathological liver and lesion segmentation. A Raju, C.-T Cheng, Y Huo, J Cai, J Huang, J Xiao, L Lu, C Liao, A P Harrison, 202016in Computer Vision-ECCVA. Raju, C.-T. Cheng, Y. Huo, J. Cai, J. Huang, J. Xiao, L. Lu, C. Liao, and A. P. Harrison, "Co-heterogeneous and adaptive segmentation from multi-source and multi-phase ct imaging data: A study on pathological liver and lesion segmentation," in Computer Vision-ECCV 2020: 16th

Proceedings, Part XXIII. Part XXIIIGlasgow, UK; Cham, SwitzerlandSpringerEuropean ConferenceEuropean Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIII. Cham, Switzerland: Springer, 2020, pp. 448-465.

Organ at risk segmentation for head and neck cancer using stratified learning and neural architecture search. D Guo, D Jin, Z Zhu, T.-Y Ho, A P Harrison, C.-H Chao, J Xiao, L Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSeattle, WAD. Guo, D. Jin, Z. Zhu, T.-Y. Ho, A. P. Harrison, C.-H. Chao, J. Xiao, and L. Lu, "Organ at risk segmentation for head and neck cancer using stratified learning and neural architecture search," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, 2020, pp. 4223-4232.

Embracing imperfect datasets: A review of deep learning solutions for medical image segmentation. N Tajbakhsh, L Jeyaseelan, Q Li, J N Chiang, Z Wu, X Ding, Medical Image Analysis. 63101693N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, and X. Ding, "Embracing imperfect datasets: A review of deep learning solutions for medical image segmentation," Medical Image Analysis, vol. 63, p. 101693, 2020.

Towards labelefficient automatic diagnosis and analysis: a comprehensive survey of advanced deep learning-based weakly-supervised, semi-supervised and self-supervised techniques in histopathological image analysis. L Qu, S Liu, X Liu, M Wang, Z Song, Physics in Medicine & Biology. 6720L. Qu, S. Liu, X. Liu, M. Wang, and Z. Song, "Towards label- efficient automatic diagnosis and analysis: a comprehensive survey of advanced deep learning-based weakly-supervised, semi-supervised and self-supervised techniques in histopathological image analysis," Physics in Medicine & Biology, vol. 67, no. 20, p. 20TR01, 2022.

Domain adaptive relational reasoning for 3d multi-organ segmentation. S Fu, Y Lu, Y Wang, Y Zhou, W Shen, E Fishman, A Yuille, Medical Image Computing and Computer Assisted Intervention-MICCAI 2020: 23rd International Conference. Lima, Peru; Cham, SwitzerlandSpringerProceedings, Part I 23S. Fu, Y. Lu, Y. Wang, Y. Zhou, W. Shen, E. Fishman, and A. Yuille, "Domain adaptive relational reasoning for 3d multi-organ segmentation," in Medical Image Computing and Computer Assisted Intervention-MICCAI 2020: 23rd International Conference, Lima, Peru, October 4-8, 2020, Proceedings, Part I 23. Cham, Switzerland: Springer, 2020, pp. 656-666.

Source-free unsupervised domain adaptation for cross-modality abdominal multi-organ segmentation. J Hong, Y.-D Zhang, W Chen, Knowledge-Based Systems. 250109155J. Hong, Y.-D. Zhang, and W. Chen, "Source-free unsupervised domain adaptation for cross-modality abdominal multi-organ segmentation," Knowledge-Based Systems, vol. 250, p. 109155, 2022.

Head and neck multi-organ autosegmentation on ct images aided by synthetic mri. Y Liu, Y Lei, Y Fu, T Wang, J Zhou, X Jiang, M Mcdonald, J J Beitler, W J Curran, T Liu, Medical physics. 479Y. Liu, Y. Lei, Y. Fu, T. Wang, J. Zhou, X. Jiang, M. McDonald, J. J. Beitler, W. J. Curran, T. Liu et al., "Head and neck multi-organ auto- segmentation on ct images aided by synthetic mri," Medical physics, vol. 47, no. 9, pp. 4294-4302, 2020.

Managing class imbalance in multi-organ ct segmentation in head and neck cancer patients. S Cros, E Vorontsov, S Kadoury, 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). Nice, FranceIEEES. Cros, E. Vorontsov, and S. Kadoury, "Managing class imbalance in multi-organ ct segmentation in head and neck cancer patients," in 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). Nice, France: IEEE, 2021, pp. 1360-1364.

Nested block self-attention multiple resolution residual network for multiorgan segmentation from ct. J Jiang, S Elguindi, S L Berry, I Onochie, L Cervino, J O Deasy, H Veeraraghavan, Medical Physics. 498J. Jiang, S. Elguindi, S. L. Berry, I. Onochie, L. Cervino, J. O. Deasy, and H. Veeraraghavan, "Nested block self-attention multiple resolution residual network for multiorgan segmentation from ct," Medical Physics, vol. 49, no. 8, pp. 5244-5257, 2022.

Sabos-net: Self-supervised attention based network for automatic organ segmentation of head and neck ct images. S Francis, G Pooloth, S B S Singam, N Puzhakkal, P Pulinthanathu Narayanan, J Balakrishnan, International Journal of Imaging Systems and Technology. 331S. Francis, G. Pooloth, S. B. S. Singam, N. Puzhakkal, P. Pulinthanathu Narayanan, and J. Pottekkattuvalappil Balakrishnan, "Sabos-net: Self-supervised attention based network for automatic organ segmentation of head and neck ct images," International Journal of Imaging Systems and Technology, vol. 33, no. 1, pp. 175-191, 2023.

The cancer imaging archive (tcia): maintaining and operating a public information repository. K Clark, B Vendt, K Smith, J Freymann, J Kirby, P Koppel, S Moore, S Phillips, D Maffitt, M Pringle, Journal of digital imaging. 266K. Clark, B. Vendt, K. Smith, J. Freymann, J. Kirby, P. Koppel, S. Moore, S. Phillips, D. Maffitt, M. Pringle et al., "The cancer imaging archive (tcia): maintaining and operating a public information repository," Journal of digital imaging, vol. 26, no. 6, pp. 1045-1057, 2013.

Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation. H R Roth, L Lu, A Farag, H.-C Shin, J Liu, E B Turkbey, R M Summers, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference. Munich, Germany; Cham, SwitzerlandSpringerProceedings, Part I 18H. R. Roth, L. Lu, A. Farag, H.-C. Shin, J. Liu, E. B. Turkbey, and R. M. Summers, "Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation," in Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part I 18. Cham, Switzerland: Springer, 2015, pp. 556-564.

Adversarial training of deep convolutional neural network for multi-organ segmentation from multi-sequence mri of the abdomen. V Kumar, M K Sharma, R Jehadeesan, B Venkatraman, D Sheet, 2021 International Conference on Intelligent Technologies (CONIT). Hubli. IndiaIEEEV. Kumar, M. K. Sharma, R. Jehadeesan, B. Venkatraman, and D. Sheet, "Adversarial training of deep convolutional neural network for multi-organ segmentation from multi-sequence mri of the ab- domen," in 2021 International Conference on Intelligent Technologies (CONIT). Hubli, India: IEEE, 2021, pp. 1-6.

Ctorg, a new dataset for multiple organ segmentation in computed tomography. B Rister, D Yi, K Shivakumar, T Nobashi, D L Rubin, Scientific Data. 71381B. Rister, D. Yi, K. Shivakumar, T. Nobashi, and D. L. Rubin, "Ct- org, a new dataset for multiple organ segmentation in computed tomography," Scientific Data, vol. 7, no. 1, p. 381, 2020.

Deep convolutional neural networks for automatic segmentation of thoracic organs-at-risk in radiation oncology-use of non-domain transfer learning. C C Vu, Z A Siddiqui, L Zamdborg, A B Thompson, T J Quinn, E Castillo, T M Guerrero, Journal of Applied Clinical Medical Physics. 216C. C. Vu, Z. A. Siddiqui, L. Zamdborg, A. B. Thompson, T. J. Quinn, E. Castillo, and T. M. Guerrero, "Deep convolutional neural networks for automatic segmentation of thoracic organs-at-risk in radiation oncology-use of non-domain transfer learning," Journal of Applied Clinical Medical Physics, vol. 21, no. 6, pp. 108-113, 2020.

Dilated u-net based segmentation of organs at risk in thoracic ct images. M S K Gali, N Garg, S Vasamsetti, in SegTHOR@ ISBIM. S. K. Gali, N. Garg, S. Vasamsetti et al., "Dilated u-net based segmentation of organs at risk in thoracic ct images," in SegTHOR@ ISBI, 2019.

Rapid segmentation of thoracic organs using u-net architecture. H Mahmood, S M S Islam, J Hill, G Tay, 2021 Digital Image Computing: Techniques and Applications (DICTA). Gold Coast, AustraliaIEEEH. Mahmood, S. M. S. Islam, J. Hill, and G. Tay, "Rapid segmentation of thoracic organs using u-net architecture," in 2021 Digital Image Computing: Techniques and Applications (DICTA). Gold Coast, Australia: IEEE, 2021, pp. 1-6.

Geometric and dosimetric evaluation of the automatic delineation of organs at risk (oars) in non-small-cell lung cancer radiotherapy based on a modified densenet deep learning network. F Zhang, Q Wang, A Yang, N Lu, H Jiang, D Chen, Y Yu, Y Wang, Frontiers in Oncology. 12861857F. Zhang, Q. Wang, A. Yang, N. Lu, H. Jiang, D. Chen, Y. Yu, and Y. Wang, "Geometric and dosimetric evaluation of the automatic delineation of organs at risk (oars) in non-small-cell lung cancer radiotherapy based on a modified densenet deep learning network," Frontiers in Oncology, vol. 12, p. 861857, 2022.

Fast and low-gpu-memory abdomen ct organ segmentation: the flare challenge. J Ma, Y Zhang, S Gu, X An, Z Wang, C Ge, C Wang, F Zhang, Y Wang, Y Xu, Medical Image Analysis. 82102616J. Ma, Y. Zhang, S. Gu, X. An, Z. Wang, C. Ge, C. Wang, F. Zhang, Y. Wang, Y. Xu et al., "Fast and low-gpu-memory abdomen ct organ segmentation: the flare challenge," Medical Image Analysis, vol. 82, p. 102616, 2022.

3d u-japa-net: mixture of convolutional networks for abdominal multi-organ ct segmentation. H Kakeya, T Okada, Y Oshiro, Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference. Granada, Spain; Cham, SwitzerlandSpringerProceedings, Part IV 11H. Kakeya, T. Okada, and Y. Oshiro, "3d u-japa-net: mixture of convo- lutional networks for abdominal multi-organ ct segmentation," in Med- ical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference, Granada, Spain, September 16- 20, 2018, Proceedings, Part IV 11. Cham, Switzerland: Springer, 2018, pp. 426-433.

Joint segmentation of multiple thoracic organs in ct images with two collaborative deep architectures. R Trullo, C Petitjean, D Nie, D Shen, S Ruan, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th International Workshop. Québec City, QC, CanadaSpringer3Conjunction with MICCAI 2017R. Trullo, C. Petitjean, D. Nie, D. Shen, and S. Ruan, "Joint segmen- tation of multiple thoracic organs in ct images with two collaborative deep architectures," in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third Interna- tional Workshop, DLMIA 2017, and 7th International Workshop, ML- CDS 2017, Held in Conjunction with MICCAI 2017, Québec City, QC, Canada, September 14, Proceedings 3, vol. 10553. Springer, 2017, pp. 21-29.

Cascaded se-resunet for segmentation of thoracic organs at risk. Z Cao, B Yu, B Lei, H Ying, X Zhang, D Z Chen, J Wu, Neurocomputing. 453Z. Cao, B. Yu, B. Lei, H. Ying, X. Zhang, D. Z. Chen, and J. Wu, "Cascaded se-resunet for segmentation of thoracic organs at risk," Neurocomputing, vol. 453, pp. 357-368, 2021.

Automatic segmentation of head-neck organs by multi-mode cnns for radiation therapy. Q Yang, S Zhang, X Sun, J Sun, K Yuan, 2019 International Conference on Medical Imaging Physics and Engineering (ICMIPE). Shenzhen, ChinaIEEEQ. Yang, S. Zhang, X. Sun, J. Sun, and K. Yuan, "Automatic segmen- tation of head-neck organs by multi-mode cnns for radiation therapy," in 2019 International Conference on Medical Imaging Physics and Engineering (ICMIPE). Shenzhen, China: IEEE, 2019, pp. 1-5.

Head and neck cancer patient images for determining autosegmentation accuracy in t2-weighted magnetic resonance imaging through expert manual segmentations. C E Cardenas, A S Mohamed, J Yang, M Gooding, H Veeraraghavan, J Kalpathy-Cramer, S P Ng, Y Ding, J Wang, S Y Lai, Medical physics. 475C. E. Cardenas, A. S. Mohamed, J. Yang, M. Gooding, H. Veer- araghavan, J. Kalpathy-Cramer, S. P. Ng, Y. Ding, J. Wang, S. Y. Lai et al., "Head and neck cancer patient images for determining auto- segmentation accuracy in t2-weighted magnetic resonance imaging through expert manual segmentations," Medical physics, vol. 47, no. 5, pp. 2317-2322, 2020.

Cloud-based evaluation of anatomical structure segmentation and landmark detection algorithms: Visceral anatomy benchmarks. O Jimenez-Del Toro, H Müller, M Krenn, K Gruenberg, A A Taha, M Winterstein, I Eggel, A Foncubierta-Rodríguez, O Goksel, A Jakab, IEEE transactions on medical imaging. 3511O. Jimenez-del Toro, H. Müller, M. Krenn, K. Gruenberg, A. A. Taha, M. Winterstein, I. Eggel, A. Foncubierta-Rodríguez, O. Goksel, A. Jakab et al., "Cloud-based evaluation of anatomical structure segmentation and landmark detection algorithms: Visceral anatomy benchmarks," IEEE transactions on medical imaging, vol. 35, no. 11, pp. 2459-2475, 2016.