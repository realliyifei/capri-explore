title,section,section_title,paragraph,num_reference,answer,QA_pair_rephrased-v0,QA_pair_rephrased-v0.5,QA_pair_rephrased-v1,QA_pair_rephrased-v2,QA_pair_rephrased-v3
A Primer in BERTology: What we know about how BERT works,s1,BERT embeddings,"['p1.0', 'p1.1']",4,"Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

BERT's embeddings are contextualized, meaning that the representation of each input token varies depending on its specific context of occurrence, unlike conventional static embeddings which assign a fixed vector to each word regardless of context (Mikolov et al., 2013a; Pennington et al., 2014). BERT's contextualized embeddings are the output vectors from a given Transformer layer, often the final one. These embeddings form distinct clusters that correspond to different word senses, supporting the distributional hypothesis. However, the representation of the same word can change based on its position within a sentence, influenced by BERT's Next Sentence Prediction (NSP) objective. Studies have shown that as one moves to higher layers in BERT, the embeddings become more context-specific. Additionally, BERT embeddings tend to occupy a narrow cone in vector space, meaning that random words are more likely to have a higher cosine similarity than if the embeddings were uniformly distributed across all directions, an effect that becomes more pronounced in higher layers of the model (Wiedemann et al., 2019; Mickus et al., 2019; Ethayarajh, 2019).","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

BERT's embeddings are contextualized, meaning that the representation of each input token varies depending on its specific context, unlike conventional static embeddings which assign a fixed vector to each token regardless of context (Mikolov et al., 2013a; Pennington et al., 2014). Studies on BERT have shown that its contextualized embeddings can form distinct clusters that correspond to different word senses, supporting the distributional hypothesis for these representations (Wiedemann et al., 2019). However, the representation of the same word can change based on its position within a sentence, influenced by BERT's Next Sentence Prediction (NSP) objective (Mickus et al., 2019). Further analysis reveals that as one moves from lower to higher layers in BERT, the embeddings for identical words become more context-specific and tend to occupy a narrower cone in the vector space, leading to a higher average cosine similarity between random words than would be expected in a directionally uniform (isotropic) embedding space (Ethayarajh, 2019).","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

BERT's embeddings are contextualized, meaning the representation of each input token varies depending on its specific context of occurrence, unlike conventional static embeddings which assign a fixed vector to each word regardless of its context. Studies have shown that BERT's contextualized embeddings can form distinct clusters corresponding to different word senses, indicating that word representations vary not only by context but also by their position in a sentence and the specific layer of BERT from which they are derived. This results in more context-specific representations in later layers, with embeddings tending to occupy a narrow cone in the vector space, leading to higher average cosine similarities between random words than would be expected in a directionally uniform (isotropic) embedding space.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

BERT's embeddings are contextualized, meaning the representation of each input token varies depending on its context of occurrence, unlike conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014). Studies such as Wiedemann et al. (2019) have shown that BERT's contextualized embeddings form distinct clusters corresponding to different word senses, supporting the distributional hypothesis. However, Mickus et al. (2019) observed that the representation of the same word changes based on its position in a sentence, likely influenced by the NSP objective. Ethayarajh (2019) found that in BERT, identical words have more context-specific representations in later layers, and the embeddings tend to occupy a narrow cone in the vector space, becoming less directionally uniform (isotropic) in higher layers, indicating a higher average cosine similarity between two random words than would be expected in a uniform distribution.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

BERT's embeddings are contextualized, meaning that the representation of each input token varies depending on its specific context of occurrence, unlike conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014). Studies such as Wiedemann et al. (2019) have shown that BERT's contextualized embeddings form distinct clusters corresponding to different word senses, supporting the distributional hypothesis for these representations. Mickus et al. (2019) observed that the representation of the same word changes based on its position in a sentence, which is attributed to BERT's Next Sentence Prediction (NSP) objective. Ethayarajh (2019) found that embeddings for identical words become more context-specific in later layers of BERT and that these embeddings tend to occupy a narrow cone in the vector space, indicating that random words have a higher average cosine similarity than would be expected in a directionally uniform (isotropic) space. This effect intensifies from lower to higher layers, suggesting that BERT's representations become increasingly refined and context-dependent as they move through the model."
A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,s7,Distant & Weak Supervision,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4']",16,"In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

Distant supervision in NLP tasks utilizes unlabeled text without modification and generates labels through a semi-automatic process from external sources, such as knowledge bases, gazetteers, and dictionaries. This method, introduced for relation extraction and extended to multi-instance and multi-label learning, is favored for tasks like named entity recognition (NER) and relation extraction (RE) due to the availability of auxiliary data like entity lists, which allows for reasonable results using simple rules. However, its application is less common in other NLP areas, though it has shown potential in tasks like discourse-structure dataset creation and topic classification, where it leverages heuristics and inputs from other classifiers. The effectiveness of distant supervision depends on the availability of high-quality auxiliary data, which poses a challenge in low-resource settings due to the scarcity of comprehensive and error-free resources. Additionally, while distant supervision can accelerate the labeling process, it still necessitates human involvement for creating annotation techniques and labeling rules, requiring a balance between manual annotation efforts and the use of automatic methods. This approach's reliance on external data and human input highlights the need for further exploration into its suitability for various NLP tasks and its efficiency in different resource settings.","Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

Distant supervision in NLP tasks utilizes unlabeled text without modification and generates labels through a semi-automatic process from external sources, such as knowledge bases, gazetteers, and dictionaries. This method, introduced by Mintz et al. (2009) for relation extraction and further developed for multi-instance and multi-label learning, is effective for tasks like named entity recognition (NER) and relation extraction (RE) by matching text tokens with entities from external lists. However, its application beyond NER and RE is less common, though there are instances of its use in building datasets for discourse structure and topic classification through heuristics and pretrained language models.

The limitations of distant supervision include its heavy reliance on the availability of auxiliary data, which poses challenges in low-resource settings due to the scarcity of high-coverage, error-free dictionaries. This reliance can lead to significant performance gaps between high-resource and low-resource languages, as demonstrated by Kann et al. (2020) in the context of POS tagging. Additionally, while distant supervision can accelerate the labeling process, it still necessitates human involvement in creating annotation techniques or defining labeling rules, which requires time and effort that could alternatively be used for manual annotation or active learning approaches. This aspect underscores the importance of evaluating distant supervision methods in realistic settings and highlights the potential for integrating human-computer interaction and human-in-the-loop setups to improve the process.","Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

Distant supervision, a method that uses unlabeled text and automatically assigns labels through an external source, is popular for tasks like named entity recognition (NER) and relation extraction (RE). This approach relies on external information from knowledge bases, gazetteers, dictionaries, and other structured sources to automatically annotate data, ranging from simple string matching to more complex pipelines. While effective for information extraction, its application in other NLP areas is less common but growing, with uses in discourse-structure dataset creation and topic classification among others. However, distant supervision's reliance on auxiliary data presents challenges in low-resource settings, where such data may be scarce or of low quality, highlighting a performance gap between high-resource and low-resource languages. Additionally, despite its efficiency over manual annotation, distant supervision still requires human effort to develop annotation techniques or labeling rules, raising questions about the comparative advantage of this approach versus direct annotation or active learning methods.","Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

Distant supervision, a method that uses unlabeled text and automatically assigns labels through a semi-automatic process from an external source, is popular for tasks like named entity recognition (NER) and relation extraction (RE). This approach relies on external information sources such as knowledge bases, gazetteers, and dictionaries for automatic annotation, which can range from simple string matching to more complex pipelines. While it has shown success in information extraction tasks, its application in other NLP areas is less prevalent but growing, with uses in building datasets and classification tasks by leveraging auxiliary data like entity lists and heuristics. However, distant supervision faces challenges, particularly in low-resource settings where auxiliary data may be scarce, highlighting a performance gap between high-resource and low-resource languages. Additionally, despite its efficiency in generating labeled data, distant supervision still requires human effort to create annotation techniques or provide labeling rules, raising questions about its comparative advantage over direct manual annotation or active learning approaches.","Questions:

What are the limitations and requirements of distant supervision in NLP tasks?

Answer:

Distant supervision, a method that uses unlabeled text and automatically assigns labels through a semi-automatic process from an external source, is primarily utilized for information extraction tasks like Named Entity Recognition (NER) and Relation Extraction (RE). This approach leverages external information from knowledge bases, gazetteers, dictionaries, and other structured knowledge sources to automatically annotate text, ranging from simple string matching to more complex pipelines that may include classifiers and manual steps. While it has shown success in areas like NER and RE, its application in other NLP tasks is less prevalent, though there are instances where it has been used effectively, such as in building discourse-structure datasets or for topic classification through heuristics combined with inputs from other classifiers.

One of the main challenges of distant supervision is its heavy reliance on auxiliary data, which can be particularly problematic in low-resource settings where such data may be scarce or of low quality. This reliance can result in a significant performance gap between high-resource and low-resource languages, as seen in tasks like POS tagging. Additionally, despite the efficiency gains from using distant supervision over manual annotation, the process still requires human effort to create automatic annotation techniques or to provide labeling rules, without clear data on the time investment required. This aspect highlights the need for further research into making distant supervision more accessible and effective across a wider range of languages and tasks, and into better integrating human expertise into the process."
A Survey on Stance Detection for Mis-and Disinformation Identification,s2,Source(s) Target,"['p2.0', 'p2.1', 'p2.2', 'p2.3']",5,"Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection involves identifying the author's position towards a specific proposition or target, which can be categorized as Favor, Against, Neither, or occasionally Neutral. This process is not only influenced by the author's personal opinions but also by external factors such as cultural norms and societal roles. Unlike stance detection, biased language detection focuses on identifying a text's inclination towards a particular viewpoint. Emotion recognition aims to identify specific emotions expressed in the text, such as love or anger. Perspective identification seeks to determine the author's viewpoint, such as political affiliations, with an explicit target. Sarcasm detection targets satirical or ironic texts intended for ridicule or mockery. Sentiment analysis evaluates the overall polarity of the text, whether positive, negative, or neutral.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection involves identifying the author's position towards a specific proposition or target, which can be categorized as Favor, Against, Neither, or occasionally Neutral. This process is distinct from related NLP tasks in several ways. Unlike biased language detection, which seeks to identify a text's leaning towards a particular viewpoint, stance detection categorizes the author's explicit position. It differs from emotion recognition, which focuses on identifying emotions like love or anger in the text. Perspective identification aims to determine the author's viewpoint (e.g., Democrat vs. Republican) with an always explicit target, setting it apart from stance detection where the target may not be explicitly mentioned. Sarcasm detection targets satirical or ironic texts intended for ridicule, and sentiment analysis evaluates the text's overall polarity, neither of which directly align with identifying the author's stance as in stance detection.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection involves identifying the author's position towards a specific target within a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This task differs from related NLP tasks in several ways: biased language detection focuses on identifying a text's inclination towards a particular perspective; emotion recognition aims to identify specific emotions like love or anger within the text; perspective identification seeks to determine the author's point of view, such as political affiliation, with an explicitly mentioned target; sarcasm detection identifies satirical or ironic texts intended for ridicule or mockery; and sentiment analysis evaluates the overall polarity of the text. Unlike these tasks, stance detection specifically categorizes the author's position regarding a predefined target, which may not be explicitly mentioned in the text.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection involves identifying the author's position towards a specific proposition or target within a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This task is distinct from related NLP tasks in several ways: biased language detection focuses on identifying a text's inclination towards a particular viewpoint; emotion recognition aims to identify specific emotions expressed in the text; perspective identification seeks to determine the author's point of view, such as political affiliation, with an explicit target; sarcasm detection identifies text written in a satirical or ironic manner, often for ridicule; and sentiment analysis evaluates the overall polarity of the text, whether positive, negative, or neutral.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection involves identifying the author's position towards a specific proposition or target within a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This task is distinct from related NLP tasks in several ways: biased language detection focuses on identifying a text's inclination towards a particular perspective; emotion recognition aims to identify specific emotions like love or anger expressed in the text; perspective identification seeks to determine the author's point of view, such as political affiliation, with an explicitly mentioned target; sarcasm detection targets texts written with satirical or ironic intent, often for ridicule; and sentiment analysis evaluates the overall polarity of the text—positive, negative, or neutral."
A Survey of Data Augmentation Approaches for NLP,s1,Background,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5']",6,"What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

Data augmentation (DA) in natural language processing (NLP) involves methods to increase the diversity of training data without the need for additional data collection. This is achieved through modifications of existing data or the creation of synthetic data, with the goal of acting as a regularizer to mitigate overfitting in machine learning model training. While DA is more straightforward in computer vision (CV) with techniques like cropping and flipping, NLP presents unique challenges due to its discrete input space, making it less clear how to generate effective augmented examples that maintain desired invariances.

The primary challenges in applying DA techniques in NLP include the difficulty of generating text that captures the necessary invariances and the balance between making augmented data neither too similar nor too divergent from the original data to avoid overfitting or underperformance. Effective DA in NLP must navigate trade-offs between ease of implementation and the potential for performance improvement. Rule-based techniques, while simpler to implement, often yield only incremental improvements. In contrast, model-based approaches can introduce greater data variation and potentially offer significant performance boosts but may be more complex and costly to develop.

Moreover, the distribution of augmented data is crucial; it should be balanced to ensure it is representative of the domain without leading to overfitting or poor generalization. Some researchers have proposed methods like a KL-Divergence-based unsupervised procedure to select among DA heuristics efficiently, addressing the cost and time-intensive nature of comparing multiple heuristics.

Despite its widespread use, there is a noted lack of deep understanding of the theoretical principles underpinning DA's effectiveness. Some studies suggest that training with noised examples can be akin to Tikhonov regularization, and DA can improve classifier margins or reduce variance through specific transformations, indicating that DA's benefits might stem from averaging features and variance regularization. However, the theoretical foundations and principles of DA in NLP remain an area requiring further exploration.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

Data augmentation (DA) in natural language processing (NLP) involves increasing the diversity of training data without the need for additional data collection. This is achieved through methods that either modify existing data slightly or generate new, synthetic data. The primary goal is to use augmented data as a regularizer to help reduce overfitting in machine learning models. While DA is well-established in computer vision (CV) with techniques like cropping and flipping, applying DA in NLP presents unique challenges due to the discrete nature of text. This requires generating effective augmented examples that maintain the desired invariances.

Several DA techniques for NLP have been proposed, ranging from simple rule-based manipulations to more complex generative approaches. These techniques aim to balance ease of implementation with the potential to improve model performance. Rule-based methods are straightforward but tend to offer only incremental improvements, whereas model-based techniques, though potentially more costly to implement, can introduce a greater variety of data and achieve significant performance enhancements. However, developing and utilizing model-based techniques tailored for specific downstream tasks can be challenging.

An ideal DA approach should ensure that the distribution of augmented data is neither too similar nor too divergent from the original data to avoid increased overfitting or poor performance due to non-representative training examples. Achieving this balance is crucial for effective DA. Additionally, there is a need for a better understanding of DA's theoretical foundations, as current interpretations, such as viewing DA as a form of regularization, may be insufficient. Research into the theoretical aspects of DA, including its role in variance reduction and feature averaging, is ongoing but still at a relatively early stage.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

Data augmentation (DA) in NLP involves generating new training examples by modifying existing data or creating synthetic data to increase diversity and act as a regularizer, aiming to reduce overfitting. Unlike in computer vision, where augmentation techniques like cropping and flipping are straightforward, applying DA in NLP is challenging due to the discrete nature of text. The goals of DA are to enhance model performance without the need for additional data collection, balancing ease of implementation with effectiveness. Rule-based DA techniques are simple to implement but may offer limited performance gains, while generative model-based approaches can introduce significant data variation and potentially lead to greater improvements at the cost of higher implementation complexity. An effective DA strategy should ensure the augmented data's distribution is neither too similar nor too divergent from the original data to avoid overfitting or training on non-representative examples. Additionally, there's a need for a deeper understanding of DA's theoretical foundations, as current explanations, such as its role in regularization and variance reduction, may not fully capture why DA is effective.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

Data augmentation (DA) in NLP involves generating new training examples either by slightly modifying existing data or creating synthetic data to increase data diversity and act as a regularizer to reduce overfitting. While DA is well-established in computer vision with techniques like cropping and flipping, its application in NLP is more challenging due to the discrete nature of text. The goals of DA in NLP include improving model performance without the need for additional data collection, balancing ease of implementation with the effectiveness of the augmentation. Rule-based DA techniques are straightforward but may offer limited performance gains, whereas model-based approaches can introduce greater data variation and potentially lead to significant improvements but are often more complex to implement. An effective DA strategy should ensure the augmented data's distribution is neither too similar nor too divergent from the original data to avoid overfitting or training on non-representative examples. Kashefi and Hwa (2020) suggest using a KL-Divergence-based unsupervised procedure to select DA heuristics efficiently. Despite the practical application of DA, there is a noted lack of deep understanding of its theoretical foundations, with some research suggesting DA can be seen as a form of regularization, increasing classifiers' positive margin, or reducing variance through averaging over data transformations (Dao et al., 2019; Rajput et al., 2019; Chen et al., 2020d).","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

Data augmentation (DA) in NLP involves increasing training data diversity through slightly modified copies of existing data or synthetic data creation, aiming to act as a regularizer and reduce overfitting. Unlike in computer vision, where augmentation techniques like cropping and flipping are straightforward due to the continuous nature of the input space, generating effective augmented examples in the discrete input space of NLP presents unique challenges. The primary goals of DA are to enhance model performance without the need for additional data collection, balancing ease of implementation with effectiveness. Rule-based techniques, while simpler to implement, often yield only incremental improvements. In contrast, generative model-based approaches, though potentially more resource-intensive, can introduce greater data variation and significant performance enhancements. However, it's crucial that the distribution of augmented data closely aligns with the original data to avoid overfitting or training on non-representative examples. Kashefi and Hwa (2020) proposed a KL-Divergence-based unsupervised method to select DA heuristics efficiently, avoiding the time and cost associated with exhaustive heuristic comparisons. Despite the practical applications of DA, there's a noted lack of deep understanding of its theoretical foundations. Some researchers argue that DA's typical characterization as a form of regularization is overly simplistic, with studies suggesting it can also increase classifiers' positive margins, average features, and regularize variance, contributing to variance reduction in model training."
Towards Argument Mining for Social Good: A Survey,s2,Framework,"['p2.0', 'p2.1', 'p2.2', 'p2.3', 'p2.4', 'p2.5', 'p2.6', 'p2.7', 'p2.8', 'p2.9', 'p2.10', 'p2.11', 'p2.12']",11,"Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

A1: Marvel Universe is better than DC Universe.

A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

A4: This is especially true due to his unfortunate passing.

A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Cabrio and Villata (2018) provide a detailed overview of the argument mining (AM) framework, highlighting its implementation in two main stages. The first stage, argument extraction, involves identifying documents with argumentative structures and pinpointing the specific textual spans where argumentation occurs. This includes defining the textual boundaries and assigning subportions of these spans to predefined argument components such as claims, premises, and rebuttals. Various models, including Näive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised learning techniques (Ein-Dor et al., 2020), have been employed for this purpose.

The second stage, relation assignment, aims to model the relations between the argumentative spans identified in the first stage. This involves classifying relations that can either support or attack between different arguments, or connect premises with the claim within an argument. Recent methods for classifying these argumentative relations include relational models (Trautmann et al., 2020) and leveraging features from various knowledge bases (Kobbe et al., 2019) to inject background knowledge. Once relations are identified, the overall argumentation structure can be represented as an argumentation graph, with arguments as nodes and relations as edges. Some approaches simplify this graph into a tree structure (Peldszus and Stede, 2015; Stab and Gurevych, 2017), and different methods like SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) and textual entailment (Cabrio and Villata, 2013; Cocarascu et al., 2020) have been explored to generate this structure. Modeling these relations and the flow of argumentation is crucial for defining argument quality.

Examples from online debates illustrate how the framework operates in practice, showing the process of determining argumentative text boundaries, extracting argument components, and assigning relations between arguments. The examples demonstrate the framework's application in both structured environments, like persuasive essays, where the language and debate structure are more standardized, and in more conversational settings, where boundaries and argument components can be less defined. The effectiveness of the AM framework and the complexity of the components and relations extracted vary significantly based on the environment and platform, impacting the system's ability to implement the framework.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Cabrio and Villata (2018) provide a detailed analysis of the argument mining (AM) framework, highlighting its implementation in two main stages. The first stage, argument extraction, involves identifying documents with argumentative structures and pinpointing the specific textual spans where argumentation occurs. This includes defining the boundaries of these spans and assigning subportions to predefined argument components such as claims, premises, and rebuttals, using various models like Näive Bayes, SVMs, RNNs, Pre-trained Language Models, and other supervised-learning techniques. The second stage, relation assignment, focuses on modeling the relations between the identified argumentative spans, which can be between different arguments (support or attack) or within an argument (connecting premises with the claim). This stage involves classifying these relations using approaches like relational models or leveraging features from knowledge bases to inject background knowledge. The complexity of argumentation structure is addressed by mapping it to an argumentation graph or simplifying it to a tree-structure representation, with methods such as SVMs or textual entailment used to generate this structure. This framework is crucial for understanding the flow of argumentation within debates and assessing argument quality. The implementation of this framework varies depending on the environment and platform, with different challenges arising from the nature of the text and the dynamics of the argumentation, ranging from structured debates to more conversational exchanges.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

The argument mining (AM) framework involves two main stages: argument extraction and relation assignment. In the argument extraction stage, the system identifies documents containing argumentative structures and specifies the textual spans where argumentation is encoded. Subportions of these spans are then categorized into pre-established argument components such as claims, premises, and rebuttals, using models like Näive Bayes, SVMs, RNNs, Pre-trained Language Models, and other supervised-learning techniques.

The second stage, relation assignment, focuses on modeling the relations between the argumentative spans identified earlier. These relations can be supportive or attacking between different arguments or connect premises with claims within an argument. Approaches to classify these relations include relational models and leveraging features from different knowledge bases. The overall structure of the argumentation is then modeled, often represented as an argumentation graph with arguments as nodes and relations as edges. Some methods simplify this graph into a tree-structure for ease of analysis.

The framework is applied to various types of argumentative texts, from structured debates to more conversational exchanges on online platforms. The complexity of implementing the AM framework varies depending on the environment and platform, with more standardized language use and structured debates offering a clearer path to argument extraction and relation assignment than conversational or less structured environments.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

The argument mining (AM) framework, as outlined by Cabrio and Villata (2018), is implemented in two main stages: argument extraction and relation assignment. In the first stage, the system identifies documents containing argumentative structures and specifies the textual spans where argumentation occurs. Subportions of these spans are then categorized into pre-established argument components such as claims, premises, and rebuttals, using models like Näive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

The second stage focuses on modeling the relations between the identified argumentative spans, which can be supportive or attacking between different arguments, or connective within an argument. This involves high-level knowledge representation and reasoning, with recent approaches exploring relational models (Trautmann et al., 2020) and leveraging features from knowledge bases (Kobbe et al., 2019). The overall argumentation structure is then represented as an argumentation graph, with arguments as nodes and relations as edges. Some methods simplify this graph into a tree structure (Peldszus and Stede, 2015; Stab and Gurevych, 2017) and employ SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013; Cocarascu et al., 2020) for structure generation.

This framework is applicable to various types of argumentative texts, from structured persuasive essays to more conversational exchanges on platforms like Kialo, demonstrating the adaptability of AM systems to different environments and the complexity of argument components and relations.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

The argument mining (AM) framework, as outlined by Cabrio and Villata (2018), involves two main stages: argument extraction and relation assignment. In the first stage, the system identifies documents containing argumentative structures and specifies the textual spans where argumentation occurs. Subportions of these spans are then categorized into pre-established argument components such as claims, premises, and rebuttals, using various models like Näive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

The second stage focuses on modeling the relations between the identified argumentative spans, which can be supportive or attacking, and may connect premises with claims within an argument. This involves using relational models (Trautmann et al., 2020) or leveraging features from different knowledge bases (Kobbe et al., 2019) to classify argumentative relations. The overall structure of the argumentation is then represented as an argumentation graph, with arguments as nodes and relations as edges. Some approaches simplify this graph into a tree-structure for ease of analysis (Peldszus and Stede, 2015; Stab and Gurevych, 2017), using methods like SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013; Cocarascu et al., 2020).

The framework's application varies depending on the environment and platform, from structured debates to more conversational dynamics seen in online forums. The complexity of extracting, assigning, and predicting components and relations is influenced by the nature of the text, whether it is overtly argumentative like in persuasive essays or more informal as in online discussions."
A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,s2,"Code-switching, Borrowing, Transfer, Loan Translation","['p2.0', 'p2.1', 'p2.2']",5,"While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009). 4.İlkokul-uİstanbul-da yap-tı-m.

primary.school-ACCİstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher après 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

Code-switching involves an active alternation between different grammatical systems, whereas borrowing typically does not entail such alternation. It is challenging to distinguish between a single word insertion being a case of borrowing or code-switching without analyzing how these elements integrate into the grammar of the recipient language. Most instances of lone-item insertions are classified as nonce borrowings, which are one-time borrowings. Complex code-switching might not always be recognized as switching; it can range from being pragmatic and intentional to serving no specific pragmatic purpose, or even forming the single linguistic code of a community, such as in the cases of Michif Cree or Gurinji Kriol, which may appear structurally similar to code-switching but are considered a single language. Surzhyk, spoken by Russian-Ukrainian bilinguals, exemplifies a linguistic phenomenon that blurs the lines between code-switching and mixed languages due to the high bilingual proficiency of its speakers and the indeterminate direction of switching. Loan translation and transfer involve adopting the semantics and grammatical structures of one language while using the lexicon of another. For instance, in Turkish spoken in the Netherlands, the verb ""yapmak"" adopts the Dutch meaning similar to ""doen,"" indicating the completion of primary school in Istanbul. Similarly, grammatical constructions, such as verb particles, can be transferred from one language to another without borrowing the actual words, as seen in the adoption of Germanic verb particles into French spoken in Brussels and Ontario, where phrases translate directly from Dutch and English equivalents, respectively, diverging from standard French usage in France.","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

Code-switching involves an active alternation between different grammatical systems, whereas borrowing typically involves the integration of words from one language into the grammar of another without such alternation. Lone word insertions are often considered nonce borrowings, which are one-time borrowings, unless analyzed within the grammar of the receiving language (Poplack et al., 1988; Sankoff et al., 1990). Complex code-switching may not always be recognized as switching; it can range from pragmatic and intentional code-switching to language mixing, which lacks a pragmatic purpose, and even to mixed languages that function as the single code of a community but may appear structurally similar to code-switching (Auer, 1999; Bakker, 1997; Meakins, 2012). Surzhyk, for example, represents a linguistic phenomenon that blurs the lines between code-switching and mixed languages among highly bilingual Russian-Ukrainian speakers in Ukraine, making the direction of switching indeterminate (Bilaniuk, 2004). Loan translation and transfer involve adopting the semantics and grammatical constructions of one language while using the words of another. This can be seen in the adaptation of the Turkish verb ""yapmak"" to carry the Dutch meaning of ""doen"" among Turkish speakers in the Netherlands (Dogruöz and Backus, 2009), and in the transfer of verb particles from Germanic languages into French, as evidenced by certain constructions in Brussels French and Ontario French that are not found in standard French (Treffers-Daller, 2012).","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

Code-switching involves actively alternating between grammatical systems, whereas borrowing does not necessarily involve such alternation. Lone word insertions can be difficult to classify as either borrowing or code-switching without analyzing how they integrate into the grammar of the receiving language. Most lone-item insertions are considered nonce borrowings. Complex code-switching may not always be perceived as switching; for example, mixed languages like Michif Cree or Gurinji Kriol, which are the single code of a community, may structurally resemble code-switching but are modeled as a single language. Surzhyk, spoken by urban Russian-Ukrainian bilinguals, blurs the lines between code-switching and mixed languages due to high levels of bilingualism and indeterminate switching direction. Loan translation and transfer involve adopting semantics and grammatical constructions from one language while using words from another, as seen in the Turkish spoken in the Netherlands where the verb ""yapmak"" adopts the Dutch meaning of ""doen."" Transfer can also occur without borrowing words, such as the transfer of verb particles from Germanic languages into French, leading to constructions like ""chercher après"" in Brussels French, which mirrors the Dutch equivalent, and ""chercher pour"" in Ontario French, reflecting the English ""look for.""","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

Code-switching involves actively alternating between grammatical systems, whereas borrowing does not necessarily involve such alternation. It's challenging to classify if a single word insertion is a case of borrowing or code-switching without analyzing how it integrates into the grammar of the receiving language (Poplack et al., 1988). Most lone-item insertions are typically considered nonce borrowings, which are one-time borrowings (Sankoff et al., 1990). On the other hand, what may appear as complex code-switching might not be perceived as switching at all. Auer (1999) outlines a continuum from prototypical code-switching, which is pragmatic and intentional, through language mixing, which serves no pragmatic purpose, to mixed languages, which are the sole code of a community and can be modeled as a single language, such as Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012). Surzhyk, spoken by urban Russian-Ukrainian bilinguals in Ukraine, is described by Bilaniuk (2004) as lying between code-switching and a mixed language due to the high bilingual proficiency of speakers and the indeterminate direction of switching. Loan translation and transfer involve adopting the semantics and grammatical constructions of one language while using the lexicon of another. For instance, in Turkish spoken in the Netherlands, the verb ""yapmak"" ('to do') adopts the Dutch meaning of ""doen"" (Dogruöz and Backus, 2009). Transfer can also occur without borrowing words, as shown by Treffers-Daller (2012) with the transfer of verb particles from Germanic languages into French, where constructions like ""chercher après"" in Brussels French and ""chercher pour"" in Ontario French are influenced by Dutch and English, respectively, diverging from the usage in reference French from France.","Questions:

How do code-switching and borrowing differ in linguistic integration?

Answer:

Code-switching involves an active alternation between grammatical systems, whereas borrowing does not necessarily integrate into the grammar of the receiving language in the same way. Most lone-item insertions, upon analysis, are identified as nonce borrowings rather than code-switching, indicating a one-time borrowing event (Sankoff et al., 1990). Complex code-switching may not always be recognized as switching; for instance, Auer (1999) outlines a continuum from prototypical code-switching, which is pragmatic and intentional, to language mixing, which serves no pragmatic purpose, and mixed languages, which are considered the single code of a community and can be modeled as a single language, such as Michif Cree or Gurinji Kriol. Surzhyk, spoken by urban Russian-Ukrainian bilinguals, blurs the lines between code-switching and mixed languages due to the high level of bilingualism and indeterminate direction of switching (Bilaniuk, 2004). Loan translation and transfer involve adopting semantics and grammatical constructions from one language while using words from another, as seen in the Turkish spoken in the Netherlands where the verb ""yapmak"" adopts the Dutch meaning of ""doen"" (Dogruöz and Backus, 2009). Similarly, grammatical constructions can be transferred from one language to another without borrowing the words, exemplified by the transfer of verb particles from Germanic languages into French, affecting variations of French spoken in Brussels and Ontario, which incorporate particles not found in standard French (Treffers-Daller, 2012)."
Measure and Improve Robustness in NLP Models: A Survey,s3,Robustness under Distribution Shift,['p3.0'],6,"Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-jussà, 2019).","Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

NLP models maintain robustness under natural distribution shifts by focusing on data drawn from different, naturally occurring distributions, where robustness is assessed based on the model's ability to perform under these shifts. This approach is distinct from domain adaptation and transfer learning, as it aligns more closely with domain generalization or out-of-distribution generalization, which involves generalizing to unforeseen distribution shifts without the need for adaptation. This means that during training, the model does not have access to the test data, whether labeled or unlabeled. In the NLP context, robustness also encompasses the model's ability to perform consistently despite variations in grammar errors, dialects, speakers, languages, or datasets collected from different domains for the same task. Additionally, fairness in NLP, which addresses issues such as gendered stereotypes or biases in various tasks, is closely related to robustness, highlighting the importance of equitable performance across diverse linguistic and demographic groups.","Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

NLP models maintain robustness under natural distribution shifts by focusing on performance under conditions where data is drawn from different, naturally occurring distributions. This approach is distinct from domain adaptation and transfer learning, emphasizing instead on domain generalization or out-of-distribution generalization, which involves preparing models to generalize to unforeseen distribution shifts without the need for adaptation. This means that during training, models are not exposed to test data, whether labeled or unlabeled. Robustness in the context of NLP also encompasses the ability of models to perform consistently despite variations in grammar errors, dialects, speakers, languages, or datasets collected from different domains for the same task. Additionally, fairness in NLP, which addresses issues like gendered stereotypes or biases in various tasks, is closely related to maintaining robustness, as it involves ensuring equitable performance across diverse NLP applications.","Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

Research on maintaining robustness under natural distribution shifts in NLP models focuses on their performance when faced with data that comes from a different, naturally occurring distribution. This concept is distinct from domain adaptation and transfer learning, aligning more closely with domain generalization or out-of-distribution generalization, where models are expected to generalize to unforeseen distribution shifts without the need for adaptation. This includes ensuring models do not degrade in performance due to variations such as grammar errors, dialects, speakers, languages, or datasets collected from different domains for the same task. Additionally, fairness in NLP, which involves addressing issues like gendered stereotypes or biases across various tasks, is closely related to robustness under distribution shifts.","Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

Research on NLP model robustness under natural distribution shifts focuses on data (x, y) originating from naturally occurring different distributions (Hendrycks et al., 2021). This approach to robustness, distinct from domain adaptation (Patel et al., 2015; Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), aligns more closely with domain generalization (Muandet et al., 2013; Gulrajani and Lopez-Paz, 2021) or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a). Here, the model is expected to generalize without adaptation, as the test data, whether labeled or unlabeled, is not available during training. In NLP, robustness to natural distribution shifts implies that a model's performance should remain stable despite variations in grammar errors, dialects, speakers, languages (Craig and Washington, 2002; Blodgett et al., 2016; Demszky et al., 2021), or even when applied to newly collected datasets for the same task across different domains (Miller et al., 2020). Additionally, fairness in NLP, which includes addressing gendered stereotypes or biases in tasks like co-reference resolution (Zhao et al., 2018a; Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019; Font and Costa-jussà, 2019), is a closely related research area that has been extensively reviewed (Sun et al., 2019).","Questions:

How do NLP models maintain robustness under natural distribution shifts?

Answer:

Research on NLP model robustness under natural distribution shifts focuses on performance when data comes from a different, naturally occurring distribution (Hendrycks et al., 2021). This concept is distinct from domain adaptation (Patel et al., 2015; Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), aligning more closely with domain generalization (Muandet et al., 2013; Gulrajani and Lopez-Paz, 2021) or out-of-distribution generalization (Hendrycks et al., 2020a), where models are expected to generalize to unforeseen distribution shifts without adaptation. In NLP, robustness to natural distribution shifts includes maintaining performance despite variations in grammar errors, dialects, speakers, languages (Craig and Washington, 2002; Blodgett et al., 2016; Demszky et al., 2021), or datasets collected from different domains for the same task (Miller et al., 2020). Additionally, fairness in NLP, which addresses issues like gendered stereotypes or biases in tasks such as co-reference resolution (Zhao et al., 2018a; Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019; Font and Costa-jussà, 2019), is closely related to robustness under distribution shifts."
"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",s9,Semantic Understanding,"['p9.0', 'p9.1', 'p9.2']",7,"Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

Semantic understanding significantly enhances Vision-and-Language Navigation (VLN) by incorporating knowledge about crucial features and employing high-level semantic representations to improve performance in unfamiliar environments. It involves decomposing visual or textual modalities into various features, identifying those that most effectively enhance performance. High-level features such as visual appearance, route structure, and detected objects are more effective than basic visual features extracted by conventional neural networks. Additionally, within instructions, different types of tokens—specifically object and direction tokens—play distinct roles and their extraction and encoding are vital for better navigation outcomes. Furthermore, semantic understanding facilitates the extraction and soft alignment of semantic connections between different modalities, including actions, scenes, observed objects, direction clues, and objects mentioned in instructions, using attention mechanisms. This alignment process not only improves the interaction between modalities but also emphasizes the parts of the instruction that are most relevant to the current navigation step.","Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

Semantic understanding significantly enhances Vision-and-Language Navigation (VLN) by incorporating knowledge of crucial features and employing high-level semantic representations to improve performance in unfamiliar environments. It involves analyzing both visual and textual modalities to identify features that are most beneficial for VLN tasks. Specifically, high-level features like visual appearance, route structure, and detected objects are more effective than basic visual features extracted by conventional neural networks. Additionally, within textual instructions, different tokens play varied roles, with object and direction tokens being particularly important for encoding. Moreover, semantic understanding facilitates the extraction and soft alignment of semantic connections across modalities, including actions, scenes, observed objects, and direction clues mentioned in instructions. This alignment is achieved through attention mechanisms, which also help in emphasizing relevant parts of the instruction in relation to the current navigation step.","Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

Semantic understanding significantly enhances Vision-and-Language Navigation (VLN) tasks by incorporating knowledge about crucial features and employing high-level semantic representations, which are particularly beneficial in unfamiliar environments. It involves the decomposition of visual and textual modalities into various features, identifying those that most effectively improve performance. High-level features such as visual appearance, route structure, and detected objects have been shown to outperform basic visual features extracted by convolutional neural networks (CNNs). Additionally, recognizing the different roles of tokens within instructions and encoding object and direction tokens are essential for improving task performance. Furthermore, semantic connections between different modalities—such as actions, scenes, observed objects, direction clues, and objects mentioned in instructions—are extracted and softly aligned using attention mechanisms. This soft alignment process emphasizes the parts of the instruction relevant to the current step, facilitating more accurate and context-aware navigation decisions.","Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

Semantic understanding significantly enhances Vision-and-Language Navigation (VLN) tasks by incorporating knowledge about critical features within VLN. It leverages both raw and high-level semantic representations to improve performance, especially in unfamiliar environments. In the realm of intra-modality, visual and textual modalities are broken down into numerous features that have varying degrees of importance in VLN. Identifying and utilizing high-level features such as visual appearance, route structure, and detected objects has been shown to outperform basic visual features extracted by CNNs (Hu et al., 2019). Moreover, recognizing the different roles of tokens within instructions and effectively encoding object and direction tokens are essential for enhancing VLN tasks (Qi et al., 2020a; Zhu et al., 2021b).

In terms of inter-modality, establishing semantic connections between different modalities—such as actions, scenes, observed objects, direction clues, and objects mentioned in instructions—facilitates their soft alignment through attention mechanisms (Qi et al., 2020a; Gao et al., 2021). This soft alignment process is crucial for highlighting the parts of the instruction that are most relevant to the current step in the navigation task (Landi et al., 2019; Zhang et al., 2020a).","Questions:

How does semantic understanding enhance Vision-and-Language Navigation (VLN) tasks?

Answer:

Semantic understanding significantly enhances Vision-and-Language Navigation (VLN) tasks by incorporating knowledge about crucial features and employing high-level semantic representations, which are particularly beneficial in unfamiliar environments. It involves the decomposition of visual and textual modalities into various features, recognizing that these features contribute differently to VLN performance. Specifically, high-level features such as visual appearance, route structure, and detected objects have been found to outperform basic visual features extracted by conventional neural networks like CNNs. Moreover, within textual instructions, different types of tokens (e.g., object tokens and direction tokens) serve distinct functions, and their effective extraction and encoding are vital for improving VLN tasks (Qi et al., 2020a; Zhu et al., 2021b).

Furthermore, semantic understanding extends to the inter-modality level, where semantic connections between different modalities—such as actions, scenes, observed objects, direction clues, and objects mentioned in instructions—are extracted. These connections can then be softly aligned using attention mechanisms, which helps in highlighting the parts of the instruction that are most relevant to the current step of navigation. This soft alignment between modalities enhances the model's ability to interpret and act upon complex instructions in dynamic environments, leading to improved performance in VLN tasks."
What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,s2,The Relation Extraction Task,"['p2.0', 'p2.1', 'p2.2']",11,"Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

The primary steps involved in the relation extraction (RE) process from text begin with identifying entities within the text and assigning them types, a process known as Named Entity Recognition (NER) or Mention Detection (MD). Following entity identification, the process may diverge based on different methodologies. One common approach involves two main steps: Relation Identification (RI), which is a binary classification task to determine which entity pairs are in a relation, and Relation Classification (RC), a multi-class classification task to assign a specific type of relation to the identified pairs. Some studies combine RI and RC into a single step by including a ""no-relation"" label, while others focus solely on RC, assuming a relation exists and only determining its type. The RI phase considers relations not included in the RC label set as negative samples, with some studies introducing a ""None-Of-The-Above"" (NOTA) label for relations that exist but are not classified within the given set. The process is further complicated by the fact that not all studies address the entire RE pipeline, with many assuming pre-identified entities due to the complexity of NER/MD. Additionally, the distinction between pipeline and joint model approaches in RE is noted, with pipeline models being traditional but prone to error propagation, and joint models aiming to alleviate this issue, though their evaluation and comparison remain challenging.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

The primary steps involved in the relation extraction (RE) process from text begin with identifying entities within the text, which can be either nominals or named entities. This step is known as Named Entity Recognition (NER) or more broadly, Mention Detection (MD). Following entity identification, the process may diverge based on different methodologies. One common approach involves two main steps: Relation Identification (RI) and Relation Classification (RC). RI entails identifying which entity pairs are in a relation through a binary classification task, often applying heuristics to manage the imbalance between positive and negative samples by filtering out unlikely combinations. RC then involves assigning a specific type of relation to the positively identified samples from RI, sometimes integrating RI and RC into a single step by including a no-relation label or focusing solely on RC by assuming a relation exists and determining its type. The process is complicated by the need to manage relations not covered by the RC label set, with some studies introducing a None-Of-The-Above (NOTA) label for such cases. Despite the conceptual clarity of these steps, actual implementation in research varies, with some studies focusing only on certain steps due to the complexity of the tasks or the assumptions made about the availability of entities. Additionally, there's a discussion on whether a traditional pipeline approach or a joint model for entity and relation extraction is more effective, with recent studies suggesting that separate encoders in the context of modern pretrained language models might offer advantages over joint models. However, the choice between these approaches remains an open question, influenced by the specific requirements of the RE task and the domain of application.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

The primary steps involved in the relation extraction (RE) process from text start with identifying entities and assigning them a type, a process known as Named Entity Recognition (NER) or Mention Detection (MD). Following entity identification, the process may involve Relation Identification (RI), where potential relational pairs are identified through binary classification, and Relation Classification (RC), where a specific type of relation is assigned to these identified pairs. Some studies combine RI and RC into a single step by including a no-relation label, while others focus solely on RC under the assumption that a relation exists and needs to be classified. The distinction between no-relation and None-Of-The-Above (NOTA) labels is important, with NOTA indicating that a relation exists but is not among the predefined types in the dataset. The process can be approached through a traditional pipeline, which may suffer from error propagation between sub-tasks, or through joint models that aim to mitigate this issue by integrating entity and relation extraction. However, the choice between pipeline and joint models remains an open question, with recent studies exploring the effectiveness of both approaches in leveraging modern pretrained language models.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

The relation extraction (RE) process from text involves several key steps, starting with identifying entities in the raw text and assigning them a type, a process known as Named Entity Recognition (NER) or Mention Detection (MD). Following entity identification, the process may involve Relation Identification (RI), where potential entity pairs are identified through a binary classification task to determine if any relation exists between them. Due to the typically unbalanced nature of positive to negative samples, heuristics are often applied to reduce the number of possible combinations. The final step, Relation Classification (RC), involves assigning a specific type of relation to the positively identified samples from the RI phase. Some studies combine RI and RC into a single step by including a no-relation label, while others focus solely on RC, assuming a relation exists and the task is to identify its type. The distinction between no-relation and None-Of-The-Above (NOTA) labels is important, with NOTA indicating a relation exists but its type is not within the considered label set. Despite the outlined pipeline, many RE studies do not address the entire process, often assuming gold entities are given due to the complexity of NER/MD. Recent work has highlighted the lack of ablation studies for steps like NER, and the difficulty in determining the RI setup, which complicates fair evaluation. The traditional RE pipeline approach is prone to error propagation, leading to the proposal of joint entity and relation extraction models as a solution, though it remains an open question which approach is more robust.","Questions:

What are the primary steps involved in the relation extraction process from text?

Answer:

The relation extraction (RE) process from text involves several key steps, starting with identifying entities within the text and assigning them types, a task known as Named Entity Recognition (NER) or Mention Detection (MD). Following entity identification, the process may involve Relation Identification (RI), where potential relational pairs of entities are identified through a binary classification task to determine if any relation exists. This step often employs heuristics to manage the imbalance between positive and negative samples by filtering out unlikely entity pairs. The final step, Relation Classification (RC), involves assigning a specific type of relation to the positively identified pairs from the RI step. This can be a multi-class classification task. Some approaches combine RI and RC into a single step by including a 'no-relation' label, while others focus solely on RC under the assumption that a relation exists and the task is to specify its type. The distinction between 'no-relation' and 'None-Of-The-Above' (NOTA) is important, with NOTA indicating that a relation exists but is not among the predefined types in the RC label set. Despite the conceptual clarity of this pipeline, actual RE studies often focus on subsets of these tasks, with many assuming pre-identified entities due to the complexity of NER/MD. The traditional RE pipeline approach is subject to error propagation from sub-tasks, leading to the exploration of joint entity and relation extraction models as alternatives. However, the effectiveness of joint models versus traditional pipeline approaches remains an open question, especially with the integration of modern pretrained language models, which may alter the performance dynamics between these approaches."
A Survey of Active Learning for Natural Language Processing,s3,Output Uncertainty,"['p3.0', 'p3.1']",11,"Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

The primary strategies for uncertainty sampling in active learning for natural language processing (NLP) include entropy-based, least-confidence, and margin-sampling strategies. These strategies are favored for their ability to identify instances where the model's predictions are most uncertain. Entropy-based sampling, introduced by Shannon (1948), focuses on selecting instances based on the unpredictability of their outcomes. Least-confidence sampling, proposed by Culotta and McCallum (2005), prioritizes instances for which the model has the lowest confidence in its prediction. Margin-sampling, discussed by Scheffer et al. (2001) and Schein and Ungar (2007), selects instances based on the smallest difference between the model's two most probable predictions. These strategies have been applied to both probabilistic and non-probabilistic models, with non-probabilistic models often selecting instances close to the decision boundary, such as in support vector machines (SVMs), as noted by Schohn and Cohn (2000) and Tong and Koller (2001). Additionally, recent approaches to measuring output uncertainty involve examining an instance's local divergence through methods like nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020), which assess how model predictions vary within an instance's local region, especially when near the decision boundary.","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

The primary strategies for uncertainty sampling in active learning for natural language processing (NLP) include several approaches. The most straightforward and widely used method is uncertainty sampling, which selects instances that the model finds most uncertain. Within this framework, for probabilistic models, the key strategies are entropy-based sampling (Shannon, 1948), least-confidence sampling (Culotta and McCallum, 2005), and margin-sampling (Scheffer et al., 2001; Schein and Ungar, 2007), as identified by Settles (2009). These strategies focus on choosing instances based on different measures of uncertainty, such as the entropy of the predictions, the confidence level of the model in its prediction, and the margin or difference between the first and second most probable predictions. Schröder et al. (2022) further explored these uncertainty-based strategies using Transformer-based models, specifically for text classification tasks, demonstrating their applicability with modern NLP models. For models that do not output probabilistic predictions, strategies still revolve around uncertainty but are adapted to the model's characteristics, such as selecting instances near the decision boundary in support vector machines (SVMs) (Schohn and Cohn, 2000; Tong and Koller, 2001).

Additionally, measuring output uncertainty can involve examining the divergence in a model's predictions within an instance's local region, especially if the instance is close to the decision boundary. This approach has been expanded through various methods, including nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020), to assess the local divergence of instances. These methods provide alternative ways to identify uncertain instances by evaluating how model predictions vary in response to small changes or perturbations in the input data.","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling is a widely used query strategy in active learning for NLP, focusing on selecting the most uncertain instances as judged by the model's outputs. For probabilistic models, the primary strategies include entropy-based, least-confidence, and margin-sampling methods. These approaches aim to identify instances where the model's predictions are least certain. For non-probabilistic models, strategies such as selecting instances close to the decision boundary in an SVM are employed. Additionally, measuring output uncertainty can involve examining the divergence of a model's predictions within an instance's local region, utilizing methods like nearest-neighbour searches, adversarial perturbation, and data augmentation to assess local divergence and uncertainty.","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling is a key strategy in active learning for NLP, focusing on selecting instances where the model's predictions are most uncertain. This approach is widely used due to its simplicity and effectiveness. For probabilistic models, the main strategies include entropy-based sampling, which relies on the Shannon entropy formula (Shannon, 1948), least-confidence sampling, where instances with the lowest predicted confidence are chosen (Culotta and McCallum, 2005), and margin sampling, which selects instances based on the smallest difference between the probabilities of the two most likely labels (Scheffer et al., 2001; Schein and Ungar, 2007) as outlined by Settles (2009). Recent studies, such as Schröder et al. (2022), have applied these uncertainty-based strategies to Transformer-based models for text classification, demonstrating their applicability to modern NLP models. For non-probabilistic models, techniques like selecting instances near the decision boundary of an SVM have been employed (Schohn and Cohn, 2000; Tong and Koller, 2001). Additionally, measuring output uncertainty by examining an instance's local region divergence—through methods like nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020)—provides alternative ways to identify uncertain instances by assessing how model predictions vary in response to small changes or perturbations in the input data.","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling is a widely used query strategy in active learning for NLP, focusing on selecting the most uncertain instances as judged by the model's outputs. For probabilistic models, the primary strategies include entropy-based sampling, which measures the unpredictability of the model's predictions; least-confidence sampling, which selects instances for which the model has the lowest confidence in its prediction; and margin-sampling, which chooses instances based on the smallest difference between the probabilities of the two most likely predictions. These strategies have been explored and validated in various studies, including those applying Transformer-based models to text classification tasks. For non-probabilistic models, similar principles apply, such as selecting instances close to the decision boundary in support vector machines (SVMs). Additionally, measuring output uncertainty can involve examining the divergence of a model's predictions within an instance's local region, employing methods like nearest-neighbour searches, adversarial perturbation, and data augmentation to identify instances with varied predictions in their vicinity, indicating higher uncertainty."
