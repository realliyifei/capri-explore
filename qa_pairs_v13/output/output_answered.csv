corpusid,title,domain,url,section,section_title,paragraph,para_listed_answer,answer,indexed_answer,filtered_refids,filtered_refids_qualified,num_reference,segmented_answer,question_wholesec_input,question_wholesec,itemized_question_wholesec,QA_pair_wholesec,question_firstpara_input,question_firstpara,itemized_question_firstpara,QA_pair_firstpara,question_firstsenteachpara_input,question_firstsenteachpara,itemized_question_firstsenteachpara,QA_pair_firstsenteachpara,question_firstpara_firstsenteachpara_input,question_firstpara_firstsenteachpara,itemized_question_firstpara_firstsenteachpara,QA_pair_firstpara_firstsenteachpara,answer_rephrase_input,answer_rephrase-v0,QA_pair_rephrased-v0,answer_rephrase-v0.5,QA_pair_rephrased-v0.5,answer_rephrase-v1,QA_pair_rephrased-v1,answer_rephrase-v2,QA_pair_rephrased-v2,answer_rephrase-v3,QA_pair_rephrased-v3
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s1,BERT embeddings,"['p1.0', 'p1.1']","[""Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence."", ""In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).""]","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","[['b22', 'b24'], ['b21', 'b65']]","[['b22', 'b24'], ['b21', 'b65']]",4,"1. Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
2. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
3. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
4. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
5. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
6. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
7. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","A Primer in BERTology: What we know about how BERT works##
BERT embeddings##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).",How do BERT's contextualized embeddings differ from static embeddings in representing word senses?,1. How do BERT's contextualized embeddings differ from static embeddings in representing word senses?,"Questions:

1. How do BERT's contextualized embeddings differ from static embeddings in representing word senses?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","A Primer in BERTology: What we know about how BERT works##
BERT embeddings##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.",How do BERT's embeddings differ from traditional static word embeddings?,1. How do BERT's embeddings differ from traditional static word embeddings?,"Questions:

1. How do BERT's embeddings differ from traditional static word embeddings?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","A Primer in BERTology: What we know about how BERT works##
BERT embeddings##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.",How do BERT's embeddings differ from conventional static embeddings?,1. How do BERT's embeddings differ from conventional static embeddings?,"Questions:

1. How do BERT's embeddings differ from conventional static embeddings?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","A Primer in BERTology: What we know about how BERT works##
BERT embeddings##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.##
In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.",How do BERT's embeddings differ from conventional static embeddings?,1. How do BERT's embeddings differ from conventional static embeddings?,"Questions:

1. How do BERT's embeddings differ from conventional static embeddings?

Answer:

(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","A Primer in BERTology: What we know about how BERT works
##
BERT embeddings
##
Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).
##
How do BERT's embeddings differ from conventional static embeddings?","BERT's embeddings are contextualized, meaning that the representation of each input token varies depending on its specific context of occurrence, unlike conventional static embeddings which assign a fixed vector to each token regardless of context (Mikolov et al., 2013a; Pennington et al., 2014). Studies on BERT's representation space reveal that its contextualized embeddings form distinct clusters that correspond to different word senses, supporting the basic distributional hypothesis. However, the representation of the same word can change based on its position within a sentence, influenced by the Next Sentence Prediction (NSP) objective. Further analysis by Ethayarajh (2019) shows that identical words receive more context-specific representations in later layers of BERT, and these embeddings tend to occupy a narrow cone in the vector space, indicating that random words are more likely to have a higher cosine similarity than if the embeddings were directionally uniform. This effect of increasing specificity and similarity in higher layers contrasts with the static, context-independent embeddings provided by earlier models.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

BERT's embeddings are contextualized, meaning that the representation of each input token varies depending on its specific context of occurrence, unlike conventional static embeddings which assign a fixed vector to each token regardless of context (Mikolov et al., 2013a; Pennington et al., 2014). Studies on BERT's representation space reveal that its contextualized embeddings form distinct clusters that correspond to different word senses, supporting the basic distributional hypothesis. However, the representation of the same word can change based on its position within a sentence, influenced by the Next Sentence Prediction (NSP) objective. Further analysis by Ethayarajh (2019) shows that identical words receive more context-specific representations in later layers of BERT, and these embeddings tend to occupy a narrow cone in the vector space, indicating that random words are more likely to have a higher cosine similarity than if the embeddings were directionally uniform. This effect of increasing specificity and similarity in higher layers contrasts with the static, context-independent embeddings provided by earlier models.","BERT's embeddings are contextualized, meaning that the representation of each input token varies depending on its specific context, unlike conventional static embeddings which assign a fixed vector to each word regardless of context (Mikolov et al., 2013a; Pennington et al., 2014). BERT's contextualized embeddings are capable of forming distinct clusters that correspond to different word senses, demonstrating the validity of the basic distributional hypothesis for these representations (Wiedemann et al., 2019). Furthermore, the position of a word within a sentence influences its representation, a variation attributed to BERT's Next Sentence Prediction (NSP) objective (Mickus et al., 2019). Studies have shown that identical words receive more context-specific representations in later layers of BERT, and these embeddings tend to be closer together in the vector space, forming a narrow cone. This phenomenon, where two randomly chosen words have a higher than expected cosine similarity, becomes more pronounced in higher layers of BERT (Ethayarajh, 2019).","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

BERT's embeddings are contextualized, meaning that the representation of each input token varies depending on its specific context, unlike conventional static embeddings which assign a fixed vector to each word regardless of context (Mikolov et al., 2013a; Pennington et al., 2014). BERT's contextualized embeddings are capable of forming distinct clusters that correspond to different word senses, demonstrating the validity of the basic distributional hypothesis for these representations (Wiedemann et al., 2019). Furthermore, the position of a word within a sentence influences its representation, a variation attributed to BERT's Next Sentence Prediction (NSP) objective (Mickus et al., 2019). Studies have shown that identical words receive more context-specific representations in later layers of BERT, and these embeddings tend to be closer together in the vector space, forming a narrow cone. This phenomenon, where two randomly chosen words have a higher than expected cosine similarity, becomes more pronounced in higher layers of BERT (Ethayarajh, 2019).","BERT's embeddings are contextualized, meaning the representation of each input token varies depending on its context within a sentence, unlike conventional static embeddings where each word has a fixed representation regardless of context. Studies have shown that BERT's contextualized embeddings can form distinct clusters corresponding to different word senses, indicating a strong adherence to the distributional hypothesis. Additionally, the position of a word within a sentence influences its representation, likely due to the Next Sentence Prediction (NSP) objective. Further analysis reveals that in later layers of BERT, embeddings become more context-specific and occupy a narrower cone in the vector space, leading to a higher average cosine similarity between random words than would be expected in a directionally uniform (isotropic) embedding space.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

BERT's embeddings are contextualized, meaning the representation of each input token varies depending on its context within a sentence, unlike conventional static embeddings where each word has a fixed representation regardless of context. Studies have shown that BERT's contextualized embeddings can form distinct clusters corresponding to different word senses, indicating a strong adherence to the distributional hypothesis. Additionally, the position of a word within a sentence influences its representation, likely due to the Next Sentence Prediction (NSP) objective. Further analysis reveals that in later layers of BERT, embeddings become more context-specific and occupy a narrower cone in the vector space, leading to a higher average cosine similarity between random words than would be expected in a directionally uniform (isotropic) embedding space.","BERT's embeddings are contextualized, meaning that the representation of each input token varies depending on its specific context, unlike conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014). Studies such as Wiedemann et al. (2019) have shown that BERT's contextualized embeddings form distinct clusters that correspond to different word senses, supporting the distributional hypothesis. However, Mickus et al. (2019) observed that the representation of the same word changes based on its position in a sentence, which is attributed to the NSP (Next Sentence Prediction) objective of BERT. Ethayarajh (2019) found that in BERT, identical words have embeddings that become more context-specific in higher layers, and that these embeddings tend to occupy a narrow cone in the vector space, leading to a higher average cosine similarity between two random words than would be expected in a directionally uniform (isotropic) space. This effect intensifies from lower to higher layers of BERT.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

BERT's embeddings are contextualized, meaning that the representation of each input token varies depending on its specific context, unlike conventional static embeddings (Mikolov et al., 2013a; Pennington et al., 2014). Studies such as Wiedemann et al. (2019) have shown that BERT's contextualized embeddings form distinct clusters that correspond to different word senses, supporting the distributional hypothesis. However, Mickus et al. (2019) observed that the representation of the same word changes based on its position in a sentence, which is attributed to the NSP (Next Sentence Prediction) objective of BERT. Ethayarajh (2019) found that in BERT, identical words have embeddings that become more context-specific in higher layers, and that these embeddings tend to occupy a narrow cone in the vector space, leading to a higher average cosine similarity between two random words than would be expected in a directionally uniform (isotropic) space. This effect intensifies from lower to higher layers of BERT.","BERT's embeddings are contextualized, meaning that the representation of each input token varies depending on its specific context within a sentence, unlike conventional static embeddings which assign a fixed vector to each word regardless of context (Mikolov et al., 2013a; Pennington et al., 2014). Studies such as Wiedemann et al. (2019) have shown that BERT's contextualized embeddings can form distinct clusters that correspond to different word senses, supporting the distributional hypothesis. However, Mickus et al. (2019) observed that the representation of the same word can change based on its position in a sentence, a likely result of BERT's Next Sentence Prediction (NSP) objective. Ethayarajh (2019) found that in BERT's architecture, identical words have embeddings that become more context-specific in higher layers, and that these embeddings tend to occupy a narrow cone in the vector space, leading to a higher average cosine similarity between two random words than would be expected in a directionally uniform (isotropic) embedding space.","Questions:

How do BERT's embeddings differ from conventional static embeddings?

Answer:

BERT's embeddings are contextualized, meaning that the representation of each input token varies depending on its specific context within a sentence, unlike conventional static embeddings which assign a fixed vector to each word regardless of context (Mikolov et al., 2013a; Pennington et al., 2014). Studies such as Wiedemann et al. (2019) have shown that BERT's contextualized embeddings can form distinct clusters that correspond to different word senses, supporting the distributional hypothesis. However, Mickus et al. (2019) observed that the representation of the same word can change based on its position in a sentence, a likely result of BERT's Next Sentence Prediction (NSP) objective. Ethayarajh (2019) found that in BERT's architecture, identical words have embeddings that become more context-specific in higher layers, and that these embeddings tend to occupy a narrow cone in the vector space, leading to a higher average cosine similarity between two random words than would be expected in a directionally uniform (isotropic) embedding space."
236460206,Towards Argument Mining for Social Good: A Survey,"Political Science, Linguistics, Computer Science",https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,s2,Framework,"['p2.0', 'p2.1', 'p2.2', 'p2.3', 'p2.4', 'p2.5', 'p2.6', 'p2.7', 'p2.8', 'p2.9', 'p2.10', 'p2.11', 'p2.12']","['Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).', 'Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.', 'To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.', 'Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.', 'A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.', ""Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation."", 'A1: Marvel Universe is better than DC Universe.', ""A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses."", 'A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.', 'A4: This is especially true due to his unfortunate passing.', ""A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics."", ""The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise)."", ""Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).""]","Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

A1: Marvel Universe is better than DC Universe.

A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

A4: This is especially true due to his unfortunate passing.

A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","(p2.0) Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

(p2.1) Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

(p2.2) To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

(p2.3) Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

(p2.4) A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

(p2.5) Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

(p2.6) A1: Marvel Universe is better than DC Universe.

(p2.7) A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

(p2.8) A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

(p2.9) A4: This is especially true due to his unfortunate passing.

(p2.10) A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

(p2.11) The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

(p2.12) Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","[['b29', None, 'b28', 'b26'], ['b49', 'b14'], ['b35', 'b44', None, 'b31'], [], [], [], [], [], [], [], [], [], ['b44']]","[['b29', None, 'b28', 'b26'], ['b49', 'b14'], ['b35', 'b44', None, 'b31'], [], [], [], [], [], [], [], [], [], ['b44']]",11,"1. Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM.
2. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.
3. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).
4. A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).Relation assignment
5. The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.
6. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).
7. Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (e.g. claims, premises, rebuttal, etc.)0.
8. Detecting these relations is necessary to model the overall structure of the argumentation (e.g. claims, premises, rebuttal, etc.)1.
9. As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues.
10. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.
11. To simplify the problem, some approaches reduce the graph to a tree-structure representation (e.g. claims, premises, rebuttal, etc.)2.
12. Different methods to generate the structure have been investigated, e.g. SVMs (e.g. claims, premises, rebuttal, etc.)3 or textual entailment (e.g. claims, premises, rebuttal, etc.)4.
13. Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly.
14. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?""
15. (e.g. claims, premises, rebuttal, etc.)5 A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects.
16. Vaccinated people become immune to a certain pathogen and do not develop a disease.
17. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.
18. A2: Many vaccines have serious and sometimes deadly side effects.
19. With many vaccines the immunity is not lifelong.
20. Sometimes the vaccines itself can cause a serious disease to develop as a side effect.
21. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.
22. Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (e.g. claims, premises, rebuttal, etc.)6 are extracted.
23. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it.
24. However, consider another example, extracted from an online debate platform Kialo 4 .
25. Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.
26. A1: Marvel Universe is better than DC Universe.
27. A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.
28. A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.
29. A4: This is especially true due to his unfortunate passing.
30. A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.
31. The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components.
32. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level.
33. Note, however, that the relations between arguments and claim are still relatively clear (e.g. claims, premises, rebuttal, etc.)7.
34. Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict.
35. Working in the realm of overtly argumentative text (e.g. claims, premises, rebuttal, etc.)8), while challenging of course, can be quite standardized.
36. The language use is generally in line with natural language expectations and often standard (e.g. claims, premises, rebuttal, etc.)9, the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (Moens et al., 2007)0.","Towards Argument Mining for Social Good: A Survey##
Framework##
Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

A1: Marvel Universe is better than DC Universe.

A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

A4: This is especially true due to his unfortunate passing.

A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).",What are the stages involved in the argument mining framework and how do they function?,1. What are the stages involved in the argument mining framework and how do they function?,"Questions:

1. What are the stages involved in the argument mining framework and how do they function?

Answer:

(p2.0) Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

(p2.1) Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

(p2.2) To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

(p2.3) Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

(p2.4) A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

(p2.5) Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

(p2.6) A1: Marvel Universe is better than DC Universe.

(p2.7) A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

(p2.8) A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

(p2.9) A4: This is especially true due to his unfortunate passing.

(p2.10) A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

(p2.11) The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

(p2.12) Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","Towards Argument Mining for Social Good: A Survey##
Framework##
Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).",What methodologies have been applied in the extraction of argumentative structures from texts?,1. What methodologies have been applied in the extraction of argumentative structures from texts?,"Questions:

1. What methodologies have been applied in the extraction of argumentative structures from texts?

Answer:

(p2.0) Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

(p2.1) Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

(p2.2) To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

(p2.3) Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

(p2.4) A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

(p2.5) Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

(p2.6) A1: Marvel Universe is better than DC Universe.

(p2.7) A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

(p2.8) A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

(p2.9) A4: This is especially true due to his unfortunate passing.

(p2.10) A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

(p2.11) The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

(p2.12) Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","Towards Argument Mining for Social Good: A Survey##
Framework##
Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM.

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).

Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly.

A2: Many vaccines have serious and sometimes deadly side effects.

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted.

A1: Marvel Universe is better than DC Universe.

A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

A4: This is especially true due to his unfortunate passing.

A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components.

Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict.",How does the argument mining framework handle the modeling of relations between argumentative components?,1. How does the argument mining framework handle the modeling of relations between argumentative components?,"Questions:

1. How does the argument mining framework handle the modeling of relations between argumentative components?

Answer:

(p2.0) Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

(p2.1) Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

(p2.2) To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

(p2.3) Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

(p2.4) A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

(p2.5) Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

(p2.6) A1: Marvel Universe is better than DC Universe.

(p2.7) A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

(p2.8) A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

(p2.9) A4: This is especially true due to his unfortunate passing.

(p2.10) A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

(p2.11) The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

(p2.12) Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","Towards Argument Mining for Social Good: A Survey##
Framework##
Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).##
Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017).

Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly.

A2: Many vaccines have serious and sometimes deadly side effects.

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted.

A1: Marvel Universe is better than DC Universe.

A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

A4: This is especially true due to his unfortunate passing.

A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components.

Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict.",What are the stages involved in the argument mining framework and how are they implemented?,1. What are the stages involved in the argument mining framework and how are they implemented?,"Questions:

1. What are the stages involved in the argument mining framework and how are they implemented?

Answer:

(p2.0) Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

(p2.1) Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

(p2.2) To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

(p2.3) Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

(p2.4) A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

(p2.5) Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

(p2.6) A1: Marvel Universe is better than DC Universe.

(p2.7) A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

(p2.8) A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

(p2.9) A4: This is especially true due to his unfortunate passing.

(p2.10) A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

(p2.11) The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

(p2.12) Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","Towards Argument Mining for Social Good: A Survey
##
Framework
##
Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).
Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.
To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.
Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.
A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.
Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.
A1: Marvel Universe is better than DC Universe.
A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.
A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.
A4: This is especially true due to his unfortunate passing.
A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.
The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).
Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).
##
What are the stages involved in the argument mining framework and how are they implemented?","Cabrio and Villata (2018) provide a detailed analysis of the argument mining (AM) framework, highlighting its implementation in two main stages. The first stage, argument extraction, involves identifying documents with argumentative structures and pinpointing the specific textual spans where argumentation occurs. This includes defining the boundaries of argumentative text and assigning segments to predefined argument components such as claims, premises, and rebuttals. Various models, including Näive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised learning techniques (Ein-Dor et al., 2020), have been employed for this purpose.

The second stage, relation assignment, aims to model the relations between the argumentative spans identified earlier. This involves recognizing support or attack relations between different arguments and connecting premises with claims within an argument. Approaches to classify these argumentative relations include relational models (Trautmann et al., 2020) and leveraging features from various knowledge bases (Kobbe et al., 2019). Successfully detecting these relations allows for the mapping of the discourse structure into an argumentation graph, where arguments are nodes and relations are edges. Some methods simplify this graph into a tree structure (Peldszus and Stede, 2015; Stab and Gurevych, 2017) and explore different techniques for generating these structures, such as SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) and textual entailment (Cabrio and Villata, 2013; Cocarascu et al., 2020). This process is crucial for modeling the flow of argumentation and defining argument quality.

Examples from online debates illustrate how the framework operates in practice, showing the extraction of argument components and the assignment of relations, which can vary significantly depending on the environment and platform. This highlights the framework's adaptability to different contexts, from structured persuasive essays to more conversational online platforms, affecting the complexity of component and relation extraction.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Cabrio and Villata (2018) provide a detailed analysis of the argument mining (AM) framework, highlighting its implementation in two main stages. The first stage, argument extraction, involves identifying documents with argumentative structures and pinpointing the specific textual spans where argumentation occurs. This includes defining the boundaries of argumentative text and assigning segments to predefined argument components such as claims, premises, and rebuttals. Various models, including Näive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised learning techniques (Ein-Dor et al., 2020), have been employed for this purpose.

The second stage, relation assignment, aims to model the relations between the argumentative spans identified earlier. This involves recognizing support or attack relations between different arguments and connecting premises with claims within an argument. Approaches to classify these argumentative relations include relational models (Trautmann et al., 2020) and leveraging features from various knowledge bases (Kobbe et al., 2019). Successfully detecting these relations allows for the mapping of the discourse structure into an argumentation graph, where arguments are nodes and relations are edges. Some methods simplify this graph into a tree structure (Peldszus and Stede, 2015; Stab and Gurevych, 2017) and explore different techniques for generating these structures, such as SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) and textual entailment (Cabrio and Villata, 2013; Cocarascu et al., 2020). This process is crucial for modeling the flow of argumentation and defining argument quality.

Examples from online debates illustrate how the framework operates in practice, showing the extraction of argument components and the assignment of relations, which can vary significantly depending on the environment and platform. This highlights the framework's adaptability to different contexts, from structured persuasive essays to more conversational online platforms, affecting the complexity of component and relation extraction.","Cabrio and Villata (2018) provide a detailed overview of the argument mining (AM) framework, highlighting its implementation in two main stages. The first stage, argument extraction, involves identifying documents with argumentative structures and pinpointing the specific textual spans where argumentation occurs. This includes defining the boundaries of these spans and assigning subportions to predefined argument components such as claims, premises, and rebuttals. Various models have been employed for this purpose, including Näive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised learning techniques (Ein-Dor et al., 2020).

The second stage, relation assignment, aims to model the relations between the argumentative spans identified in the first stage. These relations can be supportive or attacking between different arguments, or they can connect premises with claims within an argument. Approaches to classifying these argumentative relations include relational models (Trautmann et al., 2020) and leveraging features from various knowledge bases (Kobbe et al., 2019). Successfully detecting these relations allows for the modeling of the argumentation's overall structure, which can be represented as an argumentation graph with arguments as nodes and relations as edges. Some methods simplify this graph into a tree structure (Peldszus and Stede, 2015; Stab and Gurevych, 2017) and explore different techniques for generating this structure, such as SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) and textual entailment (Cabrio and Villata, 2013; Cocarascu et al., 2020).

This framework is applicable in various contexts, from structured debates to more conversational environments like online forums, where the boundaries of argumentation and the components' structure can vary significantly. The effectiveness of argument mining depends on the environment and platform, with more standardized texts like persuasive essays offering clearer configurations of claims, premises, and stances, and a more controlled debate structure.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

Cabrio and Villata (2018) provide a detailed overview of the argument mining (AM) framework, highlighting its implementation in two main stages. The first stage, argument extraction, involves identifying documents with argumentative structures and pinpointing the specific textual spans where argumentation occurs. This includes defining the boundaries of these spans and assigning subportions to predefined argument components such as claims, premises, and rebuttals. Various models have been employed for this purpose, including Näive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised learning techniques (Ein-Dor et al., 2020).

The second stage, relation assignment, aims to model the relations between the argumentative spans identified in the first stage. These relations can be supportive or attacking between different arguments, or they can connect premises with claims within an argument. Approaches to classifying these argumentative relations include relational models (Trautmann et al., 2020) and leveraging features from various knowledge bases (Kobbe et al., 2019). Successfully detecting these relations allows for the modeling of the argumentation's overall structure, which can be represented as an argumentation graph with arguments as nodes and relations as edges. Some methods simplify this graph into a tree structure (Peldszus and Stede, 2015; Stab and Gurevych, 2017) and explore different techniques for generating this structure, such as SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) and textual entailment (Cabrio and Villata, 2013; Cocarascu et al., 2020).

This framework is applicable in various contexts, from structured debates to more conversational environments like online forums, where the boundaries of argumentation and the components' structure can vary significantly. The effectiveness of argument mining depends on the environment and platform, with more standardized texts like persuasive essays offering clearer configurations of claims, premises, and stances, and a more controlled debate structure.","The argument mining (AM) framework, as outlined by Cabrio and Villata (2018), involves two main stages: argument extraction and relation assignment. In the first stage, the system identifies documents containing argumentative structures and specifies the textual spans where argumentation occurs. Subportions of these spans are then categorized into pre-established argument components such as claims, premises, and rebuttals, using various models including Näive Bayes, SVMs, RNNs, Pre-trained Language Models, and other supervised-learning techniques.

The second stage focuses on modeling the relations between the argumentative spans identified earlier. These relations can be supportive or attacking between different arguments or connect premises with claims within an argument. Recent approaches to classifying these relations include relational models and leveraging features from different knowledge bases. Detecting these relations is crucial for modeling the argumentation's overall structure, which can be complex and involve high-level knowledge representation and reasoning. The discourse structure is often represented as an argumentation graph, with arguments as nodes and relations as edges. Some approaches simplify this graph into a tree-structure for ease of analysis.

The framework is applied to various contexts, from structured debates to more conversational environments like online forums, where the boundaries of argumentation structure and components can be vaguer. The type of platform significantly impacts the system's ability to implement the framework and the complexity of the components and relations to extract, assign, and predict.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

The argument mining (AM) framework, as outlined by Cabrio and Villata (2018), involves two main stages: argument extraction and relation assignment. In the first stage, the system identifies documents containing argumentative structures and specifies the textual spans where argumentation occurs. Subportions of these spans are then categorized into pre-established argument components such as claims, premises, and rebuttals, using various models including Näive Bayes, SVMs, RNNs, Pre-trained Language Models, and other supervised-learning techniques.

The second stage focuses on modeling the relations between the argumentative spans identified earlier. These relations can be supportive or attacking between different arguments or connect premises with claims within an argument. Recent approaches to classifying these relations include relational models and leveraging features from different knowledge bases. Detecting these relations is crucial for modeling the argumentation's overall structure, which can be complex and involve high-level knowledge representation and reasoning. The discourse structure is often represented as an argumentation graph, with arguments as nodes and relations as edges. Some approaches simplify this graph into a tree-structure for ease of analysis.

The framework is applied to various contexts, from structured debates to more conversational environments like online forums, where the boundaries of argumentation structure and components can be vaguer. The type of platform significantly impacts the system's ability to implement the framework and the complexity of the components and relations to extract, assign, and predict.","The argument mining (AM) framework, as outlined by Cabrio and Villata (2018), involves two primary stages: argument extraction and relation assignment. In the first stage, the system identifies documents containing argumentative structures and specifies the textual spans where argumentation occurs. Subportions of these spans are then categorized into pre-established argument components such as claims, premises, and rebuttals, using various models including Näive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

The second stage focuses on modeling the relations between the identified argumentative spans, which can be supportive or attacking, and may connect premises with the claim within an argument. This involves high-level knowledge representation and reasoning to map the discourse structure into an argumentation graph, with arguments as nodes and relations as edges. Approaches to this stage include relational models (Trautmann et al., 2020) and leveraging features from different knowledge bases (Kobbe et al., 2019). Some methods simplify the graph into a tree-structure (Peldszus and Stede, 2015; Stab and Gurevych, 2017) and use SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013; Cocarascu et al., 2020) for structure generation.

The framework's application varies based on the environment and platform, affecting the complexity of component and relation extraction. In more structured debates, such as persuasive essays, the language use aligns with natural language expectations, making claim, premise, and stance clear. However, in conversational environments like online forums, boundaries and argumentative structures can be vaguer, with each contribution affecting the argument differently.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

The argument mining (AM) framework, as outlined by Cabrio and Villata (2018), involves two primary stages: argument extraction and relation assignment. In the first stage, the system identifies documents containing argumentative structures and specifies the textual spans where argumentation occurs. Subportions of these spans are then categorized into pre-established argument components such as claims, premises, and rebuttals, using various models including Näive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

The second stage focuses on modeling the relations between the identified argumentative spans, which can be supportive or attacking, and may connect premises with the claim within an argument. This involves high-level knowledge representation and reasoning to map the discourse structure into an argumentation graph, with arguments as nodes and relations as edges. Approaches to this stage include relational models (Trautmann et al., 2020) and leveraging features from different knowledge bases (Kobbe et al., 2019). Some methods simplify the graph into a tree-structure (Peldszus and Stede, 2015; Stab and Gurevych, 2017) and use SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013; Cocarascu et al., 2020) for structure generation.

The framework's application varies based on the environment and platform, affecting the complexity of component and relation extraction. In more structured debates, such as persuasive essays, the language use aligns with natural language expectations, making claim, premise, and stance clear. However, in conversational environments like online forums, boundaries and argumentative structures can be vaguer, with each contribution affecting the argument differently.","The argument mining (AM) framework, as outlined by Cabrio and Villata (2018), involves two primary stages: argument extraction and relation assignment. In the first stage, the system identifies documents containing argumentative structures and specifies the textual spans where argumentation occurs. It then assigns these spans to predefined argument components such as claims, premises, and rebuttals, using various models like Näive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised learning techniques (Ein-Dor et al., 2020).

The second stage focuses on modeling the relations between the identified argumentative spans, categorizing them as either supporting or attacking within or across arguments. This involves complex knowledge representation and reasoning, with recent approaches exploring relational models (Trautmann et al., 2020) and leveraging features from knowledge bases (Kobbe et al., 2019). The overall argumentation structure is then represented as an argumentation graph, with arguments as nodes and relations as edges. Some methods simplify this graph into a tree structure (Peldszus and Stede, 2015; Stab and Gurevych, 2017) and employ SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013; Cocarascu et al., 2020) for structure generation.

Examples from online debates illustrate how the framework operates in practice, highlighting the extraction of argument components and the assignment of relations between arguments. The framework's implementation can vary significantly based on the environment and platform, with more structured environments like persuasive essays offering clearer boundaries and argumentative components compared to the more conversational dynamics found in online exchanges.","Questions:

What are the stages involved in the argument mining framework and how are they implemented?

Answer:

The argument mining (AM) framework, as outlined by Cabrio and Villata (2018), involves two primary stages: argument extraction and relation assignment. In the first stage, the system identifies documents containing argumentative structures and specifies the textual spans where argumentation occurs. It then assigns these spans to predefined argument components such as claims, premises, and rebuttals, using various models like Näive Bayes (Moens et al., 2007), SVMs (Mochales and Moens, 2011), RNNs (Niculae et al., 2017; Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019; Lugini and Litman, 2020), and other supervised learning techniques (Ein-Dor et al., 2020).

The second stage focuses on modeling the relations between the identified argumentative spans, categorizing them as either supporting or attacking within or across arguments. This involves complex knowledge representation and reasoning, with recent approaches exploring relational models (Trautmann et al., 2020) and leveraging features from knowledge bases (Kobbe et al., 2019). The overall argumentation structure is then represented as an argumentation graph, with arguments as nodes and relations as edges. Some methods simplify this graph into a tree structure (Peldszus and Stede, 2015; Stab and Gurevych, 2017) and employ SVMs (Habernal and Gurevych, 2017; Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013; Cocarascu et al., 2020) for structure generation.

Examples from online debates illustrate how the framework operates in practice, highlighting the extraction of argument components and the assignment of relations between arguments. The framework's implementation can vary significantly based on the environment and platform, with more structured environments like persuasive essays offering clearer boundaries and argumentative components compared to the more conversational dynamics found in online exchanges."
234093015,A Survey of Data Augmentation Approaches for NLP,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/63d8426ba1f51a8525dd19fd8ec92934ec71aea5,s1,Background,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5']","['What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.', 'What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.', 'Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.', 'Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.', 'Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.', 'Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.']","What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","(p1.0) What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

(p1.1) What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

(p1.2) Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

(p1.3) Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

(p1.4) Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

(p1.5) Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","[[None], ['b23', None], [None, 'b1'], [], [], [None]]","[[None], ['b23', None], [None, 'b1'], [], [], [None]]",6,"1. What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
2. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020).
3. DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training.
4. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.
5. What are the goals and trade-offs?
6. Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
7. As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.
8. Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).
9. Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
10. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
11. Further, the distribution of augmented data should neither be too similar nor too different from the original.
12. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
13. Effective DA approaches should aim for a balance.
14. Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.
15. Interpretation of DA Dao et al. (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020)0 note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.
16. Overall, there indeed appears to be a lack of research on why exactly DA works.
17. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.
18. We discuss this challenge more in §6, and highlight some of the existing work below.
19. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2).
20. Rajput et al. (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020)0 show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.
21. Dao et al. (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020)0 think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.
22. Chen et al. (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020)1 show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","A Survey of Data Augmentation Approaches for NLP##
Background##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.",What are the theoretical explanations for the effectiveness of data augmentation in NLP?,1. What are the theoretical explanations for the effectiveness of data augmentation in NLP?,"Questions:

1. What are the theoretical explanations for the effectiveness of data augmentation in NLP?

Answer:

(p1.0) What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

(p1.1) What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

(p1.2) Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

(p1.3) Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

(p1.4) Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

(p1.5) Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","A Survey of Data Augmentation Approaches for NLP##
Background##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.",How does data augmentation contribute to reducing overfitting in machine learning models?,1. How does data augmentation contribute to reducing overfitting in machine learning models?,"Questions:

1. How does data augmentation contribute to reducing overfitting in machine learning models?

Answer:

(p1.0) What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

(p1.1) What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

(p1.2) Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

(p1.3) Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

(p1.4) Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

(p1.5) Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","A Survey of Data Augmentation Approaches for NLP##
Background##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.

What are the goals and trade-offs?

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Further, the distribution of augmented data should neither be too similar nor too different from the original.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.",What are the primary goals and challenges of data augmentation in NLP?,1. What are the primary goals and challenges of data augmentation in NLP?,"Questions:

1. What are the primary goals and challenges of data augmentation in NLP?

Answer:

(p1.0) What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

(p1.1) What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

(p1.2) Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

(p1.3) Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

(p1.4) Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

(p1.5) Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","A Survey of Data Augmentation Approaches for NLP##
Background##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.##
What are the goals and trade-offs?

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).

Further, the distribution of augmented data should neither be too similar nor too different from the original.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.",What are the primary challenges and considerations in applying data augmentation techniques in NLP?,1. What are the primary challenges and considerations in applying data augmentation techniques in NLP?,"Questions:

1. What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

(p1.0) What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

(p1.1) What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

(p1.2) Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

(p1.3) Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

(p1.4) Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

(p1.5) Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","A Survey of Data Augmentation Approaches for NLP
##
Background
##
What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.
What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.
Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.
Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.
Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.
##
What are the primary challenges and considerations in applying data augmentation techniques in NLP?","Data augmentation (DA) in natural language processing (NLP) involves methods to increase the diversity of training data without the need for direct data collection. This is achieved through modifications of existing data or the creation of synthetic data, with the goal of acting as a regularizer to mitigate overfitting in machine learning model training. While DA is straightforward in computer vision (CV) with techniques like cropping and flipping, it presents unique challenges in NLP due to the discrete nature of text, making it difficult to generate effective augmented examples that maintain the desired invariances.

The primary goals of DA in NLP are to provide an alternative to collecting more data, enhance model performance, and maintain a balance in the data distribution to avoid overfitting or underperformance. This involves trade-offs between ease of implementation and the extent of performance improvement. Rule-based techniques, while simpler to implement, often yield only incremental improvements. In contrast, model-based techniques, though more complex and costly, can introduce significant data variation and potentially lead to substantial performance gains. However, these techniques must be carefully developed and applied to avoid producing data that is either too similar or too divergent from the original, as this could negatively impact model training.

There is also a noted lack of deep understanding regarding the theoretical foundations of DA, with existing research primarily focusing on its practical applications rather than exploring the principles underlying its effectiveness. Some studies have linked DA to established concepts such as Tikhonov regularization and variance reduction, suggesting that DA can improve classifier margins and reduce variance through specific transformations. Nonetheless, the field acknowledges the need for further investigation into the theoretical aspects of DA to fully comprehend its mechanisms and optimize its application in NLP.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

Data augmentation (DA) in natural language processing (NLP) involves methods to increase the diversity of training data without the need for direct data collection. This is achieved through modifications of existing data or the creation of synthetic data, with the goal of acting as a regularizer to mitigate overfitting in machine learning model training. While DA is straightforward in computer vision (CV) with techniques like cropping and flipping, it presents unique challenges in NLP due to the discrete nature of text, making it difficult to generate effective augmented examples that maintain the desired invariances.

The primary goals of DA in NLP are to provide an alternative to collecting more data, enhance model performance, and maintain a balance in the data distribution to avoid overfitting or underperformance. This involves trade-offs between ease of implementation and the extent of performance improvement. Rule-based techniques, while simpler to implement, often yield only incremental improvements. In contrast, model-based techniques, though more complex and costly, can introduce significant data variation and potentially lead to substantial performance gains. However, these techniques must be carefully developed and applied to avoid producing data that is either too similar or too divergent from the original, as this could negatively impact model training.

There is also a noted lack of deep understanding regarding the theoretical foundations of DA, with existing research primarily focusing on its practical applications rather than exploring the principles underlying its effectiveness. Some studies have linked DA to established concepts such as Tikhonov regularization and variance reduction, suggesting that DA can improve classifier margins and reduce variance through specific transformations. Nonetheless, the field acknowledges the need for further investigation into the theoretical aspects of DA to fully comprehend its mechanisms and optimize its application in NLP.","Data augmentation (DA) in natural language processing (NLP) involves increasing the diversity of training data without the need to collect more data, primarily through generating slightly modified copies of existing data or creating synthetic data. This process aims to act as a regularizer and help reduce overfitting in machine learning models, as noted by Shorten and Khoshgoftaar (2019) and Hernández-García and König (2020). Unlike in computer vision (CV), where augmentation techniques like cropping and flipping are straightforward due to the continuous nature of the input space, NLP faces unique challenges due to its discrete input space, making it less obvious how to generate effective augmented examples that maintain the desired invariances.

The primary goals of DA in NLP are to provide an alternative to collecting more data and to improve model performance, with techniques ranging from simple rule-based manipulations to more complex generative approaches (Zhang et al., 2015; Liu et al., 2020b). However, these techniques often involve trade-offs between ease of implementation and the extent of performance improvement. Rule-based techniques are generally easier to implement but tend to offer only incremental improvements, whereas model-based techniques, though potentially more costly to implement, can introduce a greater variety of data and thus lead to more significant performance boosts. Moreover, it's crucial for the distribution of augmented data to strike a balance between being neither too similar nor too different from the original data to avoid issues like overfitting or poor performance due to non-representative training examples.

Despite its practical applications, there's a noted lack of deep understanding of why DA works effectively in NLP. Critics like Dao et al. (2019) argue that DA is often applied in an ad-hoc manner without a solid grasp of the underlying theoretical principles, suggesting that the common view of DA as merely a form of regularization is insufficient. Some research, such as that by Bishop (1995), Rajput et al. (2019), and Chen et al. (2020d), has begun to explore the theoretical underpinnings of DA, suggesting mechanisms like variance reduction and the averaging of features as reasons for its effectiveness. Additionally, Kashefi and Hwa (2020) have proposed a KL-Divergence-based unsupervised procedure to select among DA heuristics more efficiently, indicating a move towards a more principled approach to DA in NLP.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

Data augmentation (DA) in natural language processing (NLP) involves increasing the diversity of training data without the need to collect more data, primarily through generating slightly modified copies of existing data or creating synthetic data. This process aims to act as a regularizer and help reduce overfitting in machine learning models, as noted by Shorten and Khoshgoftaar (2019) and Hernández-García and König (2020). Unlike in computer vision (CV), where augmentation techniques like cropping and flipping are straightforward due to the continuous nature of the input space, NLP faces unique challenges due to its discrete input space, making it less obvious how to generate effective augmented examples that maintain the desired invariances.

The primary goals of DA in NLP are to provide an alternative to collecting more data and to improve model performance, with techniques ranging from simple rule-based manipulations to more complex generative approaches (Zhang et al., 2015; Liu et al., 2020b). However, these techniques often involve trade-offs between ease of implementation and the extent of performance improvement. Rule-based techniques are generally easier to implement but tend to offer only incremental improvements, whereas model-based techniques, though potentially more costly to implement, can introduce a greater variety of data and thus lead to more significant performance boosts. Moreover, it's crucial for the distribution of augmented data to strike a balance between being neither too similar nor too different from the original data to avoid issues like overfitting or poor performance due to non-representative training examples.

Despite its practical applications, there's a noted lack of deep understanding of why DA works effectively in NLP. Critics like Dao et al. (2019) argue that DA is often applied in an ad-hoc manner without a solid grasp of the underlying theoretical principles, suggesting that the common view of DA as merely a form of regularization is insufficient. Some research, such as that by Bishop (1995), Rajput et al. (2019), and Chen et al. (2020d), has begun to explore the theoretical underpinnings of DA, suggesting mechanisms like variance reduction and the averaging of features as reasons for its effectiveness. Additionally, Kashefi and Hwa (2020) have proposed a KL-Divergence-based unsupervised procedure to select among DA heuristics more efficiently, indicating a move towards a more principled approach to DA in NLP.","Data augmentation (DA) in NLP involves increasing training data diversity without collecting more data, primarily through modifying existing data or generating synthetic data. This aims to act as a regularizer and reduce overfitting. Unlike in computer vision, where augmentation techniques like cropping and flipping are straightforward, NLP faces challenges due to its discrete input space, making it less obvious how to generate effective augmented examples. The goals of DA in NLP include improving model performance without the need for additional data collection, balancing ease of implementation with the effectiveness of the augmentation. Rule-based techniques are simpler but may offer limited performance boosts, while model-based approaches can introduce more variation and potentially greater performance improvements but may be more complex to implement. Effective DA must carefully balance the distribution of augmented data to avoid being too similar or too divergent from the original data, aiming to prevent overfitting or training on non-representative examples. Additionally, there's a need for a deeper understanding of DA's underlying theoretical principles, beyond its role as a form of regularization, to fully leverage its potential in NLP.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

Data augmentation (DA) in NLP involves increasing training data diversity without collecting more data, primarily through modifying existing data or generating synthetic data. This aims to act as a regularizer and reduce overfitting. Unlike in computer vision, where augmentation techniques like cropping and flipping are straightforward, NLP faces challenges due to its discrete input space, making it less obvious how to generate effective augmented examples. The goals of DA in NLP include improving model performance without the need for additional data collection, balancing ease of implementation with the effectiveness of the augmentation. Rule-based techniques are simpler but may offer limited performance boosts, while model-based approaches can introduce more variation and potentially greater performance improvements but may be more complex to implement. Effective DA must carefully balance the distribution of augmented data to avoid being too similar or too divergent from the original data, aiming to prevent overfitting or training on non-representative examples. Additionally, there's a need for a deeper understanding of DA's underlying theoretical principles, beyond its role as a form of regularization, to fully leverage its potential in NLP.","Data augmentation (DA) in NLP involves creating additional training data through methods that either modify existing data slightly or generate new synthetic data, aiming to enhance model training by acting as a regularizer to mitigate overfitting (Shorten and Khoshgoftaar, 2019; Hernández-García and König, 2020). Unlike in computer vision, where augmentation techniques like cropping and flipping are straightforward, generating effective augmented text data that maintains the desired invariances presents unique challenges due to the discrete nature of text.

The primary goals of DA in NLP are to increase data diversity without the need for new data collection, ideally in a manner that is both simple to implement and effective at improving model performance. However, there are trade-offs involved: rule-based techniques, while easier to implement, often yield only modest performance gains (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b), whereas model-based approaches can introduce greater data variation and potentially offer significant performance improvements but may be more complex and costly to develop.

An effective DA technique must carefully balance the distribution of augmented data to avoid being too similar to or too divergent from the original data, as either extreme could lead to overfitting or training on non-representative examples, respectively. Kashefi and Hwa (2020) suggest using a KL-Divergence-based unsupervised method to select DA heuristics, avoiding the time and cost associated with exhaustive heuristic comparisons.

Despite the practical application of DA, there is a noted lack of deep understanding of its theoretical foundations. While DA is often described as a form of regularization, this explanation is considered insufficient by some researchers (Dao et al., 2019). Studies have explored various aspects of DA's effectiveness, such as its role in Tikhonov regularization (Bishop, 1995), increasing classifiers' positive margin (Rajput et al., 2019), and variance reduction (Chen et al., 2020d), but a comprehensive theoretical framework remains elusive.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

Data augmentation (DA) in NLP involves creating additional training data through methods that either modify existing data slightly or generate new synthetic data, aiming to enhance model training by acting as a regularizer to mitigate overfitting (Shorten and Khoshgoftaar, 2019; Hernández-García and König, 2020). Unlike in computer vision, where augmentation techniques like cropping and flipping are straightforward, generating effective augmented text data that maintains the desired invariances presents unique challenges due to the discrete nature of text.

The primary goals of DA in NLP are to increase data diversity without the need for new data collection, ideally in a manner that is both simple to implement and effective at improving model performance. However, there are trade-offs involved: rule-based techniques, while easier to implement, often yield only modest performance gains (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b), whereas model-based approaches can introduce greater data variation and potentially offer significant performance improvements but may be more complex and costly to develop.

An effective DA technique must carefully balance the distribution of augmented data to avoid being too similar to or too divergent from the original data, as either extreme could lead to overfitting or training on non-representative examples, respectively. Kashefi and Hwa (2020) suggest using a KL-Divergence-based unsupervised method to select DA heuristics, avoiding the time and cost associated with exhaustive heuristic comparisons.

Despite the practical application of DA, there is a noted lack of deep understanding of its theoretical foundations. While DA is often described as a form of regularization, this explanation is considered insufficient by some researchers (Dao et al., 2019). Studies have explored various aspects of DA's effectiveness, such as its role in Tikhonov regularization (Bishop, 1995), increasing classifiers' positive margin (Rajput et al., 2019), and variance reduction (Chen et al., 2020d), but a comprehensive theoretical framework remains elusive.","Data augmentation (DA) in NLP aims to increase training data diversity without the need for direct data collection by adding modified copies of existing data or creating synthetic data. This process helps act as a regularizer and reduce overfitting in machine learning models. Unlike in computer vision, where augmentation techniques like cropping and flipping are straightforward, generating effective augmented examples in the discrete input space of NLP presents unique challenges. The primary goals of DA in NLP involve improving model performance without the complexities of collecting more data, balancing the ease of implementation with the effectiveness of the augmentation. Rule-based techniques, while simpler to implement, often yield only incremental improvements. In contrast, model-based techniques, though potentially more costly and complex, can introduce greater data variation and significant performance enhancements. However, it's crucial that the distribution of augmented data closely aligns with the original data to avoid issues like overfitting or training on non-representative examples. Kashefi and Hwa (2020) suggest using a KL-Divergence-based unsupervised procedure to select DA heuristics efficiently, avoiding the time and cost of extensive heuristic comparisons. Despite the practical applications of DA, there's a noted lack of deep understanding of its theoretical foundations. Some researchers argue that viewing DA merely as a form of regularization is insufficient and call for a deeper investigation into its principles and effects, such as how it can increase classifier margins or lead to variance reduction through feature averaging and variance regularization.","Questions:

What are the primary challenges and considerations in applying data augmentation techniques in NLP?

Answer:

Data augmentation (DA) in NLP aims to increase training data diversity without the need for direct data collection by adding modified copies of existing data or creating synthetic data. This process helps act as a regularizer and reduce overfitting in machine learning models. Unlike in computer vision, where augmentation techniques like cropping and flipping are straightforward, generating effective augmented examples in the discrete input space of NLP presents unique challenges. The primary goals of DA in NLP involve improving model performance without the complexities of collecting more data, balancing the ease of implementation with the effectiveness of the augmentation. Rule-based techniques, while simpler to implement, often yield only incremental improvements. In contrast, model-based techniques, though potentially more costly and complex, can introduce greater data variation and significant performance enhancements. However, it's crucial that the distribution of augmented data closely aligns with the original data to avoid issues like overfitting or training on non-representative examples. Kashefi and Hwa (2020) suggest using a KL-Divergence-based unsupervised procedure to select DA heuristics efficiently, avoiding the time and cost of extensive heuristic comparisons. Despite the practical applications of DA, there's a noted lack of deep understanding of its theoretical foundations. Some researchers argue that viewing DA merely as a form of regularization is insufficient and call for a deeper investigation into its principles and effects, such as how it can increase classifier margins or lead to variance reduction through feature averaging and variance regularization."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s4,How Low is Low-Resource?,"['p4.0', 'p4.1', 'p4.2']","['On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.', '(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.', 'Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.']","On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","(p4.0) On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(p4.1) (2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

(p4.2) Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","[[], ['b0', 'b13'], [None, 'b13']]","[[], ['b0', 'b13'], [None, 'b13']]",4,"1. On the dimension of task-specific labels, different thresholds are used to define low-resource.
2. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens.
3. Kann et al.(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens.
4. The threshold is also task-dependent and more complex tasks might also increase the resource requirements.
5. For text generation,  frame their work as low-resource with 350k labeled training instances.
6. Similar to the task, the resource requirements can also depend on the language.
7. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.
8. Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability.
9. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches.
10. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios##
How Low is Low-Resource?##
On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.",What criteria define low-resource scenarios in natural language processing research?,1. What criteria define low-resource scenarios in natural language processing research?,"Questions:

1. What criteria define low-resource scenarios in natural language processing research?

Answer:

(p4.0) On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(p4.1) (2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

(p4.2) Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios##
How Low is Low-Resource?##
On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.",What criteria define a low-resource scenario in NLP tasks?,1. What criteria define a low-resource scenario in NLP tasks?,"Questions:

1. What criteria define a low-resource scenario in NLP tasks?

Answer:

(p4.0) On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(p4.1) (2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

(p4.2) Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios##
How Low is Low-Resource?##
On the dimension of task-specific labels, different thresholds are used to define low-resource.

(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability.",What criteria define a low-resource language scenario in NLP research?,1. What criteria define a low-resource language scenario in NLP research?,"Questions:

1. What criteria define a low-resource language scenario in NLP research?

Answer:

(p4.0) On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(p4.1) (2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

(p4.2) Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios##
How Low is Low-Resource?##
On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.##
(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability.",What criteria define a low-resource scenario in natural language processing research?,1. What criteria define a low-resource scenario in natural language processing research?,"Questions:

1. What criteria define a low-resource scenario in natural language processing research?

Answer:

(p4.0) On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(p4.1) (2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

(p4.2) Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios
##
How Low is Low-Resource?
##
On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.
(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.
Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.
##
What criteria define a low-resource scenario in natural language processing research?","In natural language processing research, a low-resource scenario is defined by various criteria depending on the task and language involved. For part-of-speech tagging, a scenario is considered low-resource if annotators are limited to 2 hours, typically resulting in 1-2k tokens, as noted by Garrette and Baldridge (2013). Languages with less than 10k labeled tokens, such as those studied in the Universal Dependency project by Kann et al. (2020), or South African languages with 40-60k labeled tokens as reported by Loubser and Puttkammer (2020), also fall into this category. The complexity of the task influences the threshold, with more complex tasks requiring more resources; for example, text generation tasks are labeled low-resource with up to 350k labeled training instances. Additionally, Plank et al. (2016) observed that task performance varies across language families even with the same limited amount of training data, indicating that resource requirements can depend on the language. The concept of low-resource settings is seen as a spectrum of resource availability rather than having a hard threshold. It is suggested that evaluating low-resource techniques across different levels of data availability could provide a better comparison between approaches. Studies by Plank et al. (2016) and Melamud et al. (2019) have shown that non-neural methods outperform more modern approaches in very small datasets, but the latter achieve better performance in resource-lean scenarios with a few hundred labeled instances available.","Questions:

What criteria define a low-resource scenario in natural language processing research?

Answer:

In natural language processing research, a low-resource scenario is defined by various criteria depending on the task and language involved. For part-of-speech tagging, a scenario is considered low-resource if annotators are limited to 2 hours, typically resulting in 1-2k tokens, as noted by Garrette and Baldridge (2013). Languages with less than 10k labeled tokens, such as those studied in the Universal Dependency project by Kann et al. (2020), or South African languages with 40-60k labeled tokens as reported by Loubser and Puttkammer (2020), also fall into this category. The complexity of the task influences the threshold, with more complex tasks requiring more resources; for example, text generation tasks are labeled low-resource with up to 350k labeled training instances. Additionally, Plank et al. (2016) observed that task performance varies across language families even with the same limited amount of training data, indicating that resource requirements can depend on the language. The concept of low-resource settings is seen as a spectrum of resource availability rather than having a hard threshold. It is suggested that evaluating low-resource techniques across different levels of data availability could provide a better comparison between approaches. Studies by Plank et al. (2016) and Melamud et al. (2019) have shown that non-neural methods outperform more modern approaches in very small datasets, but the latter achieve better performance in resource-lean scenarios with a few hundred labeled instances available.","In natural language processing research, a low-resource scenario is defined by various criteria depending on the specific task and language involved. For part-of-speech tagging, a scenario is considered low-resource if annotators are limited to 2 hours of labeling time, typically resulting in 1-2k tokens. Studies such as those by Kann et al. (2020) identify languages with less than 10k labeled tokens in the Universal Dependency project as low-resource, while for South African languages, datasets with 40-60k labeled tokens are common. The threshold for what constitutes low-resource varies with the complexity of the task; for example, text generation tasks may be considered low-resource with as many as 350k labeled training instances. Additionally, the resource requirements can vary by language, with Plank et al. (2016) finding that task performance differs across language families even with the same limited amount of training data. The concept of low-resource in NLP is seen as a spectrum of resource availability, suggesting a need for evaluating low-resource techniques across different levels of data availability to facilitate better comparisons between approaches. Research by Plank et al. (2016) and Melamud et al. (2019) indicates that non-neural methods may outperform more modern approaches in scenarios with very small datasets, but modern approaches tend to perform better in slightly more resource-lean scenarios with a few hundred labeled instances available.","Questions:

What criteria define a low-resource scenario in natural language processing research?

Answer:

In natural language processing research, a low-resource scenario is defined by various criteria depending on the specific task and language involved. For part-of-speech tagging, a scenario is considered low-resource if annotators are limited to 2 hours of labeling time, typically resulting in 1-2k tokens. Studies such as those by Kann et al. (2020) identify languages with less than 10k labeled tokens in the Universal Dependency project as low-resource, while for South African languages, datasets with 40-60k labeled tokens are common. The threshold for what constitutes low-resource varies with the complexity of the task; for example, text generation tasks may be considered low-resource with as many as 350k labeled training instances. Additionally, the resource requirements can vary by language, with Plank et al. (2016) finding that task performance differs across language families even with the same limited amount of training data. The concept of low-resource in NLP is seen as a spectrum of resource availability, suggesting a need for evaluating low-resource techniques across different levels of data availability to facilitate better comparisons between approaches. Research by Plank et al. (2016) and Melamud et al. (2019) indicates that non-neural methods may outperform more modern approaches in scenarios with very small datasets, but modern approaches tend to perform better in slightly more resource-lean scenarios with a few hundred labeled instances available.","Low-resource scenarios in natural language processing (NLP) research are defined by the availability of task-specific labels, with different thresholds used for various tasks. For part-of-speech (POS) tagging, a scenario might be considered low-resource if only up to 1-2k tokens are available, as a result of limiting annotator time to 2 hours (Garrette and Baldridge, 2013). Languages with less than 10k labeled tokens in the Universal Dependency project are studied as low-resource by Kann et al. (2020), while datasets for South African languages often have 40-60k labeled tokens, also considered low-resource (Loubser and Puttkammer, 2020). The threshold for what constitutes low-resource varies with the complexity of the task; for example, text generation tasks may be considered low-resource with as many as 350k labeled training instances. Additionally, the resource requirements and task performance can vary between language families even with the same amount of limited training data (Plank et al., 2016). The concept of low-resource settings is seen as a spectrum of resource availability, and it is suggested that evaluating low-resource techniques across different levels of data availability could facilitate better comparison between approaches. Studies have shown that non-neural methods may outperform more modern approaches for very small datasets, while neural methods gain the upper hand in resource-lean scenarios once a few hundred labeled instances are available (Plank et al., 2016; Melamud et al., 2019).","Questions:

What criteria define a low-resource scenario in natural language processing research?

Answer:

Low-resource scenarios in natural language processing (NLP) research are defined by the availability of task-specific labels, with different thresholds used for various tasks. For part-of-speech (POS) tagging, a scenario might be considered low-resource if only up to 1-2k tokens are available, as a result of limiting annotator time to 2 hours (Garrette and Baldridge, 2013). Languages with less than 10k labeled tokens in the Universal Dependency project are studied as low-resource by Kann et al. (2020), while datasets for South African languages often have 40-60k labeled tokens, also considered low-resource (Loubser and Puttkammer, 2020). The threshold for what constitutes low-resource varies with the complexity of the task; for example, text generation tasks may be considered low-resource with as many as 350k labeled training instances. Additionally, the resource requirements and task performance can vary between language families even with the same amount of limited training data (Plank et al., 2016). The concept of low-resource settings is seen as a spectrum of resource availability, and it is suggested that evaluating low-resource techniques across different levels of data availability could facilitate better comparison between approaches. Studies have shown that non-neural methods may outperform more modern approaches for very small datasets, while neural methods gain the upper hand in resource-lean scenarios once a few hundred labeled instances are available (Plank et al., 2016; Melamud et al., 2019).","In the context of natural language processing research, a low-resource scenario is defined by various thresholds of task-specific labels. For instance, for part-of-speech (POS) tagging, Garrette and Baldridge (2013) considered up to 1-2k tokens, obtained within a 2-hour annotation limit, as low-resource. Kann et al. (2020) classify languages with less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) as low-resource, while Loubser and Puttkammer (2020) note that most datasets for South African languages fall into the low-resource category with 40-60k labeled tokens. The definition of low-resource also varies with the complexity of the task; for example, text generation tasks are considered low-resource with as many as 350k labeled training instances. Additionally, the resource requirements can vary depending on the language, with Plank et al. (2016) finding that task performance differs across language families even with the same limited amount of training data.

The concept of low-resource in NLP is seen as a spectrum of resource availability rather than a fixed threshold. It is suggested that evaluating low-resource techniques across different levels of data availability could provide a better comparison between approaches. Studies by Plank et al. (2016) and Melamud et al. (2019) have shown that non-neural methods may outperform more modern approaches in very small datasets, while the latter tend to perform better in resource-lean scenarios once a few hundred labeled instances are available.","Questions:

What criteria define a low-resource scenario in natural language processing research?

Answer:

In the context of natural language processing research, a low-resource scenario is defined by various thresholds of task-specific labels. For instance, for part-of-speech (POS) tagging, Garrette and Baldridge (2013) considered up to 1-2k tokens, obtained within a 2-hour annotation limit, as low-resource. Kann et al. (2020) classify languages with less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) as low-resource, while Loubser and Puttkammer (2020) note that most datasets for South African languages fall into the low-resource category with 40-60k labeled tokens. The definition of low-resource also varies with the complexity of the task; for example, text generation tasks are considered low-resource with as many as 350k labeled training instances. Additionally, the resource requirements can vary depending on the language, with Plank et al. (2016) finding that task performance differs across language families even with the same limited amount of training data.

The concept of low-resource in NLP is seen as a spectrum of resource availability rather than a fixed threshold. It is suggested that evaluating low-resource techniques across different levels of data availability could provide a better comparison between approaches. Studies by Plank et al. (2016) and Melamud et al. (2019) have shown that non-neural methods may outperform more modern approaches in very small datasets, while the latter tend to perform better in resource-lean scenarios once a few hundred labeled instances are available.","Low-resource scenarios in natural language processing (NLP) research are defined by various criteria, primarily based on the availability of task-specific labels. For instance, for part-of-speech (POS) tagging, a scenario is considered low-resource if annotators are limited to 2 hours of labeling time, resulting in up to 1-2k tokens (Garrette and Baldridge, 2013). Languages with less than 10k labeled tokens in the Universal Dependency project are studied as low-resource by Kann et al. (2020), while datasets for South African languages are reported to have 40-60k labeled tokens, indicating a low-resource condition (Loubser and Puttkammer, 2020). The threshold for what constitutes a low-resource scenario is also dependent on the complexity of the task; for example, text generation tasks are considered low-resource with as many as 350k labeled training instances. Additionally, the resource requirements can vary depending on the language, with task performance differing between language families even with the same amount of limited training data (Plank et al., 2016). The concept of low-resource settings is seen as a spectrum of resource availability, suggesting that evaluations of low-resource techniques should span different levels of data availability to facilitate better comparisons between approaches. Studies have shown that for very small datasets, non-neural methods may outperform more modern approaches, which tend to perform better in resource-lean scenarios once a few hundred labeled instances are available (Plank et al., 2016; Melamud et al., 2019).","Questions:

What criteria define a low-resource scenario in natural language processing research?

Answer:

Low-resource scenarios in natural language processing (NLP) research are defined by various criteria, primarily based on the availability of task-specific labels. For instance, for part-of-speech (POS) tagging, a scenario is considered low-resource if annotators are limited to 2 hours of labeling time, resulting in up to 1-2k tokens (Garrette and Baldridge, 2013). Languages with less than 10k labeled tokens in the Universal Dependency project are studied as low-resource by Kann et al. (2020), while datasets for South African languages are reported to have 40-60k labeled tokens, indicating a low-resource condition (Loubser and Puttkammer, 2020). The threshold for what constitutes a low-resource scenario is also dependent on the complexity of the task; for example, text generation tasks are considered low-resource with as many as 350k labeled training instances. Additionally, the resource requirements can vary depending on the language, with task performance differing between language families even with the same amount of limited training data (Plank et al., 2016). The concept of low-resource settings is seen as a spectrum of resource availability, suggesting that evaluations of low-resource techniques should span different levels of data availability to facilitate better comparisons between approaches. Studies have shown that for very small datasets, non-neural methods may outperform more modern approaches, which tend to perform better in resource-lean scenarios once a few hundred labeled instances are available (Plank et al., 2016; Melamud et al., 2019)."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Linguistics, Computer Science",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s2,Initial Instruction,"['p2.0', 'p2.1']","['In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.', 'Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.']","In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","(p2.0) In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

(p2.1) Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","[['b25', None, 'b16', 'b19'], [None]]","[['b25', None, 'b16', 'b19'], [None]]",5,"1. In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room.
2. Turn left and go through the door in the middle.""
3. Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal.
4. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017).
5. An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views.
6. R2R is extended to create other VLN benchmarks.
7. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019).
8. Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chang et al., 2017)6, StreetLearn (Mirowski et al., 2019), StreetNav (Chang et al., 2017)8, Talk2Nav (Chang et al., 2017)0 RoomNav (Chang et al., 2017)1, EmbodiedQA (Chang et al., 2017)2, REVERIE (Chang et al., 2017)3, SOON (Chang et al., 2017)4 IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments.
9. In TOUCHDOWN (Chang et al., 2017)6, an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object.
10. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chang et al., 2017)6, StreetLearn (Chang et al., 2017)7, StreetNav(Chang et al., 2017)8, and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.
11. Some work uses natural language to guide drones.
12. LANI (Chang et al., 2017)9 is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions.
13. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions##
Initial Instruction##
In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.",What are the characteristics and challenges of Vision-and-Language Navigation (VLN) benchmarks?,1. What are the characteristics and challenges of Vision-and-Language Navigation (VLN) benchmarks?,"Questions:

1. What are the characteristics and challenges of Vision-and-Language Navigation (VLN) benchmarks?

Answer:

(p2.0) In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

(p2.1) Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions##
Initial Instruction##
In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.",How do Vision-and-Language Navigation benchmarks simulate real-world navigation tasks?,1. How do Vision-and-Language Navigation benchmarks simulate real-world navigation tasks?,"Questions:

1. How do Vision-and-Language Navigation benchmarks simulate real-world navigation tasks?

Answer:

(p2.0) In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

(p2.1) Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions##
Initial Instruction##
In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room.

Some work uses natural language to guide drones.",How do VLN benchmarks utilize natural language instructions for navigation tasks?,1. How do VLN benchmarks utilize natural language instructions for navigation tasks?,"Questions:

1. How do VLN benchmarks utilize natural language instructions for navigation tasks?

Answer:

(p2.0) In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

(p2.1) Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions##
Initial Instruction##
In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.##
Some work uses natural language to guide drones.",What are the key characteristics and datasets of Vision-and-Language Navigation (VLN) benchmarks?,1. What are the key characteristics and datasets of Vision-and-Language Navigation (VLN) benchmarks?,"Questions:

1. What are the key characteristics and datasets of Vision-and-Language Navigation (VLN) benchmarks?

Answer:

(p2.0) In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

(p2.1) Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions
##
Initial Instruction
##
In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.
Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.
##
What are the key characteristics and datasets of Vision-and-Language Navigation (VLN) benchmarks?","Key characteristics of Vision-and-Language Navigation (VLN) benchmarks include the provision of natural language instructions for navigation, requiring agents to follow these instructions closely to reach their target destinations. The R2R dataset, created by Anderson et al. (2018b) using the Matterport3D simulator (Chang et al., 2017), exemplifies this by having an embodied agent navigate a house via a graph, moving to nodes with panoramic views. This dataset has been foundational, leading to the development of additional benchmarks such as Roomfor-Room, which features longer trajectories (Jain et al., 2019), and others like Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021), LANI (Misra et al., 2018), RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a), and IQA. Outdoor environments, represented in datasets like TOUCHDOWN, StreetLearn, StreetNav, and Talk2Nav, are noted for their complexity and abundance of objects, often utilizing Google Street View for photo-realistic navigation challenges. Additionally, there are datasets focused on drone navigation, such as LANI, which operates in a 3D synthetic environment where drones navigate between landmarks following natural language instructions, typically set in synthetic environments like Unity3D (Blukis et al., 2018).","Questions:

What are the key characteristics and datasets of Vision-and-Language Navigation (VLN) benchmarks?

Answer:

Key characteristics of Vision-and-Language Navigation (VLN) benchmarks include the provision of natural language instructions for navigation, requiring agents to follow these instructions closely to reach their target destinations. The R2R dataset, created by Anderson et al. (2018b) using the Matterport3D simulator (Chang et al., 2017), exemplifies this by having an embodied agent navigate a house via a graph, moving to nodes with panoramic views. This dataset has been foundational, leading to the development of additional benchmarks such as Roomfor-Room, which features longer trajectories (Jain et al., 2019), and others like Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021), LANI (Misra et al., 2018), RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a), and IQA. Outdoor environments, represented in datasets like TOUCHDOWN, StreetLearn, StreetNav, and Talk2Nav, are noted for their complexity and abundance of objects, often utilizing Google Street View for photo-realistic navigation challenges. Additionally, there are datasets focused on drone navigation, such as LANI, which operates in a 3D synthetic environment where drones navigate between landmarks following natural language instructions, typically set in synthetic environments like Unity3D (Blukis et al., 2018).","Key characteristics of Vision-and-Language Navigation (VLN) benchmarks include the provision of natural language instructions for navigation, requiring an agent to follow these instructions closely to reach a target goal. Notable datasets in this field are the R2R dataset, developed by Anderson et al. (2018b) using the Matterport3D simulator (Chang et al., 2017), where an embodied agent navigates a house by moving through a graph of panoramic views. This dataset has been extended to create additional benchmarks such as Roomfor-Room, which features longer trajectories, and others including Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021), LANI (Misra et al., 2018), RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a), and IQA. Outdoor VLN datasets, such as TOUCHDOWN, StreetLearn, StreetNav, and Talk2Nav, are often based on Google Street View and are noted for their complexity and the abundance of objects, presenting a more challenging navigation environment. Additionally, there are datasets like LANI, designed for drone navigation in synthetic 3D environments, indicating a diverse application of VLN benchmarks across different settings and navigation challenges.","Questions:

What are the key characteristics and datasets of Vision-and-Language Navigation (VLN) benchmarks?

Answer:

Key characteristics of Vision-and-Language Navigation (VLN) benchmarks include the provision of natural language instructions for navigation, requiring an agent to follow these instructions closely to reach a target goal. Notable datasets in this field are the R2R dataset, developed by Anderson et al. (2018b) using the Matterport3D simulator (Chang et al., 2017), where an embodied agent navigates a house by moving through a graph of panoramic views. This dataset has been extended to create additional benchmarks such as Roomfor-Room, which features longer trajectories, and others including Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021), LANI (Misra et al., 2018), RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a), and IQA. Outdoor VLN datasets, such as TOUCHDOWN, StreetLearn, StreetNav, and Talk2Nav, are often based on Google Street View and are noted for their complexity and the abundance of objects, presenting a more challenging navigation environment. Additionally, there are datasets like LANI, designed for drone navigation in synthetic 3D environments, indicating a diverse application of VLN benchmarks across different settings and navigation challenges.","In Vision-and-Language Navigation (VLN) benchmarks, agents are tasked with navigating environments based on natural language instructions. These instructions are detailed, guiding the agent through complex paths like ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" The R2R dataset, created by Anderson et al. (2018b) using the Matterport3D simulator, is a foundational dataset where an agent navigates a house by moving through nodes that present panoramic views. This dataset has been extended into various other benchmarks for both indoor and outdoor navigation challenges. For instance, Room-for-Room extends R2R paths into longer trajectories, while datasets like TOUCHDOWN and StreetLearn focus on outdoor navigation using streetview simulations of cities like New York. Additionally, there are datasets for drone navigation, such as LANI, which uses a 3D synthetic environment for navigation between landmarks following instructions. These benchmarks are crucial for developing and evaluating the performance of VLN models, highlighting the diversity of environments and the complexity of navigation tasks guided by language.","Questions:

What are the key characteristics and datasets of Vision-and-Language Navigation (VLN) benchmarks?

Answer:

In Vision-and-Language Navigation (VLN) benchmarks, agents are tasked with navigating environments based on natural language instructions. These instructions are detailed, guiding the agent through complex paths like ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" The R2R dataset, created by Anderson et al. (2018b) using the Matterport3D simulator, is a foundational dataset where an agent navigates a house by moving through nodes that present panoramic views. This dataset has been extended into various other benchmarks for both indoor and outdoor navigation challenges. For instance, Room-for-Room extends R2R paths into longer trajectories, while datasets like TOUCHDOWN and StreetLearn focus on outdoor navigation using streetview simulations of cities like New York. Additionally, there are datasets for drone navigation, such as LANI, which uses a 3D synthetic environment for navigation between landmarks following instructions. These benchmarks are crucial for developing and evaluating the performance of VLN models, highlighting the diversity of environments and the complexity of navigation tasks guided by language.","In Vision-and-Language Navigation (VLN) benchmarks, agents are tasked with navigating environments based on natural language instructions. These instructions are detailed, guiding the agent through complex paths like ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" The R2R dataset, created by Anderson et al. (2018b) using the Matterport3D simulator (Chang et al., 2017), is a foundational dataset where an agent navigates a house by moving through nodes that present panoramic views. This dataset has been expanded upon to create additional benchmarks such as Roomfor-Room, which features longer trajectories (Jain et al., 2019), and others like Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), and TOUCHDOWN (Chen et al., 2019), which introduces navigation in a streetview simulation of New York City. Outdoor environments, represented in datasets like TOUCHDOWN, StreetLearn (Mirowski et al., 2019; Mehta et al., 2020), StreetNav (Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021), are based on Google Street View and are noted for their complexity and the abundance of objects. Additionally, there are datasets for drone navigation, such as LANI (Misra et al., 2018), which is set in a 3D synthetic environment where drones follow natural language instructions to navigate between landmarks, typically within synthetic environments like Unity3D (Blukis et al., 2018).","Questions:

What are the key characteristics and datasets of Vision-and-Language Navigation (VLN) benchmarks?

Answer:

In Vision-and-Language Navigation (VLN) benchmarks, agents are tasked with navigating environments based on natural language instructions. These instructions are detailed, guiding the agent through complex paths like ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" The R2R dataset, created by Anderson et al. (2018b) using the Matterport3D simulator (Chang et al., 2017), is a foundational dataset where an agent navigates a house by moving through nodes that present panoramic views. This dataset has been expanded upon to create additional benchmarks such as Roomfor-Room, which features longer trajectories (Jain et al., 2019), and others like Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), and TOUCHDOWN (Chen et al., 2019), which introduces navigation in a streetview simulation of New York City. Outdoor environments, represented in datasets like TOUCHDOWN, StreetLearn (Mirowski et al., 2019; Mehta et al., 2020), StreetNav (Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021), are based on Google Street View and are noted for their complexity and the abundance of objects. Additionally, there are datasets for drone navigation, such as LANI (Misra et al., 2018), which is set in a 3D synthetic environment where drones follow natural language instructions to navigate between landmarks, typically within synthetic environments like Unity3D (Blukis et al., 2018).","Vision-and-Language Navigation (VLN) benchmarks typically involve an agent following a natural language instruction throughout the navigation process. These instructions are detailed, guiding the agent through various environments to reach a specified goal. The R2R dataset, created by Anderson et al. (2018b) using the Matterport3D simulator (Chang et al., 2017), is a foundational dataset where an embodied agent navigates a house by moving through nodes that present panoramic views. This dataset has been expanded upon to create additional benchmarks such as Room-for-Room, which features longer trajectories (Jain et al., 2019), and others including Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021), LANI (Misra et al., 2018), RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a), and IQA. Outdoor environments, represented in datasets like TOUCHDOWN, StreetLearn, StreetNav, and Talk2Nav, are typically more complex and object-rich than indoor environments, often utilizing Google Street View for photo-realistic navigation challenges. Additionally, there are datasets like LANI (Misra et al., 2018) that focus on drone navigation within synthetic 3D environments, guiding drones through landmarks based on natural language instructions.","Questions:

What are the key characteristics and datasets of Vision-and-Language Navigation (VLN) benchmarks?

Answer:

Vision-and-Language Navigation (VLN) benchmarks typically involve an agent following a natural language instruction throughout the navigation process. These instructions are detailed, guiding the agent through various environments to reach a specified goal. The R2R dataset, created by Anderson et al. (2018b) using the Matterport3D simulator (Chang et al., 2017), is a foundational dataset where an embodied agent navigates a house by moving through nodes that present panoramic views. This dataset has been expanded upon to create additional benchmarks such as Room-for-Room, which features longer trajectories (Jain et al., 2019), and others including Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021), LANI (Misra et al., 2018), RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a), and IQA. Outdoor environments, represented in datasets like TOUCHDOWN, StreetLearn, StreetNav, and Talk2Nav, are typically more complex and object-rich than indoor environments, often utilizing Google Street View for photo-realistic navigation challenges. Additionally, there are datasets like LANI (Misra et al., 2018) that focus on drone navigation within synthetic 3D environments, guiding drones through landmarks based on natural language instructions."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,s1,Competing models of C-S,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5', 'p1.6', 'p1.7']","['For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.', ""2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007)."", ""3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '"", 'Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).', 'For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.', '3 Why do speakers code-switch?', ""In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997)."", 'According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.']","For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.

2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).

3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).

For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.

3 Why do speakers code-switch?

In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).

According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.","(p1.0) For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.

(p1.1) 2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).

(p1.2) 3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

(p1.3) Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).

(p1.4) For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.

(p1.5) 3 Why do speakers code-switch?

(p1.6) In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).

(p1.7) According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.","[[], ['b71', None, 'b3', 'b43', 'b16'], [], ['b37', None, 'b32'], ['b32', 'b33', None, 'b31', 'b53', 'b57'], [], ['b9', 'b42', 'b44', 'b57', 'b82'], ['b79', 'b5', 'b7', None, 'b10', 'b52', 'b30', 'b65']]","[[], ['b71', None, 'b3', 'b43', 'b16'], [], ['b37', None, 'b32'], ['b32', 'b33', None, 'b31', 'b53', 'b57'], [], ['b9', 'b42', 'b44', 'b57', 'b82'], ['b79', 'b5', 'b7', None, 'b10', 'b52', 'b30', 'b65']]",27,"1. For linguists, the specific ways in which languages are switched matters.
2. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1.
3. In fact, it may not signal multilingualism at all, simply borrowing.
4. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.2.
5. This is a good baile! 'This is a good dance party!'
6. (Solorio and Liu, 2008) To produce example (Solorio and Liu, 2008)4, the speaker needs to know only one Spanish word.
7. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from.
8. NLP scholars are not always concerned with the difference between examples (1) and (Solorio and Liu, 2008)4 so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997)
9. as the Matrix Language Frame (MLF) model.
10. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause.
11. Thus, it would not apply to the alternational English-Afrikaans C-S in example (Solorio and Liu, 2008)0 as each clause is in a separate language (Solorio and Liu, 2008)1.3.
12. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '
13. Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds.
14. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Solorio and Liu, 2008)2.
15. Bullock et al. (Solorio and Liu, 2008)3 computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (Solorio and Liu, 2008)4.
16. For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order.
17. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (Solorio and Liu, 2008)6 and the Functional Head Constraint (Solorio and Liu, 2008)7.
18. Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Solorio and Liu, 2008)8.
19. Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (Solorio and Liu, 2008)9, in some NLP tag sets (Al-Ghamdi et al., 2016).
20. This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages.
21. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018).
22. In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.
23. 3 Why do speakers code-switch? In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages.
24. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome.
25. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns.
26. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech.
27. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980).
28. These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so).
29. Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).
30. According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices.
31. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations.
32. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups.
33. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns.
34. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently.
35. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013).
36. Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014).
37. From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.","A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies##
Competing models of C-S##
For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.

2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).

3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).

For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.

3 Why do speakers code-switch?

In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).

According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.",What motivates speakers to engage in code-switching across languages?,1. What motivates speakers to engage in code-switching across languages?,"Questions:

1. What motivates speakers to engage in code-switching across languages?

Answer:

(p1.0) For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.

(p1.1) 2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).

(p1.2) 3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

(p1.3) Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).

(p1.4) For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.

(p1.5) 3 Why do speakers code-switch?

(p1.6) In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).

(p1.7) According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.","A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies##
Competing models of C-S##
For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.",What linguistic considerations are crucial in analyzing code-switching phenomena?,1. What linguistic considerations are crucial in analyzing code-switching phenomena?,"Questions:

1. What linguistic considerations are crucial in analyzing code-switching phenomena?

Answer:

(p1.0) For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.

(p1.1) 2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).

(p1.2) 3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

(p1.3) Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).

(p1.4) For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.

(p1.5) 3 Why do speakers code-switch?

(p1.6) In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).

(p1.7) According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.","A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies##
Competing models of C-S##
For linguists, the specific ways in which languages are switched matters.

2. This is a good baile! 'This is a good dance party!'

3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds.

For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order.

3 Why do speakers code-switch?

In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages.

According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices.",What factors influence speakers' code-switching behavior according to linguistic research?,1. What factors influence speakers' code-switching behavior according to linguistic research?,"Questions:

1. What factors influence speakers' code-switching behavior according to linguistic research?

Answer:

(p1.0) For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.

(p1.1) 2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).

(p1.2) 3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

(p1.3) Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).

(p1.4) For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.

(p1.5) 3 Why do speakers code-switch?

(p1.6) In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).

(p1.7) According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.","A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies##
Competing models of C-S##
For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.##
2. This is a good baile! 'This is a good dance party!'

3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds.

For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order.

3 Why do speakers code-switch?

In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages.

According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices.",What are the linguistic and social factors influencing code-switching behavior?,1. What are the linguistic and social factors influencing code-switching behavior?,"Questions:

1. What are the linguistic and social factors influencing code-switching behavior?

Answer:

(p1.0) For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.

(p1.1) 2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).

(p1.2) 3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

(p1.3) Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).

(p1.4) For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.

(p1.5) 3 Why do speakers code-switch?

(p1.6) In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).

(p1.7) According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.","A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies
##
Competing models of C-S
##
For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.
2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).
3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '
Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).
For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.
3 Why do speakers code-switch?
In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).
According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.
##
What are the linguistic and social factors influencing code-switching behavior?","Linguistic and social factors significantly influence code-switching (C-S) behavior. Linguistically, the complexity of switching between languages varies, with simpler insertions like a single word from one language into another being less syntactically complex than full sentence alternations that require knowledge of word order, case markers, and the grammatical frameworks of both languages involved. The Matrix Language Frame (MLF) model, which posits that one language forms the grammatical structure into which elements from another language are embedded, is a dominant theory in computational approaches to C-S, despite its limitations and the existence of competing theories. These include the Equivalence Constraint, which suggests C-S occurs where the linear structures of languages coincide, and the Government and Functional Head Constraints, which differentiate between lexical and functional elements in language switching. Constraint-free theories argue that grammatical requirements of the languages involved are the only restrictions on C-S.

Socially, the motivations behind C-S encompass pragmatic, interactional, and socio-indexical functions, such as signaling in-group identity, quoting speakers, or altering the intensity of a message. Factors like topic, type of interaction, interlocutors, role relationships, and communication channels also play a crucial role in C-S choices. Theories such as politeness theory and macro-social perspectives highlight how prestige, face-saving moves, and political choices can influence C-S patterns, with variations observed across different generations, communities, and even based on speaker-dependent factors like gender and identity. Computational studies have further explored how these factors can predict C-S choices in multilingual interactions.","Questions:

What are the linguistic and social factors influencing code-switching behavior?

Answer:

Linguistic and social factors significantly influence code-switching (C-S) behavior. Linguistically, the complexity of switching between languages varies, with simpler insertions like a single word from one language into another being less syntactically complex than full sentence alternations that require knowledge of word order, case markers, and the grammatical frameworks of both languages involved. The Matrix Language Frame (MLF) model, which posits that one language forms the grammatical structure into which elements from another language are embedded, is a dominant theory in computational approaches to C-S, despite its limitations and the existence of competing theories. These include the Equivalence Constraint, which suggests C-S occurs where the linear structures of languages coincide, and the Government and Functional Head Constraints, which differentiate between lexical and functional elements in language switching. Constraint-free theories argue that grammatical requirements of the languages involved are the only restrictions on C-S.

Socially, the motivations behind C-S encompass pragmatic, interactional, and socio-indexical functions, such as signaling in-group identity, quoting speakers, or altering the intensity of a message. Factors like topic, type of interaction, interlocutors, role relationships, and communication channels also play a crucial role in C-S choices. Theories such as politeness theory and macro-social perspectives highlight how prestige, face-saving moves, and political choices can influence C-S patterns, with variations observed across different generations, communities, and even based on speaker-dependent factors like gender and identity. Computational studies have further explored how these factors can predict C-S choices in multilingual interactions.","Linguistic and social factors significantly influence code-switching (C-S) behavior. Linguistically, the complexity of switching varies, from simple insertion of a word from one language into another to more syntactically intricate integrations requiring knowledge of word order and grammatical rules from both languages involved. The Matrix Language Frame (MLF) model, proposed by Joshi (1982) and adapted by Myers-Scotton (1997), suggests that one language provides the grammatical structure into which elements from another language are embedded. However, this model faces criticism for its empirical and theoretical limitations, such as the difficulty in consistently identifying a matrix language and its limited scope. Alternative syntactic theories, like the Equivalence Constraint (Poplack, 1980) and the Government and Functional Head Constraints (DiSciullo et al., 1986; Belazi et al., 1994), offer different perspectives on how C-S occurs, focusing on points where the structures of the languages involved align or on the roles of lexical and functional elements. Constraint-free theories argue that grammatical requirements of the contributing languages are the only restrictions on C-S.

Socially and cognitively, speakers switch languages for various reasons, including to achieve pragmatic, interactional, and socio-indexical functions. Code-switching can signal in-group identity, serve to quote another speaker, capture attention, or modify the intensity of a message. Factors such as topic, type of interaction, interlocutors, role relationships, and communication channels influence C-S choices. Social prestige, politeness, political identity, generational differences within communities, and individual speaker attributes like gender and identity also play crucial roles. Computational studies have begun to explore how these factors can predict C-S patterns in specific bilingual communities, highlighting the speaker-dependent nature of C-S.","Questions:

What are the linguistic and social factors influencing code-switching behavior?

Answer:

Linguistic and social factors significantly influence code-switching (C-S) behavior. Linguistically, the complexity of switching varies, from simple insertion of a word from one language into another to more syntactically intricate integrations requiring knowledge of word order and grammatical rules from both languages involved. The Matrix Language Frame (MLF) model, proposed by Joshi (1982) and adapted by Myers-Scotton (1997), suggests that one language provides the grammatical structure into which elements from another language are embedded. However, this model faces criticism for its empirical and theoretical limitations, such as the difficulty in consistently identifying a matrix language and its limited scope. Alternative syntactic theories, like the Equivalence Constraint (Poplack, 1980) and the Government and Functional Head Constraints (DiSciullo et al., 1986; Belazi et al., 1994), offer different perspectives on how C-S occurs, focusing on points where the structures of the languages involved align or on the roles of lexical and functional elements. Constraint-free theories argue that grammatical requirements of the contributing languages are the only restrictions on C-S.

Socially and cognitively, speakers switch languages for various reasons, including to achieve pragmatic, interactional, and socio-indexical functions. Code-switching can signal in-group identity, serve to quote another speaker, capture attention, or modify the intensity of a message. Factors such as topic, type of interaction, interlocutors, role relationships, and communication channels influence C-S choices. Social prestige, politeness, political identity, generational differences within communities, and individual speaker attributes like gender and identity also play crucial roles. Computational studies have begun to explore how these factors can predict C-S patterns in specific bilingual communities, highlighting the speaker-dependent nature of C-S.","Linguistic and social factors significantly influence code-switching behavior. Linguistically, code-switching can range from simple insertions of a word from one language into another to more complex syntactic integrations that require knowledge of word order and case markers from both languages involved. The Matrix Language Frame (MLF) model, which posits that one language provides the grammatical frame into which elements from another language are embedded, is a dominant but contested theory in computational approaches to code-switching. Other linguistic theories, such as the Equivalence Constraint, focus on the syntactic compatibility between languages, while constraint-free theories argue that code-switching is only restricted by the grammatical rules of the involved languages.

Socially, code-switching is influenced by factors such as the speaker's intent, identity signaling, in-group membership, and the sociolinguistic context. Speakers may switch languages to convey specific information, emphasize a point, quote someone, or signal a political or social identity. Factors such as topic, type of interaction, interlocutors, role relationships, and communication channels also play a crucial role in code-switching choices. Additionally, social variables like prestige, politeness, generation, gender, and community norms can influence language choice and code-switching patterns. Computational studies have shown that code-switching is speaker-dependent and can vary significantly across different communities and contexts.","Questions:

What are the linguistic and social factors influencing code-switching behavior?

Answer:

Linguistic and social factors significantly influence code-switching behavior. Linguistically, code-switching can range from simple insertions of a word from one language into another to more complex syntactic integrations that require knowledge of word order and case markers from both languages involved. The Matrix Language Frame (MLF) model, which posits that one language provides the grammatical frame into which elements from another language are embedded, is a dominant but contested theory in computational approaches to code-switching. Other linguistic theories, such as the Equivalence Constraint, focus on the syntactic compatibility between languages, while constraint-free theories argue that code-switching is only restricted by the grammatical rules of the involved languages.

Socially, code-switching is influenced by factors such as the speaker's intent, identity signaling, in-group membership, and the sociolinguistic context. Speakers may switch languages to convey specific information, emphasize a point, quote someone, or signal a political or social identity. Factors such as topic, type of interaction, interlocutors, role relationships, and communication channels also play a crucial role in code-switching choices. Additionally, social variables like prestige, politeness, generation, gender, and community norms can influence language choice and code-switching patterns. Computational studies have shown that code-switching is speaker-dependent and can vary significantly across different communities and contexts.","Linguists are interested in the specific ways languages are switched in code-switching (C-S), distinguishing between simple insertions of a word from one language into another and more complex syntactic integrations. The Matrix Language Frame (MLF) model, proposed by Joshi (1982) and adapted by Myers-Scotton (1997), is widely used in computational approaches to C-S. It posits that one language (the matrix language) provides the grammatical structure into which elements from another language are inserted. However, this model faces criticism for its empirical and theoretical limitations, such as the difficulty in consistently identifying a matrix language and its limited scope (Meakins, 2012; Bhat et al., 2016; Adamou, 2016; MacSwan, 2000; Auer and Muhamedova, 2005). 

Other syntactic theories, like the Equivalence Constraint (Poplack, 1980), suggest that C-S tends to occur at points where the linear structures of the contributing languages coincide. The Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994) focus on the differences between lexical and functional elements, and incorporating these theories in NLP experiments has shown to improve model accuracy (Li and Fung, 2014; Bhat et al., 2016). 

Beyond linguistic constraints, social and cognitive motivations also play a significant role in C-S. Speakers may switch languages for pragmatic reasons, to signal in-group identity, or to achieve certain interactional effects (Myslín and Levy, 2015; Muysken, 1995; Poplack, 1980). Factors such as topic, type of interaction, and interlocutors' relationships influence C-S choices (Clyne, 1991; Lavric, 2007; Heller, 1992; Gardner-Chloros and Edwards, 2004; Sebba, 1998). Computational studies have also shown that C-S is speaker-dependent and influenced by factors like gender and identity (Vu et al., 2013; Finnis, 2014; Papalexakis et al., 2014).","Questions:

What are the linguistic and social factors influencing code-switching behavior?

Answer:

Linguists are interested in the specific ways languages are switched in code-switching (C-S), distinguishing between simple insertions of a word from one language into another and more complex syntactic integrations. The Matrix Language Frame (MLF) model, proposed by Joshi (1982) and adapted by Myers-Scotton (1997), is widely used in computational approaches to C-S. It posits that one language (the matrix language) provides the grammatical structure into which elements from another language are inserted. However, this model faces criticism for its empirical and theoretical limitations, such as the difficulty in consistently identifying a matrix language and its limited scope (Meakins, 2012; Bhat et al., 2016; Adamou, 2016; MacSwan, 2000; Auer and Muhamedova, 2005). 

Other syntactic theories, like the Equivalence Constraint (Poplack, 1980), suggest that C-S tends to occur at points where the linear structures of the contributing languages coincide. The Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994) focus on the differences between lexical and functional elements, and incorporating these theories in NLP experiments has shown to improve model accuracy (Li and Fung, 2014; Bhat et al., 2016). 

Beyond linguistic constraints, social and cognitive motivations also play a significant role in C-S. Speakers may switch languages for pragmatic reasons, to signal in-group identity, or to achieve certain interactional effects (Myslín and Levy, 2015; Muysken, 1995; Poplack, 1980). Factors such as topic, type of interaction, and interlocutors' relationships influence C-S choices (Clyne, 1991; Lavric, 2007; Heller, 1992; Gardner-Chloros and Edwards, 2004; Sebba, 1998). Computational studies have also shown that C-S is speaker-dependent and influenced by factors like gender and identity (Vu et al., 2013; Finnis, 2014; Papalexakis et al., 2014).","Linguistic and social factors significantly influence code-switching (C-S) behavior. Linguistically, the complexity of C-S varies, from simple insertion of a word from one language into another to more syntactically integrated forms that require knowledge of word order and case markers from both languages involved. The Matrix Language Frame (MLF) model, which posits that one language provides the grammatical structure into which elements from another language are embedded, is a dominant but contested theory in computational approaches to C-S. Critics argue that identifying a consistent matrix language is challenging, and the model's scope is limited. Alternative syntactic theories, such as the Equivalence Constraint, the Government Constraint, and the Functional Head Constraint, suggest that C-S occurs at points where the structures of the involved languages coincide or based on the distinction between lexical and functional elements. These theories have been shown to improve the accuracy of computational models when incorporated into NLP experiments.

Social and cognitive motivations also play a crucial role in why speakers switch languages. These motivations can influence the structural outcome of C-S. Speakers may switch languages for pragmatic reasons, to signal in-group identity, or to achieve specific interactional goals, such as quoting another speaker, capturing attention, or modifying the intensity of a message. Factors such as the topic of conversation, the type of interaction, the relationship between interlocutors, and the communication channel can influence C-S choices. Social factors, including prestige, political choices, generational differences within communities, and individual identity, including gender, also affect language choice and C-S patterns. Computational studies have further demonstrated that C-S is speaker-dependent and can vary significantly across different communities and contexts.","Questions:

What are the linguistic and social factors influencing code-switching behavior?

Answer:

Linguistic and social factors significantly influence code-switching (C-S) behavior. Linguistically, the complexity of C-S varies, from simple insertion of a word from one language into another to more syntactically integrated forms that require knowledge of word order and case markers from both languages involved. The Matrix Language Frame (MLF) model, which posits that one language provides the grammatical structure into which elements from another language are embedded, is a dominant but contested theory in computational approaches to C-S. Critics argue that identifying a consistent matrix language is challenging, and the model's scope is limited. Alternative syntactic theories, such as the Equivalence Constraint, the Government Constraint, and the Functional Head Constraint, suggest that C-S occurs at points where the structures of the involved languages coincide or based on the distinction between lexical and functional elements. These theories have been shown to improve the accuracy of computational models when incorporated into NLP experiments.

Social and cognitive motivations also play a crucial role in why speakers switch languages. These motivations can influence the structural outcome of C-S. Speakers may switch languages for pragmatic reasons, to signal in-group identity, or to achieve specific interactional goals, such as quoting another speaker, capturing attention, or modifying the intensity of a message. Factors such as the topic of conversation, the type of interaction, the relationship between interlocutors, and the communication channel can influence C-S choices. Social factors, including prestige, political choices, generational differences within communities, and individual identity, including gender, also affect language choice and C-S patterns. Computational studies have further demonstrated that C-S is speaker-dependent and can vary significantly across different communities and contexts."
248426721,What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Computer Science,https://www.semanticscholar.org/paper/29a369d83a7d6a49f6a3259bee23bd4d95db0b16,s1,Relation Extraction Datasets Survey,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5', 'p1.6', 'p1.7', 'p1.8', 'p1.9', 'p1.10', 'p1.11']","['RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.', 'Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.', '3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).', 'Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.', 'The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.', 'Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).', 'All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).', 'Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.', 'Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.', 'Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:', '1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).', 'In the case study of this paper, given the scientific datasets available, we focus on the first setup.']","RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.","(p1.0) RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

(p1.1) Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

(p1.2) 3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

(p1.3) Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

(p1.4) The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

(p1.5) Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

(p1.6) All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

(p1.7) Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

(p1.8) Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

(p1.9) Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

(p1.10) 1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

(p1.11) In the case study of this paper, given the scientific datasets available, we focus on the first setup.","[[None, 'b30'], ['b29', None, 'b59', 'b10'], ['b7', None], [None, 'b1', 'b18'], ['b32', 'b11'], ['b6', 'b7', None, 'b31', 'b55'], ['b48', 'b52'], [None], [], [], ['b25'], []]","[[None, 'b30'], ['b29', None, 'b59', 'b10'], ['b7', None], [None, 'b1', 'b18'], ['b32', 'b11'], ['b6', 'b7', None, 'b31', 'b55'], ['b48', 'b52'], [None], [], [], ['b25'], []]",22,"1. RE has been broadly studied in the last decades and many datasets were published.
2. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia.
3. An overview of the datasets is given in Table 1.
4. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work.
5. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works.
6. It contains annotations for named entities and relations in news articles.
7. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004).
8. It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic.
9. The corpus is divided into six domains.
10. Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010).
11. It contains over 1.8 million articles by the NYT between 1987 and 2007.
12. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (Roth and Yih, 2004)0 published manually annotated versions of the test set in order to perform a more accurate evaluation.
13. 3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far.
14. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (2)1.
15. For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals.
16. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (2)3.Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks.
17. In SemEval-2017 Task 10 Augenstein et al. (2)4 proposed a dataset for the identification of keyphrases and considered two generic relations (2)5.
18. The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields.
19. The year after, Gábor et al. (3)3 proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7.
20. The data will be described in further detail in Section 4.1.
21. Following the same line, Luan et al. (3)3 published SCIERC, which is a scientific RE dataset further annotated for coreference resolution.
22. It contains abstracts from scientific AI-related conferences.
23. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (2)8.
24. We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.
25. The Wikipedia domain has been first introduced in 2013.
26. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia.
27. More recently, Kassner et al. (Roth and Yih, 2004)3 proposed mLAMA, a multilingual version (3)0 of GoogleRE with the purpose of investigating knowledge in pretrained language models.
28. The multi-lingual dimension is gaining more interest for RE.
29. Following this trend, Seganti et al. (Roth and Yih, 2004)3 presented SMiLER, a multilingual dataset (3)2 from Wikipedia with relations belonging to nine domains.
30. Previous datasets were restricted to the same label collection in the training set and in the test set.
31. To address this gap and make RE experimental scenarios more realistic,
32. Han et al. (3)3 -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (3)7 Newswire and web text 42 FSL TACRED Sabo et al. (Roth and Yih, 2004)3 TACRED   Back to the news domain, Zhang et al. (3)7 published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs.
33. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models.
34. Sabo et al. (Roth and Yih, 2004)3 used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (3)9.
35. All datasets so far present a sentence level annotation.
36. To address this, Yao et al. (Roth and Yih, 2004)0 published DocRED, a document-level RE dataset from Wikipedia and Wikidata.
37. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level.
38. In addition to RE, DocRED annotates coreference chains.
39. DWIE by Zaporojets et al. (Roth and Yih, 2004)3 is another document-level dataset, specifically designed for multi-task IE (Roth and Yih, 2004)2.
40. Lastly, there are works focusing on creating datasets for specific RE aspects.
41. Cheng et al. (Roth and Yih, 2004)3, for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.
42. Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia.
43. Similarly, we observe the emerging trend for FSL.
44. Different datasets lend themselves to study different aspects of the task.
45. Concerning crossdomain RE, we propose to distinguish three setups:1.
46. Data from different domains, but same relation types, which are general enough to be present in each domain (Roth and Yih, 2004)4 (Roth and Yih, 2004)5.
47. In the case study of this paper, given the scientific datasets available, we focus on the first setup.","What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification##
Relation Extraction Datasets Survey##
RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.",What trends and challenges are observed in the evolution of relation extraction datasets?,1. What trends and challenges are observed in the evolution of relation extraction datasets?,"Questions:

1. What trends and challenges are observed in the evolution of relation extraction datasets?

Answer:

(p1.0) RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

(p1.1) Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

(p1.2) 3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

(p1.3) Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

(p1.4) The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

(p1.5) Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

(p1.6) All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

(p1.7) Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

(p1.8) Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

(p1.9) Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

(p1.10) 1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

(p1.11) In the case study of this paper, given the scientific datasets available, we focus on the first setup.","What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification##
Relation Extraction Datasets Survey##
RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.",How are relation extraction datasets categorized based on their sources?,1. How are relation extraction datasets categorized based on their sources?,"Questions:

1. How are relation extraction datasets categorized based on their sources?

Answer:

(p1.0) RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

(p1.1) Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

(p1.2) 3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

(p1.3) Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

(p1.4) The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

(p1.5) Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

(p1.6) All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

(p1.7) Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

(p1.8) Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

(p1.9) Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

(p1.10) 1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

(p1.11) In the case study of this paper, given the scientific datasets available, we focus on the first setup.","What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification##
Relation Extraction Datasets Survey##
RE has been broadly studied in the last decades and many datasets were published.

Another widely used dataset is The New York Times (NYT)

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far.

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks.

The Wikipedia domain has been first introduced in 2013.

Previous datasets were restricted to the same label collection in the training set and in the test set.

All datasets so far present a sentence level annotation.

Lastly, there are works focusing on creating datasets for specific RE aspects.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia.

Different datasets lend themselves to study different aspects of the task.

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.",How have relation extraction datasets evolved in terms of domain diversity and annotation levels?,1. How have relation extraction datasets evolved in terms of domain diversity and annotation levels?,"Questions:

1. How have relation extraction datasets evolved in terms of domain diversity and annotation levels?

Answer:

(p1.0) RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

(p1.1) Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

(p1.2) 3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

(p1.3) Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

(p1.4) The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

(p1.5) Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

(p1.6) All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

(p1.7) Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

(p1.8) Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

(p1.9) Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

(p1.10) 1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

(p1.11) In the case study of this paper, given the scientific datasets available, we focus on the first setup.","What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification##
Relation Extraction Datasets Survey##
RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.##
Another widely used dataset is The New York Times (NYT)

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far.

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks.

The Wikipedia domain has been first introduced in 2013.

Previous datasets were restricted to the same label collection in the training set and in the test set.

All datasets so far present a sentence level annotation.

Lastly, there are works focusing on creating datasets for specific RE aspects.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia.

Different datasets lend themselves to study different aspects of the task.

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.",How have relation extraction datasets evolved across different domains?,1. How have relation extraction datasets evolved across different domains?,"Questions:

1. How have relation extraction datasets evolved across different domains?

Answer:

(p1.0) RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

(p1.1) Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

(p1.2) 3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

(p1.3) Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

(p1.4) The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

(p1.5) Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

(p1.6) All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

(p1.7) Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

(p1.8) Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

(p1.9) Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

(p1.10) 1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

(p1.11) In the case study of this paper, given the scientific datasets available, we focus on the first setup.","What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification
##
Relation Extraction Datasets Survey
##
RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.
Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.
3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).
Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.
The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.
Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).
All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).
Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.
Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.
Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:
1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).
In the case study of this paper, given the scientific datasets available, we focus on the first setup.
##
How have relation extraction datasets evolved across different domains?","Relation extraction (RE) datasets have significantly evolved over the years, expanding across various domains such as news and web, scientific publications, and Wikipedia. Initially, datasets like the CoNLL 2004 and the ACE dataset focused on news articles, broadcast transcripts, and newspaper data, marking some of the earliest efforts in the field. The New York Times Annotated Corpus introduced a distant supervision approach, leveraging over 1.8 million articles to create a dataset with a focus on named entity and relation annotations. SemEval shared tasks have also played a crucial role, with datasets targeting semantic relations between nominals and providing larger datasets for relation classification (RC) over the years.

In the scientific domain, datasets have emerged more recently, with notable examples being ScienceIE, which focuses on keyphrase identification and includes relations like HYPONYM-OF and SYNONYM-OF, and datasets from SemEval tasks that concentrate on abstracts from scientific papers. SCIERC, another significant dataset, includes annotations for coreference resolution in addition to RE, focusing on AI-related conference abstracts.

The Wikipedia domain saw the introduction of datasets like GoogleRE and its multilingual version, mLAMA, aiming to explore knowledge in pretrained language models across 53 languages. SMiLER extended this trend with a multilingual dataset covering nine domains.

Recent developments have also addressed the realism of RE datasets by introducing variations in label collections between training and test sets, as seen in the TACRED dataset and its subsequent use in creating more realistic scenarios for few-shot learning (FSL). Document-level datasets like DocRED and DWIE have been introduced to tackle both intra- and inter-sentence relations, increasing the complexity of RE tasks.

Overall, the evolution of RE datasets reflects a broadening of focus from news and web texts to scientific literature and Wikipedia, alongside a growing interest in multilingual and more complex, document-level annotation tasks. This expansion not only demonstrates the field's progression but also highlights the diverse applications and challenges in relation extraction research.","Questions:

How have relation extraction datasets evolved across different domains?

Answer:

Relation extraction (RE) datasets have significantly evolved over the years, expanding across various domains such as news and web, scientific publications, and Wikipedia. Initially, datasets like the CoNLL 2004 and the ACE dataset focused on news articles, broadcast transcripts, and newspaper data, marking some of the earliest efforts in the field. The New York Times Annotated Corpus introduced a distant supervision approach, leveraging over 1.8 million articles to create a dataset with a focus on named entity and relation annotations. SemEval shared tasks have also played a crucial role, with datasets targeting semantic relations between nominals and providing larger datasets for relation classification (RC) over the years.

In the scientific domain, datasets have emerged more recently, with notable examples being ScienceIE, which focuses on keyphrase identification and includes relations like HYPONYM-OF and SYNONYM-OF, and datasets from SemEval tasks that concentrate on abstracts from scientific papers. SCIERC, another significant dataset, includes annotations for coreference resolution in addition to RE, focusing on AI-related conference abstracts.

The Wikipedia domain saw the introduction of datasets like GoogleRE and its multilingual version, mLAMA, aiming to explore knowledge in pretrained language models across 53 languages. SMiLER extended this trend with a multilingual dataset covering nine domains.

Recent developments have also addressed the realism of RE datasets by introducing variations in label collections between training and test sets, as seen in the TACRED dataset and its subsequent use in creating more realistic scenarios for few-shot learning (FSL). Document-level datasets like DocRED and DWIE have been introduced to tackle both intra- and inter-sentence relations, increasing the complexity of RE tasks.

Overall, the evolution of RE datasets reflects a broadening of focus from news and web texts to scientific literature and Wikipedia, alongside a growing interest in multilingual and more complex, document-level annotation tasks. This expansion not only demonstrates the field's progression but also highlights the diverse applications and challenges in relation extraction research.","Relation extraction (RE) datasets have significantly evolved over the years, expanding across various domains such as news and web, scientific publications, and Wikipedia. Initially, datasets like the CoNLL 2004 and the ACE dataset, which emerged in the same year, focused on news articles and included annotations for named entities and relations. The ACE dataset, in particular, covered broadcast transcripts, newswire, and newspaper data in multiple languages and was divided into six domains. Another notable dataset is The New York Times Annotated Corpus, introduced by Riedel et al. (2010), which comprises over 1.8 million articles and employs a distant supervision approach for relation extraction, with subsequent versions providing manually annotated test sets for more accurate evaluation.

In the realm of scientific publications, since 2017, there have been notable additions such as the ScienceIE, SemEval-2018 Task 7 corpus, and SCIERC datasets, focusing on keyphrase identification, relation classification (RC), and RE, including coreference resolution within scientific paper abstracts. These datasets have contributed to expanding the scope of RE research within the scientific domain.

The Wikipedia domain saw the introduction of GoogleRE in 2013, followed by multilingual datasets like mLAMA and SMiLER, which aim to explore knowledge in pretrained language models and include relations across nine domains in up to 53 languages. These developments highlight the growing interest in multilingual RE.

Recent efforts have also been directed towards creating more realistic experimental scenarios for RE, such as the TACRED dataset, which is suitable for neural models and focuses on newswire and web text. Additionally, document-level RE datasets like DocRED and DWIE have been developed to address both intra- and inter-sentence relations, increasing the complexity of relation extraction tasks.

Overall, the evolution of RE datasets reflects a shift from focusing primarily on news texts to incorporating a broader range of domains, including scientific literature and Wikipedia. This expansion not only enriches the field of RE but also opens up new avenues for cross-domain research, highlighting the importance of general relation types that are applicable across different domains.","Questions:

How have relation extraction datasets evolved across different domains?

Answer:

Relation extraction (RE) datasets have significantly evolved over the years, expanding across various domains such as news and web, scientific publications, and Wikipedia. Initially, datasets like the CoNLL 2004 and the ACE dataset, which emerged in the same year, focused on news articles and included annotations for named entities and relations. The ACE dataset, in particular, covered broadcast transcripts, newswire, and newspaper data in multiple languages and was divided into six domains. Another notable dataset is The New York Times Annotated Corpus, introduced by Riedel et al. (2010), which comprises over 1.8 million articles and employs a distant supervision approach for relation extraction, with subsequent versions providing manually annotated test sets for more accurate evaluation.

In the realm of scientific publications, since 2017, there have been notable additions such as the ScienceIE, SemEval-2018 Task 7 corpus, and SCIERC datasets, focusing on keyphrase identification, relation classification (RC), and RE, including coreference resolution within scientific paper abstracts. These datasets have contributed to expanding the scope of RE research within the scientific domain.

The Wikipedia domain saw the introduction of GoogleRE in 2013, followed by multilingual datasets like mLAMA and SMiLER, which aim to explore knowledge in pretrained language models and include relations across nine domains in up to 53 languages. These developments highlight the growing interest in multilingual RE.

Recent efforts have also been directed towards creating more realistic experimental scenarios for RE, such as the TACRED dataset, which is suitable for neural models and focuses on newswire and web text. Additionally, document-level RE datasets like DocRED and DWIE have been developed to address both intra- and inter-sentence relations, increasing the complexity of relation extraction tasks.

Overall, the evolution of RE datasets reflects a shift from focusing primarily on news texts to incorporating a broader range of domains, including scientific literature and Wikipedia. This expansion not only enriches the field of RE but also opens up new avenues for cross-domain research, highlighting the importance of general relation types that are applicable across different domains.","Relation extraction (RE) datasets have evolved significantly across different domains, including news and web, scientific publications, and Wikipedia. Initially, datasets like the CoNLL 2004 and the ACE dataset focused on news articles, broadcast transcripts, and newspapers, covering a range of entities, relations, and events in multiple languages. The New York Times Annotated Corpus introduced the use of distant supervision with Freebase for creating a large-scale dataset from news articles. SemEval shared tasks have contributed to the domain by focusing on semantic relations between nominals and revising annotation guidelines to provide larger and more diverse datasets.

In the scientific domain, datasets like ScienceIE, SemEval-2018 Task 7, and SCIERC have emerged, focusing on keyphrases, relations in scientific paper abstracts, and coreference resolution, respectively. These datasets aim to address the unique challenges of scientific text, such as the identification of keyphrases and the extraction of relations from dense, domain-specific language.

The Wikipedia domain was first introduced with GoogleRE, followed by multilingual datasets like mLAMA and SMiLER, which expanded the scope of RE to include multiple languages and a wider range of relations across different domains.

Recent efforts have also been made to create more realistic RE experimental scenarios by introducing datasets like TACRED, which is built over newswire and web text with crowdsourced annotations, and DocRED, a document-level RE dataset that annotates both intra- and inter-sentence relations as well as coreference chains, increasing the challenge level for RE tasks.

Overall, RE datasets have shifted from focusing primarily on news text to incorporating a wider variety of domains, including scientific publications and Wikipedia. This evolution reflects the growing complexity and diversity of the RE task, as well as the need for models to generalize across different types of text and linguistic phenomena.","Questions:

How have relation extraction datasets evolved across different domains?

Answer:

Relation extraction (RE) datasets have evolved significantly across different domains, including news and web, scientific publications, and Wikipedia. Initially, datasets like the CoNLL 2004 and the ACE dataset focused on news articles, broadcast transcripts, and newspapers, covering a range of entities, relations, and events in multiple languages. The New York Times Annotated Corpus introduced the use of distant supervision with Freebase for creating a large-scale dataset from news articles. SemEval shared tasks have contributed to the domain by focusing on semantic relations between nominals and revising annotation guidelines to provide larger and more diverse datasets.

In the scientific domain, datasets like ScienceIE, SemEval-2018 Task 7, and SCIERC have emerged, focusing on keyphrases, relations in scientific paper abstracts, and coreference resolution, respectively. These datasets aim to address the unique challenges of scientific text, such as the identification of keyphrases and the extraction of relations from dense, domain-specific language.

The Wikipedia domain was first introduced with GoogleRE, followed by multilingual datasets like mLAMA and SMiLER, which expanded the scope of RE to include multiple languages and a wider range of relations across different domains.

Recent efforts have also been made to create more realistic RE experimental scenarios by introducing datasets like TACRED, which is built over newswire and web text with crowdsourced annotations, and DocRED, a document-level RE dataset that annotates both intra- and inter-sentence relations as well as coreference chains, increasing the challenge level for RE tasks.

Overall, RE datasets have shifted from focusing primarily on news text to incorporating a wider variety of domains, including scientific publications and Wikipedia. This evolution reflects the growing complexity and diversity of the RE task, as well as the need for models to generalize across different types of text and linguistic phenomena.","Relation extraction (RE) datasets have evolved significantly across various domains, including news and web, scientific publications, and Wikipedia. Initially, datasets like the CoNLL 2004 and the ACE dataset focused on news articles, broadcast transcripts, and newspaper data, covering a range of languages and domains. The New York Times Annotated Corpus introduced the use of distant supervision for creating RE datasets. SemEval shared tasks have contributed to the evolution of RE datasets by focusing on semantic relations between nominals and providing datasets for relation classification (RC) in web texts.

In the scientific domain, datasets like ScienceIE, SemEval-2018 Task 7, and SCIERC have been introduced, focusing on keyphrase extraction, relation classification, and coreference resolution within scientific paper abstracts. These datasets aim to address the lack of attention in cross-domain RE, particularly in the scientific field.

The Wikipedia domain saw the introduction of GoogleRE and its multilingual version, mLAMA, to explore knowledge in pretrained language models across multiple languages. SMiLER further expanded the multilingual aspect with a dataset covering 14 languages and relations from nine domains.

Recent efforts have also been made to create more realistic RE experimental scenarios, such as the TACRED dataset, which is suited for neural models and includes a diverse label collection. DocRED and DWIE have extended the scope to document-level RE, annotating both intra- and inter-sentence relations and integrating multiple information extraction tasks.

Overall, the evolution of RE datasets reflects a shift towards more diverse domains, including a growing interest in scientific and multilingual resources, and a move towards more complex and realistic annotation scenarios, such as document-level RE and fine-grained relation types.","Questions:

How have relation extraction datasets evolved across different domains?

Answer:

Relation extraction (RE) datasets have evolved significantly across various domains, including news and web, scientific publications, and Wikipedia. Initially, datasets like the CoNLL 2004 and the ACE dataset focused on news articles, broadcast transcripts, and newspaper data, covering a range of languages and domains. The New York Times Annotated Corpus introduced the use of distant supervision for creating RE datasets. SemEval shared tasks have contributed to the evolution of RE datasets by focusing on semantic relations between nominals and providing datasets for relation classification (RC) in web texts.

In the scientific domain, datasets like ScienceIE, SemEval-2018 Task 7, and SCIERC have been introduced, focusing on keyphrase extraction, relation classification, and coreference resolution within scientific paper abstracts. These datasets aim to address the lack of attention in cross-domain RE, particularly in the scientific field.

The Wikipedia domain saw the introduction of GoogleRE and its multilingual version, mLAMA, to explore knowledge in pretrained language models across multiple languages. SMiLER further expanded the multilingual aspect with a dataset covering 14 languages and relations from nine domains.

Recent efforts have also been made to create more realistic RE experimental scenarios, such as the TACRED dataset, which is suited for neural models and includes a diverse label collection. DocRED and DWIE have extended the scope to document-level RE, annotating both intra- and inter-sentence relations and integrating multiple information extraction tasks.

Overall, the evolution of RE datasets reflects a shift towards more diverse domains, including a growing interest in scientific and multilingual resources, and a move towards more complex and realistic annotation scenarios, such as document-level RE and fine-grained relation types.","Relation extraction (RE) datasets have evolved significantly across various domains, including news and web, scientific publications, and Wikipedia. Initially, datasets like the CoNLL 2004 and the ACE dataset focused on news articles, broadcast transcripts, and newspapers, covering multiple languages and domains. The New York Times Annotated Corpus introduced the use of distant supervision with Freebase for creating RE datasets. SemEval shared tasks have contributed datasets focusing on semantic relations between nominals and other linguistic elements.

In recent years, the scientific domain has gained attention with datasets like ScienceIE, which targets keyphrase identification and generic relations within journal articles across several scientific fields. SemEval-2018 Task 7 and SCIERC further expanded the scientific RE datasets, focusing on abstracts from scientific papers and AI-related conference proceedings, respectively. These datasets are particularly notable for their focus on coreference resolution in addition to relation extraction.

The Wikipedia domain was first explored with GoogleRE, leading to the development of multilingual datasets like mLAMA and SMiLER, which cover a wide range of languages and relations across multiple domains. These efforts reflect a growing interest in multilingual RE.

To address the limitations of previous datasets, which were restricted to the same label collection in both training and test sets, newer datasets like TACRED and DocRED have been introduced. TACRED focuses on newswire and web text with crowdsourced annotations, while DocRED is a document-level RE dataset that annotates both intra- and inter-sentence relations, as well as coreference chains, increasing the complexity of relation extraction tasks.

Overall, RE datasets have shifted from focusing primarily on news text to incorporating a broader range of domains, including scientific literature and Wikipedia. This evolution reflects the field's growing complexity and the diverse applications of relation extraction. Additionally, there's an emerging trend towards few-shot learning (FSL) and more challenging evaluation setups, such as document-level RE and datasets designed for specific RE aspects, indicating a move towards more realistic and varied experimental scenarios.","Questions:

How have relation extraction datasets evolved across different domains?

Answer:

Relation extraction (RE) datasets have evolved significantly across various domains, including news and web, scientific publications, and Wikipedia. Initially, datasets like the CoNLL 2004 and the ACE dataset focused on news articles, broadcast transcripts, and newspapers, covering multiple languages and domains. The New York Times Annotated Corpus introduced the use of distant supervision with Freebase for creating RE datasets. SemEval shared tasks have contributed datasets focusing on semantic relations between nominals and other linguistic elements.

In recent years, the scientific domain has gained attention with datasets like ScienceIE, which targets keyphrase identification and generic relations within journal articles across several scientific fields. SemEval-2018 Task 7 and SCIERC further expanded the scientific RE datasets, focusing on abstracts from scientific papers and AI-related conference proceedings, respectively. These datasets are particularly notable for their focus on coreference resolution in addition to relation extraction.

The Wikipedia domain was first explored with GoogleRE, leading to the development of multilingual datasets like mLAMA and SMiLER, which cover a wide range of languages and relations across multiple domains. These efforts reflect a growing interest in multilingual RE.

To address the limitations of previous datasets, which were restricted to the same label collection in both training and test sets, newer datasets like TACRED and DocRED have been introduced. TACRED focuses on newswire and web text with crowdsourced annotations, while DocRED is a document-level RE dataset that annotates both intra- and inter-sentence relations, as well as coreference chains, increasing the complexity of relation extraction tasks.

Overall, RE datasets have shifted from focusing primarily on news text to incorporating a broader range of domains, including scientific literature and Wikipedia. This evolution reflects the field's growing complexity and the diverse applications of relation extraction. Additionally, there's an emerging trend towards few-shot learning (FSL) and more challenging evaluation setups, such as document-level RE and datasets designed for specific RE aspects, indicating a move towards more realistic and varied experimental scenarios."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s1,Definitions of Robustness in NLP,"['p1.0', 'p1.1']","[""Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2)."", 'The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.']","Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).

The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.","(p1.0) Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).

(p1.1) The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.","[['b40'], ['b5', None, 'b18']]","[['b40'], ['b5', None, 'b18']]",4,"1. Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ].
2. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).
3. The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels.
4. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x )
5. and y when both are open-ended texts.","Measure and Improve Robustness in NLP Models: A Survey##
Definitions of Robustness in NLP##
Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).

The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.",How is robustness defined and measured in NLP models across different tasks?,1. How is robustness defined and measured in NLP models across different tasks?,"Questions:

1. How is robustness defined and measured in NLP models across different tasks?

Answer:

(p1.0) Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).

(p1.1) The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.","Measure and Improve Robustness in NLP Models: A Survey##
Definitions of Robustness in NLP##
Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).",How is robustness measured in NLP models against varying input distributions?,1. How is robustness measured in NLP models against varying input distributions?,"Questions:

1. How is robustness measured in NLP models against varying input distributions?

Answer:

(p1.0) Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).

(p1.1) The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.","Measure and Improve Robustness in NLP Models: A Survey##
Definitions of Robustness in NLP##
Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ].

The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels.",How is robustness defined and measured in NLP models?,1. How is robustness defined and measured in NLP models?,"Questions:

1. How is robustness defined and measured in NLP models?

Answer:

(p1.0) Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).

(p1.1) The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.","Measure and Improve Robustness in NLP Models: A Survey##
Definitions of Robustness in NLP##
Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).##
The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels.",How is robustness defined and measured in NLP models?,1. How is robustness defined and measured in NLP models?,"Questions:

1. How is robustness defined and measured in NLP models?

Answer:

(p1.0) Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).

(p1.1) The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.","Measure and Improve Robustness in NLP Models: A Survey
##
Definitions of Robustness in NLP
##
Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).
The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.
##
How is robustness defined and measured in NLP models?","Robustness in NLP models, although defined differently across various research areas, generally refers to a model's ability to maintain its performance across different datasets. This is measured by evaluating the model's accuracy on test data that may or may not have the same distribution as the training data. Specifically, robustness is assessed by the model's ability to correctly predict the gold label for a given input, with robust accuracy being a common metric for this evaluation. In the context of text classification and sequence labeling tasks, where outcomes are determined by a fixed set of discrete labels, this concept is straightforward. However, for text generation tasks, defining robustness becomes more complex due to issues like positional bias and hallucination, highlighting the challenge of lacking robust metrics for evaluating the quality of generated text.","Questions:

How is robustness defined and measured in NLP models?

Answer:

Robustness in NLP models, although defined differently across various research areas, generally refers to a model's ability to maintain its performance across different datasets. This is measured by evaluating the model's accuracy on test data that may or may not have the same distribution as the training data. Specifically, robustness is assessed by the model's ability to correctly predict the gold label for a given input, with robust accuracy being a common metric for this evaluation. In the context of text classification and sequence labeling tasks, where outcomes are determined by a fixed set of discrete labels, this concept is straightforward. However, for text generation tasks, defining robustness becomes more complex due to issues like positional bias and hallucination, highlighting the challenge of lacking robust metrics for evaluating the quality of generated text.","Robustness in NLP models, despite varying specific definitions across different research areas, generally refers to a model's ability to maintain its performance on test data that may differ from the training data. This concept is quantified by evaluating the model's robust accuracy, which measures the consistency of the model's predictions with the correct labels on new, potentially altered data sets. In the context of NLP tasks such as text classification and sequence labeling, robustness is assessed based on a fixed set of discrete labels. However, for text generation tasks, defining robustness becomes more complex, involving issues like positional bias and the tendency to produce irrelevant or fabricated content, known as hallucination. This complexity is further compounded by the challenge of lacking robust metrics to evaluate the quality of generated text, highlighting the need for reliable measures to assess the relationship between the model's output and the intended output when dealing with open-ended texts.","Questions:

How is robustness defined and measured in NLP models?

Answer:

Robustness in NLP models, despite varying specific definitions across different research areas, generally refers to a model's ability to maintain its performance on test data that may differ from the training data. This concept is quantified by evaluating the model's robust accuracy, which measures the consistency of the model's predictions with the correct labels on new, potentially altered data sets. In the context of NLP tasks such as text classification and sequence labeling, robustness is assessed based on a fixed set of discrete labels. However, for text generation tasks, defining robustness becomes more complex, involving issues like positional bias and the tendency to produce irrelevant or fabricated content, known as hallucination. This complexity is further compounded by the challenge of lacking robust metrics to evaluate the quality of generated text, highlighting the need for reliable measures to assess the relationship between the model's output and the intended output when dealing with open-ended texts.","Robustness in NLP models is generally defined as the model's ability to maintain its performance on test data that may be either synthetically perturbed or naturally occurring with a distribution shift, compared to the original training data distribution. This is measured by the model's robust accuracy, which evaluates how often the model's predictions on the test data match the gold standard labels. For tasks like text classification and sequence labeling, robustness is relatively straightforward to measure with discrete labels. However, for text generation tasks, robustness becomes more complex and can include issues like positional bias or hallucination, with the challenge being the lack of robust metrics to evaluate the quality of generated text in relation to the gold standard when both are open-ended texts.","Questions:

How is robustness defined and measured in NLP models?

Answer:

Robustness in NLP models is generally defined as the model's ability to maintain its performance on test data that may be either synthetically perturbed or naturally occurring with a distribution shift, compared to the original training data distribution. This is measured by the model's robust accuracy, which evaluates how often the model's predictions on the test data match the gold standard labels. For tasks like text classification and sequence labeling, robustness is relatively straightforward to measure with discrete labels. However, for text generation tasks, robustness becomes more complex and can include issues like positional bias or hallucination, with the challenge being the lack of robust metrics to evaluate the quality of generated text in relation to the gold standard when both are open-ended texts.","Robustness in NLP models is generally defined as the model's ability to maintain its performance on test data that may differ from the training data, denoted as D. This is measured by the model's robust accuracy, which evaluates how accurately the model predicts the gold label y for a given input x, even when the test data (x, y) comes from a distribution D that may include synthetic perturbations or naturally occurring distribution shifts. While robustness can be relatively straightforward to measure for tasks with discrete labels, such as text classification and sequence labeling, it becomes more complex for text generation tasks. In these cases, robustness issues may include positional bias or hallucination, and there is a noted challenge in finding robust metrics to evaluate the quality of generated text, where both the input x and the output y are open-ended texts.","Questions:

How is robustness defined and measured in NLP models?

Answer:

Robustness in NLP models is generally defined as the model's ability to maintain its performance on test data that may differ from the training data, denoted as D. This is measured by the model's robust accuracy, which evaluates how accurately the model predicts the gold label y for a given input x, even when the test data (x, y) comes from a distribution D that may include synthetic perturbations or naturally occurring distribution shifts. While robustness can be relatively straightforward to measure for tasks with discrete labels, such as text classification and sequence labeling, it becomes more complex for text generation tasks. In these cases, robustness issues may include positional bias or hallucination, and there is a noted challenge in finding robust metrics to evaluate the quality of generated text, where both the input x and the output y are open-ended texts.","Robustness in NLP models is generally defined as the model's ability to maintain its performance on test data that may differ from the training data, denoted as D. This can be measured by the model's robust accuracy, which evaluates the model's predictions against the gold labels of the test data (Tsipras et al., 2019). Research on robustness in NLP is categorized based on how the test data, D, is constructed: either through synthetic perturbation of the input or through naturally occurring distribution shifts. For tasks like text classification and sequence labeling, where outcomes are discrete labels, this definition of robustness is straightforward. However, for text generation tasks, robustness is more complex and can include issues like positional bias (Jung et al., 2019; Kryscinski et al., 2019) or hallucination (Maynez et al., 2020; Parikh et al., 2020). A significant challenge in these areas is the lack of robust metrics for evaluating the quality of generated text, necessitating a reliable metric to assess the relationship between the model's output and the open-ended target text (Sellam et al., 2020).","Questions:

How is robustness defined and measured in NLP models?

Answer:

Robustness in NLP models is generally defined as the model's ability to maintain its performance on test data that may differ from the training data, denoted as D. This can be measured by the model's robust accuracy, which evaluates the model's predictions against the gold labels of the test data (Tsipras et al., 2019). Research on robustness in NLP is categorized based on how the test data, D, is constructed: either through synthetic perturbation of the input or through naturally occurring distribution shifts. For tasks like text classification and sequence labeling, where outcomes are discrete labels, this definition of robustness is straightforward. However, for text generation tasks, robustness is more complex and can include issues like positional bias (Jung et al., 2019; Kryscinski et al., 2019) or hallucination (Maynez et al., 2020; Parikh et al., 2020). A significant challenge in these areas is the lack of robust metrics for evaluating the quality of generated text, necessitating a reliable metric to assess the relationship between the model's output and the open-ended target text (Sellam et al., 2020)."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s2,Source(s) Target,"['p2.0', 'p2.1', 'p2.2', 'p2.3']","[""Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread."", '2 What is Stance?', 'In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker\'s standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).', 'Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.']","Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","(p2.0) Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

(p2.1) 2 What is Stance?

(p2.2) In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

(p2.3) Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","[['b12', None, 'b57', 'b17'], [], [None], []]","[['b12', None, 'b57', 'b17'], [], [None], []]",5,"1. Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011)
2. Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b)
3. Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article *
4. 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '
5. 17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection.
6. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds.
7. * the article's body is summarised.
8. Sources: Twitter, ǌ News, ɀikipedia, Reddit.
9. Evidence: Single, Multiple, Thread.
10. 2 What is Stance? In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process.
11. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition.
12. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither.
13. Occasionally, the category label of Neutral is also added to the set of stance categories (Zubiaga et al., 2016b)0, and the target may or may not be explicitly mentioned in the text"" (Zubiaga et al., 2016b)1.
14. Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (Zubiaga et al., 2016b)2.
15. Finally, stance detection can be distinguished from several other closely related NLP tasks: (Zubiaga et al., 2016b)3 biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (Zubiaga et al., 2016b)4 emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (Zubiaga et al., 2016b)5 perspective identification, which aims to find the pointof-view of the author (Zubiaga et al., 2016b)6 and the target is always explicit, (Zubiaga et al., 2016b)7 sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (Zubiaga et al., 2016b)8 sentiment analysis, which checks the polarity of a piece of text.","A Survey on Stance Detection for Mis-and Disinformation Identification##
Source(s) Target##
Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.",What distinguishes stance detection from related NLP tasks like sentiment analysis or sarcasm detection?,1. What distinguishes stance detection from related NLP tasks like sentiment analysis or sarcasm detection?,"Questions:

1. What distinguishes stance detection from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

(p2.0) Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

(p2.1) 2 What is Stance?

(p2.2) In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

(p2.3) Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","A Survey on Stance Detection for Mis-and Disinformation Identification##
Source(s) Target##
Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.",What are the key characteristics of datasets used in stance detection for misinformation?,1. What are the key characteristics of datasets used in stance detection for misinformation?,"Questions:

1. What are the key characteristics of datasets used in stance detection for misinformation?

Answer:

(p2.0) Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

(p2.1) 2 What is Stance?

(p2.2) In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

(p2.3) Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","A Survey on Stance Detection for Mis-and Disinformation Identification##
Source(s) Target##
Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011)

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process.

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.",How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?,1. How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?,"Questions:

1. How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

(p2.0) Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

(p2.1) 2 What is Stance?

(p2.2) In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

(p2.3) Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","A Survey on Stance Detection for Mis-and Disinformation Identification##
Source(s) Target##
Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.##
2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process.

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.",How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?,1. How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?,"Questions:

1. How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

(p2.0) Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

(p2.1) 2 What is Stance?

(p2.2) In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

(p2.3) Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","A Survey on Stance Detection for Mis-and Disinformation Identification
##
Source(s) Target
##
Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.
2 What is Stance?
In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).
Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.
##
How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?","Stance detection is a unique NLP task that involves identifying the author's position towards a specific proposition in a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This process differs from related NLP tasks in several ways. Unlike biased language detection, which seeks to identify a text's inclination towards a particular viewpoint, stance detection categorizes the author's explicit or implicit position. It is distinct from emotion recognition, which focuses on identifying emotions like love or anger in the text. Perspective identification, another related task, aims to determine the author's point of view (e.g., Democrat vs. Republican) with an always explicit target. Sarcasm detection involves identifying satirical or ironic texts intended for ridicule or mockery, which is different from the evaluative and positioning nature of stance detection. Lastly, sentiment analysis examines the polarity of a text (positive, negative, neutral) without necessarily categorizing the author's stance towards a specific proposition, making it distinct from stance detection.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection is a unique NLP task that involves identifying the author's position towards a specific proposition in a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This process differs from related NLP tasks in several ways. Unlike biased language detection, which seeks to identify a text's inclination towards a particular viewpoint, stance detection categorizes the author's explicit or implicit position. It is distinct from emotion recognition, which focuses on identifying emotions like love or anger in the text. Perspective identification, another related task, aims to determine the author's point of view (e.g., Democrat vs. Republican) with an always explicit target. Sarcasm detection involves identifying satirical or ironic texts intended for ridicule or mockery, which is different from the evaluative and positioning nature of stance detection. Lastly, sentiment analysis examines the polarity of a text (positive, negative, neutral) without necessarily categorizing the author's stance towards a specific proposition, making it distinct from stance detection.","Stance detection involves identifying the author's position towards a specific proposition in a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This process is distinct from related NLP tasks in several ways. Unlike biased language detection, which seeks to identify a text's leaning towards a particular viewpoint, stance detection categorizes the author's explicit position. It differs from emotion recognition, which aims to identify specific emotions like love or anger in the text. Perspective identification focuses on pinpointing the author's viewpoint, such as Democrat vs. Republican, with a clear target, whereas stance detection may not always have an explicitly mentioned target. Sarcasm detection targets satirical or ironic texts intended for ridicule, and sentiment analysis evaluates the text's overall polarity. Stance detection's uniqueness lies in its comprehensive approach to evaluating objects, positioning subjects, and aligning with others, influenced by personal opinions and external factors like cultural norms.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection involves identifying the author's position towards a specific proposition in a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This process is distinct from related NLP tasks in several ways. Unlike biased language detection, which seeks to identify a text's leaning towards a particular viewpoint, stance detection categorizes the author's explicit position. It differs from emotion recognition, which aims to identify specific emotions like love or anger in the text. Perspective identification focuses on pinpointing the author's viewpoint, such as Democrat vs. Republican, with a clear target, whereas stance detection may not always have an explicitly mentioned target. Sarcasm detection targets satirical or ironic texts intended for ridicule, and sentiment analysis evaluates the text's overall polarity. Stance detection's uniqueness lies in its comprehensive approach to evaluating objects, positioning subjects, and aligning with others, influenced by personal opinions and external factors like cultural norms.","Stance detection involves identifying the author's position towards a specific target within a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This task differs from related NLP tasks in several key ways. Unlike biased language detection, which seeks to identify a text's leaning towards a particular viewpoint, stance detection categorizes the author's explicit position regarding a predefined target. It is distinct from emotion recognition, which focuses on identifying emotions like love or anger in text, and from perspective identification, which aims to determine the author's viewpoint (e.g., Democrat vs. Republican) with an explicitly mentioned target. Stance detection also differs from sarcasm detection, which identifies satirical or ironic text intended for ridicule or mockery, and from sentiment analysis, which assesses the overall polarity of the text without necessarily linking it to a specific target.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection involves identifying the author's position towards a specific target within a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This task differs from related NLP tasks in several key ways. Unlike biased language detection, which seeks to identify a text's leaning towards a particular viewpoint, stance detection categorizes the author's explicit position regarding a predefined target. It is distinct from emotion recognition, which focuses on identifying emotions like love or anger in text, and from perspective identification, which aims to determine the author's viewpoint (e.g., Democrat vs. Republican) with an explicitly mentioned target. Stance detection also differs from sarcasm detection, which identifies satirical or ironic text intended for ridicule or mockery, and from sentiment analysis, which assesses the overall polarity of the text without necessarily linking it to a specific target.","Stance detection involves identifying the author's position towards a specific proposition or target within a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This task is distinct from related NLP tasks in several ways: biased language detection focuses on identifying a text's inclination towards a particular perspective; emotion recognition aims to identify specific emotions like love or anger within the text; perspective identification seeks to determine the author's point of view, such as political affiliation, with an explicitly mentioned target; sarcasm detection targets texts written with satirical or ironic intent, often for ridicule; and sentiment analysis evaluates the overall polarity of the text, whether positive, negative, or neutral.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection involves identifying the author's position towards a specific proposition or target within a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This task is distinct from related NLP tasks in several ways: biased language detection focuses on identifying a text's inclination towards a particular perspective; emotion recognition aims to identify specific emotions like love or anger within the text; perspective identification seeks to determine the author's point of view, such as political affiliation, with an explicitly mentioned target; sarcasm detection targets texts written with satirical or ironic intent, often for ridicule; and sentiment analysis evaluates the overall polarity of the text, whether positive, negative, or neutral.","Stance detection involves identifying the author's position towards a specific target within a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This task is distinct from related NLP tasks in several ways: biased language detection focuses on identifying a text's inclination towards a particular perspective; emotion recognition aims to identify specific emotions like love or anger expressed in the text; perspective identification seeks to determine the author's point of view, such as political affiliation, with an explicit target; sarcasm detection identifies satirical or ironic text intended for ridicule or mockery; and sentiment analysis evaluates the overall polarity of the text, whether positive, negative, or neutral.","Questions:

How does stance detection differ from related NLP tasks like sentiment analysis or sarcasm detection?

Answer:

Stance detection involves identifying the author's position towards a specific target within a text, categorizing it as Favor, Against, Neither, or occasionally Neutral. This task is distinct from related NLP tasks in several ways: biased language detection focuses on identifying a text's inclination towards a particular perspective; emotion recognition aims to identify specific emotions like love or anger expressed in the text; perspective identification seeks to determine the author's point of view, such as political affiliation, with an explicit target; sarcasm detection identifies satirical or ironic text intended for ridicule or mockery; and sentiment analysis evaluates the overall polarity of the text, whether positive, negative, or neutral."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s3,Output Uncertainty,"['p3.0', 'p3.1']","['Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).', ""Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).""]","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","(p3.0) Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

(p3.1) Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","[['b11', 'b91', 'b49', 'b25', None, 'b10', 'b12', 'b14', 'b19'], [None, 'b72']]","[['b11', 'b91', 'b49', 'b25', None, 'b10', 'b12', 'b14', 'b19'], [None, 'b72']]",11,"1. Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.
2. It prefers the most uncertain instances judged by the model outputs.
3. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).
4. Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
5. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).
6. Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.
7. If an instance is near the decision boundary, the model's outputs may be different within its local region.
8. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","A Survey of Active Learning for Natural Language Processing##
Output Uncertainty##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).",What are the primary strategies for selecting instances in uncertainty-based active learning?,1. What are the primary strategies for selecting instances in uncertainty-based active learning?,"Questions:

1. What are the primary strategies for selecting instances in uncertainty-based active learning?

Answer:

(p3.0) Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

(p3.1) Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","A Survey of Active Learning for Natural Language Processing##
Output Uncertainty##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).",What are the primary strategies for uncertainty sampling in active learning for NLP?,1. What are the primary strategies for uncertainty sampling in active learning for NLP?,"Questions:

1. What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

(p3.0) Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

(p3.1) Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","A Survey of Active Learning for Natural Language Processing##
Output Uncertainty##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.",What are common strategies for measuring output uncertainty in active learning for NLP?,1. What are common strategies for measuring output uncertainty in active learning for NLP?,"Questions:

1. What are common strategies for measuring output uncertainty in active learning for NLP?

Answer:

(p3.0) Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

(p3.1) Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","A Survey of Active Learning for Natural Language Processing##
Output Uncertainty##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).##
Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.",What are the primary strategies for uncertainty sampling in active learning for NLP?,1. What are the primary strategies for uncertainty sampling in active learning for NLP?,"Questions:

1. What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

(p3.0) Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

(p3.1) Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","A Survey of Active Learning for Natural Language Processing
##
Output Uncertainty
##
Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).
Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).
##
What are the primary strategies for uncertainty sampling in active learning for NLP?","The primary strategies for uncertainty sampling in active learning for natural language processing (NLP) include entropy-based, least-confidence, and margin-sampling strategies for probabilistic models, as identified by Lewis and Gale (1994) and further detailed by Shannon (1948), Culotta and McCallum (2005), Scheffer et al. (2001), and Schein and Ungar (2007). These strategies focus on selecting the most uncertain instances as judged by the model's outputs. Schröder et al. (2022) have also explored these uncertainty-based strategies with Transformer-based models, specifically for text classification tasks. For non-probabilistic models, approaches such as selecting instances close to the decision boundary, exemplified by support vector machines (SVM), are used, as discussed by Schohn and Cohn (2000) and Tong and Koller (2001). Additionally, measuring output uncertainty can involve examining the divergence of a model's predictions within an instance's local region, employing methods like nearest-neighbour searches, adversarial perturbation, and data augmentation, as explored in recent studies by Margatina et al. (2021), Zhang et al. (2022b), and Jiang et al. (2020).","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

The primary strategies for uncertainty sampling in active learning for natural language processing (NLP) include entropy-based, least-confidence, and margin-sampling strategies for probabilistic models, as identified by Lewis and Gale (1994) and further detailed by Shannon (1948), Culotta and McCallum (2005), Scheffer et al. (2001), and Schein and Ungar (2007). These strategies focus on selecting the most uncertain instances as judged by the model's outputs. Schröder et al. (2022) have also explored these uncertainty-based strategies with Transformer-based models, specifically for text classification tasks. For non-probabilistic models, approaches such as selecting instances close to the decision boundary, exemplified by support vector machines (SVM), are used, as discussed by Schohn and Cohn (2000) and Tong and Koller (2001). Additionally, measuring output uncertainty can involve examining the divergence of a model's predictions within an instance's local region, employing methods like nearest-neighbour searches, adversarial perturbation, and data augmentation, as explored in recent studies by Margatina et al. (2021), Zhang et al. (2022b), and Jiang et al. (2020).","The primary strategies for uncertainty sampling in active learning for natural language processing (NLP) include entropy-based, least-confidence, and margin-sampling methods for probabilistic models, as identified by Lewis and Gale (1994), Shannon (1948), Culotta and McCallum (2005), Scheffer et al. (2001), Schein and Ungar (2007), and Settles (2009). These strategies focus on selecting instances that the model finds most uncertain. Schröder et al. (2022) further explored these strategies with Transformer-based models, specifically for text classification. For non-probabilistic models, approaches such as selecting instances near the decision boundary, like in support vector machines (SVM), are used, as noted by Schohn and Cohn (2000) and Tong and Koller (2001). Additionally, measuring output uncertainty can involve examining an instance's local region divergence through methods like nearest-neighbour searches, adversarial perturbation, and data augmentation, as discussed in recent studies by Margatina et al. (2021), Zhang et al. (2022b), and Jiang et al. (2020).","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

The primary strategies for uncertainty sampling in active learning for natural language processing (NLP) include entropy-based, least-confidence, and margin-sampling methods for probabilistic models, as identified by Lewis and Gale (1994), Shannon (1948), Culotta and McCallum (2005), Scheffer et al. (2001), Schein and Ungar (2007), and Settles (2009). These strategies focus on selecting instances that the model finds most uncertain. Schröder et al. (2022) further explored these strategies with Transformer-based models, specifically for text classification. For non-probabilistic models, approaches such as selecting instances near the decision boundary, like in support vector machines (SVM), are used, as noted by Schohn and Cohn (2000) and Tong and Koller (2001). Additionally, measuring output uncertainty can involve examining an instance's local region divergence through methods like nearest-neighbour searches, adversarial perturbation, and data augmentation, as discussed in recent studies by Margatina et al. (2021), Zhang et al. (2022b), and Jiang et al. (2020).","Uncertainty sampling is a key strategy in active learning for NLP, focusing on selecting the most uncertain instances as judged by the model's outputs. For probabilistic models, the primary strategies include entropy-based, least-confidence, and margin-sampling methods. These approaches aim to identify instances where the model's predictions are least certain. For non-probabilistic models, strategies such as selecting instances close to the decision boundary in an SVM are used. Additionally, measuring output uncertainty can involve examining the divergence of a model's predictions within an instance's local region, employing methods like nearest-neighbour searches, adversarial perturbation, and data augmentation to assess local divergence and uncertainty.","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling is a key strategy in active learning for NLP, focusing on selecting the most uncertain instances as judged by the model's outputs. For probabilistic models, the primary strategies include entropy-based, least-confidence, and margin-sampling methods. These approaches aim to identify instances where the model's predictions are least certain. For non-probabilistic models, strategies such as selecting instances close to the decision boundary in an SVM are used. Additionally, measuring output uncertainty can involve examining the divergence of a model's predictions within an instance's local region, employing methods like nearest-neighbour searches, adversarial perturbation, and data augmentation to assess local divergence and uncertainty.","Uncertainty sampling is a fundamental query strategy in active learning for NLP, focusing on selecting the most uncertain instances as judged by the model's outputs. For probabilistic models, the primary strategies include entropy-based sampling (Shannon, 1948), least confidence sampling (Culotta and McCallum, 2005), and margin sampling (Scheffer et al., 2001; Schein and Ungar, 2007), as outlined by Settles (2009). Recent studies, such as Schröder et al. (2022), have revisited these uncertainty-based strategies within the context of Transformer-based models, providing empirical evidence of their effectiveness in text classification tasks. For non-probabilistic models, strategies akin to selecting instances near the decision boundary, such as those used in SVMs (Schohn and Cohn, 2000; Tong and Koller, 2001), are employed. Additionally, measuring output uncertainty through the divergence of a model's predictions within an instance's local region has gained attention. Techniques to assess local divergence include nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020), offering alternative ways to identify uncertain instances for querying.","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling is a fundamental query strategy in active learning for NLP, focusing on selecting the most uncertain instances as judged by the model's outputs. For probabilistic models, the primary strategies include entropy-based sampling (Shannon, 1948), least confidence sampling (Culotta and McCallum, 2005), and margin sampling (Scheffer et al., 2001; Schein and Ungar, 2007), as outlined by Settles (2009). Recent studies, such as Schröder et al. (2022), have revisited these uncertainty-based strategies within the context of Transformer-based models, providing empirical evidence of their effectiveness in text classification tasks. For non-probabilistic models, strategies akin to selecting instances near the decision boundary, such as those used in SVMs (Schohn and Cohn, 2000; Tong and Koller, 2001), are employed. Additionally, measuring output uncertainty through the divergence of a model's predictions within an instance's local region has gained attention. Techniques to assess local divergence include nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020), offering alternative ways to identify uncertain instances for querying.","Uncertainty sampling is a fundamental query strategy in active learning for NLP, focusing on selecting the most uncertain instances as judged by the model's outputs. For probabilistic models, three primary strategies are entropy-based sampling, which relies on the concept of entropy to measure uncertainty (Shannon, 1948); least-confidence sampling, which selects instances for which the model has the lowest confidence in its predictions (Culotta and McCallum, 2005); and margin-sampling, which chooses instances based on the smallest difference between the probabilities of the two most probable classes (Scheffer et al., 2001; Schein and Ungar, 2007). These strategies have been revisited in the context of Transformer-based models for text classification, demonstrating their effectiveness with empirical results (Schröder et al., 2022). For non-probabilistic models, similar principles apply, such as selecting instances close to the decision boundary, exemplified by support vector machines (SVM) (Schohn and Cohn, 2000; Tong and Koller, 2001).

Additionally, output uncertainty can be measured by examining the divergence of a model's predictions within an instance's local region, identifying instances near the decision boundary where model outputs vary significantly. Recent approaches to assessing local divergence include nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020), offering innovative ways to identify uncertain instances for active learning in NLP.","Questions:

What are the primary strategies for uncertainty sampling in active learning for NLP?

Answer:

Uncertainty sampling is a fundamental query strategy in active learning for NLP, focusing on selecting the most uncertain instances as judged by the model's outputs. For probabilistic models, three primary strategies are entropy-based sampling, which relies on the concept of entropy to measure uncertainty (Shannon, 1948); least-confidence sampling, which selects instances for which the model has the lowest confidence in its predictions (Culotta and McCallum, 2005); and margin-sampling, which chooses instances based on the smallest difference between the probabilities of the two most probable classes (Scheffer et al., 2001; Schein and Ungar, 2007). These strategies have been revisited in the context of Transformer-based models for text classification, demonstrating their effectiveness with empirical results (Schröder et al., 2022). For non-probabilistic models, similar principles apply, such as selecting instances close to the decision boundary, exemplified by support vector machines (SVM) (Schohn and Cohn, 2000; Tong and Koller, 2001).

Additionally, output uncertainty can be measured by examining the divergence of a model's predictions within an instance's local region, identifying instances near the decision boundary where model outputs vary significantly. Recent approaches to assessing local divergence include nearest-neighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b), and data augmentation (Jiang et al., 2020), offering innovative ways to identify uncertain instances for active learning in NLP."
