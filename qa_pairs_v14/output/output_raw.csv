corpusid,title,domain,url,section,section_title,paragraph,para_listed_answer,answer,indexed_answer,filtered_refids,filtered_refids_qualified,num_reference,segmented_answer
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s1,BERT embeddings,"['p1.0', 'p1.1']","[""Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence."", ""In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).""]","Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","(p1.0) Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","[['b22', 'b24'], ['b21', 'b65']]","[['b22', 'b24'], ['b21', 'b65']]",4,"1. Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
2. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
3. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
4. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
5. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
6. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
7. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic)."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s3,Syntactic knowledge,"['p3.0', 'p3.1', 'p3.2']","['As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.', '(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).', 'Regarding syntactic competence of BERT\'s MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT\'s encoding of syntactic structure does not indicate that it actually relies on that knowledge.']","As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","(p3.0) As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it. Htut et al.

(p3.1) (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2). Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","[[], ['b5', 'b18'], [None, 'b64', 'b59']]","[[], ['b5', 'b18'], [None, 'b64', 'b59']]",5,"1. As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it.
2. Htut et al.(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.
3. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation.
4. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2).
5. Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).
6. Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.
7. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences.
8. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations.
9. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input.
10. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019).
11. This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s7,Self-attention heads,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4', 'p7.5', 'p7.6', 'p7.7', 'p7.8', 'p7.9']","['Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:', '• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);', '• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).', 'Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.', '[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.', 'Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.', 'Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.', ""(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis."", ""Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data."", ""Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.""]","Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

• attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

[SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","(p7.0) Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:

(p7.1) • attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);

(p7.2) • attending to previous/next tokens,    (Kovaleva et al., 2019) According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"". However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 . Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019). This apparent redundancy must be related to the overparametrization issue (see section 7).

(p7.3) Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.

(p7.4) [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.

(p7.5) Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.

(p7.6) Some BERT heads seem to specialize in certain types of syntactic relations. Htut et al.

(p7.7) (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj, advmod, and amod has some variation between these two studies. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.

(p7.8) Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).  present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","[[], [], [None, 'b9'], [None], [], [], [], [None, 'b58'], [], [None, 'b9']]","[[], [], [None, 'b9'], [None], [], [], [], [None, 'b58'], [], [None, 'b9']]",7,"1. Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types: attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018); attending to previous/next tokens,    (Kovaleva et al., 2019)
2. According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"".
3. However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 .
4. Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019).
5. This apparent redundancy must be related to the overparametrization issue (see section 7).
6. Attention to (Kovaleva et al., 2019)5 is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to (Kovaleva et al., 2019)4 and punctuation.
7. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as (Kovaleva et al., 2019)5 and (Kovaleva et al., 2019)4, and the model learns to rely on them.
8. They suggest also that the function of (Kovaleva et al., 2019)4 might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.
9. (Kovaleva et al., 2019)4 gets increased attention starting in layer 5, but its importance for prediction drops.
10. If this hypothesis is correct, attention probing studies that excluded the (Kovaleva et al., 2019)4 and (Kovaleva et al., 2019)5 tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.
11. Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.
12. Some BERT heads seem to specialize in certain types of syntactic relations.
13. Htut et al.(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions.
14. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline.
15. The evidence for nsubj, advmod, and amod has some variation between these two studies.
16. The overall conclusion is also supported by Voita et al. (2019)'s data for the base Transformer in machine translation context.
17. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).
18. present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora.
19. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.
20. Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.
21. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.
22. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).
23. Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s8,BERT layers,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4']","['The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.', 'There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.', 'The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.', ""The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT."", 'The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.']","The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

(p8.1) There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

(p8.4) The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"". However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","[[], ['b5'], ['b50', 'b57', 'b5'], ['b50', None, 'b9'], ['b5']]","[[], ['b5'], ['b50', 'b57', 'b5'], ['b50', None, 'b9'], ['b5']]",8,"1. The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings.
2. It stands to reason that the lower layers have the most linear word order information.
3. report a decrease in the knowledge of linear word order around layer 4 in BERT-base.
4. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.
5. There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers.
6. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large).
7. Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.
8. The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5).
9. There is conflicting evidence about syntactic chunks.
10. Tenney et al. (6-9 for base-BERT, 14-19 for BERT-large)0 conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling.
11. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing.
12. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.
13. The final layers of BERT are the most taskspecific.
14. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable .
15. In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019).
16. At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance.
17. Tenney et al. (6-9 for base-BERT, 14-19 for BERT-large)0 suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers.
18. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (6-9 for base-BERT, 14-19 for BERT-large)1.
19. But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial.
20. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.
21. The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"".
22. However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s10,Pre-training BERT,"['p10.0', 'p10.1', 'p10.2', 'p10.3', 'p10.4', 'p10.5', 'p10.6', 'p10.7', 'p10.8', 'p10.9']","['The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.', '• Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).', '• Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).', ""• Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);"", '• Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;', '• Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).', '• Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).', '• Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.', 'Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .', 'Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).']","The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

• Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

• Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

• Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

• Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

• Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

• Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

• Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","(p10.0) The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM). Multiple studies have come up with alternative training objectives to improve on BERT.

(p10.1) • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting . Wang et al. (2019a) Mikolov et al. (2013b).

(p10.2) • Permutation language modeling. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order. See also the n-gram word order reconstruction task (Wang et al., 2019a).

(p10.3) • Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);

(p10.4) • Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;

(p10.5) • Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).

(p10.6) • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).

(p10.7) • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.

(p10.8) Another obvious source of improvement is pretraining data. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","[[], ['b23', 'b62', None, 'b8', 'b16'], ['b75', 'b62'], ['b8'], [], ['b47'], [], [], ['b17'], ['b31', 'b9']]","[[], ['b23', 'b62', None, 'b8', 'b16'], ['b75', 'b62'], ['b8'], [], ['b47'], [], [], ['b17'], ['b31', 'b9']]",12,"1. The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM).
2. Multiple studies have come up with alternative training objectives to improve on BERT.
3. Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting .
4. Wang et al. (2019a) Mikolov et al. (2013b).
5. Permutation language modeling. Yang et al. (MLM)6 replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order.
6. See also the n-gram word order reconstruction task (Wang et al., 2019a).
7. Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020); Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words; Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).
8. Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (MLM)0.
9. Clinchant et al. (MLM)6 propose replacing the MASK token with (MLM)2 token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.
10. Another obvious source of improvement is pretraining data.
11. Liu et al. (MLM)3 explore the benefits of increasing the corpus volume and longer training.
12. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (MLM)4, there are ongoing efforts to incorporate structured knowledge resources .
13. Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (MLM)5 and ERNIE .
14. Alternatively, SemBERT  integrates semantic role information with BERT representations.
15. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides.
16. Hao et al. (MLM)6 conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (MLM)7.
17. However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (MLM)8."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s11,Model architecture choices,"['p11.0', 'p11.1']","[""To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks."", 'Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.']","To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

(p11.1) Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","[['b20', 'b75', 'b17', 'b58'], []]","[['b20', 'b75', 'b17', 'b58'], []]",4,"1. To date, the most systematic study of BERT architecture was performed by .
2. They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others.
3. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable.
4. Larger hidden representation size was consistently better, but the gains varied by setting.
5. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance.
6. They also publish their recommendations for other model parameters.
7. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.
8. observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.
9. Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers.
10. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s12,Fine-tuning BERT,"['p12.0', 'p12.1', 'p12.2', 'p12.3', 'p12.4', 'p12.5', 'p12.6']","['Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).', '• Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).', 'With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.', '(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.', 'An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).', 'Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.', '7 How big should BERT be?']","Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

• Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

7 How big should BERT be?","(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

(p12.1) • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).

(p12.2) With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost. Artetxe et al.

(p12.3) (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.

(p12.4) An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

(p12.6) 7 How big should BERT be?","[[None, 'b9'], ['b83'], [], [], ['b26'], ['b28'], []]","[[None, 'b9'], ['b83'], [], [], ['b26'], ['b28'], []]",5,"1. Pre-training + fine-tuning workflow is a crucial part of BERT.
2. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand.
3. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns.
4. It is understandable why fine-tuning would increase the attention to  Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).
5. Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).
6. With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules.
7. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost.
8. Artetxe et al.(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.
9. An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).Initialization can have a dramatic effect on the training process (Petrov, 2010).
10. However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018).
11. Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation.
12. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.
13. 7 How big should BERT be?"
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s13,Overparametrization,"['p13.0', 'p13.1', 'p13.2', 'p13.3', 'p13.4']","['Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.', 'Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).', 'Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .', 'Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.', '(2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.']","Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","(p13.0) Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

(p13.2) Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .

(p13.3) Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers. Clark et al.

(p13.4) (2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","[['b38', 'b70', 'b43'], ['b20', 'b9', 'b37', 'b58', 'b46', None, 'b50'], [], [], []]","[['b38', 'b70', 'b43'], ['b20', 'b9', 'b37', 'b58', 'b46', None, 'b50'], [], [], []]",10,"1. Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT.
2. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.
3. Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.
4. Voita et al. (Strubell et al., 2019;Schwartz et al., 2019)3 showed that all but a few Transformer heads could be pruned without significant losses in performance.
5. For BERT, Clark et al. (Strubell et al., 2019;Schwartz et al., 2019)3 observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (Strubell et al., 2019;Schwartz et al., 2019)3 were able to reduce most layers to a single head.
6. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance.
7. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019).
8. Additionally, Tenney et al. (Strubell et al., 2019;Schwartz et al., 2019)0 examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (Strubell et al., 2019;Schwartz et al., 2019)1.
9. Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case.
10. In particular, the opposite was observed for subjectverb agreement (Strubell et al., 2019;Schwartz et al., 2019)2 and sentence subject detection .
11. Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers.
12. Clark et al.(Strubell et al., 2019;Schwartz et al., 2019)3 suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s14,BERT compression,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4']","['Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.', 'Two main approaches include knowledge distillation and quantization.', 'The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).', ""The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware."", ""Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).""]","Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

Two main approaches include knowledge distillation and quantization.

The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","(p14.0) Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss. Such efforts to date are summarized in Table 1.

(p14.1) Two main approaches include knowledge distillation and quantization.

(p14.2) The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase). This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).

(p14.3) The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019). Note that this strategy often requires compatible hardware.

(p14.4) Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","[[], [], ['b37', 'b46', None, 'b54'], ['b40', 'b76'], ['b72', 'b11']]","[[], [], ['b37', 'b46', None, 'b54'], ['b40', 'b76'], ['b72', 'b11']]",8,"1. Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss.
2. Such efforts to date are summarized in Table 1.
3. Two main approaches include knowledge distillation and quantization.
4. The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase).
5. This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).
6. The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019).
7. Note that this strategy often requires compatible hardware.
8. Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020)."
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0,s15,Multilingual BERT,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4', 'p15.5', 'p15.6', 'p15.7']","['Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).', 'mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.', 'mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.', 'At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.', 'To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.', 'Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).', 'Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.', 'Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.']","Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.","(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

(p15.1) mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (2019) note that this task could be solvable by simple lexical matches. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.

(p15.2) mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial . It is also aware of at least some typological language features (Libovický et al., 2019;Singh et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019). Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

(p15.4) To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"". There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis . A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.

(p15.5) Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019). Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).

(p15.6) Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019). Ablations are also problematic if the same information was duplicated elsewhere in the network. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.

(p15.7) Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019). However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020). Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.","[['b68', 'b36', 'b64', 'b30', 'b13'], ['b13', 'b30'], ['b68', 'b42', 'b13', 'b30'], ['b68', 'b36', 'b13', 'b30'], ['b50'], ['b64', 'b2'], ['b20', 'b9', 'b58', None, 'b50', 'b2'], ['b39', 'b56', None, 'b66', 'b4']]","[['b68', 'b36', 'b64', 'b30', 'b13'], ['b13', 'b30'], ['b68', 'b42', 'b13', 'b30'], ['b68', 'b36', 'b13', 'b30'], ['b50'], ['b64', 'b2'], ['b20', 'b9', 'b58', None, 'b50', 'b2'], ['b39', 'b56', None, 'b66', 'b4']]",29,"1. Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary).
2. Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing.
3. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019).
4. The model seems to naturally learn high-quality cross-lingual word alignments (110K wordpiece vocabulary)9, with caveats for open-class parts of speech (Cao et al., 2019).
5. Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al. (Wu and Dredze, 2019;Pires et al., 2019)8 note that this task could be solvable by simple lexical matches.
6. Pires et al. (Wu and Dredze, 2019;Pires et al., 2019)8 conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.
7. mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (110K wordpiece vocabulary)0, and adding the IDs in pre-training was not beneficial .
8. It is also aware of at least some typological language features (110K wordpiece vocabulary)1, and transfer between structurally similar languages works better Pires et al., 2019).
9. Singh et al. (Wu and Dredze, 2019;Pires et al., 2019)8 argue that if typological features structure its representation space, it could not be considered as interlingua.
10. However, Artetxe et al. (Wu and Dredze, 2019;Pires et al., 2019)8 show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.
11. At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (110K wordpiece vocabulary)4, and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (Wu and Dredze, 2019;Pires et al., 2019)8.
12. Pires et al. (Wu and Dredze, 2019;Pires et al., 2019)8 and Wu and Dredze (Wu and Dredze, 2019;Pires et al., 2019)8 hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages.
13. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary.
14. Artetxe et al. (Wu and Dredze, 2019;Pires et al., 2019)8 also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (110K wordpiece vocabulary)9 vocabulary, without any shared word-pieces.
15. To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge.
16. However, as Tenney et al. (Wu and Dredze, 2019;Pires et al., 2019)0 aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"".
17. There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis .
18. A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.
19. Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (Wu and Dredze, 2019;Pires et al., 2019)1 would not be sufficient (Wu and Dredze, 2019;Pires et al., 2019)2.
20. Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Wu and Dredze, 2019;Pires et al., 2019)3.
21. Head and layer ablation studies (Wu and Dredze, 2019;Pires et al., 2019)4 inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Wu and Dredze, 2019;Pires et al., 2019)5, the absence of heads that would perform parsing ""in general"" (Wu and Dredze, 2019;Pires et al., 2019)6.
22. Ablations are also problematic if the same information was duplicated elsewhere in the network.
23. To mitigate that, Michel et al. (Wu and Dredze, 2019;Pires et al., 2019)8 prune heads in the order set by a proxy importance score, and Voita et al. (Wu and Dredze, 2019;Pires et al., 2019)8
24. fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.
25. Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Wu and Dredze, 2019;Pires et al., 2019)9.
26. However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Rönnqvist et al., 2019)0.
27. Also, visualization is typically limited to qualitative analysis (Rönnqvist et al., 2019)1, and should not be interpreted as definitive evidence."
236460206,Towards Argument Mining for Social Good: A Survey,"Political Science, Linguistics, Computer Science",https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,s2,Framework,"['p2.0', 'p2.1', 'p2.2', 'p2.3', 'p2.4', 'p2.5', 'p2.6', 'p2.7', 'p2.8', 'p2.9', 'p2.10', 'p2.11', 'p2.12']","['Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).', 'Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.', 'To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.', 'Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.', 'A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.', ""Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation."", 'A1: Marvel Universe is better than DC Universe.', ""A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses."", 'A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.', 'A4: This is especially true due to his unfortunate passing.', ""A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics."", ""The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise)."", ""Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).""]","Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

A1: Marvel Universe is better than DC Universe.

A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

A4: This is especially true due to his unfortunate passing.

A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","(p2.0) Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.). A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).

(p2.1) Relation assignment The goal of the second stage is to model the relations between the argumentative spans identified in the first stage. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim). Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (Kobbe et al., 2019). Detecting these relations is necessary to model the overall structure of the argumentation (discourse/debate). As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.

(p2.2) To simplify the problem, some approaches reduce the graph to a tree-structure representation (Peldszus and Stede, 2015;Stab and Gurevych, 2017). Different methods to generate the structure have been investigated, e.g. SVMs (Habernal and Gurevych, 2017;Niculae et al., 2017) or textual entailment (Cabrio and Villata, 2013;Cocarascu et al., 2020). Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.

(p2.3) Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?"" (Claims are in bold, and premises are underlined.) A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects. Vaccinated people become immune to a certain pathogen and do not develop a disease. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.

(p2.4) A2: Many vaccines have serious and sometimes deadly side effects. With many vaccines the immunity is not lifelong. Sometimes the vaccines itself can cause a serious disease to develop as a side effect. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.

(p2.5) Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (claims and premises) are extracted. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it. However, consider another example, extracted from an online debate platform Kialo 4 . Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.

(p2.6) A1: Marvel Universe is better than DC Universe.

(p2.7) A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.

(p2.8) A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.

(p2.9) A4: This is especially true due to his unfortunate passing.

(p2.10) A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.

(p2.11) The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level. Note, however, that the relations between arguments and claim are still relatively clear (e.g. A 2 supports while A 5 attacks the main claim in A 1 ; A 3 attacks A 2 directly; and A 4 closes any further 4 https://www.kialo.com/explore/ featured discussion on A 3 's premise).

(p2.12) Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict. Working in the realm of overtly argumentative text (such as persuasive essays (Stab and Gurevych, 2017)), while challenging of course, can be quite standardized. The language use is generally in line with natural language expectations and often standard (e.g. claim, premise and stance are clear), the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (e.g. in parliamentary debates, news forums, etc.).","[['b29', None, 'b28', 'b26'], ['b49', 'b14'], ['b35', 'b44', None, 'b31'], [], [], [], [], [], [], [], [], [], ['b44']]","[['b29', None, 'b28', 'b26'], ['b49', 'b14'], ['b35', 'b44', None, 'b31'], [], [], [], [], [], [], [], [], [], ['b44']]",11,"1. Cabrio and Villata (2018) provide an elaborate overview of the AM framework in their data-driven analysis of the state of the art after five years of significant developments in the field of AM.
2. Generally speaking, given a collection of natural language texts, the task at hand is implemented in two stages: Argument extraction The system first identifies the documents which contain the argumentative structure and the specific textual spans in which argumentation is encoded.
3. Once the textual boundaries are defined, subportions of the argumentative spans are assigned to a set of pre-established argument components (e.g. claims, premises, rebuttal, etc.).
4. A variety of models were used for this including Näive Bayes (Moens et al., 2007), SVMs (Mochales andMoens, 2011), RNNs (Niculae et al., 2017;Eger et al., 2017), Pre-trained Language Models (Chakrabarty et al., 2019;Lugini and Litman, 2020), and other supervised-learning techniques (Ein-Dor et al., 2020).Relation assignment
5. The goal of the second stage is to model the relations between the argumentative spans identified in the first stage.
6. These relations can exist between different arguments (support, attack) as well as within an argument (connecting the premises with the claim).
7. Recent approaches to argumentative relation classification investigate for example relational models (Trautmann et al., 2020) or inject background knowledge by leveraging features from different knowledge bases (e.g. claims, premises, rebuttal, etc.)0.
8. Detecting these relations is necessary to model the overall structure of the argumentation (e.g. claims, premises, rebuttal, etc.)1.
9. As this structure can be complex, the task is difficult, involving high-level knowledge representation and reasoning issues.
10. After the relations are detected, the discourse structure can then be mapped to a graph representation, called argumentation graph, with the arguments as nodes and relations as edges.
11. To simplify the problem, some approaches reduce the graph to a tree-structure representation (e.g. claims, premises, rebuttal, etc.)2.
12. Different methods to generate the structure have been investigated, e.g. SVMs (e.g. claims, premises, rebuttal, etc.)3 or textual entailment (e.g. claims, premises, rebuttal, etc.)4.
13. Modeling the relations and argumentation flow within a debate is an important factor when defining the notion of argument quality, which will be presented in Section 3.Consider the following example taken from an online debate about compulsory vaccinations 3 which demonstrates the framework quite clearly.
14. Given a statement presenting background and context, participants are asked to discuss the question ""Does public health demand vaccinations?""
15. (e.g. claims, premises, rebuttal, etc.)5 A1: A vaccine is the best way to prevent an outbreak of a disease or to reduce its negative effects.
16. Vaccinated people become immune to a certain pathogen and do not develop a disease.
17. Although there are occasionally side effects, these affect only a tiny number of people compared to the protection offered to the vast majority.
18. A2: Many vaccines have serious and sometimes deadly side effects.
19. With many vaccines the immunity is not lifelong.
20. Sometimes the vaccines itself can cause a serious disease to develop as a side effect.
21. If governments know that compulsory mass vaccination is likely to cause death or permanent disability in even a few cases, it is immoral for them to make it compulsory.
22. Here, the argumentative text boundaries are first determined from the natural language discussion and the argument components (e.g. claims, premises, rebuttal, etc.)6 are extracted.
23. Then, the relations between the two arguments are as follows: A 1 supports the argument while A 2 attacks it.
24. However, consider another example, extracted from an online debate platform Kialo 4 .
25. Here, the participants' contribution and the structure mirror a more direct and conversational dynamic to argumentation.
26. A1: Marvel Universe is better than DC Universe.
27. A2: Stan Lee's vision contains clarity and purpose, while DC is simply interested in churning entertainment to the masses.
28. A3: Stan Lee no-longer has control over any of marvel, which can cloud the purpose of Marvel due to it being owned by Disney.
29. A4: This is especially true due to his unfortunate passing.
30. A5: DC has been more apt to recycle parts of Intellectual Property, they even made an entire movie using the ideas of the 1960's characters and comics.
31. The seemingly simple example of an online exchange shows how a more conversational environment provides vaguer boundaries of argumentation structure and components.
32. Each argument is more direct, not necessarily consisting of a claimpremise configuration, and the strength and productive quality of each argument is particularly relative to the context, each contribution affecting the argument differently either at a local or global level.
33. Note, however, that the relations between arguments and claim are still relatively clear (e.g. claims, premises, rebuttal, etc.)7.
34. Clearly, the environment and type of platform under consideration have a significant impact on a system's capacity to implement such a framework and on the degree of complexity found in the components and relations to extract, assign, and predict.
35. Working in the realm of overtly argumentative text (e.g. claims, premises, rebuttal, etc.)8), while challenging of course, can be quite standardized.
36. The language use is generally in line with natural language expectations and often standard (e.g. claims, premises, rebuttal, etc.)9, the structure and collective goal of the debate are rather controlled and topic-specific, and the collection of participants involved is often a closed or an easilyclassified set (Moens et al., 2007)0."
236460206,Towards Argument Mining for Social Good: A Survey,"Political Science, Linguistics, Computer Science",https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,s3,Scaling Up Argument Mining,"['p3.0', 'p3.1', 'p3.2', 'p3.3', 'p3.4', 'p3.5', 'p3.6', 'p3.7']","['In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.', 'Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.', 'Recent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.', 'Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.', 'The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.', 'Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.', 'Various recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.', 'Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.']","In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.

Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.

Recent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.

Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.

Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.

Various recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.

Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.","(p3.0) In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.

(p3.1) Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020). Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.

(p3.2) Recent work has aimed to tackle such challenges in social media. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.

(p3.3) Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020). On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited. Recent work by Schaefer and Stede (2020), among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis. Bauwelinck and Lefever (2020) introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.

(p3.4) The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.

(p3.5) Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.

(p3.6) Various recent studies have investigated multilinguality for AM. Eger et al. (2019) discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (2020), which use a pre-trained multilingual BERT (Devlin et al., 2019) for modeling. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores. Multilingual stance detection in political social media text (Vamvas and Sennrich, 2020) is also investigated in Lai et al. (2020) using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.

(p3.7) Other work has also dealt with building non-English datasets (Lindahl, 2020;Bauwelinck and Lefever, 2020;Schaefer and Stede, 2020;Zotova et al., 2020), but there still seems to be a focus on Indo-European languages (and sometimes Chinese) with a lack of datasets and analysis extending to other languages. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Blodgett et al., 2016) and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Davidson et al., 2019). This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact.","[[], ['b15'], [None], ['b38'], [], [None], [None, 'b16', 'b52'], ['b25', 'b38', None, 'b62']]","[[], ['b15'], [None], ['b38'], [], [None], [None, 'b16', 'b52'], ['b25', 'b38', None, 'b62']]",11,"1. In social media While overtly argumentative text, like those described above, represents the natural domain of application for AM, social media constitute a powerful source of large amounts of data (billions of words) despite facing particular challenges in AM.
2. Social media plays an increasingly significant role in modern political and social discourse, yet resources built for conducting AM on this type of data structure remain limited for clear reasons.
3. These platforms inherently collect and spread a wide range of content, including personal opinions, facts, fake news, and additional information of interest to users.
4. Distinguishing between personal opinion, fact, and fake news, for example, is not always straightforward, as seen in recent work on fake news detection (Kotonya and Toni, 2020).
5. Further, the language used on such platforms is infamously chaotic and often non-standard in comparison to the language use in more structured environments, like parliamentary debates.
6. The combination of these aspects introduces the unique challenge of implementing AM to particularly heterogeneous, poorly annotated data.
7. Recent work has aimed to tackle such challenges in social media.
8. Dusmanu et al. (2017) apply a supervised classification approach to identify arguments on Twitter, focusing on the tasks of facts recognition and source identification.
9. They study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexitnews topics.
10. Habernal and Gurevych (2017) provide an extensive analysis of the steps and the modeling strategies necessary to analyze social media data (e.g. forum posts) in terms of their argumentative structure, while Simpson and Gurevych (2018) tackle the issue of the scalability of AM algorithms.
11. Despite the rising attention and developments to AM in social media, one of the major challenges currently facing the field is the lack of consensus on how exactly to analyse argumentative user-generated texts such as online comments (Bauwelinck and Lefever, 2020).
12. On the one hand, the amount of annotations available for the scale of this heterogeneous data remains limited.
13. Recent work by Schaefer and Stede (Kotonya and Toni, 2020)4, among others, have aimed to construct large Twitter corpora annotated for argument components, including argumentative spans within tweets.
14. On the other hand, annotation guidelines are not necessarily clear, and the theoretical motivations underlying the proposed guidelines used to generate labelled corpora rarely include motivation for the use of a particular theoretical basis.
15. Bauwelinck and Lefever (Kotonya and Toni, 2020)4 introduce a pilot study and aim to provide a clear justification of the theories and definitions underlying the design of a set of guidelines.
16. The linguistic, structural, and logistic complexity and ""openness"" of such platforms clearly present unique challenges.
17. However, being able to work well with argumentative text from social media and discussion forums is essential considering the continuously growing impact on the political and social framework of modern times.
18. Multilingual argument mining Multilinguality is an important area of research in NLP that has gained more attention recently because of the crosslingual transfer potentials of Pre-trained Language Models (Devlin et al., 2019;Conneau et al., 2020) and because of the potentials for a societal impact at a global scale.
19. The latter is particularly important when considering AM for Social Good since language should not be a barrier for participation if the goal is to allow any productive contribution.
20. Various recent studies have investigated multilinguality for AM.
21. Eger et al. (Kotonya and Toni, 2020)0 discuss a series of experiments on using machine translation and annotation projection for AM, specifically argument components extraction and classification in German, English, and Chinese.
22. A similar approach to build training data in other languages using machine translation is done in Toledo-Ronen et al. (Kotonya and Toni, 2020)4, which use a pre-trained multilingual BERT (Kotonya and Toni, 2020)2 for modeling.
23. This approach is shown to perform well for classifying argument stance and detecting evidence, but not for predicting argument quality scores.
24. Multilingual stance detection in political social media text (Kotonya and Toni, 2020)3 is also investigated in Lai et al. (Kotonya and Toni, 2020)4 using stylistic, structural, affective and contextual features from text and analysing the scenarios in which each of these features is effective.
25. Other work has also dealt with building non-English datasets (Kotonya and Toni, 2020)5, but there still seems to be a focus on Indo-European languages (Kotonya and Toni, 2020)6 with a lack of datasets and analysis extending to other languages.
26. This is a general issue in NLP research that extends to performance bias in favor of standard dialects for example in English (Kotonya and Toni, 2020)7 and bias that could target certain user groups instead of protecting them as was shown for Hate Speech Detection (Kotonya and Toni, 2020)8.
27. This is an important limitation to address in AM as well for more inclusivity and towards a more positive societal impact."
236460206,Towards Argument Mining for Social Good: A Survey,"Political Science, Linguistics, Computer Science",https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,s4,Argument Quality: An Integrated Definition,"['p4.0', 'p4.1', 'p4.2', 'p4.3', 'p4.4', 'p4.5', 'p4.6', 'p4.7', 'p4.8', 'p4.9', 'p4.10', 'p4.11']","['The second stage in the framework of AM is defined as relation assignment (c.f. Section 2.1); a complex task that aims to predict the relations holding between the arguments defined in the first stage. Being able to model the relations between arguments and components within the structure, for example in argument graphs (Besnard and Hunter, 2014; Craven and Toni, 2016), allows us to actually work with the argumentative text in an applicationbased setting, understand the stance and context of arguments, and develop a story for the consequential impact of arguments on the discourse, among other things. Generally speaking, we can use this task as an approach to analyze argument quality (AQ). However, within the AM community, an open question concerns the adequate definition and operationalization of the notion of AQ. Despite this, to move forward with the task of AQ analysis and to create large corpora with crowd-sourced annotations, some approaches rely on the relative assessment of quality: Given two arguments, which is more convincing? (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020) Thus the natural way of quantifying the success of an argument is in terms of its persuasiveness. Indeed, plenty of previous work has explored the many factors which contribute to the persuasiveness of a message: the linguistic features employed by the authors (Persing and Ng, 2017), the semantic type of claims and premises (Hidey et al., 2017), the different sources of evidence produced to support an argument (Addawood and Bashir, 2016), the effects of the personality traits and prior beliefs on persuasiveness (Lukin et al., 2017;Durmus and Cardie, 2018;Al Khatib et al., 2020), the interaction with other participants (Ji et al., 2018;Egawa et al., 2020), the use of argument invention when debating about unknown topics (Bilu et al., 2019), the structure of the arguments (Li et al., 2020), and the effect of the style of the text in achieving persuasion (El Baff et al., 2020).', 'Persuasiveness is, however, not the only way to define whether an argument is good -at least not from a deliberation point of view. A good contribution to a debate is one which uncovers a previously unnoticed aspect of a problem, thus generating a perturbation in the discourse (controversies can be productive!). Or else, a good contribution is one that settles an issue, by stating the differences between opposing views and allowing the discourse to stabilize in a series of clusters (convergence on just one position is not necessarily a good outcome).', 'Most recent research projects (Wachsmuth et al., 2017b) aim to address the challenge of redefining the notion of AQ, away from persuasiveness and towards a more ""situated"" definition which has to do with the needs of argumentation in a real-world scenario. This new definition has been the basis for the creation of new corpora from different domains , where feature-based (Wachsmuth and Werner, 2020) and neural models were tested for automatic prediction . Other aspects of AQ have become the subject of AM research such as the relevance and impact of arguments (Durmus et al., 2019), the verifiability (Park and Cardie, 2018), local acceptability (Yang et al., 2019) and the best ""deliberative move"" (Al-Khatib et al., 2018).', 'We argue that this shift is necessary for two reasons: (1) Working with real-world applications of AM naturally forces us into the more heterogeneous realm of data structures, such as social media, in which language, structure, and content are less uniform and confined to the classic notion of logical debate; and (2) In order to encourage deliberation from an open audience of citizens, we need to redefine our concept of AQ and productive discourse such that there is equal worth and participation granted to each contributor of the argument.', 'Deliberative Quality We therefore propose adapting the definition of quality to integrate the abundant research on the topic from the field of Social Sciences. Here, the quality of a discourse has been investigated in the context of deliberation with the focus on inclusivity: how can the interplay of the different participants in the discourse lead to an optimal outcome for the collective? The focus here is not on the quality of the individual contributions. Instead, an overall quality of the discourse is determined by the fact that the individual quality dimensions are distributed among different contributions (e.g some participants do more rational reasoning, others share personal experiences). We would like to integrate those aspects that focus on inclusivity and cooperation.', 'Similar to Wachsmuth et al. (2017b), social scientists have developed a taxonomy, the discourse quality index (DQI), that describes the different desirable aspects of a discourse (Steenbergen et al., 2003). This taxonomy has been used to analyze the quality of deliberation in different contexts, ranging from more formal contexts, such as parliamentary debates (Steiner et al., 2005), to informal discussions in online forums (Trénel, 2004). Both implementations integrate logical coherence as one dimension, cogency in Wachsmuth et al. (2017b), justification in the DQI. Some aspects of inclusivity are also being touched upon in the rhetorical and dialectical dimension of Wachsmuth et al. (2017b), such as using appropriate language (Appropriateness) or whether an argument supports conflict resolution (global relevance). We concentrate on the following dimensions from the DQI, which particularly focus on the collaborative aspect of discourse.', '• Respect: this dimension includes respectful tone, respect for other social groups/backgrounds, and openness towards other opinions.', '• Equality / Participation: it is not desirable that some dominant participants make the bulk of contributions while many others remain passive. All participants should have equal opportunities to contribute and all topics, including those that DQI (Steenbergen et al., 2003)  • Interactivity: beyond simply sharing opinions, acknowledging other viewpoints and interacting with other participants through listening and responding lead to new perspectives arising -compromises can emerge.', '• Testimoniality / Report of personal accounts:', 'sharing stories and personal narratives as an alternative form of communication can involve more people in the discourse, especially those who cannot identify themselves with rational argumentation. It can also make other participants aware of other perspectives as it generally increases empathy. Especially when traditional or universal norms need to be questioned, narratives are particularly well suited, as their ambiguity and vagueness creates room for interpretation. This is particularly important when new ideas or perspectives are introduced, since they cannot yet be rationally articulated. Table 1 establishes a direct comparison between discourse quality dimensions of the DQI (Steenbergen et al., 2003;Steiner et al., 2005) and argument quality dimensions as defined in Wachsmuth et al. (2017b). Apart from the potential theoretical insights, the existing guidelines can be applied to annotate new or enrich existing corpora for AM. Despite the small size, the data already annotated based on the DQI can be made usable and extended for NLP. In addition, some of the quality dimensions can be further quantified or approximated using statistical methods. For example, interactivity or equality can be assessed with frequency-based methods, such as frequency of posts by distinct participants and response rate.', 'Summing up The overview of the definitions of AQ along with the discussion of the potential of the integration of Deliberative Quality features into an AM framework has one strong take-home message:', 'The need for the scope of the investigation to go beyond (a) the persuasiveness of a an argumentative text (speeches, forum posts, tweets), and (b) their relation to the immediate preceding discourse. Instead, we pointed out the need to also assess the potential of the impact of that argumentative text on the upcoming discourse: this dimension of quality, inherently related to the interpretation of argumentation as a cooperation challenge, is currently lacking in current approaches to AQ.']","The second stage in the framework of AM is defined as relation assignment (c.f. Section 2.1); a complex task that aims to predict the relations holding between the arguments defined in the first stage. Being able to model the relations between arguments and components within the structure, for example in argument graphs (Besnard and Hunter, 2014; Craven and Toni, 2016), allows us to actually work with the argumentative text in an applicationbased setting, understand the stance and context of arguments, and develop a story for the consequential impact of arguments on the discourse, among other things. Generally speaking, we can use this task as an approach to analyze argument quality (AQ). However, within the AM community, an open question concerns the adequate definition and operationalization of the notion of AQ. Despite this, to move forward with the task of AQ analysis and to create large corpora with crowd-sourced annotations, some approaches rely on the relative assessment of quality: Given two arguments, which is more convincing? (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020) Thus the natural way of quantifying the success of an argument is in terms of its persuasiveness. Indeed, plenty of previous work has explored the many factors which contribute to the persuasiveness of a message: the linguistic features employed by the authors (Persing and Ng, 2017), the semantic type of claims and premises (Hidey et al., 2017), the different sources of evidence produced to support an argument (Addawood and Bashir, 2016), the effects of the personality traits and prior beliefs on persuasiveness (Lukin et al., 2017;Durmus and Cardie, 2018;Al Khatib et al., 2020), the interaction with other participants (Ji et al., 2018;Egawa et al., 2020), the use of argument invention when debating about unknown topics (Bilu et al., 2019), the structure of the arguments (Li et al., 2020), and the effect of the style of the text in achieving persuasion (El Baff et al., 2020).

Persuasiveness is, however, not the only way to define whether an argument is good -at least not from a deliberation point of view. A good contribution to a debate is one which uncovers a previously unnoticed aspect of a problem, thus generating a perturbation in the discourse (controversies can be productive!). Or else, a good contribution is one that settles an issue, by stating the differences between opposing views and allowing the discourse to stabilize in a series of clusters (convergence on just one position is not necessarily a good outcome).

Most recent research projects (Wachsmuth et al., 2017b) aim to address the challenge of redefining the notion of AQ, away from persuasiveness and towards a more ""situated"" definition which has to do with the needs of argumentation in a real-world scenario. This new definition has been the basis for the creation of new corpora from different domains , where feature-based (Wachsmuth and Werner, 2020) and neural models were tested for automatic prediction . Other aspects of AQ have become the subject of AM research such as the relevance and impact of arguments (Durmus et al., 2019), the verifiability (Park and Cardie, 2018), local acceptability (Yang et al., 2019) and the best ""deliberative move"" (Al-Khatib et al., 2018).

We argue that this shift is necessary for two reasons: (1) Working with real-world applications of AM naturally forces us into the more heterogeneous realm of data structures, such as social media, in which language, structure, and content are less uniform and confined to the classic notion of logical debate; and (2) In order to encourage deliberation from an open audience of citizens, we need to redefine our concept of AQ and productive discourse such that there is equal worth and participation granted to each contributor of the argument.

Deliberative Quality We therefore propose adapting the definition of quality to integrate the abundant research on the topic from the field of Social Sciences. Here, the quality of a discourse has been investigated in the context of deliberation with the focus on inclusivity: how can the interplay of the different participants in the discourse lead to an optimal outcome for the collective? The focus here is not on the quality of the individual contributions. Instead, an overall quality of the discourse is determined by the fact that the individual quality dimensions are distributed among different contributions (e.g some participants do more rational reasoning, others share personal experiences). We would like to integrate those aspects that focus on inclusivity and cooperation.

Similar to Wachsmuth et al. (2017b), social scientists have developed a taxonomy, the discourse quality index (DQI), that describes the different desirable aspects of a discourse (Steenbergen et al., 2003). This taxonomy has been used to analyze the quality of deliberation in different contexts, ranging from more formal contexts, such as parliamentary debates (Steiner et al., 2005), to informal discussions in online forums (Trénel, 2004). Both implementations integrate logical coherence as one dimension, cogency in Wachsmuth et al. (2017b), justification in the DQI. Some aspects of inclusivity are also being touched upon in the rhetorical and dialectical dimension of Wachsmuth et al. (2017b), such as using appropriate language (Appropriateness) or whether an argument supports conflict resolution (global relevance). We concentrate on the following dimensions from the DQI, which particularly focus on the collaborative aspect of discourse.

• Respect: this dimension includes respectful tone, respect for other social groups/backgrounds, and openness towards other opinions.

• Equality / Participation: it is not desirable that some dominant participants make the bulk of contributions while many others remain passive. All participants should have equal opportunities to contribute and all topics, including those that DQI (Steenbergen et al., 2003)  • Interactivity: beyond simply sharing opinions, acknowledging other viewpoints and interacting with other participants through listening and responding lead to new perspectives arising -compromises can emerge.

• Testimoniality / Report of personal accounts:

sharing stories and personal narratives as an alternative form of communication can involve more people in the discourse, especially those who cannot identify themselves with rational argumentation. It can also make other participants aware of other perspectives as it generally increases empathy. Especially when traditional or universal norms need to be questioned, narratives are particularly well suited, as their ambiguity and vagueness creates room for interpretation. This is particularly important when new ideas or perspectives are introduced, since they cannot yet be rationally articulated. Table 1 establishes a direct comparison between discourse quality dimensions of the DQI (Steenbergen et al., 2003;Steiner et al., 2005) and argument quality dimensions as defined in Wachsmuth et al. (2017b). Apart from the potential theoretical insights, the existing guidelines can be applied to annotate new or enrich existing corpora for AM. Despite the small size, the data already annotated based on the DQI can be made usable and extended for NLP. In addition, some of the quality dimensions can be further quantified or approximated using statistical methods. For example, interactivity or equality can be assessed with frequency-based methods, such as frequency of posts by distinct participants and response rate.

Summing up The overview of the definitions of AQ along with the discussion of the potential of the integration of Deliberative Quality features into an AM framework has one strong take-home message:

The need for the scope of the investigation to go beyond (a) the persuasiveness of a an argumentative text (speeches, forum posts, tweets), and (b) their relation to the immediate preceding discourse. Instead, we pointed out the need to also assess the potential of the impact of that argumentative text on the upcoming discourse: this dimension of quality, inherently related to the interpretation of argumentation as a cooperation challenge, is currently lacking in current approaches to AQ.","(p4.0) The second stage in the framework of AM is defined as relation assignment (c.f. Section 2.1); a complex task that aims to predict the relations holding between the arguments defined in the first stage. Being able to model the relations between arguments and components within the structure, for example in argument graphs (Besnard and Hunter, 2014; Craven and Toni, 2016), allows us to actually work with the argumentative text in an applicationbased setting, understand the stance and context of arguments, and develop a story for the consequential impact of arguments on the discourse, among other things. Generally speaking, we can use this task as an approach to analyze argument quality (AQ). However, within the AM community, an open question concerns the adequate definition and operationalization of the notion of AQ. Despite this, to move forward with the task of AQ analysis and to create large corpora with crowd-sourced annotations, some approaches rely on the relative assessment of quality: Given two arguments, which is more convincing? (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020) Thus the natural way of quantifying the success of an argument is in terms of its persuasiveness. Indeed, plenty of previous work has explored the many factors which contribute to the persuasiveness of a message: the linguistic features employed by the authors (Persing and Ng, 2017), the semantic type of claims and premises (Hidey et al., 2017), the different sources of evidence produced to support an argument (Addawood and Bashir, 2016), the effects of the personality traits and prior beliefs on persuasiveness (Lukin et al., 2017;Durmus and Cardie, 2018;Al Khatib et al., 2020), the interaction with other participants (Ji et al., 2018;Egawa et al., 2020), the use of argument invention when debating about unknown topics (Bilu et al., 2019), the structure of the arguments (Li et al., 2020), and the effect of the style of the text in achieving persuasion (El Baff et al., 2020).

(p4.1) Persuasiveness is, however, not the only way to define whether an argument is good -at least not from a deliberation point of view. A good contribution to a debate is one which uncovers a previously unnoticed aspect of a problem, thus generating a perturbation in the discourse (controversies can be productive!). Or else, a good contribution is one that settles an issue, by stating the differences between opposing views and allowing the discourse to stabilize in a series of clusters (convergence on just one position is not necessarily a good outcome).

(p4.2) Most recent research projects (Wachsmuth et al., 2017b) aim to address the challenge of redefining the notion of AQ, away from persuasiveness and towards a more ""situated"" definition which has to do with the needs of argumentation in a real-world scenario. This new definition has been the basis for the creation of new corpora from different domains , where feature-based (Wachsmuth and Werner, 2020) and neural models were tested for automatic prediction . Other aspects of AQ have become the subject of AM research such as the relevance and impact of arguments (Durmus et al., 2019), the verifiability (Park and Cardie, 2018), local acceptability (Yang et al., 2019) and the best ""deliberative move"" (Al-Khatib et al., 2018).

(p4.3) We argue that this shift is necessary for two reasons: (1) Working with real-world applications of AM naturally forces us into the more heterogeneous realm of data structures, such as social media, in which language, structure, and content are less uniform and confined to the classic notion of logical debate; and (2) In order to encourage deliberation from an open audience of citizens, we need to redefine our concept of AQ and productive discourse such that there is equal worth and participation granted to each contributor of the argument.

(p4.4) Deliberative Quality We therefore propose adapting the definition of quality to integrate the abundant research on the topic from the field of Social Sciences. Here, the quality of a discourse has been investigated in the context of deliberation with the focus on inclusivity: how can the interplay of the different participants in the discourse lead to an optimal outcome for the collective? The focus here is not on the quality of the individual contributions. Instead, an overall quality of the discourse is determined by the fact that the individual quality dimensions are distributed among different contributions (e.g some participants do more rational reasoning, others share personal experiences). We would like to integrate those aspects that focus on inclusivity and cooperation.

(p4.5) Similar to Wachsmuth et al. (2017b), social scientists have developed a taxonomy, the discourse quality index (DQI), that describes the different desirable aspects of a discourse (Steenbergen et al., 2003). This taxonomy has been used to analyze the quality of deliberation in different contexts, ranging from more formal contexts, such as parliamentary debates (Steiner et al., 2005), to informal discussions in online forums (Trénel, 2004). Both implementations integrate logical coherence as one dimension, cogency in Wachsmuth et al. (2017b), justification in the DQI. Some aspects of inclusivity are also being touched upon in the rhetorical and dialectical dimension of Wachsmuth et al. (2017b), such as using appropriate language (Appropriateness) or whether an argument supports conflict resolution (global relevance). We concentrate on the following dimensions from the DQI, which particularly focus on the collaborative aspect of discourse.

(p4.6) • Respect: this dimension includes respectful tone, respect for other social groups/backgrounds, and openness towards other opinions.

(p4.7) • Equality / Participation: it is not desirable that some dominant participants make the bulk of contributions while many others remain passive. All participants should have equal opportunities to contribute and all topics, including those that DQI (Steenbergen et al., 2003)  • Interactivity: beyond simply sharing opinions, acknowledging other viewpoints and interacting with other participants through listening and responding lead to new perspectives arising -compromises can emerge.

(p4.8) • Testimoniality / Report of personal accounts:

(p4.9) sharing stories and personal narratives as an alternative form of communication can involve more people in the discourse, especially those who cannot identify themselves with rational argumentation. It can also make other participants aware of other perspectives as it generally increases empathy. Especially when traditional or universal norms need to be questioned, narratives are particularly well suited, as their ambiguity and vagueness creates room for interpretation. This is particularly important when new ideas or perspectives are introduced, since they cannot yet be rationally articulated. Table 1 establishes a direct comparison between discourse quality dimensions of the DQI (Steenbergen et al., 2003;Steiner et al., 2005) and argument quality dimensions as defined in Wachsmuth et al. (2017b). Apart from the potential theoretical insights, the existing guidelines can be applied to annotate new or enrich existing corpora for AM. Despite the small size, the data already annotated based on the DQI can be made usable and extended for NLP. In addition, some of the quality dimensions can be further quantified or approximated using statistical methods. For example, interactivity or equality can be assessed with frequency-based methods, such as frequency of posts by distinct participants and response rate.

(p4.10) Summing up The overview of the definitions of AQ along with the discussion of the potential of the integration of Deliberative Quality features into an AM framework has one strong take-home message:

(p4.11) The need for the scope of the investigation to go beyond (a) the persuasiveness of a an argumentative text (speeches, forum posts, tweets), and (b) their relation to the immediate preceding discourse. Instead, we pointed out the need to also assess the potential of the impact of that argumentative text on the upcoming discourse: this dimension of quality, inherently related to the interpretation of argumentation as a cooperation challenge, is currently lacking in current approaches to AQ.","[['b6', 'b9', 'b23', 'b36', None, 'b27', 'b47'], [], ['b56', 'b33', None, 'b55', 'b61'], [], [], ['b50', 'b55', 'b46', 'b45'], [], ['b45'], [], ['b46', 'b55', 'b45'], [], []]","[['b6', 'b9', 'b23', 'b36', None, 'b27', 'b47'], [], ['b56', 'b33', None, 'b55', 'b61'], [], [], ['b50', 'b55', 'b46', 'b45'], [], ['b45'], [], ['b46', 'b55', 'b45'], [], []]",20,"1. The second stage in the framework of AM is defined as relation assignment (c.f. Section 2.1); a complex task that aims to predict the relations holding between the arguments defined in the first stage.
2. Being able to model the relations between arguments and components within the structure, for example in argument graphs (Besnard and Hunter, 2014; Craven and Toni, 2016), allows us to actually work with the argumentative text in an applicationbased setting, understand the stance and context of arguments, and develop a story for the consequential impact of arguments on the discourse, among other things.
3. Generally speaking, we can use this task as an approach to analyze argument quality (AQ).
4. However, within the AM community, an open question concerns the adequate definition and operationalization of the notion of AQ.
5. Despite this, to move forward with the task of AQ analysis and to create large corpora with crowd-sourced annotations, some approaches rely on the relative assessment of quality: Given two arguments, which is more convincing?
6. (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)
7. Thus the natural way of quantifying the success of an argument is in terms of its persuasiveness.
8. Indeed, plenty of previous work has explored the many factors which contribute to the persuasiveness of a message: the linguistic features employed by the authors (Persing and Ng, 2017), the semantic type of claims and premises (Hidey et al., 2017), the different sources of evidence produced to support an argument (Addawood and Bashir, 2016), the effects of the personality traits and prior beliefs on persuasiveness (Lukin et al., 2017;Durmus and Cardie, 2018;Al Khatib et al., 2020), the interaction with other participants (Ji et al., 2018;Egawa et al., 2020), the use of argument invention when debating about unknown topics (Bilu et al., 2019), the structure of the arguments (Besnard and Hunter, 2014; Craven and Toni, 2016)0, and the effect of the style of the text in achieving persuasion (Besnard and Hunter, 2014; Craven and Toni, 2016)1.
9. Persuasiveness is, however, not the only way to define whether an argument is good -at least not from a deliberation point of view.
10. A good contribution to a debate is one which uncovers a previously unnoticed aspect of a problem, thus generating a perturbation in the discourse (Besnard and Hunter, 2014; Craven and Toni, 2016)2.
11. Or else, a good contribution is one that settles an issue, by stating the differences between opposing views and allowing the discourse to stabilize in a series of clusters (Besnard and Hunter, 2014; Craven and Toni, 2016)3.
12. Most recent research projects (Besnard and Hunter, 2014; Craven and Toni, 2016)4 aim to address the challenge of redefining the notion of AQ, away from persuasiveness and towards a more ""situated"" definition which has to do with the needs of argumentation in a real-world scenario.
13. This new definition has been the basis for the creation of new corpora from different domains , where feature-based (Besnard and Hunter, 2014; Craven and Toni, 2016)5 and neural models were tested for automatic prediction .
14. Other aspects of AQ have become the subject of AM research such as the relevance and impact of arguments (Besnard and Hunter, 2014; Craven and Toni, 2016)6, the verifiability (Besnard and Hunter, 2014; Craven and Toni, 2016)7, local acceptability (Besnard and Hunter, 2014; Craven and Toni, 2016)8 and the best ""deliberative move"" (Besnard and Hunter, 2014; Craven and Toni, 2016)9.
15. We argue that this shift is necessary for two reasons: (AQ)0 Working with real-world applications of AM naturally forces us into the more heterogeneous realm of data structures, such as social media, in which language, structure, and content are less uniform and confined to the classic notion of logical debate; and
16. (AQ)1 In order to encourage deliberation from an open audience of citizens, we need to redefine our concept of AQ and productive discourse such that there is equal worth and participation granted to each contributor of the argument.
17. Deliberative Quality We therefore propose adapting the definition of quality to integrate the abundant research on the topic from the field of Social Sciences.
18. Here, the quality of a discourse has been investigated in the context of deliberation with the focus on inclusivity: how can the interplay of the different participants in the discourse lead to an optimal outcome for the collective?
19. The focus here is not on the quality of the individual contributions.
20. Instead, an overall quality of the discourse is determined by the fact that the individual quality dimensions are distributed among different contributions (AQ)2.
21. We would like to integrate those aspects that focus on inclusivity and cooperation.
22. Similar to Wachsmuth et al. (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)4, social scientists have developed a taxonomy, the discourse quality index (AQ)4, that describes the different desirable aspects of a discourse (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)2.
23. This taxonomy has been used to analyze the quality of deliberation in different contexts, ranging from more formal contexts, such as parliamentary debates (AQ)6, to informal discussions in online forums (AQ)7.
24. Both implementations integrate logical coherence as one dimension, cogency in Wachsmuth et al. (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)4, justification in the DQI.
25. Some aspects of inclusivity are also being touched upon in the rhetorical and dialectical dimension of Wachsmuth et al. (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)4, such as using appropriate language (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)0 or whether an argument supports conflict resolution (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)1.
26. We concentrate on the following dimensions from the DQI, which particularly focus on the collaborative aspect of discourse.
27. Respect: this dimension includes respectful tone, respect for other social groups/backgrounds, and openness towards other opinions.
28. Equality / Participation: it is not desirable that some dominant participants make the bulk of contributions while many others remain passive.
29. All participants should have equal opportunities to contribute and all topics, including those that DQI (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)2   Interactivity: beyond simply sharing opinions, acknowledging other viewpoints and interacting with other participants through listening and responding lead to new perspectives arising -compromises can emerge.
30. Testimoniality / Report of personal accounts:sharing stories and personal narratives as an alternative form of communication can involve more people in the discourse, especially those who cannot identify themselves with rational argumentation.
31. It can also make other participants aware of other perspectives as it generally increases empathy.
32. Especially when traditional or universal norms need to be questioned, narratives are particularly well suited, as their ambiguity and vagueness creates room for interpretation.
33. This is particularly important when new ideas or perspectives are introduced, since they cannot yet be rationally articulated.
34. Table 1 establishes a direct comparison between discourse quality dimensions of the DQI (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)3 and argument quality dimensions as defined in Wachsmuth et al. (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)4.
35. Apart from the potential theoretical insights, the existing guidelines can be applied to annotate new or enrich existing corpora for AM.
36. Despite the small size, the data already annotated based on the DQI can be made usable and extended for NLP.
37. In addition, some of the quality dimensions can be further quantified or approximated using statistical methods.
38. For example, interactivity or equality can be assessed with frequency-based methods, such as frequency of posts by distinct participants and response rate.
39. Summing up The overview of the definitions of AQ along with the discussion of the potential of the integration of Deliberative Quality features into an AM framework has one strong take-home message:
40. The need for the scope of the investigation to go beyond (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)5 the persuasiveness of a an argumentative text (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)6, and (Habernal and Gurevych, 2016;Toledo et al., 2019;Gretz et al., 2020)7 their relation to the immediate preceding discourse.
41. Instead, we pointed out the need to also assess the potential of the impact of that argumentative text on the upcoming discourse: this dimension of quality, inherently related to the interpretation of argumentation as a cooperation challenge, is currently lacking in current approaches to AQ."
236460206,Towards Argument Mining for Social Good: A Survey,"Political Science, Linguistics, Computer Science",https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,s5,Grounding AQ in deliberation: moderation as a real-world application,"['p5.0', 'p5.1', 'p5.2', 'p5.3', 'p5.4', 'p5.5', 'p5.6']","['Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies. We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Trénel, 2009). To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom. This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix). Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers). The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks? 5 The example involves two users who clearly differ in their argumentation style and position. User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text. User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair. This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones. A moderator then joins the discussion to (a) provide a clarification as to why the focus is on texting and a link to further information on the matter, and (b) ask User 2 to elaborate on the personal communication issue, and to propose alternatives. In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.', ""The guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (eRulemaking Initiative et al., 2017) which reflect the moderator actions mentioned in the examples. In the protocol the moderator roles were divided into two main classes. Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties. Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse. As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences)."", 'RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants. However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling. Park et al. (2012) used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol. Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment. However this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".', 'Besides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy. 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.', 'Digital innovation supporting deliberation Argument maps (Walton, 2005) are widely employed to support online discussions, as an emerging optimization of the deliberation. Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post. Their contribution to the argument maps is often reviewed by a moderator. So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (the user) and above (the moderator).', ""Thanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (or many) of the available options, without having to read a large number of long posts. An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (Klein, 2011). Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (Yang et al., 2021;Ito, 2018). Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (2015) tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament. This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps. The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (fewer new ideas as compared to the traditional forum), it also reduced the rate of claims without further discussion."", ""Yet, the need for trained moderators tends to be a significant bottleneck (both in terms of time and of costs) in digital deliberation. Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation. For example, the experiment in Spada and Vreeland (2013) tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process.""]","Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies. We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Trénel, 2009). To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom. This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix). Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers). The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks? 5 The example involves two users who clearly differ in their argumentation style and position. User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text. User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair. This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones. A moderator then joins the discussion to (a) provide a clarification as to why the focus is on texting and a link to further information on the matter, and (b) ask User 2 to elaborate on the personal communication issue, and to propose alternatives. In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.

The guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (eRulemaking Initiative et al., 2017) which reflect the moderator actions mentioned in the examples. In the protocol the moderator roles were divided into two main classes. Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties. Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse. As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences).

RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants. However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling. Park et al. (2012) used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol. Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment. However this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".

Besides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy. 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.

Digital innovation supporting deliberation Argument maps (Walton, 2005) are widely employed to support online discussions, as an emerging optimization of the deliberation. Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post. Their contribution to the argument maps is often reviewed by a moderator. So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (the user) and above (the moderator).

Thanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (or many) of the available options, without having to read a large number of long posts. An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (Klein, 2011). Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (Yang et al., 2021;Ito, 2018). Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (2015) tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament. This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps. The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (fewer new ideas as compared to the traditional forum), it also reduced the rate of claims without further discussion.

Yet, the need for trained moderators tends to be a significant bottleneck (both in terms of time and of costs) in digital deliberation. Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation. For example, the experiment in Spada and Vreeland (2013) tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process.","(p5.0) Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies. We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Trénel, 2009). To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom. This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix). Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers). The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks? 5 The example involves two users who clearly differ in their argumentation style and position. User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text. User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair. This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones. A moderator then joins the discussion to (a) provide a clarification as to why the focus is on texting and a link to further information on the matter, and (b) ask User 2 to elaborate on the personal communication issue, and to propose alternatives. In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.

(p5.1) The guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (eRulemaking Initiative et al., 2017) which reflect the moderator actions mentioned in the examples. In the protocol the moderator roles were divided into two main classes. Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (policing), or helping with technical difficulties. Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse. As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (broadening the scope of the discussion), or helping users to improve the content of their posts (requests for clarification, focusing on one topic, substantive reasoning, sharing personal experiences).

(p5.2) RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants. However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling. Park et al. (2012) used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol. Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment. However this work is limited as it only focuses on two types of moderator interventions (broadening the scope of the discussion, improving argument quality) and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".

(p5.3) Besides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy. 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.

(p5.4) Digital innovation supporting deliberation Argument maps (Walton, 2005) are widely employed to support online discussions, as an emerging optimization of the deliberation. Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post. Their contribution to the argument maps is often reviewed by a moderator. So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (the user) and above (the moderator).

(p5.5) Thanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (or many) of the available options, without having to read a large number of long posts. An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (Klein, 2011). Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (Yang et al., 2021;Ito, 2018). Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (2015) tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament. This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps. The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (fewer new ideas as compared to the traditional forum), it also reduced the rate of claims without further discussion.

(p5.6) Yet, the need for trained moderators tends to be a significant bottleneck (both in terms of time and of costs) in digital deliberation. Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation. For example, the experiment in Spada and Vreeland (2013) tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process.","[['b12', 'b51'], [], ['b34'], [None], ['b57'], ['b8', 'b13', 'b41', 'b60'], ['b42']]","[['b12', 'b51'], [], ['b34'], [None], ['b57'], ['b8', 'b13', 'b41', 'b60'], ['b42']]",10,"1. Grounding AQ in a discourse perspective which quantifies ""team-playing"" and its impact on discourse dynamics is a clear challenge, both theoretically, in the Social Sciences and Argumentation Theory, and concretely, as the empirical quantification of discourse-grounded AQ will require large annotation efforts, real-time implementations, and thorough evaluation strategies.
2. We propose to make a first step in tackling this challenge by mapping it into a concrete application: (semi-)automatic moderation implemented as a form of discourse optimization, or, as it is commonly referred to in the Social Sciences, facilitation (Kaner et al., 2007;Trénel, 2009).
3. To illustrate the dynamics of moderation, let us start from concrete examples from a deliberation platform, RegulationRoom.
4. This discussion forum has been employed by public institutions to gather citizens contributions on discussions targeting very heterogeneous issues (more details can be found in Appendix).
5. Let us consider the following example from a discussion on the distracted driving by commercial vehicle operators (e.g., truckers and bus drivers).
6. The posts we selected (arrows indicate comment nesting) are from the discussion sub-thread: Texting -what are the risks?
7. 5 The example involves two users who clearly differ in their argumentation style and position.
8. User 1 has a clear position on the topic (claim in bold: not just texting, but all cellphone interactions should be banned), which she/he supports with personal reports (underlined text) an emotional tone, and a style which is typical of social media text.
9. User 2 replies, opening the post on a sarcastic note, which serves as the first premise to her/his (implicit) claim which is encoded in three rethorical questions (in bold): there should be no restrictions at all, because imposing them would be unfair.
10. This is the case because (premises underlined): any distraction can cause an accident, some people are capable of using their phone while driving, people who spend lot of time in the car for professional reasons still need to communicate with loved ones.
11. A moderator then joins the discussion to (Kaner et al., 2007;Trénel, 2009)0 provide a clarification as to why the focus is on texting and a link to further information on the matter, and (Kaner et al., 2007;Trénel, 2009)1 ask User 2 to elaborate on the personal communication issue, and to propose alternatives.
12. In the Appendix we report another example from the same topic and thread, where the user acts as a problematizer, challenging the scope and definition of the rule under discussion and the moderator acts as a ""discourse traffic director"", pointing out that the user should read and contribute to different threads in the discussion.
13. The guidelines for human moderators in Reg-ulationRoom have been defined in advance in a 'moderator protocol' (Kaner et al., 2007;Trénel, 2009)2 which reflect the moderator actions mentioned in the examples.
14. In the protocol the moderator roles were divided into two main classes.
15. Supervision functions include general moderator actions that do not necessarily target the specific content of the posts, e.g., greeting participants, monitoring compliance with netiquette (Kaner et al., 2007;Trénel, 2009)3, or helping with technical difficulties.
16. Substantive moderator functions aim to improve the quality of comments and promote fruitful discourse.
17. As the examples above clearly show, this can both mean that the moderator encourages exchanges between discourse participants and participation in other posts (Kaner et al., 2007;Trénel, 2009)4, or helping users to improve the content of their posts (Kaner et al., 2007;Trénel, 2009)5.RegulationRoom represents an excellent example of the beneficial role of the moderator in maintaining productive argumentation from participants.
18. However, to the best of our knowledge, there is little to no NLP work targeting moderation modeling.
19. Park et al. (Kaner et al., 2007;Trénel, 2009)6 used data from Regula-tionRoom and conducted an annotation study to empirically categorize the types of moderator interventions specified in the moderator protocol.
20. Classification experiments were conducted using SVM to predict the type of action a moderator would perform, given the previous comment.
21. However this work is limited as it only focuses on two types of moderator interventions (Kaner et al., 2007;Trénel, 2009)7 and as it does not predict whether the moderator should intervene, building on the assumption that a given comment has already been flagged as ""in need for moderation"".
22. Besides the concrete example of Regulation-Room, moderation and discourse facilitation have been, and still are, a crucial topic in digital democracy.
23. 6 The know-how of digital democracy experts is an invaluable starting point for the application of AM to moderation, as current research targets both the integration of digital solutions to facilitate online campaigns, and a critical reflection of the effects of such innovations on the deliberation outcomes.
24. Digital innovation supporting deliberation Argument maps (Kaner et al., 2007;Trénel, 2009)8 are widely employed to support online discussions, as an emerging optimization of the deliberation.
25. Given a specific topic, for example possible reactions to climate change, users who wish to contribute to the discussion are requested to structure their contribution by producing an item in a conceptual map and optionally writing an accompanying post.
26. Their contribution to the argument maps is often reviewed by a moderator.
27. So in a sense, the argument map for a given deliberation process is the outcome of a process that comes both from below (Kaner et al., 2007;Trénel, 2009)9 and above (more details can be found in Appendix)0.
28. Thanks to argument maps, the overall discourse picture can be overviewed and it is easier for the group of contributors to express support for one (more details can be found in Appendix)1 of the available options, without having to read a large number of long posts.
29. An example of this approach is represented in Deliberatorium 7 , an e-deliberation platform which has been extensively employed in many reference studies on the effect of digital innovation on deliberation (more details can be found in Appendix)2.
30. Another example of a digital deliberation platform which integrates argument maps and offers an option for moderation is COLAGREE (more details can be found in Appendix)3.
31. Among the studies testing the impact of such digital platforms on online deliberation, Spada et al. (more details can be found in Appendix)4 tests the effect of Deliberatorium's argument maps on an online discussion among the supporters of the Italian Democratic party concerning the desired features of electoral law to be proposed by the party to the Parliament.
32. This study compared the discussion of users employing Deliberatorium and a control group using a traditional forum format which was then encoded into argument maps.
33. The comparison showed that the argument map modality did not discourage participation, and while it appeared to make users less creative (more details can be found in Appendix)5, it also reduced the rate of claims without further discussion.
34. Yet, the need for trained moderators tends to be a significant bottleneck (more details can be found in Appendix)6 in digital deliberation.
35. Moreover, empirical research on the effect of moderation on deliberation has uncovered the risks of biased moderation.
36. For example, the experiment in Spada and Vreeland (more details can be found in Appendix)7 tests the extent to which moderators can influence participants' behavior by expressing their views during the moderation process."
236460206,Towards Argument Mining for Social Good: A Survey,"Political Science, Linguistics, Computer Science",https://www.semanticscholar.org/paper/dcb0b23685c9c116d8d53fe47e5157753659d3bd,s6,NLP-Supported Moderation: desiderata and challenges,"['p6.0', 'p6.1', 'p6.2', 'p6.3', 'p6.4', 'p6.5']","['NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is ""flaming"", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014). Here, moderators could benefit from hate-speech and trolling detection methods in NLP. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017). Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).', 'Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019). In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion. Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.', 'Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).', 'How to represent discourse? Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input. One question remains open: How to represent the argumentative discourse within a contribution (e.g. a forum post) and across contributions (e.g. an entire online deliberation campaign)? We can approach also this question from an interdisciplinary perspective. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017). A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim. Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013). Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Padó et al., 2019;Haunss et al., 2020). Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).', 'Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.', 'Who moderates the (NLP) moderators? The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Spliethöver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups. This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited.']","NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is ""flaming"", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014). Here, moderators could benefit from hate-speech and trolling detection methods in NLP. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017). Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019). In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion. Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.

Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).

How to represent discourse? Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input. One question remains open: How to represent the argumentative discourse within a contribution (e.g. a forum post) and across contributions (e.g. an entire online deliberation campaign)? We can approach also this question from an interdisciplinary perspective. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017). A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim. Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013). Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Padó et al., 2019;Haunss et al., 2020). Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).

Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.

Who moderates the (NLP) moderators? The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Spliethöver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups. This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited.","(p6.0) NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is ""flaming"", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014). Here, moderators could benefit from hate-speech and trolling detection methods in NLP. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017). Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).

(p6.1) Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019). In addition, detecting and highlighting parts of an argument that are a good target for attacks (Jo et al., 2020a) can help the moderator to motivate more participation and argumentation from opposing sides of a discussion. Another important source is the detection of implicitly asserted prepositions (Jo et al., 2020b) which has a counterpart in the framing detection task (Card et al., 2015;Akyürek et al., 2020), as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.

(p6.2) Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (Warner and Hirschberg, 2012;Waseem and Hovy, 2016;Schmidt and Wiegand, 2017), Fact Checking (Vlachos and Riedel, 2014;Kotonya and Toni, 2020), Facts recognition and source identification (Dusmanu et al., 2017).

(p6.3) How to represent discourse? Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input. One question remains open: How to represent the argumentative discourse within a contribution (e.g. a forum post) and across contributions (e.g. an entire online deliberation campaign)? We can approach also this question from an interdisciplinary perspective. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (2017). A discourse network is a bipartite graph, containing two classes of nodes: actors (e.g. Angela Merkel; the left-wing party; etc.) and claims (e.g. housing opportunities should be established for refugees); Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim. Discourse coalitions (Hajer, 1993) and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Leifeld and Haunss, 2012;Haunss et al., 2013). Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Padó et al., 2019;Haunss et al., 2020). Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Niculae et al., 2017) and theoretically (Afantenos and Asher, 2014).

(p6.4) Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (the grounded notion of AQ we have been advocating thus far) can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.

(p6.5) Who moderates the (NLP) moderators? The problem of biased moderation obviously relates to the issue of bias in NLP (Blodgett et al., 2020;Caliskan et al., 2017;Bolukbasi et al., 2016;Spliethöver and Wachsmuth, 2020) and it has a clear implication in the application of NLP methods to moderation. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups. This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (NLP or humans, online or in-person) have to ensure will not be left unexploited.","[[None, 'b24', 'b17', 'b19'], ['b37', None, 'b2', 'b11'], ['b39', 'b59', 'b58', None, 'b53', 'b15'], ['b32', 'b22', 'b21', None, 'b3', 'b4', 'b31', 'b1'], [], [None, 'b43']]","[[None, 'b24', 'b17', 'b19'], ['b37', None, 'b2', 'b11'], ['b39', 'b59', 'b58', None, 'b53', 'b15'], ['b32', 'b22', 'b21', None, 'b3', 'b4', 'b31', 'b1'], [], [None, 'b43']]",24,"1. NLP-supported moderation represents a clear solution to the bottleneck problem affecting facilitation in digital democracy.
2. Automatic tools can take over some of the tasks that human moderators typically perform when monitoring online discussions.
3. For example, in Social Sciences, one of the most discussed issues in crowd-scale deliberation is ""flaming"", i.e., aggressive and disrespectful communicative behavior (Lampe et al., 2014).
4. Here, moderators could benefit from hate-speech and trolling detection methods in NLP.
5. NLP methods to support deliberative decisionmaking have already been applied for the realtime visualisation of argument maps (El-Assady et al., 2017).
6. Deliberation in real-time applications has the clear potential of structured arguments extraction from the news media (Daxenberger and Gurevych, 2020), the identification of the argumentative structure in deliberative contexts (Liebeck et al., 2016), as well as automatic argument summarization (Lawrence et al., 2017).
7. Beyond the real-time support to users (and moderators) provided by the methods described above, further tasks specific to AM which are part of the role of a human or (semi-)automoated moderator include: detecting fallacies (Habernal et al., 2018b), reasoning and common-sense (Habernal et al., 2018a), relevance estimation (Potthast et al., 2019).
8. In addition, detecting and highlighting parts of an argument that are a good target for attacks (El-Assady et al., 2017)0 can help the moderator to motivate more participation and argumentation from opposing sides of a discussion.
9. Another important source is the detection of implicitly asserted prepositions (El-Assady et al., 2017)1 which has a counterpart in the framing detection task (El-Assady et al., 2017)2, as framing is a manipulation strategy which highlights specific aspects of an issue under discussion to promote certain interpretations.
10. Further NLP tasks which can play a crucial role in ensuring a healthy interaction are, for example, Hate Speech Detection (El-Assady et al., 2017)3, Fact Checking (El-Assady et al., 2017)4, Facts recognition and source identification (El-Assady et al., 2017)5.How to represent discourse?
11. Thus far, we have discussed the main ingredients of a rich NLP-informed approach to deliberative discourse.
12. These components, together with the deliberationaugmented definition of AQ sketched in section 3 are the features that the NLP moderator takes as an input.
13. One question remains open: How to represent the argumentative discourse within a contribution (El-Assady et al., 2017)6 and across contributions (El-Assady et al., 2017)7?
14. We can approach also this question from an interdisciplinary perspective.
15. Reference work in political science aims at modeling the mechanisms of political discourse in forms of discourse networks, as defined in Leifeld (El-Assady et al., 2017)8.
16. A discourse network is a bipartite graph, containing two classes of nodes: actors (El-Assady et al., 2017)9 and claims (Daxenberger and Gurevych, 2020)0; Edges between actors and claims indicate the support or opposition of a certain actor to a specific claim.
17. Discourse coalitions (Daxenberger and Gurevych, 2020)1 and argumentative clusters are the projection of the affiliation network on the actor and claim sides of the network (Daxenberger and Gurevych, 2020)2.
18. Recent NLP research has targeted integration machine learning in the discourse network analysis workflow (Daxenberger and Gurevych, 2020)3.
19. Crucially for AM, discourse networks can integrate claims and actors with a third class of nodes, the frame nodes, which encode the reason put forward by an actor to support or reject a claim.
20. This type of representation is perfectly compatible with a graph-based approach on argument representation which has already been established as to be preferred to a tree-structure representation both empirically (Daxenberger and Gurevych, 2020)4 and theoretically (Daxenberger and Gurevych, 2020)5.
21. Moderation can thus be modeled as optimization of specific quantitative properties of the discourse network: participant inclusion, can be enforced by ensuring that the contributions of peripheric actor nodes receive the deserved salience; argument mapping and summarization can be modeled by identifying ""hot"" sub-graphs in the network; the impact of a contribution (Daxenberger and Gurevych, 2020)6 can be quantified as the perturbation introduced in the network, with its long term effects on convergence or polarization.
22. Who moderates the (Daxenberger and Gurevych, 2020)7 moderators?
23. The problem of biased moderation obviously relates to the issue of bias in NLP (Daxenberger and Gurevych, 2020)8 and it has a clear implication in the application of NLP methods to moderation.
24. For example, we would not want our NLP models to infer a negative impact on AQ from cues which just reveal that the user belongs to certain groups.
25. This is a real risk when quality is equated to ""success"", in turn quantified in terms of likes, replies, retweets.
26. The public of a forum may be sensitive to such cues, but the moderator should be unbiased with respect to them.
27. Another source of bias is the degree of literacy of a contribution: while users who express themselves poorly are likely to be less popular with the forum public, their contributions may still be a very good move in the ""cooperation challenge"" -one that moderators (Daxenberger and Gurevych, 2020)9 have to ensure will not be left unexploited."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,s1,Competing models of C-S,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5', 'p1.6', 'p1.7']","['For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.', ""2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007)."", ""3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '"", 'Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).', 'For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.', '3 Why do speakers code-switch?', ""In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997)."", 'According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.']","For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.

2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).

3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).

For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.

3 Why do speakers code-switch?

In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).

According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.","(p1.0) For linguists, the specific ways in which languages are switched matters. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1. In fact, it may not signal multilingualism at all, simply borrowing. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.

(p1.1) 2. This is a good baile! 'This is a good dance party!' (Solorio and Liu, 2008) To produce example (2), the speaker needs to know only one Spanish word. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from. NLP scholars are not always concerned with the difference between examples (1) and (2) so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997) as the Matrix Language Frame (MLF) model. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause. Thus, it would not apply to the alternational English-Afrikaans C-S in example (3) as each clause is in a separate language (Dulm, 2007).

(p1.2) 3. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '

(p1.3) Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Meakins, 2012;Bhat et al., 2016;Adamou, 2016;MacSwan, 2000;Auer and Muhamedova, 2005). Bullock et al. (2018) computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (2).

(p1.4) For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (DiSciullo et al., 1986) and the Functional Head Constraint (Belazi et al., 1994). Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Li and Fung, 2014;Bhat et al., 2016). Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (lexical classes), in some NLP tag sets (Al-Ghamdi et al., 2016). This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018). In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.

(p1.5) 3 Why do speakers code-switch?

(p1.6) In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980). These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so). Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).

(p1.7) According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013). Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014). From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically.","[[], ['b71', None, 'b3', 'b43', 'b16'], [], ['b37', None, 'b32'], ['b32', 'b33', None, 'b31', 'b53', 'b57'], [], ['b9', 'b42', 'b44', 'b57', 'b82'], ['b79', 'b5', 'b7', None, 'b10', 'b52', 'b30', 'b65']]","[[], ['b71', None, 'b3', 'b43', 'b16'], [], ['b37', None, 'b32'], ['b32', 'b33', None, 'b31', 'b53', 'b57'], [], ['b9', 'b42', 'b44', 'b57', 'b82'], ['b79', 'b5', 'b7', None, 'b10', 'b52', 'b30', 'b65']]",27,"1. For linguists, the specific ways in which languages are switched matters.
2. The use of a single Spanish word in an English tweet (ex. 2) is not as syntactically complicated as the integration in ex. 1.
3. In fact, it may not signal multilingualism at all, simply borrowing.
4. Many words, particularly anglicisms, circulate globally: marketing, feedback, gay.2.
5. This is a good baile! 'This is a good dance party!'
6. (Solorio and Liu, 2008) To produce example (Solorio and Liu, 2008)4, the speaker needs to know only one Spanish word.
7. But, to produce example (1), the speaker has to know what word order and case marker to use, and which languages they should be drawn from.
8. NLP scholars are not always concerned with the difference between examples (1) and (Solorio and Liu, 2008)4 so that, with some exceptions (Bhat et al., 2016), grammatical work in NLP tends to rely heavily on the notion of a matrix language model advanced by Joshi (1982) and later adapted by Myers-Scotton (1997)
9. as the Matrix Language Frame (MLF) model.
10. The MLF holds that one language provides the grammatical frame into which words or phrases from another are embedded and its scope of application is a clause.
11. Thus, it would not apply to the alternational English-Afrikaans C-S in example (Solorio and Liu, 2008)0 as each clause is in a separate language (Solorio and Liu, 2008)1.3.
12. I love Horlicks maar hierś niks 'I love Horlicks but there's nothing there '
13. Although it dominates computational approaches to C-S, the MLF is contested on empirical and theoretical grounds.
14. The consistent identification of a matrix language is not always possible, the criteria for defining it are ambiguous, and its scope is limited (Solorio and Liu, 2008)2.
15. Bullock et al. (Solorio and Liu, 2008)3 computationally show that different ways of determining the matrix language only reliably converge over sentences with simple insertions as in example (Solorio and Liu, 2008)4.
16. For many linguists, the MLF is not the only way, or even an adequate way, to theorize C-S. The Equivalence Constraint (Poplack, 1980) captures the fact that C-S tends to occur at points where the linear structures of the contributing languages coincide, as when the languages involved share word order.
17. Other syntactic theories are built on the differences between lexical and functional elements, including the Government Constraint (Solorio and Liu, 2008)6 and the Functional Head Constraint (Solorio and Liu, 2008)7.
18. Incorporating the latter in NLP experiments has been shown to improve the accuracy of computational and speech models (Solorio and Liu, 2008)8.
19. Functional elements include negative particles and auxiliaries, which are respectively classified as Adverbs and Verbs (Solorio and Liu, 2008)9, in some NLP tag sets (Al-Ghamdi et al., 2016).
20. This means that NLP experiments often use annotations that are too coarse to be linguistically informative with regard to C-S. Constraint-free theories (Mahootian and Santorini, 1996;MacSwan, 2000) hold that nothing restricts switching apart from the grammatical requirements of the contributing languages.
21. Testing such theories in NLP experiments would require syntactically parsed corpora that are rare for mixed language data (Partanen et al., 2018).
22. In sum, working together, theoretical and computational linguists could create better tools for processing C-S than those currently available.
23. 3 Why do speakers code-switch? In addition to focusing on the linguistic aspects and constraints on C-S, linguists are also interested in the social and cognitive motivations for switching across languages.
24. What a (multilingual) speaker is trying to achieve by switching languages can affect its structural outcome.
25. Linguists recognize that pragmatic, interactional, and socio-indexical functions may condition C-S patterns.
26. For instance, Myslín and Levy (2015) demonstrate that Czech-English speakers switch to English for highinformation content words in prominent prosodic positions when speaking Czech.
27. Other uses of C-S with structural traces include signalling an in-group identity through backflagging (Muysken, 1995) or emblematic tag-switching (Poplack, 1980).
28. These are words or phrases that are used at the edge of clauses (e.g., Spanish ojalá or English so).
29. Other functions, among these, quoting a speaker, getting the attention of an interlocutor, or reiterating an utterance to soften or intensify a message will also be indicated via C-S in predictable linguistic constructions, such as with verbs of 'saying', vocative expressions, and sequential translation equivalents (Gumperz, 1982;Zentella, 1997).
30. According to Clyne (1991), there are eight factors (e.g. topic, type of interaction, interlocutors, role relationship, communication channel) that can influence C-S choices.
31. Lavric (2007) explains C-S choices in line with politeness theory, focusing on prestige and face-saving moves in multilingual conversations.
32. Heller (1992) takes a macro-social view, arguing that French-English C-S in Quebec may signal a political choice among both dominant and subordinate groups.
33. Gardner-Chloros and Edwards (2004) suggest that social factors influence language choice, with different generations of speakers from the same community exhibiting very different C-S patterns.
34. Similarly Sebba (1998) argues that as speakers cognitively construct equivalence between morphemes, words, and phrases across their languages, communities of the same languages may do this differently.
35. Evidence from computational studies suggests that C-S is speaker-dependent (Vu et al., 2013).
36. Gender and identity also play a role for C-S practices in English and Greek Cypriot community in London (Finnis, 2014).
37. From a computational perspective, Papalexakis et al. (2014) investigated the factors that influence C-S choices (Turkish-Dutch) in computer mediated interaction and how to predict them 1656 automatically."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,s2,"Code-switching, Borrowing, Transfer, Loan Translation","['p2.0', 'p2.1', 'p2.2']","[""While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009). 4.İlkokul-uİstanbul-da yap-tı-m."", ""primary.school-ACCİstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'"", ""In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher après 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.""]","While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009). 4.İlkokul-uİstanbul-da yap-tı-m.

primary.school-ACCİstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher après 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","(p2.0) While C-S implies active alternation between grammatical systems, borrowing does not. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988). When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990). Similarly, what looks like complex C-S may not be perceived as switching at all. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009). 4.İlkokul-uİstanbul-da yap-tı-m.

(p2.1) primary.school-ACCİstanbul-LOC do-past-1sg. 'I finished primary school in Istanbul.'

(p2.2) In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French. In Brussels French (Belgium), the construction chercher après 'look after' (for 'look for') is a translation of the Dutch equivalent and, in Ontario French (Canada), chercher pour is the translation equivalent of English 'look for'. In reference French (France), there is normally no particle following the verb. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown.","[['b37', 'b58', None, 'b64', 'b2'], [], []]","[['b37', 'b58', None, 'b64', 'b2'], [], []]",5,"1. While C-S implies active alternation between grammatical systems, borrowing does not.
2. It is difficult to know if a lone word insertion (e.g. example (2)) constitutes a borrowing or a C-S without considering how the items are integrated into the grammar of the receiving language (Poplack et al., 1988).
3. When such analyses are done, most lone-item insertions are analyzable as one-time borrowings, called nonce borrowings (Sankoff et al., 1990).
4. Similarly, what looks like complex C-S may not be perceived as switching at all.
5. Auer (1999) distinguishes a continuum of mixing types: prototypical C-S is pragmatic and intentional, Language Mixing serves no pragmatic purpose, and Mixed Languages are the single code of a community.
6. These can look structurally identical, but the latter can be modeled as a single language (e.g. languages like Michif Cree (Bakker, 1997) or Gurinji Kriol (Meakins, 2012)) rather than the intertwining of two.
7. Bilaniuk (2004) describes the Surzhyk spoken by urban Russian-Ukrainian bilinguals (in Ukraine) as 'between C-S and Mixed Language' since speakers are highly bilingual and the direction of switching is indeterminate.
8. Loan translation and transfer involve the words from only one language but the semantics and grammatical constructions from the other.
9. In example 4, the Turkish verb yapmak,' to do', takes on the Dutch meaning of doen in Turkish spoken in the Netherlands (Dogruöz and Backus, 2009).
10. 4.İlkokul-uİstanbul-da yap-tı-m.primary.school-ACCİstanbul-LOC do-past-1sg.
11. 'I finished primary school in Istanbul.'
12. In transfer, grammatical constructions can be borrowed from one language to another without the words being borrowed.
13. Treffers-Daller (2012) demonstrates the transfer of verb particles from Germanic languages into French.
14. In Brussels French (Poplack et al., 1988)0, the construction chercher après 'look after' (Poplack et al., 1988)1 is a translation of the Dutch equivalent and, in Ontario French (Poplack et al., 1988)2, chercher pour is the translation equivalent of English 'look for'.
15. In reference French (Poplack et al., 1988)3, there is normally no particle following the verb.
16. The degree to which linguistic features like loan translation and transfer can be found alongside C-S is unknown."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,s3,C-S across Languages: European Context,"['p3.0', 'p3.1', 'p3.2', 'p3.3', 'p3.4', 'p3.5', 'p3.6']","[""The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively)."", 'Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.', 'C-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.', 'C-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.', 'In addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.', 'Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.', 'Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.']","The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).

Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.

C-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.

C-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.

In addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.

Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.

Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.","(p3.0) The contexts in which people acquire and use multiple languages in Europe are diverse. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (1998) investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).

(p3.1) Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway. Similarly, Jørgensen (1998) and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.

(p3.2) C-S does not only take place between standard languages but between minority languages and dialects as well. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (2006) focused on the C-S between Welsh and English in the UK. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy. In the Balkans, Kyuchukov (2006) analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (2006) among Swiss-German dialects and by Robert-Tissot and Morel (2017) through SMS corpora collected across Germanic (i.e. English and German) and Romance languages (French, Spanish, Italian) in Switzerland.

(p3.3) C-S is commonly observable across immigrant contexts in Europe. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (2006) focused on the C-S between English and Cypriot Turkish communities in London. Wei and Milroy (1995) analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (Chinese-English) families in Northeastern England. In addition, Ożańska-Ponikwia (2016) investigated the Polish-English C-S in the UK as well. C-S among immigrant community members have also been widely studied in Germany (e.g. Turkish-German C-S by Keim (2008) and Ç etinoglu (2017), Russian-German C-S by Khakimov (2016)). In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990). In Belgium, Meeuws and Blommaert (1998) studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.

(p3.4) In addition to daily communication, some linguists are also interested in the C-S observed in historical documents. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.

(p3.5) Within the media context, Martin (1998) investigated English C-S in written French advertising, and Onysko (2007) investigated the English C-S in German written media through corpus analyses. Zhiganova (2016) indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.

(p3.6) Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions.","[['b11', 'b39', 'b56', 'b40', 'b29', None, 'b54', 'b17'], [], ['b75', 'b69', 'b28'], ['b80', 'b23', 'b38', 'b24', 'b46', 'b12', None, 'b52', 'b1', 'b51'], [], ['b35', 'b83'], ['b81']]","[['b11', 'b39', 'b56', 'b40', 'b29', None, 'b54', 'b17'], [], ['b75', 'b69', 'b28'], ['b80', 'b23', 'b38', 'b24', 'b46', 'b12', None, 'b52', 'b1', 'b51'], [], ['b35', 'b83'], ['b81']]",24,"1. The contexts in which people acquire and use multiple languages in Europe are diverse.
2. Some acquire their languages simultaneously from birth, while others acquire them sequentially, either naturally or via explicit instruction.
3. Multilingualism is the norm in many zones where local residents may speak different languages to accommodate their interlocutors.
4. Speakers who use local dialects or minoritized varieties may also be engaged in C-S when switching between their variety and a dominant one (Mills and Washington, 2015;Blom and Gumperz, 1972). C-S in bilingual language acquisition of children has been studied across language contact contexts in Europe.
5. In Germany, Herkenrath (2012) and Pfaff (1999) focused on Turkish-German C-S and Meisel (1994) on German-French C-S of bilingual children.
6. From a comparative perspective, Poeste et al. (2019) analyzed C-S among bilingual, trilingual, and multilingual children growing up in Spain and Germany.
7. In the Netherlands, Bosma and Blom (2019) focused on C-S among bilingual Frisian-Dutch children.
8. In addition to analyzing C-S in children's speech, Juan-Garau and Perez-Vidal (2001) and Lanza (2019)0 investigated C-S in the interaction patterns between bilingual children and their parents (i.e. Spanish-Catalan and English-Norwegian respectively).
9. Within an educational setting, Kleeman (2012) observed C-S among bilingual (North Sami-Norwegian) kindergarten children in the North of Norway.
10. Similarly, Jørgensen (2019)0 and Cromdal (2004) report the use of C-S for resolving disputes among bilingual (Turkish-Danish) children in Denmark and multilingual (Swedish-English and/or a Non-Scandinavian Language) children in Sweden respectively.
11. C-S does not only take place between standard languages but between minority languages and dialects as well.
12. For example, Themistocleous (2013) studied C-S between Greek and Cypriot Greek and Deuchar (1999)4 focused on the C-S between Welsh and English in the UK.
13. Berruto (2005) reports cases of language mixing between standard Italian and Italoromance dialects in Italy.
14. In the Balkans, Kyuchukov (1999)4 analyzed C-S between Turkish-Bulgarian and Romani in Bulgaria.
15. C-S between dialects and/or standard vs. minority languages in computer mediated interaction was analyzed by Siebenhaar (1999)4 among Swiss-German dialects and by Robert-Tissot and Morel (1999)9 through SMS corpora collected across Germanic (1999)1 and Romance languages (1999)2 in Switzerland.C-S is commonly observable across immigrant contexts in Europe.
16. In the UK, Georgakopoulou and Finnis (2009) described the C-S patterns between English and Cypriot Greek while Issa (1999)4 focused on the C-S between English and Cypriot Turkish communities in London.
17. Wei and Milroy (1999)5 analyzed the C-S between English and Chinese from a conversational analysis point of view based on the interactions of bilingual (1999)6 families in Northeastern England.
18. In addition, Ożańska-Ponikwia (2019)2 investigated the Polish-English C-S in the UK as well.
19. C-S among immigrant community members have also been widely studied in Germany (1999)8 and Ç etinoglu (1999)9, Russian-German C-S by Khakimov (2019)2).
20. In the Netherlands, C-S studies include Turkish-Dutch C-S by Backus (2010) and Dutch-Morroccan C-S by Nortier (1990).
21. In Belgium, Meeuws and Blommaert (2019)0 studied the French-Lingala-Swahili C-S among immigrants of Zaire and Treffers-Daller (1994) studied French-Dutch C-S in Brussels.
22. In Spain, Jieanu (2013) describes the Romanian-Spanish C-S among the Romanian immigrants.
23. In addition to the C-S analyses within spoken interactions of immigrant communities across Europe, there are also studies about C-S within computer mediated communication as well.
24. These studies include Greek-German C-S by Androutsopoulos (2015) in Germany, Turkish-Dutch C-S by Papalexakis et al. (2014), Papalexakis and Dogruöz (2015) and a comparison of Turkish-Dutch and Moroccan-Dutch C-S by Dorleijn and Nortier (2009) in the Netherlands.
25. Similarly, Marley (2011) compared French-Arabic C-S within computer mediated interaction across Moroccan communities in France and the UK.
26. In addition to daily communication, some linguists are also interested in the C-S observed in historical documents.
27. While Swain (2002) explored Latin-Greek C-S by Cicero (Roman Statesman), Dunkel (2000) analyzed C-S in his communication with Atticus (Roman philosopher who studied in Athens) in the Roman Empire.
28. Argenter (2001) reports cases of language mixing within the Catalan Jewish community (in Spain) in the 14th and 15th centuries and Rothman (2011) highlights the C-S between Italian, Slavic and Turkish in the historical documents about Ottoman-Venetian relations.
29. In Switzerland, Volk and Clematide (2014) worked on detecting and annotating C-S patterns in diachronic and multilingual (English, French, German, Italian, Romansh and Swiss German) Alpine Heritage corpus.
30. Within the media context, Martin (2019)0 investigated English C-S in written French advertising, and Onysko (2019)1 investigated the English C-S in German written media through corpus analyses.
31. Zhiganova (2019)2 indicates that German speakers perceive C-S into English for advertising purposes with both positive and negative consequences.
32. Similar to humans, institutions and/or organizations could also have multilingual communication with their members and/or audience.
33. For example, Wodak et al. (2012) analyzed the C-S and language choice at the institutional level for European Union institutions."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,s4,C-S across Languages: Indian Context,"['p4.0', 'p4.1', 'p4.2', 'p4.3', 'p4.4', 'p4.5']","['According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.', 'In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.', 'Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).', 'C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.', 'From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.', 'In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.']","According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.

In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.

Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).

C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.

From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.

In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.","(p4.0) According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages). Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families. The census excludes languages with a population lower than 10,000 speakers. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.

(p4.1) In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (i.e. social and stylistic) aspects of Hindi-English C-S from a theoretical point of view. More recently, Doley (2013) explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele. Another observation about C-S in daily life comes from Boro (2020) who provides examples of English, Assamese and Bodo (another language spoken in the Assam region) C-S and borrowings. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India. For example, Kapp (1997) explains the Portuguese influence through borrowings in Dravidian languages (i.e. Kannada and Telugu) spoken in India.

(p4.2) Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (probably) encountered and collected by the authors themselves in daily life interactions over a period of time with limited means. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost. For example, Nadkarni (1975) proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Indo-Aryan language) and Kannada (Dravidian language) in the South of India. Both languages have been in contact with each other for over four hundred years. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada. Nadkarni (1975) attributes this preference to the high prestige associated with Konkani within the given social context. Since Kannada (perceived as less prestigious) is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S. However, it is not common for Kannada speakers to learn and speak Konkani (Nadkarni, 1975).

(p4.3) C-S in India has been investigated through written media, advertising and film industry as well. Si (2011) analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004. Her results indicate a change of direction C-S over the years. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions. A similar trend has been observed for Bengali movie scripts as well. Through analyzing movie scripts (between 1970s and 2010s), Chatterjee (2016) finds a drastic increase in the use of bilingual verbs (e.g. renovate koreche ""renovation do"") over time and attributes this rise to the increasing popularity of English in Indian society. Within the immigrant context, Gardner-Chloros and Charles (2007) focused on the types and functions of C-S between Hindi and English across the TV programs (e.g. highly scripted vs. loosely scripted programs) of a British/Asian cable channel in the UK. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (i.e. highly scripted). In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use (instead of C-S). Similarly, Zipp (2017) analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters (group identity). Pratapa and Choudhury (2017) perform a quantitative study of 18 recent Bollywood (Hindi) movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.

(p4.4) From an advertising point of view, Kathpalia and Wee Ong (2015) analyzed C-S in Hinglish (i.e. Hindi, English, Urdu, Sanskrit according to their definition) billboards about the Amul brand in India. After compiling the advertisements on billboards (1191), they classified the structures and functions of C-S. Their results indicate more intrasentential C-S than intersentential ones on the billboards. In terms of function, the advertisers used C-S to indicate figures of speech (e.g. puns, associations, contradictory associations, word-creation and repetitions) to attract the attention of the target group. Mohanty (2006) provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum. In general, high-cost English Medium (EM) education is valued by upper-class and affluent families. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages. Sridhar (2002) explains that C-S is commonly practiced among students in schools across India. In addition, she finds it unrealistic to ask the students to separate the two languages harshly. In immigrant contexts, Martin et al. (2006) investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Al-Azami, 2006). In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali. There are also studies about Bengali-English C-S in the UK school settings (Pagett, 2006) and Bangladesh (Obaidullah, 2016) as well. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.

(p4.5) In their study about aphasic patients, Shyamala Chengappa and Bhat (2004) report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (2019) find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well.","[[None, 'b19'], ['b27', 'b73'], [None], ['b84', 'b67'], ['b36', 'b41', None, 'b72', 'b50'], []]","[[None, 'b19'], ['b27', 'b73'], [None], ['b84', 'b67'], ['b36', 'b41', None, 'b72', 'b50'], []]",12,"1. According to the 2011 Census (Chandramouli, 2011), 26% of the population of India is bilingual, while 7% is trilingual.
2. There are 121 major languages and 1599 other languages in India, out of which 22 (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu) are scheduled languages with an official recognition (almost 97% of the population speaks one of the scheduled languages).
3. Most of the population ( 93%) speak languages from the Indo-Aryan (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia) and Dravidian (Kannada, Malayalam, Telugu, Tamil) language families.
4. The census excludes languages with a population lower than 10,000 speakers.
5. Given this, it is probably difficult to find monolingual speakers in India considering the linguistic diversity and wide-spread multilingualism.
6. Kachru (1978) provides one of the early studies on the types and functions of C-S in India with a historical understanding of the multilingual context.
7. In addition to the mutual influences and convergence of Indo-Aryan and Dravidian languages internally, he mentions Persian and English as outside influences on Indian languages.
8. Similarly, Sridhar (1978) provides an excellent comparative overview about the functions of C-S in Kannada in relation to the Perso-Arabic vs. English influences.
9. Kumar (1986) gives examples about the formal (e.g. within NPs, PPs, VPs) and functional (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu)0 aspects of Hindi-English C-S from a theoretical point of view.
10. More recently, Doley (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu)1 explains how fish mongers in a local fish market in Assam adjust and switch between Assamese, English and local languages strategically to sell their products to multilingual clientele.
11. Another observation about C-S in daily life comes from Boro (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu)2 who provides examples of English, Assamese and Bodo (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu)3 C-S and borrowings.
12. In addition to English, Portuguese was also in contact with the local languages as a result colonization in South India.
13. For example, Kapp (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu)4 explains the Portuguese influence through borrowings in Dravidian languages (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu)5 spoken in India.
14. Instead of automatic data collection and methods of analyses, the C-S examples for the abovementioned studies are (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu)6 encountered and collected by the authors themselves in daily life interactions over a period of time with limited means.
15. Nowadays, these small sets of data would be regarded as insignificant in computational areas of research.
16. However, ignoring these studies and data could have serious consequences since crucial information about the social and cultural dynamics in a multilingual setting would also be lost.
17. For example, Nadkarni (almost 97% of the population speaks one of the scheduled languages)0 proves this point by explaining how social factors influence the C-S between Saraswat Brahmin dialect of Konkani (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu)8 and Kannada (Assamese, Bangla, Bodo, Dogri, Gujarati, Hindi, Kashmiri, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Tamil, Telugu, Sanskrit, Santali, Sindhi, Urdu)9 in the South of India.
18. Both languages have been in contact with each other for over four hundred years.
19. Saraswat Brahmins are fluent in both Konkani and Kannada but they do not speak Konkani with Kannada speakers and they also do not C-S between Konkani and Kannada.
20. Nadkarni (almost 97% of the population speaks one of the scheduled languages)0 attributes this preference to the high prestige associated with Konkani within the given social context.
21. Since Kannada (almost 97% of the population speaks one of the scheduled languages)1 is widely spoken in that region, Konkani speakers learn and speak Kannada for functional purposes in daily life which does not involve C-S.
22. However, it is not common for Kannada speakers to learn and speak Konkani (almost 97% of the population speaks one of the scheduled languages)2.C-S in India has been investigated through written media, advertising and film industry as well.
23. Si (almost 97% of the population speaks one of the scheduled languages)3 analyzed Hindi-English C-S in the scripts of seven Bollywood movies which were filmed between 1982 and 2004.
24. Her results indicate a change of direction C-S over the years.
25. More specifically, Hindi was the dominant language with occasional switches to English for the early productions but English became the dominant language especially for younger generations in the later productions.
26. A similar trend has been observed for Bengali movie scripts as well.
27. Through analyzing movie scripts (almost 97% of the population speaks one of the scheduled languages)4, Chatterjee (almost 97% of the population speaks one of the scheduled languages)5 finds a drastic increase in the use of bilingual verbs (almost 97% of the population speaks one of the scheduled languages)6 over time and attributes this rise to the increasing popularity of English in Indian society.
28. Within the immigrant context, Gardner-Chloros and Charles (almost 97% of the population speaks one of the scheduled languages)7 focused on the types and functions of C-S between Hindi and English across the TV programs (almost 97% of the population speaks one of the scheduled languages)8 of a British/Asian cable channel in the UK.
29. Although they have come across C-S in a variety of TV shows, the least amount of C-S was encountered in the news broadcasts (almost 97% of the population speaks one of the scheduled languages)9.
30. In general, they have encountered less C-S on TV broadcasts in comparison to the natural speech and attribute this factor to the consciousness of TV personalities about pure language use ( 93%)0.
31. Similarly, Zipp ( 93%)3 analyzed Gujarati-English C-S within a radio show targeting British South Asians living in the US and concluded that C-S was part of identity construction among youngsters ( 93%)2.
32. Pratapa and Choudhury ( 93%)3 perform a quantitative study of 18 recent Bollywood ( 93%)4 movies and find that C-S is used for establishing identity, social dynamics between characters and the socio-cultural context of the movie.
33. From an advertising point of view, Kathpalia and Wee Ong ( 93%)5 analyzed C-S in Hinglish ( 93%)6 billboards about the Amul brand in India.
34. After compiling the advertisements on billboards ( 93%)7, they classified the structures and functions of C-S.
35. Their results indicate more intrasentential C-S than intersentential ones on the billboards.
36. In terms of function, the advertisers used C-S to indicate figures of speech ( 93%)8 to attract the attention of the target group.
37. Mohanty (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia)2 provides an extended overview of the multilingual education system in India exploring the types and quality of schools across a wide spectrum.
38. In general, high-cost English Medium (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia)0 education is valued by upper-class and affluent families.
39. Although low-cost EM education is also available for lower income families, he questions its impact in comparison to education in the local languages.
40. Sridhar (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia)1 explains that C-S is commonly practiced among students in schools across India.
41. In addition, she finds it unrealistic to ask the students to separate the two languages harshly.
42. In immigrant contexts, Martin et al. (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia)2 investigates how Gujarati-English C-S is used among the South Asian students in educational settings in the UK.
43. Another analysis reveals a shift from Bengali toward English among the younger generations of the immigrant Bengali community in the UK (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia)3.
44. In terms of the C-S patterns, first generation immigrants integrate English words while speaking Bengali whereas English dominates the conversations of younger generations with occasional switches to Bengali.
45. There are also studies about Bengali-English C-S in the UK school settings (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia)4 and Bangladesh (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia)5 as well.
46. However, a systematic comparison between Bengali-English C-S in India, Bangladesh and immigrant settings are lacking.
47. In their study about aphasic patients, Shyamala Chengappa and Bhat (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia)6 report increased frequency of C-S between Malayalam and English for aphasic patients in comparison to the control group.
48. However, there were less differences between the groups in terms of functions of C-S. Deepa and Shyamala (Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese, Maithili, Odia)7 find that amount and types of C-S could be used to differentiate between healthy and mild dementia patients who are bilingual in Kannada and English.
49. Although both studies are carried out with limited subjects, they offer insights about the use of C-S in health settings as well."
236460241,A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/ee6d66efc86746d42ace14db30fcbaf9d3380e25,s8,User-facing applications,"['p8.0', 'p8.1', 'p8.2']","['Although speech and NLP models for C-S have been built for various applications, a major limitation of the work done so far in computational processing of C-S is the lack of end-to-end userfacing applications that interact directly with users in multilingual communities. For example, there is no widely-used spoken dialogue system that can understand as well as produce code-switched speech, although some voice assistants may recognize and produce C-S in limited scenarios in some locales. Although computational implementations of grammatical models of C-S exist (Bhat et al., 2016), they do not necessarily generate natural C-S utterances that a bilingual speaker would produce (Pratapa et al., 2018). Most crucially, current computational approaches to C-S language technologies do not usually take into account the linguistic and social factors that influence why and when speakers/users choose to code-switch.', ""Bawa et al. (2020) conducted a Wizard-of-Oz study using a Hindi-English chatbot and found that not only did bilingual users prefer chatbots that could code-switch, they also showed a preference towards bots that mimicked their own C-S patterns. Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment. In a follow-up study, Agarwal et al. (2017) find that Hindi is the preferred language for swearing in Hindi-English C-S tweets, and swearing may be a motivating factor for users to switch to Hindi. The study also finds a gender difference, with women preferring to swear in English more often than Hindi. Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch."", 'Due to the paucity of user-facing systems and standard benchmarks covering only a handful of simpler NLP tasks, it is likely that we overestimate how well computational models are able to handle C-S. In sum, language technologies for C-S seem to be constrained by the lack of availability of diverse C-S training data, evaluation benchmarks and the absence of user-facing applications. They need to go beyond pattern recognition and grammatical constraints of C-S in order to process and produce C-S the way humans do. Hence, it is important for the CL community to be aware of the vast litera-ture around C-S in linguistics, particularly as we proceed to solving more challenging tasks.']","Although speech and NLP models for C-S have been built for various applications, a major limitation of the work done so far in computational processing of C-S is the lack of end-to-end userfacing applications that interact directly with users in multilingual communities. For example, there is no widely-used spoken dialogue system that can understand as well as produce code-switched speech, although some voice assistants may recognize and produce C-S in limited scenarios in some locales. Although computational implementations of grammatical models of C-S exist (Bhat et al., 2016), they do not necessarily generate natural C-S utterances that a bilingual speaker would produce (Pratapa et al., 2018). Most crucially, current computational approaches to C-S language technologies do not usually take into account the linguistic and social factors that influence why and when speakers/users choose to code-switch.

Bawa et al. (2020) conducted a Wizard-of-Oz study using a Hindi-English chatbot and found that not only did bilingual users prefer chatbots that could code-switch, they also showed a preference towards bots that mimicked their own C-S patterns. Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment. In a follow-up study, Agarwal et al. (2017) find that Hindi is the preferred language for swearing in Hindi-English C-S tweets, and swearing may be a motivating factor for users to switch to Hindi. The study also finds a gender difference, with women preferring to swear in English more often than Hindi. Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch.

Due to the paucity of user-facing systems and standard benchmarks covering only a handful of simpler NLP tasks, it is likely that we overestimate how well computational models are able to handle C-S. In sum, language technologies for C-S seem to be constrained by the lack of availability of diverse C-S training data, evaluation benchmarks and the absence of user-facing applications. They need to go beyond pattern recognition and grammatical constraints of C-S in order to process and produce C-S the way humans do. Hence, it is important for the CL community to be aware of the vast litera-ture around C-S in linguistics, particularly as we proceed to solving more challenging tasks.","(p8.0) Although speech and NLP models for C-S have been built for various applications, a major limitation of the work done so far in computational processing of C-S is the lack of end-to-end userfacing applications that interact directly with users in multilingual communities. For example, there is no widely-used spoken dialogue system that can understand as well as produce code-switched speech, although some voice assistants may recognize and produce C-S in limited scenarios in some locales. Although computational implementations of grammatical models of C-S exist (Bhat et al., 2016), they do not necessarily generate natural C-S utterances that a bilingual speaker would produce (Pratapa et al., 2018). Most crucially, current computational approaches to C-S language technologies do not usually take into account the linguistic and social factors that influence why and when speakers/users choose to code-switch.

(p8.1) Bawa et al. (2020) conducted a Wizard-of-Oz study using a Hindi-English chatbot and found that not only did bilingual users prefer chatbots that could code-switch, they also showed a preference towards bots that mimicked their own C-S patterns. Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment. In a follow-up study, Agarwal et al. (2017) find that Hindi is the preferred language for swearing in Hindi-English C-S tweets, and swearing may be a motivating factor for users to switch to Hindi. The study also finds a gender difference, with women preferring to swear in English more often than Hindi. Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch.

(p8.2) Due to the paucity of user-facing systems and standard benchmarks covering only a handful of simpler NLP tasks, it is likely that we overestimate how well computational models are able to handle C-S. In sum, language technologies for C-S seem to be constrained by the lack of availability of diverse C-S training data, evaluation benchmarks and the absence of user-facing applications. They need to go beyond pattern recognition and grammatical constraints of C-S in order to process and produce C-S the way humans do. Hence, it is important for the CL community to be aware of the vast litera-ture around C-S in linguistics, particularly as we proceed to solving more challenging tasks.","[[None, 'b59'], ['b63', None], []]","[[None, 'b59'], ['b63', None], []]",4,"1. Although speech and NLP models for C-S have been built for various applications, a major limitation of the work done so far in computational processing of C-S is the lack of end-to-end userfacing applications that interact directly with users in multilingual communities.
2. For example, there is no widely-used spoken dialogue system that can understand as well as produce code-switched speech, although some voice assistants may recognize and produce C-S in limited scenarios in some locales.
3. Although computational implementations of grammatical models of C-S exist (Bhat et al., 2016), they do not necessarily generate natural C-S utterances that a bilingual speaker would produce (Pratapa et al., 2018).
4. Most crucially, current computational approaches to C-S language technologies do not usually take into account the linguistic and social factors that influence why and when speakers/users choose to code-switch.
5. Bawa et al. (2020) conducted a Wizard-of-Oz study using a Hindi-English chatbot and found that not only did bilingual users prefer chatbots that could code-switch, they also showed a preference towards bots that mimicked their own C-S patterns.
6. Rudra et al. (2016) report a study on 430k tweets from Hindi-English bilingual users and find that Hindi is preferred for the expression of negative sentiment.
7. In a follow-up study, Agarwal et al. (2017) find that Hindi is the preferred language for swearing in Hindi-English C-S tweets, and swearing may be a motivating factor for users to switch to Hindi.
8. The study also finds a gender difference, with women preferring to swear in English more often than Hindi.
9. Such studies indicate that multilingual chatbots and intelligent agents need to be able to adapt to users' linguistic styles, while also being capable of determining when and how to code-switch.
10. Due to the paucity of user-facing systems and standard benchmarks covering only a handful of simpler NLP tasks, it is likely that we overestimate how well computational models are able to handle C-S. In sum, language technologies for C-S seem to be constrained by the lack of availability of diverse C-S training data, evaluation benchmarks and the absence of user-facing applications.
11. They need to go beyond pattern recognition and grammatical constraints of C-S in order to process and produce C-S the way humans do.
12. Hence, it is important for the CL community to be aware of the vast litera-ture around C-S in linguistics, particularly as we proceed to solving more challenging tasks."
234093015,A Survey of Data Augmentation Approaches for NLP,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/63d8426ba1f51a8525dd19fd8ec92934ec71aea5,s1,Background,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5']","['What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.', 'What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.', 'Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.', 'Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.', 'Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.', 'Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.']","What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","(p1.0) What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020). DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.

(p1.1) What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two.

(p1.2) Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.

(p1.3) Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.

(p1.4) Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.

(p1.5) Interpretation of DA Dao et al. (2019) note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient. Overall, there indeed appears to be a lack of research on why exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and highlight some of the existing work below. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. (2019) show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods. Dao et al. (2019) think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.","[[None], ['b23', None], [None, 'b1'], [], [], [None]]","[[None], ['b23', None], [None, 'b1'], [], [], [None]]",6,"1. What is data augmentation? Data augmentation (DA) encompasses methods of increasing training data diversity without directly collecting more data.
2. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020).
3. DA has been commonly used in CV, where techniques like cropping, flipping, and color jittering are a standard component of model training.
4. In NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.
5. What are the goals and trade-offs?
6. Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b).
7. As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance.
8. Most offer trade-offs between these two.Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017;Wei and Zou, 2019;Wei et al., 2021b).
9. Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts.
10. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize.
11. Further, the distribution of augmented data should neither be too similar nor too different from the original.
12. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively.
13. Effective DA approaches should aim for a balance.
14. Kashefi and Hwa (2020) devise a KL-Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than a typical ""run-all-heuristics"" comparison, which can be very time and cost intensive.
15. Interpretation of DA Dao et al. (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020)0 note that ""data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles"", and claim the typical explanation of DA as regularization to be insufficient.
16. Overall, there indeed appears to be a lack of research on why exactly DA works.
17. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles.
18. We discuss this challenge more in §6, and highlight some of the existing work below.
19. Bishop (1995) show training with noised examples is reducible to Tikhonov regularization (subsumes L2).
20. Rajput et al. (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020)0 show that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.
21. Dao et al. (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020)0 think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.
22. Chen et al. (Shorten and Khoshgoftaar, 2019;Hernández-García and König, 2020)1 show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant."
234093015,A Survey of Data Augmentation Approaches for NLP,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/63d8426ba1f51a8525dd19fd8ec92934ec71aea5,s3,Rule-Based Techniques,"['p3.0', 'p3.1']","['Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model\'s feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap. They show improved performance on many text classification tasks. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.', 'For paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges. They use balance theory and transitivity to infer augmented sentence pairs from this graph. Motivated by image cropping and rotation, Şahin and Steedman (2018) propose dependency tree morphing. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (à la cropping), as seen in Figure 2. This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic).']","Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model's feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap. They show improved performance on many text classification tasks. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.

For paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges. They use balance theory and transitivity to infer augmented sentence pairs from this graph. Motivated by image cropping and rotation, Şahin and Steedman (2018) propose dependency tree morphing. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (à la cropping), as seen in Figure 2. This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic).","(p3.0) Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model's feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap. They show improved performance on many text classification tasks. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.

(p3.1) For paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges. They use balance theory and transitivity to infer augmented sentence pairs from this graph. Motivated by image cropping and rotation, Şahin and Steedman (2018) propose dependency tree morphing. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (à la cropping), as seen in Figure 2. This is most beneficial for language families with rich case marking systems (e.g. Baltic and Slavic).","[['b7', None, 'b1'], [None]]","[['b7', None, 'b1'], [None]]",4,"1. Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components.
2. Feature space DA approaches generate augmented examples in the model's feature space rather than input data.
3. Many few-shot learning approaches (Hariharan and Girshick, 2017;Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4).
4. Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold.
5. Wei and Zou (2019) propose EASY DATA AUG-MENTATION (EDA), a set of token-level random perturbation operations including random insertion, deletion, and swap.
6. They show improved performance on many text classification tasks.
7. UDA (Xie et al., 2020) show how supervised DA methods can be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.
8. For paraphrase identification, Chen et al. (2020b) construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges.
9. They use balance theory and transitivity to infer augmented sentence pairs from this graph.
10. Motivated by image cropping and rotation, Şahin and
11. Steedman (2018) propose dependency tree morphing.
12. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (see §4.4)0, as seen in Figure 2.
13. This is most beneficial for language families with rich case marking systems (see §4.4)1."
234093015,A Survey of Data Augmentation Approaches for NLP,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/63d8426ba1f51a8525dd19fd8ec92934ec71aea5,s4,Example Interpolation Techniques,"['p4.0', 'p4.1', 'p4.2']","['Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).', 'Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).', 'A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).']","Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).

Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).

A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).","(p4.0) Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).

(p4.1) Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020). For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).

(p4.2) A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs. This has been overcome by mixing embeddings or higher hidden layers (Chen et al., 2020c). Later variants propose speech-tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. SEQ2MIXUP (Guo et al., 2020) generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (Wang et al., 2018a).","[['b20', None], [None, 'b19'], [None]]","[['b20', None], [None, 'b19'], [None]]",5,"1. Another class of DA techniques, pioneered by MIXUP (Zhang et al., 2017), interpolates the inputs and labels of two or more real examples.
2. This class of techniques is also sometimes referred to as Mixed Sample Data Augmentation (MSDA).
3. Ensuing work has explored interpolating inner components (Verma et al., 2019;Faramarzi et al., 2020), more general mixing schemes (Guo, 2020), and adding adversaries (Beckham et al., 2019).
4. Another class of extensions of MIXUP which has been growing in the vision community attempts to fuse raw input image pairs together into a single 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements.
5. Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Şahin and Steedman (2018) input image, rather than improve the continuous interpolation mechanism.
6. Examples of this paradigm include CUTMIX (Yun et al., 2019), CUTOUT (De-Vries and Taylor, 2017) and COPY-PASTE (Ghiasi et al., 2020).
7. For instance, CUTMIX replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes.
8. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see ""Multimodal challenges"" in §6).
9. A bottleneck to using MIXUP for NLP tasks was the requirement of continuous inputs.
10. This has been overcome by mixing embeddings or higher hidden layers (MSDA)0.
11. Later variants propose speech-tailored mixing schemes (MSDA)1 and interpolation with adversarial examples (MSDA)2, among others.
12. SEQ2MIXUP (MSDA)3 generalizes MIXUP for sequence transduction tasks in two ways -the ""hard"" version samples a binary mask (MSDA)4 prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(MSDA)5.
13. The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like SWITCHOUT (MSDA)6."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Linguistics, Computer Science",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s2,Initial Instruction,"['p2.0', 'p2.1']","['In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.', 'Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.']","In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","(p2.0) In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room. Turn left and go through the door in the middle."" Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019). Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019), StreetNav (Hermann et al., 2020), Talk2Nav (Vasudevan et al., 2021, LANI (Misra et al., 2018) RoomNav (Wu et al., 2018), EmbodiedQA (Das et al., 2018), REVERIE (Qi et al., 2020b), SOON (Zhu et al., 2021a) IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN (Chen et al., 2019), an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chen et al., 2019), StreetLearn (Mirowski et al., 2019;Mehta et al., 2020), StreetNav(Hermann et al., 2020), and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.

(p2.1) Some work uses natural language to guide drones. LANI (Misra et al., 2018) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018.","[['b25', None, 'b16', 'b19'], [None]]","[['b25', None, 'b16', 'b19'], [None]]",5,"1. In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as ""Go upstairs and pass the table in the living room.
2. Turn left and go through the door in the middle.""
3. Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal.
4. Anderson et al. (2018b) create the R2R dataset based on the Matterport3D simulator (Chang et al., 2017).
5. An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views.
6. R2R is extended to create other VLN benchmarks.
7. Roomfor-Room joins paths in R2R to longer trajectories (Jain et al., 2019).
8. Yan et al. (2020) (Yan et al., 2020), Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020), TOUCHDOWN (Chang et al., 2017)6, StreetLearn (Mirowski et al., 2019), StreetNav (Chang et al., 2017)8, Talk2Nav (Chang et al., 2017)0 RoomNav (Chang et al., 2017)1, EmbodiedQA (Chang et al., 2017)2, REVERIE (Chang et al., 2017)3, SOON (Chang et al., 2017)4 IQA (  Outdoor environments are usually more complex and contain more objects than indoor environments.
9. In TOUCHDOWN (Chang et al., 2017)6, an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object.
10. Most photo-realistic outdoor VLN datasets including TOUCHDOWN (Chang et al., 2017)6, StreetLearn (Chang et al., 2017)7, StreetNav(Chang et al., 2017)8, and Talk2Nav (Vasudevan et al., 2021 are proposed based on Google Street View.
11. Some work uses natural language to guide drones.
12. LANI (Chang et al., 2017)9 is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions.
13. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D (Blukis et al., 2018."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Linguistics, Computer Science",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s3,Coarse-grained Navigation,"['p3.0', 'p3.1', 'p3.2', 'p3.3']","['In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle). Usually, instructions are more concise and contain merely information of the target goal.', 'RoomNav (Wu et al., 2018) requires agent navigate according to instruction ""go to X"", where X is a predefined room or object.', 'In Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question. The instructions in REVERIE (Qi et al., 2020b) are annotated by humans, and thus more complicated and diverse. The agent navigates through the rooms and differentiates the object against multiple competing candidates. In SOON (Zhu et al., 2021a), an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.', 'Navigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available). In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple). Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question. Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment. CHAI (Misra et al., 2018) requires the agent to navigate and simply interact with the environments.']","In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle). Usually, instructions are more concise and contain merely information of the target goal.

RoomNav (Wu et al., 2018) requires agent navigate according to instruction ""go to X"", where X is a predefined room or object.

In Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question. The instructions in REVERIE (Qi et al., 2020b) are annotated by humans, and thus more complicated and diverse. The agent navigates through the rooms and differentiates the object against multiple competing candidates. In SOON (Zhu et al., 2021a), an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.

Navigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available). In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple). Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question. Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment. CHAI (Misra et al., 2018) requires the agent to navigate and simply interact with the environments.","(p3.0) In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle). Usually, instructions are more concise and contain merely information of the target goal.

(p3.1) RoomNav (Wu et al., 2018) requires agent navigate according to instruction ""go to X"", where X is a predefined room or object.

(p3.2) In Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question. The instructions in REVERIE (Qi et al., 2020b) are annotated by humans, and thus more complicated and diverse. The agent navigates through the rooms and differentiates the object against multiple competing candidates. In SOON (Zhu et al., 2021a), an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.

(p3.3) Navigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available). In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple). Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question. Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (2020) propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment. CHAI (Misra et al., 2018) requires the agent to navigate and simply interact with the environments.","[[], ['b16'], ['b25', None], ['b31', None]]","[[], ['b16'], ['b25', None], ['b31', None]]",5,"1. In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle).
2. Usually, instructions are more concise and contain merely information of the target goal.
3. RoomNav (Wu et al., 2018) requires agent navigate according to instruction ""go to X"", where X is a predefined room or object.
4. In Embodied QA (Das et al., 2018), the agent navigates through the environment to find answer for a given question.
5. The instructions in REVERIE (Qi et al., 2020b) are annotated by humans, and thus more complicated and diverse.
6. The agent navigates through the rooms and differentiates the object against multiple competing candidates.
7. In SOON (Zhu et al., 2021a), an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.
8. Navigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available).
9. In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple).
10. Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question.
11. Based on indoor scenes in AI2-THOR (Kolve et al., 2017), Shridhar et al. (Wu et al., 2018)0 propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment.
12. CHAI (Wu et al., 2018)1 requires the agent to navigate and simply interact with the environments."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Linguistics, Computer Science",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s9,Semantic Understanding,"['p9.0', 'p9.1', 'p9.2']","['Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.', 'Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).', 'Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).']","Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","(p9.0) Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.

(p9.1) Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b). Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019). Different types of tokens within the instruction also function differently (Zhu et al., 2021b). Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).

(p9.2) Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021). The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a).","[[], ['b22', 'b21', None, 'b4', 'b28'], ['b21', None]]","[[], ['b22', 'b21', None, 'b4', 'b28'], ['b21', None]]",7,"1. Semantic understanding of VLN tasks incorporates knowledge about important features in VLN.
2. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.
3. Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN.
4. The overall visual features extracted by a neural model may actually hurt the performance in some cases (Thomason et al., 2019a;Hu et al., 2019;Zhang et al., 2020b).
5. Therefore, it is important to find the feature(s) that best improve performance.
6. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN (Hu et al., 2019).
7. Different types of tokens within the instruction also function differently (Zhu et al., 2021b).
8. Extracting these tokens and encoding the object tokens and directions tokens are crucial (Qi et al., 2020a;Zhu et al., 2021b).
9. Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism (Qi et al., 2020a;Gao et al., 2021).
10. The soft alignment also highlights relevant parts of the instruction with respect to the current step (Landi et al., 2019;Zhang et al., 2020a)."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Linguistics, Computer Science",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s14,Reinforcement Learning,"['p14.0', 'p14.1']","['VLN is a sequential decision-making problem and can naturally be modeled as a Markov decision process. So Reinforcement Learning (RL) methods are proposed to learn better policy for VLN tasks. A critical challenge for RL methods is that VLN agents only receive the success signal at the end of the episode, so it is difficult to know which actions to attribute success to, and which to penalize. To address the ill-posed feedback issue, Wang et al. (2019) propose RCM model to enforces cross-modal grounding both locally and globally, with goal-oriented extrinsic reward and instructionfidelity intrinsic reward. He et al. (2021) propose to utilize the local alignment between the instruction and critical landmarks as the reward. Evaluation metrics such as CLS (Jain et al., 2019) or nDTW (Ilharco et al., 2019) can also provide informative reward signal (Landi et al., 2020), and natural language may also provide suggestions for reward (Fu et al., 2019).', 'To model the dynamics in the environment, Wang et al. (2018) leverage model-based reinforcement learning to predict the next state and improve the generalization in unseen environment. Zhang et al. (2020a) find recursively alternating the learning schemes of imitation and reinforcement learning improve the performance.']","VLN is a sequential decision-making problem and can naturally be modeled as a Markov decision process. So Reinforcement Learning (RL) methods are proposed to learn better policy for VLN tasks. A critical challenge for RL methods is that VLN agents only receive the success signal at the end of the episode, so it is difficult to know which actions to attribute success to, and which to penalize. To address the ill-posed feedback issue, Wang et al. (2019) propose RCM model to enforces cross-modal grounding both locally and globally, with goal-oriented extrinsic reward and instructionfidelity intrinsic reward. He et al. (2021) propose to utilize the local alignment between the instruction and critical landmarks as the reward. Evaluation metrics such as CLS (Jain et al., 2019) or nDTW (Ilharco et al., 2019) can also provide informative reward signal (Landi et al., 2020), and natural language may also provide suggestions for reward (Fu et al., 2019).

To model the dynamics in the environment, Wang et al. (2018) leverage model-based reinforcement learning to predict the next state and improve the generalization in unseen environment. Zhang et al. (2020a) find recursively alternating the learning schemes of imitation and reinforcement learning improve the performance.","(p14.0) VLN is a sequential decision-making problem and can naturally be modeled as a Markov decision process. So Reinforcement Learning (RL) methods are proposed to learn better policy for VLN tasks. A critical challenge for RL methods is that VLN agents only receive the success signal at the end of the episode, so it is difficult to know which actions to attribute success to, and which to penalize. To address the ill-posed feedback issue, Wang et al. (2019) propose RCM model to enforces cross-modal grounding both locally and globally, with goal-oriented extrinsic reward and instructionfidelity intrinsic reward. He et al. (2021) propose to utilize the local alignment between the instruction and critical landmarks as the reward. Evaluation metrics such as CLS (Jain et al., 2019) or nDTW (Ilharco et al., 2019) can also provide informative reward signal (Landi et al., 2020), and natural language may also provide suggestions for reward (Fu et al., 2019).

(p14.1) To model the dynamics in the environment, Wang et al. (2018) leverage model-based reinforcement learning to predict the next state and improve the generalization in unseen environment. Zhang et al. (2020a) find recursively alternating the learning schemes of imitation and reinforcement learning improve the performance.","[[None, 'b11'], ['b12', 'b21']]","[[None, 'b11'], ['b12', 'b21']]",4,"1. VLN is a sequential decision-making problem and can naturally be modeled as a Markov decision process.
2. So Reinforcement Learning (RL) methods are proposed to learn better policy for VLN tasks.
3. A critical challenge for RL methods is that VLN agents only receive the success signal at the end of the episode, so it is difficult to know which actions to attribute success to, and which to penalize.
4. To address the ill-posed feedback issue, Wang et al. (2019) propose RCM model to enforces cross-modal grounding both locally and globally, with goal-oriented extrinsic reward and instructionfidelity intrinsic reward.
5. He et al. (2021) propose to utilize the local alignment between the instruction and critical landmarks as the reward.
6. Evaluation metrics such as CLS (Jain et al., 2019) or nDTW (Ilharco et al., 2019) can also provide informative reward signal (Landi et al., 2020), and natural language may also provide suggestions for reward (Fu et al., 2019).
7. To model the dynamics in the environment, Wang et al. (2018) leverage model-based reinforcement learning to predict the next state and improve the generalization in unseen environment.
8. Zhang et al. (2020a) find recursively alternating the learning schemes of imitation and reinforcement learning improve the performance."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Linguistics, Computer Science",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s19,Data Augmentation,['p19.0'],"['Trajectory-Instruction Augmentation Augmented path-instruction pairs could be used in VLN directly. Currently the common practice is to train a speaker module to generate instructions given a navigation path (Fried et al., 2018). This generated data have varying quality (Zhao et al., 2021). Therefore an alignment scorer (Huang et al., 2019) or adversarial discriminator (Fu et al., 2020) can select high-quality pairs for augmentation. Environment Augmentation Generating more environment data not only helps generate more trajectories, but also alleviates the problem of overfitting in seen environments. Randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or simply splitting the house scenes and remixing them (Liu et al., 2021) could create new environments, which could further be used to generate more trajectory-instruction pairs (Fried et al., 2018). Training data may also be augmented by replacing some visual features with counterfactual ones (Parvaneh et al., 2020).']","Trajectory-Instruction Augmentation Augmented path-instruction pairs could be used in VLN directly. Currently the common practice is to train a speaker module to generate instructions given a navigation path (Fried et al., 2018). This generated data have varying quality (Zhao et al., 2021). Therefore an alignment scorer (Huang et al., 2019) or adversarial discriminator (Fu et al., 2020) can select high-quality pairs for augmentation. Environment Augmentation Generating more environment data not only helps generate more trajectories, but also alleviates the problem of overfitting in seen environments. Randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or simply splitting the house scenes and remixing them (Liu et al., 2021) could create new environments, which could further be used to generate more trajectory-instruction pairs (Fried et al., 2018). Training data may also be augmented by replacing some visual features with counterfactual ones (Parvaneh et al., 2020).","(p19.0) Trajectory-Instruction Augmentation Augmented path-instruction pairs could be used in VLN directly. Currently the common practice is to train a speaker module to generate instructions given a navigation path (Fried et al., 2018). This generated data have varying quality (Zhao et al., 2021). Therefore an alignment scorer (Huang et al., 2019) or adversarial discriminator (Fu et al., 2020) can select high-quality pairs for augmentation. Environment Augmentation Generating more environment data not only helps generate more trajectories, but also alleviates the problem of overfitting in seen environments. Randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or simply splitting the house scenes and remixing them (Liu et al., 2021) could create new environments, which could further be used to generate more trajectory-instruction pairs (Fried et al., 2018). Training data may also be augmented by replacing some visual features with counterfactual ones (Parvaneh et al., 2020).","[['b23', None, 'b1', 'b11']]","[['b23', None, 'b1', 'b11']]",4,"1. Trajectory-Instruction Augmentation Augmented path-instruction pairs could be used in VLN directly.
2. Currently the common practice is to train a speaker module to generate instructions given a navigation path (Fried et al., 2018).
3. This generated data have varying quality (Zhao et al., 2021).
4. Therefore an alignment scorer (Huang et al., 2019) or adversarial discriminator (Fu et al., 2020) can select high-quality pairs for augmentation.
5. Environment Augmentation Generating more environment data not only helps generate more trajectories, but also alleviates the problem of overfitting in seen environments.
6. Randomly masking the same visual feature across different viewpoints (Tan et al., 2019) or simply splitting the house scenes and remixing them (Liu et al., 2021) could create new environments, which could further be used to generate more trajectory-instruction pairs (Fried et al., 2018).
7. Training data may also be augmented by replacing some visual features with counterfactual ones (Parvaneh et al., 2020)."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Linguistics, Computer Science",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s21,Multitask Learning,['p21.0'],"['Different VLN tasks can benefit from each other by cross-task knowledge transfer. Wang et al. (2020c) propose an environment-agnostic multitask navigation model for both VLN and Navigation from Dialog History tasks (Thomason et al., 2019b). Chaplot et al. (2020) propose an attention module to train a multitask navigation agent to follow instructions and answer questions (Wijmans et al., 2019a).']","Different VLN tasks can benefit from each other by cross-task knowledge transfer. Wang et al. (2020c) propose an environment-agnostic multitask navigation model for both VLN and Navigation from Dialog History tasks (Thomason et al., 2019b). Chaplot et al. (2020) propose an attention module to train a multitask navigation agent to follow instructions and answer questions (Wijmans et al., 2019a).","(p21.0) Different VLN tasks can benefit from each other by cross-task knowledge transfer. Wang et al. (2020c) propose an environment-agnostic multitask navigation model for both VLN and Navigation from Dialog History tasks (Thomason et al., 2019b). Chaplot et al. (2020) propose an attention module to train a multitask navigation agent to follow instructions and answer questions (Wijmans et al., 2019a).","[['b5', None, 'b13', 'b14']]","[['b5', None, 'b13', 'b14']]",4,"1. Different VLN tasks can benefit from each other by cross-task knowledge transfer.
2. Wang et al. (2020c) propose an environment-agnostic multitask navigation model for both VLN and Navigation from Dialog History tasks (Thomason et al., 2019b).
3. Chaplot et al. (2020) propose an attention module to train a multitask navigation agent to follow instructions and answer questions (Wijmans et al., 2019a)."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Linguistics, Computer Science",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s23,Prior Exploration,['p23.0'],"[""Good performance in seen environments often cannot generalize to unseen environments (Hu et al., 2019;Parvaneh et al., 2020;Tan et al., 2019). Prior exploration methods allow the agent to observe and adapt to unseen environments, 3 bridging the performance gap between seen and unseen environments. Wang et al. (2019) introduce a self-supervised imitation learning to learn from the agent's own past, good behaviors. The best navigation path determined to align the instruction the best by a matching critic will be used to update the agent. Tan et al. (2019) leverage the testing environments to sample and augment paths for adaptation. Fu et al. (2020) propose environment-based prior exploration, where the agent can only explore a particular environment where it is deployed. When utilizing graph, prior exploration may construct a map or overview about the unseen environment to provide explicit guidance for navigation (Chen et al., 2021a;Zhou et al., 2021).""]","Good performance in seen environments often cannot generalize to unseen environments (Hu et al., 2019;Parvaneh et al., 2020;Tan et al., 2019). Prior exploration methods allow the agent to observe and adapt to unseen environments, 3 bridging the performance gap between seen and unseen environments. Wang et al. (2019) introduce a self-supervised imitation learning to learn from the agent's own past, good behaviors. The best navigation path determined to align the instruction the best by a matching critic will be used to update the agent. Tan et al. (2019) leverage the testing environments to sample and augment paths for adaptation. Fu et al. (2020) propose environment-based prior exploration, where the agent can only explore a particular environment where it is deployed. When utilizing graph, prior exploration may construct a map or overview about the unseen environment to provide explicit guidance for navigation (Chen et al., 2021a;Zhou et al., 2021).","(p23.0) Good performance in seen environments often cannot generalize to unseen environments (Hu et al., 2019;Parvaneh et al., 2020;Tan et al., 2019). Prior exploration methods allow the agent to observe and adapt to unseen environments, 3 bridging the performance gap between seen and unseen environments. Wang et al. (2019) introduce a self-supervised imitation learning to learn from the agent's own past, good behaviors. The best navigation path determined to align the instruction the best by a matching critic will be used to update the agent. Tan et al. (2019) leverage the testing environments to sample and augment paths for adaptation. Fu et al. (2020) propose environment-based prior exploration, where the agent can only explore a particular environment where it is deployed. When utilizing graph, prior exploration may construct a map or overview about the unseen environment to provide explicit guidance for navigation (Chen et al., 2021a;Zhou et al., 2021).","[[None, 'b1', 'b24', 'b11']]","[[None, 'b1', 'b24', 'b11']]",4,"1. Good performance in seen environments often cannot generalize to unseen environments (Hu et al., 2019;Parvaneh et al., 2020;Tan et al., 2019).
2. Prior exploration methods allow the agent to observe and adapt to unseen environments, 3 bridging the performance gap between seen and unseen environments.
3. Wang et al. (2019) introduce a self-supervised imitation learning to learn from the agent's own past, good behaviors.
4. The best navigation path determined to align the instruction the best by a matching critic will be used to update the agent.
5. Tan et al. (2019) leverage the testing environments to sample and augment paths for adaptation.
6. Fu et al. (2020) propose environment-based prior exploration, where the agent can only explore a particular environment where it is deployed.
7. When utilizing graph, prior exploration may construct a map or overview about the unseen environment to provide explicit guidance for navigation (Chen et al., 2021a;Zhou et al., 2021)."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Linguistics, Computer Science",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s24,Related Visual-and-Language Tasks,"['p24.0', 'p24.1']","['This paper focuses on Vision-and-Language Navigation tasks with an emphasis on photo-realistic environments. 2D map may also be a uesful virtual environment for navigation tasks (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Paz-Argaman and Tsarfaty, 2019). Synthetic environments may also be a substitute for realistic environment (MacMahon et al., 2006;Blukis et al., 2020). Tellex et al. (2011) propose to instantiate a probabilistic graphical model for natural language commands in robotic navigation and mobile manipulation process.', 'In VLN, an agent needs to follow the given instruction and even ask for assistants in human language. An agent in Visual Navigation tasks is usually not required to understand information from textual modality. Visual Navigation is a problem of navigating an agent from the current location to find the goal target. Researchers have achieved success in both simulated environments (Zhu et al., 2017;Mirowski, 2019) and real environments (Mirowski et al., 2018).']","This paper focuses on Vision-and-Language Navigation tasks with an emphasis on photo-realistic environments. 2D map may also be a uesful virtual environment for navigation tasks (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Paz-Argaman and Tsarfaty, 2019). Synthetic environments may also be a substitute for realistic environment (MacMahon et al., 2006;Blukis et al., 2020). Tellex et al. (2011) propose to instantiate a probabilistic graphical model for natural language commands in robotic navigation and mobile manipulation process.

In VLN, an agent needs to follow the given instruction and even ask for assistants in human language. An agent in Visual Navigation tasks is usually not required to understand information from textual modality. Visual Navigation is a problem of navigating an agent from the current location to find the goal target. Researchers have achieved success in both simulated environments (Zhu et al., 2017;Mirowski, 2019) and real environments (Mirowski et al., 2018).","(p24.0) This paper focuses on Vision-and-Language Navigation tasks with an emphasis on photo-realistic environments. 2D map may also be a uesful virtual environment for navigation tasks (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Paz-Argaman and Tsarfaty, 2019). Synthetic environments may also be a substitute for realistic environment (MacMahon et al., 2006;Blukis et al., 2020). Tellex et al. (2011) propose to instantiate a probabilistic graphical model for natural language commands in robotic navigation and mobile manipulation process.

(p24.1) In VLN, an agent needs to follow the given instruction and even ask for assistants in human language. An agent in Visual Navigation tasks is usually not required to understand information from textual modality. Visual Navigation is a problem of navigating an agent from the current location to find the goal target. Researchers have achieved success in both simulated environments (Zhu et al., 2017;Mirowski, 2019) and real environments (Mirowski et al., 2018).","[['b7', None, 'b3'], ['b31', None]]","[['b7', None, 'b3'], ['b31', None]]",5,"1. This paper focuses on Vision-and-Language Navigation tasks with an emphasis on photo-realistic environments.
2. 2D map may also be a uesful virtual environment for navigation tasks (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Paz-Argaman and Tsarfaty, 2019).
3. Synthetic environments may also be a substitute for realistic environment (MacMahon et al., 2006;Blukis et al., 2020).
4. Tellex et al. (2011) propose to instantiate a probabilistic graphical model for natural language commands in robotic navigation and mobile manipulation process.
5. In VLN, an agent needs to follow the given instruction and even ask for assistants in human language.
6. An agent in Visual Navigation tasks is usually not required to understand information from textual modality.
7. Visual Navigation is a problem of navigating an agent from the current location to find the goal target.
8. Researchers have achieved success in both simulated environments (Zhu et al., 2017;Mirowski, 2019) and real environments (Mirowski et al., 2018)."
247627890,"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions","Linguistics, Computer Science",https://www.semanticscholar.org/paper/4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,s28,B Simulator,"['p28.0', 'p28.1', 'p28.2', 'p28.3', 'p28.4', 'p28.5', 'p28.6']","['The virtual features of the dataset are deeply connected with the simulator in which datasets are built. Here we summarize simulators frequently used during the VLN dataset creation process. House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.', 'Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017). Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches. An agent can navigate between viewpoints along a pre-defined graph. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.', 'Habitat ( where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED (Shridhar et al., 2020).', 'Gibson (Xia et al., 2018) is a real-world perception interactive environment with complex semantics. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.', ""House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects)."", 'LANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks. An agent needs to navigate between landmarks following the natural language instruction. Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.', 'Currently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity. Google Street View 4 , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN (Chen et al., 2019).']","The virtual features of the dataset are deeply connected with the simulator in which datasets are built. Here we summarize simulators frequently used during the VLN dataset creation process. House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.

Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017). Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches. An agent can navigate between viewpoints along a pre-defined graph. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.

Habitat ( where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED (Shridhar et al., 2020).

Gibson (Xia et al., 2018) is a real-world perception interactive environment with complex semantics. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.

House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects).

LANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks. An agent needs to navigate between landmarks following the natural language instruction. Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.

Currently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity. Google Street View 4 , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN (Chen et al., 2019).","(p28.0) The virtual features of the dataset are deeply connected with the simulator in which datasets are built. Here we summarize simulators frequently used during the VLN dataset creation process. House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.

(p28.1) Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017). Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches. An agent can navigate between viewpoints along a pre-defined graph. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.

(p28.2) Habitat ( where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED (Shridhar et al., 2020).

(p28.3) Gibson (Xia et al., 2018) is a real-world perception interactive environment with complex semantics. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.

(p28.4) House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects).

(p28.5) LANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks. An agent needs to navigate between landmarks following the natural language instruction. Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.

(p28.6) Currently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity. Google Street View 4 , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN (Chen et al., 2019).","[['b16', None], [None], [None], [None, 'b17'], ['b16'], [None], []]","[['b16', None], [None], [None], [None, 'b17'], ['b16'], [None], []]",8,"1. The virtual features of the dataset are deeply connected with the simulator in which datasets are built.
2. Here we summarize simulators frequently used during the VLN dataset creation process.
3. House3D (Wu et al., 2018) is a realistic virtual 3D environment built based on the SUNCG (Song et al., 2017) dataset.
4. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.
5. Matterport3D (Anderson et al., 2018b) simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset (Chang et al., 2017).
6. Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches.
7. An agent can navigate between viewpoints along a pre-defined graph.
8. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.
9. Habitat ( where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED (Shridhar et al., 2020).
10. Gibson (Xia et al., 2018) is a real-world perception interactive environment with complex semantics.
11. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes.
12. Matterport3D dataset (Chang et al., 2017) is also integrated into the Gibson simulator.
13. House3D (Wu et al., 2018) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects).
14. LANI (Misra et al., 2018) is a 3D simulator built in Unity3D platform.
15. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks.
16. An agent needs to navigate between landmarks following the natural language instruction.
17. Drone navigation tasks (Blukis et al., 2018(Blukis et al., , 2019 are also built based on LANI.
18. Currently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity.
19. Google Street View 4 , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas.
20. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN (Song et al., 2017)0."
248426721,What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Computer Science,https://www.semanticscholar.org/paper/29a369d83a7d6a49f6a3259bee23bd4d95db0b16,s1,Relation Extraction Datasets Survey,"['p1.0', 'p1.1', 'p1.2', 'p1.3', 'p1.4', 'p1.5', 'p1.6', 'p1.7', 'p1.8', 'p1.9', 'p1.10', 'p1.11']","['RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.', 'Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.', '3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).', 'Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.', 'The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.', 'Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).', 'All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).', 'Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.', 'Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.', 'Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:', '1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).', 'In the case study of this paper, given the scientific datasets available, we focus on the first setup.']","RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

In the case study of this paper, given the scientific datasets available, we focus on the first setup.","(p1.0) RE has been broadly studied in the last decades and many datasets were published. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia. An overview of the datasets is given in Table 1. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works. It contains annotations for named entities and relations in news articles. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004). It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic. The corpus is divided into six domains.

(p1.1) Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010). It contains over 1.8 million articles by the NYT between 1987 and 2007. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (2019) published manually annotated versions of the test set in order to perform a more accurate evaluation.

(p1.2) 3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (Nastase et al., 2021). For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (10k instances, in comparison to 1.5k of the 2007 shared task).

(p1.3) Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks. In SemEval-2017 Task 10 Augenstein et al. (2017) proposed a dataset for the identification of keyphrases and considered two generic relations (HYPONYM-OF and SYNONYM-OF). The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields. The year after, Gábor et al. (2018) proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7. The data will be described in further detail in Section 4.1. Following the same line, Luan et al. (2018) published SCIERC, which is a scientific RE dataset further annotated for coreference resolution. It contains abstracts from scientific AI-related conferences. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (SemEval-2018 and SCIERC). We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.

(p1.4) The Wikipedia domain has been first introduced in 2013. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia. More recently, Kassner et al. (2021) proposed mLAMA, a multilingual version (53 languages) of GoogleRE with the purpose of investigating knowledge in pretrained language models. The multi-lingual dimension is gaining more interest for RE. Following this trend, Seganti et al. (2021) presented SMiLER, a multilingual dataset (14 languages) from Wikipedia with relations belonging to nine domains.

(p1.5) Previous datasets were restricted to the same label collection in the training set and in the test set. To address this gap and make RE experimental scenarios more realistic, Han et al. (2018) -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (2017b) Newswire and web text 42 FSL TACRED Sabo et al. (2021) TACRED   Back to the news domain, Zhang et al. (2017b) published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models. Sabo et al. (2021) used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (i.e., non-uniform label distribution, inclusion of pronouns and common nouns).

(p1.6) All datasets so far present a sentence level annotation. To address this, Yao et al. (2019) published DocRED, a document-level RE dataset from Wikipedia and Wikidata. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level. In addition to RE, DocRED annotates coreference chains. DWIE by Zaporojets et al. (2021) is another document-level dataset, specifically designed for multi-task IE (Named Entity Recognition, Coreference Resolution, Relation Extraction, and Entity Linking).

(p1.7) Lastly, there are works focusing on creating datasets for specific RE aspects. Cheng et al. (2021), for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.

(p1.8) Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia. Similarly, we observe the emerging trend for FSL.

(p1.9) Different datasets lend themselves to study different aspects of the task. Concerning crossdomain RE, we propose to distinguish three setups:

(p1.10) 1. Data from different domains, but same relation types, which are general enough to be present in each domain (limited and often confined to the ACE dataset) (e.g., Plank and Moschitti, 2013).

(p1.11) In the case study of this paper, given the scientific datasets available, we focus on the first setup.","[[None, 'b30'], ['b29', None, 'b59', 'b10'], ['b7', None], [None, 'b1', 'b18'], ['b32', 'b11'], ['b6', 'b7', None, 'b31', 'b55'], ['b48', 'b52'], [None], [], [], ['b25'], []]","[[None, 'b30'], ['b29', None, 'b59', 'b10'], ['b7', None], [None, 'b1', 'b18'], ['b32', 'b11'], ['b6', 'b7', None, 'b31', 'b55'], ['b48', 'b52'], [None], [], [], ['b25'], []]",22,"1. RE has been broadly studied in the last decades and many datasets were published.
2. We survey widely used RE datasets in chronological order, and broadly classify them into three domains based on the data source: (1) news and web, (2) scientific publications and (3) Wikipedia.
3. An overview of the datasets is given in Table 1.
4. Our empirical target here focuses on the scientific domain as so far it has received no attention in the cross-domain direction; a similar investigation on overlaps in data, annotation, and model transferability between datasets in other domains is interesting future work.
5. The CoNLL 2004 dataset (Roth and Yih, 2004) is one of the first works.
6. It contains annotations for named entities and relations in news articles.
7. In the same year, the widely studied ACE dataset was published by Doddington et al. (2004).
8. It contains annotated entities, relations and events in broadcast transcripts, newswire and newspaper data in English, Chinese and Arabic.
9. The corpus is divided into six domains.
10. Another widely used dataset is The New York Times (NYT) Annotated Corpus, 3 first presented by Riedel et al. (2010).
11. It contains over 1.8 million articles by the NYT between 1987 and 2007.
12. NYT has been created with a distant supervision approach (Mintz et al., 2009), using Freebase (Bollacker et al., 2008 as knowledge base. Two further versions of it followed recently: Zhu et al. (2020b) (NYT-H) and Jia et al. (Roth and Yih, 2004)0 published manually annotated versions of the test set in order to perform a more accurate evaluation.
13. 3 http://iesl.cs.umass.edu/riedel/ecml/ RE has also been part of the SemEval shared tasks for four times so far.
14. The two early Se-mEval shared tasks focused on the identification of semantic relations between nominals (2)1.
15. For SemEval-2007Task 4, Girju et al. (2007 released a dataset for RC into seven generic semantic relations between nominals.
16. Three years later, for SemEval-2010 Task 8, Hendrickx et al. (2010) revised the annotation guidelines and published a corpus for RC, by providing a much larger dataset (2)3.Since 2017, three RE datasets in the scientific domain emerged, two of the three as SemEval shared tasks.
17. In SemEval-2017 Task 10 Augenstein et al. (2)4 proposed a dataset for the identification of keyphrases and considered two generic relations (2)5.
18. The dataset is called ScienceIE and consists of 500 journal articles from the Computer Science, Material Sciences and Physics fields.
19. The year after, Gábor et al. (3)3 proposed a corpus for RC and RE made of abstracts of scientific papers from the ACL Anthology for SemEval-2018 Task 7.
20. The data will be described in further detail in Section 4.1.
21. Following the same line, Luan et al. (3)3 published SCIERC, which is a scientific RE dataset further annotated for coreference resolution.
22. It contains abstracts from scientific AI-related conferences.
23. From the existing three scientific RE datasets summarized in Table 1, in our empirical investigation we focus on two (2)8.
24. We leave out ScienceIE as it focuses on keyphrase extraction and it contains two generic relations only.
25. The Wikipedia domain has been first introduced in 2013.
26. Google released GoogleRE, 4 a RE corpus consisting of snippets from Wikipedia.
27. More recently, Kassner et al. (Roth and Yih, 2004)3 proposed mLAMA, a multilingual version (3)0 of GoogleRE with the purpose of investigating knowledge in pretrained language models.
28. The multi-lingual dimension is gaining more interest for RE.
29. Following this trend, Seganti et al. (Roth and Yih, 2004)3 presented SMiLER, a multilingual dataset (3)2 from Wikipedia with relations belonging to nine domains.
30. Previous datasets were restricted to the same label collection in the training set and in the test set.
31. To address this gap and make RE experimental scenarios more realistic,
32. Han et al. (3)3 -2007- Girju et al. (2007 Sentences from the web 7 SemEval-2010 Hendrickx et al. (2010) Sentences from the web 10 TACRED Zhang et al. (3)7 Newswire and web text 42 FSL TACRED Sabo et al. (Roth and Yih, 2004)3 TACRED   Back to the news domain, Zhang et al. (3)7 published a large-scale RE dataset built over newswire and web text, by crowdsourcing relation annotations for sentences with named entity pairs.
33. This resulted in the TACRED dataset with over 100k instances, which is particularly well-suited for neural models.
34. Sabo et al. (Roth and Yih, 2004)3 used TA-CRED to make a FSL RC dataset and compared it to FewRel 1.0 and FewRel 2.0, aiming at a more realistic scenario (3)9.
35. All datasets so far present a sentence level annotation.
36. To address this, Yao et al. (Roth and Yih, 2004)0 published DocRED, a document-level RE dataset from Wikipedia and Wikidata.
37. The difference with a traditional sentence-level corpus is that both the intraand inter-sentence relations are annotated, increasing the challenge level.
38. In addition to RE, DocRED annotates coreference chains.
39. DWIE by Zaporojets et al. (Roth and Yih, 2004)3 is another document-level dataset, specifically designed for multi-task IE (Roth and Yih, 2004)2.
40. Lastly, there are works focusing on creating datasets for specific RE aspects.
41. Cheng et al. (Roth and Yih, 2004)3, for example, proposed a Chinese documentlevel RE dataset for hard cases in order to move towards even more challenging evaluation setups.
42. Domains in RE Given our analysis, we observe a shift in target domains: from news text in seminal works, over web texts, to emerging corpora in the scientific domain and the most recent focus on Wikipedia.
43. Similarly, we observe the emerging trend for FSL.
44. Different datasets lend themselves to study different aspects of the task.
45. Concerning crossdomain RE, we propose to distinguish three setups:1.
46. Data from different domains, but same relation types, which are general enough to be present in each domain (Roth and Yih, 2004)4 (Roth and Yih, 2004)5.
47. In the case study of this paper, given the scientific datasets available, we focus on the first setup."
248426721,What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification,Computer Science,https://www.semanticscholar.org/paper/29a369d83a7d6a49f6a3259bee23bd4d95db0b16,s2,The Relation Extraction Task,"['p2.0', 'p2.1', 'p2.2']","['Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.', 'One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.', 'Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.']","Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","(p2.0) Conceptually, RE involves a pipeline of steps (see Figure 2). Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD). 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.

(p2.1) One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI). As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered). The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (no-rel) label. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (without a no-rel label). Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NOTA) relation (Gao et al., 2019). Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set. 6 What Do You Mean by Relation Extraction? RE studies rarely address the whole pipeline. We 5 Some studies divide the entity extraction into two substeps: identification (often called MD), and subsequent classification into entity types. 6 Some studies name such relation Other (Hendrickx et al., 2010). analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER/MD, RI, RC). Table 2 shows such investigation. We leave out from this analysis (a) papers which make use of distant supervision or which somehow involve knowledge bases, (b) shared task papers, (c) the bioNLP field, (d) temporal RE, and (e) Open RE. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (Taillé et al., 2020). Our analysis further shows that it is difficult to determine the RI setup. While RC is always performed, the situation is different for RI (or no-rel). Sometimes RI is clearly not done (i.e., the paper assumes a scenario in which every instance contains at least one relation), but most of the times it is either not clear from the paper, or done in a simplified scenario (e.g., datasets which already clear out most of the no-rel entity pair instances). As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (no-rel and NOTA) are handled. These details are utterly important as they impact both model estimation and evaluation.

(p2.2) Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (Miwa and Bansal, 2016;Zhang et al., 2017a;Bekoulis et al., 2018a,b;Wang and Lu, 2020;Wang et al., 2021). However, Taillé et al. (2020) recently discussed the challenge of properly evaluating such complex models. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (2018a). They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models. Since the output label space is different, separate encoders could better capture distinct contextual information. At the moment it is not clear if one approach is more suitable than the other for RE. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task.","[[], ['b32', 'b36', 'b7', None, 'b3'], ['b36', 'b56', None, 'b43', 'b44', 'b54']]","[[], ['b32', 'b36', 'b7', None, 'b3'], ['b36', 'b56', None, 'b43', 'b44', 'b54']]",11,"1. Conceptually, RE involves a pipeline of steps (see Figure 2).
2. Starting from the raw text, the first step consists in identifying the entities and eventually assigning them a type.
3. Entities involve either nominals or named entities, and hence it is either Named Entity Recognition (NER) or, more broadly, Mention Detection (MD).
4. 5 After entities are identified, approaches start to be more blurry as studies have approached RE via different angles.
5. One way is to take two steps, Relation Identification (RI) and subsequent Relation Classification (RC) , as illustrated in Figure 2.
6. This means to first identify from all the possible entity pairs the ones which are in some kind of relation via a binary classification task (RI).
7. As the proportion of positive samples over the negative is usually extremely unbalanced towards the latter (Gormley et al., 2015), a priori heuristics are generally applied to reduce the possible combinations (e.g., entity pairs involving distant entities, or entity type pairs not licensed by the relations are not even considered).
8. The last step (RC) is usually a multi-class classification to assign a relation type r to the positive samples from the previous step.
9. Some studies merge RI and RC (Seganti et al., 2021) into one step, by adding a no-relation (NER)0 label.
10. Other studies instead reduce the task to RC, and assume there exists a relation between two entities and the task is to determine the type (NER)1.
11. Regardless, RI is influenced by the RC setup: Relations which are not in the RC label set are considered as negative samples in the RI phase.
12. Some studies address this approximation by distinguishing between the no-rel and the None-Of-The-Above (NER)2 relation (NER)3.
13. Note that, in our definition, the NOTA label differs from no-rel in the sense that a relation holds between the two entities, but its type is not in the considered RC label set.
14. 6 What Do You Mean by Relation Extraction?
15. RE studies rarely address the whole pipeline.
16. We 5 Some studies divide the entity extraction into two substeps: identification (NER)4, and subsequent classification into entity types.
17. 6 Some studies name such relation Other (NER)5. analyze all the ACL papers published in the last five years which contain the Relation Extraction keyword in the title and determine which sub-task is performed (NER)6.
18. Table 2 shows such investigation.
19. We leave out from this analysis (NER)7 papers which make use of distant supervision or which somehow involve knowledge bases, (NER)8 shared task papers, (NER)9 the bioNLP field, (MD)0 temporal RE, and (MD)1 Open RE.
20. The result shows that gold entities are usually assumed for RE, presumably given the complexity of the NER/MD task on its own.
21. Most importantly, for end-to-end models, recent work has shown that ablations for steps like NER are lacking (MD)2.
22. Our analysis further shows that it is difficult to determine the RI setup.
23. While RC is always performed, the situation is different for RI (MD)3.
24. Sometimes RI is clearly not done (MD)4, but most of the times it is either not clear from the paper, or done in a simplified scenario (MD)5.
25. As this blurriness hampers fair evaluation, we propose that studies clearly state which step they include, i.e., whether the work focus is on RC, RI+RC or the full RE pipeline and how special cases (MD)6 are handled.
26. These details are utterly important as they impact both model estimation and evaluation.
27. Pipeline or Joint Model? The traditional RE pipeline is, by definition of pipeline, prone to error propagation by sub-tasks.
28. Joint entity and relation extraction approaches have been proposed in order to alleviate this problem (MD)7.
29. However, Taillé et al. (MD)8 recently discussed the challenge of properly evaluating such complex models.
30. They surveyed the evaluation metrics of recently published works on end-to-end RE referring to the Strict, Boundaries, Relaxed evaluation setting pro-  posed by Bekoulis et al. (MD)9.
31. They observe unfair comparisons and overestimations of end-toend models, and claim the need for more rigorous reports of evaluation settings, including detailed datasets statistics.
32. While some recent work shifts to joint models, it is still an open question which approach (joint or pipeline) is the most robust.
33. Zhong and Chen (2021) found that when incorporating modern pretrained language models (e.g., BERT) using separate encoders can surpass existing joint models.
34. Since the output label space is different, separate encoders could better capture distinct contextual information.
35. At the moment it is not clear if one approach is more suitable than the other for RE.
36. For this reason and because of our final goal, which is a closer look to sub-domains in the scientific field, we follow the pipeline approach and, following most work from Table 2, we here restrict the setup by focusing on the RC task."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s3,Output Uncertainty,"['p3.0', 'p3.1']","['Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).', ""Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).""]","Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","(p3.0) Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy. It prefers the most uncertain instances judged by the model outputs. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009). Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).

(p3.1) Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region. If an instance is near the decision boundary, the model's outputs may be different within its local region. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020).","[['b11', 'b91', 'b49', 'b25', None, 'b10', 'b12', 'b14', 'b19'], [None, 'b72']]","[['b11', 'b91', 'b49', 'b25', None, 'b10', 'b12', 'b14', 'b19'], [None, 'b72']]",11,"1. Uncertainty sampling (Lewis and Gale, 1994) is probably the simplest and the most commonly utilized query strategy.
2. It prefers the most uncertain instances judged by the model outputs.
3. For probabilistic models, entropy-based (Shannon, 1948), least-confidence (Culotta and McCallum, 2005) and margin-sampling (Scheffer et al., 2001;Schein and Ungar, 2007) are three typical uncertainty sampling strategies (Settles, 2009).
4. Schröder et al. (2022) revisit some of these uncertainty-based strategies with Transformer-based models and provide empirical results for text classification.
5. For non-probabilistic models, similar ideas can be utilized, such as selecting the instances that are close to the decision boundary in an SVM (Schohn and Cohn, 2000;Tong and Koller, 2001).
6. Another way to measure output uncertainty is to check the divergence of a model's predictions with respect to an instance's local region.
7. If an instance is near the decision boundary, the model's outputs may be different within its local region.
8. In this spirit, recent works examine different ways to check instances' local divergence, such as nearestneighbour searches (Margatina et al., 2021), adversarial perturbation (Zhang et al., 2022b) and data augmentation (Jiang et al., 2020)."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s6,Performance Prediction,"['p6.0', 'p6.1', 'p6.2']","['Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.', 'Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.', 'A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).']","Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.

Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.

A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).","(p6.0) Predicting performance can be another indicator for querying. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model. This strategy can be computationally costly since retraining is needed for each candidate.

(p6.1) Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b). This learning-to-select strategy may have some constraints. First, it requires labeled data (maybe from another domain) to train the policy. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.

(p6.2) A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Haffari et al., 2009; or quality estimation (Logacheva and Specia, 2014a,b).","[[None], [None, 'b98', 'b53'], [None, 'b38', 'b106', 'b62']]","[[None], [None, 'b98', 'b53'], [None, 'b38', 'b106', 'b62']]",8,"1. Predicting performance can be another indicator for querying.
2. Ideally, the selected instances should be the ones that most reduce future errors if labeled and added to the training set.
3. This motivates the expected error reduction strategy (Roy and McCallum, 2001), which chooses instances that lead to the least expected error if added to retrain a model.
4. This strategy can be computationally costly since retraining is needed for each candidate.
5. Recently, methods have been proposed to learn another model to select instances that lead to the fewest errors, usually measured on a held-out development set.
6. Reinforcement learning and imitation learning have been utilized to train such policy models (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b).
7. This learning-to-select strategy may have some constraints.
8. First, it requires labeled data (maybe from another domain) to train the policy.
9. To mitigate this reliance, Vu et al. (2019) use the current task model as an imperfect annotator for AL simulations.
10. Moreover, the learning signals may be unstable for complex tasks, as Koshorek et al. (2019) show for semantic tasks.
11. A similar and simpler idea is to select the most erroneous or ambiguous instances with regard to the current task model, which can also be done with another performance-prediction model.
12. Yoo and Kweon (2019) directly train a smaller model to predict the instance losses for CV tasks, which have been also adopted for NLP (Cai et al., 2021;. In a similar spirit,  employ a neural model to judge the correctness of the model prediction for SRL and Brantley et al. (2020) learn a policy to decide whether expert querying is required for each state in sequence labeling.
13. Inspired by data maps (Swayamdipta et al., 2020), Zhang and Plank (2021) train a model to select ambiguous instances whose average correct-ness over the training iterations is close to a predefined threshold.
14. For machine translation (MT), special techniques can be utilized to seek erroneous instances, such as using a backward translator to check round-trip translations (Bachman et al., 2017;Fang et al., 2017;Liu et al., 2018a,b)0."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s8,Density,['p8.0'],"[""With the motivation to avoid outliers, density-based strategies prefer instances that are more representative of the unlabeled set. Selecting by n-gram or word counts (Ambati et al., 2010a;Zhao et al., 2020b) can be regarded as a simple way of density measurement. Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998;. While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., , 2009).""]","With the motivation to avoid outliers, density-based strategies prefer instances that are more representative of the unlabeled set. Selecting by n-gram or word counts (Ambati et al., 2010a;Zhao et al., 2020b) can be regarded as a simple way of density measurement. Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998;. While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., , 2009).","(p8.0) With the motivation to avoid outliers, density-based strategies prefer instances that are more representative of the unlabeled set. Selecting by n-gram or word counts (Ambati et al., 2010a;Zhao et al., 2020b) can be regarded as a simple way of density measurement. Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998;. While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., , 2009).","[['b111', None, 'b107', 'b84']]","[['b111', None, 'b107', 'b84']]",4,"1. With the motivation to avoid outliers, density-based strategies prefer instances that are more representative of the unlabeled set.
2. Selecting by n-gram or word counts (Ambati et al., 2010a;Zhao et al., 2020b) can be regarded as a simple way of density measurement.
3. Generally, the common measurement is an instance's average similarity to all other instances (McCallum and Nigam, 1998;. While it may be costly to calculate similarities of all instance pairs, considering only k-nearest neighbor instances has been proposed as an alternative option (Zhu et al., , 2009)."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s9,Discriminative 3,['p9.0'],"['Another direction is to select instances that are different from already labeled instances. Again, for NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005;Bloodgood and Callison-Burch, 2010;Erdmann et al., 2019). Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006;). Another interesting idea is to train a model to discriminate the labeled and unlabeled sets. Gissin and Shalev-Shwartz (2019) directly train a classifier for this purpose, while naturally adversarial training can be also adopted (Sinha et al., 2019;. In domain adaptation scenarios, the same motivation leads to the usage of a domain separator to filter instances (Rai et al., 2010).']","Another direction is to select instances that are different from already labeled instances. Again, for NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005;Bloodgood and Callison-Burch, 2010;Erdmann et al., 2019). Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006;). Another interesting idea is to train a model to discriminate the labeled and unlabeled sets. Gissin and Shalev-Shwartz (2019) directly train a classifier for this purpose, while naturally adversarial training can be also adopted (Sinha et al., 2019;. In domain adaptation scenarios, the same motivation leads to the usage of a domain separator to filter instances (Rai et al., 2010).","(p9.0) Another direction is to select instances that are different from already labeled instances. Again, for NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005;Bloodgood and Callison-Burch, 2010;Erdmann et al., 2019). Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006;). Another interesting idea is to train a model to discriminate the labeled and unlabeled sets. Gissin and Shalev-Shwartz (2019) directly train a classifier for this purpose, while naturally adversarial training can be also adopted (Sinha et al., 2019;. In domain adaptation scenarios, the same motivation leads to the usage of a domain separator to filter instances (Rai et al., 2010).","[['b34', 'b106', 'b2', 'b91']]","[['b34', 'b106', 'b2', 'b91']]",4,"1. Another direction is to select instances that are different from already labeled instances.
2. Again, for NLP tasks, simple feature-based metrics can be utilized for this purpose by preferring instances with more unseen n-grams or out-of-vocabulary words (Eck et al., 2005;Bloodgood and Callison-Burch, 2010;Erdmann et al., 2019).
3. Generally, similarity scores can also be utilized to select the instances that are less similar to the labeled set (Kim et al., 2006;).
4. Another interesting idea is to train a model to discriminate the labeled and unlabeled sets.
5. Gissin and Shalev-Shwartz (2019) directly train a classifier for this purpose, while naturally adversarial training can be also adopted (Sinha et al., 2019;. In domain adaptation scenarios, the same motivation leads to the usage of a domain separator to filter instances (Rai et al., 2010)."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s10,Batch Diversity,"['p10.0', 'p10.1', 'p10.2']","['Ideally, only one most useful instance would be selected in each iteration. However, it is more efficient and practical to adopt batch-mode AL (Settles, 2009), where each time a batch of instances is selected. In this case, we need to consider the dissimilarities not only between selected instances and labeled ones but also within the selected batch.', 'To select a batch of diverse instances, there are two common approaches. 1) Iterative selection collects the batch in an iterative greedy way (Brinker, 2003;. In each iteration, an instance is selected by comparing it with previously chosen instances to avoid redundancy. Some more advanced diversity-based criteria, like coreset (Geifman and El-Yaniv, 2017;Sener and Savarese, 2018) and determinantal point processes , can also be approximated in a similar way. 2) Clustering-based methods partition the unlabeled data into clusters and select instances among them Xu et al., 2003;Nguyen and Smeulders, 2004;Zhdanov, 2019;Yu et al., 2022). Since the chosen instances come from different clusters, diversity can be achieved to some extent.', 'For the calculation of similarity, in addition to comparing the input features or intermediate neural representations, other methods are also investigated, such as utilizing model-based similarity ), gradients (Ash et al., 2020Kim, 2020), and masked LM surprisal embeddings .']","Ideally, only one most useful instance would be selected in each iteration. However, it is more efficient and practical to adopt batch-mode AL (Settles, 2009), where each time a batch of instances is selected. In this case, we need to consider the dissimilarities not only between selected instances and labeled ones but also within the selected batch.

To select a batch of diverse instances, there are two common approaches. 1) Iterative selection collects the batch in an iterative greedy way (Brinker, 2003;. In each iteration, an instance is selected by comparing it with previously chosen instances to avoid redundancy. Some more advanced diversity-based criteria, like coreset (Geifman and El-Yaniv, 2017;Sener and Savarese, 2018) and determinantal point processes , can also be approximated in a similar way. 2) Clustering-based methods partition the unlabeled data into clusters and select instances among them Xu et al., 2003;Nguyen and Smeulders, 2004;Zhdanov, 2019;Yu et al., 2022). Since the chosen instances come from different clusters, diversity can be achieved to some extent.

For the calculation of similarity, in addition to comparing the input features or intermediate neural representations, other methods are also investigated, such as utilizing model-based similarity ), gradients (Ash et al., 2020Kim, 2020), and masked LM surprisal embeddings .","(p10.0) Ideally, only one most useful instance would be selected in each iteration. However, it is more efficient and practical to adopt batch-mode AL (Settles, 2009), where each time a batch of instances is selected. In this case, we need to consider the dissimilarities not only between selected instances and labeled ones but also within the selected batch.

(p10.1) To select a batch of diverse instances, there are two common approaches. 1) Iterative selection collects the batch in an iterative greedy way (Brinker, 2003;. In each iteration, an instance is selected by comparing it with previously chosen instances to avoid redundancy. Some more advanced diversity-based criteria, like coreset (Geifman and El-Yaniv, 2017;Sener and Savarese, 2018) and determinantal point processes , can also be approximated in a similar way. 2) Clustering-based methods partition the unlabeled data into clusters and select instances among them Xu et al., 2003;Nguyen and Smeulders, 2004;Zhdanov, 2019;Yu et al., 2022). Since the chosen instances come from different clusters, diversity can be achieved to some extent.

(p10.2) For the calculation of similarity, in addition to comparing the input features or intermediate neural representations, other methods are also investigated, such as utilizing model-based similarity ), gradients (Ash et al., 2020Kim, 2020), and masked LM surprisal embeddings .","[[], ['b63', None, 'b60', 'b78', 'b17'], [None, 'b91']]","[[], ['b63', None, 'b60', 'b78', 'b17'], [None, 'b91']]",7,"1. Ideally, only one most useful instance would be selected in each iteration.
2. However, it is more efficient and practical to adopt batch-mode AL (Settles, 2009), where each time a batch of instances is selected.
3. In this case, we need to consider the dissimilarities not only between selected instances and labeled ones but also within the selected batch.
4. To select a batch of diverse instances, there are two common approaches.
5. 1) Iterative selection collects the batch in an iterative greedy way (Brinker, 2003;. In each iteration, an instance is selected by comparing it with previously chosen instances to avoid redundancy. Some more advanced diversity-based criteria, like coreset (Geifman and El-Yaniv, 2017;Sener and Savarese, 2018) and determinantal point processes , can also be approximated in a similar way.
6. 2) Clustering-based methods partition the unlabeled data into clusters and select instances among them Xu et al., 2003;Nguyen and Smeulders, 2004;Zhdanov, 2019;Yu et al., 2022).
7. Since the chosen instances come from different clusters, diversity can be achieved to some extent.
8. For the calculation of similarity, in addition to comparing the input features or intermediate neural representations, other methods are also investigated, such as utilizing model-based similarity ), gradients (Ash et al., 2020Kim, 2020), and masked LM surprisal embeddings ."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s11,Hybrid,"['p11.0', 'p11.1', 'p11.2', 'p11.3']","['There is no surprise that informativeness and representativeness can be combined for instance querying, leading to hybrid strategies. A simple combination can be used to merge multiple criteria into one. This can be achieved by a weighted sum (Kim et al., 2006;Chen et al., 2011) or multiplication .', 'There are several strategies to naturally integrate multiple criteria. Examples include (uncertainty) weighted clustering (Zhdanov, 2019), diverse gradient selection (Ash et al., 2020;Kim, 2020) where the gradients themselves contain uncertainty information ( §2.1.3) and determinantal point processes (DPP) with quality-diversity decomposition .', 'Moreover, multi-step querying, which applies multiple criteria in series, is another natural hybrid method. For example, one can consider first filtering certain highly uncertain instances and then performing clustering to select a diverse batch from them (Xu et al., 2003;Mirroshandel et al., 2011). An alternative strategy of selecting the most uncertain instances per cluster has also been utilized .', 'Instead of statically merging into one query strategy, dynamic combination may better fit the AL learning process, since different strategies may excel at different AL phases. For example, at the start of AL, uncertainty sampling may be unreliable due to little labeled data, and representativenessbased methods could be preferable, whereas in later stages where we have enough data and target finergrained decision boundaries, uncertainty may be a suitable strategy. DUAL (Donmez et al., 2007) is such a dynamic strategy that can switch from a density-based selector to an uncertainty-based one. Ambati et al. (2011b) further propose GraDUAL, which gradually switches strategies within a switching range. Wu et al. (2017) adopt a similar idea with a pre-defined monotonic function to control the combination weights.']","There is no surprise that informativeness and representativeness can be combined for instance querying, leading to hybrid strategies. A simple combination can be used to merge multiple criteria into one. This can be achieved by a weighted sum (Kim et al., 2006;Chen et al., 2011) or multiplication .

There are several strategies to naturally integrate multiple criteria. Examples include (uncertainty) weighted clustering (Zhdanov, 2019), diverse gradient selection (Ash et al., 2020;Kim, 2020) where the gradients themselves contain uncertainty information ( §2.1.3) and determinantal point processes (DPP) with quality-diversity decomposition .

Moreover, multi-step querying, which applies multiple criteria in series, is another natural hybrid method. For example, one can consider first filtering certain highly uncertain instances and then performing clustering to select a diverse batch from them (Xu et al., 2003;Mirroshandel et al., 2011). An alternative strategy of selecting the most uncertain instances per cluster has also been utilized .

Instead of statically merging into one query strategy, dynamic combination may better fit the AL learning process, since different strategies may excel at different AL phases. For example, at the start of AL, uncertainty sampling may be unreliable due to little labeled data, and representativenessbased methods could be preferable, whereas in later stages where we have enough data and target finergrained decision boundaries, uncertainty may be a suitable strategy. DUAL (Donmez et al., 2007) is such a dynamic strategy that can switch from a density-based selector to an uncertainty-based one. Ambati et al. (2011b) further propose GraDUAL, which gradually switches strategies within a switching range. Wu et al. (2017) adopt a similar idea with a pre-defined monotonic function to control the combination weights.","(p11.0) There is no surprise that informativeness and representativeness can be combined for instance querying, leading to hybrid strategies. A simple combination can be used to merge multiple criteria into one. This can be achieved by a weighted sum (Kim et al., 2006;Chen et al., 2011) or multiplication .

(p11.1) There are several strategies to naturally integrate multiple criteria. Examples include (uncertainty) weighted clustering (Zhdanov, 2019), diverse gradient selection (Ash et al., 2020;Kim, 2020) where the gradients themselves contain uncertainty information ( §2.1.3) and determinantal point processes (DPP) with quality-diversity decomposition .

(p11.2) Moreover, multi-step querying, which applies multiple criteria in series, is another natural hybrid method. For example, one can consider first filtering certain highly uncertain instances and then performing clustering to select a diverse batch from them (Xu et al., 2003;Mirroshandel et al., 2011). An alternative strategy of selecting the most uncertain instances per cluster has also been utilized .

(p11.3) Instead of statically merging into one query strategy, dynamic combination may better fit the AL learning process, since different strategies may excel at different AL phases. For example, at the start of AL, uncertainty sampling may be unreliable due to little labeled data, and representativenessbased methods could be preferable, whereas in later stages where we have enough data and target finergrained decision boundaries, uncertainty may be a suitable strategy. DUAL (Donmez et al., 2007) is such a dynamic strategy that can switch from a density-based selector to an uncertainty-based one. Ambati et al. (2011b) further propose GraDUAL, which gradually switches strategies within a switching range. Wu et al. (2017) adopt a similar idea with a pre-defined monotonic function to control the combination weights.","[['b91'], [None, 'b91', 'b78'], ['b60', 'b102'], [None, 'b57']]","[['b91'], [None, 'b91', 'b78'], ['b60', 'b102'], [None, 'b57']]",8,"1. There is no surprise that informativeness and representativeness can be combined for instance querying, leading to hybrid strategies.
2. A simple combination can be used to merge multiple criteria into one.
3. This can be achieved by a weighted sum (Kim et al., 2006;Chen et al., 2011) or multiplication .
4. There are several strategies to naturally integrate multiple criteria.
5. Examples include (uncertainty) weighted clustering (Zhdanov, 2019), diverse gradient selection (Ash et al., 2020;Kim, 2020) where the gradients themselves contain uncertainty information ( §2.1.3) and determinantal point processes (DPP) with quality-diversity decomposition .
6. Moreover, multi-step querying, which applies multiple criteria in series, is another natural hybrid method.
7. For example, one can consider first filtering certain highly uncertain instances and then performing clustering to select a diverse batch from them (Xu et al., 2003;Mirroshandel et al., 2011).
8. An alternative strategy of selecting the most uncertain instances per cluster has also been utilized .
9. Instead of statically merging into one query strategy, dynamic combination may better fit the AL learning process, since different strategies may excel at different AL phases.
10. For example, at the start of AL, uncertainty sampling may be unreliable due to little labeled data, and representativenessbased methods could be preferable, whereas in later stages where we have enough data and target finergrained decision boundaries, uncertainty may be a suitable strategy.
11. DUAL (Donmez et al., 2007) is such a dynamic strategy that can switch from a density-based selector to an uncertainty-based one.
12. Ambati et al. (2011b) further propose GraDUAL, which gradually switches strategies within a switching range.
13. Wu et al. (2017) adopt a similar idea with a pre-defined monotonic function to control the combination weights."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s14,Full-structure AL,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4']","['First, if we regard the full output structure of an instance as a whole and perform query and annotation at the full-instance level, then AL for structured prediction tasks is not very different than for simpler classification tasks. Nevertheless, considering that the output space is usually exponentially large and infeasible to explicitly enumerate, querying may require further inspection.', 'Some uncertainty sampling strategies, such as entropy, need to consider the full output space. Instead of the infeasible explicit enumeration, dynamic-programming algorithms that are similar to the ones in decoding and inference processes can be utilized, such as algorithms for tree-entropy (Hwa, 2000(Hwa, , 2004 and sequence-entropy (Mann and McCallum, 2007;. Instead of considering the full output space, topk approximation is a simpler alternative that takes k-best predicted structures as a proxy. This is also a frequently utilized method Kim et al., 2006;Rocha and Sanchez, 2013).', 'For disagreement-based strategies, the measurement of partial disagreement may be required, since full-match can be too strict for structured objects. Fine-grained evaluation scores can be reasonable choices for this purpose, such as F1 score for sequence labeling (Ngai and Yarowsky, 2000).', 'Since longer instances usually have larger uncertainties and might be preferred, length normalization is a commonly-used heuristic to avoid this bias Hwa, 2000Hwa, , 2004). Yet,  argue that longer sequences should not be discouraged and may contain more information.', 'Instead of directly specifying the full utility of an instance, aggregation is also often utilized by gathering utilities of its sub-structures, usually along the factorization of the structured modeling. For example, the sequence uncertainty can be obtained by summing or averaging the uncertainties of all the tokens . Other aggregation methods are also applicable, such as weighted sum by word frequency  or using only the most uncertain (least probable) one (Myers and Palmer, 2021;Liu et al., 2022).']","First, if we regard the full output structure of an instance as a whole and perform query and annotation at the full-instance level, then AL for structured prediction tasks is not very different than for simpler classification tasks. Nevertheless, considering that the output space is usually exponentially large and infeasible to explicitly enumerate, querying may require further inspection.

Some uncertainty sampling strategies, such as entropy, need to consider the full output space. Instead of the infeasible explicit enumeration, dynamic-programming algorithms that are similar to the ones in decoding and inference processes can be utilized, such as algorithms for tree-entropy (Hwa, 2000(Hwa, , 2004 and sequence-entropy (Mann and McCallum, 2007;. Instead of considering the full output space, topk approximation is a simpler alternative that takes k-best predicted structures as a proxy. This is also a frequently utilized method Kim et al., 2006;Rocha and Sanchez, 2013).

For disagreement-based strategies, the measurement of partial disagreement may be required, since full-match can be too strict for structured objects. Fine-grained evaluation scores can be reasonable choices for this purpose, such as F1 score for sequence labeling (Ngai and Yarowsky, 2000).

Since longer instances usually have larger uncertainties and might be preferred, length normalization is a commonly-used heuristic to avoid this bias Hwa, 2000Hwa, , 2004). Yet,  argue that longer sequences should not be discouraged and may contain more information.

Instead of directly specifying the full utility of an instance, aggregation is also often utilized by gathering utilities of its sub-structures, usually along the factorization of the structured modeling. For example, the sequence uncertainty can be obtained by summing or averaging the uncertainties of all the tokens . Other aggregation methods are also applicable, such as weighted sum by word frequency  or using only the most uncertain (least probable) one (Myers and Palmer, 2021;Liu et al., 2022).","(p14.0) First, if we regard the full output structure of an instance as a whole and perform query and annotation at the full-instance level, then AL for structured prediction tasks is not very different than for simpler classification tasks. Nevertheless, considering that the output space is usually exponentially large and infeasible to explicitly enumerate, querying may require further inspection.

(p14.1) Some uncertainty sampling strategies, such as entropy, need to consider the full output space. Instead of the infeasible explicit enumeration, dynamic-programming algorithms that are similar to the ones in decoding and inference processes can be utilized, such as algorithms for tree-entropy (Hwa, 2000(Hwa, , 2004 and sequence-entropy (Mann and McCallum, 2007;. Instead of considering the full output space, topk approximation is a simpler alternative that takes k-best predicted structures as a proxy. This is also a frequently utilized method Kim et al., 2006;Rocha and Sanchez, 2013).

(p14.2) For disagreement-based strategies, the measurement of partial disagreement may be required, since full-match can be too strict for structured objects. Fine-grained evaluation scores can be reasonable choices for this purpose, such as F1 score for sequence labeling (Ngai and Yarowsky, 2000).

(p14.3) Since longer instances usually have larger uncertainties and might be preferred, length normalization is a commonly-used heuristic to avoid this bias Hwa, 2000Hwa, , 2004). Yet,  argue that longer sequences should not be discouraged and may contain more information.

(p14.4) Instead of directly specifying the full utility of an instance, aggregation is also often utilized by gathering utilities of its sub-structures, usually along the factorization of the structured modeling. For example, the sequence uncertainty can be obtained by summing or averaging the uncertainties of all the tokens . Other aggregation methods are also applicable, such as weighted sum by word frequency  or using only the most uncertain (least probable) one (Myers and Palmer, 2021;Liu et al., 2022).","[[], [None, 'b104', 'b91'], [None], [None], ['b99', None]]","[[], [None, 'b104', 'b91'], [None], [None], ['b99', None]]",7,"1. First, if we regard the full output structure of an instance as a whole and perform query and annotation at the full-instance level, then AL for structured prediction tasks is not very different than for simpler classification tasks.
2. Nevertheless, considering that the output space is usually exponentially large and infeasible to explicitly enumerate, querying may require further inspection.
3. Some uncertainty sampling strategies, such as entropy, need to consider the full output space.
4. Instead of the infeasible explicit enumeration, dynamic-programming algorithms that are similar to the ones in decoding and inference processes can be utilized, such as algorithms for tree-entropy (Hwa, 2000(Hwa, , 2004 and sequence-entropy (Mann and McCallum, 2007;. Instead of considering the full output space, topk approximation is a simpler alternative that takes k-best predicted structures as a proxy. This is also a frequently utilized method Kim et al., 2006;Rocha and Sanchez, 2013).
5. For disagreement-based strategies, the measurement of partial disagreement may be required, since full-match can be too strict for structured objects.
6. Fine-grained evaluation scores can be reasonable choices for this purpose, such as F1 score for sequence labeling (Ngai and Yarowsky, 2000).
7. Since longer instances usually have larger uncertainties and might be preferred, length normalization is a commonly-used heuristic to avoid this bias Hwa, 2000Hwa, , 2004).
8. Yet,  argue that longer sequences should not be discouraged and may contain more information.
9. Instead of directly specifying the full utility of an instance, aggregation is also often utilized by gathering utilities of its sub-structures, usually along the factorization of the structured modeling.
10. For example, the sequence uncertainty can be obtained by summing or averaging the uncertainties of all the tokens .
11. Other aggregation methods are also applicable, such as weighted sum by word frequency  or using only the most uncertain (least probable) one (Myers and Palmer, 2021;Liu et al., 2022)."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s15,Partial-structure AL,"['p15.0', 'p15.1', 'p15.2', 'p15.3', 'p15.4']","['A structured object can be decomposed into smaller sub-structures with different training utilities. For example, in a dependency tree, functional relations are usually easier to judge while prepositional attachment links may be more informative for the learning purpose. This naturally leads to AL with partial structures, where querying and annotating can be performed at the sub-structure level.', 'Factorizing full structures into the finestgrained sub-structures and regarding them as the annotation units could be a natural choice. Typical examples include individual tokens for sequence labeling , word boundaries for segmentation (Neubig et al., 2011;Li et al., 2012b), syntactic-unit pairs for dependency parsing (Sassano and Kurohashi, 2010) and mention pairs for coreference (Gasperin, 2009;Miller et al., 2012;Sachan et al., 2015). The querying strategy for the sub-structures can be similar to the classification cases, though inferences are usually needed to calculate marginal probabilities. Moreover, if full structures are desired as annotation outputs, semi-supervised techniques such as self-training ( §4.2) could be utilized to assign pseudo labels to the unannotated parts Majidi and Crane, 2013).', 'At many times, choosing larger sub-structures is preferable, since partial annotation still needs the understanding of larger contexts and frequently jumping among different contexts may require more reading time ( §3.2.1). Moreover, increasing the sampling granularity may mitigate the missed class effect, where certain classes may be overlooked . Typical examples of larger sub-structures include sub-sequences for sequence labeling Chaudhary et al., 2019;, word-wise head edges for dependency parsing (Flannery and Mori, 2015;Li et al., 2016), neighborhood pools (Laws et al., 2012) or mention-wise anaphoric links Espeland et al., 2020) for coreference, and phrases for MT (Bloodgood and Callison-Burch, 2010;Miura et al., 2016;Hu and Neubig, 2021). In addition to increasing granularity, grouping queries can also help to make annotation easier, such as adopting a two-stage selection of choosing uncertain tokens from uncertain sentences (Mirroshandel and Nasr, 2011;Flannery and Mori, 2015) and selecting nearby instances in a row (Miller et al., 2012).', 'For AL with partial structures, output modeling is of particular interest since the model needs to learn from partial annotations. If directly using local discriminative models where each substructure is decided independently, learning with partial annotations is straightforward since the annotations are already complete to the models (Neubig et al., 2011;Flannery and Mori, 2015). For more complex models that consider interactions among output sub-structures, such as global models, special algorithms are required to learn from incomplete annotations (Scheffer et al., 2001;Wanvarie et al., 2011;Li et al., 2016). One advantage of these more complex models is the interaction of the partial labels and the remaining parts. For example, considering the output constraints for structured prediction tasks, combining the annotated parts and the constraints may reduce the output space of other parts and thus lower their uncertainties, leading to better queries (Roth and Small, 2006;Sassano and Kurohashi, 2010;Mirroshandel and Nasr, 2011). More generally, the annotation of one label can intermediately influence others with cheap re-inference, which can help batch-mode selection (Marcheggiani and Artières, 2014) and interactive correction (Culotta and McCallum, 2005).', 'In addition to classical structured-prediction tasks, classification tasks can also be cast as structured predictions with partial labeling. Partial feedback is an example that is adopted to make the annotating of classification tasks simpler, especially when there are a large number of target labels. For example, annotators may find it much easier to answer yes/no questions (Hu et al., 2019) or rule out negative classes (Lippincott and Van Durme, 2021) than to identify the correct one.']","A structured object can be decomposed into smaller sub-structures with different training utilities. For example, in a dependency tree, functional relations are usually easier to judge while prepositional attachment links may be more informative for the learning purpose. This naturally leads to AL with partial structures, where querying and annotating can be performed at the sub-structure level.

Factorizing full structures into the finestgrained sub-structures and regarding them as the annotation units could be a natural choice. Typical examples include individual tokens for sequence labeling , word boundaries for segmentation (Neubig et al., 2011;Li et al., 2012b), syntactic-unit pairs for dependency parsing (Sassano and Kurohashi, 2010) and mention pairs for coreference (Gasperin, 2009;Miller et al., 2012;Sachan et al., 2015). The querying strategy for the sub-structures can be similar to the classification cases, though inferences are usually needed to calculate marginal probabilities. Moreover, if full structures are desired as annotation outputs, semi-supervised techniques such as self-training ( §4.2) could be utilized to assign pseudo labels to the unannotated parts Majidi and Crane, 2013).

At many times, choosing larger sub-structures is preferable, since partial annotation still needs the understanding of larger contexts and frequently jumping among different contexts may require more reading time ( §3.2.1). Moreover, increasing the sampling granularity may mitigate the missed class effect, where certain classes may be overlooked . Typical examples of larger sub-structures include sub-sequences for sequence labeling Chaudhary et al., 2019;, word-wise head edges for dependency parsing (Flannery and Mori, 2015;Li et al., 2016), neighborhood pools (Laws et al., 2012) or mention-wise anaphoric links Espeland et al., 2020) for coreference, and phrases for MT (Bloodgood and Callison-Burch, 2010;Miura et al., 2016;Hu and Neubig, 2021). In addition to increasing granularity, grouping queries can also help to make annotation easier, such as adopting a two-stage selection of choosing uncertain tokens from uncertain sentences (Mirroshandel and Nasr, 2011;Flannery and Mori, 2015) and selecting nearby instances in a row (Miller et al., 2012).

For AL with partial structures, output modeling is of particular interest since the model needs to learn from partial annotations. If directly using local discriminative models where each substructure is decided independently, learning with partial annotations is straightforward since the annotations are already complete to the models (Neubig et al., 2011;Flannery and Mori, 2015). For more complex models that consider interactions among output sub-structures, such as global models, special algorithms are required to learn from incomplete annotations (Scheffer et al., 2001;Wanvarie et al., 2011;Li et al., 2016). One advantage of these more complex models is the interaction of the partial labels and the remaining parts. For example, considering the output constraints for structured prediction tasks, combining the annotated parts and the constraints may reduce the output space of other parts and thus lower their uncertainties, leading to better queries (Roth and Small, 2006;Sassano and Kurohashi, 2010;Mirroshandel and Nasr, 2011). More generally, the annotation of one label can intermediately influence others with cheap re-inference, which can help batch-mode selection (Marcheggiani and Artières, 2014) and interactive correction (Culotta and McCallum, 2005).

In addition to classical structured-prediction tasks, classification tasks can also be cast as structured predictions with partial labeling. Partial feedback is an example that is adopted to make the annotating of classification tasks simpler, especially when there are a large number of target labels. For example, annotators may find it much easier to answer yes/no questions (Hu et al., 2019) or rule out negative classes (Lippincott and Van Durme, 2021) than to identify the correct one.","(p15.0) A structured object can be decomposed into smaller sub-structures with different training utilities. For example, in a dependency tree, functional relations are usually easier to judge while prepositional attachment links may be more informative for the learning purpose. This naturally leads to AL with partial structures, where querying and annotating can be performed at the sub-structure level.

(p15.1) Factorizing full structures into the finestgrained sub-structures and regarding them as the annotation units could be a natural choice. Typical examples include individual tokens for sequence labeling , word boundaries for segmentation (Neubig et al., 2011;Li et al., 2012b), syntactic-unit pairs for dependency parsing (Sassano and Kurohashi, 2010) and mention pairs for coreference (Gasperin, 2009;Miller et al., 2012;Sachan et al., 2015). The querying strategy for the sub-structures can be similar to the classification cases, though inferences are usually needed to calculate marginal probabilities. Moreover, if full structures are desired as annotation outputs, semi-supervised techniques such as self-training ( §4.2) could be utilized to assign pseudo labels to the unannotated parts Majidi and Crane, 2013).

(p15.2) At many times, choosing larger sub-structures is preferable, since partial annotation still needs the understanding of larger contexts and frequently jumping among different contexts may require more reading time ( §3.2.1). Moreover, increasing the sampling granularity may mitigate the missed class effect, where certain classes may be overlooked . Typical examples of larger sub-structures include sub-sequences for sequence labeling Chaudhary et al., 2019;, word-wise head edges for dependency parsing (Flannery and Mori, 2015;Li et al., 2016), neighborhood pools (Laws et al., 2012) or mention-wise anaphoric links Espeland et al., 2020) for coreference, and phrases for MT (Bloodgood and Callison-Burch, 2010;Miura et al., 2016;Hu and Neubig, 2021). In addition to increasing granularity, grouping queries can also help to make annotation easier, such as adopting a two-stage selection of choosing uncertain tokens from uncertain sentences (Mirroshandel and Nasr, 2011;Flannery and Mori, 2015) and selecting nearby instances in a row (Miller et al., 2012).

(p15.3) For AL with partial structures, output modeling is of particular interest since the model needs to learn from partial annotations. If directly using local discriminative models where each substructure is decided independently, learning with partial annotations is straightforward since the annotations are already complete to the models (Neubig et al., 2011;Flannery and Mori, 2015). For more complex models that consider interactions among output sub-structures, such as global models, special algorithms are required to learn from incomplete annotations (Scheffer et al., 2001;Wanvarie et al., 2011;Li et al., 2016). One advantage of these more complex models is the interaction of the partial labels and the remaining parts. For example, considering the output constraints for structured prediction tasks, combining the annotated parts and the constraints may reduce the output space of other parts and thus lower their uncertainties, leading to better queries (Roth and Small, 2006;Sassano and Kurohashi, 2010;Mirroshandel and Nasr, 2011). More generally, the annotation of one label can intermediately influence others with cheap re-inference, which can help batch-mode selection (Marcheggiani and Artières, 2014) and interactive correction (Culotta and McCallum, 2005).

(p15.4) In addition to classical structured-prediction tasks, classification tasks can also be cast as structured predictions with partial labeling. Partial feedback is an example that is adopted to make the annotating of classification tasks simpler, especially when there are a large number of target labels. For example, annotators may find it much easier to answer yes/no questions (Hu et al., 2019) or rule out negative classes (Lippincott and Van Durme, 2021) than to identify the correct one.","[[], ['b101', None, 'b100', 'b91'], ['b101', 'b95', 'b91', None, 'b106', 'b109', 'b100'], ['b95', 'b56', 'b91', None, 'b10'], []]","[[], ['b101', None, 'b100', 'b91'], ['b101', 'b95', 'b91', None, 'b106', 'b109', 'b100'], ['b95', 'b56', 'b91', None, 'b10'], []]",16,"1. A structured object can be decomposed into smaller sub-structures with different training utilities.
2. For example, in a dependency tree, functional relations are usually easier to judge while prepositional attachment links may be more informative for the learning purpose.
3. This naturally leads to AL with partial structures, where querying and annotating can be performed at the sub-structure level.
4. Factorizing full structures into the finestgrained sub-structures and regarding them as the annotation units could be a natural choice.
5. Typical examples include individual tokens for sequence labeling , word boundaries for segmentation (Neubig et al., 2011;Li et al., 2012b), syntactic-unit pairs for dependency parsing (Sassano and Kurohashi, 2010) and mention pairs for coreference (Gasperin, 2009;Miller et al., 2012;Sachan et al., 2015).
6. The querying strategy for the sub-structures can be similar to the classification cases, though inferences are usually needed to calculate marginal probabilities.
7. Moreover, if full structures are desired as annotation outputs, semi-supervised techniques such as self-training ( §4.2) could be utilized to assign pseudo labels to the unannotated parts Majidi and Crane, 2013).
8. At many times, choosing larger sub-structures is preferable, since partial annotation still needs the understanding of larger contexts and frequently jumping among different contexts may require more reading time ( §3.2.1).
9. Moreover, increasing the sampling granularity may mitigate the missed class effect, where certain classes may be overlooked .
10. Typical examples of larger sub-structures include sub-sequences for sequence labeling Chaudhary et al., 2019;, word-wise head edges for dependency parsing (Flannery and Mori, 2015;Li et al., 2016), neighborhood pools (Laws et al., 2012) or mention-wise anaphoric links Espeland et al., 2020) for coreference, and phrases for MT (Bloodgood and Callison-Burch, 2010;Miura et al., 2016;Hu and Neubig, 2021).
11. In addition to increasing granularity, grouping queries can also help to make annotation easier, such as adopting a two-stage selection of choosing uncertain tokens from uncertain sentences (Mirroshandel and Nasr, 2011;Flannery and Mori, 2015) and selecting nearby instances in a row (Miller et al., 2012).
12. For AL with partial structures, output modeling is of particular interest since the model needs to learn from partial annotations.
13. If directly using local discriminative models where each substructure is decided independently, learning with partial annotations is straightforward since the annotations are already complete to the models (Sassano and Kurohashi, 2010)0.
14. For more complex models that consider interactions among output sub-structures, such as global models, special algorithms are required to learn from incomplete annotations (Sassano and Kurohashi, 2010)1.
15. One advantage of these more complex models is the interaction of the partial labels and the remaining parts.
16. For example, considering the output constraints for structured prediction tasks, combining the annotated parts and the constraints may reduce the output space of other parts and thus lower their uncertainties, leading to better queries (Sassano and Kurohashi, 2010)2.
17. More generally, the annotation of one label can intermediately influence others with cheap re-inference, which can help batch-mode selection (Sassano and Kurohashi, 2010)3 and interactive correction (Sassano and Kurohashi, 2010)4.
18. In addition to classical structured-prediction tasks, classification tasks can also be cast as structured predictions with partial labeling.
19. Partial feedback is an example that is adopted to make the annotating of classification tasks simpler, especially when there are a large number of target labels.
20. For example, annotators may find it much easier to answer yes/no questions (Sassano and Kurohashi, 2010)5 or rule out negative classes (Sassano and Kurohashi, 2010)6 than to identify the correct one."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s17,Cost Measurement,"['p17.0', 'p17.1', 'p17.2']","['Most AL works adopt simple measurements of unit cost, that is, assuming that annotating each instance requires the same cost. Nevertheless, the annotation efforts for different instances may vary . For example, longer sentences may cost more to annotate than shorter ones. Because of this, many works assume unit costs to tokens instead of sequences, which may still be inaccurate. Especially, AL tends to select difficult and ambiguous instances, which may require more annotation efforts (Hachey et al., 2005;Lynn et al., 2012). It is important to properly measure annotation cost since the measurement directly affects the evaluation of AL algorithms. The comparisons of query strategies may vary if adopting different cost measurement (Haertel et al., 2008a;Bloodgood and Callison-Burch, 2010;Chen et al., 2015).', 'Probably the best cost measurement is the actual annotation time (Baldridge and Palmer, 2009). Especially, when the cost comparisons are not that straightforward, such as comparing annotating data against writing rules (Ngai and Yarowsky, 2000) or partial against full annotations ( §3.1; Flannery and Mori, 2015;Li et al., 2016, time-based evaluation is an ideal choice. This requires actual annotating exercises rather than simulations.', 'Since cost measurement can also be used for querying ( §3.2.2), it would be helpful to be able to predict the real cost before annotating. This can be cast as a regression problem, for which several works learn a linear cost model based on input features Ringger et al., 2008;Haertel et al., 2008a;Arora et al., 2009).']","Most AL works adopt simple measurements of unit cost, that is, assuming that annotating each instance requires the same cost. Nevertheless, the annotation efforts for different instances may vary . For example, longer sentences may cost more to annotate than shorter ones. Because of this, many works assume unit costs to tokens instead of sequences, which may still be inaccurate. Especially, AL tends to select difficult and ambiguous instances, which may require more annotation efforts (Hachey et al., 2005;Lynn et al., 2012). It is important to properly measure annotation cost since the measurement directly affects the evaluation of AL algorithms. The comparisons of query strategies may vary if adopting different cost measurement (Haertel et al., 2008a;Bloodgood and Callison-Burch, 2010;Chen et al., 2015).

Probably the best cost measurement is the actual annotation time (Baldridge and Palmer, 2009). Especially, when the cost comparisons are not that straightforward, such as comparing annotating data against writing rules (Ngai and Yarowsky, 2000) or partial against full annotations ( §3.1; Flannery and Mori, 2015;Li et al., 2016, time-based evaluation is an ideal choice. This requires actual annotating exercises rather than simulations.

Since cost measurement can also be used for querying ( §3.2.2), it would be helpful to be able to predict the real cost before annotating. This can be cast as a regression problem, for which several works learn a linear cost model based on input features Ringger et al., 2008;Haertel et al., 2008a;Arora et al., 2009).","(p17.0) Most AL works adopt simple measurements of unit cost, that is, assuming that annotating each instance requires the same cost. Nevertheless, the annotation efforts for different instances may vary . For example, longer sentences may cost more to annotate than shorter ones. Because of this, many works assume unit costs to tokens instead of sequences, which may still be inaccurate. Especially, AL tends to select difficult and ambiguous instances, which may require more annotation efforts (Hachey et al., 2005;Lynn et al., 2012). It is important to properly measure annotation cost since the measurement directly affects the evaluation of AL algorithms. The comparisons of query strategies may vary if adopting different cost measurement (Haertel et al., 2008a;Bloodgood and Callison-Burch, 2010;Chen et al., 2015).

(p17.1) Probably the best cost measurement is the actual annotation time (Baldridge and Palmer, 2009). Especially, when the cost comparisons are not that straightforward, such as comparing annotating data against writing rules (Ngai and Yarowsky, 2000) or partial against full annotations ( §3.1; Flannery and Mori, 2015;Li et al., 2016, time-based evaluation is an ideal choice. This requires actual annotating exercises rather than simulations.

(p17.2) Since cost measurement can also be used for querying ( §3.2.2), it would be helpful to be able to predict the real cost before annotating. This can be cast as a regression problem, for which several works learn a linear cost model based on input features Ringger et al., 2008;Haertel et al., 2008a;Arora et al., 2009).","[[None, 'b106', 'b91', 'b89'], [None, 'b95'], ['b8', 'b89', None]]","[[None, 'b106', 'b91', 'b89'], [None, 'b95'], ['b8', 'b89', None]]",9,"1. Most AL works adopt simple measurements of unit cost, that is, assuming that annotating each instance requires the same cost.
2. Nevertheless, the annotation efforts for different instances may vary .
3. For example, longer sentences may cost more to annotate than shorter ones.
4. Because of this, many works assume unit costs to tokens instead of sequences, which may still be inaccurate.
5. Especially, AL tends to select difficult and ambiguous instances, which may require more annotation efforts (Hachey et al., 2005;Lynn et al., 2012).
6. It is important to properly measure annotation cost since the measurement directly affects the evaluation of AL algorithms.
7. The comparisons of query strategies may vary if adopting different cost measurement (Haertel et al., 2008a;Bloodgood and Callison-Burch, 2010;Chen et al., 2015).
8. Probably the best cost measurement is the actual annotation time (Baldridge and Palmer, 2009).
9. Especially, when the cost comparisons are not that straightforward, such as comparing annotating data against writing rules (Ngai and Yarowsky, 2000) or partial against full annotations ( §3.1; Flannery and Mori, 2015;Li et al., 2016, time-based evaluation is an ideal choice.
10. This requires actual annotating exercises rather than simulations.
11. Since cost measurement can also be used for querying ( §3.2.2), it would be helpful to be able to predict the real cost before annotating.
12. This can be cast as a regression problem, for which several works learn a linear cost model based on input features Ringger et al., 2008;Haertel et al., 2008a;Arora et al., 2009)."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s19,Directly Reducing Cost,"['p19.0', 'p19.1', 'p19.2']","['In addition to better query strategies, there are other ways of directly reducing annotation cost, such as computer-assisted annotation. In AL, models and annotators usually interact in an indirect way where models only query the instances to present to the annotators, while there could be closer interactions.', ""Pre-annotation is such an idea, where not only the raw data instances but also the model's best or top-k predictions are sent to the annotators to help them make decisions. If the model's predictions are reasonable, the annotators can simply select or make a few corrections to obtain the gold annotations rather than creating from scratch. This method has been shown effective when combined with AL (Baldridge and Osborne, 2004;Vlachos, 2006;Ringger et al., 2008;Skeppstedt, 2013;Cañizares-Díaz et al., 2021). Post-editing for MT is also a typical example (Dara et al., 2014)."", ""Moreover, the models could provide help at real annotating time. For example, Culotta and Mc-Callum (2005) present an interactive AL system where the user's corrections can propagate to the model, which generates new predictions for the user to further refine. Interactive machine translation (IMT) adopts a similar idea, where the annotator corrects the first erroneous character, based on which the model reproduces the prediction. AL has also been combined with IMT to further reduce manual efforts (González-Rubio et al., 2012;Peris and Casacuberta, 2018;.""]","In addition to better query strategies, there are other ways of directly reducing annotation cost, such as computer-assisted annotation. In AL, models and annotators usually interact in an indirect way where models only query the instances to present to the annotators, while there could be closer interactions.

Pre-annotation is such an idea, where not only the raw data instances but also the model's best or top-k predictions are sent to the annotators to help them make decisions. If the model's predictions are reasonable, the annotators can simply select or make a few corrections to obtain the gold annotations rather than creating from scratch. This method has been shown effective when combined with AL (Baldridge and Osborne, 2004;Vlachos, 2006;Ringger et al., 2008;Skeppstedt, 2013;Cañizares-Díaz et al., 2021). Post-editing for MT is also a typical example (Dara et al., 2014).

Moreover, the models could provide help at real annotating time. For example, Culotta and Mc-Callum (2005) present an interactive AL system where the user's corrections can propagate to the model, which generates new predictions for the user to further refine. Interactive machine translation (IMT) adopts a similar idea, where the annotator corrects the first erroneous character, based on which the model reproduces the prediction. AL has also been combined with IMT to further reduce manual efforts (González-Rubio et al., 2012;Peris and Casacuberta, 2018;.","(p19.0) In addition to better query strategies, there are other ways of directly reducing annotation cost, such as computer-assisted annotation. In AL, models and annotators usually interact in an indirect way where models only query the instances to present to the annotators, while there could be closer interactions.

(p19.1) Pre-annotation is such an idea, where not only the raw data instances but also the model's best or top-k predictions are sent to the annotators to help them make decisions. If the model's predictions are reasonable, the annotators can simply select or make a few corrections to obtain the gold annotations rather than creating from scratch. This method has been shown effective when combined with AL (Baldridge and Osborne, 2004;Vlachos, 2006;Ringger et al., 2008;Skeppstedt, 2013;Cañizares-Díaz et al., 2021). Post-editing for MT is also a typical example (Dara et al., 2014).

(p19.2) Moreover, the models could provide help at real annotating time. For example, Culotta and Mc-Callum (2005) present an interactive AL system where the user's corrections can propagate to the model, which generates new predictions for the user to further refine. Interactive machine translation (IMT) adopts a similar idea, where the annotator corrects the first erroneous character, based on which the model reproduces the prediction. AL has also been combined with IMT to further reduce manual efforts (González-Rubio et al., 2012;Peris and Casacuberta, 2018;.","[[], ['b102', None, 'b94', 'b35', 'b8', 'b51'], [None, 'b107']]","[[], ['b102', None, 'b94', 'b35', 'b8', 'b51'], [None, 'b107']]",8,"1. In addition to better query strategies, there are other ways of directly reducing annotation cost, such as computer-assisted annotation.
2. In AL, models and annotators usually interact in an indirect way where models only query the instances to present to the annotators, while there could be closer interactions.
3. Pre-annotation is such an idea, where not only the raw data instances but also the model's best or top-k predictions are sent to the annotators to help them make decisions.
4. If the model's predictions are reasonable, the annotators can simply select or make a few corrections to obtain the gold annotations rather than creating from scratch.
5. This method has been shown effective when combined with AL (Baldridge and Osborne, 2004;Vlachos, 2006;Ringger et al., 2008;Skeppstedt, 2013;Cañizares-Díaz et al., 2021).
6. Post-editing for MT is also a typical example (Dara et al., 2014).
7. Moreover, the models could provide help at real annotating time.
8. For example, Culotta and Mc-Callum (2005) present an interactive AL system where the user's corrections can propagate to the model, which generates new predictions for the user to further refine.
9. Interactive machine translation (IMT) adopts a similar idea, where the annotator corrects the first erroneous character, based on which the model reproduces the prediction.
10. AL has also been combined with IMT to further reduce manual efforts (González-Rubio et al., 2012;Peris and Casacuberta, 2018;."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s20,Wait Time,"['p20.0', 'p20.1', 'p20.2', 'p20.3']","['In AL iterations, the annotators may need to wait for the training and querying steps (Line 3 and 4 in Algorithm 1). This wait time may bring some hidden costs, thus more efficient querying and training would be preferable for faster turnarounds.', 'To speed up querying, sub-sampling is a simple method to deal with large unlabeled pools (Roy and McCallum, 2001;Ertekin et al., 2007;Tsvigun et al., 2022). For some querying strategies, precalculating and caching unchanging information can also help to speed up (Ashrafi Asli et al., 2020;Citovsky et al., 2021). In addition, approximation with k-nearest neighbours can also be utilized to calculate density (Zhu et al., 2009) or search for instances after adversarial attacks (Ru et al., 2020).', 'To reduce training time, a seemingly reasonable strategy is to apply incremental training across AL iterations, that is, continuing training previous models on the new instances. However, Ash and Adams (2020) show that this type of warm-start may lead to sub-optimal performance for neural models and many recent AL works usually train models from scratch (Hu et al., 2019;Ein-Dor et al., 2020). Another method is to use an efficient model for querying and a more powerful model for final training. However, this might lead to sub-optimal results, which will be discussed in §4.1.', 'Another idea to reduce wait time is to simply allow querying with stale information. Actually, batch-mode AL ( §2.2.3) is such an example where instances in the same batch are queried with the same model. Haertel et al. (2010) propose parallel AL, which maintains separate loops of annotating, training, and scoring, and allows dynamic and parameterless instance selection at any time.']","In AL iterations, the annotators may need to wait for the training and querying steps (Line 3 and 4 in Algorithm 1). This wait time may bring some hidden costs, thus more efficient querying and training would be preferable for faster turnarounds.

To speed up querying, sub-sampling is a simple method to deal with large unlabeled pools (Roy and McCallum, 2001;Ertekin et al., 2007;Tsvigun et al., 2022). For some querying strategies, precalculating and caching unchanging information can also help to speed up (Ashrafi Asli et al., 2020;Citovsky et al., 2021). In addition, approximation with k-nearest neighbours can also be utilized to calculate density (Zhu et al., 2009) or search for instances after adversarial attacks (Ru et al., 2020).

To reduce training time, a seemingly reasonable strategy is to apply incremental training across AL iterations, that is, continuing training previous models on the new instances. However, Ash and Adams (2020) show that this type of warm-start may lead to sub-optimal performance for neural models and many recent AL works usually train models from scratch (Hu et al., 2019;Ein-Dor et al., 2020). Another method is to use an efficient model for querying and a more powerful model for final training. However, this might lead to sub-optimal results, which will be discussed in §4.1.

Another idea to reduce wait time is to simply allow querying with stale information. Actually, batch-mode AL ( §2.2.3) is such an example where instances in the same batch are queried with the same model. Haertel et al. (2010) propose parallel AL, which maintains separate loops of annotating, training, and scoring, and allows dynamic and parameterless instance selection at any time.","(p20.0) In AL iterations, the annotators may need to wait for the training and querying steps (Line 3 and 4 in Algorithm 1). This wait time may bring some hidden costs, thus more efficient querying and training would be preferable for faster turnarounds.

(p20.1) To speed up querying, sub-sampling is a simple method to deal with large unlabeled pools (Roy and McCallum, 2001;Ertekin et al., 2007;Tsvigun et al., 2022). For some querying strategies, precalculating and caching unchanging information can also help to speed up (Ashrafi Asli et al., 2020;Citovsky et al., 2021). In addition, approximation with k-nearest neighbours can also be utilized to calculate density (Zhu et al., 2009) or search for instances after adversarial attacks (Ru et al., 2020).

(p20.2) To reduce training time, a seemingly reasonable strategy is to apply incremental training across AL iterations, that is, continuing training previous models on the new instances. However, Ash and Adams (2020) show that this type of warm-start may lead to sub-optimal performance for neural models and many recent AL works usually train models from scratch (Hu et al., 2019;Ein-Dor et al., 2020). Another method is to use an efficient model for querying and a more powerful model for final training. However, this might lead to sub-optimal results, which will be discussed in §4.1.

(p20.3) Another idea to reduce wait time is to simply allow querying with stale information. Actually, batch-mode AL ( §2.2.3) is such an example where instances in the same batch are queried with the same model. Haertel et al. (2010) propose parallel AL, which maintains separate loops of annotating, training, and scoring, and allows dynamic and parameterless instance selection at any time.","[[], ['b50', 'b84', None], [None], [None]]","[[], ['b50', 'b84', None], [None], [None]]",5,"1. In AL iterations, the annotators may need to wait for the training and querying steps (Line 3 and 4 in Algorithm 1).
2. This wait time may bring some hidden costs, thus more efficient querying and training would be preferable for faster turnarounds.
3. To speed up querying, sub-sampling is a simple method to deal with large unlabeled pools (Roy and McCallum, 2001;Ertekin et al., 2007;Tsvigun et al., 2022).
4. For some querying strategies, precalculating and caching unchanging information can also help to speed up (Ashrafi Asli et al., 2020;Citovsky et al., 2021).
5. In addition, approximation with k-nearest neighbours can also be utilized to calculate density (Zhu et al., 2009) or search for instances after adversarial attacks (Ru et al., 2020).
6. To reduce training time, a seemingly reasonable strategy is to apply incremental training across AL iterations, that is, continuing training previous models on the new instances.
7. However, Ash and Adams (2020) show that this type of warm-start may lead to sub-optimal performance for neural models and many recent AL works usually train models from scratch (Hu et al., 2019;Ein-Dor et al., 2020).
8. Another method is to use an efficient model for querying and a more powerful model for final training.
9. However, this might lead to sub-optimal results, which will be discussed in §4.1.
10. Another idea to reduce wait time is to simply allow querying with stale information.
11. Actually, batch-mode AL ( §2.2.3) is such an example where instances in the same batch are queried with the same model.
12. Haertel et al. (2010) propose parallel AL, which maintains separate loops of annotating, training, and scoring, and allows dynamic and parameterless instance selection at any time."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s22,Model Mismatch,"['p22.0', 'p22.1']","['While it is natural to adopt the same bestperforming model throughout the AL process, there are cases where the query and final (successor) models can mismatch (Lewis and Catlett, 1994). Firstly, more efficient models are preferable for querying to reduce wait time ( §3.2.4). Moreover, since data usually outlive models, re-using ALbase data to train another model would be desired (Baldridge and Osborne, 2004;Tomanek et al., 2007). Several works show that model mismatch may make the gains from AL be negligible or even negative (Baldridge and Osborne, 2004;Lowell et al., 2019;, which raises concerns about the utilization of AL in practice.', 'For efficiency purposes, distillation can be utilized to improve querying efficiency while keeping reasonable AL performance.  show that using a smaller distilled version of a pre-trained model for querying does not lead to too much performance drop. Tsvigun et al. (2022) combine this idea with pseudo-labeling and sub-sampling to further reduce computational cost. Similarly, Nguyen et al. (2022) keep a smaller proxy model for query and synchronize the proxy with the main model by distillation.']","While it is natural to adopt the same bestperforming model throughout the AL process, there are cases where the query and final (successor) models can mismatch (Lewis and Catlett, 1994). Firstly, more efficient models are preferable for querying to reduce wait time ( §3.2.4). Moreover, since data usually outlive models, re-using ALbase data to train another model would be desired (Baldridge and Osborne, 2004;Tomanek et al., 2007). Several works show that model mismatch may make the gains from AL be negligible or even negative (Baldridge and Osborne, 2004;Lowell et al., 2019;, which raises concerns about the utilization of AL in practice.

For efficiency purposes, distillation can be utilized to improve querying efficiency while keeping reasonable AL performance.  show that using a smaller distilled version of a pre-trained model for querying does not lead to too much performance drop. Tsvigun et al. (2022) combine this idea with pseudo-labeling and sub-sampling to further reduce computational cost. Similarly, Nguyen et al. (2022) keep a smaller proxy model for query and synchronize the proxy with the main model by distillation.","(p22.0) While it is natural to adopt the same bestperforming model throughout the AL process, there are cases where the query and final (successor) models can mismatch (Lewis and Catlett, 1994). Firstly, more efficient models are preferable for querying to reduce wait time ( §3.2.4). Moreover, since data usually outlive models, re-using ALbase data to train another model would be desired (Baldridge and Osborne, 2004;Tomanek et al., 2007). Several works show that model mismatch may make the gains from AL be negligible or even negative (Baldridge and Osborne, 2004;Lowell et al., 2019;, which raises concerns about the utilization of AL in practice.

(p22.1) For efficiency purposes, distillation can be utilized to improve querying efficiency while keeping reasonable AL performance.  show that using a smaller distilled version of a pre-trained model for querying does not lead to too much performance drop. Tsvigun et al. (2022) combine this idea with pseudo-labeling and sub-sampling to further reduce computational cost. Similarly, Nguyen et al. (2022) keep a smaller proxy model for query and synchronize the proxy with the main model by distillation.","[[None, 'b48', 'b94'], ['b50']]","[[None, 'b48', 'b94'], ['b50']]",4,"1. While it is natural to adopt the same bestperforming model throughout the AL process, there are cases where the query and final (successor) models can mismatch (Lewis and Catlett, 1994).
2. Firstly, more efficient models are preferable for querying to reduce wait time ( §3.2.4).
3. Moreover, since data usually outlive models, re-using ALbase data to train another model would be desired (Baldridge and Osborne, 2004;Tomanek et al., 2007).
4. Several works show that model mismatch may make the gains from AL be negligible or even negative (Baldridge and Osborne, 2004;Lowell et al., 2019;, which raises concerns about the utilization of AL in practice.
5. For efficiency purposes, distillation can be utilized to improve querying efficiency while keeping reasonable AL performance.
6. show that using a smaller distilled version of a pre-trained model for querying does not lead to too much performance drop.
7. Tsvigun et al. (2022) combine this idea with pseudo-labeling and sub-sampling to further reduce computational cost.
8. Similarly, Nguyen et al. (2022) keep a smaller proxy model for query and synchronize the proxy with the main model by distillation."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s23,Learning,"['p23.0', 'p23.1', 'p23.2', 'p23.3', 'p23.4']","['AL can be combined with other advanced learning techniques to further reduce required annotations.', 'Semi-supervised learning. Since AL usually assumes an unlabeled pool, semi-supervised learning can be a natural fit. Combining these two is not a new idea: (McCallum and Nigam, 1998) adopt the EM algorithm to estimate the outputs of unlabeled data and utilize them for learning. This type of self-training or pseudo-labeling technique is often utilized in AL Majidi and Crane, 2013;Yu et al., 2022). With a similar motivation, (Dasgupta and Ng, 2009) use an unsupervised algorithm to identify the unambiguous instances to train an active learner. For the task of word alignment, which can be learned in an unsupervised manner, incorporating supervision with AL can bring further improvements in a data-efficient way (Ambati et al., 2010b,c).', 'Transfer learning. AL can be easily combined with transfer learning, another technique to reduce required annotations. Utilizing pre-trained models is already a good example (Ein-Dor et al., 2020;Tamkin et al., 2022) and continual training (Gururangan et al., 2020) can also be applied (Hua and Wang, 2022;Margatina et al., 2022). Moreover, transductive learning is commonly combined with AL by transferring learning signals from different domains (Chan and Ng, 2007;Shi et al., 2008;Rai et al., 2010;Saha et al., 2011;Wu et al., 2017;Kasai et al., 2019; or languages (Qian et al., 2014;Fang and Cohn, 2017;Fang et al., 2017;Chaudhary et al., 2019Moniz et al., 2022). In addition to the task model, the model-based query policy ( §2.1.4) is also often obtained with transfer learning.', 'Weak supervision. AL can also be combined with weakly supervised learning. Examples include learning from inputs and execution results for semantic parsing (Ni et al., 2020), labeling based on identical structure vectors for entity representations (Qian et al., 2020), learning from gazetteers and dictionaries for sequence labeling  and interactively discovering labeling rules .', 'Data augmentation. Augmentation is also applicable in AL and has been explored with iterative back-translation , mixup for sequence labeling  and phraseto-sentence augmentation for MT (Hu and Neubig, 2021). As discussed in §2.1.1, augmentation can also be helpful for instance querying (Jiang et al., 2020;Zhang et al., 2022b). Another interesting scenario involving augmentation and AL is query synthesis, which directly generates instances to be annotated instead of selecting existing unlabeled ones. Though synthesizing texts is still a hard problem generally, there have been successful applications for simple classification tasks (Schumann and Rehbein, 2019;Quteineh et al., 2020).']","AL can be combined with other advanced learning techniques to further reduce required annotations.

Semi-supervised learning. Since AL usually assumes an unlabeled pool, semi-supervised learning can be a natural fit. Combining these two is not a new idea: (McCallum and Nigam, 1998) adopt the EM algorithm to estimate the outputs of unlabeled data and utilize them for learning. This type of self-training or pseudo-labeling technique is often utilized in AL Majidi and Crane, 2013;Yu et al., 2022). With a similar motivation, (Dasgupta and Ng, 2009) use an unsupervised algorithm to identify the unambiguous instances to train an active learner. For the task of word alignment, which can be learned in an unsupervised manner, incorporating supervision with AL can bring further improvements in a data-efficient way (Ambati et al., 2010b,c).

Transfer learning. AL can be easily combined with transfer learning, another technique to reduce required annotations. Utilizing pre-trained models is already a good example (Ein-Dor et al., 2020;Tamkin et al., 2022) and continual training (Gururangan et al., 2020) can also be applied (Hua and Wang, 2022;Margatina et al., 2022). Moreover, transductive learning is commonly combined with AL by transferring learning signals from different domains (Chan and Ng, 2007;Shi et al., 2008;Rai et al., 2010;Saha et al., 2011;Wu et al., 2017;Kasai et al., 2019; or languages (Qian et al., 2014;Fang and Cohn, 2017;Fang et al., 2017;Chaudhary et al., 2019Moniz et al., 2022). In addition to the task model, the model-based query policy ( §2.1.4) is also often obtained with transfer learning.

Weak supervision. AL can also be combined with weakly supervised learning. Examples include learning from inputs and execution results for semantic parsing (Ni et al., 2020), labeling based on identical structure vectors for entity representations (Qian et al., 2020), learning from gazetteers and dictionaries for sequence labeling  and interactively discovering labeling rules .

Data augmentation. Augmentation is also applicable in AL and has been explored with iterative back-translation , mixup for sequence labeling  and phraseto-sentence augmentation for MT (Hu and Neubig, 2021). As discussed in §2.1.1, augmentation can also be helpful for instance querying (Jiang et al., 2020;Zhang et al., 2022b). Another interesting scenario involving augmentation and AL is query synthesis, which directly generates instances to be annotated instead of selecting existing unlabeled ones. Though synthesizing texts is still a hard problem generally, there have been successful applications for simple classification tasks (Schumann and Rehbein, 2019;Quteineh et al., 2020).","(p23.0) AL can be combined with other advanced learning techniques to further reduce required annotations.

(p23.1) Semi-supervised learning. Since AL usually assumes an unlabeled pool, semi-supervised learning can be a natural fit. Combining these two is not a new idea: (McCallum and Nigam, 1998) adopt the EM algorithm to estimate the outputs of unlabeled data and utilize them for learning. This type of self-training or pseudo-labeling technique is often utilized in AL Majidi and Crane, 2013;Yu et al., 2022). With a similar motivation, (Dasgupta and Ng, 2009) use an unsupervised algorithm to identify the unambiguous instances to train an active learner. For the task of word alignment, which can be learned in an unsupervised manner, incorporating supervision with AL can bring further improvements in a data-efficient way (Ambati et al., 2010b,c).

(p23.2) Transfer learning. AL can be easily combined with transfer learning, another technique to reduce required annotations. Utilizing pre-trained models is already a good example (Ein-Dor et al., 2020;Tamkin et al., 2022) and continual training (Gururangan et al., 2020) can also be applied (Hua and Wang, 2022;Margatina et al., 2022). Moreover, transductive learning is commonly combined with AL by transferring learning signals from different domains (Chan and Ng, 2007;Shi et al., 2008;Rai et al., 2010;Saha et al., 2011;Wu et al., 2017;Kasai et al., 2019; or languages (Qian et al., 2014;Fang and Cohn, 2017;Fang et al., 2017;Chaudhary et al., 2019Moniz et al., 2022). In addition to the task model, the model-based query policy ( §2.1.4) is also often obtained with transfer learning.

(p23.3) Weak supervision. AL can also be combined with weakly supervised learning. Examples include learning from inputs and execution results for semantic parsing (Ni et al., 2020), labeling based on identical structure vectors for entity representations (Qian et al., 2020), learning from gazetteers and dictionaries for sequence labeling  and interactively discovering labeling rules .

(p23.4) Data augmentation. Augmentation is also applicable in AL and has been explored with iterative back-translation , mixup for sequence labeling  and phraseto-sentence augmentation for MT (Hu and Neubig, 2021). As discussed in §2.1.1, augmentation can also be helpful for instance querying (Jiang et al., 2020;Zhang et al., 2022b). Another interesting scenario involving augmentation and AL is query synthesis, which directly generates instances to be annotated instead of selecting existing unlabeled ones. Though synthesizing texts is still a hard problem generally, there have been successful applications for simple classification tasks (Schumann and Rehbein, 2019;Quteineh et al., 2020).","[[], [None, 'b63'], ['b32', 'b39', 'b91', None, 'b67', 'b57', 'b2'], ['b112'], [None, 'b15', 'b72', 'b0']]","[[], [None, 'b63'], ['b32', 'b39', 'b91', None, 'b67', 'b57', 'b2'], ['b112'], [None, 'b15', 'b72', 'b0']]",14,"1. AL can be combined with other advanced learning techniques to further reduce required annotations.
2. Semi-supervised learning. Since AL usually assumes an unlabeled pool, semi-supervised learning can be a natural fit.
3. Combining these two is not a new idea: (McCallum and Nigam, 1998) adopt the EM algorithm to estimate the outputs of unlabeled data and utilize them for learning.
4. This type of self-training or pseudo-labeling technique is often utilized in AL Majidi and Crane, 2013;Yu et al., 2022).
5. With a similar motivation, (Dasgupta and Ng, 2009) use an unsupervised algorithm to identify the unambiguous instances to train an active learner.
6. For the task of word alignment, which can be learned in an unsupervised manner, incorporating supervision with AL can bring further improvements in a data-efficient way (Ambati et al., 2010b,c).
7. Transfer learning. AL can be easily combined with transfer learning, another technique to reduce required annotations.
8. Utilizing pre-trained models is already a good example (Ein-Dor et al., 2020;Tamkin et al., 2022) and continual training (Gururangan et al., 2020) can also be applied (Hua and Wang, 2022;Margatina et al., 2022).
9. Moreover, transductive learning is commonly combined with AL by transferring learning signals from different domains (Chan and Ng, 2007;Shi et al., 2008;Rai et al., 2010;Saha et al., 2011;Wu et al., 2017;Kasai et al., 2019; or languages (Qian et al., 2014;Fang and Cohn, 2017;Fang et al., 2017;Chaudhary et al., 2019Moniz et al., 2022).
10. In addition to the task model, the model-based query policy ( §2.1.4) is also often obtained with transfer learning.
11. Weak supervision. AL can also be combined with weakly supervised learning.
12. Examples include learning from inputs and execution results for semantic parsing (Ni et al., 2020), labeling based on identical structure vectors for entity representations (Qian et al., 2020), learning from gazetteers and dictionaries for sequence labeling  and interactively discovering labeling rules .
13. Data augmentation. Augmentation is also applicable in AL and has been explored with iterative back-translation , mixup for sequence labeling  and phraseto-sentence augmentation for MT (Dasgupta and Ng, 2009)0.
14. As discussed in §2.1.1, augmentation can also be helpful for instance querying (Dasgupta and Ng, 2009)1.
15. Another interesting scenario involving augmentation and AL is query synthesis, which directly generates instances to be annotated instead of selecting existing unlabeled ones.
16. Though synthesizing texts is still a hard problem generally, there have been successful applications for simple classification tasks (Dasgupta and Ng, 2009)2."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s25,Starting AL,"['p25.0', 'p25.1']","['While there are cases where there are already enough labeled data to train a reasonable model and AL is utilized to provide further improvements (Bloodgood and Callison-Burch, 2010;Geifman and El-Yaniv, 2017), at many times we are facing the cold-start problem, where instances need to be selected without a reasonable model. Especially, how to select the seed data to start the AL process is an interesting question, which may greatly influence the performance in initial AL stages Horbach and Palmer, 2016).', 'Random sampling is probably the most commonly utilized strategy, which is reasonable since it preserves the original data distribution. Some representativeness-based querying strategies ( §2.2) can also be utilized, for example, selecting points near the clustering centroids is a way to obtain representative and diverse seeds (Kang et al., 2004;Hu et al., 2010). Moreover, some advanced learning techniques ( §4.2) can also be helpful here, such as transfer learning (Wu et al., 2017) and unsupervised methods (Vlachos, 2006;Dasgupta and Ng, 2009). In addition, language model can be a useful tool, with which Dligach and Palmer (2011) select low-probability words in the context of word sense disambiguation and  choose cluster centers with surprisal embeddings by pre-trained contextualized LMs.']","While there are cases where there are already enough labeled data to train a reasonable model and AL is utilized to provide further improvements (Bloodgood and Callison-Burch, 2010;Geifman and El-Yaniv, 2017), at many times we are facing the cold-start problem, where instances need to be selected without a reasonable model. Especially, how to select the seed data to start the AL process is an interesting question, which may greatly influence the performance in initial AL stages Horbach and Palmer, 2016).

Random sampling is probably the most commonly utilized strategy, which is reasonable since it preserves the original data distribution. Some representativeness-based querying strategies ( §2.2) can also be utilized, for example, selecting points near the clustering centroids is a way to obtain representative and diverse seeds (Kang et al., 2004;Hu et al., 2010). Moreover, some advanced learning techniques ( §4.2) can also be helpful here, such as transfer learning (Wu et al., 2017) and unsupervised methods (Vlachos, 2006;Dasgupta and Ng, 2009). In addition, language model can be a useful tool, with which Dligach and Palmer (2011) select low-probability words in the context of word sense disambiguation and  choose cluster centers with surprisal embeddings by pre-trained contextualized LMs.","(p25.0) While there are cases where there are already enough labeled data to train a reasonable model and AL is utilized to provide further improvements (Bloodgood and Callison-Burch, 2010;Geifman and El-Yaniv, 2017), at many times we are facing the cold-start problem, where instances need to be selected without a reasonable model. Especially, how to select the seed data to start the AL process is an interesting question, which may greatly influence the performance in initial AL stages Horbach and Palmer, 2016).

(p25.1) Random sampling is probably the most commonly utilized strategy, which is reasonable since it preserves the original data distribution. Some representativeness-based querying strategies ( §2.2) can also be utilized, for example, selecting points near the clustering centroids is a way to obtain representative and diverse seeds (Kang et al., 2004;Hu et al., 2010). Moreover, some advanced learning techniques ( §4.2) can also be helpful here, such as transfer learning (Wu et al., 2017) and unsupervised methods (Vlachos, 2006;Dasgupta and Ng, 2009). In addition, language model can be a useful tool, with which Dligach and Palmer (2011) select low-probability words in the context of word sense disambiguation and  choose cluster centers with surprisal embeddings by pre-trained contextualized LMs.","[[None, 'b106'], [None, 'b57', 'b51']]","[[None, 'b106'], [None, 'b57', 'b51']]",5,"1. While there are cases where there are already enough labeled data to train a reasonable model and AL is utilized to provide further improvements (Bloodgood and Callison-Burch, 2010;Geifman and El-Yaniv, 2017), at many times we are facing the cold-start problem, where instances need to be selected without a reasonable model.
2. Especially, how to select the seed data to start the AL process is an interesting question, which may greatly influence the performance in initial AL stages Horbach and Palmer, 2016).
3. Random sampling is probably the most commonly utilized strategy, which is reasonable since it preserves the original data distribution.
4. Some representativeness-based querying strategies ( §2.2) can also be utilized, for example, selecting points near the clustering centroids is a way to obtain representative and diverse seeds (Kang et al., 2004;Hu et al., 2010).
5. Moreover, some advanced learning techniques ( §4.2) can also be helpful here, such as transfer learning (Wu et al., 2017) and unsupervised methods (Vlachos, 2006;Dasgupta and Ng, 2009).
6. In addition, language model can be a useful tool, with which Dligach and Palmer (2011) select low-probability words in the context of word sense disambiguation and  choose cluster centers with surprisal embeddings by pre-trained contextualized LMs."
252992688,A Survey of Active Learning for Natural Language Processing,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f,s26,Stopping AL,"['p26.0', 'p26.1', 'p26.2', 'p26.3', 'p26.4']","['When adopting AL in practice, it would be desirable to know the time to stop AL when the model performance is already near the upper limits, before running out of all the budgets. For this purpose, a stopping criterion is needed, which checks certain metrics satisfying certain conditions. There can be simple heuristics. For example, AL can be stopped when all unlabeled instances are no closer than any of the support vectors with an SVM (Schohn and Cohn, 2000;Ertekin et al., 2007) or no new n-grams remain in the unlabeled set for MT (Bloodgood and Callison-Burch, 2010). Nevertheless, these are specific to the underlying models or target tasks. For the design of a general stopping criterion, there are three main aspects to consider: metric, dataset and condition.', 'For the metric, measuring performance on a development set seems a natural option. However, the results would be unstable if this set is too small and it would be impractical to assume a large development set. Cross-validation on the training set also has problems since the labeled data by AL is usually biased. In this case, metrics from the query strategies can be utilized. Examples include uncertainty or confidence (Zhu and Hovy, 2007;Vlachos, 2008), disagreement (Tomanek et al., 2007;Olsson and Tomanek, 2009), estimated performance (Laws and Schütze, 2008), expected error (Zhu et al., 2008a), confidence variation (Ghayoomi, 2010), as well as actual performance on the selected instances (Zhu and Hovy, 2007). Moreover, comparing the predictions between consecutive AL iterations is another reasonable option (Zhu et al., 2008b;Bloodgood and Vijay-Shanker, 2009a).', 'The dataset to calculate the stopping metric requires careful choosing. The results could be unstable if not adopting a proper set . Many works suggest that a separate unlabeled dataset should be utilized Vlachos, 2008;Bloodgood and Vijay-Shanker, 2009a;Beatty et al., 2019;Kurlandski and Bloodgood, 2022). Since the stopping metrics usually do not rely on gold labels, this dataset could potentially be very large to provide more stable results, though wait time would be another factor to consider in this case ( §3.2.4).', 'The condition to stop AL is usually comparing the metrics to a pre-defined threshold. Earlier works only look at the metric at the current iteration, for example, stopping if the uncertainty or the error is less than the threshold (Zhu and Hovy, 2007). In this case, the threshold is hard to specify since it relies on the model and the task. (Zhu et al., 2008b) cascade multiple stopping criteria to mitigate this reliance. A more stable option is to track the change of the metrics over several AL iterations, such as stopping when the confidence consistently drops (Vlachos, 2008), the changing rate flattens (Laws and Schütze, 2008) or the predictions stabilize across iterations (Bloodgood and Vijay-Shanker, 2009a;Bloodgood and Grothendieck, 2013). Pullar-Strecker et al. (2021) provide an empirical comparison over common stopping criteria and would be a nice reference. Moreover, stopping AL can be closely related to performance prediction and early stopping. Especially, the latter can be of particular interest to AL since learning in early AL stages need to face the low-resource problem and how to perform early stopping may also require careful considerations.', '6 Related Topics and Future Directions']","When adopting AL in practice, it would be desirable to know the time to stop AL when the model performance is already near the upper limits, before running out of all the budgets. For this purpose, a stopping criterion is needed, which checks certain metrics satisfying certain conditions. There can be simple heuristics. For example, AL can be stopped when all unlabeled instances are no closer than any of the support vectors with an SVM (Schohn and Cohn, 2000;Ertekin et al., 2007) or no new n-grams remain in the unlabeled set for MT (Bloodgood and Callison-Burch, 2010). Nevertheless, these are specific to the underlying models or target tasks. For the design of a general stopping criterion, there are three main aspects to consider: metric, dataset and condition.

For the metric, measuring performance on a development set seems a natural option. However, the results would be unstable if this set is too small and it would be impractical to assume a large development set. Cross-validation on the training set also has problems since the labeled data by AL is usually biased. In this case, metrics from the query strategies can be utilized. Examples include uncertainty or confidence (Zhu and Hovy, 2007;Vlachos, 2008), disagreement (Tomanek et al., 2007;Olsson and Tomanek, 2009), estimated performance (Laws and Schütze, 2008), expected error (Zhu et al., 2008a), confidence variation (Ghayoomi, 2010), as well as actual performance on the selected instances (Zhu and Hovy, 2007). Moreover, comparing the predictions between consecutive AL iterations is another reasonable option (Zhu et al., 2008b;Bloodgood and Vijay-Shanker, 2009a).

The dataset to calculate the stopping metric requires careful choosing. The results could be unstable if not adopting a proper set . Many works suggest that a separate unlabeled dataset should be utilized Vlachos, 2008;Bloodgood and Vijay-Shanker, 2009a;Beatty et al., 2019;Kurlandski and Bloodgood, 2022). Since the stopping metrics usually do not rely on gold labels, this dataset could potentially be very large to provide more stable results, though wait time would be another factor to consider in this case ( §3.2.4).

The condition to stop AL is usually comparing the metrics to a pre-defined threshold. Earlier works only look at the metric at the current iteration, for example, stopping if the uncertainty or the error is less than the threshold (Zhu and Hovy, 2007). In this case, the threshold is hard to specify since it relies on the model and the task. (Zhu et al., 2008b) cascade multiple stopping criteria to mitigate this reliance. A more stable option is to track the change of the metrics over several AL iterations, such as stopping when the confidence consistently drops (Vlachos, 2008), the changing rate flattens (Laws and Schütze, 2008) or the predictions stabilize across iterations (Bloodgood and Vijay-Shanker, 2009a;Bloodgood and Grothendieck, 2013). Pullar-Strecker et al. (2021) provide an empirical comparison over common stopping criteria and would be a nice reference. Moreover, stopping AL can be closely related to performance prediction and early stopping. Especially, the latter can be of particular interest to AL since learning in early AL stages need to face the low-resource problem and how to perform early stopping may also require careful considerations.

6 Related Topics and Future Directions","(p26.0) When adopting AL in practice, it would be desirable to know the time to stop AL when the model performance is already near the upper limits, before running out of all the budgets. For this purpose, a stopping criterion is needed, which checks certain metrics satisfying certain conditions. There can be simple heuristics. For example, AL can be stopped when all unlabeled instances are no closer than any of the support vectors with an SVM (Schohn and Cohn, 2000;Ertekin et al., 2007) or no new n-grams remain in the unlabeled set for MT (Bloodgood and Callison-Burch, 2010). Nevertheless, these are specific to the underlying models or target tasks. For the design of a general stopping criterion, there are three main aspects to consider: metric, dataset and condition.

(p26.1) For the metric, measuring performance on a development set seems a natural option. However, the results would be unstable if this set is too small and it would be impractical to assume a large development set. Cross-validation on the training set also has problems since the labeled data by AL is usually biased. In this case, metrics from the query strategies can be utilized. Examples include uncertainty or confidence (Zhu and Hovy, 2007;Vlachos, 2008), disagreement (Tomanek et al., 2007;Olsson and Tomanek, 2009), estimated performance (Laws and Schütze, 2008), expected error (Zhu et al., 2008a), confidence variation (Ghayoomi, 2010), as well as actual performance on the selected instances (Zhu and Hovy, 2007). Moreover, comparing the predictions between consecutive AL iterations is another reasonable option (Zhu et al., 2008b;Bloodgood and Vijay-Shanker, 2009a).

(p26.2) The dataset to calculate the stopping metric requires careful choosing. The results could be unstable if not adopting a proper set . Many works suggest that a separate unlabeled dataset should be utilized Vlachos, 2008;Bloodgood and Vijay-Shanker, 2009a;Beatty et al., 2019;Kurlandski and Bloodgood, 2022). Since the stopping metrics usually do not rely on gold labels, this dataset could potentially be very large to provide more stable results, though wait time would be another factor to consider in this case ( §3.2.4).

(p26.3) The condition to stop AL is usually comparing the metrics to a pre-defined threshold. Earlier works only look at the metric at the current iteration, for example, stopping if the uncertainty or the error is less than the threshold (Zhu and Hovy, 2007). In this case, the threshold is hard to specify since it relies on the model and the task. (Zhu et al., 2008b) cascade multiple stopping criteria to mitigate this reliance. A more stable option is to track the change of the metrics over several AL iterations, such as stopping when the confidence consistently drops (Vlachos, 2008), the changing rate flattens (Laws and Schütze, 2008) or the predictions stabilize across iterations (Bloodgood and Vijay-Shanker, 2009a;Bloodgood and Grothendieck, 2013). Pullar-Strecker et al. (2021) provide an empirical comparison over common stopping criteria and would be a nice reference. Moreover, stopping AL can be closely related to performance prediction and early stopping. Especially, the latter can be of particular interest to AL since learning in early AL stages need to face the low-resource problem and how to perform early stopping may also require careful considerations.

(p26.4) 6 Related Topics and Future Directions","[['b12', 'b106', None], ['b48', 'b52', None, 'b83', 'b81', 'b82', 'b47'], ['b52', None], ['b83', 'b52', 'b81', None], []]","[['b12', 'b106', None], ['b48', 'b52', None, 'b83', 'b81', 'b82', 'b47'], ['b52', None], ['b83', 'b52', 'b81', None], []]",16,"1. When adopting AL in practice, it would be desirable to know the time to stop AL when the model performance is already near the upper limits, before running out of all the budgets.
2. For this purpose, a stopping criterion is needed, which checks certain metrics satisfying certain conditions.
3. There can be simple heuristics.
4. For example, AL can be stopped when all unlabeled instances are no closer than any of the support vectors with an SVM (Schohn and Cohn, 2000;Ertekin et al., 2007) or no new n-grams remain in the unlabeled set for MT (Bloodgood and Callison-Burch, 2010).
5. Nevertheless, these are specific to the underlying models or target tasks.
6. For the design of a general stopping criterion, there are three main aspects to consider: metric, dataset and condition.
7. For the metric, measuring performance on a development set seems a natural option.
8. However, the results would be unstable if this set is too small and it would be impractical to assume a large development set.
9. Cross-validation on the training set also has problems since the labeled data by AL is usually biased.
10. In this case, metrics from the query strategies can be utilized.
11. Examples include uncertainty or confidence (Zhu and Hovy, 2007;Vlachos, 2008), disagreement (Tomanek et al., 2007;Olsson and Tomanek, 2009), estimated performance (Bloodgood and Callison-Burch, 2010)3, expected error (Zhu et al., 2008a), confidence variation (Ghayoomi, 2010), as well as actual performance on the selected instances (Bloodgood and Callison-Burch, 2010)0.
12. Moreover, comparing the predictions between consecutive AL iterations is another reasonable option (Zhu et al., 2008b;Bloodgood and Vijay-Shanker, 2009a).
13. The dataset to calculate the stopping metric requires careful choosing.
14. The results could be unstable if not adopting a proper set .
15. Many works suggest that a separate unlabeled dataset should be utilized Vlachos, 2008;Bloodgood and Vijay-Shanker, 2009a;Beatty et al., 2019;Kurlandski and Bloodgood, 2022).
16. Since the stopping metrics usually do not rely on gold labels, this dataset could potentially be very large to provide more stable results, though wait time would be another factor to consider in this case ( §3.2.4).
17. The condition to stop AL is usually comparing the metrics to a pre-defined threshold.
18. Earlier works only look at the metric at the current iteration, for example, stopping if the uncertainty or the error is less than the threshold (Bloodgood and Callison-Burch, 2010)0.
19. In this case, the threshold is hard to specify since it relies on the model and the task.
20. (Bloodgood and Callison-Burch, 2010)1 cascade multiple stopping criteria to mitigate this reliance.
21. A more stable option is to track the change of the metrics over several AL iterations, such as stopping when the confidence consistently drops (Bloodgood and Callison-Burch, 2010)2, the changing rate flattens (Bloodgood and Callison-Burch, 2010)3 or the predictions stabilize across iterations (Bloodgood and Callison-Burch, 2010)4.
22. Pullar-Strecker et al. (Bloodgood and Callison-Burch, 2010)5 provide an empirical comparison over common stopping criteria and would be a nice reference.
23. Moreover, stopping AL can be closely related to performance prediction and early stopping.
24. Especially, the latter can be of particular interest to AL since learning in early AL stages need to face the low-resource problem and how to perform early stopping may also require careful considerations.
25. 6 Related Topics and Future Directions"
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s4,How Low is Low-Resource?,"['p4.0', 'p4.1', 'p4.2']","['On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.', '(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.', 'Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.']","On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","(p4.0) On the dimension of task-specific labels, different thresholds are used to define low-resource. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. Kann et al.

(p4.1) (2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens. The threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation,  frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.

(p4.2) Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.","[[], ['b0', 'b13'], [None, 'b13']]","[[], ['b0', 'b13'], [None, 'b13']]",4,"1. On the dimension of task-specific labels, different thresholds are used to define low-resource.
2. For part-of-speech (POS) tagging, Garrette and Baldridge (2013) limit the time of the annotators to 2 hours resulting in up to 1-2k tokens.
3. Kann et al.(2020) study languages that have less than 10k labeled tokens in the Universal Dependency project (Nivre et al., 2020) and Loubser and Puttkammer (2020) report that most available datasets for South African languages have 40-60k labeled tokens.
4. The threshold is also task-dependent and more complex tasks might also increase the resource requirements.
5. For text generation,  frame their work as low-resource with 350k labeled training instances.
6. Similar to the task, the resource requirements can also depend on the language.
7. Plank et al. (2016) find that task performance varies between language families given the same amount of limited training data.
8. Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability.
9. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches.
10. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s6,Data Augmentation,"['p6.0', 'p6.1', 'p6.2', 'p6.3']","[""New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019). Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018)."", 'To go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (Şahin and Steedman, 2018;Vania et al., 2019;Dehouck and Gómez-Rodríguez, 2020), simplification of sentences by removal of sentence parts (Şahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020). For whole sentences, paraphrasing through backtranslation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018). An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019). Back-translation has also been leveraged for text classification (Xie et al., 2020;Hegde and Patil, 2020). This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020). It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. Ding et al. (2020) extend this idea for token level tasks.', 'Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020). They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020). Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score). This approach is often applied on the level of vector representations. For instance, Grundkiewicz et al. (2019) reverse the augmentation setting by applying transformations that flip the (binary) label. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.', 'Open Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing. A reason might be that several of the approaches require an indepth understanding of the language. There is not yet a unified framework that allows applying data augmentation across tasks and languages. Recently, Longpre et al. (2020) hypothesised that data augmentation provides the same benefits as pretraining in transformer models. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited.']","New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019). Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018).

To go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (Şahin and Steedman, 2018;Vania et al., 2019;Dehouck and Gómez-Rodríguez, 2020), simplification of sentences by removal of sentence parts (Şahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020). For whole sentences, paraphrasing through backtranslation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018). An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019). Back-translation has also been leveraged for text classification (Xie et al., 2020;Hegde and Patil, 2020). This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020). It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. Ding et al. (2020) extend this idea for token level tasks.

Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020). They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020). Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score). This approach is often applied on the level of vector representations. For instance, Grundkiewicz et al. (2019) reverse the augmentation setting by applying transformations that flip the (binary) label. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.

Open Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing. A reason might be that several of the approaches require an indepth understanding of the language. There is not yet a unified framework that allows applying data augmentation across tasks and languages. Recently, Longpre et al. (2020) hypothesised that data augmentation provides the same benefits as pretraining in transformer models. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited.","(p6.0) New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019). Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018).

(p6.1) To go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (Şahin and Steedman, 2018;Vania et al., 2019;Dehouck and Gómez-Rodríguez, 2020), simplification of sentences by removal of sentence parts (Şahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020). For whole sentences, paraphrasing through backtranslation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018). An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019). Back-translation has also been leveraged for text classification (Xie et al., 2020;Hegde and Patil, 2020). This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020;Anaby-Tavor et al., 2020). It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. Ding et al. (2020) extend this idea for token level tasks.

(p6.2) Adversarial methods are often used to find weaknesses in machine learning models (Jin et al., 2020;Garg and Ramakrishnan, 2020). They can, however, also be utilized to augment NLP datasets (Yasunaga et al., 2018;Morris et al., 2020). Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score). This approach is often applied on the level of vector representations. For instance, Grundkiewicz et al. (2019) reverse the augmentation setting by applying transformations that flip the (binary) label. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.

(p6.3) Open Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing. A reason might be that several of the approaches require an indepth understanding of the language. There is not yet a unified framework that allows applying data augmentation across tasks and languages. Recently, Longpre et al. (2020) hypothesised that data augmentation provides the same benefits as pretraining in transformer models. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited.","[['b49', None, 'b46', 'b18'], ['b46', None, 'b55', 'b9'], [None, 'b60'], [None]]","[['b49', None, 'b46', 'b18'], ['b46', None, 'b55', 'b9'], [None, 'b60'], [None]]",11,"1. New instances can be obtained based on existing ones by modifying the features with transformations that do not change the label.
2. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content.
3. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms (Wei and Zou, 2019), entities of the same type (Raiman and Miller, 2017;Dai and Adel, 2020) or words that share the same morphology (Gulordava et al., 2018;Vania et al., 2019).
4. Such replacements can also be guided by a language model that takes context into consideration (Fadaee et al., 2017;Kobayashi, 2018).
5. To go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts.
6. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree (Şahin and Steedman, 2018;Vania et al., 2019;Dehouck and Gómez-Rodríguez, 2020), simplification of sentences by removal of sentence parts (Şahin and Steedman, 2018) and inversion of the subject-object relation (Min et al., 2020).
7. For whole sentences, paraphrasing through backtranslation can be used.
8. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011;Hoang et al., 2018).
9. An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict.
10. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Raiman and Miller, 2017;Dai and Adel, 2020)0.
11. Back-translation has also been leveraged for text classification (Raiman and Miller, 2017;Dai and Adel, 2020)1.
12. This setting assumes, however, the availability of a translation system.
13. Instead, a language model can also be used for augmenting text classification datasets (Raiman and Miller, 2017;Dai and Adel, 2020)2.
14. It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label.
15. It then generates additional sentences that fit this label.
16. Ding et al. (Raiman and Miller, 2017;Dai and Adel, 2020)9 extend this idea for token level tasks.
17. Adversarial methods are often used to find weaknesses in machine learning models (Raiman and Miller, 2017;Dai and Adel, 2020)4.
18. They can, however, also be utilized to augment NLP datasets (Raiman and Miller, 2017;Dai and Adel, 2020)5.
19. Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (Raiman and Miller, 2017;Dai and Adel, 2020)6.
20. This approach is often applied on the level of vector representations.
21. For instance, Grundkiewicz et al. (Raiman and Miller, 2017;Dai and Adel, 2020)7 reverse the augmentation setting by applying transformations that flip the (Raiman and Miller, 2017;Dai and Adel, 2020)8 label.
22. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task.
23. Open Issues: While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are taskindependent, it has not found such widespread use in natural language processing.
24. A reason might be that several of the approaches require an indepth understanding of the language.
25. There is not yet a unified framework that allows applying data augmentation across tasks and languages.
26. Recently, Longpre et al. (Raiman and Miller, 2017;Dai and Adel, 2020)9 hypothesised that data augmentation provides the same benefits as pretraining in transformer models.
27. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s7,Distant & Weak Supervision,"['p7.0', 'p7.1', 'p7.2', 'p7.3', 'p7.4']","['In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).', 'While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.', 'Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.', 'Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.', 'While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).']","In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","(p7.0) In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a). The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019). This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).

(p7.1) While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully em-  (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019;). An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.

(p7.2) Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.

(p7.3) Distant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.

(p7.4) While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018;Qian et al., 2020).","[['b20', 'b23', 'b22', 'b38', None, 'b67', 'b1', 'b2', 'b61'], ['b5', None], ['b54', None], [None], [None, 'b14']]","[['b20', 'b23', 'b22', 'b38', None, 'b67', 'b1', 'b2', 'b61'], ['b5', None], ['b54', None], [None], [None, 'b14']]",16,"1. In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified.
2. The corresponding labels are obtained through a (semi-)automatic process from an external source of information.
3. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations.
4. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012).
5. It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017;Hedderich and Klakow, 2018;Deng and Sun, 2019;Alt et al., 2019;Ye et al., 2019;Lange et al., 2019a;Nooralahzadeh et al., 2019;Le and Titov, 2019;Cao et al., 2019;Lison et al., 2020;Hedderich et al., 2021a).
6. The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps (Norman et al., 2019).
7. This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules.
8. These encompass also other ideas like reg-ex rules or simple programming functions (Ratner et al., 2017;Zheng et al., 2019;Adelani et al., 2020;Hedderich et al., 2020;Lison et al., 2020;Ren et al., 2020;Karamanolakis et al., 2021).
9. While distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP.
10. Nevertheless, distant supervision has also been successfully em-
11. (NER)3 build a discourse-structure dataset using guidance from sentiment annotations.
12. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (NER)0 or from entity lists (NER)1.
13. For some classification tasks, the labels can be rephrased with simple rules into sentences.
14. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (NER)2.
15. An unlabeled review, for instance, might be continued with ""It was great/bad"" for obtaining binary sentiment labels.
16. Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited.
17. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules.
18. It is an open question whether a task needs to have specific properties to be suitable for this approach.
19. The existing work on other tasks and the popularity in other fields like image classification (Xiao et al., 2015;Li et al., 2017;Lee et al., 2018;Mahajan et al., 2018; suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.
20. Distant supervision methods heavily rely on auxiliary data.
21. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.
22. Kann et al. (NER)3 find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages.
23. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.
24. While distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules.
25. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme.
26. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches.
27. Taking the human expert into the focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (NER)4."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s8,Cross-Lingual Annotation Projections,"['p8.0', 'p8.1']","['For cross-lingual projections, a task-specific classifier is trained in a high-resource language. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013;Wisniewski et al., 2014;Plank and Agić, 2018;Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014;Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agić and Vulić, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019;Zhang et al., 2019a;Fei et al., 2020;Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015;Marasović et al., 2016;Friedrich and Gateva, 2017).', 'Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both labels in a high-resource language and means to project them into a low-resource language. Especially the latter can be an issue as machine translation by itself might be problematic for a specific low-resource language. A limitation of the parallel corpora is their domains like political proceedings or religious texts. Mayhew et al. (2017), Fang and Cohn (2017) and Karamanolakis et al. (2020) propose systems with fewer requirements based on word translations, bilingual dictionaries and taskspecific seed words, respectively.']","For cross-lingual projections, a task-specific classifier is trained in a high-resource language. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013;Wisniewski et al., 2014;Plank and Agić, 2018;Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014;Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agić and Vulić, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019;Zhang et al., 2019a;Fei et al., 2020;Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015;Marasović et al., 2016;Friedrich and Gateva, 2017).

Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both labels in a high-resource language and means to project them into a low-resource language. Especially the latter can be an issue as machine translation by itself might be problematic for a specific low-resource language. A limitation of the parallel corpora is their domains like political proceedings or religious texts. Mayhew et al. (2017), Fang and Cohn (2017) and Karamanolakis et al. (2020) propose systems with fewer requirements based on word translations, bilingual dictionaries and taskspecific seed words, respectively.","(p8.0) For cross-lingual projections, a task-specific classifier is trained in a high-resource language. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013;Wisniewski et al., 2014;Plank and Agić, 2018;Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014;Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agić and Vulić, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019;Zhang et al., 2019a;Fei et al., 2020;Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015;Marasović et al., 2016;Friedrich and Gateva, 2017).

(p8.1) Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both labels in a high-resource language and means to project them into a low-resource language. Especially the latter can be an issue as machine translation by itself might be problematic for a specific low-resource language. A limitation of the parallel corpora is their domains like political proceedings or religious texts. Mayhew et al. (2017), Fang and Cohn (2017) and Karamanolakis et al. (2020) propose systems with fewer requirements based on word translations, bilingual dictionaries and taskspecific seed words, respectively.","[['b39', 'b59', 'b41', 'b12', 'b52', None, 'b69', 'b65'], [None]]","[['b39', 'b59', 'b41', 'b12', 'b52', None, 'b69', 'b65'], [None]]",9,"1. For cross-lingual projections, a task-specific classifier is trained in a high-resource language.
2. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier.
3. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001).
4. This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages.
5. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013;Wisniewski et al., 2014;Plank and Agić, 2018;Eskander et al., 2020).
6. Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014;Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agić and Vulić, 2019).
7. Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019;Zhang et al., 2019a;Fei et al., 2020;Amjad et al., 2020).
8. Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015;Marasović et al., 2016;Friedrich and Gateva, 2017).
9. Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both labels in a high-resource language and means to project them into a low-resource language.
10. Especially the latter can be an issue as machine translation by itself might be problematic for a specific low-resource language.
11. A limitation of the parallel corpora is their domains like political proceedings or religious texts.
12. Mayhew et al. (2017), Fang and Cohn (2017) and Karamanolakis et al. (Yarowsky et al., 2001)0 propose systems with fewer requirements based on word translations, bilingual dictionaries and taskspecific seed words, respectively."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s9,Learning with Noisy Labels,"['p9.0', 'p9.1', 'p9.2', 'p9.3']","['The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.', 'Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).', 'The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.', 'In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).']","The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.

Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.

In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","(p9.0) The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.

(p9.1) Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019). Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).

(p9.2) The noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), Lison et al. (2020) and Ren et al. (2020) leverage several sources of distant supervision and learn how to combine them.

(p9.3) In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly. Related approaches learn latent variables (Jie et al., 2019), use constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019).","[[], [None, 'b1'], ['b48', 'b22', 'b21', None, 'b10', 'b67', 'b61'], [None, 'b1', 'b11']]","[[], [None, 'b1'], ['b48', 'b22', 'b21', None, 'b10', 'b67', 'b61'], [None, 'b1', 'b11']]",12,"1. The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations.
2. These labels tend, however, to contain more errors.
3. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance.
4. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision.
5. We categorize these into two ideas: noise filtering and noise modeling.
6. Noise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled.
7. This often includes training a classifier to make the filtering decision.
8. The filtering can remove the instances completely from the training data, e.g., through a probability threshold (Jia et al., 2019), a binary classifier (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019), or the use of a reinforcement-based agent Nooralahzadeh et al., 2019).
9. Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019).
10. The noise in the labels can also be modeled.
11. A common model is a confusion matrix estimating the relationship between clean and noisy labels (Fang and Cohn, 2016;Luo et al., 2017;Hedderich and Klakow, 2018;Paul et al., 2019;Lange et al., 2019a,c;Chen et al., 2019;Wang et al., 2019;Hedderich et al., 2021b).
12. The classifier is no longer trained directly on the noisily-labeled data.
13. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution.
14. This can be interpreted as the original classifier being trained on a ""cleaned"" version of the noisy labels.
15. In Ye et al. (2019), the prediction is shifted from the noisy to the clean distribution during testing.
16. In Chen et al. (2020a), a group of reinforcement agents relabels noisy instances.
17. Rehbein and Ruppenhofer (2017), Lison et al. (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019)0 and Ren et al. (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019)0 leverage several sources of distant supervision and learn how to combine them.
18. In NER, the noise in distantly supervised labels tends to be false negatives, i.e., mentions of entities that have been missed by the automatic method.
19. Partial annotation learning Nooralahzadeh et al., 2019;Cao et al., 2019) takes this into account explicitly.
20. Related approaches learn latent variables (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019)1, use constrained binary learning (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019)2 or construct a loss assuming that only unlabeled positive instances exist (Adel and Schütze, 2015; Onoe and Durrett, 2019; Huang and Du, 2019)3."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s12,Pre-Trained Language Representations,"['p12.0', 'p12.1']","['Feature vectors are the core input component of many neural network-based models for NLP tasks. They are numerical representations of words or sentences, as neural architectures do not allow the processing of strings and characters as such. Collobert et al. (2011) showed that training these models for the task of language-modeling on a large-scale corpus results in high-quality word representations, which can be reused for other downstream tasks as well. Subword-based embeddings such as fastText n-gram embeddings (Bojanowski et al., 2017) and byte-pair-encoding embeddings (Heinzerling and Strube, 2018) addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word.  showed that these embeddings leveraging subword information are beneficial for lowresource sequence labeling tasks, such as named entity recognition and typing, and outperform wordlevel embeddings. Jungmaier et al. (2020) added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings. In addition, pre-trained embeddings were published for more than 270 languages for both embedding methods. This enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence. This includes pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b). These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019).', 'Open Issues: While pre-trained language models achieve significant performance increases compared to standard word embeddings, it is still questionable if these methods are suited for real-world low-resource scenarios. For example, all of these models require large hardware requirements, in particular, considering that the transformer model size keeps increasing to boost performance (Raffel et al., 2020). Therefore, these large-scale methods might not be suited for low-resource scenarios where hardware is also low-resource. Biljon et al. (2020) showed that low-to mediumdepth transformer sizes perform better than larger models for low-resource languages and Schick and Schütze (2020) managed to train models with three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT-3 on few-shot task by reformulating the training task and using ensembling. Melamud et al. (2019) showed that simple bag-of-words approaches are better when there are only a few dozen training instances or less for text classification, while more complex transformer models require more training data. Bhattacharjee et al. (2020) found that crossview training (Clark et al., 2018) leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages. Alabi et al. (2020) found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources.']","Feature vectors are the core input component of many neural network-based models for NLP tasks. They are numerical representations of words or sentences, as neural architectures do not allow the processing of strings and characters as such. Collobert et al. (2011) showed that training these models for the task of language-modeling on a large-scale corpus results in high-quality word representations, which can be reused for other downstream tasks as well. Subword-based embeddings such as fastText n-gram embeddings (Bojanowski et al., 2017) and byte-pair-encoding embeddings (Heinzerling and Strube, 2018) addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word.  showed that these embeddings leveraging subword information are beneficial for lowresource sequence labeling tasks, such as named entity recognition and typing, and outperform wordlevel embeddings. Jungmaier et al. (2020) added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings. In addition, pre-trained embeddings were published for more than 270 languages for both embedding methods. This enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence. This includes pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b). These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019).

Open Issues: While pre-trained language models achieve significant performance increases compared to standard word embeddings, it is still questionable if these methods are suited for real-world low-resource scenarios. For example, all of these models require large hardware requirements, in particular, considering that the transformer model size keeps increasing to boost performance (Raffel et al., 2020). Therefore, these large-scale methods might not be suited for low-resource scenarios where hardware is also low-resource. Biljon et al. (2020) showed that low-to mediumdepth transformer sizes perform better than larger models for low-resource languages and Schick and Schütze (2020) managed to train models with three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT-3 on few-shot task by reformulating the training task and using ensembling. Melamud et al. (2019) showed that simple bag-of-words approaches are better when there are only a few dozen training instances or less for text classification, while more complex transformer models require more training data. Bhattacharjee et al. (2020) found that crossview training (Clark et al., 2018) leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages. Alabi et al. (2020) found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources.","(p12.0) Feature vectors are the core input component of many neural network-based models for NLP tasks. They are numerical representations of words or sentences, as neural architectures do not allow the processing of strings and characters as such. Collobert et al. (2011) showed that training these models for the task of language-modeling on a large-scale corpus results in high-quality word representations, which can be reused for other downstream tasks as well. Subword-based embeddings such as fastText n-gram embeddings (Bojanowski et al., 2017) and byte-pair-encoding embeddings (Heinzerling and Strube, 2018) addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word.  showed that these embeddings leveraging subword information are beneficial for lowresource sequence labeling tasks, such as named entity recognition and typing, and outperform wordlevel embeddings. Jungmaier et al. (2020) added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings. In addition, pre-trained embeddings were published for more than 270 languages for both embedding methods. This enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence. This includes pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b). These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019).

(p12.1) Open Issues: While pre-trained language models achieve significant performance increases compared to standard word embeddings, it is still questionable if these methods are suited for real-world low-resource scenarios. For example, all of these models require large hardware requirements, in particular, considering that the transformer model size keeps increasing to boost performance (Raffel et al., 2020). Therefore, these large-scale methods might not be suited for low-resource scenarios where hardware is also low-resource. Biljon et al. (2020) showed that low-to mediumdepth transformer sizes perform better than larger models for low-resource languages and Schick and Schütze (2020) managed to train models with three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT-3 on few-shot task by reformulating the training task and using ensembling. Melamud et al. (2019) showed that simple bag-of-words approaches are better when there are only a few dozen training instances or less for text classification, while more complex transformer models require more training data. Bhattacharjee et al. (2020) found that crossview training (Clark et al., 2018) leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages. Alabi et al. (2020) found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources.","[[None, 'b47'], ['b16', None]]","[[None, 'b47'], ['b16', None]]",4,"1. Feature vectors are the core input component of many neural network-based models for NLP tasks.
2. They are numerical representations of words or sentences, as neural architectures do not allow the processing of strings and characters as such.
3. Collobert et al. (2011) showed that training these models for the task of language-modeling on a large-scale corpus results in high-quality word representations, which can be reused for other downstream tasks as well.
4. Subword-based embeddings such as fastText n-gram embeddings (Bojanowski et al., 2017) and byte-pair-encoding embeddings (Heinzerling and Strube, 2018) addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word.
5. showed that these embeddings leveraging subword information are beneficial for lowresource sequence labeling tasks, such as named entity recognition and typing, and outperform wordlevel embeddings.
6. Jungmaier et al. (Bojanowski et al., 2017)4 added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings.
7. In addition, pre-trained embeddings were published for more than 270 languages for both embedding methods.
8. This enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia.
9. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence.
10. This includes pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b).
11. These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019).
12. Open Issues: While pre-trained language models achieve significant performance increases compared to standard word embeddings, it is still questionable if these methods are suited for real-world low-resource scenarios.
13. For example, all of these models require large hardware requirements, in particular, considering that the transformer model size keeps increasing to boost performance (Raffel et al., 2020).
14. Therefore, these large-scale methods might not be suited for low-resource scenarios where hardware is also low-resource.
15. Biljon et al. (Bojanowski et al., 2017)4 showed that low-to mediumdepth transformer sizes perform better than larger models for low-resource languages and Schick and Schütze (Bojanowski et al., 2017)4 managed to train models with three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT-3 on few-shot task by reformulating the training task and using ensembling.
16. Melamud et al. (Bojanowski et al., 2017)1 showed that simple bag-of-words approaches are better when there are only a few dozen training instances or less for text classification, while more complex transformer models require more training data.
17. Bhattacharjee et al. (Bojanowski et al., 2017)4 found that crossview training (Bojanowski et al., 2017)3 leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT.
18. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages.
19. Alabi et al. (Bojanowski et al., 2017)4 found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources."
225062337,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,"Linguistics, Computer Science",https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d,s14,Multilingual Language Models,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4']","['Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages. This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) . These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.', 'In cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language. Instead, labeled data from a high-resource language is leveraged. A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.', 'The transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages. This is useful for standard word embeddings  as well as pre-trained language models. For example, by aligning the languages inside a single multilin- This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018). This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020). For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.', 'Open Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold. For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier. Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT. Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 . In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages. This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.', '(2020) showed.']","Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages. This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) . These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.

In cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language. Instead, labeled data from a high-resource language is leveraged. A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.

The transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages. This is useful for standard word embeddings  as well as pre-trained language models. For example, by aligning the languages inside a single multilin- This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018). This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020). For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.

Open Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold. For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier. Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT. Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 . In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages. This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.

(2020) showed.","(p14.0) Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages. This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) . These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.

(p14.1) In cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language. Instead, labeled data from a high-resource language is leveraged. A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.

(p14.2) The transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages. This is useful for standard word embeddings  as well as pre-trained language models. For example, by aligning the languages inside a single multilin- This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018). This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020). For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.

(p14.3) Open Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold. For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier. Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT. Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 . In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages. This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.

(p14.4) (2020) showed.","[[None], [], [None, 'b66'], ['b53'], []]","[[None], [], [None, 'b66'], ['b53'], []]",4,"1. Analogously to low-resource domains, lowresource languages can also benefit from labeled resources available in other high-resource languages.
2. This usually requires the training of multilingual language representations by combining monolingual representations (Lange et al., 2020a) or training a single model for many languages, such as multilingual BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020) .
3. These models are trained using unlabeled, monolingual corpora from different languages and can be used in crossand multilingual settings, due to many languages seen during pre-training.
4. In cross-lingual zero-shot learning, no taskspecific labeled data is available in the low-resource target language.
5. Instead, labeled data from a high-resource language is leveraged.
6. A multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition ( 2020) proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a significant boost in performance for classification in low-resource languages.
7. The transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages.
8. This is useful for standard word embeddings  as well as pre-trained language models.
9. For example, by aligning the languages inside a single multilin-
10. This alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping (Mikolov et al., 2013;Joulin et al., 2018).
11. This allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model (Cao et al., 2020).
12. For example, Zhang et al. (2019b) used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the highresource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.
13. Open Issues: While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold.
14. For example, mBERT covers 104 and XLM-R 100 languages, which is a third of all languages in Wikipedia as outlined earlier.
15. Further, Wu and Dredze (2020) showed that, in particular, low-resource languages are not well-represented in mBERT.
16. Figure 2 shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa 2 .
17. In particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages.
18. This can be problematic, as languages from more distant language families are less suited for transfer learning, as Lauscher et al.(2020) showed."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s1,Definitions of Robustness in NLP,"['p1.0', 'p1.1']","[""Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2)."", 'The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.']","Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).

The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.","(p1.0) Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ]. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).

(p1.1) The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x ) and y when both are open-ended texts.","[['b40'], ['b5', None, 'b18']]","[['b40'], ['b5', None, 'b18']]",4,"1. Robustness, despite its specific definitions in various lines of research, can typically be unified as follows: denote the input as x, and its associated gold label for the main task as y, assume a model f is trained on (x, y) ∼ D and its prediction over x as f (x); now given test data (x , y ) ∼ D = D, we can measure a model's robustness by its performance on D , e.g., using the model's robust accuracy (Tsipras et al., 2019;, defined as E (x ,y )∼D [f (x ) = y ].
2. Existing literature on robustness in NLP can be roughly categorized by how D is constructed: by synthetically perturbing the input (Section 2.1), or D is naturally occurring with a distribution shift (Section 2.2).
3. The above definition works for a range of NLP tasks like text classification and sequence labeling where y is defined over a fixed set of discrete labels.
4. For tasks like text generation, robustness is less well defined and can manifest as positional bias (Jung et al., 2019;Kryscinski et al., 2019), or hallucination (Maynez et al., 2020;Parikh et al., 2020;. One major challenge here is a lack of robust metrics in evaluating the quality of the generated text (Sellam et al., 2020;, i.e., we need a reliable metric to determine the relationship between f (x )
5. and y when both are open-ended texts."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s2,Robustness against Adversarial Attacks,"['p2.0', 'p2.1', 'p2.2']","[""In one line of research, D is constructed by perturbations around input x to form x (x typically being defined within some proximity of x). This topic has been widely explored in computer vision under the concept of adversarial robustness, which measures models' performances against carefully crafted noises generated deliberately to deceive the model to predict wrongly, pioneered by (Szegedy et al., 2013;Goodfellow et al., 2015), and later extended to NLP, such as (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022) and multilingual adversaries . The generation of adversarial examples primarily builds upon the observation that we can generate samples that are meaningful to humans (e.g., by perturbing the samples with changes that are imperceptible to humans) while altering the prediction of the models for this sample. In this regard, human's remarkable ability in understanding a large set of synonyms (Li et al., 2020) or interesting characteristics in ignoring the exact order of letters  are often opportunities to create adversarial examples. A related line of work such as data-poisoning (Wallace et al., 2021) and weight-poisoning (Kurita et al., 2020) exposes NLP models' vulnerability against attacks during the training process. One can refer to more comprehensive reviews and broader discussions on this topic in Zhang et al. (2020c) and Morris et al. (2020b). Assumptions around Label-preserving and Semantic-preserving Most existing work in vision makes a relatively simplified assumption that the gold label of x remains unchanged under a bounded perturbation over x, i.e., y = y, and a model's robust behaviour should be f (x ) = y (Szegedy et al., 2013;Goodfellow et al., 2015). A similar line of work in NLP follows the same label-preserving assumption with small text perturbations like token and character swapping (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019;Ebrahimi et al., 2018), paraphrasing (Iyyer et al., 2018;Gan and Ng, 2019), semantically equivalent adversarial rules (Ribeiro et al., 2018), and adding distractors (Jia and Liang, 2017). However, this label-preserving assumption might not always hold, e.g., Wang et al. (2021b) studied several existing text perturbation techniques and found that a significant portion of perturbed examples are not label-preserving (despite their label-preserving assumptions), or the resulting labels have a high disagreement among human raters (i.e., can even fool humans). Morris et al. (2020a) also call for more attention to the validity of perturbed examples for a more accurate robustness evaluation."", ""Another line of work aims to perturb the input x to x in small but meaningful ways that explicitly change the gold label, i.e., y = y, under which case the robust behaviour of a model should be f (x ) = y and f (x ) = y (Gardner et al., 2020;Kaushik et al., 2019;Schlegel et al., 2021). We believe these two lines of work are complementary to each other, and both should be explored in future research to measure models' robustness more comprehensively."", 'One alternative notion is whether the perturbation from x to x is ""semantic-preseving"" (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019) or ""semantic-modifying"" (Shi and Huang, 2020;Jia and Liang, 2017). Note this is slightly different from the above label-preserving assumptions, as it is defined over the perturbations on (x, x ) rather than making an assumption on (y, y ), e.g., semantic-modifying perturbations can be either label-preserving (Jia and Liang, 2017;Huang, 2020) or label-changing (Gardner et al., 2020;Kaushik et al., 2019).']","In one line of research, D is constructed by perturbations around input x to form x (x typically being defined within some proximity of x). This topic has been widely explored in computer vision under the concept of adversarial robustness, which measures models' performances against carefully crafted noises generated deliberately to deceive the model to predict wrongly, pioneered by (Szegedy et al., 2013;Goodfellow et al., 2015), and later extended to NLP, such as (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022) and multilingual adversaries . The generation of adversarial examples primarily builds upon the observation that we can generate samples that are meaningful to humans (e.g., by perturbing the samples with changes that are imperceptible to humans) while altering the prediction of the models for this sample. In this regard, human's remarkable ability in understanding a large set of synonyms (Li et al., 2020) or interesting characteristics in ignoring the exact order of letters  are often opportunities to create adversarial examples. A related line of work such as data-poisoning (Wallace et al., 2021) and weight-poisoning (Kurita et al., 2020) exposes NLP models' vulnerability against attacks during the training process. One can refer to more comprehensive reviews and broader discussions on this topic in Zhang et al. (2020c) and Morris et al. (2020b). Assumptions around Label-preserving and Semantic-preserving Most existing work in vision makes a relatively simplified assumption that the gold label of x remains unchanged under a bounded perturbation over x, i.e., y = y, and a model's robust behaviour should be f (x ) = y (Szegedy et al., 2013;Goodfellow et al., 2015). A similar line of work in NLP follows the same label-preserving assumption with small text perturbations like token and character swapping (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019;Ebrahimi et al., 2018), paraphrasing (Iyyer et al., 2018;Gan and Ng, 2019), semantically equivalent adversarial rules (Ribeiro et al., 2018), and adding distractors (Jia and Liang, 2017). However, this label-preserving assumption might not always hold, e.g., Wang et al. (2021b) studied several existing text perturbation techniques and found that a significant portion of perturbed examples are not label-preserving (despite their label-preserving assumptions), or the resulting labels have a high disagreement among human raters (i.e., can even fool humans). Morris et al. (2020a) also call for more attention to the validity of perturbed examples for a more accurate robustness evaluation.

Another line of work aims to perturb the input x to x in small but meaningful ways that explicitly change the gold label, i.e., y = y, under which case the robust behaviour of a model should be f (x ) = y and f (x ) = y (Gardner et al., 2020;Kaushik et al., 2019;Schlegel et al., 2021). We believe these two lines of work are complementary to each other, and both should be explored in future research to measure models' robustness more comprehensively.

One alternative notion is whether the perturbation from x to x is ""semantic-preseving"" (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019) or ""semantic-modifying"" (Shi and Huang, 2020;Jia and Liang, 2017). Note this is slightly different from the above label-preserving assumptions, as it is defined over the perturbations on (x, x ) rather than making an assumption on (y, y ), e.g., semantic-modifying perturbations can be either label-preserving (Jia and Liang, 2017;Huang, 2020) or label-changing (Gardner et al., 2020;Kaushik et al., 2019).","(p2.0) In one line of research, D is constructed by perturbations around input x to form x (x typically being defined within some proximity of x). This topic has been widely explored in computer vision under the concept of adversarial robustness, which measures models' performances against carefully crafted noises generated deliberately to deceive the model to predict wrongly, pioneered by (Szegedy et al., 2013;Goodfellow et al., 2015), and later extended to NLP, such as (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022) and multilingual adversaries . The generation of adversarial examples primarily builds upon the observation that we can generate samples that are meaningful to humans (e.g., by perturbing the samples with changes that are imperceptible to humans) while altering the prediction of the models for this sample. In this regard, human's remarkable ability in understanding a large set of synonyms (Li et al., 2020) or interesting characteristics in ignoring the exact order of letters  are often opportunities to create adversarial examples. A related line of work such as data-poisoning (Wallace et al., 2021) and weight-poisoning (Kurita et al., 2020) exposes NLP models' vulnerability against attacks during the training process. One can refer to more comprehensive reviews and broader discussions on this topic in Zhang et al. (2020c) and Morris et al. (2020b). Assumptions around Label-preserving and Semantic-preserving Most existing work in vision makes a relatively simplified assumption that the gold label of x remains unchanged under a bounded perturbation over x, i.e., y = y, and a model's robust behaviour should be f (x ) = y (Szegedy et al., 2013;Goodfellow et al., 2015). A similar line of work in NLP follows the same label-preserving assumption with small text perturbations like token and character swapping (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019;Ebrahimi et al., 2018), paraphrasing (Iyyer et al., 2018;Gan and Ng, 2019), semantically equivalent adversarial rules (Ribeiro et al., 2018), and adding distractors (Jia and Liang, 2017). However, this label-preserving assumption might not always hold, e.g., Wang et al. (2021b) studied several existing text perturbation techniques and found that a significant portion of perturbed examples are not label-preserving (despite their label-preserving assumptions), or the resulting labels have a high disagreement among human raters (i.e., can even fool humans). Morris et al. (2020a) also call for more attention to the validity of perturbed examples for a more accurate robustness evaluation.

(p2.1) Another line of work aims to perturb the input x to x in small but meaningful ways that explicitly change the gold label, i.e., y = y, under which case the robust behaviour of a model should be f (x ) = y and f (x ) = y (Gardner et al., 2020;Kaushik et al., 2019;Schlegel et al., 2021). We believe these two lines of work are complementary to each other, and both should be explored in future research to measure models' robustness more comprehensively.

(p2.2) One alternative notion is whether the perturbation from x to x is ""semantic-preseving"" (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019) or ""semantic-modifying"" (Shi and Huang, 2020;Jia and Liang, 2017). Note this is slightly different from the above label-preserving assumptions, as it is defined over the perturbations on (x, x ) rather than making an assumption on (y, y ), e.g., semantic-modifying perturbations can be either label-preserving (Jia and Liang, 2017;Huang, 2020) or label-changing (Gardner et al., 2020;Kaushik et al., 2019).","[['b63', 'b6', 'b80', 'b23', 'b24', 'b7', None, 'b52', 'b50', 'b74'], [None, 'b1'], [None, 'b1']]","[['b63', 'b6', 'b80', 'b23', 'b24', 'b7', None, 'b52', 'b50', 'b74'], [None, 'b1'], [None, 'b1']]",14,"1. In one line of research, D is constructed by perturbations around input x to form x (x typically being defined within some proximity of x).
2. This topic has been widely explored in computer vision under the concept of adversarial robustness, which measures models' performances against carefully crafted noises generated deliberately to deceive the model to predict wrongly, pioneered by (Szegedy et al., 2013;Goodfellow et al., 2015), and later extended to NLP, such as (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022) and multilingual adversaries .
3. The generation of adversarial examples primarily builds upon the observation that we can generate samples that are meaningful to humans (e.g., by perturbing the samples with changes that are imperceptible to humans) while altering the prediction of the models for this sample.
4. In this regard, human's remarkable ability in understanding a large set of synonyms (Li et al., 2020) or interesting characteristics in ignoring the exact order of letters  are often opportunities to create adversarial examples.
5. A related line of work such as data-poisoning (Wallace et al., 2021) and weight-poisoning (Kurita et al., 2020) exposes NLP models' vulnerability against attacks during the training process.
6. One can refer to more comprehensive reviews and broader discussions on this topic in Zhang et al. (2020c) and Morris et al. (2020b).
7. Assumptions around Label-preserving and Semantic-preserving Most existing work in vision makes a relatively simplified assumption that the gold label of x remains unchanged under a bounded perturbation over x, i.e., y = y, and a model's robust behaviour should be f (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022)0 = y (Szegedy et al., 2013;Goodfellow et al., 2015).
8. A similar line of work in NLP follows the same label-preserving assumption with small text perturbations like token and character swapping (Alzantot et al., 2018;Jin et al., 2020;Ren et al., 2019;Ebrahimi et al., 2018), paraphrasing (Iyyer et al., 2018;Gan and Ng, 2019), semantically equivalent adversarial rules (Ribeiro et al., 2018), and adding distractors (Jia and Liang, 2017).
9. However, this label-preserving assumption might not always hold, e.g., Wang et al. (2021b) studied several existing text perturbation techniques and found that a significant portion of perturbed examples are not label-preserving (despite their label-preserving assumptions), or the resulting labels have a high disagreement among human raters (i.e., can even fool humans).
10. Morris et al. (2020a) also call for more attention to the validity of perturbed examples for a more accurate robustness evaluation.
11. Another line of work aims to perturb the input x to x in small but meaningful ways that explicitly change the gold label, i.e., y = y, under which case the robust behaviour of a model should be f (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022)0 = y and f (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022)0 = y (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022)1.
12. We believe these two lines of work are complementary to each other, and both should be explored in future research to measure models' robustness more comprehensively.
13. One alternative notion is whether the perturbation from x to x is ""semantic-preseving"" (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022)2 or ""semantic-modifying"" (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022)3.
14. Note this is slightly different from the above label-preserving assumptions, as it is defined over the perturbations on (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022)4 rather than making an assumption on (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022)5, e.g., semantic-modifying perturbations can be either label-preserving (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022)6 or label-changing (Ebrahimi et al., 2018;Alzantot et al., 2018;Li et al., 2019;Feng et al., 2018;Kuleshov et al., 2018;Jia et al., 2019;Zang et al., 2020;Pruthi et al., 2019;Wang et al., 2019e;Garg and Ramakrishnan, 2020;Tan et al., 2020a,b;Schwinn et al., 2021;Li et al., 2021;Boucher et al., 2022)7."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s3,Robustness under Distribution Shift,['p3.0'],"[""Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-jussà, 2019).""]","Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-jussà, 2019).","(p3.0) Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020). Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Zhao et al., 2018a;Rudinger et al., 2017), occupation classification (De-Arteaga et al., 2019), and neural machine translation (Prates et al., 2019;Font and Costa-jussà, 2019).","[['b26', 'b21', 'b33', None, 'b67', 'b83']]","[['b26', 'b21', 'b33', None, 'b67', 'b83']]",6,"1. Another line of research focuses on (x , y ) drawn from a different distribution that is naturallyoccurring (Hendrycks et al., 2021), where robustness can be defined around model's performance under distribution shift.
2. Different from work on domain adaptation (Patel et al., 2015;Wilson and Cook, 2020) and transfer learning (Pan and Yang, 2010), existing definitions of robustness are closer to the concept of domain generalization (Muandet et al., 2013;Gulrajani and Lopez-Paz, 2021), or out-of-distribution generalization to unforeseen distribution shifts (Hendrycks et al., 2020a), where the test data (either labeled or unlabeled) is assumed not available during training, i.e., generalization without adaptation.
3. In the context of NLP, robustness to natural distribution shifts can also mean models' performance should not degrade due to the differences in grammar errors, dialects, speakers, languages (Craig and Washington, 2002;Blodgett et al., 2016;Demszky et al., 2021), or newly collected datasets for the same task but in different domains (Miller et al., 2020).
4. Another closely connected line of research is fairness, which has been studied in various NLP applications, see (Sun et al., 2019) for a more in-depth survey in this area.
5. For example, gendered stereotypes or biases have been observed in NLP tasks including co-reference resolution (Hendrycks et al., 2021)0, occupation classification (Hendrycks et al., 2021)1, and neural machine translation (Hendrycks et al., 2021)2."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s4,Connections and A Common Theme,"['p4.0', 'p4.1', 'p4.2']","[""The above two categories of robustness can be unified under the same framework, i.e., whether D represents a synthetic distribution shift (via adversarial attacks) or a natural distribution shift. Existing work has shown a model's performance might degrade substantially in both cases, but the transferability of the two categories is relatively underexplored. In the vision domain, Taori et al. (2020) investigate models' robustness to natural distribution shift, and show that robustness to synthetic distribution shift might offer little to no robustness improvement under natural distribution shift. Some studies show NLP models might not generalize to unseen adversarial patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), but more work is needed to systematically bridge the gap between NLP models' robustness under natural and synthetic distribution shifts."", 'To better understand why models exhibit a lack of robustness, some existing work attributed this to the fact that models sometimes utilize spurious correlations between input features and labels, rather than the genuine ones, where spurious features are commonly defined as features that do not causally affect a task\'s label (Srivastava et al., 2020;Wang and Culotta, 2020b): they correlate with task labels but fail to transfer to more challenging test conditions or out-of-distribution data (Geirhos et al., 2020). Some other work defined it as ""prediction rules that work for the majority examples but do not hold in general"" (Tu et al., 2020). Such spurious correlations are sometimes referred as dataset bias (Clark et al., 2019;He et al., 2019), annotation artifacts (Gururangan et al., 2018), or group shift (Oren et al., 2019) in the literature. Further, evidence showed that controlling model\'s learning in spurious features will improve model\'s performances in distribution shifts (Wang et al., 2019a,b); also, discussions on the connections between adversarial robustness and learning of spurious features have been raised (Ilyas et al., 2019;. Theoretical discussions connecting these fields have also been offered by crediting a reason of model\'s lack of robustness in either distribution shift or adversarial attack to model\'s learning of spurious features (Wang et al., 2021c).', 'Further, in certain applications, model ""robustness"" can also be connected with models\' instability (Milani Fard et al., 2016), or models having poorly-calibrated uncertainty estimation (Guo et al., 2017), where Bayesian methods (Graves, 2011;Blundell et al., 2015), dropout-based (Gal and Ghahramani, 2016;Kingma et al., 2015) and ensemble-based approaches (Lakshminarayanan et al., 2017) have been proposed to improve models\' uncertainty estimation. Recently, Ovadia et al. (2019) have shown models\' uncertainty estimation can degrade significantly under distributional shift, and call for more work to ensure a model ""knows when it doesn\'t know"" by giving lower uncertainty estimates over out-of-distribution data. This is another example where models can be less robust under distributional shifts, and again emphasizes the need of building more unified benchmarks to measure a model\'s performance (e.g., robust accuracy, calibration, stability) under distribution shifts, in addition to in-distribution accuracy.']","The above two categories of robustness can be unified under the same framework, i.e., whether D represents a synthetic distribution shift (via adversarial attacks) or a natural distribution shift. Existing work has shown a model's performance might degrade substantially in both cases, but the transferability of the two categories is relatively underexplored. In the vision domain, Taori et al. (2020) investigate models' robustness to natural distribution shift, and show that robustness to synthetic distribution shift might offer little to no robustness improvement under natural distribution shift. Some studies show NLP models might not generalize to unseen adversarial patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), but more work is needed to systematically bridge the gap between NLP models' robustness under natural and synthetic distribution shifts.

To better understand why models exhibit a lack of robustness, some existing work attributed this to the fact that models sometimes utilize spurious correlations between input features and labels, rather than the genuine ones, where spurious features are commonly defined as features that do not causally affect a task's label (Srivastava et al., 2020;Wang and Culotta, 2020b): they correlate with task labels but fail to transfer to more challenging test conditions or out-of-distribution data (Geirhos et al., 2020). Some other work defined it as ""prediction rules that work for the majority examples but do not hold in general"" (Tu et al., 2020). Such spurious correlations are sometimes referred as dataset bias (Clark et al., 2019;He et al., 2019), annotation artifacts (Gururangan et al., 2018), or group shift (Oren et al., 2019) in the literature. Further, evidence showed that controlling model's learning in spurious features will improve model's performances in distribution shifts (Wang et al., 2019a,b); also, discussions on the connections between adversarial robustness and learning of spurious features have been raised (Ilyas et al., 2019;. Theoretical discussions connecting these fields have also been offered by crediting a reason of model's lack of robustness in either distribution shift or adversarial attack to model's learning of spurious features (Wang et al., 2021c).

Further, in certain applications, model ""robustness"" can also be connected with models' instability (Milani Fard et al., 2016), or models having poorly-calibrated uncertainty estimation (Guo et al., 2017), where Bayesian methods (Graves, 2011;Blundell et al., 2015), dropout-based (Gal and Ghahramani, 2016;Kingma et al., 2015) and ensemble-based approaches (Lakshminarayanan et al., 2017) have been proposed to improve models' uncertainty estimation. Recently, Ovadia et al. (2019) have shown models' uncertainty estimation can degrade significantly under distributional shift, and call for more work to ensure a model ""knows when it doesn't know"" by giving lower uncertainty estimates over out-of-distribution data. This is another example where models can be less robust under distributional shifts, and again emphasizes the need of building more unified benchmarks to measure a model's performance (e.g., robust accuracy, calibration, stability) under distribution shifts, in addition to in-distribution accuracy.","(p4.0) The above two categories of robustness can be unified under the same framework, i.e., whether D represents a synthetic distribution shift (via adversarial attacks) or a natural distribution shift. Existing work has shown a model's performance might degrade substantially in both cases, but the transferability of the two categories is relatively underexplored. In the vision domain, Taori et al. (2020) investigate models' robustness to natural distribution shift, and show that robustness to synthetic distribution shift might offer little to no robustness improvement under natural distribution shift. Some studies show NLP models might not generalize to unseen adversarial patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), but more work is needed to systematically bridge the gap between NLP models' robustness under natural and synthetic distribution shifts.

(p4.1) To better understand why models exhibit a lack of robustness, some existing work attributed this to the fact that models sometimes utilize spurious correlations between input features and labels, rather than the genuine ones, where spurious features are commonly defined as features that do not causally affect a task's label (Srivastava et al., 2020;Wang and Culotta, 2020b): they correlate with task labels but fail to transfer to more challenging test conditions or out-of-distribution data (Geirhos et al., 2020). Some other work defined it as ""prediction rules that work for the majority examples but do not hold in general"" (Tu et al., 2020). Such spurious correlations are sometimes referred as dataset bias (Clark et al., 2019;He et al., 2019), annotation artifacts (Gururangan et al., 2018), or group shift (Oren et al., 2019) in the literature. Further, evidence showed that controlling model's learning in spurious features will improve model's performances in distribution shifts (Wang et al., 2019a,b); also, discussions on the connections between adversarial robustness and learning of spurious features have been raised (Ilyas et al., 2019;. Theoretical discussions connecting these fields have also been offered by crediting a reason of model's lack of robustness in either distribution shift or adversarial attack to model's learning of spurious features (Wang et al., 2021c).

(p4.2) Further, in certain applications, model ""robustness"" can also be connected with models' instability (Milani Fard et al., 2016), or models having poorly-calibrated uncertainty estimation (Guo et al., 2017), where Bayesian methods (Graves, 2011;Blundell et al., 2015), dropout-based (Gal and Ghahramani, 2016;Kingma et al., 2015) and ensemble-based approaches (Lakshminarayanan et al., 2017) have been proposed to improve models' uncertainty estimation. Recently, Ovadia et al. (2019) have shown models' uncertainty estimation can degrade significantly under distributional shift, and call for more work to ensure a model ""knows when it doesn't know"" by giving lower uncertainty estimates over out-of-distribution data. This is another example where models can be less robust under distributional shifts, and again emphasizes the need of building more unified benchmarks to measure a model's performance (e.g., robust accuracy, calibration, stability) under distribution shifts, in addition to in-distribution accuracy.","[[None, 'b39'], ['b32', None, 'b30', 'b55', 'b65'], ['b20', None, 'b4', 'b9']]","[[None, 'b39'], ['b32', None, 'b30', 'b55', 'b65'], ['b20', None, 'b4', 'b9']]",11,"1. The above two categories of robustness can be unified under the same framework, i.e., whether D represents a synthetic distribution shift (via adversarial attacks) or a natural distribution shift.
2. Existing work has shown a model's performance might degrade substantially in both cases, but the transferability of the two categories is relatively underexplored.
3. In the vision domain, Taori et al. (2020) investigate models' robustness to natural distribution shift, and show that robustness to synthetic distribution shift might offer little to no robustness improvement under natural distribution shift.
4. Some studies show NLP models might not generalize to unseen adversarial patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), but more work is needed to systematically bridge the gap between NLP models' robustness under natural and synthetic distribution shifts.
5. To better understand why models exhibit a lack of robustness, some existing work attributed this to the fact that models sometimes utilize spurious correlations between input features and labels, rather than the genuine ones, where spurious features are commonly defined as features that do not causally affect a task's label (Srivastava et al., 2020;Wang and Culotta, 2020b): they correlate with task labels but fail to transfer to more challenging test conditions or out-of-distribution data (Geirhos et al., 2020).
6. Some other work defined it as ""prediction rules that work for the majority examples but do not hold in general"" (Tu et al., 2020).
7. Such spurious correlations are sometimes referred as dataset bias (Clark et al., 2019;He et al., 2019), annotation artifacts (Gururangan et al., 2018), or group shift (Oren et al., 2019) in the literature.
8. Further, evidence showed that controlling model's learning in spurious features will improve model's performances in distribution shifts (Wang et al., 2019a,b); also, discussions on the connections between adversarial robustness and learning of spurious features have been raised (2020)0.
9. Further, in certain applications, model ""robustness"" can also be connected with models' instability (2020)1, or models having poorly-calibrated uncertainty estimation (2020)2, where Bayesian methods (2020)3, dropout-based (2020)4 and ensemble-based approaches (2020)5 have been proposed to improve models' uncertainty estimation.
10. Recently, Ovadia et al. (2020)6 have shown models' uncertainty estimation can degrade significantly under distributional shift, and call for more work to ensure a model ""knows when it doesn't know"" by giving lower uncertainty estimates over out-of-distribution data.
11. This is another example where models can be less robust under distributional shifts, and again emphasizes the need of building more unified benchmarks to measure a model's performance (2020)7 under distribution shifts, in addition to in-distribution accuracy."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s6,Continuous vs. Discrete in Search Space,"['p6.0', 'p6.1', 'p6.2']","['The most obvious characteristic is probably the discrete nature of the space of text. This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP (Lei et al., 2019;Zhang et al., 2020c), in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space, and multiple novel attack methods are proposed to fill the gap, as we will discuss in later sections.', 'Perceptible to Human vs. Not On a related topic, one of the most impressive property of ad-versarial attack in vision is that small perturbation of the image data imperceptible to human are sufficient to deceive the model (Szegedy et al., 2013), while this can hardly be true for NLP attacks. Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible). On the other hand, there are ways to generate samples where the changes, although being perceptible, are often ignored by human brain due to some psychological prior on how a human processes the text (Anastasopoulos et al., 2019;. Support vs. Density Difference of the Data Distributions Another difference is more likely seen in the discussion of the domain adaptation of vision and NLP study. In vision study, although the images from training distribution and test distribution can be sufficiently different, the train and test distributions mostly share the same support (the pixels are always sampled from a 0-255 integer space), although the density of these distributions can be very different (e.g., photos vs. sketches). On the other hand, domain adaptation of NLP sometimes studies the regime where the supports of the data differ, e.g., the vocabularies can be significantly different in cross-lingual studies (Abad et al., 2020;Zhang et al., 2020a).', ""A Common Theme Despite the disparities between vision and NLP, the common theme of pushing the model to generalize from D to D preserves. The practical difference between D and D is more than often defined by the human's understanding of the data, and can differ in vision and NLP as humans perceive and process images and texts in subtly different ways, which creates both opportunities for learning and barriers for direct transfer. Certain lines of research try to bridge the learning in the vision domain to the embedding space in the NLP domain, while other lines of research create more interpretable attacks in the discrete text space (see Table 1 for these two lines of work). How those two lines of research transfer to each other, or complement each other, is not fully explored and calls for additional research.""]","The most obvious characteristic is probably the discrete nature of the space of text. This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP (Lei et al., 2019;Zhang et al., 2020c), in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space, and multiple novel attack methods are proposed to fill the gap, as we will discuss in later sections.

Perceptible to Human vs. Not On a related topic, one of the most impressive property of ad-versarial attack in vision is that small perturbation of the image data imperceptible to human are sufficient to deceive the model (Szegedy et al., 2013), while this can hardly be true for NLP attacks. Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible). On the other hand, there are ways to generate samples where the changes, although being perceptible, are often ignored by human brain due to some psychological prior on how a human processes the text (Anastasopoulos et al., 2019;. Support vs. Density Difference of the Data Distributions Another difference is more likely seen in the discussion of the domain adaptation of vision and NLP study. In vision study, although the images from training distribution and test distribution can be sufficiently different, the train and test distributions mostly share the same support (the pixels are always sampled from a 0-255 integer space), although the density of these distributions can be very different (e.g., photos vs. sketches). On the other hand, domain adaptation of NLP sometimes studies the regime where the supports of the data differ, e.g., the vocabularies can be significantly different in cross-lingual studies (Abad et al., 2020;Zhang et al., 2020a).

A Common Theme Despite the disparities between vision and NLP, the common theme of pushing the model to generalize from D to D preserves. The practical difference between D and D is more than often defined by the human's understanding of the data, and can differ in vision and NLP as humans perceive and process images and texts in subtly different ways, which creates both opportunities for learning and barriers for direct transfer. Certain lines of research try to bridge the learning in the vision domain to the embedding space in the NLP domain, while other lines of research create more interpretable attacks in the discrete text space (see Table 1 for these two lines of work). How those two lines of research transfer to each other, or complement each other, is not fully explored and calls for additional research.","(p6.0) The most obvious characteristic is probably the discrete nature of the space of text. This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP (Lei et al., 2019;Zhang et al., 2020c), in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space, and multiple novel attack methods are proposed to fill the gap, as we will discuss in later sections.

(p6.1) Perceptible to Human vs. Not On a related topic, one of the most impressive property of ad-versarial attack in vision is that small perturbation of the image data imperceptible to human are sufficient to deceive the model (Szegedy et al., 2013), while this can hardly be true for NLP attacks. Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible). On the other hand, there are ways to generate samples where the changes, although being perceptible, are often ignored by human brain due to some psychological prior on how a human processes the text (Anastasopoulos et al., 2019;. Support vs. Density Difference of the Data Distributions Another difference is more likely seen in the discussion of the domain adaptation of vision and NLP study. In vision study, although the images from training distribution and test distribution can be sufficiently different, the train and test distributions mostly share the same support (the pixels are always sampled from a 0-255 integer space), although the density of these distributions can be very different (e.g., photos vs. sketches). On the other hand, domain adaptation of NLP sometimes studies the regime where the supports of the data differ, e.g., the vocabularies can be significantly different in cross-lingual studies (Abad et al., 2020;Zhang et al., 2020a).

(p6.2) A Common Theme Despite the disparities between vision and NLP, the common theme of pushing the model to generalize from D to D preserves. The practical difference between D and D is more than often defined by the human's understanding of the data, and can differ in vision and NLP as humans perceive and process images and texts in subtly different ways, which creates both opportunities for learning and barriers for direct transfer. Certain lines of research try to bridge the learning in the vision domain to the embedding space in the NLP domain, while other lines of research create more interpretable attacks in the discrete text space (see Table 1 for these two lines of work). How those two lines of research transfer to each other, or complement each other, is not fully explored and calls for additional research.","[['b80', 'b11'], [None, 'b0', 'b76'], []]","[['b80', 'b11'], [None, 'b0', 'b76'], []]",5,"1. The most obvious characteristic is probably the discrete nature of the space of text.
2. This particularly posed a challenge towards the adversarial attack and defense regime when the study in vision is transferred to NLP (Lei et al., 2019;Zhang et al., 2020c), in the sense that simple gradient-based adversarial attacks will not directly translate to meaningful attacks in the discrete text space, and multiple novel attack methods are proposed to fill the gap, as we will discuss in later sections.
3. Perceptible to Human vs. Not On a related topic, one of the most impressive property of ad-versarial attack in vision is that small perturbation of the image data imperceptible to human are sufficient to deceive the model (Szegedy et al., 2013), while this can hardly be true for NLP attacks.
4. Instead of being imperceptible, the adversarial attacks in NLP typically are bounded by the fact that the meaning of the sentences are not altered (despite being perceptible).
5. On the other hand, there are ways to generate samples where the changes, although being perceptible, are often ignored by human brain due to some psychological prior on how a human processes the text (Anastasopoulos et al., 2019;. Support vs. Density Difference of the Data Distributions Another difference is more likely seen in the discussion of the domain adaptation of vision and NLP study. In vision study, although the images from training distribution and test distribution can be sufficiently different, the train and test distributions mostly share the same support (the pixels are always sampled from a 0-255 integer space), although the density of these distributions can be very different (e.g., photos vs. sketches).
6. On the other hand, domain adaptation of NLP sometimes studies the regime where the supports of the data differ, e.g., the vocabularies can be significantly different in cross-lingual studies (Abad et al., 2020;Zhang et al., 2020a).
7. A Common Theme Despite the disparities between vision and NLP, the common theme of pushing the model to generalize from D to D preserves.
8. The practical difference between D and D is more than often defined by the human's understanding of the data, and can differ in vision and NLP as humans perceive and process images and texts in subtly different ways, which creates both opportunities for learning and barriers for direct transfer.
9. Certain lines of research try to bridge the learning in the vision domain to the embedding space in the NLP domain, while other lines of research create more interpretable attacks in the discrete text space (see Table 1 for these two lines of work).
10. How those two lines of research transfer to each other, or complement each other, is not fully explored and calls for additional research."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s8,Human Prior and Error Analyses Driven,"['p8.0', 'p8.1', 'p8.2', 'p8.3']","['An increasing body of work has been conducted on understanding and measuring robustness in NLP models (Tu et al., 2020;Sagawa et al., 2020b;Geirhos et al., 2020) across various NLP tasks, largely relying on human priors and error analyses.', 'Natural Language Inference Naik et al. (2018) sampled misclassified examples and analyzed their potential sources of errors, which are then grouped into a typology of common reasons for error. Such error types then served as the bases to construct the stress test set, to further evaluate whether NLI models have the ability to make real inferential decisions, or simply rely on sophisticated pattern matching. Gururangan et al. (2018) found that current NLI models are likely to identify the label by relying only on the hypothesis, and Poliak et al. (2018)     showed another approach by limiting the input space of the characters so that the models will be likely to perceive data typos and misspellings.', 'Syntactic and Semantic Parsing Robust parsing has been studied in several existing works (Lee et al., 1995;Aït-Mokhtar et al., 2002). More recent work showed that neural semantic parsers are still not robust against lexical and stylistic variations, or meaning-preserving perturbations (Marzinotto et al., 2019;Huang et al., 2021), and proposed ways to improve their robustness through data augmentation (Huang et al., 2021) and adversarial learning (Marzinotto et al., 2019).', ""Text Generation Existing work found that text generation models also suffer from robustness issues, e.g., text summarization models suffer from positional bias (Jung et al., 2019), layout bias (Kryscinski et al., 2019), and a lack of faithfulness and factuality (Kryscinski et al., 2019;Maynez et al., 2020;Chen et al., 2021b); data-to-text models sometimes hallucinate texts that are not supported by the data (Parikh et al., 2020;Wang et al., 2020d). In addition, Sellam et al. (2020);  pointed out the deficiency of existing automatic evaluation metrics and proposed new metrics to better align the generation quality with human judgements.  (2019) show that commonly used crowdsourced datasets for training NLI models might make certain syntactic heuristics more easily adopted by statistical learners. Further, Bras et al. (2020) propose to use a lightweight adversarial filtering approach to filter dataset biases, which is approximated using each instance's predictability score.""]","An increasing body of work has been conducted on understanding and measuring robustness in NLP models (Tu et al., 2020;Sagawa et al., 2020b;Geirhos et al., 2020) across various NLP tasks, largely relying on human priors and error analyses.

Natural Language Inference Naik et al. (2018) sampled misclassified examples and analyzed their potential sources of errors, which are then grouped into a typology of common reasons for error. Such error types then served as the bases to construct the stress test set, to further evaluate whether NLI models have the ability to make real inferential decisions, or simply rely on sophisticated pattern matching. Gururangan et al. (2018) found that current NLI models are likely to identify the label by relying only on the hypothesis, and Poliak et al. (2018)     showed another approach by limiting the input space of the characters so that the models will be likely to perceive data typos and misspellings.

Syntactic and Semantic Parsing Robust parsing has been studied in several existing works (Lee et al., 1995;Aït-Mokhtar et al., 2002). More recent work showed that neural semantic parsers are still not robust against lexical and stylistic variations, or meaning-preserving perturbations (Marzinotto et al., 2019;Huang et al., 2021), and proposed ways to improve their robustness through data augmentation (Huang et al., 2021) and adversarial learning (Marzinotto et al., 2019).

Text Generation Existing work found that text generation models also suffer from robustness issues, e.g., text summarization models suffer from positional bias (Jung et al., 2019), layout bias (Kryscinski et al., 2019), and a lack of faithfulness and factuality (Kryscinski et al., 2019;Maynez et al., 2020;Chen et al., 2021b); data-to-text models sometimes hallucinate texts that are not supported by the data (Parikh et al., 2020;Wang et al., 2020d). In addition, Sellam et al. (2020);  pointed out the deficiency of existing automatic evaluation metrics and proposed new metrics to better align the generation quality with human judgements.  (2019) show that commonly used crowdsourced datasets for training NLI models might make certain syntactic heuristics more easily adopted by statistical learners. Further, Bras et al. (2020) propose to use a lightweight adversarial filtering approach to filter dataset biases, which is approximated using each instance's predictability score.","(p8.0) An increasing body of work has been conducted on understanding and measuring robustness in NLP models (Tu et al., 2020;Sagawa et al., 2020b;Geirhos et al., 2020) across various NLP tasks, largely relying on human priors and error analyses.

(p8.1) Natural Language Inference Naik et al. (2018) sampled misclassified examples and analyzed their potential sources of errors, which are then grouped into a typology of common reasons for error. Such error types then served as the bases to construct the stress test set, to further evaluate whether NLI models have the ability to make real inferential decisions, or simply rely on sophisticated pattern matching. Gururangan et al. (2018) found that current NLI models are likely to identify the label by relying only on the hypothesis, and Poliak et al. (2018)     showed another approach by limiting the input space of the characters so that the models will be likely to perceive data typos and misspellings.

(p8.2) Syntactic and Semantic Parsing Robust parsing has been studied in several existing works (Lee et al., 1995;Aït-Mokhtar et al., 2002). More recent work showed that neural semantic parsers are still not robust against lexical and stylistic variations, or meaning-preserving perturbations (Marzinotto et al., 2019;Huang et al., 2021), and proposed ways to improve their robustness through data augmentation (Huang et al., 2021) and adversarial learning (Marzinotto et al., 2019).

(p8.3) Text Generation Existing work found that text generation models also suffer from robustness issues, e.g., text summarization models suffer from positional bias (Jung et al., 2019), layout bias (Kryscinski et al., 2019), and a lack of faithfulness and factuality (Kryscinski et al., 2019;Maynez et al., 2020;Chen et al., 2021b); data-to-text models sometimes hallucinate texts that are not supported by the data (Parikh et al., 2020;Wang et al., 2020d). In addition, Sellam et al. (2020);  pointed out the deficiency of existing automatic evaluation metrics and proposed new metrics to better align the generation quality with human judgements.  (2019) show that commonly used crowdsourced datasets for training NLI models might make certain syntactic heuristics more easily adopted by statistical learners. Further, Bras et al. (2020) propose to use a lightweight adversarial filtering approach to filter dataset biases, which is approximated using each instance's predictability score.","[[None], ['b27', None], [None, 'b10', 'b17'], ['b5', None, 'b66', 'b18']]","[[None], ['b27', None], [None, 'b10', 'b17'], ['b5', None, 'b66', 'b18']]",10,"1. An increasing body of work has been conducted on understanding and measuring robustness in NLP models (Tu et al., 2020;Sagawa et al., 2020b;Geirhos et al., 2020) across various NLP tasks, largely relying on human priors and error analyses.
2. Natural Language Inference Naik et al. (2018) sampled misclassified examples and analyzed their potential sources of errors, which are then grouped into a typology of common reasons for error.
3. Such error types then served as the bases to construct the stress test set, to further evaluate whether NLI models have the ability to make real inferential decisions, or simply rely on sophisticated pattern matching.
4. Gururangan et al. (2018) found that current NLI models are likely to identify the label by relying only on the hypothesis, and Poliak et al. (2018)     showed another approach by limiting the input space of the characters so that the models will be likely to perceive data typos and misspellings.
5. Syntactic and Semantic Parsing Robust parsing has been studied in several existing works (Lee et al., 1995;Aït-Mokhtar et al., 2002).
6. More recent work showed that neural semantic parsers are still not robust against lexical and stylistic variations, or meaning-preserving perturbations (Marzinotto et al., 2019;Huang et al., 2021), and proposed ways to improve their robustness through data augmentation (Huang et al., 2021) and adversarial learning (Marzinotto et al., 2019).Text Generation Existing work found that text generation models also suffer from robustness issues, e.g., text summarization models suffer from positional bias (Jung et al., 2019), layout bias (Kryscinski et al., 2019), and a lack of faithfulness and factuality (Kryscinski et al., 2019;Maynez et al., 2020;Chen et al., 2021b); data-to-text models sometimes hallucinate texts that are not supported by the data (Parikh et al., 2020;Wang et al., 2020d).
7. In addition, Sellam et al. (2020);  pointed out the deficiency of existing automatic evaluation metrics and proposed new metrics to better align the generation quality with human judgements.
8. (2019) show that commonly used crowdsourced datasets for training NLI models might make certain syntactic heuristics more easily adopted by statistical learners.
9. Further, Bras et al. (2020) propose to use a lightweight adversarial filtering approach to filter dataset biases, which is approximated using each instance's predictability score."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s11,Model-in-the-loop vs.,['p11.0'],"['Human-in-the-loop Some work adopts human-in-the-loop to generate challenging examples, e.g., Counterfacutal-NLI (Kaushik et al., 2019) and Natural-Perturbed-QA (Khashabi et al., 2020). Other work applies modelin-the-loop to increase the likelihood that the perturbed examples are challenging for state-of-the-art models, but it might also introduce biases towards the particular model used. For example, SWAG (Zellers et al., 2018) was introduced that fooled most models at the time of publishing but was soon ""solved"" after BERT (Devlin et al., 2019) was introduced. As a result, Yuan et al. (2021) present a study over the transferability of adversarial examples, and Contrast Sets (Gardner et al., 2020) intentionally avoid using model-in-the-loop. Further, more recent work adopts adversarial humanand-model-in-the-loop to create more difficult examples for benchmarking, e.g., Adv-QA (Bartolo et al., 2020), Adv-Quizbowl (Wallace et al., 2019b), ANLI (Nie et al., 2020), and Dynabench (Kiela et al., 2021).']","Human-in-the-loop Some work adopts human-in-the-loop to generate challenging examples, e.g., Counterfacutal-NLI (Kaushik et al., 2019) and Natural-Perturbed-QA (Khashabi et al., 2020). Other work applies modelin-the-loop to increase the likelihood that the perturbed examples are challenging for state-of-the-art models, but it might also introduce biases towards the particular model used. For example, SWAG (Zellers et al., 2018) was introduced that fooled most models at the time of publishing but was soon ""solved"" after BERT (Devlin et al., 2019) was introduced. As a result, Yuan et al. (2021) present a study over the transferability of adversarial examples, and Contrast Sets (Gardner et al., 2020) intentionally avoid using model-in-the-loop. Further, more recent work adopts adversarial humanand-model-in-the-loop to create more difficult examples for benchmarking, e.g., Adv-QA (Bartolo et al., 2020), Adv-Quizbowl (Wallace et al., 2019b), ANLI (Nie et al., 2020), and Dynabench (Kiela et al., 2021).","(p11.0) Human-in-the-loop Some work adopts human-in-the-loop to generate challenging examples, e.g., Counterfacutal-NLI (Kaushik et al., 2019) and Natural-Perturbed-QA (Khashabi et al., 2020). Other work applies modelin-the-loop to increase the likelihood that the perturbed examples are challenging for state-of-the-art models, but it might also introduce biases towards the particular model used. For example, SWAG (Zellers et al., 2018) was introduced that fooled most models at the time of publishing but was soon ""solved"" after BERT (Devlin et al., 2019) was introduced. As a result, Yuan et al. (2021) present a study over the transferability of adversarial examples, and Contrast Sets (Gardner et al., 2020) intentionally avoid using model-in-the-loop. Further, more recent work adopts adversarial humanand-model-in-the-loop to create more difficult examples for benchmarking, e.g., Adv-QA (Bartolo et al., 2020), Adv-Quizbowl (Wallace et al., 2019b), ANLI (Nie et al., 2020), and Dynabench (Kiela et al., 2021).","[['b48', 'b75', 'b29', 'b3', 'b73', 'b1', 'b2']]","[['b48', 'b75', 'b29', 'b3', 'b73', 'b1', 'b2']]",7,"1. Human-in-the-loop Some work adopts human-in-the-loop to generate challenging examples, e.g., Counterfacutal-NLI (Kaushik et al., 2019) and Natural-Perturbed-QA (Khashabi et al., 2020).
2. Other work applies modelin-the-loop to increase the likelihood that the perturbed examples are challenging for state-of-the-art models, but it might also introduce biases towards the particular model used.
3. For example, SWAG (Zellers et al., 2018) was introduced that fooled most models at the time of publishing but was soon ""solved"" after BERT (Devlin et al., 2019) was introduced.
4. As a result, Yuan et al. (2021) present a study over the transferability of adversarial examples, and Contrast Sets (Gardner et al., 2020) intentionally avoid using model-in-the-loop.
5. Further, more recent work adopts adversarial humanand-model-in-the-loop to create more difficult examples for benchmarking, e.g., Adv-QA (Bartolo et al., 2020), Adv-Quizbowl (Wallace et al., 2019b), ANLI (Nie et al., 2020), and Dynabench (Kiela et al., 2021)."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s13,Data-driven Approaches,"['p13.0', 'p13.1']","['Data augmentation recently gained a lot of interest, in improving performance in low-resourced language settings, few-shot learning, mitigating biases, and improving robustness in NLP models (Feng et al., 2021;Dhole et al., 2021). Techniques like Mixup (Zhang et al., 2018), MixText (Chen et al., 2020), CutOut (DeVries and Taylor, 2017), Aug-Mix (Hendrycks et al., 2020b), HiddenCut (Chen et al., 2021a), have been shown to substantially improve the robustness and the generalization of models. Such mitigation strategies are operated at the data level, and often hard to be interpreted in terms of how and why mitigation works.', 'Other lines of work deal with spans or regions associated within data points to prevent models from heavily relying on spurious patterns. To make NLP models more robust on sentiment analysis and NLI tasks, Kaushik et al. (2019) proposed curating counterfactually augmented data via a human-inthe-loop process, and showed that models trained on the combination of this augmented data and original data are less sensitive to spurious patterns. Differently, Wang et al. (2021d) performed strategic data augmentation to perturb the set of ""shortcuts"" that are automatically identified, and found that mitigating these leads to more robust models in multiple NLP tasks. This line of mitigation strategies closely relates to how spurious correlations can be measured and identified, as many of the challenging or adversarial examples (Table 1) can sometimes be used to augment the original model to improve its robustness, either in the discrete input space as additional training examples (Liu et al., 2019;Kaushik et al., 2019;Anastasopoulos et al., 2019;Vaibhav et al., 2019;Khashabi et al., 2020), or in the embedding space (Zhu et al., 2020;Zhao et al., 2018b;Miyato et al., 2017;.']","Data augmentation recently gained a lot of interest, in improving performance in low-resourced language settings, few-shot learning, mitigating biases, and improving robustness in NLP models (Feng et al., 2021;Dhole et al., 2021). Techniques like Mixup (Zhang et al., 2018), MixText (Chen et al., 2020), CutOut (DeVries and Taylor, 2017), Aug-Mix (Hendrycks et al., 2020b), HiddenCut (Chen et al., 2021a), have been shown to substantially improve the robustness and the generalization of models. Such mitigation strategies are operated at the data level, and often hard to be interpreted in terms of how and why mitigation works.

Other lines of work deal with spans or regions associated within data points to prevent models from heavily relying on spurious patterns. To make NLP models more robust on sentiment analysis and NLI tasks, Kaushik et al. (2019) proposed curating counterfactually augmented data via a human-inthe-loop process, and showed that models trained on the combination of this augmented data and original data are less sensitive to spurious patterns. Differently, Wang et al. (2021d) performed strategic data augmentation to perturb the set of ""shortcuts"" that are automatically identified, and found that mitigating these leads to more robust models in multiple NLP tasks. This line of mitigation strategies closely relates to how spurious correlations can be measured and identified, as many of the challenging or adversarial examples (Table 1) can sometimes be used to augment the original model to improve its robustness, either in the discrete input space as additional training examples (Liu et al., 2019;Kaushik et al., 2019;Anastasopoulos et al., 2019;Vaibhav et al., 2019;Khashabi et al., 2020), or in the embedding space (Zhu et al., 2020;Zhao et al., 2018b;Miyato et al., 2017;.","(p13.0) Data augmentation recently gained a lot of interest, in improving performance in low-resourced language settings, few-shot learning, mitigating biases, and improving robustness in NLP models (Feng et al., 2021;Dhole et al., 2021). Techniques like Mixup (Zhang et al., 2018), MixText (Chen et al., 2020), CutOut (DeVries and Taylor, 2017), Aug-Mix (Hendrycks et al., 2020b), HiddenCut (Chen et al., 2021a), have been shown to substantially improve the robustness and the generalization of models. Such mitigation strategies are operated at the data level, and often hard to be interpreted in terms of how and why mitigation works.

(p13.1) Other lines of work deal with spans or regions associated within data points to prevent models from heavily relying on spurious patterns. To make NLP models more robust on sentiment analysis and NLI tasks, Kaushik et al. (2019) proposed curating counterfactually augmented data via a human-inthe-loop process, and showed that models trained on the combination of this augmented data and original data are less sensitive to spurious patterns. Differently, Wang et al. (2021d) performed strategic data augmentation to perturb the set of ""shortcuts"" that are automatically identified, and found that mitigating these leads to more robust models in multiple NLP tasks. This line of mitigation strategies closely relates to how spurious correlations can be measured and identified, as many of the challenging or adversarial examples (Table 1) can sometimes be used to augment the original model to improve its robustness, either in the discrete input space as additional training examples (Liu et al., 2019;Kaushik et al., 2019;Anastasopoulos et al., 2019;Vaibhav et al., 2019;Khashabi et al., 2020), or in the embedding space (Zhu et al., 2020;Zhao et al., 2018b;Miyato et al., 2017;.","[['b50', None, 'b78'], ['b84', 'b22', 'b62', 'b86', None, 'b14', 'b44', 'b1', 'b2']]","[['b50', None, 'b78'], ['b84', 'b22', 'b62', 'b86', None, 'b14', 'b44', 'b1', 'b2']]",12,"1. Data augmentation recently gained a lot of interest, in improving performance in low-resourced language settings, few-shot learning, mitigating biases, and improving robustness in NLP models (Feng et al., 2021;Dhole et al., 2021).
2. Techniques like Mixup (Zhang et al., 2018), MixText (Chen et al., 2020), CutOut (DeVries and Taylor, 2017), Aug-Mix (Hendrycks et al., 2020b), HiddenCut (Chen et al., 2021a), have been shown to substantially improve the robustness and the generalization of models.
3. Such mitigation strategies are operated at the data level, and often hard to be interpreted in terms of how and why mitigation works.
4. Other lines of work deal with spans or regions associated within data points to prevent models from heavily relying on spurious patterns.
5. To make NLP models more robust on sentiment analysis and NLI tasks, Kaushik et al. (2019) proposed curating counterfactually augmented data via a human-inthe-loop process, and showed that models trained on the combination of this augmented data and original data are less sensitive to spurious patterns.
6. Differently, Wang et al. (2021d) performed strategic data augmentation to perturb the set of ""shortcuts"" that are automatically identified, and found that mitigating these leads to more robust models in multiple NLP tasks.
7. This line of mitigation strategies closely relates to how spurious correlations can be measured and identified, as many of the challenging or adversarial examples (Table 1) can sometimes be used to augment the original model to improve its robustness, either in the discrete input space as additional training examples (Liu et al., 2019;Kaushik et al., 2019;Anastasopoulos et al., 2019;Vaibhav et al., 2019;Khashabi et al., 2020), or in the embedding space (Zhu et al., 2020;Zhao et al., 2018b;Miyato et al., 2017;."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s14,Model and Training-based Approaches,"['p14.0', 'p14.1']","['Pre-training Recent work has demonstrated pretraining as an effective way to improve NLP models\' out-of-distribution robustness (Hendrycks et al., 2020a;Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations. Tu et al. (2020) showed a few other factors can also contribute to robust accuracy, including larger model size, more fine-tuning data, and longer fine-tuning. A similar observation is made by Taori et al. (2020) in the vision domain, where the authors found training with larger and more diverse datasets offer better robustness consistently in multiple cases, compared to various robustness interventions proposed in the existing literature. In general, the training strategy with an emphasis on a subset of samples that are particularly hard for the model to learn is sometimes also referred to as group DRO (Sagawa et al., 2020a), as an extension of vanilla distributional robust optimization (DRO) (Ben-Tal et al., 2013;Duchi et al., 2021). Extensions of DRO are mostly discussing the strategies on how to identify the samples considered as minority: Nam et al. (2020) trained two models in parallel, where the ""debiased"" model focuses on examples not learned by the ""biased"" model; Lahoti et al. (2020) used an adversary model to identify samples that are challenging to the main model; Liu et al. (2021) proposed to train the model a second time via up-weighting examples that have high training losses during the first time.', ""When to Use Data-driven or Model-based Approaches? In many cases both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness. One interesting phenomenon observed by (Liu et al., 2019) is to attribute models' robustness failures to blind spots in the training data, or the intrinsic learning ability of the model. The authors found that both patterns are possible: in some cases models can be inoculated via being exposed to a small amount of challenging data, similar to the data augmentation approaches mentioned in Section 5.1; on the other hand, some challenging patterns remain difficult which connects to the larger question around generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), which is relatively under-explored but deserves much attention.""]","Pre-training Recent work has demonstrated pretraining as an effective way to improve NLP models' out-of-distribution robustness (Hendrycks et al., 2020a;Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations. Tu et al. (2020) showed a few other factors can also contribute to robust accuracy, including larger model size, more fine-tuning data, and longer fine-tuning. A similar observation is made by Taori et al. (2020) in the vision domain, where the authors found training with larger and more diverse datasets offer better robustness consistently in multiple cases, compared to various robustness interventions proposed in the existing literature. In general, the training strategy with an emphasis on a subset of samples that are particularly hard for the model to learn is sometimes also referred to as group DRO (Sagawa et al., 2020a), as an extension of vanilla distributional robust optimization (DRO) (Ben-Tal et al., 2013;Duchi et al., 2021). Extensions of DRO are mostly discussing the strategies on how to identify the samples considered as minority: Nam et al. (2020) trained two models in parallel, where the ""debiased"" model focuses on examples not learned by the ""biased"" model; Lahoti et al. (2020) used an adversary model to identify samples that are challenging to the main model; Liu et al. (2021) proposed to train the model a second time via up-weighting examples that have high training losses during the first time.

When to Use Data-driven or Model-based Approaches? In many cases both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness. One interesting phenomenon observed by (Liu et al., 2019) is to attribute models' robustness failures to blind spots in the training data, or the intrinsic learning ability of the model. The authors found that both patterns are possible: in some cases models can be inoculated via being exposed to a small amount of challenging data, similar to the data augmentation approaches mentioned in Section 5.1; on the other hand, some challenging patterns remain difficult which connects to the larger question around generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), which is relatively under-explored but deserves much attention.","(p14.0) Pre-training Recent work has demonstrated pretraining as an effective way to improve NLP models' out-of-distribution robustness (Hendrycks et al., 2020a;Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations. Tu et al. (2020) showed a few other factors can also contribute to robust accuracy, including larger model size, more fine-tuning data, and longer fine-tuning. A similar observation is made by Taori et al. (2020) in the vision domain, where the authors found training with larger and more diverse datasets offer better robustness consistently in multiple cases, compared to various robustness interventions proposed in the existing literature. In general, the training strategy with an emphasis on a subset of samples that are particularly hard for the model to learn is sometimes also referred to as group DRO (Sagawa et al., 2020a), as an extension of vanilla distributional robust optimization (DRO) (Ben-Tal et al., 2013;Duchi et al., 2021). Extensions of DRO are mostly discussing the strategies on how to identify the samples considered as minority: Nam et al. (2020) trained two models in parallel, where the ""debiased"" model focuses on examples not learned by the ""biased"" model; Lahoti et al. (2020) used an adversary model to identify samples that are challenging to the main model; Liu et al. (2021) proposed to train the model a second time via up-weighting examples that have high training losses during the first time.

(p14.1) When to Use Data-driven or Model-based Approaches? In many cases both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness. One interesting phenomenon observed by (Liu et al., 2019) is to attribute models' robustness failures to blind spots in the training data, or the intrinsic learning ability of the model. The authors found that both patterns are possible: in some cases models can be inoculated via being exposed to a small amount of challenging data, similar to the data augmentation approaches mentioned in Section 5.1; on the other hand, some challenging patterns remain difficult which connects to the larger question around generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), which is relatively under-explored but deserves much attention.","[[None, 'b13', 'b28', 'b39'], [None, 'b14']]","[[None, 'b13', 'b28', 'b39'], [None, 'b14']]",6,"1. Pre-training Recent work has demonstrated pretraining as an effective way to improve NLP models' out-of-distribution robustness (Hendrycks et al., 2020a;Tu et al., 2020), potentially due to its self-supervised objective and the use of large amounts of diverse pre-training data that encourages generalization from a small number of examples that counter the spurious correlations.
2. Tu et al. (2020) showed a few other factors can also contribute to robust accuracy, including larger model size, more fine-tuning data, and longer fine-tuning.
3. A similar observation is made by Taori et al. (2020) in the vision domain, where the authors found training with larger and more diverse datasets offer better robustness consistently in multiple cases, compared to various robustness interventions proposed in the existing literature.
4. In general, the training strategy with an emphasis on a subset of samples that are particularly hard for the model to learn is sometimes also referred to as group DRO (Sagawa et al., 2020a), as an extension of vanilla distributional robust optimization (DRO) (Ben-Tal et al., 2013;Duchi et al., 2021).
5. Extensions of DRO are mostly discussing the strategies on how to identify the samples considered as minority: Nam et al. (2020) trained two models in parallel, where the ""debiased"" model focuses on examples not learned by the ""biased"" model; Lahoti et al. (2020) used an adversary model to identify samples that are challenging to the main model; Liu et al. (2021) proposed to train the model a second time via up-weighting examples that have high training losses during the first time.
6. When to Use Data-driven or Model-based Approaches?
7. In many cases both the data and the model can contribute to a model's lack of robustness, hence data-driven and model-based approaches could be combined to further improve a model's robustness.
8. One interesting phenomenon observed by (Liu et al., 2019) is to attribute models' robustness failures to blind spots in the training data, or the intrinsic learning ability of the model.
9. The authors found that both patterns are possible: in some cases models can be inoculated via being exposed to a small amount of challenging data, similar to the data augmentation approaches mentioned in Section 5.1; on the other hand, some challenging patterns remain difficult which connects to the larger question around generalizability to unseen adversarial and counterfactual patterns (Huang et al., 2020;Jha et al., 2020;Joshi and He, 2021), which is relatively under-explored but deserves much attention."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s15,Inductive-prior-based Approaches,"['p15.0', 'p15.1', 'p15.2']","['Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features. This is closely connected to the human-prior-based identification approaches in Section 4.1 as those human-priors can often be used to re-formulate the training objective with additional regularizers. To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component. The construction of this side component usually relies on prior knowledge of what the misaligned features are. Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020). Similarly, Clark et al. (2019Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases. More recent work (Xiong et al., 2021) shows the ensemble-based approaches can be further improved via better calibrating the bias-only model. Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).', ""In a broader scope, given that one of the main challenges of domain adaptation is to counter the model's tendency in learning domain-specific spurious features (Ganin et al., 2016), some methods contributing to domain adaption may have also progressed along the line of our interest, e.g., domain adversarial neural network (Ganin et al., 2016). This line of work also inspires a family of methods forcing the model to learn auxiliary-annotationinvariant representations with a side component (Ghifary et al., 2016;Wang et al., 2017;Rozantsev et al., 2018;Motiian et al., 2017;Li et al., 2018;Vernikos et al., 2020)."", 'Despite the diverse concrete ideas introduced, the above is mainly training for small empirical loss across different domains or distributions in addition to forcing the model to be invariant to domain-specific spurious features. As an extension along this direction, invariant risk minimization (IRM) (Arjovsky et al., 2019) introduces the idea of invariant predictors across multiple environments, which was later followed and discussed by a variety of extensions (Choe et al., 2020;Ahmed et al., 2020;Rosenfeld et al., 2021). More recently, Dranker et al. (2021) applied IRM in natural language inference and found that a more naturalistic characterization of the problem setup is needed.']","Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features. This is closely connected to the human-prior-based identification approaches in Section 4.1 as those human-priors can often be used to re-formulate the training objective with additional regularizers. To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component. The construction of this side component usually relies on prior knowledge of what the misaligned features are. Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020). Similarly, Clark et al. (2019Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases. More recent work (Xiong et al., 2021) shows the ensemble-based approaches can be further improved via better calibrating the bias-only model. Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).

In a broader scope, given that one of the main challenges of domain adaptation is to counter the model's tendency in learning domain-specific spurious features (Ganin et al., 2016), some methods contributing to domain adaption may have also progressed along the line of our interest, e.g., domain adversarial neural network (Ganin et al., 2016). This line of work also inspires a family of methods forcing the model to learn auxiliary-annotationinvariant representations with a side component (Ghifary et al., 2016;Wang et al., 2017;Rozantsev et al., 2018;Motiian et al., 2017;Li et al., 2018;Vernikos et al., 2020).

Despite the diverse concrete ideas introduced, the above is mainly training for small empirical loss across different domains or distributions in addition to forcing the model to be invariant to domain-specific spurious features. As an extension along this direction, invariant risk minimization (IRM) (Arjovsky et al., 2019) introduces the idea of invariant predictors across multiple environments, which was later followed and discussed by a variety of extensions (Choe et al., 2020;Ahmed et al., 2020;Rosenfeld et al., 2021). More recently, Dranker et al. (2021) applied IRM in natural language inference and found that a more naturalistic characterization of the problem setup is needed.","(p15.0) Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features. This is closely connected to the human-prior-based identification approaches in Section 4.1 as those human-priors can often be used to re-formulate the training objective with additional regularizers. To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component. The construction of this side component usually relies on prior knowledge of what the misaligned features are. Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020). Similarly, Clark et al. (2019Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases. More recent work (Xiong et al., 2021) shows the ensemble-based approaches can be further improved via better calibrating the bias-only model. Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).

(p15.1) In a broader scope, given that one of the main challenges of domain adaptation is to counter the model's tendency in learning domain-specific spurious features (Ganin et al., 2016), some methods contributing to domain adaption may have also progressed along the line of our interest, e.g., domain adversarial neural network (Ganin et al., 2016). This line of work also inspires a family of methods forcing the model to learn auxiliary-annotationinvariant representations with a side component (Ghifary et al., 2016;Wang et al., 2017;Rozantsev et al., 2018;Motiian et al., 2017;Li et al., 2018;Vernikos et al., 2020).

(p15.2) Despite the diverse concrete ideas introduced, the above is mainly training for small empirical loss across different domains or distributions in addition to forcing the model to be invariant to domain-specific spurious features. As an extension along this direction, invariant risk minimization (IRM) (Arjovsky et al., 2019) introduces the idea of invariant predictors across multiple environments, which was later followed and discussed by a variety of extensions (Choe et al., 2020;Ahmed et al., 2020;Rosenfeld et al., 2021). More recently, Dranker et al. (2021) applied IRM in natural language inference and found that a more naturalistic characterization of the problem setup is needed.","[[None, 'b69', 'b28', 'b16', 'b51'], ['b25', None, 'b56'], [None]]","[[None, 'b69', 'b28', 'b16', 'b51'], ['b25', None, 'b56'], [None]]",9,"1. Another thread is to introduce inductive bias (i.e., to regularize the hypothesis space) to force the model to discard some spurious features.
2. This is closely connected to the human-prior-based identification approaches in Section 4.1 as those human-priors can often be used to re-formulate the training objective with additional regularizers.
3. To achieve this goal, one usually needs to first construct a side component to inform the main model about the misaligned features, and then to regularize the main model according to the side component.
4. The construction of this side component usually relies on prior knowledge of what the misaligned features are.
5. Then, methods can be built accordingly to counter the features such as label-associated keywords (He et al., 2019), label-associated text fragments (Mahabadi et al., 2020), and general easy-to-learn patterns of data (Nam et al., 2020).
6. Similarly, Clark et al. (2019Utama et al. (2020a,b) propose to ensemble with a model explicitly capturing bias, where the main model is trained together with this ""bias-only"" model such that the main model is discouraged from using biases.
7. More recent work (Xiong et al., 2021) shows the ensemble-based approaches can be further improved via better calibrating the bias-only model.
8. Furthermore, additional regularizers have been introduced for robust fine-tuning over pre-trained models, e.g., mutual-information-based regularizers (Wang et al., 2021a) and smoothness-inducing adversarial regularization (Jiang et al., 2020).
9. In a broader scope, given that one of the main challenges of domain adaptation is to counter the model's tendency in learning domain-specific spurious features (Ganin et al., 2016), some methods contributing to domain adaption may have also progressed along the line of our interest, e.g., domain adversarial neural network (Ganin et al., 2016).
10. This line of work also inspires a family of methods forcing the model to learn auxiliary-annotationinvariant representations with a side component (He et al., 2019)0.
11. Despite the diverse concrete ideas introduced, the above is mainly training for small empirical loss across different domains or distributions in addition to forcing the model to be invariant to domain-specific spurious features.
12. As an extension along this direction, invariant risk minimization (He et al., 2019)1 (He et al., 2019)2 introduces the idea of invariant predictors across multiple environments, which was later followed and discussed by a variety of extensions (He et al., 2019)3.
13. More recently, Dranker et al. (He et al., 2019)4 applied IRM in natural language inference and found that a more naturalistic characterization of the problem setup is needed."
245144787,Measure and Improve Robustness in NLP Models: A Survey,Computer Science,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,s18,Open Questions,"['p18.0', 'p18.1', 'p18.2', 'p18.3']","['In addition to the challenges mentioned above, we list below a few open questions that call for additional research going forward.', ""Identifying Unknown Robustness Failures Existing identification around robustness failures rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to. This requires extensive amount of expertise and efforts, and might still suffer from human or subjective biases in the end. How to proactively discover and identify models' unrobust regions automatically and comprehensively remains challenging."", ""Interpreting and Mitigating Spurious Correlations Interpretability matters for large NLP models, especially key to the robustness and spurious patterns. How can we develop ways to attribute or interpret these vulnerable portions of NLP models and communicate these robustness failures with designers, practitioners, and users? In addition, recent work (Wallace et al., 2019c;Wang et al., 2021d;Zhang et al., 2021) show interpretability methods can be utilized to better understand how a model makes its decision, which in turn can be used to uncover models' bias, diagnose errors, and discover spurious correlations."", 'Furthermore, the mitigation of spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance Zhang et al., 2019a). Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process. How to design end-to-end frameworks for automatic mitigation deserves much attention.']","In addition to the challenges mentioned above, we list below a few open questions that call for additional research going forward.

Identifying Unknown Robustness Failures Existing identification around robustness failures rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to. This requires extensive amount of expertise and efforts, and might still suffer from human or subjective biases in the end. How to proactively discover and identify models' unrobust regions automatically and comprehensively remains challenging.

Interpreting and Mitigating Spurious Correlations Interpretability matters for large NLP models, especially key to the robustness and spurious patterns. How can we develop ways to attribute or interpret these vulnerable portions of NLP models and communicate these robustness failures with designers, practitioners, and users? In addition, recent work (Wallace et al., 2019c;Wang et al., 2021d;Zhang et al., 2021) show interpretability methods can be utilized to better understand how a model makes its decision, which in turn can be used to uncover models' bias, diagnose errors, and discover spurious correlations.

Furthermore, the mitigation of spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance Zhang et al., 2019a). Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process. How to design end-to-end frameworks for automatic mitigation deserves much attention.","(p18.0) In addition to the challenges mentioned above, we list below a few open questions that call for additional research going forward.

(p18.1) Identifying Unknown Robustness Failures Existing identification around robustness failures rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to. This requires extensive amount of expertise and efforts, and might still suffer from human or subjective biases in the end. How to proactively discover and identify models' unrobust regions automatically and comprehensively remains challenging.

(p18.2) Interpreting and Mitigating Spurious Correlations Interpretability matters for large NLP models, especially key to the robustness and spurious patterns. How can we develop ways to attribute or interpret these vulnerable portions of NLP models and communicate these robustness failures with designers, practitioners, and users? In addition, recent work (Wallace et al., 2019c;Wang et al., 2021d;Zhang et al., 2021) show interpretability methods can be utilized to better understand how a model makes its decision, which in turn can be used to uncover models' bias, diagnose errors, and discover spurious correlations.

(p18.3) Furthermore, the mitigation of spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance Zhang et al., 2019a). Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process. How to design end-to-end frameworks for automatic mitigation deserves much attention.","[[], [], ['b49', None, 'b62'], ['b77']]","[[], [], ['b49', None, 'b62'], ['b77']]",4,"1. In addition to the challenges mentioned above, we list below a few open questions that call for additional research going forward.
2. Identifying Unknown Robustness Failures Existing identification around robustness failures rely heavily on human priors and error analyses, which usually pre-define a small or limited set of patterns that the model could be vulnerable to.
3. This requires extensive amount of expertise and efforts, and might still suffer from human or subjective biases in the end.
4. How to proactively discover and identify models' unrobust regions automatically and comprehensively remains challenging.
5. Interpreting and Mitigating Spurious Correlations Interpretability matters for large NLP models, especially key to the robustness and spurious patterns.
6. How can we develop ways to attribute or interpret these vulnerable portions of NLP models and communicate these robustness failures with designers, practitioners, and users?
7. In addition, recent work (Wallace et al., 2019c;Wang et al., 2021d;Zhang et al., 2021) show interpretability methods can be utilized to better understand how a model makes its decision, which in turn can be used to uncover models' bias, diagnose errors, and discover spurious correlations.
8. Furthermore, the mitigation of spurious correlations often suffers from the trade-off between removing shortcuts and sacrificing model performance Zhang et al., 2019a).
9. Additionally, most existing mitigation strategies work in a pipeline fashion where defining and detecting spurious correlations are prerequisites, which might lead to error cascades in this process.
10. How to design end-to-end frameworks for automatic mitigation deserves much attention."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s2,Source(s) Target,"['p2.0', 'p2.1', 'p2.2', 'p2.3']","[""Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread."", '2 What is Stance?', 'In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker\'s standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).', 'Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.']","Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","(p2.0) Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

(p2.1) 2 What is Stance?

(p2.2) In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

(p2.3) Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","[['b12', None, 'b57', 'b17'], [], [None], []]","[['b12', None, 'b57', 'b17'], [], [None], []]",5,"1. Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011)
2. Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b)
3. Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article *
4. 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '
5. 17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection.
6. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds.
7. * the article's body is summarised.
8. Sources: Twitter, ǌ News, ɀikipedia, Reddit.
9. Evidence: Single, Multiple, Thread.
10. 2 What is Stance? In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process.
11. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition.
12. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither.
13. Occasionally, the category label of Neutral is also added to the set of stance categories (Zubiaga et al., 2016b)0, and the target may or may not be explicitly mentioned in the text"" (Zubiaga et al., 2016b)1.
14. Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (Zubiaga et al., 2016b)2.
15. Finally, stance detection can be distinguished from several other closely related NLP tasks: (Zubiaga et al., 2016b)3 biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (Zubiaga et al., 2016b)4 emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (Zubiaga et al., 2016b)5 perspective identification, which aims to find the pointof-view of the author (Zubiaga et al., 2016b)6 and the target is always explicit, (Zubiaga et al., 2016b)7 sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (Zubiaga et al., 2016b)8 sentiment analysis, which checks the polarity of a piece of text."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s6,Multiple languages,"['p6.0', 'p6.1', 'p6.2', 'p6.3', 'p6.4', 'p6.5', 'p6.6', 'p6.7', 'p6.8', 'p6.9', 'p6.10', 'p6.11', 'p6.12', 'p6.13']","['In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.', 'Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.', ""Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim."", 'More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.', 'Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).', 'Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.', 'Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).', 'Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.', 'Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.', ""These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019)."", 'A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).', 'Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.', 'Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.', 'Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.']","In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","(p6.0) In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

(p6.1) Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

(p6.2) Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

(p6.3) More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

(p6.4) Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

(p6.5) Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

(p6.6) Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

(p6.7) Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

(p6.8) Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

(p6.9) These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

(p6.10) A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

(p6.11) Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

(p6.12) Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

(p6.13) Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","[[], [None, 'b19'], [], ['b31'], [None], ['b43', None, 'b52', 'b30', 'b50', 'b8', 'b51', 'b47'], [None], [], [None, 'b57'], ['b55', None, 'b36', 'b49'], ['b49', None, 'b57'], [None, 'b38'], [], ['b25', 'b24', None]]","[[], [None, 'b19'], [], ['b31'], [None], ['b43', None, 'b52', 'b30', 'b50', 'b8', 'b51', 'b47'], [None], [], [None, 'b57'], ['b55', None, 'b36', 'b49'], ['b49', None, 'b57'], [None, 'b38'], [], ['b25', 'b24', None]]",27,"1. In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1.
2. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance.
3. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead.
4. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a).
5. Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help.
6. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful.
7. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance.
8. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1).
9. Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general.
10. This can be attributed to the use of n-grams, topic models, and lexica.
11. Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents.
12. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t.
13. the target claim. More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.
14. Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help.
15. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree.
16. The success of these models is also seen in cross-lingual settings.
17. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT.
18. Similarly, Hardalov et al. (F1)6 applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets.
19. Alhindi et al. (over 99 F1)6 showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).
20. Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER .
21. To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted.
22. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021).
23. Zhou et al. (over 99 F1)3 showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score.
24. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points.
25. Liu et al. (F1)0; Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70.
26. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes.
27. Si et al. (over 99 F1)6 and Ostrowski et al. (over 99 F1)6 improved multi-hop reasoning using a model with eXtra Hop attention (F1)5, a capsule network aggregation layer, and LDA topic information.
28. Atanasova et al. (F1)6 introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.
29. Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (F1)7, or to use the perplexity of the entire claim with respect to the target document (F1)9.
30. Such models do not require a retrieval step, as they use the knowledge stored in language models.
31. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"".
32. Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type.
33. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (F1)9.
34. Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer.""
35. vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club.
36. "", (over 99 F1)0 sensitivity to spelling errors, (over 99 F1)1 lack of relation between the article and the entities in the claim, (Riedel et al., 2017;Hanselowski et al., 2018a)3 dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers.""
37. (Riedel et al., 2017;Hanselowski et al., 2018a)4 is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (Riedel et al., 2017;Hanselowski et al., 2018a)5 embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm.""
38. is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.
39. Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.
40. These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (Riedel et al., 2017;Hanselowski et al., 2018a)7.
41. A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Riedel et al., 2017;Hanselowski et al., 2018a)8 or dictionary-based token scores (Riedel et al., 2017;Hanselowski et al., 2018a)9.
42. Kumar and Carley (over 99 F1)3 replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets.
43. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020).
44. Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1).
45. Li et al. (2020) deviated from this structure and modelled the conversations as a graph.
46. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection.
47. Yang et al. (over 99 F1)3 took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task.
48. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification .
49. Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).
50. A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0.
51. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1.
52. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class.
53. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).
54. Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (over 99 F1)0 it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (over 99 F1)1 the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.
55. Multi-Dataset Learning (over 99 F1)2 Mixing data from different domains and sources can improve robustness.
56. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.
57. Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (over 99 F1)3 achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance.
58. Recently, Schiller et al. (over 99 F1)6 proposed a benchmark to evaluate the robustness of stance detection models.
59. They leveraged a pre-trained multi-task deep neural network, MT-DNN (over 99 F1)5, and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets.
60. Hardalov et al. (over 99 F1)6 (over 99 F1)7 in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.
61. They showed that MDL helps for low-resource and substantively for full-resource scenarios.
62. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance.
63. Table 2 shows the state-of-theart (over 99 F1)8 results for each dataset discussed in Section 3 and Table 1.
64. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s8,Lessons Learned and Future Trends,"['p8.0', 'p8.1', 'p8.2', 'p8.3', 'p8.4', 'p8.5']","['Dataset Size A major limitation holding back the performance of machine learning for stance detection is the size of the existing stance datasets, the vast majority of which contain at most a few thousand examples. Contrasted with the related task of Natural Language Inference, where datasets such as SNLI (Bowman et al., 2015) of more than half a million samples have been collected, this is far from optimal. Moreover, the small dataset sizes are often accompanied with skewed class distribution with very few examples from the minority classes, including many of the datasets in this study (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021).', 'This can lead to a significant disparity for label performance (see Section 4). Several techniques have been proposed to mitigate this, such as sampling strategies (Nie et al., 2019), weighting classes (Veyseh et al., 2017), 3 crafting artificial examples from auxiliary tasks Hardalov et al., 2022), or training on multiple datasets (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.', 'Data Mixing A potential way of overcoming limitations in terms of dataset size and focus is to combine multiple datasets. Yet, as we previously discussed (see Section 3), task definitions and label inventories vary across stance datasets. Further, large-scale studies of approaches that leverage the relationships between label inventories, or the similarity between datasets are still largely lacking. One promising direction is the use of label embeddings (Augenstein et al., 2018), as they offer a convenient way to learn interactions between disjoint label sets that carry semantic relations. One such first study was recently presented by Hardalov et al. (2021), which explored different strategies for leveraging inter-dataset signals and label interactions in both in-(seen targets) and out-of-domain (unseen targets) settings. This could help to overcome challenges faced by models trained on smallsize datasets, and even for smaller minority classes.', 'Multilinguality Multi-linguality is important for several reasons: (i) the content may originate in various languages, (ii) the evidence or the stance may not be expressed in the same language, thus (iii) posing a challenge for fact-checkers, who might not be speakers of the language the claim was originally made in, and (iv) it adds more data that can be leveraged for modelling stance. Currently, only a handful of datasets for factuality and stance cover languages other than English (see Table 1), and they are small in size and do not offer a cross-lingual setup. Recently, Vamvas and Sennrich (2020) proposed such a setup for three languages for stance in debates, Schick andSchütze (2021) explored few-shot learning, andHardalov et al. (2022) extended that paradigm with sentiment and stance pre-training and evaluated on twelve languages from various domains. Since cultural norms and expressed linguistic phenomena play a crucial role in understanding the context of a claim (Sap et al., 2019), we do not argue for a completely language-agnostic framework. Yet, empirically, training in cross-lingual setups improves performance by leveraging better representations learned on a similar language or by acting as a regulariser.', 'Modelling the Context Modelling the context is a particularly important, yet challenging task. In many cases, there is a need to consider the background of the stance-taker as well as the characteristics of the targeted object. In particular, in the context of social media, one can provide information about the users such as their previous activity, other users they interact most with, the threads they participate in, or even their interests (Zubiaga et al., 2016b;Gorrell et al., 2019;Li et al., 2019b). The context of the stance expressed in news articles is related to the features of the media outlets, such as source of funding, previously known biases, or credibility (Baly et al., 2019;Darwish et al., 2020;Stefanov et al., 2020;Baly et al., 2020). When using contextual information about the object, factual information about the real world, and the time of posting are all important. Incorporating these into a stance detection pipeline, while challenging, paves the way towards a robust detection process.', ""Multimodal Content Spreading mis-and disinformation through multiple modalities is becoming increasingly popular. One such example are deepfakes, i.e., synthetically created images or videos, in which (usually) the face of one person is replaced with another person's face. Another example are information propagation techniques such as memetic warfare. Hence, it is increasingly important to combine different modalities to understand the full context stance is being expressed in. Some work in this area is on fake news detection for images (Nakamura et al., 2020), claim verification for images (Zlatkova et al., 2019), or searching for fact-checked information to alleviate the spread of fake news (Vo and Lee, 2020). There has been work on meme analysis for related tasks: detecting hateful (Kiela et al., 2020), harmful (Pramanick et al., 2021;Sharma et al., 2022a), and propagandistic memes (Dimitrov et al., 2021a,b); see also a recent survey of harmful memes (Sharma et al., 2022b). This line of research is especially relevant for mis-and disinformation tasks that depend on the wisdom of the crowd in social media as it adds additional information sources (Qazvinian et al., 2011;Zubiaga et al., 2016b;Derczynski et al., 2017;Hossain et al., 2020); see Section 5.""]","Dataset Size A major limitation holding back the performance of machine learning for stance detection is the size of the existing stance datasets, the vast majority of which contain at most a few thousand examples. Contrasted with the related task of Natural Language Inference, where datasets such as SNLI (Bowman et al., 2015) of more than half a million samples have been collected, this is far from optimal. Moreover, the small dataset sizes are often accompanied with skewed class distribution with very few examples from the minority classes, including many of the datasets in this study (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021).

This can lead to a significant disparity for label performance (see Section 4). Several techniques have been proposed to mitigate this, such as sampling strategies (Nie et al., 2019), weighting classes (Veyseh et al., 2017), 3 crafting artificial examples from auxiliary tasks Hardalov et al., 2022), or training on multiple datasets (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.

Data Mixing A potential way of overcoming limitations in terms of dataset size and focus is to combine multiple datasets. Yet, as we previously discussed (see Section 3), task definitions and label inventories vary across stance datasets. Further, large-scale studies of approaches that leverage the relationships between label inventories, or the similarity between datasets are still largely lacking. One promising direction is the use of label embeddings (Augenstein et al., 2018), as they offer a convenient way to learn interactions between disjoint label sets that carry semantic relations. One such first study was recently presented by Hardalov et al. (2021), which explored different strategies for leveraging inter-dataset signals and label interactions in both in-(seen targets) and out-of-domain (unseen targets) settings. This could help to overcome challenges faced by models trained on smallsize datasets, and even for smaller minority classes.

Multilinguality Multi-linguality is important for several reasons: (i) the content may originate in various languages, (ii) the evidence or the stance may not be expressed in the same language, thus (iii) posing a challenge for fact-checkers, who might not be speakers of the language the claim was originally made in, and (iv) it adds more data that can be leveraged for modelling stance. Currently, only a handful of datasets for factuality and stance cover languages other than English (see Table 1), and they are small in size and do not offer a cross-lingual setup. Recently, Vamvas and Sennrich (2020) proposed such a setup for three languages for stance in debates, Schick andSchütze (2021) explored few-shot learning, andHardalov et al. (2022) extended that paradigm with sentiment and stance pre-training and evaluated on twelve languages from various domains. Since cultural norms and expressed linguistic phenomena play a crucial role in understanding the context of a claim (Sap et al., 2019), we do not argue for a completely language-agnostic framework. Yet, empirically, training in cross-lingual setups improves performance by leveraging better representations learned on a similar language or by acting as a regulariser.

Modelling the Context Modelling the context is a particularly important, yet challenging task. In many cases, there is a need to consider the background of the stance-taker as well as the characteristics of the targeted object. In particular, in the context of social media, one can provide information about the users such as their previous activity, other users they interact most with, the threads they participate in, or even their interests (Zubiaga et al., 2016b;Gorrell et al., 2019;Li et al., 2019b). The context of the stance expressed in news articles is related to the features of the media outlets, such as source of funding, previously known biases, or credibility (Baly et al., 2019;Darwish et al., 2020;Stefanov et al., 2020;Baly et al., 2020). When using contextual information about the object, factual information about the real world, and the time of posting are all important. Incorporating these into a stance detection pipeline, while challenging, paves the way towards a robust detection process.

Multimodal Content Spreading mis-and disinformation through multiple modalities is becoming increasingly popular. One such example are deepfakes, i.e., synthetically created images or videos, in which (usually) the face of one person is replaced with another person's face. Another example are information propagation techniques such as memetic warfare. Hence, it is increasingly important to combine different modalities to understand the full context stance is being expressed in. Some work in this area is on fake news detection for images (Nakamura et al., 2020), claim verification for images (Zlatkova et al., 2019), or searching for fact-checked information to alleviate the spread of fake news (Vo and Lee, 2020). There has been work on meme analysis for related tasks: detecting hateful (Kiela et al., 2020), harmful (Pramanick et al., 2021;Sharma et al., 2022a), and propagandistic memes (Dimitrov et al., 2021a,b); see also a recent survey of harmful memes (Sharma et al., 2022b). This line of research is especially relevant for mis-and disinformation tasks that depend on the wisdom of the crowd in social media as it adds additional information sources (Qazvinian et al., 2011;Zubiaga et al., 2016b;Derczynski et al., 2017;Hossain et al., 2020); see Section 5.","(p8.0) Dataset Size A major limitation holding back the performance of machine learning for stance detection is the size of the existing stance datasets, the vast majority of which contain at most a few thousand examples. Contrasted with the related task of Natural Language Inference, where datasets such as SNLI (Bowman et al., 2015) of more than half a million samples have been collected, this is far from optimal. Moreover, the small dataset sizes are often accompanied with skewed class distribution with very few examples from the minority classes, including many of the datasets in this study (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021).

(p8.1) This can lead to a significant disparity for label performance (see Section 4). Several techniques have been proposed to mitigate this, such as sampling strategies (Nie et al., 2019), weighting classes (Veyseh et al., 2017), 3 crafting artificial examples from auxiliary tasks Hardalov et al., 2022), or training on multiple datasets (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.

(p8.2) Data Mixing A potential way of overcoming limitations in terms of dataset size and focus is to combine multiple datasets. Yet, as we previously discussed (see Section 3), task definitions and label inventories vary across stance datasets. Further, large-scale studies of approaches that leverage the relationships between label inventories, or the similarity between datasets are still largely lacking. One promising direction is the use of label embeddings (Augenstein et al., 2018), as they offer a convenient way to learn interactions between disjoint label sets that carry semantic relations. One such first study was recently presented by Hardalov et al. (2021), which explored different strategies for leveraging inter-dataset signals and label interactions in both in-(seen targets) and out-of-domain (unseen targets) settings. This could help to overcome challenges faced by models trained on smallsize datasets, and even for smaller minority classes.

(p8.3) Multilinguality Multi-linguality is important for several reasons: (i) the content may originate in various languages, (ii) the evidence or the stance may not be expressed in the same language, thus (iii) posing a challenge for fact-checkers, who might not be speakers of the language the claim was originally made in, and (iv) it adds more data that can be leveraged for modelling stance. Currently, only a handful of datasets for factuality and stance cover languages other than English (see Table 1), and they are small in size and do not offer a cross-lingual setup. Recently, Vamvas and Sennrich (2020) proposed such a setup for three languages for stance in debates, Schick andSchütze (2021) explored few-shot learning, andHardalov et al. (2022) extended that paradigm with sentiment and stance pre-training and evaluated on twelve languages from various domains. Since cultural norms and expressed linguistic phenomena play a crucial role in understanding the context of a claim (Sap et al., 2019), we do not argue for a completely language-agnostic framework. Yet, empirically, training in cross-lingual setups improves performance by leveraging better representations learned on a similar language or by acting as a regulariser.

(p8.4) Modelling the Context Modelling the context is a particularly important, yet challenging task. In many cases, there is a need to consider the background of the stance-taker as well as the characteristics of the targeted object. In particular, in the context of social media, one can provide information about the users such as their previous activity, other users they interact most with, the threads they participate in, or even their interests (Zubiaga et al., 2016b;Gorrell et al., 2019;Li et al., 2019b). The context of the stance expressed in news articles is related to the features of the media outlets, such as source of funding, previously known biases, or credibility (Baly et al., 2019;Darwish et al., 2020;Stefanov et al., 2020;Baly et al., 2020). When using contextual information about the object, factual information about the real world, and the time of posting are all important. Incorporating these into a stance detection pipeline, while challenging, paves the way towards a robust detection process.

(p8.5) Multimodal Content Spreading mis-and disinformation through multiple modalities is becoming increasingly popular. One such example are deepfakes, i.e., synthetically created images or videos, in which (usually) the face of one person is replaced with another person's face. Another example are information propagation techniques such as memetic warfare. Hence, it is increasingly important to combine different modalities to understand the full context stance is being expressed in. Some work in this area is on fake news detection for images (Nakamura et al., 2020), claim verification for images (Zlatkova et al., 2019), or searching for fact-checked information to alleviate the spread of fake news (Vo and Lee, 2020). There has been work on meme analysis for related tasks: detecting hateful (Kiela et al., 2020), harmful (Pramanick et al., 2021;Sharma et al., 2022a), and propagandistic memes (Dimitrov et al., 2021a,b); see also a recent survey of harmful memes (Sharma et al., 2022b). This line of research is especially relevant for mis-and disinformation tasks that depend on the wisdom of the crowd in social media as it adds additional information sources (Qazvinian et al., 2011;Zubiaga et al., 2016b;Derczynski et al., 2017;Hossain et al., 2020); see Section 5.","[['b12', None, 'b57'], ['b7', None, 'b25'], [None], ['b22', None, 'b24'], [None, 'b57', 'b32'], ['b17', 'b39', 'b29', None, 'b28', 'b53', 'b16', 'b57', 'b2']]","[['b12', None, 'b57'], ['b7', None, 'b25'], [None], ['b22', None, 'b24'], [None, 'b57', 'b32'], ['b17', 'b39', 'b29', None, 'b28', 'b53', 'b16', 'b57', 'b2']]",22,"1. Dataset Size A major limitation holding back the performance of machine learning for stance detection is the size of the existing stance datasets, the vast majority of which contain at most a few thousand examples.
2. Contrasted with the related task of Natural Language Inference, where datasets such as SNLI (Bowman et al., 2015) of more than half a million samples have been collected, this is far from optimal.
3. Moreover, the small dataset sizes are often accompanied with skewed class distribution with very few examples from the minority classes, including many of the datasets in this study (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021).
4. This can lead to a significant disparity for label performance (see Section 4).
5. Several techniques have been proposed to mitigate this, such as sampling strategies (Nie et al., 2019), weighting classes (Veyseh et al., 2017), 3 crafting artificial examples from auxiliary tasks Hardalov et al., 2022), or training on multiple datasets (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.
6. Data Mixing A potential way of overcoming limitations in terms of dataset size and focus is to combine multiple datasets.
7. Yet, as we previously discussed (see Section 3), task definitions and label inventories vary across stance datasets.
8. Further, large-scale studies of approaches that leverage the relationships between label inventories, or the similarity between datasets are still largely lacking.
9. One promising direction is the use of label embeddings (Augenstein et al., 2018), as they offer a convenient way to learn interactions between disjoint label sets that carry semantic relations.
10. One such first study was recently presented by Hardalov et al. (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021)6, which explored different strategies for leveraging inter-dataset signals and label interactions in both in-(seen targets) and out-of-domain (unseen targets) settings.
11. This could help to overcome challenges faced by models trained on smallsize datasets, and even for smaller minority classes.
12. Multilinguality Multi-linguality is important for several reasons: (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021)0 the content may originate in various languages, (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021)1 the evidence or the stance may not be expressed in the same language, thus (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021)2 posing a challenge for fact-checkers, who might not be speakers of the language the claim was originally made in, and (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021)3 it adds more data that can be leveraged for modelling stance.
13. Currently, only a handful of datasets for factuality and stance cover languages other than English (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021)4, and they are small in size and do not offer a cross-lingual setup.
14. Recently, Vamvas and Sennrich (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021)5 proposed such a setup for three languages for stance in debates, Schick andSchütze (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021)6 explored few-shot learning, andHardalov et al. (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021)7 extended that paradigm with sentiment and stance pre-training and evaluated on twelve languages from various domains.
15. Since cultural norms and expressed linguistic phenomena play a crucial role in understanding the context of a claim (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021)8, we do not argue for a completely language-agnostic framework.
16. Yet, empirically, training in cross-lingual setups improves performance by leveraging better representations learned on a similar language or by acting as a regulariser.
17. Modelling the Context Modelling the context is a particularly important, yet challenging task.
18. In many cases, there is a need to consider the background of the stance-taker as well as the characteristics of the targeted object.
19. In particular, in the context of social media, one can provide information about the users such as their previous activity, other users they interact most with, the threads they participate in, or even their interests (Zubiaga et al., 2016b;Derczynski et al., 2017;Pomerleau and Rao, 2017;Baly et al., 2018b;Gorrell et al., 2019;Lillie et al., 2019;Alhindi et al., 2021)9.
20. The context of the stance expressed in news articles is related to the features of the media outlets, such as source of funding, previously known biases, or credibility (see Section 4)0.
21. When using contextual information about the object, factual information about the real world, and the time of posting are all important.
22. Incorporating these into a stance detection pipeline, while challenging, paves the way towards a robust detection process.
23. Multimodal Content Spreading mis-and disinformation through multiple modalities is becoming increasingly popular.
24. One such example are deepfakes, i.e., synthetically created images or videos, in which (see Section 4)1 the face of one person is replaced with another person's face.
25. Another example are information propagation techniques such as memetic warfare.
26. Hence, it is increasingly important to combine different modalities to understand the full context stance is being expressed in.
27. Some work in this area is on fake news detection for images (see Section 4)2, claim verification for images (see Section 4)3, or searching for fact-checked information to alleviate the spread of fake news (see Section 4)4.
28. There has been work on meme analysis for related tasks: detecting hateful (see Section 4)5, harmful (see Section 4)6, and propagandistic memes (see Section 4)7; see also a recent survey of harmful memes (see Section 4)8.
29. This line of research is especially relevant for mis-and disinformation tasks that depend on the wisdom of the crowd in social media as it adds additional information sources (see Section 4)9; see Section 5."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s9,Shades of Truth,"['p9.0', 'p9.1']","['The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).', 'Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.']","The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).

Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.","(p9.0) The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).

(p9.1) Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.","[['b20', 'b3', 'b18', 'b9'], ['b25', None]]","[['b20', 'b3', 'b18', 'b9'], ['b25', None]]",6,"1. The notion of shades of truth is important in mis-and disinformation detection.
2. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false.
3. We believe that such shades could be applied to stance and used in a larger pipeline.
4. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).
5. Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance.
6. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance.
7. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s10,Explainability,"['p10.0', 'p10.1', 'p10.2']","['The ability for a model to be able to explain its decisions is getting increasingly important, especially for mis-and disinformation detection, as one could argue that it is a crucial step towards adopting fully automated fact-checking. The FEVER 2.0 task formulation (Thorne et al., 2019) can be viewed as a step towards obtaining such explanations, e.g., there have been efforts to identify adversarial triggers that offer explanations for the vulnerabilities at the model level (Atanasova et al., 2020b). However, FEVER is artificially created and is limited to Wikipedia, which may not reflect real-world settings. To mitigate this, explanation by professional journalists can be found on fact-checking websites, and can be further combined with stance detection in an automated system. In a step in this direction, Atanasova et al. (2020a) generated natural language explanations for claims from PolitiFact 4 given gold evidence document summaries by journalists.', ""Moreover, partial explanations can be obtained automatically from the underlying models, e.g., from memory networks (Mohtarami et al., 2018), attention weights (Zhou et al., 2019;Liu et al., 2020b), or topic relations (Si et al., 2021). However, such approaches are limited as they can require gold snippets justifying the document's stance, attention weights can be misleading (Jain and Wallace, 2019), and topics might be noisy due to their unsupervised nature. Other existing systems (Popat et al., 2017(Popat et al., , 2018Nadeem et al., 2019) offer explanations to a more limited extent, highlighting span overlaps between the target text and the evidence documents. Overall, there is a need for holistic and realistic explanations of how a factchecking model arrived at its prediction."", 'Integration People question false information more and tend to confirm true information (Mendoza et al., 2010). Thus, stance can play a vital role in verifying dubious content. In Appendix C, we discuss existing systems and real-world applications of stance for mis-and disinformation identification in more detail. However, we argue that a tighter integration between stance and factchecking is needed. Stance can be expressed in different forms, e.g., tweets, news articles, user posts, sentences in Wikipedia, and Wiki tables, among others and can have different formulations as part of the fact-checking pipeline (see Section 3). All these can guide human fact-checkers through the process of fact-checking, and can point them to relevant evidence. Moreover, the wisdom of the crowd can be a powerful instrument in the fight against mis-and disinformation (Pennycook and Rand, 2019), but we should note that vocal minorities can derail public discourse (Scannell et al., 2021). Nevertheless, these risks can be mitigated by taking into account the credibility of the user or of the information source, which can be done automatically or with the help of human fact-checkers.']","The ability for a model to be able to explain its decisions is getting increasingly important, especially for mis-and disinformation detection, as one could argue that it is a crucial step towards adopting fully automated fact-checking. The FEVER 2.0 task formulation (Thorne et al., 2019) can be viewed as a step towards obtaining such explanations, e.g., there have been efforts to identify adversarial triggers that offer explanations for the vulnerabilities at the model level (Atanasova et al., 2020b). However, FEVER is artificially created and is limited to Wikipedia, which may not reflect real-world settings. To mitigate this, explanation by professional journalists can be found on fact-checking websites, and can be further combined with stance detection in an automated system. In a step in this direction, Atanasova et al. (2020a) generated natural language explanations for claims from PolitiFact 4 given gold evidence document summaries by journalists.

Moreover, partial explanations can be obtained automatically from the underlying models, e.g., from memory networks (Mohtarami et al., 2018), attention weights (Zhou et al., 2019;Liu et al., 2020b), or topic relations (Si et al., 2021). However, such approaches are limited as they can require gold snippets justifying the document's stance, attention weights can be misleading (Jain and Wallace, 2019), and topics might be noisy due to their unsupervised nature. Other existing systems (Popat et al., 2017(Popat et al., , 2018Nadeem et al., 2019) offer explanations to a more limited extent, highlighting span overlaps between the target text and the evidence documents. Overall, there is a need for holistic and realistic explanations of how a factchecking model arrived at its prediction.

Integration People question false information more and tend to confirm true information (Mendoza et al., 2010). Thus, stance can play a vital role in verifying dubious content. In Appendix C, we discuss existing systems and real-world applications of stance for mis-and disinformation identification in more detail. However, we argue that a tighter integration between stance and factchecking is needed. Stance can be expressed in different forms, e.g., tweets, news articles, user posts, sentences in Wikipedia, and Wiki tables, among others and can have different formulations as part of the fact-checking pipeline (see Section 3). All these can guide human fact-checkers through the process of fact-checking, and can point them to relevant evidence. Moreover, the wisdom of the crowd can be a powerful instrument in the fight against mis-and disinformation (Pennycook and Rand, 2019), but we should note that vocal minorities can derail public discourse (Scannell et al., 2021). Nevertheless, these risks can be mitigated by taking into account the credibility of the user or of the information source, which can be done automatically or with the help of human fact-checkers.","(p10.0) The ability for a model to be able to explain its decisions is getting increasingly important, especially for mis-and disinformation detection, as one could argue that it is a crucial step towards adopting fully automated fact-checking. The FEVER 2.0 task formulation (Thorne et al., 2019) can be viewed as a step towards obtaining such explanations, e.g., there have been efforts to identify adversarial triggers that offer explanations for the vulnerabilities at the model level (Atanasova et al., 2020b). However, FEVER is artificially created and is limited to Wikipedia, which may not reflect real-world settings. To mitigate this, explanation by professional journalists can be found on fact-checking websites, and can be further combined with stance detection in an automated system. In a step in this direction, Atanasova et al. (2020a) generated natural language explanations for claims from PolitiFact 4 given gold evidence document summaries by journalists.

(p10.1) Moreover, partial explanations can be obtained automatically from the underlying models, e.g., from memory networks (Mohtarami et al., 2018), attention weights (Zhou et al., 2019;Liu et al., 2020b), or topic relations (Si et al., 2021). However, such approaches are limited as they can require gold snippets justifying the document's stance, attention weights can be misleading (Jain and Wallace, 2019), and topics might be noisy due to their unsupervised nature. Other existing systems (Popat et al., 2017(Popat et al., , 2018Nadeem et al., 2019) offer explanations to a more limited extent, highlighting span overlaps between the target text and the evidence documents. Overall, there is a need for holistic and realistic explanations of how a factchecking model arrived at its prediction.

(p10.2) Integration People question false information more and tend to confirm true information (Mendoza et al., 2010). Thus, stance can play a vital role in verifying dubious content. In Appendix C, we discuss existing systems and real-world applications of stance for mis-and disinformation identification in more detail. However, we argue that a tighter integration between stance and factchecking is needed. Stance can be expressed in different forms, e.g., tweets, news articles, user posts, sentences in Wikipedia, and Wiki tables, among others and can have different formulations as part of the fact-checking pipeline (see Section 3). All these can guide human fact-checkers through the process of fact-checking, and can point them to relevant evidence. Moreover, the wisdom of the crowd can be a powerful instrument in the fight against mis-and disinformation (Pennycook and Rand, 2019), but we should note that vocal minorities can derail public discourse (Scannell et al., 2021). Nevertheless, these risks can be mitigated by taking into account the credibility of the user or of the information source, which can be done automatically or with the help of human fact-checkers.","[[None], ['b30', 'b52', None, 'b14', 'b1', 'b13'], ['b23', None, 'b10']]","[[None], ['b30', 'b52', None, 'b14', 'b1', 'b13'], ['b23', None, 'b10']]",10,"1. The ability for a model to be able to explain its decisions is getting increasingly important, especially for mis-and disinformation detection, as one could argue that it is a crucial step towards adopting fully automated fact-checking.
2. The FEVER 2.0 task formulation (Thorne et al., 2019) can be viewed as a step towards obtaining such explanations, e.g., there have been efforts to identify adversarial triggers that offer explanations for the vulnerabilities at the model level (Atanasova et al., 2020b).
3. However, FEVER is artificially created and is limited to Wikipedia, which may not reflect real-world settings.
4. To mitigate this, explanation by professional journalists can be found on fact-checking websites, and can be further combined with stance detection in an automated system.
5. In a step in this direction, Atanasova et al. (2020a) generated natural language explanations for claims from PolitiFact 4 given gold evidence document summaries by journalists.
6. Moreover, partial explanations can be obtained automatically from the underlying models, e.g., from memory networks (Mohtarami et al., 2018), attention weights (Zhou et al., 2019;Liu et al., 2020b), or topic relations (Si et al., 2021).
7. However, such approaches are limited as they can require gold snippets justifying the document's stance, attention weights can be misleading (Jain and Wallace, 2019), and topics might be noisy due to their unsupervised nature.
8. Other existing systems (Popat et al., 2017(Popat et al., , 2018Nadeem et al., 2019) offer explanations to a more limited extent, highlighting span overlaps between the target text and the evidence documents.
9. Overall, there is a need for holistic and realistic explanations of how a factchecking model arrived at its prediction.
10. Integration People question false information more and tend to confirm true information (Mendoza et al., 2010).
11. Thus, stance can play a vital role in verifying dubious content.
12. In Appendix C, we discuss existing systems and real-world applications of stance for mis-and disinformation identification in more detail.
13. However, we argue that a tighter integration between stance and factchecking is needed.
14. Stance can be expressed in different forms, e.g., tweets, news articles, user posts, sentences in Wikipedia, and Wiki tables, among others and can have different formulations as part of the fact-checking pipeline (see Section 3).
15. All these can guide human fact-checkers through the process of fact-checking, and can point them to relevant evidence.
16. Moreover, the wisdom of the crowd can be a powerful instrument in the fight against mis-and disinformation (Atanasova et al., 2020b)0, but we should note that vocal minorities can derail public discourse (Atanasova et al., 2020b)1.
17. Nevertheless, these risks can be mitigated by taking into account the credibility of the user or of the information source, which can be done automatically or with the help of human fact-checkers."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s14,B Additional Formulations of Stance as a Component for Fact-Checking,"['p14.0', 'p14.1', 'p14.2', 'p14.3', 'p14.4', 'p14.5', 'p14.6', 'p14.7', 'p14.8']","['Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline. Below, we describe some work that follows these formulations.', 'Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020). This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.', 'Media Profiling Stance detection has also been used for media profiling. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020). This is an important step towards understanding media biases. Tweet: Wow, that is fascinating! I hope you never mock our proud Scandi heritage again.', '(b) Examples from Qazvinian et al. (2011) andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992. ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.', '(c) Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how do you know its an ISIS flag? Can you actually confirm that? ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table 3: Illustrative examples for different stance detection scenarios included in our survey. We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).', 'The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.', ""More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling."", 'There is a well-known connection between factuality and bias. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.', 'User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).']","Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline. Below, we describe some work that follows these formulations.

Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020). This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.

Media Profiling Stance detection has also been used for media profiling. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020). This is an important step towards understanding media biases. Tweet: Wow, that is fascinating! I hope you never mock our proud Scandi heritage again.

(b) Examples from Qazvinian et al. (2011) andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992. ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.

(c) Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how do you know its an ISIS flag? Can you actually confirm that? ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table 3: Illustrative examples for different stance detection scenarios included in our survey. We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).

The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.

More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.

There is a well-known connection between factuality and bias. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.

User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).","(p14.0) Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline. Below, we describe some work that follows these formulations.

(p14.1) Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020). This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.

(p14.2) Media Profiling Stance detection has also been used for media profiling. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020). This is an important step towards understanding media biases. Tweet: Wow, that is fascinating! I hope you never mock our proud Scandi heritage again.

(p14.3) (b) Examples from Qazvinian et al. (2011) andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992. ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.

(p14.4) (c) Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how do you know its an ISIS flag? Can you actually confirm that? ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table 3: Illustrative examples for different stance detection scenarios included in our survey. We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).

(p14.5) The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.

(p14.6) More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.

(p14.7) There is a well-known connection between factuality and bias. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.

(p14.8) User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).","[[], [None], ['b32'], [None, 'b17'], [], [None, 'b0', 'b13', 'b14'], ['b16'], ['b15'], [None]]","[[], [None], ['b32'], [None, 'b17'], [], [None, 'b0', 'b13', 'b14'], ['b16'], ['b15'], [None]]",11,"1. Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline.
2. Below, we describe some work that follows these formulations.
3. Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia.
4. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions.
5. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020).
6. This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.
7. Media Profiling Stance detection has also been used for media profiling.
8. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics.
9. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020).
10. This is an important step towards understanding media biases.
11. Tweet: Wow, that is fascinating!
12. I hope you never mock our proud Scandi heritage again.
13. (b) Examples from Qazvinian et al. (2011)
14. andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki
15. Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992.
16. ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.(c)
17. Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ
18. Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ
19. Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (Shaar et al., 2020)0 u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry
20. -how do you know its an ISIS flag?
21. Can you actually confirm that? ɳ u4: @u3
22. no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney,
23. AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table 3: Illustrative examples for different stance detection scenarios included in our survey.
24. We annotate the expressed stance with (Shaar et al., 2020)1, (Shaar et al., 2020)2, ɳ (Shaar et al., 2020)3, and (Shaar et al., 2020)4.
25. The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Shaar et al., 2020)5 proposed that a trustworthy source is one that contains very few false claims.
26. More recently, Baly et al. (Shaar et al., 2020)6 used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information.
27. In follow-up work, Baly et al. (Shaar et al., 2020)7 used the same representation to jointly predict a medium's factuality of reporting (Shaar et al., 2020)8 and its bias (Shaar et al., 2020)9 on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.
28. There is a well-known connection between factuality and bias.
29. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.
30. User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user.
31. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017)."
